# GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization 深度分析

## 1. 研究背景
随着大语言模型（LLM）能力的提升，用户不仅要求其提供准确的回答，还期望其行为能在多样化的场景中与复杂的人类偏好（如效率、安全性、格式、逻辑性等）保持一致。为实现这一目标，基于强化学习（RL）的对齐方法开始整合多个奖励信号，每个信号捕获一种特定的偏好，共同引导模型产生符合期望的行为。然而，现有工作（如ToolRL）在应对多奖励优化时，通常默认直接应用为单奖励设计的 Group Relative Policy Optimization（GRPO）算法，而未检验其适用性。本文指出，在多奖励设置下，直接应用GRPO会导致关键训练信号的丢失和训练不稳定，因此需要设计更适合多目标优化的新算法。

## 2. 相关工作
传统的策略优化方法如 Proximal Policy Optimization（PPO）依赖于价值函数模型，计算复杂。GRPO及其变体（如DAPO）通过组内相对优势估计，无需价值模型，因其高效简洁而得到广泛应用。然而，GRPO主要被用于优化单一目标奖励（如准确率）。随着对多目标对齐需求的增长，近期工作开始尝试优化多个奖励，但方法较为简单，通常只是将多个奖励求和后直接应用GRPO进行优化。本文重新审视了GRPO在多奖励场景下的适用性，指出其直接应用会导致信号坍缩问题，并以此为基础提出了改进方法GDPO，定位为解决多奖励RL优化中信号精细化和训练稳定性的新SOTA方法。

## 3. 技术方法
GDPO（Group reward-Decoupled normalization Policy Optimization）的核心创新在于将多奖励的归一化过程“解耦”，以更精确地保留不同奖励组合间的相对差异。与GRPO先将所有奖励求和再进行组归一化不同，GDPO采用三步法：

1.  **独立组归一化**：对每个独立的奖励分量 \( r_k \) 分别进行组内（group-wise）归一化，计算其归一化优势值 \( A_{norm,k}^{(i,j)} \)。
    \[
    A_{norm,k}^{(i,j)} = \frac{r_k^{(i,j)} - \mu_k^{(i)}}{\sigma_k^{(i)}}
    \]
    其中，\( \mu_k^{(i)} \) 和 \( \sigma_k^{(i)} \) 是针对第 \(i\) 个问题下所有 \(G\) 个回复计算的第 \(k\) 个奖励的均值和标准差。

2.  **归一化优势聚合**：将所有奖励的归一化优势值求和，得到初步的聚合优势 \( A_{sum}^{(i,j)} \)。
    \[
    A_{sum}^{(i,j)} = \sum_{k=1}^{n} A_{norm,k}^{(i,j)}
    \]

3.  **批归一化**：对聚合优势 \( A_{sum}^{(i,j)} \) 进行批级别（batch-wise）的归一化，得到最终用于策略更新的优势值 \( \hat{A}_{sum}^{(i,j)} \)。这一步确保了最终优势值的尺度稳定，不会随奖励数量的增加而膨胀，从而提升了训练稳定性。

**关键创新**：通过解耦归一化，GDPO有效解决了GRPO的信号坍缩问题。例如，两个不同的奖励组合(0,1)和(0,2)在GRPO下可能被归一化为相同的优势值，丢失了“2优于1”的差异信息。而GDPO会分别为它们分配不同的优势值（如(-0.7071, 0.7071)和(-1.4142, 1.4142)），更忠实地反映了奖励间的相对大小，提供了更精细的训练信号。

## 4. 实验分析
**基于现有研究资料，此部分信息有限**。根据论文摘要和部分正文推断，实验主要在三个任务上对比GDPO和GRPO：
- **任务与数据集**：包括工具调用（Tool Calling）、数学推理（Math Reasoning）和代码推理（Coding Reasoning）任务。
- **评估指标**：同时评估正确性指标（如准确率、通过率、错误率）和约束遵守指标（如格式合规性、长度）。
- **主要结果**：
    - **工具调用**：GDPO在格式奖励和正确性奖励上均实现了比GRPO更好的收敛。
    - **数学推理**：在AIME数据集上，使用GDPO训练的DeepSeek-R1-1.5B和Qwen3-4B模型，相比GRPO分别取得了最高6.3%和2.3%的准确率提升，同时更好地满足了长度约束。
    - **代码推理**：GDPO在联合优化代码生成准确率、长度约束和错误率三个目标时，同样展现出优于GRPO的性能和泛化能力。
- **稳定性分析**：GDPO消除了GRPO在训练中出现的早期崩溃（early training failure）问题，训练曲线更稳定。

## 5. 核心创新
1.  **提出GDPO算法**：创新性地采用“先解耦归一化，后聚合”的流程，解决了GRPO在多奖励RL中因直接聚合导致的训练信号坍缩问题。
2.  **保留细粒度信号**：通过对每个奖励独立进行组归一化，GDPO能产生更多不同的优势组，更精确地反映不同奖励组合之间的相对差异，从而提供更高分辨率的训练信号。
3.  **增强训练稳定性**：引入额外的批归一化步骤，稳定了多奖励优势值的尺度，有效避免了训练发散和早期失败，提升了算法的鲁棒性。
4.  **系统探讨优先级整合**：分析了通过权重调整和奖励函数设计（如条件奖励）来体现目标优先级差异的策略，为实际应用提供了指导。

## 6. 局限性
**（注：论文中未明确讨论局限性，以下为基于方法特点的合理推断）**
- **计算开销**：对每个奖励独立进行组归一化，相较于GRPO直接归一化总和，可能带来轻微的计算和内存开销增加，尤其是在奖励数量非常多时。
- **超参数敏感性**：批归一化步骤的引入可能引入新的超参数（如用于数值稳定的epsilon），需要谨慎调整以确保稳定。
- **解耦归一化的假设**：该方法假设对不同奖励进行独立归一化是合理的，但在某些高度相关的多奖励场景下，这种处理方式是否最优有待进一步验证。

## 7. 应用前景
GDPO方法为复杂的大语言模型对齐任务提供了更强大的优化工具。其核心价值在于能够稳定、高效地同时优化多个可能相互竞争或补充的目标。应用前景广阔：
- **AI对齐**：更精细地平衡安全性、有用性、诚实性等多维度对齐目标。
- **复杂任务**：在需要同时满足内容质量、格式规范、长度控制、特定风格等要求的场景（如代码生成、报告撰写、对话机器人）中，实现更精准的行为塑造。
- **未来方向**：可将解耦归一化思想与其他高级RL算法结合，探索在离线RL、多智能体等更复杂设置中的应用，并进一步研究自适应权重分配与动态优先级机制。

## 8. 参考资料
- [GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization](https://arxiv.org/abs/2601.05242)
- 论文中引用的相关资源：HF-TRL, verl, Nemo-RL 实现库；Project, Lab 相关链接。