[
  {
    "id": "2601.05242",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Peter Belcak",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05242.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05242",
    "published": "2026-01-08T18:59:24Z",
    "updated": "2026-01-08T18:59:24Z",
    "comment": "NVIDIA-Tech Report",
    "light_analysis": {
      "overview": "本文提出GDPO方法，通过解耦奖励归一化，解决多奖励强化学习中GRPO导致的训练信号坍缩问题。",
      "motivation": "随着语言模型能力增强，用户期望其在多种场景下与多样人类偏好对齐，强化学习管道开始采用多个奖励引导模型。然而，现有方法如GRPO在多奖励设置下直接应用，由于归一化导致不同奖励组合坍缩为相同优势值，降低训练信号分辨率，引发次优收敛和早期训练失败，突显改进多奖励优化方法的必要性。",
      "method": "论文引入Group reward-Decoupled Normalization Policy Optimization (GDPO)，一种新政策优化方法。核心创新在于解耦个体奖励的归一化过程，独立处理每个奖励的归一化，而非像GRPO那样组合归一化。这更忠实保留奖励相对差异，提升多奖励优化的准确性和训练稳定性，适用于各种强化学习任务。",
      "result": "在工具调用、数学推理和编码推理三个任务上比较GDPO与GRPO，评估正确性指标（如准确率、错误率）和约束遵守指标（如格式、长度）。结果显示，GDPO在所有任务和指标上都一致优于GRPO，证明其在多奖励强化学习优化中的有效性和泛化能力，显著改善训练收敛和性能。",
      "conclusion": "本文主要贡献是提出GDPO方法，解决多奖励强化学习中GRPO的归一化问题，通过解耦奖励归一化保留差异，实现更准确优化和更高训练稳定性。研究具有学术价值，为多目标强化学习提供新思路，实际应用可提升语言模型与人类偏好的对齐能力，未来工作可能包括扩展场景或进一步优化算法。",
      "tags": [
        "Reinforcement Learning",
        "Multi-reward Optimization",
        "Policy Optimization",
        "Normalization",
        "Group Relative Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:48.677497Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05241",
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "authors": [
      "Boyang Wang",
      "Haoran Zhang",
      "Shujie Zhang",
      "Jinkun Hao",
      "Mingda Jia",
      "Qi Lv",
      "Yucheng Mao",
      "Zhaoyang Lyu",
      "Jia Zeng",
      "Xudong Xu",
      "Jiangmiao Pang"
    ],
    "abstract": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05241.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05241",
    "published": "2026-01-08T18:59:22Z",
    "updated": "2026-01-08T18:59:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种使用视觉身份提示技术引导多视图视频生成的方法，以增强机器人操纵数据的多样性和质量，从而提升策略模型的性能。",
      "motivation": "机器人操纵数据的多样性、数量和质量对训练有效策略至关重要，但由于硬件和物理设置的限制，收集大规模真实世界数据仍然困难，难以扩展到多样化环境。现有方法依赖于文本提示的图像扩散模型来增强数据，但往往忽视了对多视图和时间一致性观察的需求，这些是先进策略模型所必需的；此外，仅使用文本提示无法可靠地指定场景设置，导致生成数据不准确。因此，研究动机在于开发一种能提供明确视觉指导的方法，以克服现有技术的不足，提高数据增强的实用性和可靠性。",
      "method": "论文的核心方法是引入视觉身份提示技术，使用示例图像作为条件输入来引导扩散模型生成所需场景设置的多视图视频，取代了仅依赖文本提示的传统方式。关键创新点在于通过视觉身份提供明确的视觉指导，确保生成的观察具有多视角和时间一致性。此外，研究建立了一个可扩展的流水线，从大型机器人数据集中构建和管理视觉身份池，以支持高效的数据生成。该方法结合了扩散模型和示例图像，旨在生成高质量、多样化的操纵数据，适用于下游的视觉-语言-动作和视觉运动策略模型训练。",
      "result": "使用视觉身份提示增强的操纵数据训练下游的视觉-语言-动作和视觉运动策略模型后，在仿真和真实机器人设置中取得了稳定的性能增益。摘要未提供具体的准确率或效率改进数值，但结果表明该方法能一致地提升策略模型的效果，与基线方法相比有明显进步。这些性能提升验证了视觉身份提示在生成多视图和时间一致数据方面的有效性，为机器人学习提供了更可靠的训练数据，支持了其在复杂环境中的泛化能力。",
      "conclusion": "该研究的主要贡献是提出了视觉身份提示技术，有效解决了机器人操纵数据增强中的多视图和时间一致性问题，显著提升了策略模型的训练效果。这不仅拓展了扩散模型在机器人领域的应用，还提供了实际应用价值，例如促进真实世界机器人任务的泛化能力。未来工作可进一步优化生成数据的质量和多样性，或探索更广泛的环境设置，以应对潜在的局限性如场景复杂性或数据保真度不足。",
      "tags": [
        "Visual Identity Prompting",
        "Multi-View Video Generation",
        "Diffusion Models",
        "Robot Manipulation",
        "Data Augmentation"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:35.570244Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05240",
    "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
    "authors": [
      "Ilmo Sung"
    ],
    "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "hep-th"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05240.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05240",
    "published": "2026-01-08T18:58:34Z",
    "updated": "2026-01-08T18:58:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出将鲁棒推理建模为对称保护拓扑相，并基于此设计 Holonomic Network 来解决大型语言模型的幻觉问题。",
      "motivation": "研究动机是大型语言模型存在‘幻觉’问题，表现为逻辑不一致，由语义噪声引起。当前架构如 Transformer 和 RNN 处于‘度量相’，其因果顺序易受自发对称破缺影响，导致推理不稳定。因此，需要一种鲁棒方法来实现可靠的推理，以避免逻辑错误，这对提升 AI 系统的可信度和在复杂任务中的表现至关重要。",
      "method": "研究方法是将鲁棒推理识别为对称保护拓扑相，逻辑操作形式上同构于非阿贝尔任意子编织。核心创新是设计 Holonomic Network，用鲁棒的拓扑不变量替代脆弱的几何插值，依赖于非阿贝尔规范对称性来保持因果顺序。具体细节包括在变量绑定任务上验证，使用 S10 数据集（3.6×10^6 个状态），模型架构基于拓扑不变性实现因果保护。",
      "result": "实验结果显示，Holonomic Network 在拓扑相变中显示出宏观‘质量间隙’，在临界噪声阈值下保持不变的保真度。在变量绑定任务中，拓扑模型能外推到训练数据 100 倍以外（从 L=50 到 5000），保持完美保真度，而 Transformers 和 RNNs 则失去逻辑一致性。消融研究确认这种保护严格源于非阿贝尔规范对称性，与基线方法相比，性能显著提升。",
      "conclusion": "论文的主要贡献是提出了逻辑推理的新普适性类别，将因果稳定性与语义流形的拓扑联系起来。这具有重要学术价值，为开发鲁棒的 AI 系统提供了理论基础，可应用于符号推理和语言模型等领域。摘要未明确说明局限性，未来工作可能涉及扩展到更广泛的推理任务和噪声环境。",
      "tags": [
        "Symmetry-Protected Topological Phase",
        "Non-Abelian Anyons",
        "Topological Invariants",
        "Holonomic Network",
        "Causal Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:45.277892Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05251",
    "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
    "authors": [
      "Zeren Jiang",
      "Chuanxia Zheng",
      "Iro Laina",
      "Diane Larlus",
      "Andrea Vedaldi"
    ],
    "abstract": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05251.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05251",
    "published": "2026-01-08T18:59:56Z",
    "updated": "2026-01-08T18:59:56Z",
    "comment": "15 pages, 8 figures, project page: https://mesh-4d.github.io/",
    "light_analysis": {
      "overview": "论文提出 Mesh4D 模型，通过紧凑潜在空间和潜在扩散模型，实现从单目视频进行 4D 网格重建的前馈方法。",
      "motivation": "该研究旨在解决从单目视频重建动态对象完整 3D 形状和运动的实际问题，这在计算机视觉、图形学和动画领域有重要应用。现有方法可能依赖骨骼结构等外部信息，导致处理效率低下或泛化能力不足。Mesh4D 的设计动机是无需推理时骨骼信息，通过更稳定的表示来改进动态对象重建的准确性和效率，以克服现有技术的限制。摘要未明确说明所有现有方法的具体不足，但强调了无需骨骼推理的优势。",
      "method": "Mesh4D 采用前馈模型架构，核心方法包括一个自动编码器来学习紧凑潜在空间，该空间通过单次前向传播编码整个动画序列。编码器使用时空注意力机制，提升对象整体变形的表示稳定性。训练时，自动编码器利用训练对象的骨骼结构作为先验，引导学习合理的变形，但推理时不需骨骼信息。在此基础上，模型训练一个潜在扩散模型，以输入视频和第一帧重建的网格为条件，一次性预测完整动画。关键创新点包括紧凑潜在空间设计、时空注意力应用以及潜在扩散模型的集成。",
      "result": "论文在重建和新视角合成基准测试上评估 Mesh4D，结果表明其优于现有方法，在恢复准确 3D 形状和变形方面表现优异。摘要未明确提供具体性能指标如准确率或效率改进数据，但强调了在基准测试中的领先地位。与基线方法的对比显示出 Mesh4D 在稳定性和准确性上的提升，尽管具体数值未在摘要中详细说明。",
      "conclusion": "Mesh4D 的主要贡献是提出了一种高效的 4D 网格重建方法，结合紧凑潜在空间和潜在扩散模型，实现从单目视频的稳定动画预测。其学术价值在于引入新的表示学习技术，为动态对象重建提供了创新思路；实际应用价值体现在动画制作、虚拟现实等领域。局限性或未来工作方向未在摘要中明确提及，但可能涉及扩展到更复杂场景或实时处理优化。",
      "tags": [
        "4D Mesh Reconstruction",
        "Monocular Video",
        "Autoencoder",
        "Spatio-temporal Attention",
        "Latent Diffusion Model"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:30.558033Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05249",
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "authors": [
      "Yuan-Kang Lee",
      "Kuan-Lin Chen",
      "Chia-Che Chang",
      "Yu-Lun Liu"
    ],
    "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05249.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05249",
    "published": "2026-01-08T18:59:55Z",
    "updated": "2026-01-08T18:59:55Z",
    "comment": "Project page: https://ntuneillee.github.io/research/rl-awb/",
    "light_analysis": {
      "overview": "RL-AWB框架首次将深度强化学习应用于夜间色彩恒常性，结合统计方法动态优化白平衡参数。",
      "motivation": "夜间场景的低光噪声和复杂照明条件使得色彩恒常性成为计算摄影中的挑战性问题，传统方法在低光环境下可能因色彩失真和噪声干扰而表现不佳。本研究旨在改进夜间自动白平衡校正技术，通过结合统计和深度强化学习来解决现有方法的不足，提升图像质量和适应性，满足摄影应用中对鲁棒性和准确性的需求。",
      "method": "RL-AWB方法首先采用为夜间场景定制的统计算法，整合显著灰度像素检测和新颖的照明估计。在此基础上，开发了首个基于深度强化学习的色彩恒常性方法，将统计算法作为核心，通过动态优化每个图像的参数来模拟专业AWB调优专家的行为。该方法还引入了第一个多传感器夜间数据集，用于跨传感器评估和泛化性能验证。",
      "result": "实验结果表明，RL-AWB方法在低光夜间和良好照明图像上均展现出卓越的泛化能力。尽管摘要未明确说明具体性能指标，但通过与多传感器数据集上的评估，该方法实现了优于传统方法的适应性和准确性，证明了其在复杂环境下的有效性。其在跨传感器场景中显示出更好的鲁棒性，提升了图像色彩一致性。",
      "conclusion": "本研究的主要贡献是提出了RL-AWB框架，首次将深度强化学习应用于色彩恒常性领域，结合统计方法提升夜间白平衡性能。其学术价值在于扩展了自适应算法在计算机视觉中的应用，实际价值在于改善低光环境下的图像质量，为计算摄影提供新工具。未来工作可探索更多场景适应和传感器集成，以应对潜在局限性如计算成本。",
      "tags": [
        "Deep Reinforcement Learning",
        "Color Constancy",
        "Auto White Balance",
        "Statistical Methods",
        "Multi-sensor Dataset"
      ]
    },
    "analyzed_at": "2026-01-09T05:12:44.643474Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05250",
    "title": "QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer",
    "authors": [
      "Daniele Lizzio Bosco",
      "Shuteng Wang",
      "Giuseppe Serra",
      "Vladislav Golyanik"
    ],
    "abstract": "Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05250.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05250",
    "published": "2026-01-08T18:59:55Z",
    "updated": "2026-01-08T18:59:55Z",
    "comment": "30 pages, 15 figures, 11 tables; project page: https://4dqv.mpi-inf.mpg.de/QNeRF/",
    "light_analysis": {
      "overview": "QNeRF 是首个用于新颖视图合成的混合量子-经典模型，通过量子叠加和纠缠实现更紧凑的表示。",
      "motivation": "量子视觉场（QVFs）最近在模型紧凑性和收敛速度方面显示潜力，而神经辐射场（NeRFs）在新颖视图合成中取得进展，但模型参数多、训练成本高。现有方法如 NeRF 的模型较大，限制了实际部署和效率。本研究动机是结合量子计算的优势，扩展 QVFs 到 NeRF 任务，解决模型紧凑性和训练密集问题，探索量子机器学习在计算机视觉中中等任务的应用前景。",
      "method": "研究提出 QNeRF，一种混合量子-经典模型，在模拟门控量子计算机上使用参数化量子电路编码空间和视图相关信息，核心创新是通过量子叠加和纠缠实现紧凑表示。设计了两个变体：Full QNeRF 最大化利用量子振幅增强表示能力；Dual-Branch QNeRF 引入任务引导的归纳偏置，通过分支空间和视图相关的量子态准备来降低复杂度，确保可扩展性和潜在硬件兼容性。",
      "result": "在中等分辨率图像上训练时，QNeRF 的性能匹配或优于经典 NeRF 基线模型，同时参数数量少于基线的一半。具体实验表明，量子模型在减少模型大小的同时保持了或提升了表示能力，验证了其在紧凑性方面的优势，但摘要未提供准确率等详细量化数据，仅指出优于基线。",
      "conclusion": "论文主要贡献是引入 QNeRF，证明量子机器学习在计算机视觉中连续信号表示任务的竞争性，为从 2D 观测进行 3D 表示学习提供了新方法。学术价值在于推动混合量子-经典模型在中等视觉任务中的应用。局限性摘要未明确说明，未来工作可能涉及扩展到更高分辨率任务或实际量子硬件部署。",
      "tags": [
        "Quantum Machine Learning",
        "Neural Radiance Fields",
        "Parameterized Quantum Circuits",
        "Quantum Superposition and Entanglement",
        "Hybrid Quantum-Classical Model"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:38.439186Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05246",
    "title": "Pixel-Perfect Visual Geometry Estimation",
    "authors": [
      "Gangwei Xu",
      "Haotong Lin",
      "Hongcheng Luo",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Sida Peng",
      "Hangjun Ye",
      "Xin Yang"
    ],
    "abstract": "Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05246.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05246",
    "published": "2026-01-08T18:59:49Z",
    "updated": "2026-01-08T18:59:49Z",
    "comment": "Code: https://github.com/gangweix/pixel-perfect-depth",
    "light_analysis": {
      "overview": "该论文提出基于像素空间扩散变换器的像素完美视觉几何模型，通过生成建模和关键设计实现高质量、无飞行像素的深度估计。",
      "motivation": "该研究旨在解决从图像中恢复几何结构时存在的飞行像素和细节丢失问题，这对于机器人和增强现实应用至关重要。现有几何基础模型在这些方面表现不足，导致输出不准确，影响下游任务。因此，需要开发一种新方法来预测高质量、无飞行像素的点云，以提升实际应用的性能和可靠性。",
      "method": "该论文提出像素完美深度模型，利用像素空间生成建模，基于扩散变换器进行深度估计。关键创新包括语义提示的扩散变换器，结合视觉基础模型的语义表示来增强细节；以及级联扩散变换器架构，逐步增加图像令牌以提高效率和准确率。扩展至视频时，引入语义一致的扩散变换器，从多视图几何基础模型提取时间一致语义，并通过参考引导的令牌传播维持时间连贯性，减少计算和内存开销。",
      "result": "实验结果表明，该模型在所有生成式单目和视频深度估计模型中取得了最佳性能，点云输出比所有其他模型都显著更干净，减少了飞行像素和细节丢失。然而，摘要未明确说明具体的准确率提升或效率改进等详细数据，仅强调了整体性能的优越性和与基线方法的对比优势。",
      "conclusion": "该论文的主要贡献是提出了像素完美视觉几何模型，通过生成建模和扩散变换器的创新应用，实现了高质量、无飞行像素的几何估计。这具有重要的学术价值，推动了基础模型在视觉几何任务中的发展，并展示了实际应用潜力，如提升机器人和增强现实系统的性能。未来工作可能涉及进一步优化计算效率或扩展到更多复杂场景。",
      "tags": [
        "Diffusion Transformers",
        "Semantic Prompting",
        "Cascade Architecture",
        "Token Propagation",
        "Monocular Depth Estimation"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:26.000334Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05244",
    "title": "GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation",
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Shuting He",
      "Xudong Jiang",
      "Yu-Gang Jiang"
    ],
    "abstract": "Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05244.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05244",
    "published": "2026-01-08T18:59:30Z",
    "updated": "2026-01-08T18:59:30Z",
    "comment": "IJCV, Project Page: https://henghuiding.com/GREx/",
    "light_analysis": {
      "overview": "本文提出了GREx框架，扩展Referring Expression任务以支持多目标和无目标表达式，并构建了gRefCOCO数据集及ReLA方法，提升了任务的实用性和性能。",
      "motivation": "现有Referring Expression Segmentation (RES)、Comprehension (REC)和Generation (REG)任务通常仅支持单目标表达式，即每个表达式对应一个对象，忽略了现实应用中常见的多目标和无目标场景。这限制了REx技术在复杂环境下的应用，因为实际场景往往涉及多个对象或无明确对象指代。现有方法的不足在于缺乏通用性和适应性，无法处理更广泛的表达式类型。因此，本研究旨在解决这一局限性，通过扩展任务定义来涵盖任意数量对象的表达式，从而推动视觉-语言交互领域的实际应用。",
      "method": "论文提出了三个新基准：Generalized Referring Expression Segmentation (GRES)、Comprehension (GREC)和Generation (GREG)，统称为GREx，以扩展经典REx任务。构建了首个大规模数据集gRefCOCO，包含多目标、无目标和单目标表达式，以及标注对象的图像，确保向后兼容现有REx。为应对GRES/GREC中的复杂关系建模挑战，设计了基线方法ReLA，通过自适应地将图像划分为具有子实例线索的区域，并显式建模区域-区域和区域-语言依赖关系。ReLA利用了区域划分和依赖捕捉技术，增强了模型对复杂场景的理解能力。",
      "result": "ReLA方法在GRES和GREC任务上取得了state-of-the-art结果，证明了其在处理泛化表达式任务上的有效性。摘要未明确说明具体性能指标，如准确率或效率提升，但指出ReLA优于现有方法，并揭示了现有REx方法在GREx任务上的性能差距。实验表明，该方法在建模多目标和复杂关系方面具有优势，为后续研究提供了基准。结果还强调了gRefCOCO数据集的实用性，通过后向兼容性，促进了广泛实验评估现有方法的局限性。",
      "conclusion": "论文的主要贡献在于提出GREx框架和gRefCOCO数据集，扩展了REx任务以支持多目标和无目标表达式，解决了现有方法的局限。这具有重要的学术价值，推动了视觉-语言交互研究的泛化方向，并提供向后兼容基准以促进比较实验。实际应用价值体现在增强模型对现实复杂场景的处理能力，例如多对象识别和无目标表达生成。潜在局限性包括数据集的规模和多样性，未来工作可进一步优化方法效率或探索更多应用场景，如扩展到其他视觉任务中。",
      "tags": [
        "Generalized Referring Expression",
        "Multi-target Recognition",
        "Relationship Modeling",
        "Dataset gRefCOCO",
        "ReLA Method"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:24.855698Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05239",
    "title": "Plenoptic Video Generation",
    "authors": [
      "Xiao Fu",
      "Shitao Tang",
      "Min Shi",
      "Xian Liu",
      "Jinwei Gu",
      "Ming-Yu Liu",
      "Dahua Lin",
      "Chen-Hsuan Lin"
    ],
    "abstract": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05239.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05239",
    "published": "2026-01-08T18:58:32Z",
    "updated": "2026-01-08T18:58:32Z",
    "comment": "Project Page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "light_analysis": {
      "overview": "PlenopticDreamer框架通过同步生成幻觉来保持时空一致性，实现高效的多视图视频重渲染。",
      "motivation": "现有相机控制生成视频重渲染方法（如ReCamMaster）在单视图设置中取得进展，但在多视图场景中难以维持时空一致性。生成模型的固有随机性导致幻觉区域的时空一致性挑战，这一问题在多视图应用中尤为重要，因为它影响视觉效果的真实性和可靠性，现有方法在处理多视图同步时表现不足，亟需改进。",
      "method": "本论文提出PlenopticDreamer框架，核心是训练一个多输入单输出的视频条件模型，采用自回归方式。关键创新点包括相机指导的视频检索策略，自适应选择先前生成的显著视频作为条件输入以保持一致性；结合渐进上下文缩放提高收敛效率，自条件机制增强对错误积累导致的长范围视觉退化的鲁棒性，以及长视频条件机制支持扩展视频生成。这些技术共同优化了模型性能和稳定性。",
      "result": "在Basic和Agibot基准测试中，PlenopticDreamer实现了state-of-the-art视频重渲染性能，提供优越的视图同步、高保真视觉、准确相机控制和多样视图转换（如第三人称到第三人称，以及机器人操作中的头视图到抓取视图）。实验数据显示其在多视图场景中显著优于现有基线方法，验证了框架的有效性和鲁棒性。",
      "conclusion": "该研究贡献了PlenopticDreamer框架，有效解决多视图视频重渲染中的时空一致性问题，具有高学术价值，推动了生成视频技术的发展。实际应用潜力体现在机器人操作等场景，未来工作可能包括进一步优化模型效率、扩展到更复杂的多视图环境，并探索其他生成任务的适应性。",
      "tags": [
        "Generative Video Re-rendering",
        "Autoregressive Modeling",
        "Camera-Guided Retrieval",
        "Spatio-Temporal Consistency",
        "Multi-View Generation"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:22.116851Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05237",
    "title": "ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos",
    "authors": [
      "Rustin Soraki",
      "Homanga Bharadhwaj",
      "Ali Farhadi",
      "Roozbeh Mottaghi"
    ],
    "abstract": "Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05237.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05237",
    "published": "2026-01-08T18:58:08Z",
    "updated": "2026-01-08T18:58:08Z",
    "comment": "Preprint. Project Website: objectforesight.github.io",
    "light_analysis": {
      "overview": "ObjectForesight 引入了一个以3D物体为中心的动力学模型，直接从人类视频预测未来3D物体轨迹，解决了从被动视觉观察预测物体运动的挑战。",
      "motivation": "本研究旨在赋予计算系统类似人类预测物体未来运动的能力，这对于机器人学、增强现实等应用至关重要。现有方法如世界或动力学模型通常在像素或潜在空间操作，缺乏对三维几何结构的显式考虑，导致预测在几何上不连贯或忽略物体可供性，因此需要开发一种直接从视觉观察学习物理基础物体动力学的新方法，以提升预测的准确性和泛化性。",
      "method": "ObjectForesight 模型从短的自中心视频序列预测刚性物体的未来6-DoF位姿和轨迹，核心创新在于使用3D显式物体级表示，实现几何基础和时间一致的预测。模型通过结合分割、网格重建和3D位姿估计技术，构建了一个包含200多万个短剪辑的数据集，具有伪地面真实3D物体轨迹，以大规模训练物体中心动力学模型，从而捕捉物体的运动规律和交互特性。",
      "result": "实验结果显示，ObjectForesight 在预测准确性、几何一致性和泛化到未见过的物体和场景方面取得了显著增益。与基线方法相比，该模型表现出更好的性能，例如在几何一致性上提升，但摘要未提供具体指标如准确率数值；这些结果证明了模型的优越性和可扩展性，为直接从观察学习物理基础动力学提供了实证支持。",
      "conclusion": "本研究的主要贡献是提出并验证了一个可扩展框架，用于从被动视觉观察中学习物理基础的物体中心动力学模型，这为机器视觉系统的预测能力提供了新途径。学术价值在于推动3D视频理解和物体动力学建模的发展，实际应用包括机器人交互、自动驾驶等；未来工作可探索模型的进一步扩展，如处理非刚性物体或更复杂场景，但摘要未明确说明具体局限性。",
      "tags": [
        "3D Object Dynamics",
        "6-DoF Pose Estimation",
        "Video-based Prediction",
        "Geometric Consistency",
        "Object-centric Modeling"
      ]
    },
    "analyzed_at": "2026-01-09T05:11:26.468929Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05245",
    "title": "Optimal Lower Bounds for Online Multicalibration",
    "authors": [
      "Natalie Collina",
      "Jiuyao Lu",
      "Georgy Noarov",
      "Aaron Roth"
    ],
    "abstract": "We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.   In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.   We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05245.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05245",
    "published": "2026-01-08T18:59:32Z",
    "updated": "2026-01-08T18:59:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究证明了在线多校准的最优下界，并建立了与边缘校准的信息论分离。",
      "motivation": "多校准是机器学习中确保预测在不同子组中公平准确的关键问题，比边缘校准更严格。现有研究主要提供了上界，但缺乏下界来深入理解多校准的复杂度及其与边缘校准的差异。本研究的动机是通过证明紧下界，填补理论空白，阐明多校准问题的本质困难，为算法性能提供理论极限，并评估现有方法的局限性。",
      "method": "论文采用信息论方法，通过构造特定组函数来证明下界。在一般设置中，设计三个不相交的二元组函数，这些函数可以依赖于上下文和学习器的预测；对于更困难情况（组函数仅依赖于上下文），利用正交函数系统构造大小为Θ(T)的组族。关键创新在于组函数的精细构造，以最大化多校准误差的下界，展示多校准的复杂性。",
      "result": "研究结果显示，在一般设置中，在线多校准的期望误差下界为Ω(T^{2/3})，与Noarov等人的上界匹配至对数因子，并超过边缘校准的O(T^{2/3-ε})上界，表明多校准问题更困难。在组函数不依赖预测的情况下，下界为Ω̃(T^{2/3})，同样与现有上界一致，验证了下界的紧性，从而分离了多校准与边缘校准。",
      "conclusion": "论文的主要贡献是证明了在线多校准的最优下界，并建立了与边缘校准的信息论分离，深化了对校准理论的理解，为设计高效多校准算法提供理论基础。学术上，它推动了机器学习公平性和可靠性研究；实际中，有助于改进预测系统。摘要未明确说明局限性，未来工作可能探索更广泛的应用场景或扩展下界证明方法。",
      "tags": [
        "Online Multicalibration",
        "Lower Bounds",
        "Information Theory",
        "Group Functions",
        "Orthogonal Function Systems"
      ]
    },
    "analyzed_at": "2026-01-09T05:13:06.639069Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]