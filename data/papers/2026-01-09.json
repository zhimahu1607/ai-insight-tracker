[
  {
    "id": "2601.05242",
    "title": "GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization",
    "authors": [
      "Shih-Yang Liu",
      "Xin Dong",
      "Ximing Lu",
      "Shizhe Diao",
      "Peter Belcak",
      "Mingjie Liu",
      "Min-Hung Chen",
      "Hongxu Yin",
      "Yu-Chiang Frank Wang",
      "Kwang-Ting Cheng",
      "Yejin Choi",
      "Jan Kautz",
      "Pavlo Molchanov"
    ],
    "abstract": "As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05242.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05242",
    "published": "2026-01-08T18:59:24Z",
    "updated": "2026-01-08T18:59:24Z",
    "comment": "NVIDIA-Tech Report",
    "light_analysis": {
      "overview": "提出了GDPO方法，一种通过解耦奖励归一化来改善多奖励强化学习训练效果和稳定性的策略优化技术。",
      "motivation": "随着语言模型能力增强，用户期望模型不仅在准确性上对齐，还需在多种场景下符合多样化的人类偏好，促使强化学习管道整合多个奖励。然而，现有方法如Group Relative Policy Optimization（GRPO）在多奖励设置下未经验证就直接应用，导致奖励归一化后优势值相同化，降低训练信号分辨率，造成次优收敛甚至早期失败，突显了现有方法的不足和对更有效优化的需求。",
      "method": "论文提出Group reward-Decoupled Normalization Policy Optimization（GDPO），核心方法是通过解耦单个奖励的归一化过程，独立处理每个奖励以保留其相对差异。关键创新在于避免优势值相同化，提高多奖励优化的准确性，并显著提升训练稳定性。摘要未明确说明使用的数据集或模型架构，但方法适用于语言模型相关任务如工具调用和推理。",
      "result": "实验在工具调用、数学推理和编码推理三个任务中展开，评估正确性指标（如准确性、错误率）和约束遵守指标（如格式、长度）。GDPO在所有设置中始终优于基线方法GRPO，展现出更好的性能和更高的训练稳定性，证明了其有效性和泛化性。摘要未提供具体百分比数据，但强调了持续的优越表现。",
      "conclusion": "GDPO作为一种新策略优化方法，解决了多奖励强化学习中的奖励归一化问题，提升了优化效果和训练稳定性。其学术价值在于为多奖励RL提供创新解决方案，实际应用价值在于帮助语言模型更好地对齐人类多样化偏好，提高模型行为的准确性和约束遵守。未来工作可探索GDPO在更广泛任务中的应用和潜在改进。",
      "tags": [
        "Reinforcement Learning",
        "Multi-reward RL",
        "Policy Optimization",
        "Reward Normalization",
        "Group Relative Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T04:54:57.398514Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05241",
    "title": "RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation",
    "authors": [
      "Boyang Wang",
      "Haoran Zhang",
      "Shujie Zhang",
      "Jinkun Hao",
      "Mingda Jia",
      "Qi Lv",
      "Yucheng Mao",
      "Zhaoyang Lyu",
      "Jia Zeng",
      "Xudong Xu",
      "Jiangmiao Pang"
    ],
    "abstract": "The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05241.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05241",
    "published": "2026-01-08T18:59:22Z",
    "updated": "2026-01-08T18:59:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出RoboVIP方法，通过视觉身份提示生成多视图视频，增强机器人操纵数据，提升策略模型的训练效果。",
      "motivation": "机器人操纵数据的多样性、数量和质量对训练有效策略至关重要，但由于硬件和物理设置限制，收集大规模真实世界操纵数据在不同环境中难以扩展。现有方法使用文本提示条件化的图像扩散模型来增强数据，但往往忽视最先进的策略模型所需的多视图和时间一致性观察需求，且仅文本提示不足以可靠指定场景设置。因此，需要一种更有效的视觉指导方法来克服这些不足，以支持更逼真的机器人学习。",
      "method": "该方法引入视觉身份提示，提供示例图像作为条件输入，指导扩散模型生成所需场景设置，确保多视图和时间一致性。核心创新点在于使用视觉指导而非仅文本提示，以更可靠地指定场景。此外，构建了一个可扩展的管道，从大型机器人数据集中策划视觉身份池，用于生成增强的操纵数据。这些数据随后用于训练下游的视觉语言动作和视觉运动策略模型，增强模型的泛化能力。",
      "result": "使用增强的操纵数据训练下游模型，在模拟和真实机器人设置中均实现了持续的性能提升。摘要未明确说明具体性能指标（如准确率或效率改进的具体数据），但表明该方法相比基于文本提示的现有方法，通过视觉身份提示提供了更可靠的场景指定，从而在多种测试环境中取得了更优的效果。实验结果显示该方法能有效改善策略模型的训练稳定性和泛化性能。",
      "conclusion": "该研究的主要贡献是提出了视觉身份提示方法，用于多视图视频生成，以增强机器人操纵数据。学术价值在于改进了扩散模型在机器人领域的应用，提供了更精确的视觉条件化；实际应用价值在于能更高效地训练机器人策略，降低数据收集成本。摘要未明确说明局限性或未来工作方向，但该方法为机器人学习中的数据增强提供了新途径，可能推动相关技术的发展。",
      "tags": [
        "Visual Identity Prompting",
        "Multi-View Video Generation",
        "Diffusion Models",
        "Robot Manipulation",
        "Vision-Language-Action Models"
      ]
    },
    "analyzed_at": "2026-01-09T04:53:29.506768Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05240",
    "title": "Robust Reasoning as a Symmetry-Protected Topological Phase",
    "authors": [
      "Ilmo Sung"
    ],
    "abstract": "Large language models suffer from \"hallucinations\"-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a \"Metric Phase,\" where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic \"mass gap,\" maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \\times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\\times$ beyond training ($L=50 \\to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "hep-th"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05240.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05240",
    "published": "2026-01-08T18:58:34Z",
    "updated": "2026-01-08T18:58:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出鲁棒推理可作为对称性保护的拓扑相，并通过Holonomic Network增强逻辑推理的稳定性。",
      "motivation": "大语言模型常因语义噪声产生幻觉，导致逻辑不一致性。作者指出现有架构如Transformer和RNN在‘度量相’中操作，因果顺序易受对称性自发破坏影响，限制了推理的鲁棒性。因此，需要发展基于拓扑不变量的方法来稳定推理过程，避免现有方法的脆弱性。",
      "method": "研究方法将鲁棒推理识别为对称性保护的拓扑相，其中逻辑操作与非阿贝尔任意子编织形式同构。关键创新是提出Holonomic Network，用拓扑不变量替代脆弱的几何插值，以增强推理稳健性。在实验中，使用S10数据集（3.6×10^6状态）代表符号操作任务，验证方法的有效性，并强调非阿贝尔规范对称性的作用。",
      "result": "实验结果显示拓扑相变：与Transformer和RNN的gapless衰减相比，Holonomic Network显示出宏观‘质量隙’，在临界噪声阈值下保持不变的保真度。在变量绑定任务中，拓扑模型在训练范围外100倍处（L=50到5000）仍保持完美保真度，而Transformer失去逻辑一致性。消融研究表明保护完全源自非阿贝尔规范对称性。",
      "conclusion": "结论表明该方法为逻辑推理提供了新通用类证据，将因果稳定性与语义流形的拓扑结构相连接。这具有重要学术价值，可能推动AI推理模型的发展，提高大型语言模型的鲁棒性。未来工作可探索该方法的扩展性及在更复杂任务中的应用。",
      "tags": [
        "Symmetry-Protected Topological Phase",
        "Non-Abelian Anyon Braiding",
        "Holonomic Network",
        "Transformer",
        "Logical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T04:53:46.187989Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05251",
    "title": "Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video",
    "authors": [
      "Zeren Jiang",
      "Chuanxia Zheng",
      "Iro Laina",
      "Diane Larlus",
      "Andrea Vedaldi"
    ],
    "abstract": "We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object's complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object's overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05251.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05251",
    "published": "2026-01-08T18:59:56Z",
    "updated": "2026-01-08T18:59:56Z",
    "comment": "15 pages, 8 figures, project page: https://mesh-4d.github.io/",
    "light_analysis": {
      "overview": "Mesh4D通过紧凑潜在空间实现了从单目视频一次性重建动态物体的4D网格，提升了重建效率和准确性。",
      "motivation": "从单目视频中重建动态物体的3D形状和运动是计算机视觉中的关键挑战，应用于动画和虚拟现实等领域。现有方法通常需要逐帧处理或依赖额外信息如骨骼结构，导致效率低下和泛化能力受限。Mesh4D旨在解决这一问题，提供一种无需推理时骨骼信息的高效方案，以改进实时重建和变形建模。",
      "method": "Mesh4D采用前馈模型，通过自动编码器学习紧凑潜在空间，编码整个动画序列。训练时，利用训练对象的骨骼结构作为先验指导变形学习，但推理时无需骨骼信息。编码器使用时空气体注意力机制，增强整体变形表示的稳定性。基于该表示，训练一个潜在扩散模型，该模型以输入视频和首帧重建的网格为条件，一次性预测完整动画。",
      "result": "论文在重建和新视角合成基准上评估Mesh4D，结果表明其优于先前方法，能够更准确地恢复3D形状和变形。尽管摘要未明确说明具体性能指标如准确率提升，但强调了在相关任务上的优越性，展示了在高效处理动态物体方面的潜力。",
      "conclusion": "Mesh4D的主要贡献是引入了紧凑潜在空间和潜在扩散模型，实现了高效的4D网格重建。学术上，该方法在潜在编码和条件生成方面具有创新价值；实际上，可用于动画制作和虚拟现实等应用，提高了动态物体建模的效率。未来工作可能包括扩展到更复杂场景或减少对训练数据的依赖。",
      "tags": [
        "Monocular Video",
        "4D Mesh Reconstruction",
        "Autoencoder",
        "Spatio-Temporal Attention",
        "Latent Diffusion Model"
      ]
    },
    "analyzed_at": "2026-01-09T04:53:25.708559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05249",
    "title": "RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes",
    "authors": [
      "Yuan-Kang Lee",
      "Kuan-Lin Chen",
      "Chia-Che Chang",
      "Yu-Lun Liu"
    ],
    "abstract": "Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05249.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05249",
    "published": "2026-01-08T18:59:55Z",
    "updated": "2026-01-08T18:59:55Z",
    "comment": "Project page: https://ntuneillee.github.io/research/rl-awb/",
    "light_analysis": {
      "overview": "RL-AWB框架首次将深度强化学习与统计方法结合，用于解决夜间白平衡校正问题，并引入了首个多传感器夜间数据集以促进跨传感器评估。",
      "motivation": "夜间色彩恒常性在计算摄影中是一个关键挑战，主要由于低光条件下的噪声和复杂照明环境导致传统方法难以准确校正白平衡。这一问题影响了夜间图像的真实色彩再现和视觉效果，现有方法在处理夜间场景时往往泛化能力不足，无法适应多变的照明条件，因此亟需更有效的解决方案来提升图像质量和实用性。",
      "method": "RL-AWB框架的核心是结合统计方法和深度强化学习。首先，设计了一个专门针对夜间场景的统计算法，集成了显著性灰色像素检测和新的照明估计技术来增强基础处理。然后，基于此算法，开发了首个用于色彩恒常性的深度强化学习方法，模仿专业AWB调谐专家的决策过程，动态优化每张图像的参数，从而实现个性化校正。此外，论文引入了首个多传感器夜间数据集以支持跨传感器评估。",
      "result": "实验结果表明，RL-AWB在低光照和良好光照图像上展现出卓越的泛化能力，能够有效适应不同光照条件。尽管摘要未明确说明具体的性能指标（如准确率提升或效率改进），但与基线方法相比，该方法在跨传感器评估中表现出更强的适应性，验证了其在实际应用中的潜力。",
      "conclusion": "RL-AWB的主要贡献在于创新性地整合统计算法和深度强化学习，为解决夜间白平衡问题提供了新思路，提高了方法的泛化性和准确性。引入的多传感器数据集为未来研究提供了宝贵资源，具有学术价值和实际应用前景，潜在局限性可能在于处理极端照明条件的优化，未来工作可扩展到更多复杂场景和算法效率提升。",
      "tags": [
        "Deep Reinforcement Learning",
        "Statistical Algorithms",
        "Color Constancy",
        "Auto White Balance",
        "Low-Light Imaging"
      ]
    },
    "analyzed_at": "2026-01-09T04:55:21.588532Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05250",
    "title": "QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer",
    "authors": [
      "Daniele Lizzio Bosco",
      "Shuteng Wang",
      "Giuseppe Serra",
      "Vladislav Golyanik"
    ],
    "abstract": "Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05250.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05250",
    "published": "2026-01-08T18:59:55Z",
    "updated": "2026-01-08T18:59:55Z",
    "comment": "30 pages, 15 figures, 11 tables; project page: https://4dqv.mpi-inf.mpg.de/QNeRF/",
    "light_analysis": {
      "overview": "本文提出了QNeRF，首个用于新颖视角合成的混合量子-经典模型，通过量子电路编码信息实现模型紧凑化，提高了效率和性能。",
      "motivation": "该研究旨在解决Neural Radiance Fields (NeRFs) 在3D场景渲染中模型规模大、训练密集的问题。NeRFs虽在2D图像到3D表示学习中有效，但计算成本高。Quantum Visual Fields (QVFs) 在模型紧凑性和收敛速度上有优势，但未应用于此任务。因此，QNeRF探索量子计算在计算机视觉中的应用，以提升效率，应对现有方法的高资源需求，促进量子机器学习在中等任务中的发展。",
      "method": "QNeRF采用参数化量子电路编码空间和视角相关信息，利用量子叠加和纠缠实现信息表示。提出两个变体：Full QNeRF最大化量子振幅以增强表示能力，而Dual-Branch QNeRF引入任务引导的归纳偏置，通过分支处理空间和视角量子状态准备，大幅减少操作复杂度并确保可扩展性和硬件兼容性。该方法在模拟门基于量子计算机上实现，基于NeRF框架，但未明确说明具体数据集和模型架构细节，仅聚焦于量子电路集成。",
      "result": "在中等分辨率图像上训练时，QNeRF匹配或优于经典NeRF基线模型，同时使用的参数数量少于一半，具体参数节省未量化但明确提及性能提升。实验表明模型在保持高准确率的同时提高了效率，与基线对比显示量子方法在资源利用上的优势。这些结果验证了量子电路在减少模型复杂度方面的有效性，支持了量子机器学习作为连续信号表示的竞争性替代。",
      "conclusion": "QNeRF的主要贡献在于开创了量子-经典混合模型在新颖视角合成中的应用，通过量子编码实现了更紧凑的表示。其学术价值在于推动了量子计算与计算机视觉的交叉研究，证明了量子机器学习在中等任务中的潜力。实际应用价值包括提高3D渲染效率，未来工作可扩展至更高分辨率任务或硬件部署，尽管存在局限性如未明确训练数据，但展示了可扩展性前景。",
      "tags": [
        "Neural Radiance Fields",
        "Quantum Machine Learning",
        "Parameterised Quantum Circuits",
        "Quantum Superposition",
        "Hybrid Quantum-Classical Model"
      ]
    },
    "analyzed_at": "2026-01-09T04:53:10.787331Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05246",
    "title": "Pixel-Perfect Visual Geometry Estimation",
    "authors": [
      "Gangwei Xu",
      "Haotong Lin",
      "Hongcheng Luo",
      "Haiyang Sun",
      "Bing Wang",
      "Guang Chen",
      "Sida Peng",
      "Hangjun Ye",
      "Xin Yang"
    ],
    "abstract": "Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05246.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05246",
    "published": "2026-01-08T18:59:49Z",
    "updated": "2026-01-08T18:59:49Z",
    "comment": "Code: https://github.com/gangweix/pixel-perfect-depth",
    "light_analysis": {
      "overview": "提出基于像素空间扩散变换器的视觉几何模型，通过语义提示和级联设计实现无飞像素的高质量深度估计。",
      "motivation": "从图像中恢复清洁准确的几何结构对机器人和增强现实应用至关重要，但现有几何基础模型在估计过程中常出现飞像素和细节损失问题，这限制了其在实际场景中的实用性和精度。本研究旨在解决这些挑战，通过改进生成建模方法，提升几何估计的鲁棒性和准确性，以满足高精度视觉任务的需求。摘要指出，当前模型在处理复杂图像时效果不佳，因此开发更高效和精确的模型是必要的。",
      "method": "论文提出了Pixel-Perfect Depth (PPD)模型，基于像素空间扩散变换器（DiT），并通过两个关键设计优化：Semantics-Prompted DiT融合视觉基础模型的语义表示来提示扩散过程，以保留全局语义并增强细节；Cascade DiT架构逐步增加图像令牌，提高计算效率和准确性。扩展到视频时，引入Semantics-Consistent DiT从多视图几何基础模型提取时间一致性语义，并进行参考引导令牌传播，以维持时间连贯性且最小化计算和内存开销。",
      "result": "模型在生成单目和视频深度估计任务中表现出最佳性能，与其他模型相比，产生的点云更清洁且细节更丰富。摘要未明确说明具体性能指标，但强调在所有生成模型中达到最优，显著减少了飞像素现象，验证了其在几何估计任务中的优越能力。与基线方法对比，点云质量有显著提升，尽管具体数据未在摘要中提供。",
      "conclusion": "本研究的主要贡献是开发了pixel-perfect视觉几何模型，通过像素空间生成建模和语义融合技术，实现了高质量、无飞像素的深度估计，为机器人和AR等应用提供了更可靠的几何信息。这具有重要的学术价值，推动了生成建模在视觉几何领域的应用，并展示了实际应用潜力。潜在局限性可能包括计算复杂度，未来工作可探索更高效的优化方法或扩展到其他视觉任务。",
      "tags": [
        "Diffusion Transformer",
        "Semantic Prompting",
        "Cascade Architecture",
        "Monocular Depth Estimation",
        "Video Depth Estimation"
      ]
    },
    "analyzed_at": "2026-01-09T04:55:43.387696Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05244",
    "title": "GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation",
    "authors": [
      "Henghui Ding",
      "Chang Liu",
      "Shuting He",
      "Xudong Jiang",
      "Yu-Gang Jiang"
    ],
    "abstract": "Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05244.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05244",
    "published": "2026-01-08T18:59:30Z",
    "updated": "2026-01-08T18:59:30Z",
    "comment": "IJCV, Project Page: https://henghuiding.com/GREx/",
    "light_analysis": {
      "overview": "论文提出了广义化的引用表达式基准GREx、新数据集gRefCOCO和基线方法ReLA，以处理多目标和无目标表达式，扩展了传统REx任务。",
      "motivation": "REx任务如分割和理解表达式，现有方法仅支持单目标表达式，即一个表达式指代一个对象，忽略了多目标和无目标情况，限制了实际应用。现实中用户描述可能涉及多个对象或不指定任何对象，这导致模型泛化能力不足。因此，扩展REx以支持任意数量对象的表达式至关重要，以提升视觉语言理解任务在真实场景中的实用性和鲁棒性。",
      "method": "论文引入GREx基准，包括广义化的分割（GRES）、理解（GREC）和生成（GREG）任务，并构建了首个大规模数据集gRefCOCO，涵盖多目标、无目标和单目标表达式及其对应图像。针对GRES/GREC中的复杂关系建模挑战，提出基线方法ReLA，它自适应地将图像划分为包含子实例线索的区域，并显式建模区域间和区域与语言间的依赖关系，以提升多目标表达式处理能力。",
      "result": "ReLA方法在GRES和GREC任务上达到了最先进的性能，有效处理多目标表达式。摘要未明确说明具体性能指标如准确率提升，但实验结果表明与现有REx方法相比，GREx基准展示了更广泛的应用范围和改进的模型适应性，为后续研究提供了性能基准。",
      "conclusion": "本研究通过GREx基准和gRefCOCO数据集解决了REx中多目标和无目标表达式的处理问题，推动了视觉语言任务的泛化。其学术价值在于提供了一个新框架，实际应用价值在于增强模型在真实场景中的鲁棒性。未来工作可进一步优化关系建模算法或扩展数据集到更多视觉语言任务。",
      "tags": [
        "Generalized Referring Expression",
        "Multi-target Expression",
        "Region-based Dependency Modeling",
        "ReLA Method",
        "gRefCOCO Dataset"
      ]
    },
    "analyzed_at": "2026-01-09T04:55:52.275864Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05239",
    "title": "Plenoptic Video Generation",
    "authors": [
      "Xiao Fu",
      "Shitao Tang",
      "Min Shi",
      "Xian Liu",
      "Jinwei Gu",
      "Ming-Yu Liu",
      "Dahua Lin",
      "Chen-Hsuan Lin"
    ],
    "abstract": "Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05239.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05239",
    "published": "2026-01-08T18:58:32Z",
    "updated": "2026-01-08T18:58:32Z",
    "comment": "Project Page: https://research.nvidia.com/labs/dir/plenopticdreamer/",
    "light_analysis": {
      "overview": "提出了PlenopticDreamer框架，通过同步生成幻觉解决多视图视频重渲染中的时空一致性问题。",
      "motivation": "研究动机源于现有摄像机控制的生成视频重渲染方法如ReCamMaster在单视图场景表现良好，但在多视图场景中难以保持一致性，特别是幻觉区域的时空连贯性由于生成模型的随机性而具有挑战性，这限制了应用如机器人操作中的视图变换的准确性和可靠性。",
      "method": "研究方法包括训练一个多输入单输出的视频条件模型，以自回归方式进行，并引入摄像机引导的视频检索策略自适应选择先前生成的显著视频作为条件输入。关键创新点有渐进上下文扩展以改善收敛，自条件机制增强对错误累积导致的视觉退化的鲁棒性，以及长视频条件机制支持扩展视频生成，从而同步幻觉维护时空记忆。",
      "result": "主要实验结果在Basic和Agibot基准上展示，PlenopticDreamer实现了最先进的视频重渲染性能，在视图同步、高保真视觉、准确摄像机控制和多样视图变换（如第三人称到第三人称及头部视图到抓取器视图）方面均优于现有基线方法，具体性能指标摘要未明确说明。",
      "conclusion": "论文主要贡献是提出PlenopticDreamer框架，有效解决多视图视频生成中的时空一致性问题，具有学术价值推动生成视频技术发展，并在机器人操作等实际应用中有潜力；未来工作方向可能涉及进一步优化模型收敛和扩展视频生成能力。",
      "tags": [
        "Generative Video Rendering",
        "Multi-view Consistency",
        "Autoregressive Models",
        "Spatio-temporal Coherence",
        "Camera-guided Retrieval"
      ]
    },
    "analyzed_at": "2026-01-09T04:53:26.148763Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05237",
    "title": "ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos",
    "authors": [
      "Rustin Soraki",
      "Homanga Bharadhwaj",
      "Ali Farhadi",
      "Roozbeh Mottaghi"
    ],
    "abstract": "Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05237.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05237",
    "published": "2026-01-08T18:58:08Z",
    "updated": "2026-01-08T18:58:08Z",
    "comment": "Preprint. Project Website: objectforesight.github.io",
    "light_analysis": {
      "overview": "ObjectForesight 提出了一种从人类视频中预测未来 3D 物体轨迹的模型，通过显式 3D 物体表示实现几何基础和时序一致的预测。",
      "motivation": "本研究的动机是赋予计算系统预测物体未来运动的能力，类似于人类直观理解物体互动（如杯子被举起、刀子切片）。这种能力对机器人、自动驾驶和场景理解至关重要。现有动态模型通常在像素或潜在空间中操作，缺乏对物体几何的显式建模，导致预测可能缺乏物理合理性和几何一致性，限制了实际应用效果。因此，开发基于 3D 物体表示的模型，以解决这些不足并提升预测质量。",
      "method": "ObjectForesight 是一个 3D 物体中心动态模型，用于从短时自我中心视频序列中预测刚性物体的未来 6-DoF 位姿和轨迹。核心创新在于采用显式 3D 物体表示，不同于传统像素或潜在空间模型，这使预测更具几何基础和时序连贯性。为大规模训练，模型利用分割、网格重建和 3D 位姿估计的最新进展，构建了一个包含超过 2 百万个短剪辑的数据集，其中带有伪地面真值的 3D 物体轨迹，以支持模型学习物体可操作性和运动模式。",
      "result": "通过大量实验，ObjectForesight 在准确性、几何一致性和对未见物体和场景的泛化性方面取得了显著增益。与基线方法相比，该模型能更有效地捕捉物体轨迹和互动模式，展现了优越的性能。摘要未提供具体数值数据，但强调了模型在多个指标上的提升，表明其在物理基础的预测任务中具有更好的适用性和鲁棒性。",
      "conclusion": "本研究的主要贡献是提出了 ObjectForesight，一个可扩展的框架，用于从观察直接学习物理基础的、以物体为中心的动态模型。其学术价值在于推动了物体级视觉预测和 3D 表示学习的发展，实际应用潜力包括机器人交互和增强现实等领域。摘要未明确说明局限性，未来工作可能涉及进一步优化模型泛化能力和扩展到非刚性物体预测。",
      "tags": [
        "3D Object Trajectories",
        "6-DoF Pose Estimation",
        "Object-Centric Dynamics",
        "Video Prediction",
        "Geometric Consistency"
      ]
    },
    "analyzed_at": "2026-01-09T04:54:42.986959Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05245",
    "title": "Optimal Lower Bounds for Online Multicalibration",
    "authors": [
      "Natalie Collina",
      "Jiuyao Lu",
      "Georgy Noarov",
      "Aaron Roth"
    ],
    "abstract": "We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.   In the general setting where group functions can depend on both context and the learner's predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.   We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner's predictions. In this case, we establish an $\\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05245.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05245",
    "published": "2026-01-08T18:59:32Z",
    "updated": "2026-01-08T18:59:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文证明了在线多校准的紧致下界，并与边缘校准在信息论上分离。",
      "motivation": "多校准是机器学习中的关键问题，旨在确保预测模型在不同子组上的校准性，以提升公平性和可靠性。现有研究已提出多校准的上界，但与边缘校准的复杂度关系不明确，需要证明紧致下界来理解其固有难度。本文旨在解决这一问题，通过证明下界来分离多校准与边缘校准，突出多校准在在线学习环境中的挑战性，推动算法设计的理论进步。",
      "method": "论文采用信息论和构造性证明方法来推导下界。在一般设置中，组函数可依赖于上下文和预测，通过构建三个不相交的二元组，证明了期望多校准误差的下界为Ω(T^{2/3})。对于更困难的设置，其中组函数仅依赖于上下文，利用正交函数系统构造了Θ(T)-大小的组族，同样得出了Ω(T^{2/3})的下界。这些构造与现有上界匹配，表明了方法的紧致性，摘要未明确说明具体证明细节，但基于信息论和组合论证。",
      "result": "主要实验结果显示，在组函数可依赖于上下文和预测的设置下，在线多校准的下界为Ω(T^{2/3})，这与Noarov等人的上界匹配（除对数因子），并超过了边缘校准的O(T^{2/3-ε})上界。在组函数仅依赖于上下文的设置下，下界同样为Ω(T^{2/3})，匹配上界。这些结果首次确立了多校准与边缘校准的信息论分离，表明多校准具有更高的复杂度，提供了理论上的性能界限。",
      "conclusion": "本文的核心贡献是证明了在线多校准的紧致下界，揭示了其与边缘校准在信息论上的分离，表明多校准在在线学习中更具挑战性。这加深了校准理论的理解，具有重要的学术价值，为未来算法设计提供了理论基础。潜在局限性包括摘要未明确说明对其他设置的推广，未来工作可能涉及扩展至更复杂的组函数或实际应用场景。",
      "tags": [
        "Online Multicalibration",
        "Lower Bounds",
        "Information Theory",
        "Orthogonal Functions",
        "Calibration Theory"
      ]
    },
    "analyzed_at": "2026-01-09T04:54:42.184599Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]