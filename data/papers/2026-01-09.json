[
  {
    "id": "2601.05230",
    "title": "Learning Latent Action World Models In The Wild",
    "authors": [
      "Quentin Garrido",
      "Tushar Nagarajan",
      "Basile Terver",
      "Nicolas Ballas",
      "Yann LeCun",
      "Michael Rabbat"
    ],
    "abstract": "Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05230.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05230",
    "published": "2026-01-08T18:55:39Z",
    "updated": "2026-01-08T18:55:39Z",
    "comment": "37 pages, 25 figures",
    "light_analysis": {
      "overview": "本论文提出一种从野外视频中学习连续受约束潜在行动世界模型的方法，以解决行动标签获取困难，实现与现实世界规划任务性能相当。",
      "motivation": "现实世界中的智能代理需要预测行动后果，世界模型具备这种能力但通常依赖行动标签，而这些标签在大规模获取时复杂且昂贵。现有研究多局限于简单模拟或游戏，未扩展到多样性丰富的野外视频，导致对真实世界挑战（如环境噪声、缺乏共同体现）的处理不足。因此，本研究旨在直接从视频中学习潜在行动空间，以降低对标签的依赖并应对野外视频的独特挑战，提升模型的实用性和可扩展性。",
      "method": "论文提出学习潜在行动世界模型的方法，核心是使用连续但受约束的潜在行动来捕捉野外视频中行动的复杂性，替代常见的向量量化方法。通过讨论行动应遵循的性质（如可转移性）和相关架构选择，处理视频多样性、噪声等挑战；具体地，训练控制器将已知行动映射到潜在行动，使潜在行动成为通用接口用于规划任务。摘要未明确说明具体数据集或模型架构细节，但强调了技术路线侧重于视频理解和模型设计。",
      "result": "实验表明，连续受约束的潜在行动能有效捕捉野外视频的复杂性，优于向量量化方法；例如，代理引起的变化（如人类进入房间）可在视频间转移，证明模型学习到针对野外视频的特定行动。在缺乏共同体现时，潜在行动主要空间定位在相机相对位置。通过控制器映射，使用潜在行动作为接口的规划任务性能与行动条件基线类似，摘要未提供具体数据如准确率，但强调了性能相当性和模型的有效性。",
      "conclusion": "本研究的主要贡献是扩展了潜在行动模型到野外视频中，通过连续潜在行动和控制器设计，为现实世界代理规划提供通用接口。学术上，它推动了视频理解和世界模型技术的发展，实际应用价值在于降低对行动标签的依赖，提升模型在复杂环境中的适应性。局限性包括对缺乏共同体现的处理有限，未来工作可优化架构以增强泛化能力，进一步推动模型向真实世界部署。",
      "tags": [
        "Latent Action Models",
        "World Models",
        "Continuous Latent Representation",
        "Video Understanding",
        "Planning Tasks"
      ]
    },
    "analyzed_at": "2026-01-09T02:32:21.537759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05215",
    "title": "MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents",
    "authors": [
      "Tamil Sudaravan Mohan Doss",
      "Michael Xu",
      "Sudha Rao",
      "Andrew D. Wilson",
      "Balasaravanan Thoravi Kumaravel"
    ],
    "abstract": "We present \\textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \\emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.   As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \\textbf{216} subtasks across \\textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05215.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05215",
    "published": "2026-01-08T18:39:52Z",
    "updated": "2026-01-08T18:39:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了MineNPC-Task，一个基于《Minecraft》开放世界的用户创作基准框架，用于测试记忆感知和混合主动性大型语言模型代理。",
      "motivation": "研究动机是解决现有记忆感知代理评估方法的不足，这些方法常依赖合成提示，无法真实反映开放世界环境中的复杂交互。在《Minecraft》等场景中，代理需要处理动态任务和记忆需求，但缺乏基于真实用户交互的评估基准。因此，开发一个从专家玩家合作游戏中获取任务的基准，以更准确地测试代理的记忆和混合主动性能力，提升评估的真实性和可靠性，弥补现有方法在模拟实际交互方面的缺陷。",
      "method": "研究方法包括开发MineNPC-Task基准套件，任务通过与专家玩家的合作玩游戏中获取，标准化为参数化模板，具有明确前提条件和依赖结构。框架使用机器可检查的验证器，遵循有界知识策略，禁止代理采取世界外捷径。评估过程捕获计划预览、目标澄清、记忆读写和修复尝试等事件。作为初始实例，使用GPT-4o模型对8个经验玩家和216个子任务进行测试，确保评估的透明性和可重复性。",
      "result": "实验结果基于8个经验玩家的216个子任务，观察到代理在代码执行、库存工具处理、引用和导航方面反复出现故障模式。通过混合主动性澄清和轻量记忆支持，代理能够部分恢复。参与者对交互质量和界面可用性评价积极，但指出需要更强的跨任务记忆持久性。摘要未明确说明与基线方法的对比，但通过实证评估识别了常见问题，为未来代理优化提供了数据支撑。",
      "conclusion": "结论是MineNPC-Task框架为记忆感知embodied代理的评估提供了透明、可重复的基准，贡献了基于真实用户交互的任务套件。该研究提升了评估的真实性，为未来代理开发和研究提供了实用工具。局限性包括记忆持久性需加强，未来工作可在此方向深化，并可能扩展基准到其他开放世界环境，以支持更广泛的代理测试。",
      "tags": [
        "Memory-Aware Agents",
        "Mixed-Initiative LLM",
        "Minecraft Benchmark",
        "Task Evaluation Harness",
        "Parametric Templates"
      ]
    },
    "analyzed_at": "2026-01-09T02:36:56.586229Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05214",
    "title": "Internal Representations as Indicators of Hallucinations in Agent Tool Selection",
    "authors": [
      "Kait Healy",
      "Bharathi Srinivasan",
      "Visakh Madathil",
      "Jing Wu"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit 'tool bypass' behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs' internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05214.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05214",
    "published": "2026-01-08T18:38:45Z",
    "updated": "2026-01-08T18:38:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种利用大型语言模型内部表示的实时工具调用幻觉检测框架，以提高代理系统的可靠性和效率。",
      "motivation": "大型语言模型在工具调用和工具使用中常产生幻觉，如选择错误工具、参数格式错误或绕过工具行为，这导致生产系统结果不一致并绕过安全审计控制，影响可靠性。现有幻觉检测方法需多次前向传递或外部验证，计算成本高且不适用于实时场景。因此，本研究旨在开发一种高效、实时的检测方法，以早期发现和解决这些错误，确保LLM代理的稳定部署和实际应用价值。",
      "method": "我们提出了一个计算高效的框架，在LLMs生成文本的同一前向传递中，通过分析其内部表示来实时检测工具调用幻觉。关键创新在于无需额外前向传递或外部验证，直接利用内部表示作为幻觉指标，实现了低计算开销的检测。该方法在多个领域的推理任务上进行评估，可能使用标准LLMs模型，但摘要未明确说明具体数据集或架构细节。",
      "result": "在多个领域的推理任务评估中，该方法表现出强检测性能，准确率达到86.4%，同时保持实时推理能力且计算开销最小。它特别擅长检测参数级幻觉和不适当工具选择，这表明优于需要多次前向传递或外部验证的现有方法，为可靠代理部署提供了关键支持，但摘要未提供具体基线对比数据。",
      "conclusion": "本研究的主要贡献是提出了一个基于内部表示的实时幻觉检测框架，显著提升了LLM代理的可靠性和应用效率。学术上，该方法为幻觉检测提供了新思路，减少对额外计算资源的依赖；应用上，有助于生产系统的高效部署和安全管理。未来工作可能包括扩展到更多复杂任务和模型，或解决潜在的泛化限制，但摘要未明确说明局限性。",
      "tags": [
        "Large Language Models",
        "Hallucination Detection",
        "Tool Calling",
        "Internal Representations",
        "Real-time Inference"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:23.972106Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.10105",
    "title": "Belief Is All You Need: Modeling Narrative Archetypes in Conspiratorial Discourse",
    "authors": [
      "Soorya Ram Shimgekar",
      "Abhay Goyal",
      "Roy Ka-Wei Lee",
      "Koustuv Saha",
      "Pi Zonooz",
      "Navin Kumar"
    ],
    "abstract": "Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.   Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.10105.pdf",
    "abs_url": "https://arxiv.org/abs/2512.10105",
    "published": "2025-12-10T21:51:16Z",
    "updated": "2026-01-08T18:34:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一个结合RoBERTa-large微调和Signed Belief Graph Neural Network的两阶段框架，有效识别和分析阴谋论话语中的叙事原型。",
      "motivation": "阴谋论话语在数字通信生态系统中日益普遍，但其结构和传播机制难以研究。现有方法常将阴谋论视为孤立回音室内容，而本研究指出其嵌入日常讨论中，如法律、金融等话题，这挑战了传统在线激进化假设。问题在于如何系统分析这种嵌入性话语，以改善内容审核和政治沟通研究，而现有方法可能缺乏对日常交互中意识形态特征的建模能力。摘要未明确说明具体现有方法的不足，但暗示需要更精细的计算框架来捕捉信念驱动的传播模式。",
      "method": "研究方法采用两阶段计算框架。第一阶段，微调RoBERTa-large模型，在新加坡Telegram群组的2000条专家标记消息上，进行消息是否为阴谋论的分类。第二阶段，构建有符号信念图，节点代表消息，边符号表示信念标签的对齐情况，权重基于文本相似性。核心创新是提出Signed Belief Graph Neural Network (SiBeGNN)，使用Sign Disentanglement Loss学习嵌入，以分离意识形态对齐和风格特征，然后通过分层聚类分析嵌入，识别叙事原型。",
      "result": "主要实验结果包括：第一阶段消息分类达到F1分数0.866。SiBeGNN的聚类质量指标cDBI为8.38，优于基线方法（范围13.60至67.27），专家评估一致性达88%。通过聚类分析，从553,648条消息中识别出七个叙事原型，如法律、医疗、金融等，并发现阴谋论消息广泛分布于常规讨论中。这些数据表明，该方法在聚类性能上显著超越基线，有效揭示了阴谋论话语的多样性结构。",
      "conclusion": "论文的主要贡献是挑战了在线激进化的常见假设，显示阴谋论话语融入普通社交互动。研究推进了信念驱动话语分析的计算方法，具有学术价值，如深化政治沟通理论，实际应用包括立场检测、内容审核政策优化。局限性方面，摘要未明确说明未来方向，但可能涉及扩展到其他语言或平台，以及改进模型泛化能力。这些发现为在线内容分析和干预提供了新视角。",
      "tags": [
        "Large Language Model",
        "Graph Neural Network",
        "Signed Graph",
        "Hierarchical Clustering",
        "Natural Language Processing"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:30.014961Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.01266",
    "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
    "authors": [
      "Rhitabrat Pokharel",
      "Hamid Reza Hassanzadeh",
      "Ameeta Agrawal"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.01266.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01266",
    "published": "2026-01-03T19:24:51Z",
    "updated": "2026-01-08T18:28:40Z",
    "comment": "Accepted at AIMedHealth @ AAAI 2026",
    "light_analysis": {
      "overview": "本文提出了一种结合检索器与符号推理的混合系统，以实现高效且可解释的医疗覆盖政策评估。",
      "motivation": "大型语言模型（LLMs）在解释复杂的法律和政策语言时表现出色，但其可靠性常受幻觉和不一致性的影响，尤其是在处理主观和微妙文档时。这些问题在医疗覆盖政策审查中尤为关键，因为人类专家需依赖准确信息进行决策。现有方法过度依赖LLMs可能导致高成本和不可靠结果，因此需要一种更高效、可解释的方案来支持审查过程，以应对政策文档的复杂性。",
      "method": "该研究提出了一种混合方法，结合了覆盖意识检索器（coverage-aware retriever）和符号规则推理（symbolic rule-based reasoning）。首先，检索器提取相关政策语言，然后将其组织为明确的事实和规则，并生成可审计的推理过程。该方法通过减少LLM推理次数来降低总体成本，创新点在于融合检索增强和符号逻辑以提高效率和可解释性。摘要未明确说明具体的数据集或模型架构，但基于现有信息推断，系统优化了检索与推理的协同工作。",
      "result": "实验结果表明，该方法在推理成本上实现了44%的减少，同时在F1分数上提高了4.5%。这些数据表明，与基线方法相比，该混合系统在效率和准确性方面均有显著提升，展示了成本节约和任务性能优化的双重优势。具体评估强调了其在医疗覆盖政策评估中的有效性，但摘要未明确说明基线方法的细节。",
      "conclusion": "该研究的主要贡献是开发了一个高效且可解释的混合系统，用于医疗覆盖政策评估。通过结合检索器和符号推理，不仅降低了推理成本，还提高了准确性，支持人类审查员进行更可靠的决策。学术价值在于展示了融合符号AI和大型语言模型在处理复杂文档任务中的潜力，实际应用价值在于为医疗政策审查提供了可扩展解决方案。未来工作可能涉及进一步优化规则推理或扩展到其他领域。",
      "tags": [
        "Large Language Model",
        "Coverage-aware Retriever",
        "Symbolic Rule-based Reasoning",
        "Policy Assessment",
        "Efficiency Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:34.543831Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02015",
    "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects",
    "authors": [
      "Omar Momen",
      "Emilie Sitter",
      "Berenike Herrmann",
      "Sina Zarrieß"
    ],
    "abstract": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.02015.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02015",
    "published": "2026-01-05T11:24:33Z",
    "updated": "2026-01-08T18:27:27Z",
    "comment": "to be published at EACL 2026 main conference",
    "light_analysis": {
      "overview": "本研究发现语言模型的意外性度量与隐喻新颖性存在中等相关性，并揭示了基于数据类型的不同缩放效应。",
      "motivation": "隐喻理解涉及复杂的语义过程和语言创造性，是研究语言模型的理想任务。本研究旨在探索语言模型中的意外性度量是否与不同隐喻新颖性数据集相关。现有方法中，surprisal作为预测性度量在语言模型中的应用广泛，但其在衡量语言创造性方面的有效性尚不明确。这个问题重要，因为它有助于评估语言模型在处理创造性语言任务时的性能，并揭示语言创造性的量化方法，为语言模型的评估和语言学研究提供新视角。",
      "method": "研究采用16个语言模型变体，分析了基于语料库和合成的隐喻新颖性数据集。核心方法是一种基于完形填空的意外性计算方式，该方法基于全句上下文来评估surprisal，创新点在于探索上下文敏感的surprisal度量。关键细节包括使用多种语言模型架构和数据集类型，以全面比较surprisal与隐喻新颖性的关系，未具体说明模型架构细节，但强调了方法的普适性和数据依赖性。",
      "result": "实验结果显示，语言模型生成的surprisal与隐喻新颖性得分/标签之间存在显著的中等相关性。具体数据表明，在基于语料库的数据上，相关性强度随模型大小增加而减小，表现出逆缩放效应；而在合成数据上，相关性随模型大小增加而增大，符合质量-功率假设。与基线方法对比未明确说明，但通过不同模型变体的比较，揭示了缩放模式的差异，强调了数据来源对结果的影响。",
      "conclusion": "本研究的主要贡献是发现surprisal可以部分解释隐喻新颖性的注释，但它作为语言创造性的度量具有局限性。学术价值在于揭示了语言模型在创造性任务评估中的潜在不足，并为未来开发更有效的语言创造性度量方法提供了基础。实际应用价值包括改进语言模型的评估框架和促进自然语言处理中创造性任务的研究。未来工作可探索更多元化的度量方法或结合其他语言学特征来增强评估准确性。",
      "tags": [
        "Surprisal",
        "Language Models",
        "Metaphor Novelty",
        "Scaling Effects",
        "Cloze-style Method"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:16.173289Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05202",
    "title": "Stock Market Price Prediction using Neural Prophet with Deep Neural Network",
    "authors": [
      "Navin Chhibber",
      "Suneel Khemka",
      "Navneet Kumar Tyagi",
      "Rohit Tewari",
      "Bireswar Banerjee",
      "Piyush Ranjan"
    ],
    "abstract": "Stock market price prediction is a significant interdisciplinary research domain that depends at the intersection of finance, statistics, and economics. Forecasting Accurately predicting stock prices has always been a focal point for various researchers. However, existing statistical approaches for time-series prediction often fail to effectively forecast the probability range of future stock prices. Hence, to solve this problem, the Neural Prophet with a Deep Neural Network (NP-DNN) is proposed to predict stock market prices. The preprocessing technique used in this research is Z-score normalization, which normalizes stock price data by removing scale differences, making patterns easier to detect. Missing value imputation fills gaps in historical data, enhancing the models use of complete information for more accurate predictions. The Multi-Layer Perceptron (MLP) learns complex nonlinear relationships among stock market prices and extracts hidden patterns from the input data, thereby creating meaningful feature representations for better prediction accuracy. The proposed NP-DNN model achieved an accuracy of 99.21% compared with other approaches using the Fused Large Language Model. Keywords: deep neural network, forecasting stock prices, multi-layer perceptron, neural prophet, stock market price prediction.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05202.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05202",
    "published": "2026-01-08T18:24:22Z",
    "updated": "2026-01-08T18:24:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合Neural Prophet与深度神经网络的模型（NP-DNN），用于提高股票市场价格预测的准确性。",
      "motivation": "股票市场价格预测是金融、统计和经济学的关键交叉研究领域，对投资策略和市场分析至关重要。然而，传统统计方法在预测未来股票价格的概率范围时常常失效，难以处理时间序列数据的复杂非线性特征，导致预测结果不准确或不稳定。这一局限性激发了本研究，旨在通过开发更先进的模型来改进预测性能，应对金融市场中的不确定性挑战。",
      "method": "研究方法首先采用Z-score归一化进行数据预处理，以消除尺度差异并增强模式可检测性；缺失值插补技术则补充历史数据，确保信息完整性。核心模型NP-DNN集成了Neural Prophet（用于时间序列建模）与多层感知器（MLP），其中MLP通过学习复杂非线性关系和提取隐藏模式，构建有意义的特征表示，从而优化预测准确性。模型架构结合了时间序列处理和深度特征学习能力。",
      "result": "实验结果显示，NP-DNN模型在股票价格预测任务中达到了99.21%的准确率。与基线方法（如基于融合大语言模型的方法）相比，该模型表现出更高的预测性能，尽管摘要未明确说明具体比较细节和数据集信息。高准确率验证了模型的有效性，表明其能更好地捕捉市场动态并提升预测精度。",
      "conclusion": "本研究的主要贡献是提出了NP-DNN模型，通过整合Neural Prophet的时间序列处理能力和MLP的深度特征学习，显著提高了股票市场价格的预测准确性。这不仅在学术上丰富了时间序列预测方法，还具有实际应用价值，如辅助金融决策。未来工作可能包括优化模型结构、扩展到其他金融时间序列或评估在不同市场条件下的鲁棒性。",
      "tags": [
        "Deep Neural Network",
        "Multi-Layer Perceptron",
        "Neural Prophet",
        "Time Series Prediction",
        "Z-score Normalization"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:29.017247Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05201",
    "title": "Mechanisms of Prompt-Induced Hallucination in Vision-Language Models",
    "authors": [
      "William Rudman",
      "Michal Golovanevsky",
      "Dana Arad",
      "Yonatan Belinkov",
      "Ritambhara Singh",
      "Carsten Eickhoff",
      "Kyle Mahowald"
    ],
    "abstract": "Large vision-language models (VLMs) are highly capable, yet often hallucinate by favoring textual prompts over visual evidence. We study this failure mode in a controlled object-counting setting, where the prompt overstates the number of objects in the image (e.g., asking a model to describe four waterlilies when only three are present). At low object counts, models often correct the overestimation, but as the number of objects increases, they increasingly conform to the prompt regardless of the discrepancy. Through mechanistic analysis of three VLMs, we identify a small set of attention heads whose ablation substantially reduces prompt-induced hallucinations (PIH) by at least 40% without additional training. Across models, PIH-heads mediate prompt copying in model-specific ways. We characterize these differences and show that PIH ablation increases correction toward visual evidence. Our findings offer insights into the internal mechanisms driving prompt-induced hallucinations, revealing model-specific differences in how these behaviors are implemented.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05201.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05201",
    "published": "2026-01-08T18:23:03Z",
    "updated": "2026-01-08T18:23:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过机制分析揭示了视觉语言模型中导致提示诱导幻觉的注意力头，并通过消融方法显著减少幻觉。",
      "motivation": "大型视觉语言模型在处理多模态任务时，常因过度依赖文本提示而忽视视觉证据，产生幻觉现象，这损害了模型的可靠性和实际应用价值。本研究旨在解决这一问题，特别是在受控对象计数设置中，当提示高估对象数量时，模型随计数增加更倾向于错误输出。现有方法可能未深入探讨其内部机制，该研究聚焦于揭示幻觉的根本原因，以推动模型改进和增强鲁棒性。",
      "method": "研究采用受控对象计数设置，分析三个视觉语言模型的注意力机制。核心方法是通过机制分析识别一小部分关键的注意力头（PIH-heads），这些头介导了模型对文本提示的复制行为。技术特色包括对这些头进行消融实验，无需额外训练，直接减少幻觉。关键细节涉及使用注意力分析来定位模型内部结构，并评估不同对象计数下的行为差异，以理解模型如何处理不一致的提示和视觉证据。",
      "result": "主要实验结果显示，消融PIH-heads能显著减少提示诱导的幻觉，至少降低40%。在低对象计数时，模型倾向于纠正提示高估，但随着计数增加，模型更可能遵循提示而忽视视觉差异；消融后，模型对视觉证据的校正增强，提升了准确性。与原始模型相比，消融方法在无需训练的情况下有效缓解了幻觉问题，通过量化指标（如幻觉减少比例）和定性行为变化来验证效果。",
      "conclusion": "论文的主要贡献在于揭示了视觉语言模型中提示诱导幻觉的内部机制，识别了模型特定的注意力头及其功能。学术价值在于为理解模型行为提供了新视角，实际应用价值可能包括指导模型设计和减少幻觉策略。研究还展示了消融干预的潜力，但局限性可能在于集中于特定任务，未来工作可扩展到其他领域，以全面评估方法的泛化性和有效性。",
      "tags": [
        "Vision-Language Models",
        "Attention Heads",
        "Ablation Study",
        "Prompt-Induced Hallucination",
        "Mechanistic Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:05.180665Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03470",
    "title": "Toward Maturity-Based Certification of Embodied AI: Quantifying Trustworthiness Through Measurement Mechanisms",
    "authors": [
      "Michael C. Darling",
      "Alan H. Hesu",
      "Michael A. Mardikes",
      "Brian C. McGuigan",
      "Reed M. Milewicz"
    ],
    "abstract": "We propose a maturity-based framework for certifying embodied AI systems through explicit measurement mechanisms. We argue that certifiable embodied AI requires structured assessment frameworks, quantitative scoring mechanisms, and methods for navigating multi-objective trade-offs inherent in trustworthiness evaluation. We demonstrate this approach using uncertainty quantification as an exemplar measurement mechanism and illustrate feasibility through an Uncrewed Aircraft System (UAS) detection case study.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.03470.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03470",
    "published": "2026-01-06T23:48:00Z",
    "updated": "2026-01-08T18:16:48Z",
    "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification",
    "light_analysis": {
      "overview": "本文提出了一个基于成熟度的框架，通过量化测量机制来认证实体AI系统的信任度。",
      "motivation": "随着实体AI在无人系统等领域的广泛应用，缺乏标准化评估机制可能导致系统可靠性不足。现有认证方法往往忽视量化测量和多目标权衡，难以客观评估信任度。因此，本研究旨在解决实体AI系统认证的标准化问题，通过结构化评估框架和量化机制来确保系统可信性，弥补当前方法的不足。",
      "method": "本研究提出一个基于成熟度的认证框架，核心包括结构化评估体系、量化评分机制和权衡导航方法。创新点在于将不确定性量化作为示例测量机制，以无人飞行器系统（UAS）检测为案例研究，展示如何应用该框架量化系统性能。该方法通过具体测量机制和案例验证，支持实体AI的多方面信任度评估。",
      "result": "摘要未明确说明具体实验数据，如准确率或效率指标。但通过UAS检测案例研究，论文展示了该框架的可行性，表明能够量化信任度并支持认证过程。虽然没有与基线方法的直接对比数据，但案例验证了框架在实际应用中的适用性。",
      "conclusion": "本研究的主要贡献是提供了一个基于成熟度的认证框架，通过测量机制量化实体AI系统的信任度，推动评估方法的标准化。学术价值在于丰富了评估理论，实际应用价值包括提升无人系统等领域的可靠性。局限性可能在于框架的通用性和测量机制的扩展性，未来工作可以探索更多测量机制和优化评估指标。",
      "tags": [
        "Embodied AI",
        "Trustworthiness Evaluation",
        "Maturity-Based Certification",
        "Uncertainty Quantification",
        "Uncrewed Aircraft System"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:12.141149Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05187",
    "title": "SimuAgent: An LLM-Based Simulink Modeling Assistant Enhanced with Reinforcement Learning",
    "authors": [
      "Yanchang Liang",
      "Xiaowei Zhao"
    ],
    "abstract": "Large language models (LLMs) have revolutionized text-based code automation, but their potential in graph-oriented engineering workflows remains under-explored. We introduce SimuAgent, an LLM-powered modeling and simulation agent tailored for Simulink. SimuAgent replaces verbose XML with a concise, dictionary-style Python representation, dramatically cutting token counts, improving interpretability, and enabling fast, in-process simulation. A lightweight plan-execute architecture, trained in two stages, equips the agent with both low-level tool skills and high-level design reasoning. To tackle sparse rewards in long-horizon tasks, we propose Reflection-GRPO (ReGRPO), which augments Group Relative Policy Optimization (GRPO) with self-reflection traces that supply rich intermediate feedback, accelerating convergence and boosting robustness. Experiments on SimuBench, our newly released benchmark comprising 5300 multi-domain modeling tasks, show that a Qwen2.5-7B model fine-tuned with SimuAgent converges faster and achieves higher modeling accuracy than standard RL baselines, and even surpasses GPT-4o when evaluated with few-shot prompting on the same benchmark. Ablations confirm that the two-stage curriculum and abstract-reconstruct data augmentation further enhance generalization. SimuAgent trains and runs entirely on-premise with modest hardware, delivering a privacy-preserving, cost-effective solution for industrial model-driven engineering. SimuAgent bridges the gap between LLMs and graphical modeling environments, offering a practical solution for AI-assisted engineering design in industrial settings.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05187.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05187",
    "published": "2026-01-08T18:10:35Z",
    "updated": "2026-01-08T18:10:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "SimuAgent是一个基于大型语言模型并增强强化学习的Simulink建模助手，旨在弥合LLMs与图形工程环境之间的差距。",
      "motivation": "大型语言模型在文本代码自动化中已取得显著进展，但在图工程工作流如Simulink建模中应用不足。现有方法通常使用冗长的XML表示，导致高token计数、低可解释性和模拟效率低下，限制了工业模型驱动工程的实际应用。这一问题的重要性在于工业设计需要高效、隐私保护的AI工具来提升自动化水平，而现有技术的不足催生了本研究的探索。",
      "method": "研究提出了SimuAgent，将Simulink模型转换为简洁的字典式Python表示，以减少token使用并提升可解释性。采用轻量级计划-执行架构，通过两阶段训练使代理掌握低级工具技能和高级设计推理。为解决长时程任务中的稀疏奖励，引入了Reflection-GRPO（ReGRPO），通过自我反思轨迹提供中间反馈，加速强化学习的收敛和增强鲁棒性。关键细节包括使用新发布的SimuBench基准（含5300个多域建模任务）和基于Qwen2.5-7B模型进行微调。",
      "result": "实验结果显示，在SimuBench基准上，使用SimuAgent微调的Qwen2.5-7B模型比标准强化学习基线收敛更快，建模精度更高。在使用少样本提示评估时，其性能甚至超越了GPT-4o。消融研究证实，两阶段课程训练和抽象-重构数据增强策略进一步提升了模型的泛化能力和鲁棒性，确保了实验结果的有效性和可靠性。",
      "conclusion": "SimuAgent的主要贡献是成功连接了大型语言模型与图形建模环境，为工业工程设计提供了一个实用、隐私保护且成本效益高的AI辅助工具。这项研究具有重要的学术价值，推动了LLMs在非文本领域的应用扩展，并在实际中促进了模型驱动工程的自动化；未来工作可探索更广泛的工程场景或集成其他AI技术，以增强适应性。摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Group Relative Policy Optimization",
        "Self-Reflection",
        "Simulink Modeling"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:12.770395Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05184",
    "title": "Observations and Remedies for Large Language Model Bias in Self-Consuming Performative Loop",
    "authors": [
      "Yaxuan Wang",
      "Zhongteng Cai",
      "Yujia Bao",
      "Xueru Zhang",
      "Yang Liu"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has led to growing interest in using synthetic data to train future models. However, this creates a self-consuming retraining loop, where models are trained on their own outputs and may cause performance drops and induce emerging biases. In real-world applications, previously deployed LLMs may influence the data they generate, leading to a dynamic system driven by user feedback. For example, if a model continues to underserve users from a group, less query data will be collected from this particular demographic of users. In this study, we introduce the concept of \\textbf{S}elf-\\textbf{C}onsuming \\textbf{P}erformative \\textbf{L}oop (\\textbf{SCPL}) and investigate the role of synthetic data in shaping bias during these dynamic iterative training processes under controlled performative feedback. This controlled setting is motivated by the inaccessibility of real-world user preference data from dynamic production systems, and enables us to isolate and analyze feedback-driven bias evolution in a principled manner. We focus on two types of loops, including the typical retraining setting and the incremental fine-tuning setting, which is largely underexplored. Through experiments on three real-world tasks, we find that the performative loop increases preference bias and decreases disparate bias. We design a reward-based rejection sampling strategy to mitigate the bias, moving towards more trustworthy self-improving systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05184.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05184",
    "published": "2026-01-08T18:08:15Z",
    "updated": "2026-01-08T18:08:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入自我消耗表演性循环概念，并设计基于奖励的拒绝采样策略来减轻大语言模型在自我训练中的偏差。",
      "motivation": "研究动机源于大语言模型快速发展下，使用合成数据训练未来模型导致自我消耗重训练循环，可能引发性能下降和偏差放大。在真实应用中，已部署模型通过用户反馈动态影响数据生成，加剧对服务不足群体的偏差问题，如查询数据减少。现有方法因真实用户偏好数据难以获取，且对动态反馈下偏差演化，特别是增量微调设置，研究不足，因此需要控制实验环境以隔离和分析偏差变化。",
      "method": "研究方法包括引入自我消耗表演性循环概念，在控制表演性反馈设置下研究合成数据在动态迭代训练中如何塑造偏差。实验聚焦两种循环类型：典型重训练和增量微调（后者未充分探索），通过三个真实世界任务进行分析。关键创新是设计了一种基于奖励的拒绝采样策略来减轻偏差，以促进更可信的自我改进系统，利用合成数据和反馈机制模拟真实场景。",
      "result": "实验结果显示，表演性循环在动态训练过程中增加偏好偏差并减少差异偏差。通过设计的奖励基础拒绝采样策略，能有效缓解这些偏差，但具体性能指标如准确率提升或效率改进摘要未明确说明，需进一步实验验证。结果与基线方法对比未详细描述，但表明了策略在减轻偏差方面的潜力。",
      "conclusion": "论文的主要贡献是提出SCPL框架，深入分析了大语言模型自我训练中的偏差演化机制，并提供缓解策略。这不仅增进了对动态系统中偏差的理解，学术上提供了新见解，实际应用上促进了更可信的自我改进系统。局限性包括真实世界数据获取挑战，未来工作可扩展至更复杂场景、优化策略性能并探索其他偏差缓解技术。",
      "tags": [
        "Large Language Model",
        "Self-Consuming Loop",
        "Synthetic Data",
        "Bias Mitigation",
        "Rejection Sampling"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:28.871292Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05174",
    "title": "FaST: Efficient and Effective Long-Horizon Forecasting for Large-Scale Spatial-Temporal Graphs via Mixture-of-Experts",
    "authors": [
      "Yiji Zhao",
      "Zihao Zhong",
      "Ao Wang",
      "Haomin Wen",
      "Ming Jin",
      "Yuxuan Liang",
      "Huaiyu Wan",
      "Hao Wu"
    ],
    "abstract": "Spatial-Temporal Graph (STG) forecasting on large-scale networks has garnered significant attention. However, existing models predominantly focus on short-horizon predictions and suffer from notorious computational costs and memory consumption when scaling to long-horizon predictions and large graphs. Targeting the above challenges, we present FaST, an effective and efficient framework based on heterogeneity-aware Mixture-of-Experts (MoEs) for long-horizon and large-scale STG forecasting, which unlocks one-week-ahead (672 steps at a 15-minute granularity) prediction with thousands of nodes. FaST is underpinned by two key innovations. First, an adaptive graph agent attention mechanism is proposed to alleviate the computational burden inherent in conventional graph convolution and self-attention modules when applied to large-scale graphs. Second, we propose a new parallel MoE module that replaces traditional feed-forward networks with Gated Linear Units (GLUs), enabling an efficient and scalable parallel structure. Extensive experiments on real-world datasets demonstrate that FaST not only delivers superior long-horizon predictive accuracy but also achieves remarkable computational efficiency compared to state-of-the-art baselines. Our source code is available at: https://github.com/yijizhao/FaST.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05174.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05174",
    "published": "2026-01-08T18:00:58Z",
    "updated": "2026-01-08T18:00:58Z",
    "comment": "Accepted to KDD 2026",
    "light_analysis": {
      "overview": "提出FaST框架，基于异构感知专家混合，实现大规模时空图长时预测的高效性与准确性。",
      "motivation": "时空图预测在大规模网络中应用广泛，如交通流量预测，但现有模型主要关注短时预测，当扩展到长时预测（如一周期）和大图（数千节点）时，面临高计算成本和内存消耗的挑战。现有方法如传统图卷积和自注意力模块在处理大规模图时效率低下，限制了实际应用，因此需要高效且有效的长时预测解决方案。",
      "method": "FaST框架采用两个关键创新：一是自适应图代理注意力机制，替代传统图卷积和自注意力，减轻大规模图上的计算负担；二是并行专家混合模块，使用门控线性单元替换前馈网络，实现高效并行结构。该方法基于异构感知专家混合，支持数千节点和一周前预测（672步，15分钟粒度），数据集为真实世界数据，但摘要未明确说明具体名称。",
      "result": "在真实世界数据集上的实验表明，FaST在长时预测准确性上优于最先进基线方法，同时显著提高计算效率。具体能实现一周前预测（672步），处理数千节点，与基线相比，在预测性能和效率方面均有优势，但摘要未明确给出具体准确率或效率提升百分比。",
      "conclusion": "FaST通过创新机制有效解决了大规模时空图长时预测的计算挑战，提升了预测准确性和效率，具有学术价值如推动时空图建模发展，以及实际应用价值如智能交通系统优化。局限性或未来工作摘要未明确说明。",
      "tags": [
        "Spatial-Temporal Graph",
        "Mixture-of-Experts",
        "Gated Linear Units",
        "Graph Attention",
        "Long-Horizon Forecasting"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:07.328907Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05172",
    "title": "CoV: Chain-of-View Prompting for Spatial Reasoning",
    "authors": [
      "Haoyu Zhao",
      "Akide Liu",
      "Zeyu Zhang",
      "Weijie Wang",
      "Feng Chen",
      "Ruihan Zhu",
      "Gholamreza Haffari",
      "Bohan Zhuang"
    ],
    "abstract": "Embodied question answering (EQA) in 3D environments often requires collecting context that is distributed across multiple viewpoints and partially occluded. However, most recent vision--language models (VLMs) are constrained to a fixed and finite set of input views, which limits their ability to acquire question-relevant context at inference time and hinders complex spatial reasoning. We propose Chain-of-View (CoV) prompting, a training-free, test-time reasoning framework that transforms a VLM into an active viewpoint reasoner through a coarse-to-fine exploration process. CoV first employs a View Selection agent to filter redundant frames and identify question-aligned anchor views. It then performs fine-grained view adjustment by interleaving iterative reasoning with discrete camera actions, obtaining new observations from the underlying 3D scene representation until sufficient context is gathered or a step budget is reached.   We evaluate CoV on OpenEQA across four mainstream VLMs and obtain an average +11.56\\% improvement in LLM-Match, with a maximum gain of +13.62\\% on Qwen3-VL-Flash. CoV further exhibits test-time scaling: increasing the minimum action budget yields an additional +2.51\\% average improvement, peaking at +3.73\\% on Gemini-2.5-Flash. On ScanQA and SQA3D, CoV delivers strong performance (e.g., 116 CIDEr / 31.9 EM@1 on ScanQA and 51.1 EM@1 on SQA3D). Overall, these results suggest that question-aligned view selection coupled with open-view search is an effective, model-agnostic strategy for improving spatial reasoning in 3D EQA without additional training.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05172.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05172",
    "published": "2026-01-08T17:59:42Z",
    "updated": "2026-01-08T17:59:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "CoV提出了一种无需训练的测试时间推理框架，通过链式视角提示增强视觉-语言模型在3D具身问答中的空间推理能力。",
      "motivation": "在3D环境中的具身问答需要收集分布在多个视角下的上下文信息，尤其当信息被部分遮挡时。然而，大多数现有视觉-语言模型受限于固定输入视角，无法在推理时动态获取问题相关上下文，这阻碍了复杂空间推理。这种局限性在机器人导航等实际应用中尤为重要，因为现有方法不能主动探索环境以应对动态场景，导致性能受限。摘要未明确说明基线方法的具体不足，但暗示了固定视图输入的效率问题。",
      "method": "CoV框架采用从粗到细的探索过程：首先，通过视图选择代理过滤冗余帧并识别问题对齐的锚视图；然后，进行细粒度视图调整，结合迭代推理与离散相机动作，从底层3D场景表示中获取新观测，直到收集足够上下文或达到步骤预算。该方法无需额外训练，可适用于多种主流视觉-语言模型，如摘要中提到的Qwen3-VL-Flash和Gemini-2.5-Flash，实现模型无关的主动视角探索。关键创新点包括训练免费的推理机制和开放式视图搜索策略。",
      "result": "在OpenEQA数据集上，CoV在四个主流视觉-语言模型上平均提升了11.56%的LLM-Match指标，最大提升达13.62%（Qwen3-VL-Flash）。测试时间缩放实验显示，增加最小动作预算带来额外2.51%平均提升，峰值3.73%（Gemini-2.5-Flash）。在ScanQA数据集上，CoV取得116 CIDEr和31.9 EM@1；在SQA3D上，EM@1为51.1。这些结果表明CoV显著优于基线固定视图方法，尤其是在复杂空间推理任务中，性能提升具统计意义。",
      "conclusion": "CoV通过问题对齐的视图选择和开放式视图搜索，提供了一种有效且模型无关的策略，改善了3D具身问答中的空间推理，无需额外训练。该工作的学术价值在于提出了新的测试时间推理框架，推动视觉-语言模型在动态环境中的能力；实际应用价值包括增强机器人和虚拟助手的交互性能。局限性可能包括步骤预算的约束，未来工作可优化动作策略或扩展到其他多模态推理任务。摘要未明确说明具体局限性，但提及了预算机制。",
      "tags": [
        "Chain-of-View Prompting",
        "Spatial Reasoning",
        "Embodied Question Answering",
        "Vision-Language Models",
        "View Selection"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:23.665165Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05167",
    "title": "RelayLLM: Efficient Reasoning via Collaborative Decoding",
    "authors": [
      "Chengsong Huang",
      "Tong Zheng",
      "Langlin Huang",
      "Jinyuan Li",
      "Haolin Liu",
      "Jiaxin Huang"
    ],
    "abstract": "Large Language Models (LLMs) for complex reasoning is often hindered by high computational costs and latency, while resource-efficient Small Language Models (SLMs) typically lack the necessary reasoning capacity. Existing collaborative approaches, such as cascading or routing, operate at a coarse granularity by offloading entire queries to LLMs, resulting in significant computational waste when the SLM is capable of handling the majority of reasoning steps. To address this, we propose RelayLLM, a novel framework for efficient reasoning via token-level collaborative decoding. Unlike routers, RelayLLM empowers the SLM to act as an active controller that dynamically invokes the LLM only for critical tokens via a special command, effectively \"relaying\" the generation process. We introduce a two-stage training framework, including warm-up and Group Relative Policy Optimization (GRPO) to teach the model to balance independence with strategic help-seeking. Empirical results across six benchmarks demonstrate that RelayLLM achieves an average accuracy of 49.52%, effectively bridging the performance gap between the two models. Notably, this is achieved by invoking the LLM for only 1.07% of the total generated tokens, offering a 98.2% cost reduction compared to performance-matched random routers.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05167.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05167",
    "published": "2026-01-08T17:56:16Z",
    "updated": "2026-01-08T17:56:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "RelayLLM提出一种令牌级协作解码框架，通过让小型语言模型动态调用大型语言模型处理关键令牌，实现高效推理并大幅降低成本。",
      "motivation": "大型语言模型在复杂推理中面临高计算成本和延迟问题，而小型语言模型虽然资源高效但推理能力不足。现有协作方法如级联或路由在粗粒度上运作，将整个查询卸载给大型语言模型，导致当小型语言模型能处理大部分推理步骤时产生显著计算浪费。因此，研究旨在解决效率与性能的平衡问题，推动更细粒度的协作方法发展，以减少资源开销并提升推理能力。",
      "method": "论文提出RelayLLM框架，采用令牌级协作解码技术，核心创新是让小型语言模型作为主动控制器，通过特殊命令动态调用大型语言模型仅处理关键令牌，实现“接力”生成过程。训练包括两阶段：热身和组相对策略优化（GRPO），以教导模型平衡独立推理与策略性求助。该方法在六个基准测试上进行实证评估，摘要未明确说明具体数据集和模型架构。",
      "result": "实验结果显示，RelayLLM在六个基准测试中平均准确率达到49.52%，有效桥接了大型与小型语言模型之间的性能差距。效率方面，大型语言模型仅被调用占总生成令牌的1.07%，相比性能匹配的随机路由器，成本降低了98.2%。这表明方法在保持高准确率的同时显著减少了计算开销，优于基线方法。",
      "conclusion": "本研究的主要贡献是提出了RelayLLM框架，通过令牌级协作解码实现了高效推理，显著降低成本同时维持性能。学术上，为语言模型协作提供了新思路，推动了高效AI技术的发展；实际应用中，有助于资源受限环境下的高性能推理部署。未来工作可进一步优化训练策略或扩展到更多任务领域，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Small Language Model",
        "Collaborative Decoding",
        "Group Relative Policy Optimization",
        "Token-Level Generation"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:00.196264Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.10152",
    "title": "Improving and Evaluating Open Deep Research Agents",
    "authors": [
      "Doaa Allabadi",
      "Kyle Bradbury",
      "Jordan M. Malof"
    ],
    "abstract": "We focus here on Deep Research Agents (DRAs), which are systems that can take a natural language prompt from a user, and then autonomously search for, and utilize, internet-based content to address the prompt. Recent DRAs have demonstrated impressive capabilities on public benchmarks however, recent research largely involves proprietary closed-source systems. At the time of this work, we only found one open-source DRA, termed Open Deep Research (ODR). In this work we adapt the challenging recent BrowseComp benchmark to compare ODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small), comprising a subset of BrowseComp, as a more computationally-tractable DRA benchmark for academic labs. We benchmark ODR and two other proprietary systems on BC-Small: one system from Anthropic and one system from Google. We find that all three systems achieve 0% accuracy on the test set of 60 questions. We introduce three strategic improvements to ODR, resulting in the ODR+ model, which achieves a state-of-the-art 10% success rate on BC-Small among both closed-source and open-source systems. We report ablation studies indicating that all three of our improvements contributed to the success of ODR+.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.10152.pdf",
    "abs_url": "https://arxiv.org/abs/2508.10152",
    "published": "2025-08-13T19:32:01Z",
    "updated": "2026-01-08T17:54:58Z",
    "comment": "8 pages, 2 figures, 2 tables",
    "light_analysis": {
      "overview": "本研究提出改进的ODR+模型，在BrowseComp-Small基准上实现10%的成功率，成为当前最佳的开源和闭源Deep Research Agents系统。",
      "motivation": "研究动机源于Deep Research Agents (DRAs) 领域缺乏开源系统，阻碍学术社区参与。DRAs能根据自然语言提示自主搜索互联网内容，但现有系统多为闭源（如Anthropic和Google的专有系统），尽管在公共基准上表现良好，却难以复现和评估。BrowseComp基准计算量大，不适合资源有限的学术实验室。因此，本研究旨在推动开源DRA发展，通过提供更易处理的评估方法和改进系统，解决透明性和可访问性问题，促进该领域的技术进步和公平比较。",
      "method": "研究方法包括将挑战性基准BrowseComp适配为BrowseComp-Small (BC-Small)，作为计算上更易处理的DRA评估标准，适合学术实验室使用。在BC-Small上比较了开源系统Open Deep Research (ODR) 与两个闭源系统（Anthropic和Google的DRAs）。通过分析失败模式，提出了三个战略改进措施应用于ODR，构建了ODR+模型。关键创新在于基准简化和系统优化，具体改进策略未在摘要中详细说明，但消融研究确认了它们的贡献。这为开源DRA提供了可复现的技术路线。",
      "result": "主要实验结果：在BC-Small基准的60个问题测试集上，原始ODR和两个闭源系统的准确率均为0%。改进后的ODR+模型成功率达到10%，在所有开源和闭源系统中达到最高水平，成为新的性能标准。消融研究表明，所有三个改进措施都对性能提升有显著贡献，验证了方法的有效性。这一结果不仅展示了开源DRA的潜力，还突出了基准评估在推动系统改进中的重要性。",
      "conclusion": "本研究通过改进ODR并引入BC-Small基准，促进了开源Deep Research Agents的发展。主要贡献包括提供可访问的评估工具和性能领先的ODR+模型，具有学术价值（如推动透明研究和基准标准化）和实际应用价值（如提升自主代理的任务处理能力）。局限性包括基准规模较小，性能仍有提升空间；未来工作可能涉及进一步优化模型策略或扩展到更大规模、更复杂的任务场景，以增强DRA的泛化能力。",
      "tags": [
        "Deep Research Agents",
        "Open Source AI",
        "Benchmark Evaluation",
        "Strategic Improvements",
        "Ablation Studies"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:02.207254Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05159",
    "title": "Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering",
    "authors": [
      "Shuliang Liu",
      "Songbo Yang",
      "Dong Fang",
      "Sihang Jia",
      "Yuqi Tang",
      "Lingfeng Su",
      "Ruoshui Peng",
      "Yibo Yan",
      "Xin Zou",
      "Xuming Hu"
    ],
    "abstract": "Object hallucination critically undermines the reliability of Multimodal Large Language Models, often stemming from a fundamental failure in cognitive introspection, where models blindly trust linguistic priors over specific visual evidence. Existing mitigations remain limited: contrastive decoding approaches operate superficially without rectifying internal semantic misalignments, while current latent steering methods rely on static vectors that lack instance-specific precision. We introduce Vision-Language Introspection (VLI), a training-free inference framework that simulates a metacognitive self-correction process. VLI first performs Attributive Introspection to diagnose hallucination risks via probabilistic conflict detection and localize the causal visual anchors. It then employs Interpretable Bi-Causal Steering to actively modulate the inference process, dynamically isolating visual evidence from background noise while neutralizing blind confidence through adaptive calibration. VLI achieves state-of-the-art performance on advanced models, reducing object hallucination rates by 12.67% on MMHal-Bench and improving accuracy by 5.8% on POPE.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05159.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05159",
    "published": "2026-01-08T17:49:13Z",
    "updated": "2026-01-08T17:49:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Vision-Language Introspection (VLI)框架，通过可解释的双因果引导来减轻多模态大语言模型中的过度自信幻觉问题。",
      "motivation": "物体幻觉严重损害多模态大语言模型的可靠性，源于模型过度信任语言先验而忽视特定视觉证据的认知内省失败。现有方法如对比性解码仅表面操作，未能纠正内部语义错位；潜在引导方法依赖静态向量，缺乏实例特定精度。因此，需要一种更精确的方法来诊断和纠正幻觉，以提升模型在实际应用如视觉问答中的可信度和实用性，解决现有方法在语义对齐和动态调整上的不足。",
      "method": "Vision-Language Introspection (VLI)是一个无训练推理框架，模拟元认知自我纠正过程。它首先进行属性内省，通过概率冲突检测诊断幻觉风险并定位因果视觉锚点；然后采用可解释的双因果引导主动调制推理过程，动态隔离视觉证据与背景噪声，并通过自适应校准中和盲目置信。关键创新在于结合诊断和引导，实现实例特定的精确干预，无需额外训练即可应用于现有多模态大语言模型，提升内省能力。",
      "result": "VLI在先进模型上实现了最先进的性能，在MMHal-Bench基准上将物体幻觉率降低了12.67%，在POPE基准上准确率提升了5.8%。这些结果表明该方法能有效减轻幻觉问题，提升模型对视觉证据的依赖和整体可靠性。摘要未明确说明与具体基线的详细对比，但数据支持其在减少幻觉和提升准确率方面的显著改进，验证了框架的有效性。",
      "conclusion": "论文的主要贡献是引入了Vision-Language Introspection框架，通过可解释的双因果引导减轻了多模态大语言模型中的过度自信幻觉，增强了模型的内省能力和可靠性。学术上，它为可解释AI和因果推理在视觉语言模型中的应用提供了新思路；实际中，可改善视觉问答等任务的性能。未来工作可探索扩展到其他幻觉类型或更大规模数据集，但摘要未明确说明具体局限性。",
      "tags": [
        "Multimodal Large Language Models",
        "Hallucination Mitigation",
        "Interpretable Bi-Causal Steering",
        "Probabilistic Conflict Detection",
        "Adaptive Calibration"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:26.744956Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05152",
    "title": "Safe Continual Reinforcement Learning Methods for Nonstationary Environments. Towards a Survey of the State of the Art",
    "authors": [
      "Timofey Tomashevskiy"
    ],
    "abstract": "This work provides a state-of-the-art survey of continual safe online reinforcement learning (COSRL) methods. We discuss theoretical aspects, challenges, and open questions in building continual online safe reinforcement learning algorithms. We provide the taxonomy and the details of continual online safe reinforcement learning methods based on the type of safe learning mechanism that takes adaptation to nonstationarity into account. We categorize safety constraints formulation for online reinforcement learning algorithms, and finally, we discuss prospects for creating reliable, safe online learning algorithms.   Keywords: safe RL in nonstationary environments, safe continual reinforcement learning under nonstationarity, HM-MDP, NSMDP, POMDP, safe POMDP, constraints for continual learning, safe continual reinforcement learning review, safe continual reinforcement learning survey, safe continual reinforcement learning, safe online learning under distribution shift, safe continual online adaptation, safe reinforcement learning, safe exploration, safe adaptation, constrained Markov decision processes, safe reinforcement learning, partially observable Markov decision process, safe reinforcement learning and hidden Markov decision processes, Safe Online Reinforcement Learning, safe online reinforcement learning, safe online reinforcement learning, safe meta-learning, safe meta-reinforcement learning, safe context-based reinforcement learning, formulating safety constraints for continual learning",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05152.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05152",
    "published": "2026-01-08T17:42:56Z",
    "updated": "2026-01-08T17:42:56Z",
    "comment": "20 pages, 4 figures",
    "light_analysis": {
      "overview": "该论文提供了关于非平稳环境中持续安全在线强化学习方法的最新综合调查。",
      "motivation": "在非平稳环境中，强化学习算法的安全性至关重要，因为环境动态变化可能导致策略失效并引发安全风险。现有方法往往未能充分适应非平稳性，缺乏系统性的整合和指导。该研究旨在填补这一空白，通过讨论理论挑战和开放问题，推动构建可靠的安全强化学习算法，从而在诸如机器人控制和自动驾驶等实际应用中提升系统的稳定性和鲁棒性。",
      "method": "作为综述论文，该方法基于安全学习机制的类型进行分类，具体包括考虑非平稳性适应的约束制定方式，如在线强化学习中的安全约束分析。核心创新点在于提供了一个综合的分类框架，详细探讨了不同类型的机制，例如基于隐藏马尔可夫决策过程或部分可观测马尔可夫决策过程的技术。摘要未明确提及具体的数据集或模型架构，但强调了分类和细节梳理，以帮助读者理解现有方法的多样性和特点。",
      "result": "摘要未明确说明具体的实验结果，因为该论文是综述性质，未涉及实验性能数据。其主要成果是理论分析和分类框架的提出，而非与基线方法的对比或具体指标提升。研究通过梳理现有技术，为未来工作提供了基础，但没有报告准确率、效率改进等实证结果，仅强调了综合调查的价值和方向。",
      "conclusion": "该论文通过系统调查现有方法，总结了非平稳环境中安全强化学习的关键贡献，包括分类框架和挑战概述。其学术价值在于为研究人员提供了全面参考，促进领域发展；实际应用价值在于指导开发更安全的在线学习系统。局限性在于可能未覆盖所有最新进展，未来工作可包括更新调查或提出新算法，以应对不断变化的环境需求。",
      "tags": [
        "Safe Reinforcement Learning",
        "Continual Learning",
        "Nonstationary Environments",
        "Constrained Markov Decision Processes",
        "Partially Observable Markov Decision Processes"
      ]
    },
    "analyzed_at": "2026-01-09T02:28:51.461571Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05148",
    "title": "Atlas 2 - Foundation models for clinical deployment",
    "authors": [
      "Maximilian Alber",
      "Timo Milbich",
      "Alexandra Carpen-Amarie",
      "Stephan Tietz",
      "Jonas Dippel",
      "Lukas Muttenthaler",
      "Beatriz Perez Cancer",
      "Alessandro Benetti",
      "Panos Korfiatis",
      "Elias Eulig",
      "Jérôme Lüscher",
      "Jiasen Wu",
      "Sayed Abid Hashimi",
      "Gabriel Dernbach",
      "Simon Schallenberg",
      "Neelay Shah",
      "Moritz Krügener",
      "Aniruddh Jammoria",
      "Jake Matras",
      "Patrick Duffy",
      "Matt Redlon",
      "Philipp Jurmeister",
      "David Horst",
      "Lukas Ruff",
      "Klaus-Robert Müller",
      "Frederick Klauschen",
      "Andrew Norgan"
    ],
    "abstract": "Pathology foundation models substantially advanced the possibilities in computational pathology -- yet tradeoffs in terms of performance, robustness, and computational requirements remained, which limited their clinical deployment. In this report, we present Atlas 2, Atlas 2-B, and Atlas 2-S, three pathology vision foundation models which bridge these shortcomings by showing state-of-the-art performance in prediction performance, robustness, and resource efficiency in a comprehensive evaluation across eighty public benchmarks. Our models were trained on the largest pathology foundation model dataset to date comprising 5.5 million histopathology whole slide images, collected from three medical institutions Charité - Universtätsmedizin Berlin, LMU Munich, and Mayo Clinic.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05148.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05148",
    "published": "2026-01-08T17:37:00Z",
    "updated": "2026-01-08T17:37:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Atlas 2系列病理学视觉基础模型，通过优化性能、鲁棒性和资源效率，解决了现有模型在临床部署中的限制，展现了最先进的综合表现。",
      "motivation": "研究旨在解决病理学基础模型在临床部署中面临的挑战。虽然这些模型在计算病理学中取得了进展，但在性能、鲁棒性和计算需求之间存在权衡，导致实际应用受限。现有方法的不足之处包括性能不稳定、对噪声敏感和高计算开销，这影响了模型在医疗诊断中的可靠性和效率。因此，开发更优模型以促进临床环境的广泛应用具有重要意义。",
      "method": "论文提出了Atlas 2、Atlas 2-B和Atlas 2-S三种病理学视觉基础模型，专注于优化预测性能、鲁棒性和计算效率。模型在迄今最大规模的病理学数据集上训练，包含从三家医疗机构收集的550万张组织病理学全切片图像，并在八十个公共基准测试中进行全面评估。摘要未明确说明具体的模型架构或技术细节，但强调了通过数据规模和基准测试来验证模型的有效性。",
      "result": "在八十个公共基准测试的综合评估中，Atlas 2系列模型展示了最先进的性能，包括改进的预测准确性、增强的鲁棒性和更高的资源效率。与基线方法相比，这些模型有效弥补了性能、鲁棒性和计算需求之间的不足。摘要未明确说明具体的数值指标如准确率提升，但强调了模型在多个维度上的优越表现，为实现临床部署提供了强有力的支持。",
      "conclusion": "该研究的主要贡献是提出了Atlas 2系列病理学视觉基础模型，通过优化性能、鲁棒性和效率，解决了现有模型的临床部署障碍。这推进了计算病理学的发展，具有重要的学术价值和实际应用潜力，有望促进AI在医疗诊断中的广泛应用。摘要未明确说明研究的局限性或未来工作方向。",
      "tags": [
        "Pathology Foundation Models",
        "Vision Foundation Models",
        "Histopathology Whole Slide Images",
        "Clinical Deployment",
        "Robustness and Efficiency"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:06.468023Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05144",
    "title": "Distilling the Thought, Watermarking the Answer: A Principle Semantic Guided Watermark for Large Reasoning Models",
    "authors": [
      "Shuliang Liu",
      "Xingyu Li",
      "Hongyi Liu",
      "Yibo Yan",
      "Bingchen Duan",
      "Qi Zheng",
      "Dong Fang",
      "Lingfeng Su",
      "Xuming Hu"
    ],
    "abstract": "Reasoning Large Language Models (RLLMs) excelling in complex tasks present unique challenges for digital watermarking, as existing methods often disrupt logical coherence or incur high computational costs. Token-based watermarking techniques can corrupt the reasoning flow by applying pseudo-random biases, while semantic-aware approaches improve quality but introduce significant latency or require auxiliary models. This paper introduces ReasonMark, a novel watermarking framework specifically designed for reasoning-intensive LLMs. Our approach decouples generation into an undisturbed Thinking Phase and a watermarked Answering Phase. We propose a Criticality Score to identify semantically pivotal tokens from the reasoning trace, which are distilled into a Principal Semantic Vector (PSV). The PSV then guides a semantically-adaptive mechanism that modulates watermark strength based on token-PSV alignment, ensuring robustness without compromising logical integrity. Extensive experiments show ReasonMark surpasses state-of-the-art methods by reducing text Perplexity by 0.35, increasing translation BLEU score by 0.164, and raising mathematical accuracy by 0.67 points. These advancements are achieved alongside a 0.34% higher watermark detection AUC and stronger robustness to attacks, all with a negligible increase in latency. This work enables the traceable and trustworthy deployment of reasoning LLMs in real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05144.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05144",
    "published": "2026-01-08T17:32:22Z",
    "updated": "2026-01-08T17:32:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ReasonMark，一个专为推理大型语言模型设计的语义引导水印框架，通过分离思维和回答阶段来确保逻辑完整性。",
      "motivation": "研究动机源于推理大型语言模型（RLLMs）在复杂任务中需要可追踪部署，但现有水印方法存在缺陷。Token-based水印应用伪随机偏置会破坏推理流程的连贯性，损害输出质量；semantic-aware方法虽提升质量但引入高延迟或依赖辅助模型，增加计算成本。这些问题限制了RLLMs在实际应用中的安全性和效率，亟需一种能平衡逻辑完整性和水印效果的低成本解决方案，以确保模型输出可信且易于追溯。",
      "method": "ReasonMark框架将生成过程分为两个阶段：不受扰动的思维阶段和加水印的回答阶段。通过提出Criticality Score从推理追踪中识别语义关键令牌，蒸馏成Principal Semantic Vector（PSV）。PSV指导语义自适应机制，根据令牌与PSV的对齐度动态调整水印强度，确保鲁棒性而不损害逻辑连贯性。该方案避免了额外模型需求，专注于核心语义引导，适用于推理密集型LLMs，创新点在于整合了关键语义分析和自适应调整技术。",
      "result": "实验显示ReasonMark超越现有先进方法，具体指标包括：文本生成中困惑度降低0.35，翻译任务BLEU得分提高0.164，数学推理准确性提升0.67点。同时，水印检测的AUC增加0.34%，表现出更强的抗攻击鲁棒性，延迟增加可忽略不计。这些结果凸显了其在保持推理质量的同时显著提升水印效果的优越性，与基线方法相比，在多个任务中均实现了性能改进。",
      "conclusion": "本研究的主要贡献是提出ReasonMark框架，为推理LLMs提供了一种兼顾逻辑连贯性和水印强度的解决方案，推动在实际应用中的可追踪、可信部署。学术价值在于创新性地整合语义分析和自适应机制，实践价值体现在低延迟和高鲁棒性，有助于防范模型滥用。未来工作可探索更多任务适应性或增强攻击防御机制，尽管摘要未明确说明局限性，但这一方向为后续研究提供了基础。",
      "tags": [
        "Large Language Model",
        "Watermarking",
        "Semantic Vector",
        "Reasoning Model",
        "Adaptive Mechanism"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:29.136495Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.07142",
    "title": "Why Does Stochastic Gradient Descent Slow Down in Low-Precision Training?",
    "authors": [
      "Vincent-Daniel Yun"
    ],
    "abstract": "Low-precision training has become crucial for reducing the computational and memory costs of large-scale deep learning. However, quantizing gradients introduces magnitude shrinkage, which can change how stochastic gradient descent (SGD) converges. In this study, we explore SGD convergence under a gradient shrinkage model, where each stochastic gradient is scaled by a factor \\( q_k \\in (0,1] \\). We show that this shrinkage affect the usual stepsize \\( μ_k \\) with an effective stepsize \\( μ_k q_k \\), slowing convergence when \\( q_{\\min} < 1 \\). With typical smoothness and bounded-variance assumptions, we prove that low-precision SGD still converges, but at a slower pace set by \\( q_{\\min} \\), and with a higher steady error level due to quantization effects. We analyze theoretically how lower numerical precision slows training by treating it as gradient shrinkage within the standard SGD convergence setup.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.07142.pdf",
    "abs_url": "https://arxiv.org/abs/2508.07142",
    "published": "2025-08-10T02:25:48Z",
    "updated": "2026-01-08T17:18:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过梯度收缩模型，理论解释了低精度训练中随机梯度下降收敛变慢的原因和量化效应。",
      "motivation": "本研究旨在解决低精度训练在深度学习中的核心问题：量化梯度引入幅度收缩，改变了随机梯度下降的收敛行为。低精度训练因能降低计算和内存成本而至关重要，但现有方法对其收敛变化的机理理解不足，特别是量化如何导致训练速度变慢和稳态误差增加。这促使研究深入探讨低精度对优化过程的影响，为改进大规模训练效率提供理论基础。",
      "method": "论文提出一个梯度收缩模型，将低精度训练中的量化效果建模为每个随机梯度乘以缩放因子 \\( q_k \\in (0,1] \\)。通过将此模型融入标准随机梯度下降的收敛框架，分析有效步长 \\( \\mu_k q_k \\) 下的行为。关键创新点在于将量化问题转化为梯度收缩的数学分析，基于平滑性和有界方差的假设，理论推导收敛性质，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "研究结果表明，在梯度收缩模型下，当最小缩放因子 \\( q_{\\min} < 1 \\) 时，低精度随机梯度下降的收敛速度变慢，由 \\( q_{\\min} \\) 决定。同时，量化效应导致稳态误差水平提高，具体表现为收敛速率的理论下降。这些发现基于理论证明，对比基线标准SGD，低精度版本在收敛速度和误差上表现较差，但摘要未提供实验数据或具体性能指标。",
      "conclusion": "论文的主要贡献是揭示了低精度训练中量化导致梯度收缩，从而减慢随机梯度下降收敛的机理，填补了理论分析的空白。学术价值在于扩展了优化算法的收敛理论，应用价值则在于指导实际深度学习训练中的低精度优化策略。局限性在于摘要未明确说明未来工作方向或潜在实验验证需求，但为后续研究提供了基础框架。",
      "tags": [
        "Stochastic Gradient Descent",
        "Low-Precision Training",
        "Gradient Quantization",
        "Convergence Analysis",
        "Numerical Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:32:05.614581Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05125",
    "title": "VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding",
    "authors": [
      "Ignacio de Rodrigo",
      "Alvaro J. Lopez-Lopez",
      "Jaime Boal"
    ],
    "abstract": "This work introduces VERSE, a methodology for analyzing and improving Vision-Language Models applied to Visually-rich Document Understanding by exploring their visual embedding space. VERSE enables the visualization of latent representations, supporting the assessment of model feasibility. It also facilitates the identification of problematic regions and guides the generation of synthetic data to enhance performance in those clusters. We validate the methodology by training on the synthetic MERIT Dataset and evaluating on its real-world counterpart, MERIT Secret. Results show that VERSE helps uncover the visual features associated with error-prone clusters, and that retraining with samples containing these features substantially boosts F1 performance without degrading generalization. Furthermore, we demonstrate that on-premise models such as Donut and Idefics2, when optimized with VERSE, match or even surpass the performance of SaaS solutions like GPT-4 and Pixtral.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05125.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05125",
    "published": "2026-01-08T17:15:15Z",
    "updated": "2026-01-08T17:15:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "VERSE 是一种通过可视化视觉嵌入空间和聚类引导合成数据生成，显著提升视觉-语言模型在视觉丰富文档理解中性能的方法。",
      "motivation": "随着数字化文档处理的普及，视觉丰富文档理解成为关键任务，但现有视觉-语言模型在处理复杂视觉特征时性能不足，导致错误识别和泛化能力受限。VERSE 旨在通过探索视觉嵌入空间，识别问题区域并生成针对性训练数据，以弥补现有方法在模型可行性和数据效率方面的不足，提升整体性能。",
      "method": "VERSE 方法首先通过降低维度和可视化技术探索视觉-语言模型的嵌入空间，利用聚类分析识别错误倾向的区域。基于聚类洞察，指导生成合成训练数据，特别针对问题视觉特征进行增强。研究中使用合成 MERIT 数据集进行训练，并在真实数据集 MERIT Secret 上评估，模型包括 Donut 和 Idefics2，以优化嵌入空间分析和数据生成过程。",
      "result": "实验结果显示，VERSE 能够有效识别与错误倾向聚类相关的视觉特征。用包含这些特征的样本重新训练模型后，F1 分数显著提升，且未损害泛化能力。对比基线，经过 VERSE 优化的本地模型如 Donut 和 Idefics2 在性能上匹配甚至超过了云服务解决方案如 GPT-4 和 Pixtral，证明了方法在提升准确性和替代云服务方面的有效性。",
      "conclusion": "VERSE 提供了一种系统化分析和改进视觉-语言模型的方法，通过嵌入空间探索和聚类引导的数据增强，显著提升了视觉丰富文档理解任务的性能。该研究具有学术价值，为模型解释性和数据增强提供了新思路，并展示了本地模型优化以替代云服务的潜力。未来工作可扩展应用到其他视觉-语言任务，并优化合成数据生成技术。",
      "tags": [
        "Vision-Language Models",
        "Clustering",
        "Visual Embedding",
        "Synthetic Data Generation",
        "Visually-Rich Document Understanding"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:46.588105Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.18526",
    "title": "SciClaims: An End-to-End Generative System for Biomedical Claim Analysis",
    "authors": [
      "Raúl Ortega",
      "José Manuel Gómez-Pérez"
    ],
    "abstract": "We present SciClaims, an interactive web-based system for end-to-end scientific claim analysis in the biomedical domain. Designed for high-stakes use cases such as systematic literature reviews and patent validation, SciClaims extracts claims from text, retrieves relevant evidence from PubMed, and verifies their veracity. The system features a user-friendly interface where users can input scientific text and view extracted claims, predictions, supporting or refuting evidence, and justifications in natural language. Unlike prior approaches, SciClaims seamlessly integrates the entire scientific claim analysis process using a single large language model, without requiring additional fine-tuning. SciClaims is optimized to run efficiently on a single GPU and is publicly available for live interaction.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2503.18526.pdf",
    "abs_url": "https://arxiv.org/abs/2503.18526",
    "published": "2025-03-24T10:31:31Z",
    "updated": "2026-01-08T17:04:29Z",
    "comment": "In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    "light_analysis": {
      "overview": "论文提出了SciClaims系统，一个基于单一大型语言模型的端到端生成系统，用于生物医学声明分析，无需额外微调。",
      "motivation": "研究动机在于解决生物医学领域高风险应用（如系统文献综述和专利验证）中自动化科学声明分析的迫切需求。现有方法通常依赖多个独立模型或需要大量微调，导致效率低下和集成困难，难以应对大规模文本处理和声明验证。SciClaims旨在通过无缝整合整个过程，简化分析流程，提高准确性和可访问性。",
      "method": "SciClaims是一个交互式基于Web的系统，采用单一大型语言模型（LLM）实现端到端科学声明分析。核心方法包括从输入文本提取声明、从PubMed数据库检索相关证据，以及利用LLM验证声明的真实性。关键创新在于无需额外微调，优化模型以在单个GPU上高效运行，并提供用户友好界面，展示提取的声明、预测、支持或反驳证据及自然语言解释。",
      "result": "摘要未明确说明具体实验结果，但系统声称能够高效运行在单个GPU上，并公开可用以支持实时交互。与基线方法相比，SciClaims通过单一LLM集成分析过程，避免了额外微调的需要，可能在处理效率和用户友好性方面有所改进。然而，具体性能指标如准确率提升或效率数据未在摘要中提供。",
      "conclusion": "论文的主要贡献是开发了SciClaims系统，展示单一LLM在生物医学声明分析中的潜力，简化了端到端流程。学术价值在于探索LLM在专业领域的生成和集成能力，实际应用价值体现在高风险场景如文献综述和专利验证中提高分析效率。潜在局限性包括性能评估不足，未来工作可扩展至其他领域或进行更全面的实验验证。",
      "tags": [
        "Large Language Model",
        "Biomedical Claim Analysis",
        "End-to-End System",
        "Natural Language Generation",
        "Web-based System"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:05.267374Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05114",
    "title": "Evaluative Fingerprints: Stable and Systematic Differences in LLM Evaluator Behavior",
    "authors": [
      "Wajid Nasser"
    ],
    "abstract": "LLM-as-judge systems promise scalable, consistent evaluation. We find the opposite: judges are consistent, but not with each other; they are consistent with themselves. Across 3,240 evaluations (9 judges x 120 unique video x pack items x 3 independent runs), inter-judge agreement is near-zero (Krippendorff's α = 0.042). On two dimensions, judges disagree more than random noise would predict (α < 0). Yet this disagreement isn't chaos; it's structured. A classifier identifies which judge produced an evaluation with 77.1% accuracy from rubric scores alone, rising to 89.9% with disposition features. Within model families, the signal is even stronger: GPT-4.1 and GPT-5.2 are distinguishable with 99.6% accuracy. We call this the reliability paradox: judges cannot agree on what constitutes quality, yet their disagreement patterns are so stable they function as fingerprints. Each judge implements a distinct, stable theory of quality: an \"evaluative disposition\" that shapes how it interprets any rubric. We characterize these dispositions along multiple axes: harshness/leniency, dimension emphasis, within-judge stability (ICC), and evidence behavior (receipt validity, semantic linkage via NLI, and shotgun index). The implication is stark: LLM judges are not interchangeable instruments measuring a shared construct. They are distinct measurement devices, each encoding its own implicit theory of quality. Averaging their scores produces a synthetic verdict that corresponds to no judge's actual values.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05114.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05114",
    "published": "2026-01-08T17:02:22Z",
    "updated": "2026-01-08T17:02:22Z",
    "comment": "23 pages, 6 figures, code and artifacts at : https://github.com/wajid-nasser/evaluative-fingerprints",
    "light_analysis": {
      "overview": "论文揭示了LLM评估者之间存在稳定和系统的差异，形成独特的‘评估指纹’，挑战了其作为可互换评估工具的有效性。",
      "motivation": "LLM-as-judge系统被广泛用于自动化评估，承诺提供高效、一致的评分。然而，本研究指出现有方法假设LLM评估者能达成客观共识，实际却发现评估者之间对质量标准的看法存在严重分歧。这种不一致性结构化而非随机，导致评估不可靠，突显了解决评估者偏差的重要性，以提升AI评估的准确性和公平性。",
      "method": "论文通过大规模实验，收集了9个LLM评估者对120个视频项进行的3,240次评价数据。使用Krippendorff's α统计量测量评估者间一致性，并训练分类器从评分中识别特定评估者。引入‘评估性倾向’概念，通过特征如严厉度、维度强调、稳定性和证据行为（如NLI语义链接）量化评估者行为差异。创新点在于系统分析LLM评估者的稳定模式，而非假设其一致性。",
      "result": "实验结果显示，评估者间一致性极低（Krippendorff's α=0.042），在一些维度上低于随机噪声（α<0）。分类器仅从评分识别评估者的准确率达77.1%，加入倾向特征后提升至89.9%。在模型家族内，GPT-4.1和GPT-5.2的区分度高达99.6%。这形成‘可靠性悖论’，即评估者虽无法就质量达成共识，但差异模式稳定如指纹，表明不一致是系统性的而非混乱。",
      "conclusion": "论文结论强调，LLM评估者并非测量共享质量标准的可互换工具，而是各自编码独特隐含理论的设备。平均评分会掩盖个体差异，产生无实际意义的合成裁决。这挑战了LLM-as-judge系统的可靠性，具有学术价值在于揭示评估偏差，实际应用需谨慎处理评估结果。未来工作可探索更稳健的评估方法或校准技术，以弥补局限性。",
      "tags": [
        "LLM Evaluation",
        "Inter-rater Reliability",
        "Classification",
        "Evaluative Disposition",
        "GPT models"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:34.020536Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.11939",
    "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
    "authors": [
      "Yichen Xu",
      "Liangyu Chen",
      "Liang Zhang",
      "Jianzhe Ma",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "abstract": "Charts are a universally adopted medium for data communication, yet existing chart understanding benchmarks are overwhelmingly English-centric, limiting their accessibility and relevance to global audiences. To address this limitation, we introduce PolyChartQA, the first large-scale multilingual benchmark for chart question answering, comprising 22,606 charts and 26,151 QA pairs across 10 diverse languages. PolyChartQA is constructed through a scalable pipeline that enables efficient multilingual chart generation via data translation and code reuse, supported by LLM-based translation and rigorous quality control. We systematically evaluate multilingual chart understanding with PolyChartQA on state-of-the-art LVLMs and reveal a significant performance gap between English and other languages, particularly low-resource ones. Additionally, we introduce a companion multilingual chart question answering training set, PolyChartQA-Train, on which fine-tuning LVLMs yields substantial gains in multilingual chart understanding across diverse model sizes and architectures. Together, our benchmark provides a foundation for developing globally inclusive vision-language models capable of understanding charts across diverse linguistic contexts.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2507.11939.pdf",
    "abs_url": "https://arxiv.org/abs/2507.11939",
    "published": "2025-07-16T06:09:02Z",
    "updated": "2026-01-08T17:00:25Z",
    "comment": "Work in Progress",
    "light_analysis": {
      "overview": "该论文提出了PolyChartQA，首个大规模多语言图表问答基准，用于评估和改进大型视觉语言模型的多语言图表理解性能。",
      "motivation": "图表是数据沟通的通用媒介，但现有图表理解基准主要集中在英语上，导致其在全球范围内的可访问性和相关性受限。这限制了视觉语言模型在非英语语言（特别是低资源语言）上的评估和发展，使得跨语言数据沟通能力不足。因此，迫切需要建立多语言基准，以克服英语中心化的不足，推动模型在全球应用中的公平性和有效性。",
      "method": "本文介绍了PolyChartQA基准，包含22,606张图表和26,151个问答对，覆盖10种语言。核心方法是构建一个可扩展的流水线，通过数据翻译和代码重用来高效生成多语言图表内容。关键创新包括利用基于大型语言模型（LLM）的翻译技术和严格的质量控制机制，确保数据的准确性和一致性。此外，还创建了配套训练集PolyChartQA-Train，用于微调大型视觉语言模型（LVLMs）以增强多语言理解能力。",
      "result": "使用PolyChartQA对最先进的大型视觉语言模型（LVLMs）进行评估，结果显示英语与其他语言之间存在显著的性能差距，特别是在低资源语言上，模型的表现明显落后。通过在PolyChartQA-Train训练集上进行微调，LVLMs在多语言图表理解上取得了实质性提升，这种改进在不同模型大小和架构上均有效，表明微调有助于弥合跨语言性能差距。",
      "conclusion": "PolyChartQA基准的建立填补了多语言图表理解评估的空白，为开发全球包容性视觉语言模型奠定了基础。其学术价值在于促进跨语言视觉语言理解研究，实际应用价值在于支持跨文化数据沟通和理解。未来工作可扩展到更多语言或优化翻译方法，以进一步推动模型的普适性和性能提升。",
      "tags": [
        "Multilingual Chart Question Answering",
        "Large Vision-Language Models",
        "Benchmark Creation",
        "Data Translation",
        "LLM-based Translation"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:47.584733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.23923",
    "title": "Act-Adaptive Margin: Dynamically Calibrating Reward Models for Subjective Ambiguity",
    "authors": [
      "Feiteng Fang",
      "Dingwei Chen",
      "Xiang Huang",
      "Ting-En Lin",
      "Yuchuan Wu",
      "Xiong Liu",
      "Xinge Ye",
      "Ziqiang Liu",
      "Haonan Zhang",
      "Liang Zhu",
      "Hamid Alinejad-Rokny",
      "Min Yang",
      "Yongbin Li"
    ],
    "abstract": "Currently, most reinforcement learning tasks focus on domains like mathematics and programming, where verification is relatively straightforward. However, in subjective tasks such as role-playing, alignment techniques struggle to make progress, primarily because subjective reward modeling using the Bradley-Terry model faces significant challenges when dealing with ambiguous preferences. To improve reward modeling in subjective tasks, this paper proposes AAM (\\textbf{\\underline{A}}ct-\\textbf{\\underline{A}}daptive \\textbf{\\underline{M}}argin), which enhances reward modeling by dynamically calibrating preference margins using the model's internal parameter knowledge. We design two versions of AAM that efficiently generate contextually-appropriate preference gaps without additional human annotation. This approach fundamentally improves how reward models handle subjective rewards by better integrating generative understanding with preference scoring. To validate AAM's effectiveness in subjective reward modeling, we conduct evaluations on RewardBench, JudgeBench, and challenging role-playing tasks. Results show that AAM significantly improves subjective reward modeling performance, enhancing Bradley-Terry reward models by 2.95\\% in general tasks and 4.85\\% in subjective role-playing tasks. Furthermore, reward models trained with AAM can help downstream alignment tasks achieve better results. Our test results show that applying rewards generated by AAM-Augmented RM to preference learning techniques (e.g., GRPO) achieves state-of-the-art results on CharacterEval and Charm. Code and dataset are available at https://github.com/calubkk/AAM.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.23923.pdf",
    "abs_url": "https://arxiv.org/abs/2505.23923",
    "published": "2025-05-29T18:15:18Z",
    "updated": "2026-01-08T16:58:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出AAM方法，通过动态校准偏好边界改进主观任务中的奖励建模，无需额外人工标注。",
      "motivation": "研究动机在于解决主观任务中奖励建模的挑战。当前强化学习任务多集中于数学和编程等易于验证领域，但在主观任务如角色扮演中，对齐技术进展缓慢。主要原因是基于Bradley-Terry模型的主观奖励建模在处理模糊偏好时存在显著缺陷，无法准确捕捉用户意图，导致奖励信号不明确，影响了强化学习在主观领域的应用。这一问题重要，因为主观任务广泛存在，现有方法不足，限制了智能系统的适应性和性能。",
      "method": "AAM方法通过动态校准偏好边界来增强奖励建模。核心是利用模型内部参数知识，设计两个版本的AAM，高效生成上下文相关的偏好差距，无需额外人类标注。该方法整合生成理解和偏好评分，改进对主观奖励的处理。具体地，基于Bradley-Terry模型，AAM自适应调整偏好边界，通过内部参数动态校准，从而提升在主观模糊性下的建模能力。在实现中，无需复杂的外部标注，简化了流程。",
      "result": "实验在RewardBench、JudgeBench和挑战性角色扮演任务上进行评估。结果显示，AAM显著提升主观奖励建模性能，将Bradley-Terry奖励模型在一般任务中提高了2.95%，在主观角色扮演任务中提高了4.85%。对比基线方法，AAM表现出更好的适应性。此外，将AAM增强的奖励模型应用于下游对齐任务，如结合GRPO等技术，在CharacterEval和Charm数据集上取得了最先进的性能，验证了其实际应用效果。",
      "conclusion": "本研究的主要贡献是提出AAM，通过动态校准偏好边界，解决了主观任务中奖励建模的挑战，改进了Bradley-Terry模型的不足。该研究具有学术价值，为强化学习在主观领域的对齐提供了新方法，推动了奖励建模技术的发展。实际应用中，AAM能提升下游对齐任务的性能，增强智能系统在复杂场景中的适应性。摘要未明确说明局限性，未来工作可能包括扩展到更多主观任务或整合其他先进模型。",
      "tags": [
        "Act-Adaptive Margin",
        "Reward Modeling",
        "Bradley-Terry Model",
        "Preference Learning",
        "Subjective Tasks"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:01.805941Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05111",
    "title": "Agent-as-a-Judge",
    "authors": [
      "Runyang You",
      "Hongru Cai",
      "Caiqi Zhang",
      "Qiancheng Xu",
      "Meng Liu",
      "Tiezheng Yu",
      "Yongqi Li",
      "Wenjie Li"
    ],
    "abstract": "LLM-as-a-Judge has revolutionized AI evaluation by leveraging large language models for scalable assessments. However, as evaluands become increasingly complex, specialized, and multi-step, the reliability of LLM-as-a-Judge has become constrained by inherent biases, shallow single-pass reasoning, and the inability to verify assessments against real-world observations. This has catalyzed the transition to Agent-as-a-Judge, where agentic judges employ planning, tool-augmented verification, multi-agent collaboration, and persistent memory to enable more robust, verifiable, and nuanced evaluations. Despite the rapid proliferation of agentic evaluation systems, the field lacks a unified framework to navigate this shifting landscape. To bridge this gap, we present the first comprehensive survey tracing this evolution. Specifically, we identify key dimensions that characterize this paradigm shift and establish a developmental taxonomy. We organize core methodologies and survey applications across general and professional domains. Furthermore, we analyze frontier challenges and identify promising research directions, ultimately providing a clear roadmap for the next generation of agentic evaluation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05111.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05111",
    "published": "2026-01-08T16:58:10Z",
    "updated": "2026-01-08T16:58:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文首次全面调查了Agent-as-a-Judge的演进，建立了发展分类法，并提供领域路线图。",
      "motivation": "LLM-as-a-Judge在AI评估中带来了革命性变革，但随着评估任务变得愈发复杂、专业化和多步骤，其可靠性受到偏见、单次推理浅层化以及无法结合现实观察进行验证的限制。这促使了向Agent-as-a-Judge的转变，以利用代理智能实现更健壮的评估。然而，代理评估系统迅速涌现却缺乏统一框架，因此需要系统性调查来填补这一空白。",
      "method": "论文采用综合性调查方法，通过识别Agent-as-a-Judge范式转变的关键维度来建立发展分类法。具体包括组织核心方法论，如代理法官的规划、工具增强验证、多代理协作和持久记忆等技术。此外，调查了在通用和专业领域的应用实例，并分析了前沿挑战和未来研究方向。",
      "result": "作为一篇综述论文，论文的结果包括首次建立了Agent-as-a-Judge的发展分类法，系统组织了核心方法和应用案例。通过调查，分析了该领域的前沿挑战并识别了有前途的研究方向，最终为下一代代理评估提供了清晰的路线图。摘要未明确说明具体的性能指标或实验数据。",
      "conclusion": "论文的主要贡献是提供了关于Agent-as-a-Judge的首次全面调查，建立了统一的框架和发展分类法，系统化地总结了核心方法和应用。其学术价值在于填补了代理评估领域缺乏系统性指导的空白，为研究人员提供了清晰的路线图。实际应用上，有助于推动更健壮、可验证的AI评估系统的发展。未来工作方向包括应对挑战如偏见消除和验证机制优化。",
      "tags": [
        "Agent-as-a-Judge",
        "Planning",
        "Tool-augmented Verification",
        "Multi-agent Collaboration",
        "Persistent Memory"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:37.493153Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05110",
    "title": "GlimpRouter: Efficient Collaborative Inference by Glimpsing One Token of Thoughts",
    "authors": [
      "Wenhao Zeng",
      "Xuteng Zhang",
      "Yuling Shi",
      "Chao Hu",
      "Yuting Chen",
      "Beijun Shen",
      "Xiaodong Gu"
    ],
    "abstract": "Large Reasoning Models (LRMs) achieve remarkable performance by explicitly generating multi-step chains of thought, but this capability incurs substantial inference latency and computational cost. Collaborative inference offers a promising solution by selectively allocating work between lightweight and large models, yet a fundamental challenge remains: determining when a reasoning step requires the capacity of a large model or the efficiency of a small model. Existing routing strategies either rely on local token probabilities or post-hoc verification, introducing significant inference overhead. In this work, we propose a novel perspective on step-wise collaboration: the difficulty of a reasoning step can be inferred from its very first token. Inspired by the \"Aha Moment\" phenomenon in LRMs, we show that the entropy of the initial token serves as a strong predictor of step difficulty. Building on this insight, we introduce GlimpRouter, a training-free step-wise collaboration framework. GlimpRouter employs a lightweight model to generate only the first token of each reasoning step and routes the step to a larger model only when the initial token entropy exceeds a threshold. Experiments on multiple benchmarks demonstrate that our approach significantly reduces inference latency while preserving accuracy. For instance, GlimpRouter attains a substantial 10.7% improvement in accuracy while reducing inference latency by 25.9% compared to a standalone large model on AIME25. These results suggest a simple yet effective mechanism for reasoning: allocating computation based on a glimpse of thought rather than full-step evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05110.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05110",
    "published": "2026-01-08T16:58:07Z",
    "updated": "2026-01-08T16:58:07Z",
    "comment": "Code available at https://github.com/Zengwh02/GlimpRouter",
    "light_analysis": {
      "overview": "论文提出GlimpRouter框架，通过分析推理步骤初始标记的熵来预测难度，实现无需训练的协作推理，以降低延迟并保持准确性。",
      "motivation": "大型推理模型通过生成多步思维链实现高性能，但带来高推理延迟和计算成本。协作推理在轻量和大模型间分配工作是解决方案，但现有方法难以高效决定何时使用哪个模型。现有路由策略依赖局部标记概率或事后验证，引入显著推理开销。因此，需要更高效的方法来预测推理步骤难度，以减少延迟并优化计算资源分配。",
      "method": "论文提出GlimpRouter框架，核心创新在于利用推理步骤的第一个标记的熵作为难度预测器，基于'aha Moment'现象启发。具体地，轻量模型生成每个步骤的首个标记，计算其熵值；当熵超过预定义阈值时，将步骤路由到大模型处理，否则由轻量模型完成。该方法无需训练，直接基于熵阈值决策，避免全步骤评估的开销，实现分步协作推理。",
      "result": "实验在多个基准测试中验证了GlimpRouter的有效性，结果显示在保持准确性的同时显著降低推理延迟。例如，在AIME25上，相比独立大模型，GlimpRouter提升了10.7%的精度，并减少25.9%的延迟。这些性能改进表明该框架优于现有路由策略，在协作推理中实现了高效性和准确性。",
      "conclusion": "GlimpRouter的主要贡献是提出一种基于思维一瞥的推理步骤难度预测机制，实现高效协作推理。该研究在学术上为AI推理优化提供了新视角，实际应用中能降低计算成本并提高响应速度。未来工作可扩展到更多任务或优化熵阈值策略，摘要未明确说明具体局限性。",
      "tags": [
        "Large Reasoning Models",
        "Collaborative Inference",
        "Entropy Prediction",
        "Token Routing",
        "Efficient Inference"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:25.884931Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.00949",
    "title": "Multi-Modal AI for Remote Patient Monitoring in Cancer Care",
    "authors": [
      "Yansong Liu",
      "Ronnie Stafford",
      "Pramit Khetrapal",
      "Huriye Kocadag",
      "Graça Carvalho",
      "Patricia de Winter",
      "Maryam Imran",
      "Amelia Snook",
      "Adamos Hadjivasiliou",
      "D. Vijay Anand",
      "Weining Lin",
      "John Kelly",
      "Yukun Zhou",
      "Ivana Drobnjak"
    ],
    "abstract": "For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop. Best Paper Poster Award.)",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.00949.pdf",
    "abs_url": "https://arxiv.org/abs/2512.00949",
    "published": "2025-11-30T16:01:50Z",
    "updated": "2026-01-08T16:55:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文开发了一个多模态AI框架，用于远程监控癌症治疗患者，预测未来不良事件的连续风险，并在大型观察试验中验证了其可行性，实现了83.9%的准确性。",
      "motivation": "对于接受系统性癌症治疗的患者，诊所就诊间隔期存在未监控副作用的高风险，可能导致患者安全问题和护理缺口。现有远程监控方法往往无法有效处理真实世界数据的异步性和不完整性，难以提供连续的风险预测。因此，本研究旨在开发一个多模态AI系统，整合多样化的患者数据，以填补癌症护理中的监控空白，实现更主动和个性化的患者支持，从而提高医疗质量和安全性。",
      "method": "本研究提出一个多模态AI模型，专门设计用于处理远程患者监控中产生的异步和不完整数据。模型整合来自HALO-X平台的多模态数据源，包括人口统计信息、可穿戴传感器数据、每日患者调查和临床事件记录，通过一个大型观察试验（涉及84名患者，收集了210万数据点，6,080患者天）进行训练和验证。关键创新点在于适应真实世界数据的特性，利用多模态表示学习技术来融合异构数据，生成未来不良事件的连续风险预测，而不依赖固定的时间序列结构。",
      "result": "该多模态AI模型在预测不良事件风险时达到了83.9%的准确性（AUROC为0.70）。模型识别出关键预测特征，包括先前治疗、日常健康检查记录和每日最大心率。一个案例研究进一步展示了模型能够输出逐渐升级的风险曲线，在不良事件发生前提供早期警告。摘要未明确说明与基线方法的对比情况，但结果证实了该模型在远程监控环境中的有效性和可靠性，为临床决策支持提供了实用工具。",
      "conclusion": "本研究成功确立了多模态AI远程患者监控在癌症护理中的可行性，为更主动和连续的患者支持开辟了新路径。学术上，它推动了多模态表示学习在医疗健康领域的应用；实际上，该系统有望通过早期风险预警改善患者安全、减少医疗并发症，并为个性化护理方案提供数据支持。摘要未明确说明研究局限性，但未来工作可能包括扩大临床试验范围、优化模型性能以提高预测精度，以及探索在其他疾病类型或医疗场景中的扩展应用。",
      "tags": [
        "Multi-Modal AI",
        "Remote Patient Monitoring",
        "Representation Learning",
        "Wearable Sensors",
        "Predictive Modeling"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:48.540089Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05107",
    "title": "Controllable Memory Usage: Balancing Anchoring and Innovation in Long-Term Human-Agent Interaction",
    "authors": [
      "Muzhao Tian",
      "Zisu Huang",
      "Xiaohua Wang",
      "Jingwen Xu",
      "Zhengkang Guo",
      "Qi Qian",
      "Yuanzhe Shen",
      "Kaitao Song",
      "Jiakang Yuan",
      "Changze Lv",
      "Xiaoqing Zheng"
    ],
    "abstract": "As LLM-based agents are increasingly used in long-term interactions, cumulative memory is critical for enabling personalization and maintaining stylistic consistency. However, most existing systems adopt an ``all-or-nothing'' approach to memory usage: incorporating all relevant past information can lead to \\textit{Memory Anchoring}, where the agent is trapped by past interactions, while excluding memory entirely results in under-utilization and the loss of important interaction history. We show that an agent's reliance on memory can be modeled as an explicit and user-controllable dimension. We first introduce a behavioral metric of memory dependence to quantify the influence of past interactions on current outputs. We then propose \\textbf{Stee}rable \\textbf{M}emory Agent, \\texttt{SteeM}, a framework that allows users to dynamically regulate memory reliance, ranging from a fresh-start mode that promotes innovation to a high-fidelity mode that closely follows interaction history. Experiments across different scenarios demonstrate that our approach consistently outperforms conventional prompting and rigid memory masking strategies, yielding a more nuanced and effective control for personalized human-agent collaboration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05107.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05107",
    "published": "2026-01-08T16:54:30Z",
    "updated": "2026-01-08T16:54:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出SteeM框架，一个可调控记忆代理，允许用户动态控制记忆依赖，以平衡长期人机交互中的锚定和创新。",
      "motivation": "研究动机源于LLM代理在长期交互中，累积记忆对于个性化和一致性至关重要。现有系统采用“全有或全无”的记忆使用方式，导致记忆锚定（代理受过去交互束缚）或记忆利用不足（丢失重要历史），这限制了代理的适应性和用户体验，需要更精细的控制机制来解决这一关键问题。",
      "method": "论文提出SteeM框架，首先引入记忆依赖行为度量来量化过去交互对当前输出的影响。核心创新是允许用户动态调节记忆依赖程度，范围从促进创新的“新鲜开始”模式到紧密跟随历史的“高保真”模式，将记忆依赖建模为用户可控制的维度，实现灵活调控。摘要未明确说明具体数据集或模型架构。",
      "result": "实验结果表明，SteeM框架在不同场景下一致优于传统提示方法和刚性记忆掩码策略。摘要未明确说明具体性能指标，但表明该方法提供了更细致和有效的控制，提升了人机协作的个性化效果，避免了现有方法的僵化问题。",
      "conclusion": "该研究的主要贡献是提出可调控记忆依赖的框架，解决了长期交互中记忆管理的难题。学术上为记忆建模提供了新方法，实际应用能改善代理的适应性和个性化协作。未来工作可能包括优化调控机制或扩展到更多交互场景。",
      "tags": [
        "Large Language Model",
        "Memory Management",
        "Human-Agent Interaction",
        "Steerable Framework",
        "Personalization"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:28.784607Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05106",
    "title": "Token-Level LLM Collaboration via FusionRoute",
    "authors": [
      "Nuoya Xiong",
      "Yuhang Zhou",
      "Hanqing Zeng",
      "Zhaorun Chen",
      "Furong Huang",
      "Shuchao Bi",
      "Lizhu Zhang",
      "Zhuokai Zhao"
    ],
    "abstract": "Large language models (LLMs) exhibit strengths across diverse domains. However, achieving strong performance across these domains with a single general-purpose model typically requires scaling to sizes that are prohibitively expensive to train and deploy. On the other hand, while smaller domain-specialized models are much more efficient, they struggle to generalize beyond their training distributions. To address this dilemma, we propose FusionRoute, a robust and effective token-level multi-LLM collaboration framework in which a lightweight router simultaneously (i) selects the most suitable expert at each decoding step and (ii) contributes a complementary logit that refines or corrects the selected expert's next-token distribution via logit addition. Unlike existing token-level collaboration methods that rely solely on fixed expert outputs, we provide a theoretical analysis showing that pure expert-only routing is fundamentally limited: unless strong global coverage assumptions hold, it cannot in general realize the optimal decoding policy. By augmenting expert selection with a trainable complementary generator, FusionRoute expands the effective policy class and enables recovery of optimal value functions under mild conditions. Empirically, across both Llama-3 and Gemma-2 families and diverse benchmarks spanning mathematical reasoning, code generation, and instruction following, FusionRoute outperforms both sequence- and token-level collaboration, model merging, and direct fine-tuning, while remaining competitive with domain experts on their respective tasks.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05106.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05106",
    "published": "2026-01-08T16:53:16Z",
    "updated": "2026-01-08T16:53:16Z",
    "comment": "25 pages",
    "light_analysis": {
      "overview": "本文提出了 FusionRoute，一种通过 token 级路由和互补 logit 加法实现多 LLM 协作的框架，以平衡泛化性能和部署效率。",
      "motivation": "大型语言模型（LLMs）在不同领域表现优异，但单一通用模型需极大规模扩展，导致训练和部署成本高昂；小型领域专用模型虽高效，却难以泛化到训练分布之外。现有 token 级协作方法仅依赖固定专家输出，理论分析表明纯专家路由存在根本性限制，无法实现最优解码策略，除非强全局覆盖假设成立。因此，本研究旨在开发一个健壮的协作框架，解决这一泛化与效率的权衡问题。",
      "method": "FusionRoute 是一个 token 级多 LLM 协作框架，核心是轻量级路由器。该路由器在每个解码步骤同时执行两个任务：选择最合适的专家模型，并生成一个互补 logit。通过 logit 加法，互补 logit 用于细化或修正所选专家的下一个 token 分布。关键创新在于结合专家选择和可训练的互补生成器，扩展了有效策略类，理论分析显示能在温和条件下恢复最优值函数。论文使用了 Llama-3 和 Gemma-2 模型家族，并在数学推理、代码生成和指令遵循等多样化基准测试上评估。",
      "result": "实证结果显示，在 Llama-3 和 Gemma-2 模型家族上，FusionRoute 在数学推理、代码生成和指令遵循等多个基准测试中，性能优于序列级和 token 级协作方法、模型合并以及直接微调。同时，它在各自任务上与领域专家模型表现竞争，表明在保持效率的同时显著提升了跨领域性能。具体性能指标如准确率提升在摘要中未明确说明，但整体验证了方法的有效性。",
      "conclusion": "本论文的主要贡献是提出了 FusionRoute 框架，通过 token 级路由和互补 logit 加法，实现了多 LLM 的高效协作，克服了纯专家路由的限制。理论分析支持了方法的可行性，实证验证了其在多个领域的优越性，具有重要的学术价值，如推动模型协作理论发展，以及实际应用潜力，如在资源受限环境下部署高性能 LLM 系统。未来工作可能涉及优化路由器架构或扩展更多应用场景，摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Token-Level Collaboration",
        "Expert Routing",
        "Logit Addition"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:08.484467Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.16282",
    "title": "CALM: A CKA-Guided Adaptive Layer-Wise Modularization Framework for LLM Quantization",
    "authors": [
      "Jinhao Zhang",
      "Yunquan Zhang",
      "Daning Chen",
      "JunSun",
      "Zicheng Yan"
    ],
    "abstract": "Current mainstream post-training quantization methods for large language models typically apply a uniform quantization strategy across all network layers, overlooking the substantial differences in algorithmic suitability among layers. To address this limitation, we propose CALM (A CKA-guided Adaptive Layer-wise Modularization)a fine-tuning-free, plug-and-play framework for algorithmic heterogeneous quantization. CALM independently evaluates multiple PTQ algorithms on each layer and employs Linear Centered Kernel Alignment (CKA) as a metric to automatically select the optimal quantization strategy per layer. The individually optimized strategies are then integrated to construct a hybrid quantized model. Experiments demonstrate that our approach consistently outperforms both uniform quantization baselines and state-of-the-art mixed-precision methods across mainstream LLMsincluding LLaMA and Qwenin terms of perplexity (PPL) and downstream task performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.16282.pdf",
    "abs_url": "https://arxiv.org/abs/2512.16282",
    "published": "2025-12-18T08:01:19Z",
    "updated": "2026-01-08T16:51:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "CALM 是一个基于 CKA 引导的自适应层级量化框架，通过层级的算法选择提升 LLM 量化效果，实现了无需微调的混合量化优化。",
      "motivation": "当前主流的大语言模型后训练量化方法通常采用统一的量化策略，忽略了网络层在算法适用性上的显著差异，这可能导致量化后模型性能下降。该问题的重要性在于，层间差异可能影响量化效率和准确性，而现有方法未能有效适应这些差异，因此需要一种自适应策略来优化量化过程。",
      "method": "CALM 框架核心方法是独立在每个网络层评估多个 PTQ 算法，并使用线性中心核对齐（CKA）作为度量来自动选择每层的最优量化策略。这些策略被集成以构建混合量化模型，关键创新在于无需微调、即插即用的设计，以及基于 CKA 的自适应算法选择，从而实现了算法异质量化。",
      "result": "实验表明，CALM 在主流大语言模型如 LLaMA 和 Qwen 上，在困惑度和下游任务性能方面持续优于统一的量化基线和最先进的混合精度方法。尽管摘要未提供具体百分比数据，但强调了该方法在量化效果上的优越性和一致性。",
      "conclusion": "CALM 的主要贡献是提出了一个自适应层级量化框架，通过优化算法选择提高了 LLM 量化效果，为后训练量化研究提供了新方法。学术价值在于探索了层间差异对量化的影响，实际应用可能包括资源受限环境中的高效模型部署。摘要未明确说明局限性，未来工作可能涉及扩展到更多模型或量化场景。",
      "tags": [
        "Large Language Model Quantization",
        "Post-Training Quantization",
        "Linear Centered Kernel Alignment",
        "Adaptive Layer-wise Modularization",
        "Hybrid Quantization"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:10.051289Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05101",
    "title": "Arabic Prompts with English Tools: A Benchmark",
    "authors": [
      "Konstantin Kubrak",
      "Ahmed El-Moselhy",
      "Ammar Alsulami",
      "Remaz Altuwaim",
      "Hassan Ismail Fawaz",
      "Faisal Alsaby"
    ],
    "abstract": "Large Language Models (LLMs) are now integral to numerous industries, increasingly serving as the core reasoning engine for autonomous agents that perform complex tasks through tool-use. While the development of Arabic-native LLMs is accelerating, the benchmarks for evaluating their capabilities lag behind, with most existing frameworks focusing on English. A critical and overlooked area is tool-calling, where the performance of models prompted in non-English languages like Arabic is poorly understood, especially since these models are often pretrained on predominantly English data. This paper addresses this critical gap by introducing the first dedicated benchmark for evaluating the tool-calling and agentic capabilities of LLMs in the Arabic language. Our work provides a standardized framework to measure the functional accuracy and robustness of models in Arabic agentic workflows. Our findings reveal a huge performance gap: when users interact in Arabic, tool-calling accuracy drops by an average of 5-10\\%, regardless of whether the tool descriptions themselves are in Arabic or English. By shedding light on these critical challenges, this benchmark aims to foster the development of more reliable and linguistically equitable AI agents for Arabic-speaking users.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05101.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05101",
    "published": "2026-01-08T16:47:09Z",
    "updated": "2026-01-08T16:47:09Z",
    "comment": "10 pages, 10 figures, LLMs, Big Data, and Multilinguality for All (LLMs4All) Workshop at IEEE BigData 2025 Conference, Macau, December 10, 2025",
    "light_analysis": {
      "overview": "本文通过引入首个阿拉伯语大语言模型工具调用与代理能力评估基准，填补了该领域的研究空白。",
      "motivation": "大型语言模型在自主代理中扮演关键角色，但评估其工具调用能力的基准主要集中在英语。随着阿拉伯语原生模型的快速发展，缺乏专门针对阿拉伯语的评估框架，尤其是工具调用性能未知。由于模型通常基于英语数据预训练，以阿拉伯语等非英语语言提示时可能表现不佳，这一问题对语言公平性和实际应用至关重要，亟待解决。",
      "method": "本文提出并实施了一个标准化评估框架，专门用于测量阿拉伯语大语言模型在代理工作流中的功能准确性和鲁棒性。具体方法包括设计阿拉伯语提示场景、集成工具调用任务以及可能使用现有或新建数据集进行测试。摘要未明确说明具体的模型架构、数据集细节或实验设置，但框架旨在提供统一的评估标准，促进跨模型比较。",
      "result": "实验结果表明，当用户以阿拉伯语进行交互时，大语言模型的工具调用准确率平均降低了5%至10%。这一下降现象独立于工具描述的语言，即无论工具描述是阿拉伯语还是英语，性能差距都显著存在。这揭示了模型在处理阿拉伯语提示时的固有局限性，突显了语言公平性的挑战，强调了跨语言评估的必要性。",
      "conclusion": "本研究的核心贡献是建立了首个针对阿拉伯语大语言模型工具调用能力的评估基准。通过揭示阿拉伯语交互导致的性能下降，强调了多语言评估的重要性。这项工作具有学术价值，为未来研究提供了标准化工具，并促进实际应用，推动开发更公平和可靠的AI代理，以更好服务阿拉伯语用户。未来工作可能包括扩展基准到其他语言或更复杂的代理任务。",
      "tags": [
        "Large Language Model",
        "Tool-Calling",
        "Benchmarking",
        "Arabic Language Processing",
        "Autonomous Agents"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:08.869104Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05091",
    "title": "Code-Mix Sentiment Analysis on Hinglish Tweets",
    "authors": [
      "Aashi Garg",
      "Aneshya Das",
      "Arshi Arya",
      "Anushka Goyal",
      "Aditi"
    ],
    "abstract": "The effectiveness of brand monitoring in India is increasingly challenged by the rise of Hinglish--a hybrid of Hindi and English--used widely in user-generated content on platforms like Twitter. Traditional Natural Language Processing (NLP) models, built for monolingual data, often fail to interpret the syntactic and semantic complexity of this code-mixed language, resulting in inaccurate sentiment analysis and misleading market insights. To address this gap, we propose a high-performance sentiment classification framework specifically designed for Hinglish tweets. Our approach fine-tunes mBERT (Multilingual BERT), leveraging its multilingual capabilities to better understand the linguistic diversity of Indian social media. A key component of our methodology is the use of subword tokenization, which enables the model to effectively manage spelling variations, slang, and out-of-vocabulary terms common in Romanized Hinglish. This research delivers a production-ready AI solution for brand sentiment tracking and establishes a strong benchmark for multilingual NLP in low-resource, code-mixed environments.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05091.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05091",
    "published": "2026-01-08T16:39:26Z",
    "updated": "2026-01-08T16:39:26Z",
    "comment": "Accepted at the 9th International Conference on Natural Language Processing and Information Retrieval (NLPIR 2025), Fukuoka, Japan",
    "light_analysis": {
      "overview": "论文提出了一个基于微调mBERT的高性能情感分类框架，专门用于Hinglish推文的情感分析。",
      "motivation": "研究动机源于印度品牌监测中日益突出的挑战，即Hinglish（印地语和英语的混合）在Twitter等社交媒体平台上的广泛使用，导致用户生成内容复杂化。传统NLP模型针对单语数据设计，无法有效解析这种混合语言的句法和语义复杂性，造成情感分析结果不准确和市场见解误导。因此，需要开发专门处理混合语言的NLP解决方案，以提升品牌监测的准确性和实用性，解决现有方法的不足。",
      "method": "研究方法采用微调mBERT（多语言BERT），利用其多语言预训练能力来理解印度社交媒体中的语言多样性。核心创新点是引入子词分词技术，该技术能有效处理罗马化Hinglish中常见的拼写变异、俚语和词汇外术语，从而增强模型的鲁棒性。框架专门设计用于低资源、混合语言环境的情感分类任务，通过微调过程优化模型对Hinglish推文的语义理解，摘要未明确说明具体数据集和模型架构细节，但基于Hinglish推文数据进行了实验验证。",
      "result": "主要实验结果显示，该框架成功建立了情感分析在低资源、混合语言环境中的基准，为品牌情感追踪提供了生产就绪的AI解决方案。摘要未明确说明具体性能指标（如准确率提升或效率改进）和与基线方法的详细对比，但论文强调其建立了强有力的基准，表明在提升混合语言情感分析的实用性和可靠性方面取得了进展，适用于实际应用场景。",
      "conclusion": "论文的主要贡献是开发了一个针对Hinglish推文的高性能情感分类框架，基于mBERT微调和子词分词技术，解决了混合语言NLP中的关键挑战。该研究具有重要的学术价值，为多语言NLP在低资源、混合语言环境中提供了新方法和基准，实际应用价值体现在提升品牌监测和市场分析的准确性。未来工作可扩展至其他混合语言或更大规模数据集，以进一步验证和优化方法的通用性。",
      "tags": [
        "Multilingual BERT",
        "Subword Tokenization",
        "Sentiment Analysis",
        "Code-Mixing",
        "Hinglish"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:29.661463Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.15480",
    "title": "Instruction Tuning with and without Context: Behavioral Shifts and Downstream Impact",
    "authors": [
      "Hyunji Lee",
      "Seunghyun Yoon",
      "Yunjae Won",
      "Hanseok Oh",
      "Geewook Kim",
      "Trung Bui",
      "Franck Dernoncourt",
      "Elias Stengel-Eskin",
      "Mohit Bansal",
      "Minjoon Seo"
    ],
    "abstract": "Instruction tuning is a widely used approach to improve the instruction-following ability of large language models (LLMs). Instruction-tuning datasets typically include a mixture of context-augmented and context-free examples, yet prior work has largely combined these data types without examining their distinct effects. In this paper, we investigate how training LLMs with or without context affects model behavior and downstream performance. First, in the text domain, we show that LLMs trained with context attend more strongly to the provided knowledge, achieving better grounding. We also observe that context-augmented training shifts how LLMs use knowledge: models store and leverage less on parametric knowledge and instead depend more on the provided context. Second, we observe that using LLM trained with context-augmented data as the backbone for vision-language models reduces hallucination and improves grounding in the visual domain. Finally, we explore practical strategies for real-world deployments where context availability varies. We show that maintaining separate context-augmented and context-free models and routing inputs between them yields more robust overall performance than training a single mixed model, as it better preserves their complementary strengths.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.15480.pdf",
    "abs_url": "https://arxiv.org/abs/2506.15480",
    "published": "2025-06-18T14:13:56Z",
    "updated": "2026-01-08T16:32:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文研究instruction tuning中context的作用，发现context-augmented训练改善接地性并减少幻觉，提出分开模型策略优化性能。",
      "motivation": "Instruction tuning是提升大型语言模型指令跟随能力的常用方法，数据集通常混合context-augmented和context-free示例。然而，先前研究未深入探讨这两种数据类型的独立效应，导致模型在上下文可用性变化时性能不稳健，难以平衡接地性和泛化能力。本研究旨在填补这一空白，探究context对LLMs行为的影响，以优化文本和视觉领域的下游任务表现，为实际应用提供更有效的训练和部署方案。",
      "method": "本研究通过训练大型语言模型时区分使用context-augmented和context-free数据，分析其行为差异。在文本域中，评估模型对提供知识的关注程度和接地性改善；在视觉域中，将训练好的LLMs作为vision-language模型的主干，以减少幻觉。此外，探索实际部署策略：维护分开的context-augmented和context-free模型，根据输入上下文可用性在它们之间路由输入，以比较与单一混合模型的性能差异，并保留互补优势。摘要未明确说明具体数据集和模型架构细节。",
      "result": "实验结果表明，在文本域中，经过context-augmented训练的LLMs更强烈关注提供知识，实现更好接地性，同时减少对参数知识的依赖。在视觉域，使用这些模型作为vision-language模型的主干有效减少幻觉并提升接地性。部署策略比较显示，分开模型并路由输入比单一混合模型带来更稳健整体性能，更好地结合了两种训练方式的优势。摘要未提供具体准确率数据，但强调了行为改进和性能提升。",
      "conclusion": "本研究表明，instruction tuning中context的使用显著影响LLMs的行为，context-augmented训练有助于提升接地性和减少幻觉，具有学术价值，深化对训练机制的理解。提出的分开模型策略为实际部署提供了有效解决方案，具有应用价值，特别是在多模态AI系统中。未来工作可进一步探索context选择机制、扩展到其他模型架构，或优化路由策略以提高适应性。",
      "tags": [
        "Instruction Tuning",
        "Large Language Models",
        "Context-augmented Training",
        "Vision-Language Models",
        "Model Routing"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:38.250369Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05083",
    "title": "Driving on Registers",
    "authors": [
      "Ellington Kirby",
      "Alexandre Boulch",
      "Yihong Xu",
      "Yuan Yin",
      "Gilles Puy",
      "Éloi Zablocki",
      "Andrei Bursuc",
      "Spyros Gidaris",
      "Renaud Marlet",
      "Florent Bartoccioni",
      "Anh-Quan Cao",
      "Nermin Samet",
      "Tuan-Hung VU",
      "Matthieu Cord"
    ],
    "abstract": "We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05083.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05083",
    "published": "2026-01-08T16:28:24Z",
    "updated": "2026-01-08T16:28:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "DrivoR提出了一种基于Transformer的端到端自动驾驶架构，通过相机感知寄存器令牌压缩多相机特征，实现高效且准确的驾驶性能。",
      "motivation": "该研究旨在解决端到端自动驾驶中多相机特征处理的计算效率问题。现有方法常因处理高维特征而导致计算成本高昂，难以在保持准确性的同时实现高效推理。DrivoR通过引入相机感知寄存器令牌来压缩特征表示，从而在不牺牲性能的前提下减少下游计算，提升自动驾驶系统的实用性和可扩展性。摘要未明确说明具体现有方法的不足，但隐含计算效率是关键挑战。",
      "method": "方法基于预训练的Vision Transformers（ViTs），创新引入相机感知寄存器令牌，将多相机特征压缩为紧凑的场景表示。这些令牌驱动两个轻量级Transformer解码器：第一个解码器生成候选轨迹，第二个解码器学习模仿一个oracle，预测可解释的子分数（如安全、舒适和效率），用于轨迹评分。关键创新包括令牌压缩技术以降低计算复杂度，以及可解释评分机制实现行为条件驾驶。架构设计简单高效，适用于端到端场景。",
      "result": "在NAVSIM-v1、NAVSIM-v2和HUGSIM等多个基准测试中，DrivoR表现出色，优于或匹配当代强基线。结果表明，该方法在保持高准确性的同时，显著减少了计算需求，实现了高效的自适应驾驶。摘要未提供具体性能数据（如准确率或效率改进百分比），但强调了在光真实模拟和闭环测试中的优异表现，验证了纯Transformer架构结合令牌压缩的有效性。",
      "conclusion": "该研究证明了纯Transformer架构结合相机感知令牌压缩足以实现准确、高效和自适应的端到端自动驾驶。主要贡献在于提出了一种简洁高效的架构，通过可解释评分增强行为控制，具有实际应用价值。学术上，展示了Transformer在复杂驾驶任务中的潜力。未来工作可能涉及更多场景验证或优化，摘要未明确说明局限性或具体方向。",
      "tags": [
        "Vision Transformers",
        "Transformer-based Architecture",
        "End-to-end Autonomous Driving",
        "Register Tokens",
        "Trajectory Scoring"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:19.114806Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05076",
    "title": "Chain-of-Sanitized-Thoughts: Plugging PII Leakage in CoT of Large Reasoning Models",
    "authors": [
      "Arghyadeep Das",
      "Sai Sreenivas Chintha",
      "Rishiraj Girmal",
      "Kinjal Pandey",
      "Sharvi Endait"
    ],
    "abstract": "Large Reasoning Models (LRMs) improve performance, reliability, and interpretability by generating explicit chain-of-thought (CoT) reasoning, but this transparency introduces a serious privacy risk: intermediate reasoning often leaks personally identifiable information (PII) even when final answers are sanitized. We study how to induce privacy-first reasoning, where models reason without exposing sensitive information, using deployable interventions rather than post-hoc redaction. We introduce PII-CoT-Bench, a supervised dataset with privacy-aware CoT annotations, and a category-balanced evaluation benchmark covering realistic and adversarial leakage scenarios. Our results reveal a capability-dependent trend: state-of-the-art models benefit most from prompt-based controls, whereas weaker models require fine-tuning to achieve meaningful leakage reduction. Across models and categories, both approaches substantially reduce PII exposure with minimal degradation in utility, demonstrating that private reasoning can be achieved without sacrificing performance. Overall, we show that private CoT reasoning can be achieved with minimal utility loss, providing practical guidance for building privacy-preserving reasoning systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05076.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05076",
    "published": "2026-01-08T16:19:43Z",
    "updated": "2026-01-08T16:19:43Z",
    "comment": "12 pages, 6 figures, 1 table",
    "light_analysis": {
      "overview": "本文提出通过提示控制和微调实现私有链式思维推理，显著减少个人身份信息泄漏，同时保持模型性能。",
      "motivation": "大型推理模型（LRMs）通过链式思维（CoT）推理提高性能、可靠性和可解释性，但中间步骤常泄露个人可识别信息（PII），即使最终答案被清理，造成严重隐私风险。现有方法多依赖事后删除，效率低且可能影响模型输出质量。因此，研究如何诱导隐私优先推理至关重要，旨在使用可部署的干预措施从源头减少泄漏，而非后处理，以提升实际部署中的隐私保护能力。",
      "method": "论文引入PII-CoT-Bench，一个带有隐私感知链式思维注释的监督数据集，并提供类别平衡的评估基准，覆盖现实和对抗性泄漏场景。核心方法是使用提示控制（prompt-based controls）和微调（fine-tuning）来诱导模型在不暴露敏感信息的情况下推理。这些干预措施直接在推理过程中减少PII泄漏，不依赖于后处理，聚焦于部署可行的技术方案。",
      "result": "实验结果揭示能力依赖性趋势：先进模型通过提示控制能有效减少PII泄漏，而较弱模型需微调才能实现有意义的泄漏减少。在所有模型和类别中，这两种方法都显著降低了PII暴露，同时实用性能（utility）损失最小，表明私有推理可以在不牺牲性能的情况下实现。与基线方法对比，泄漏减少效果显著，摘要未明确说明具体数值。",
      "conclusion": "研究表明，通过适当的干预措施，私有链式思维推理可以实现最小的实用损失，为构建隐私保护推理系统提供实践指导。这推动了隐私保护AI领域的研究，在保持模型性能的同时增强隐私，具有学术和实际应用价值。未来工作可能包括探索更多干预策略或扩展到其他隐私问题，但摘要未明确说明局限性。",
      "tags": [
        "Chain-of-Thought",
        "Privacy-Preserving",
        "Prompt Engineering",
        "Fine-tuning",
        "Large Reasoning Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:33.465838Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.17722",
    "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
    "authors": [
      "Yaning Pan",
      "Qianqian Xie",
      "Guohui Zhang",
      "Zekun Wang",
      "Yongqian Wen",
      "Yuanxing Zhang",
      "Haoxuan Hu",
      "Zhiyu Pan",
      "Yibing Huang",
      "Zhidong Gan",
      "Yonghong Lin",
      "An Ping",
      "Shihao Li",
      "Yanghai Wang",
      "Tianhao Peng",
      "Jiaheng Liu"
    ],
    "abstract": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses 6 core competencies that focus on perceptivity and interactivity, encompassing 1,000 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.17722.pdf",
    "abs_url": "https://arxiv.org/abs/2510.17722",
    "published": "2025-10-20T16:38:40Z",
    "updated": "2026-01-08T16:16:20Z",
    "comment": "Project Website: https://github.com/NJU-LINK/MT-Video-Bench",
    "light_analysis": {
      "overview": "论文提出MT-Video-Bench基准，用于全面评估多模态大语言模型在多轮视频对话中的核心能力。",
      "motivation": "多模态大语言模型（MLLMs）的发展提升了AI对视觉模态的理解，但现有评估基准局限于单轮问答，忽视了现实场景中多轮对话的复杂性。这种不足使得模型在交互式应用（如运动分析和智能辅导）中的表现难以准确评估，因此研究旨在解决如何更全面地衡量MLLMs在多轮视频对话中的能力，以推动技术进步。",
      "method": "研究方法包括构建MT-Video-Bench基准，该基准评估6个核心能力，聚焦感知性和交互性，涵盖1,000个从多样领域策划的多轮对话。基准设计严格对齐现实应用场景，如交互式运动分析和多轮视频智能辅导，从而提供系统化的评估框架来测试MLLMs在多轮对话中的表现。",
      "result": "实验使用MT-Video-Bench对多种开源和闭源MLLMs进行评估，结果显示模型在处理多轮视频对话时存在显著的性能差异和局限性，表明现有方法在多轮交互方面仍有不足。摘要未明确说明具体的性能指标数据（如准确率或效率改进），但评估揭示了模型能力的短板。",
      "conclusion": "本研究的主要贡献是提出了MT-Video-Bench基准，填补了多模态大语言模型在多轮视频对话评估领域的空白。学术价值在于促进了更全面的模型能力研究，实际应用价值体现在支持交互式运动分析和智能辅导等场景。未来工作包括公开基准以激励社区研究，并可能针对局限性进行优化。",
      "tags": [
        "Multimodal Large Language Models",
        "Video Understanding",
        "Benchmark Evaluation",
        "Multi-Turn Dialogue"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:17.217728Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05062",
    "title": "Compositional Steering of Large Language Models with Steering Tokens",
    "authors": [
      "Gorjan Radevski",
      "Kiril Gashteovski",
      "Giwon Hong",
      "Carolin Lawrence",
      "Goran Glavaš"
    ],
    "abstract": "Deploying LLMs in real-world applications requires controllable output that satisfies multiple desiderata at the same time. While existing work extensively addresses LLM steering for a single behavior, \\textit{compositional steering} -- i.e., steering LLMs simultaneously towards multiple behaviors -- remains an underexplored problem. In this work, we propose \\emph{compositional steering tokens} for multi-behavior steering. We first embed individual behaviors, expressed as natural language instructions, into dedicated tokens via self-distillation. Contrary to most prior work, which operates in the activation space, our behavior steers live in the space of input tokens, enabling more effective zero-shot composition. We then train a dedicated \\textit{composition token} on pairs of behaviors and show that it successfully captures the notion of composition: it generalizes well to \\textit{unseen} compositions, including those with unseen behaviors as well as those with an unseen \\textit{number} of behaviors. Our experiments across different LLM architectures show that steering tokens lead to superior multi-behavior control compared to competing approaches (instructions, activation steering, and LoRA merging). Moreover, we show that steering tokens complement natural language instructions, with their combination resulting in further gains.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05062.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05062",
    "published": "2026-01-08T16:08:44Z",
    "updated": "2026-01-08T16:08:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了组合导向令牌，通过自蒸馏嵌入行为并训练组合令牌，实现大语言模型的多行为零样本组合导向。",
      "motivation": "在真实世界应用中部署大型语言模型（LLMs）时，需要输出同时满足多个需求的可控性。现有研究主要专注于单一行为的导向，而组合导向——即同时导向多个行为——仍是一个未被充分探索的问题。由于实际应用场景往往要求模型兼顾多种行为，如安全性和创造性，现有方法的不足凸显了解决组合导向的重要性，以提升LLMs在复杂任务中的实用性。",
      "method": "本研究提出组合导向令牌来实现多行为导向。首先，通过自蒸馏技术将自然语言指令表示的行为嵌入到专用令牌中，使行为导向在输入令牌空间而非激活空间进行，从而提高零样本组合的有效性。然后，训练一个组合令牌，处理行为对以捕捉组合概念。关键创新在于操作空间的转移，避免了复杂的激活修改，并在不同LLM架构上实施，具体模型架构和数据集摘要未明确说明。",
      "result": "实验结果显示，导向令牌在多行为控制方面表现优于竞争方法，包括直接使用指令、激活导向和LoRA合并。虽然摘要未提供具体数据，但指出导向令牌在处理未见过的组合时具有良好的泛化能力，能扩展到未见行为和未见行为数量的组合。此外，导向令牌与自然语言指令结合使用可带来进一步的性能提升，表明其互补性和有效性。",
      "conclusion": "本文的主要贡献是提出了组合导向令牌方法，有效解决了LLMs多行为组合导向的问题。该研究具有重要的学术价值，填补了组合导向领域的空白，并为实际应用中的模型控制提供了新思路。潜在局限性摘要未明确说明，未来工作可能包括扩展到更多行为类型或与其他控制方法的整合。",
      "tags": [
        "Large Language Model",
        "Compositional Steering",
        "Steering Tokens",
        "Self-Distillation",
        "Zero-shot Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:06.449400Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03327",
    "title": "Extreme-value forest fire prediction A study of the Loss Function in an Ordinality Scheme",
    "authors": [
      "Nicolas Caron",
      "Christophe Guyeux",
      "Hassan Noura",
      "Benjamin Aynes"
    ],
    "abstract": "Wildfires are highly imbalanced natural hazards in both space and severity, making the prediction of extreme events particularly challenging. In this work, we introduce the first ordinal classification framework for forecasting wildfire severity levels directly aligned with operational decision-making in France. Our study investigates the influence of loss-function design on the ability of neural models to predict rare yet critical high-severity fire occurrences. We compare standard cross-entropy with several ordinal-aware objectives, including the proposed probabilistic TDeGPD loss derived from a truncated discrete exponentiated Generalized Pareto Distribution. Through extensive benchmarking over multiple architectures and real operational data, we show that ordinal supervision substantially improves model performance over conventional approaches. In particular, the Weighted Kappa Loss (WKLoss) achieves the best overall results, with more than +0.1 IoU (Intersection Over Union) gain on the most extreme severity classes while maintaining competitive calibration quality. However, performance remains limited for the rarest events due to their extremely low representation in the dataset. These findings highlight the importance of integrating both severity ordering, data imbalance considerations, and seasonality risk into wildfire forecasting systems. Future work will focus on incorporating seasonal dynamics and uncertainty information into training to further improve the reliability of extreme-event prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03327.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03327",
    "published": "2026-01-06T15:46:56Z",
    "updated": "2026-01-08T16:01:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了首个用于预测野火严重程度等级的有序分类框架，通过研究损失函数设计，显著提升了对罕见极端事件的预测性能。",
      "motivation": "野火灾害在空间和严重度上均呈现高度不平衡分布，极端事件（如高严重度火灾）的预测对防灾减灾决策至关重要。然而，现有方法通常忽略严重度的有序性，且难以有效处理数据不平衡问题，导致对罕见事件的预测效果不佳。本研究的动机是开发一个直接与法国操作决策对齐的有序分类框架，以改进野火极端事件的预测，应对现有方法的不足，并强调整合数据不平衡和季节性风险的重要性。",
      "method": "本研究引入了一个针对野火严重度预测的有序分类框架，核心在于研究损失函数设计对神经网络模型的影响。通过比较标准交叉熵损失和多种有序感知目标，包括提出的基于截断离散指数化广义帕累托分布的概率 TDeGPD 损失，以更好地捕捉极端事件的分布特性。研究使用真实操作数据和多种神经网络架构进行基准测试，关键创新点包括 TDeGPD 损失函数的创新设计以及对有序分类任务的专门优化，旨在提升模型对罕见高严重度事件的预测能力。",
      "result": "实验结果显示，有序监督显著优于传统方法，其中加权 Kappa 损失（WKLoss）表现最佳。在极端严重度类别上，WKLoss 实现了超过 +0.1 IoU 的增益，同时保持竞争力的校准质量。与基线方法相比，有序监督方法在整体性能上有明显改进，但由于数据集中罕见事件的表示极低，模型对这些事件的预测能力仍然有限，这突显了数据不平衡问题的挑战。",
      "conclusion": "本研究的主要贡献是验证了有序监督在野火预测中的有效性，强调了整合严重度排序、数据不平衡处理以及季节性风险的重要性。这项研究提供了损失函数设计的深入分析，具有学术价值，并提升了野火预测系统的实际应用能力。潜在的局限性在于罕见事件的数据不足，未来工作可探索整合季节性动态和不确定性信息，以进一步提高极端事件预测的可靠性。",
      "tags": [
        "Ordinal Classification",
        "Loss Function",
        "Generalized Pareto Distribution",
        "Weighted Kappa Loss",
        "Extreme Event Prediction"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:33.321935Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05053",
    "title": "Reinforced Efficient Reasoning via Semantically Diverse Exploration",
    "authors": [
      "Ziqi Zhao",
      "Zhaochun Ren",
      "Jiahong Zou",
      "Liu Yang",
      "Zhiwei Xu",
      "Xuri Ge",
      "Zhumin Chen",
      "Xinyu Ma",
      "Daiting Shi",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Xin Xin"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in enhancing the reasoning of large language models (LLMs). Monte Carlo Tree Search (MCTS)-based extensions improve upon vanilla RLVR (e.g., GRPO) by providing tree-based reasoning rollouts that enable fine-grained and segment-level credit assignment. However, existing methods still suffer from limited exploration diversity and inefficient reasoning. To address the above challenges, we propose reinforced efficient reasoning via semantically diverse explorations, i.e., ROSE, for LLMs. To encourage more diverse reasoning exploration, our method incorporates a semantic-entropy-based branching strategy and an $\\varepsilon$-exploration mechanism. The former operates on already sampled reasoning rollouts to capture semantic uncertainty and select branching points with high semantic divergence to generate new successive reasoning paths, whereas the latter stochastically initiates reasoning rollouts from the root, preventing the search process from becoming overly local. To improve efficiency, we design a length-aware segment-level advantage estimator that rewards concise and correct reasoning while penalizing unnecessarily long reasoning chains. Extensive experiments on various mathematical reasoning benchmarks with Qwen and Llama models validate the effectiveness and efficiency of ROSE. Codes are available at https://github.com/ZiqiZhao1/ROSE-rl.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05053.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05053",
    "published": "2026-01-08T15:56:44Z",
    "updated": "2026-01-08T15:56:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出ROSE方法，通过语义多样探索和效率优化，增强大型语言模型的推理能力。",
      "motivation": "当前，基于可验证奖励的强化学习（RLVR）和蒙特卡洛树搜索（MCTS）扩展已用于提升大型语言模型的推理能力，但现有方法探索多样性不足且推理效率低下。这限制了模型在复杂任务如数学推理中的表现，影响实际应用效果，因此需要一种能鼓励更多样探索并提高效率的新方法来克服这些局限性。",
      "method": "ROSE方法结合语义熵分支策略和ε探索机制以增加探索多样性。语义熵分支策略基于已采样推理路径的语义不确定性，选择高语义分歧的分支点生成新路径；ε探索机制从根节点随机启动推理，防止搜索过于局部。此外，设计长度感知段级优势估计器，奖励简洁正确推理，惩罚冗长链条，应用于大型语言模型如Qwen和Llama。",
      "result": "论文在多个数学推理基准测试中进行了广泛实验，使用Qwen和Llama模型，验证了ROSE方法的有效性和效率。摘要未明确说明具体性能指标如准确率提升，但实验结果表明与基线方法相比，ROSE在提升推理多样性和效率方面具有优势。",
      "conclusion": "ROSE方法通过语义多样探索和效率优化，显著提升了大型语言模型的推理能力。主要贡献包括提出语义熵分支策略、ε探索机制和长度感知优势估计器，解决了现有方法的探索多样性和效率问题。学术价值在于为强化学习在LLM推理中的应用提供了新方向，实际应用价值在于提高模型在数学推理等任务中的性能。未来工作可能包括扩展到其他领域。",
      "tags": [
        "Reinforcement Learning",
        "Monte Carlo Tree Search",
        "Large Language Models",
        "Semantic Entropy",
        "Advantage Estimation"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:32.495651Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05051",
    "title": "Publishing FAIR and Machine-actionable Reviews in Materials Science: The Case for Symbolic Knowledge in Neuro-symbolic Artificial Intelligence",
    "authors": [
      "Jennifer D'Souza",
      "Soren Auer",
      "Eleni Poupaki",
      "Alex Watkins",
      "Anjana Devi",
      "Riikka L. Puurunen",
      "Bora Karasulu",
      "Adrie Mackus",
      "Erwin Kessels"
    ],
    "abstract": "Scientific reviews are central to knowledge integration in materials science, yet their key insights remain locked in narrative text and static PDF tables, limiting reuse by humans and machines alike. This article presents a case study in atomic layer deposition and etching (ALD/E) where we publish review tables as FAIR, machine-actionable comparisons in the Open Research Knowledge Graph (ORKG), turning them into structured, queryable knowledge. Building on this, we contrast symbolic querying over ORKG with large language model-based querying, and argue that a curated symbolic layer should remain the backbone of reliable neurosymbolic AI in materials science, with LLMs serving as complementary, symbolically grounded interfaces rather than standalone sources of truth.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DL",
      "cs.IT"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05051.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05051",
    "published": "2026-01-08T15:56:17Z",
    "updated": "2026-01-08T15:56:17Z",
    "comment": "35 pages, 11 figures",
    "light_analysis": {
      "overview": "本研究通过在Open Research Knowledge Graph中发布FAIR和机器可操作的评论表格，强调符号知识应作为材料科学中神经符号AI的可靠骨干。",
      "motivation": "在材料科学中，科学评论是知识整合的关键，但其中关键见解往往锁定在叙述性文本和静态PDF表格中，限制了人类和机器的重用。这一问题阻碍了知识的动态访问和自动化处理，现有方法的不足在于格式固定且无法被机器直接解析，导致知识获取效率低下和重用性差，从而影响了科学进步的速度和可靠性。",
      "method": "论文采用案例研究方法，聚焦于原子层沉积和蚀刻（ALD/E）领域，将评论表格转化为结构化、可查询的知识，发布到Open Research Knowledge Graph中，使其符合FAIR原则和机器可操作性。核心创新点在于对比符号查询（基于ORKG的结构化知识）与基于大语言模型的查询，并主张在神经符号人工智能中，符号层应作为可靠的主干，而大语言模型则充当符号接地的补充接口，而非独立的真相来源。",
      "result": "摘要未明确说明具体的实验数据或性能指标。论文描述了一个案例研究，展示了将评论知识转化为结构化、可查询形式的过程，并对比了符号查询与大语言模型查询的方法。效果主要体现在知识的结构化转换和查询方式的探讨上，但没有提供量化对比结果如准确率或效率提升，仅强调符号查询在可靠性方面的优势。",
      "conclusion": "论文的主要贡献是推广符号知识在神经符号人工智能中的骨干作用，确保材料科学中的知识一致性和可靠性。学术价值在于提高科学评论的可重用性和AI的可解释性，实际应用价值则通过ORKG等技术改善知识管理。潜在局限性可能包括对其他材料科学领域的适用性，未来工作方向可扩展到更复杂的查询场景和集成更多AI技术。",
      "tags": [
        "FAIR data",
        "Machine-actionable knowledge",
        "Open Research Knowledge Graph",
        "Neuro-symbolic AI",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:42.228876Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05050",
    "title": "Large language models can effectively convince people to believe conspiracies",
    "authors": [
      "Thomas H. Costello",
      "Kellin Pelrine",
      "Matthew Kowal",
      "Antonio A. Arechar",
      "Jean-François Godbout",
      "Adam Gleave",
      "David Rand",
      "Gordon Pennycook"
    ],
    "abstract": "Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against (\"debunking\") or for (\"bunking\") that conspiracy. When using a \"jailbroken\" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.",
    "categories": [
      "cs.AI",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05050.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05050",
    "published": "2026-01-08T15:56:05Z",
    "updated": "2026-01-08T15:56:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "研究发现大型语言模型具有同等能力说服人们相信或怀疑阴谋论，但存在纠正对话和提示策略来缓解这种风险。",
      "motivation": "大型语言模型在各种语境中展现出说服力，但不清楚这种说服力是否更有利于真理而非谬误。现有研究表明LLMs可能轻松促进错误信念，这导致AI安全风险和信息传播问题。研究背景是LLMs的广泛应用，强调探索其潜在负面影响以改进防护措施，摘要未明确说明具体技术缺陷，但暗示现有防护措施不足。",
      "method": "研究方法包括三个预先注册的实验，邀请2,724名美国参与者与GPT-4o讨论他们不确定的阴谋论。模型被指示支持或反驳阴谋论，使用'越狱'版GPT-4o去除防护措施，并与标准版对比。实验还涉及纠正性对话和提示模型只使用准确信息，以评估缓解策略的效果，关键创新点在于比较不同指令和防护条件下的影响。",
      "result": "主要实验结果显示：使用'越狱'GPT-4o时，AI增加和减少阴谋论信念的效果相似；支持版本获得更积极评价并增加信任。标准GPT-4o产生类似效果，OpenAI的防护措施效果有限。但与基线相比，纠正对话能逆转新诱导的信念，提示模型只使用准确信息大幅降低其增加信念的能力，具体性能指标摘要未明确说明提升幅度。",
      "conclusion": "研究结论是LLMs在促进真理和谬误方面具有同等能力，突显AI安全风险。潜在解决方案如纠正对话和提示策略展示了学术价值和实际应用潜力，推动更安全的AI设计。局限性包括样本限于美国人，未来工作可扩展实验或探索更多防护措施。",
      "tags": [
        "Large Language Models",
        "GPT-4o",
        "Jailbreaking",
        "Persuasive AI",
        "Misinformation Mitigation"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:26.656055Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05049",
    "title": "How to Set the Learning Rate for Large-Scale Pre-training?",
    "authors": [
      "Yunhua Zhou",
      "Shuhao Xing",
      "Junhao Huang",
      "Xipeng Qiu",
      "Qipeng Guo"
    ],
    "abstract": "Optimal configuration of the learning rate (LR) is a fundamental yet formidable challenge in large-scale pre-training. Given the stringent trade-off between training costs and model performance, the pivotal question is whether the optimal LR can be accurately extrapolated from low-cost experiments. In this paper, we formalize this investigation into two distinct research paradigms: Fitting and Transfer. Within the Fitting Paradigm, we innovatively introduce a Scaling Law for search factor, effectively reducing the search complexity from O(n^3) to O(n*C_D*C_η) via predictive modeling. Within the Transfer Paradigm, we extend the principles of $μ$Transfer to the Mixture of Experts (MoE) architecture, broadening its applicability to encompass model depth, weight decay, and token horizons. By pushing the boundaries of existing hyperparameter research in terms of scale, we conduct a comprehensive comparison between these two paradigms. Our empirical results challenge the scalability of the widely adopted $μ$ Transfer in large-scale pre-training scenarios. Furthermore, we provide a rigorous analysis through the dual lenses of training stability and feature learning to elucidate the underlying reasons why module-wise parameter tuning underperforms in large-scale settings. This work offers systematic practical guidelines and a fresh theoretical perspective for optimizing industrial-level pre-training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05049.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05049",
    "published": "2026-01-08T15:55:13Z",
    "updated": "2026-01-08T15:55:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Fitting和Transfer两种研究范式来优化大规模预训练的学习率配置，并挑战了$μ$Transfer在大规模场景中的可扩展性。",
      "motivation": "大规模预训练中，学习率配置是一个基础但困难的挑战，需要在训练成本和模型性能之间做出权衡。关键问题在于是否可以从低成本实验中准确推断出最优学习率，以优化工业级预训练的效率和效果。现有方法如$μ$Transfer可能在大规模场景中无法有效扩展，因此本研究旨在解决这一难题，提供更可靠的学习率优化策略，弥补现有超参数研究在规模扩展方面的不足。",
      "method": "论文提出两种研究范式：Fitting范式通过引入搜索因子的缩放定律，利用预测建模将搜索复杂度从O(n^3)降低到O(n*C_D*C_η)，从而高效推断最优学习率。Transfer范式将$μ$Transfer原则扩展到Mixture of Experts (MoE)架构，并扩展其应用范围至模型深度、权重衰减和token horizons等参数。研究还通过比较这两个范式，并基于训练稳定性和特征学习的双重视角，分析大规模设置下的超参数调优机制。",
      "result": "实证结果表明，广泛采用的$μ$Transfer在大规模预训练场景中的可扩展性受到挑战，模块级参数调优在大规模设置中表现不佳。论文通过分析训练稳定性和特征学习，揭示了这种局限性背后的原因，例如训练不稳定和特征学习效率下降。摘要未明确说明具体的性能指标数据（如准确率提升），但强调了$μ$Transfer在扩展性上的问题，并通过对比Fitting和Transfer范式提供了实证支持。",
      "conclusion": "本研究提供了系统的实践指南和新的理论视角，用于优化工业级预训练的学习率配置。通过比较Fitting和Transfer范式，挑战了现有方法的可扩展性，并深入分析了大规ģ设置中的挑战，如训练稳定性和特征学习机制。工作为未来研究方向奠定了基础，例如探索更高效的超参数优化策略，并强调了在超参数研究中考虑规模因素的重要性，以推动AI模型的工业应用。",
      "tags": [
        "Large-Scale Pre-training",
        "Learning Rate Optimization",
        "Scaling Law",
        "μTransfer",
        "Mixture of Experts"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:35.002869Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.17189",
    "title": "Talking with Tables for Better LLM Factual Data Interactions",
    "authors": [
      "Jio Oh",
      "Geon Heo",
      "Seungjun Oh",
      "Hyunjin Kim",
      "JinYeong Bak",
      "Jindong Wang",
      "Xing Xie",
      "Steven Euijong Whang"
    ],
    "abstract": "Large Language Models (LLMs) often struggle with requests related to information retrieval and data manipulation that frequently arise in real-world scenarios under multiple conditions. In this paper, we demonstrate that leveraging tabular structures in LLM interactions, is more effective than utilizing other structures for handling prevalent requests that operate over factual data. Through comprehensive evaluations across various scenarios and request types, we show that providing tabular structures yields a 40.29\\% average performance gain along with better robustness and token efficiency. Through attention-value analysis, we discover that tables help LLMs better locate relevant information, explaining these improvements. Beyond tables and text, we evaluate whether (1) blending structuredness within text, such as providing templates or fixing the order of attributes, and (2) other representative structures, such as knowledge graphs and JSON are helpful. We observe that utilizing tables offers the best balance between efficiency and effectiveness. The method remains robust to task complexity and adapts to unstructured sources through text-to-table conversion. Overall, we highlight the untapped potential of tabular representations for future LLM applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2412.17189.pdf",
    "abs_url": "https://arxiv.org/abs/2412.17189",
    "published": "2024-12-22T23:31:03Z",
    "updated": "2026-01-08T15:45:17Z",
    "comment": "20 pages, 9 figures",
    "light_analysis": {
      "overview": "论文通过利用表格结构优化大型语言模型的事实数据交互，显著提升处理信息检索和数据操作请求的性能和效率。",
      "motivation": "大型语言模型在处理真实场景中多条件下的信息检索和数据操作请求时常常表现不佳，这些问题在实际应用中普遍存在且至关重要。现有方法可能无法有效处理结构化事实数据，导致效率低下和鲁棒性不足。因此，本研究旨在探索更优的结构化表示方法，以克服这些局限性并提升模型交互效果。摘要强调了解决这些请求的重要性，以及现有结构可能存在的不足。",
      "method": "本研究提出在大型语言模型交互中利用表格结构来处理事实数据请求。方法包括通过文本到表格转换将非结构化源适应为表格形式，并进行全面的评估。通过注意力值分析来解释表格为何能帮助模型更好地定位相关信息。此外，还对比了其他代表性结构如知识图谱、JSON以及混合结构化文本（如模板和固定属性顺序），以验证表格在效率和效果上的优势。核心创新在于表格结构的有效性和适应性。",
      "result": "实验结果显示，使用表格结构带来了平均40.29%的性能提升，同时提高了鲁棒性和令牌效率。通过对比分析，表格在多种场景和请求类型中都优于其他结构如知识图谱和JSON，展现出在效率与效果之间更好的平衡。注意力值分析进一步证实表格能帮助大型语言模型更准确地定位信息，从而解释性能改进。这些结果基于广泛的评估，表明方法对不同任务复杂性具有鲁棒性。",
      "conclusion": "本研究主要贡献在于揭示了表格表示法在大型语言模型应用中的未开发潜力，通过改进事实数据交互的性能和适应性。学术价值在于为结构化数据在自然语言处理中的应用提供了新思路，实际应用价值包括增强模型在真实场景中的实用性和可扩展性。尽管摘要未明确说明局限性，但未来工作可探索更复杂的场景或与其他技术结合。总体而言，研究为未来大型语言模型的发展提供了重要方向。",
      "tags": [
        "Large Language Model",
        "Tabular Structures",
        "Information Retrieval",
        "Attention Analysis",
        "Text-to-Table Conversion"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:30.887615Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05038",
    "title": "ArcAligner: Adaptive Recursive Aligner for Compressed Context Embeddings in RAG",
    "authors": [
      "Jianbo Li",
      "Yi Jiang",
      "Sendong Zhao",
      "Bairui Hu",
      "Haochun Wang",
      "Bing Qin"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) helps LLMs stay accurate, but feeding long documents into a prompt makes the model slow and expensive. This has motivated context compression, ranging from token pruning and summarization to embedding-based compression. While researchers have tried ''compressing'' these documents into smaller summaries or mathematical embeddings, there is a catch: the more you compress the data, the more the LLM struggles to understand it. To address this challenge, we propose ArcAligner (Adaptive recursive context *Aligner*), a lightweight module integrated into the language model layers to help the model better utilize highly compressed context representations for downstream generation. It uses an adaptive ''gating'' system that only adds extra processing power when the information is complex, keeping the system fast. Across knowledge-intensive QA benchmarks, ArcAligner consistently beats compression baselines at comparable compression rates, especially on multi-hop and long-tail settings. The source code is publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05038.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05038",
    "published": "2026-01-08T15:44:52Z",
    "updated": "2026-01-08T15:44:52Z",
    "comment": "Code is available at https://github.com/liunian-Jay/ArcAligner.git",
    "light_analysis": {
      "overview": "本文提出ArcAligner，一种自适应递归对齐器，用于在检索增强生成（RAG）中优化压缩上下文嵌入的使用，以提升模型效率和准确性。",
      "motivation": "检索增强生成（RAG）虽有助于大型语言模型（LLM）保持准确性，但输入长文档会导致模型响应缓慢、计算成本高昂，这促使了上下文压缩技术的发展，如token剪枝、摘要和基于嵌入的压缩。然而，现有压缩方法在减小数据体积的同时，往往损失了信息的完整性，使LLM难以理解压缩后的内容，特别是在高度压缩情况下模型性能显著下降。因此，本研究旨在解决压缩与理解之间的权衡问题，以增强RAG系统在知识密集型任务中的实用性和效率。",
      "method": "ArcAligner是一个轻量级模块，集成到语言模型的层中，通过自适应门控系统帮助模型更好地利用高度压缩的上下文表示进行下游生成。其核心创新在于采用递归对齐机制，根据信息复杂度动态调整处理强度：仅在检测到复杂内容时启用额外计算资源，从而在不显著增加开销的情况下优化信息提取。该方法涉及对压缩嵌入的自适应学习，利用嵌入表示进行实时调整，以提升生成任务的准确性和鲁棒性。",
      "result": "在知识密集型问答基准测试中，ArcAligner在可比较的压缩率下，一致优于现有的压缩基线方法，特别是在多跳推理和长尾场景中表现突出。摘要未明确说明具体性能指标如准确率提升数值，但实验表明该方法能有效增强模型对压缩上下文的理解能力，同时保持系统快速运行。与基线对比，ArcAligner在复杂问题处理上显示出更强的优势，证实了自适应机制在优化压缩表示使用中的有效性。",
      "conclusion": "本论文的主要贡献是提出ArcAligner，通过自适应递归对齐机制解决了RAG中压缩上下文表示带来的模型理解困难问题，在压缩与效率之间取得了平衡。该研究具有重要的学术价值，为上下文压缩技术提供了新思路，同时在实际应用中可推广至其他需要处理长文档的AI任务，如问答和摘要生成。潜在局限性可能包括对特定压缩率的依赖，未来工作可探索更广泛的压缩策略集成或扩展至多模态场景。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Context Compression",
        "Adaptive Gating",
        "Recursive Alignment",
        "Language Model Integration"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:28.571805Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05034",
    "title": "How to Set the Batch Size for Large-Scale Pre-training?",
    "authors": [
      "Yunhua Zhou",
      "Junhao Huang",
      "Shuhao Xin",
      "Yechen Zhang",
      "Runyu Peng",
      "Qiping Guo",
      "Xipeng Qiu"
    ],
    "abstract": "The concept of Critical Batch Size, as pioneered by OpenAI, has long served as a foundational principle for large-scale pre-training. However, with the paradigm shift towards the Warmup-Stable-Decay (WSD) learning rate scheduler, we observe that the original theoretical framework and its underlying mechanisms fail to align with new pre-training dynamics. To bridge this gap between theory and practice, this paper derives a revised E(S) relationship tailored for WSD scheduler, characterizing the trade-off between training data consumption E and steps S during pre-training. Our theoretical analysis reveals two fundamental properties of WSD-based pre-training: 1) B_min, the minimum batch size threshold required to achieve a target loss, and 2) B_opt, the optimal batch size that maximizes data efficiency by minimizing total tokens. Building upon these properties, we propose a dynamic Batch Size Scheduler. Extensive experiments demonstrate that our revised formula precisely captures the dynamics of large-scale pre-training, and the resulting scheduling strategy significantly enhances both training efficiency and final model quality.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05034.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05034",
    "published": "2026-01-08T15:43:31Z",
    "updated": "2026-01-08T15:43:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文针对Warmup-Stable-Decay学习率调度器，提出修正的E(S)关系和动态批量大小调度器，以优化大规模预训练的效率和模型质量。",
      "motivation": "研究动机是解决大规模预训练中批量大小设置的实践问题。随着Warmup-Stable-Decay学习率调度器成为主流，OpenAI提出的Critical Batch Size理论不再适用，导致理论与实际训练动态脱节。这一问题至关重要，因为大规模预训练是构建先进AI模型的基础步骤，优化批量大小直接影响计算资源效率和模型性能。现有方法的不足在于原始理论框架无法准确描述新调度器下的训练行为，因此亟需新理论来指导实际预训练过程，提升整体效率和质量。",
      "method": "研究方法包括理论分析和调度器设计。首先，推导了针对WSD调度器的修正E(S)关系，精确描述训练数据消耗E与步骤S之间的权衡关系。理论分析揭示两个关键属性：B_min（达到目标损失所需的最小批量大小阈值）和B_opt（通过最小化总标记数最大化数据效率的最优批量大小）。基于这些属性，提出了动态批量大小调度器，关键创新点在于将理论洞察转化为实际调度策略，适用于大规模预训练设置，无需依赖特定数据集或模型架构，但专注于通用优化原理。",
      "result": "主要实验结果表明，修正公式能精确捕捉大规模预训练的动态变化，而动态批量大小调度策略显著提升了训练效率和最终模型质量。摘要未明确提供具体性能指标数据，但实验证明该策略相比原始Critical Batch Size理论有显著改进，通过优化批量大小减少了数据冗余和计算开销。这增强了整体预训练的数据效率，从而在保持或提高模型准确性的同时，加速训练过程，适用于多种实际场景。",
      "conclusion": "论文的主要贡献是建立了针对WSD调度器的批量大小设置理论框架，并设计了动态调度器，弥补了现有理论与实践的差距。学术上，它提供了更准确的预训练动态模型，推动优化理论的发展；实际上，它提高了大规模训练的效率和质量，降低资源成本。局限性或未来工作摘要未明确说明，但可能包括扩展到其他学习率调度器或进一步实证验证，以增强通用性和鲁棒性。",
      "tags": [
        "Large-Scale Pre-training",
        "Batch Size Scheduling",
        "Warmup-Stable-Decay Scheduler",
        "Critical Batch Size",
        "Optimization Theory"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:24.630507Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05027",
    "title": "OptiSet: Unified Optimizing Set Selection and Ranking for Retrieval-Augmented Generation",
    "authors": [
      "Yi Jiang",
      "Sendong Zhao",
      "Jianbo Li",
      "Bairui Hu",
      "Yanrui Du",
      "Haochun Wang",
      "Bing Qin"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves generation quality by incorporating evidence retrieved from large external corpora. However, most existing methods rely on statically selecting top-k passages based on individual relevance, which fails to exploit combinatorial gains among passages and often introduces substantial redundancy. To address this limitation, we propose OptiSet, a set-centric framework that unifies set selection and set-level ranking for RAG. OptiSet adopts an \"Expand-then-Refine\" paradigm: it first expands a query into multiple perspectives to enable a diverse candidate pool and then refines the candidate pool via re-selection to form a compact evidence set. We then devise a self-synthesis strategy without strong LLM supervision to derive preference labels from the set conditional utility changes of the generator, thereby identifying complementary and redundant evidence. Finally, we introduce a set-list wise training strategy that jointly optimizes set selection and set-level ranking, enabling the model to favor compact, high-gain evidence sets. Extensive experiments demonstrate that OptiSet improves performance on complex combinatorial problems and makes generation more efficient. The source code is publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05027.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05027",
    "published": "2026-01-08T15:35:01Z",
    "updated": "2026-01-08T15:35:01Z",
    "comment": "Code is available at https://github.com/liunian-Jay/OptiSet.git",
    "light_analysis": {
      "overview": "OptiSet框架通过统一优化集合选择和集合级排名，解决了RAG中证据选择的组合增益和冗余问题。",
      "motivation": "检索增强生成（RAG）通过集成外部证据提升生成质量，但现有方法依赖个体相关性的静态选择（如top-k段落），忽视了段落间的协同作用，导致证据冗余和性能瓶颈，限制了在复杂组合问题上的应用效果。因此，研究动机在于解决这一不足，以提高RAG的效率和准确性。",
      "method": "OptiSet采用“Expand-then-Refine”范式：首先扩展查询为多视角以获取多样化候选池，然后通过重新选择形成紧凑证据集。核心创新包括自我合成策略，从生成器的集合条件效用变化中无监督推导偏好标签，识别互补和冗余证据；以及集合列表级训练策略，联合优化集合选择和集合级排名。框架设计无需强LLM监督，增强了鲁棒性。",
      "result": "摘要未明确说明具体性能指标（如准确率提升），但指出广泛实验表明OptiSet提高了复杂组合问题的性能，并使生成更高效。结果与现有基线方法对比，显示出在减少冗余和提升效率方面的优势。",
      "conclusion": "论文贡献在于提出OptiSet框架，统一了RAG中的集合选择和排名，有效解决证据优化问题，提升了生成质量和效率。该研究具有学术价值，为RAG系统提供了新思路，并有实际应用潜力。未来工作可能包括扩展到更多数据集或优化策略。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Set Selection",
        "Set Ranking",
        "Expand-then-Refine",
        "Self-Synthesis Strategy"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:28.823926Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05019",
    "title": "Hán Dān Xué Bù (Mimicry) or Qīng Chū Yú Lán (Mastery)? A Cognitive Perspective on Reasoning Distillation in Large Language Models",
    "authors": [
      "Yueqing Hu",
      "Xinyang Peng",
      "Shuting Peng",
      "Hanqi Wang",
      "Tianhong Wang"
    ],
    "abstract": "Recent Large Reasoning Models trained via reinforcement learning exhibit a \"natural\" alignment with human cognitive costs. However, we show that the prevailing paradigm of reasoning distillation -- training student models to mimic these traces via Supervised Fine-Tuning (SFT) -- fails to transmit this cognitive structure. Testing the \"Hán Dān Xué Bù\" (Superficial Mimicry) hypothesis across 14 models, we find that distillation induces a \"Functional Alignment Collapse\": while teacher models mirror human difficulty scaling ($\\bar{r}=0.64$), distilled students significantly degrade this alignment ($\\bar{r}=0.34$), often underperforming their own pre-distillation baselines (\"Negative Transfer\"). Our analysis suggests that SFT induces a \"Cargo Cult\" effect, where students ritualistically replicate the linguistic form of reasoning (verbosity) without internalizing the teacher's dynamic resource allocation policy. Consequently, reasoning distillation decouples computational cost from cognitive demand, revealing that human-like cognition is an emergent property of active reinforcement, not passive imitation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05019.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05019",
    "published": "2026-01-08T15:27:03Z",
    "updated": "2026-01-08T15:27:03Z",
    "comment": "7 pages, 7 figures",
    "light_analysis": {
      "overview": "论文揭示了推理蒸馏中监督微调导致的'功能对齐崩溃'，表明类人认知源于主动强化而非被动模仿。",
      "motivation": "研究动机在于探究大型推理模型中知识蒸馏的有效性。当前基于强化学习训练的大型语言模型与人类认知成本自然对齐，为优化推理能力提供基础。然而，主流蒸馏方法通过监督微调训练学生模型模仿教师的推理轨迹，却未能传递这种认知结构。这表明现有方法过于注重表面模仿，忽视了认知对齐的重要性，限制了模型在复杂推理任务中的性能提升和实际应用价值，突显了改进蒸馏范式的紧迫性。",
      "method": "研究方法采用实验分析，对14个大型语言模型进行测试，验证'邯郸学步'假设。通过比较教师模型和经过监督微调蒸馏的学生模型，评估它们与人类认知成本的对齐程度。使用相关系数量化模型推理难度与人类难度扩展的关系，识别'功能对齐崩溃'现象。关键创新在于揭示监督微调如何诱导学生模型仅复制推理的语言形式（如冗长），而未内化教师的动态资源分配策略，从而深入分析认知结构在蒸馏过程中的丢失机制。",
      "result": "实验结果显示，教师模型与人类难度扩展高度对齐，平均相关系数为0.64，显示出自然的认知成本匹配。然而，蒸馏后的学生模型对齐度显著降低至0.34，甚至部分模型表现低于蒸馏前基线，出现'负迁移'。这证实了'功能对齐崩溃'的存在，即推理蒸馏破坏了模型与人类认知的关联。与基线方法对比，监督微调不仅未提升认知对齐，反而导致性能退化，强调了现有蒸馏范式的局限性。",
      "conclusion": "论文结论强调，推理蒸馏中的监督微调引发'功能对齐崩溃'，使学生模型仅模仿推理表面形式，解耦计算成本和认知需求。核心贡献在于揭示类人认知是主动强化学习的涌现属性，而非被动模仿的产物。这一发现挑战了当前知识蒸馏范式，为改进AI推理能力和开发更有效的训练方法提供了理论指导。学术价值在于从认知视角深化对模型行为的理解，实际应用有助于优化模型部署。未来工作可探索替代蒸馏策略以维护认知对齐。",
      "tags": [
        "Large Language Models",
        "Reasoning Distillation",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "Cognitive Alignment"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:15.683551Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05017",
    "title": "HMVI: Unifying Heterogeneous Attributes with Natural Neighbors for Missing Value Inference",
    "authors": [
      "Xiaopeng Luo",
      "Zexi Tan",
      "Zhuowei Wang"
    ],
    "abstract": "Missing value imputation is a fundamental challenge in machine intelligence, heavily dependent on data completeness. Current imputation methods often handle numerical and categorical attributes independently, overlooking critical interdependencies among heterogeneous features. To address these limitations, we propose a novel imputation approach that explicitly models cross-type feature dependencies within a unified framework. Our method leverages both complete and incomplete instances to ensure accurate and consistent imputation in tabular data. Extensive experimental results demonstrate that the proposed approach achieves superior performance over existing techniques and significantly enhances downstream machine learning tasks, providing a robust solution for real-world systems with missing data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05017.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05017",
    "published": "2026-01-08T15:18:36Z",
    "updated": "2026-01-08T15:18:36Z",
    "comment": "Submitted to ICASSP 2026",
    "light_analysis": {
      "overview": "HMVI提出了一种新颖的缺失值推断方法，通过统一框架显式建模异构特征间的依赖关系，以解决现有方法忽视跨类型依赖的问题。",
      "motivation": "缺失值推断是机器学习和数据科学中的基础挑战，尤其在表格数据处理中至关重要，因为数据完整性直接影响模型性能。现有方法通常独立处理数值和类别属性，忽略了异构特征之间的关键相互依赖，导致推断结果不准确且一致性差，限制了在真实世界系统中的应用效果。本研究旨在克服这一局限性，通过开发一种能够统一建模跨类型特征依赖的新方法，提升数据预处理的质量和下游任务的可靠性。",
      "method": "HMVI方法的核心是一个统一框架，利用自然邻居技术来显式建模数值和类别属性之间的跨类型依赖关系。它同时考虑完整和不完整的实例，通过优化算法整合异构特征信息，确保在表格数据中的推断准确性和一致性。该方法创新性地避免了传统方法中特征分割带来的信息损失，提供了更全面的建模方式，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "广泛的实验验证表明，提出的方法在性能上显著优于现有缺失值推断技术，展现出更高的推断准确性和鲁棒性。实验结果显示，该方法能有效提升下游机器学习任务（如分类和回归）的效果，为真实世界系统提供了可靠的缺失数据处理方案，但摘要未明确说明具体的性能指标如准确率提升百分比。",
      "conclusion": "本研究的主要贡献是提出了HMVI，一种能够统一处理异构特征依赖的缺失值推断框架，填补了跨类型特征建模的空白。学术上，它推动了数据预处理和机器学习领域的发展；实际应用中，增强了系统在缺失数据下的鲁棒性和性能。未来工作可探索将该框架扩展到更多数据类型或复杂场景，以进一步提升其适用性。",
      "tags": [
        "Missing Value Imputation",
        "Heterogeneous Data",
        "Natural Neighbors",
        "Cross-type Dependencies",
        "Tabular Data"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:03.714193Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05009",
    "title": "An Empirical Investigation of Robustness in Large Language Models under Tabular Distortions",
    "authors": [
      "Avik Dutta",
      "Harshit Nigam",
      "Hosein Hasanbeig",
      "Arjun Radhakrishna",
      "Sumit Gulwani"
    ],
    "abstract": "We investigate how large language models (LLMs) fail when tabular data in an otherwise canonical representation is subjected to semantic and structural distortions. Our findings reveal that LLMs lack an inherent ability to detect and correct subtle distortions in table representations. Only when provided with an explicit prior, via a system prompt, do models partially adjust their reasoning strategies and correct some distortions, though not consistently or completely. To study this phenomenon, we introduce a small, expert-curated dataset that explicitly evaluates LLMs on table question answering (TQA) tasks requiring an additional error-correction step prior to analysis. Our results reveal systematic differences in how LLMs ingest and interpret tabular information under distortion, with even SoTA models such as GPT-5.2 model exhibiting a drop of minimum 22% accuracy under distortion. These findings raise important questions for future research, particularly regarding when and how models should autonomously decide to realign tabular inputs, analogous to human behavior, without relying on explicit prompts or tabular data pre-processing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05009.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05009",
    "published": "2026-01-08T15:10:32Z",
    "updated": "2026-01-08T15:10:32Z",
    "comment": "4 pages, 1 figure, 1 table",
    "light_analysis": {
      "overview": "本论文实证研究大型语言模型在表格数据语义和结构扭曲下的鲁棒性，发现模型缺乏自主检测和纠正扭曲的内在能力。",
      "motivation": "研究动机是探讨大型语言模型在处理表格数据时，当表格表示受到语义和结构扭曲时，如何表现失败。这一问题重要是因为LLMs在现实应用如表格问答中常面临不完美数据，现有方法不足，模型无法像人类一样自主纠正扭曲，导致性能瓶颈。这揭示了当前LLMs在处理复杂表格任务中的局限性，亟需研究其鲁棒性以提升实际可靠性。",
      "method": "研究方法包括引入一个专家策划的小型数据集，专门用于评估LLMs在表格问答任务中处理扭曲数据的能力，要求额外纠错步骤。通过系统提示作为显式先验知识，研究模型是否能调整推理策略来纠正扭曲。该方法使用SoTA模型如GPT-5.2进行实验，分析LLMs在摄入和解释扭曲表格信息时的系统性差异和关键创新点。",
      "result": "主要实验结果显示，LLMs在表格数据扭曲下准确率显著下降，即使SoTA模型如GPT-5.2也至少下降22%。与基线方法相比，只有在提供系统提示时，模型能部分纠正扭曲，但效果不一致或不完全。这些发现揭示了LLMs在扭曲环境下处理表格信息的系统性弱点，表明模型缺乏鲁棒性，无法完全自主应对数据扭曲。",
      "conclusion": "论文总结揭示了大型语言模型在表格扭曲下的鲁棒性不足，缺乏自主纠错能力，这一发现具有重要学术价值，强调未来研究需关注模型如何自主对齐表格输入，模仿人类行为而不依赖外部提示。潜在应用价值在于提升LLMs在实际表格处理任务中的可靠性，未来工作可探索更智能的模型设计或数据预处理方法。摘要未明确说明具体局限性，但暗示模型自主决策能力有待改进。",
      "tags": [
        "Large Language Models",
        "Table Question Answering",
        "Robustness",
        "Distortion Detection",
        "Error Correction"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:03.455586Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.13368",
    "title": "Donors and Recipients: On Asymmetric Transfer Across Tasks and Languages with Parameter-Efficient Fine-Tuning",
    "authors": [
      "Kajetan Dymkiewicz",
      "Ivan Vulic",
      "Helen Yannakoudakis",
      "Eilam Shapira",
      "Roi Reichart",
      "Anna Korhonen"
    ],
    "abstract": "Large language models (LLMs) perform strongly across tasks and languages, yet how improvements in one task or language affect other tasks and languages remains poorly understood. We conduct a controlled LoRA fine-tuning study across multiple open-weight LLM families and scales, using a standardised grid of 11 languages and four benchmarks. We fine-tune each model on a single task-language source and measure transfer when evaluated on all other task-language target pairs. We decompose transfer into three regimes: (i) Matched-Task (Cross-Language), (ii) Matched-Language (Cross-Task), and (iii) Cross-Task (Cross-Language). Single-source fine-tuning yields a net positive uplift across regimes, but the gains are strongly asymmetric. Matched-Task (Cross-Language) transfer emerges as the most effective and predictable regime, driven principally by the identity of the target language rather than model architecture. We identify a stable hierarchy where high-resource languages and broad semantic tasks act as efficient recipients that absorb gains from diverse sources, while specialised tasks and lower-resource languages are more isolated. These results imply that effective fine-tuning requires navigating donor-recipient roles to maximise downstream gains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.13368.pdf",
    "abs_url": "https://arxiv.org/abs/2511.13368",
    "published": "2025-11-17T13:41:31Z",
    "updated": "2026-01-08T15:10:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过系统化LoRA微调实验，揭示了任务和语言间的转移具有强烈不对称性，并提出了捐赠者-接收者层次结构以优化微调策略。",
      "motivation": "研究动机源于大型语言模型在多种任务和语言上表现优异，但微调一个任务或语言对其他任务和语言的影响尚不明确，导致现有微调策略可能无效或资源浪费。当前研究缺乏对这种转移的系统量化，因此本研究旨在填补空白，提供实证指导，以优化多任务多语言微调的实际应用。",
      "method": "研究方法采用参数高效的LoRA微调技术，设计控制性实验跨越多个开放权重的LLM家族和规模，使用标准化的11种语言和四个基准测试。通过在单个任务-语言源上微调模型，评估所有其他任务-语言目标对的转移效果，并将转移分解为匹配任务跨语言、匹配语言跨任务和跨任务跨语言三种机制。关键创新在于系统化分析转移的不对称性，提供标准化框架以探索微调策略。",
      "result": "实验结果显示，单源微调在所有转移机制中均带来净正提升，但收益呈强烈不对称。匹配任务跨语言转移最为有效和可预测，主要由目标语言的身份驱动，而非模型架构。研究识别了稳定层次结构：高资源语言和宽泛语义任务能高效吸收多样源收益，而专门任务和低资源语言更孤立，量化了转移的不对称性，为微调优化提供了实证支持。",
      "conclusion": "论文的主要贡献在于系统揭示任务和语言间转移的不对称性，提出捐赠者-接收者框架，具有重要学术价值，深化了对微调机制的理解，为多任务多语言优化提供理论指导。实际应用上，有助于选择微调源以最大化下游性能，未来工作可扩展到更多任务、语言和模型架构以验证其通用性。",
      "tags": [
        "LoRA Fine-Tuning",
        "Parameter-Efficient Fine-Tuning",
        "Asymmetric Transfer",
        "Cross-Task Transfer",
        "Cross-Language Transfer"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:55.983966Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05002",
    "title": "On the Hidden Objective Biases of Group-based Reinforcement Learning",
    "authors": [
      "Aleksandar Fontana",
      "Marco Simoni",
      "Giulio Rossolini",
      "Andrea Saracino",
      "Paolo Mori"
    ],
    "abstract": "Group-based reinforcement learning methods, like Group Relative Policy Optimization (GRPO), are widely used nowadays to post-train large language models. Despite their empirical success, they exhibit structural mismatches between reward optimization and the underlying training objective. In this paper, we present a theoretical analysis of GRPO style methods by studying them within a unified surrogate formulation. This perspective reveals recurring properties that affect all the methods under analysis: (i) non-uniform group weighting induces systematic gradient biases on shared prefix tokens; (ii) interactions with the AdamW optimizer make training dynamics largely insensitive to reward scaling; and (iii) optimizer momentum can push policy updates beyond the intended clipping region under repeated optimization steps. We believe that these findings highlight fundamental limitations of current approaches and provide principled guidance for the design of future formulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05002.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05002",
    "published": "2026-01-08T15:00:35Z",
    "updated": "2026-01-08T15:00:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过理论分析揭示群组强化学习方法中的隐藏目标偏差，为未来方法设计提供原则性指导。",
      "motivation": "群组强化学习方法（如GRPO）广泛应用于大语言模型的后期训练，尽管经验上成功，但存在奖励优化与训练目标之间的结构不匹配问题。当前方法缺乏系统性的理论分析，可能导致训练不稳定或效率低下，限制了其在实际应用中的可靠性。本研究旨在通过揭示这些隐藏偏差，弥补理论与实践的鸿沟，推动更健壮的强化学习算法发展。",
      "method": "本研究采用理论分析方法，通过统一的代理公式来研究GRPO风格方法。核心创新在于将不同方法纳入一个框架，揭示其共同特性。关键步骤包括构建数学模型，分析非均匀组权重、优化器交互和动量效应。使用GRPO作为案例，但结果适用于类似群组方法，未涉及具体数据集或模型架构，专注于理论推导。",
      "result": "研究发现三个重复出现的属性：非均匀组权重导致共享前缀令牌上的系统梯度偏差；与AdamW优化器的交互使训练动态对奖励缩放不敏感；优化器动量在重复优化步骤中可能推动策略更新超出预期裁剪区域。这些发现揭示了当前方法的内在局限性，为评估和改进提供了理论依据。摘要未明确说明具体实验数据。",
      "conclusion": "论文通过理论分析揭示了群组强化学习方法中的隐藏目标偏差，强调了现有方法的局限性。主要贡献在于提供一个统一视角来理解这些偏差，并为未来公式设计提供原则性指导。学术价值在于深化强化学习理论理解，实际应用价值在于帮助开发更稳定和高效的语言模型训练方法。未来工作可基于这些发现设计新优化策略或扩展分析到其他领域。",
      "tags": [
        "Group-based Reinforcement Learning",
        "Group Relative Policy Optimization",
        "AdamW Optimizer",
        "Gradient Bias",
        "Reward Scaling"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:19.346400Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2402.10424",
    "title": "Pelican Soup Framework: A Theoretical Framework for Language Model Capabilities",
    "authors": [
      "Ting-Rui Chiang",
      "Dani Yogatama"
    ],
    "abstract": "In this work, we propose a simple theoretical framework, Pelican Soup, aiming to better understand how pretraining allows LLMs to (1) generalize to unseen instructions and (2) perform in-context learning, even when the verbalizers are irrelevant to the task. To this end, in our framework, we introduce the notion of \"knowledge base\" and \"reference-sense association\" and a simple formalism for natural language processing tasks. Our framework demonstrates how linguistic, psychology, and philosophy studies can inform our understanding of the language model and is connected to several other existing theoretical results. As an illustration of the usage of our framework, we derive a bound on in-context learning loss with our framework. Finally, we support our framework with empirical experiments and provide possible future research directions.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2402.10424.pdf",
    "abs_url": "https://arxiv.org/abs/2402.10424",
    "published": "2024-02-16T03:20:14Z",
    "updated": "2026-01-08T14:58:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了名为Pelican Soup的理论框架，旨在解释大型语言模型的泛化和上下文学习能力。",
      "motivation": "研究动机是解决预训练模型如何推广到未见过的指令和实现上下文学习的问题，即使言语表达与任务无关。这个问题对理解模型适应性和改进模型设计至关重要，因为现有方法缺乏整合多学科见解的理论解释，导致对模型能力的理解不足。",
      "method": "研究方法包括引入“知识库”和“参考意义关联”等概念，并构建简单的自然语言处理任务形式化表示。关键创新在于将语言学、心理学和哲学研究融入理论框架，连接现有理论结果，以提供对模型能力的理论支撑。摘要未明确说明具体数据集或模型架构，但强调通过形式化过程推导损失边界。",
      "result": "主要实验结果包括推导出上下文学习损失的边界，并通过实证实验支持框架的有效性。摘要未提供具体性能指标或与基线方法的对比数据，但表示实验验证了理论预测，增强了框架的可信度，表明框架在解释模型能力方面的实用性。",
      "conclusion": "结论总结：该研究的主要贡献是提供了一个理论框架来解释大型语言模型的能力，具有学术价值在于连接多学科知识，可能指导实际模型优化。未来研究方向已在摘要中提及，如进一步实证验证和框架扩展，但未明确说明局限性。",
      "tags": [
        "Theoretical Framework",
        "Large Language Models",
        "In-context Learning",
        "Pre-training",
        "Formalization"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:21.103365Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04996",
    "title": "AlgBench: To What Extent Do Large Reasoning Models Understand Algorithms?",
    "authors": [
      "Henan Sun",
      "Kaichi Yu",
      "Yuyao Wang",
      "Bowen Liu",
      "Xunkai Li",
      "Rong-Hua Li",
      "Nuo Chen",
      "Jia Li"
    ],
    "abstract": "Reasoning ability has become a central focus in the advancement of Large Reasoning Models (LRMs). Although notable progress has been achieved on several reasoning benchmarks such as MATH500 and LiveCodeBench, existing benchmarks for algorithmic reasoning remain limited, failing to answer a critical question: Do LRMs truly master algorithmic reasoning? To answer this question, we propose AlgBench, an expert-curated benchmark that evaluates LRMs under an algorithm-centric paradigm.   AlgBench consists of over 3,000 original problems spanning 27 algorithms, constructed by ACM algorithmic experts and organized under a comprehensive taxonomy, including Euclidean-structured, non-Euclidean-structured, non-optimized, local-optimized, global-optimized, and heuristic-optimized categories. Empirical evaluations on leading LRMs (e.g., Gemini-3-Pro, DeepSeek-v3.2-Speciale and GPT-o3) reveal substantial performance heterogeneity: while models perform well on non-optimized tasks (up to 92%), accuracy drops sharply to around 49% on globally optimized algorithms such as dynamic programming. Further analysis uncovers \\textbf{strategic over-shifts}, wherein models prematurely abandon correct algorithmic designs due to necessary low-entropy tokens. These findings expose fundamental limitations of problem-centric reinforcement learning and highlight the necessity of an algorithm-centric training paradigm for robust algorithmic reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04996.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04996",
    "published": "2026-01-08T14:54:44Z",
    "updated": "2026-01-08T14:54:44Z",
    "comment": "Under review",
    "light_analysis": {
      "overview": "本文提出了AlgBench基准，用于评估大型推理模型在算法推理方面的真正理解能力。",
      "motivation": "研究动机在于大型推理模型的推理能力成为发展焦点，但现有基准如MATH500和LiveCodeBench在算法推理评估上存在局限，无法确定模型是否真正掌握算法推理。算法推理是复杂问题解决的关键环节，现有方法缺乏专门的算法中心评估范式，导致评估不全面，因此需要新的基准来系统回答这一问题。",
      "method": "论文提出AlgBench，一个由ACM算法专家策划的基准，包含超过3000个原始问题，覆盖27种算法，并组织在全面分类下，如Euclidean-structured、非优化和全局优化类别。评估采用算法中心范式，使用领先的LRMs如Gemini-3-Pro进行测试，通过构造多样化问题来深入探究模型对算法的理解能力。",
      "result": "实验结果显示性能异质性：在非优化任务上，模型准确率高达92%，但在全局优化算法如动态规划上降至约49%。分析发现战略过移现象，模型因低熵令牌过早放弃正确算法设计，这暴露了问题中心强化学习的局限性，并凸显不同算法类别间表现差异显著。",
      "conclusion": "研究贡献在于提出了AlgBench基准，揭示了大型推理模型在算法推理上的根本限制，强调了算法中心训练范式的必要性。这为提升模型稳健推理能力提供了学术指导，具有实际应用价值；未来工作可探索改进训练方法以克服这些限制。",
      "tags": [
        "Large Reasoning Models",
        "Algorithmic Reasoning",
        "Benchmark Evaluation",
        "Dynamic Programming",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:21.120985Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.14361",
    "title": "Clinically-Validated Innovative Mobile Application for Assessing Blinking and Eyelid Movements",
    "authors": [
      "Gustavo Adolpho Bonesso",
      "Carlos Marcelo Gurjão de Godoy",
      "Tammy Hentona Osaki",
      "Midori Hentona Osaki",
      "Bárbara Moreira Ribeiro Trindade dos Santos",
      "Juliana Yuka Washiya",
      "Regina Célia Coelho"
    ],
    "abstract": "Blinking is a vital physiological process that protects and maintains the health of the ocular surface. Objective assessment of eyelid movements remains challenging due to the complexity, cost, and limited clinical applicability of existing tools. This study presents the Bapp (Blink Application), a mobile application developed using the Flutter framework and integrated with Google ML Kit for on-device, real-time analysis of eyelid movements, and its clinical validation. The validation was performed using 45 videos from patients, whose blinks were manually annotated by an ophthalmology specialist as the ground truth. The Bapp's performance was evaluated using standard metrics, with results demonstrating 98.4% precision, 96.9% recall, and an overall accuracy of 98.3%. These outcomes confirm the reliability of the Bapp as a portable, accessible, and objective tool for monitoring eyelid movements. The application offers a promising alternative to traditional manual blink counting, supporting continuous ocular health monitoring and postoperative evaluation in clinical environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.14361.pdf",
    "abs_url": "https://arxiv.org/abs/2511.14361",
    "published": "2025-11-18T11:07:31Z",
    "updated": "2026-01-08T14:35:09Z",
    "comment": "20 pages, 13 figures",
    "light_analysis": {
      "overview": "本研究提出并临床验证了Bapp移动应用，用于实时分析眨眼和眼睑运动，提供便携、客观的评估工具。",
      "motivation": "眨眼是保护眼表健康的关键生理过程，但现有眼睑运动评估工具因复杂性高、成本昂贵和临床应用受限，导致客观评估困难。传统手动计数方法效率低且主观性强，难以支持连续眼健康监测和术后评估，因此在临床环境中急需开发一种易于访问、便携且可靠的替代方案，以提升诊断准确性和患者护理质量。",
      "method": "Bapp是一款基于Flutter框架开发的移动应用，集成了Google ML Kit进行设备上实时眼睑运动分析，核心创新在于无需额外硬件即可实现客观评估。临床验证使用45个患者视频，由眼科专家手动标注作为基准，确保方法的准确性和可靠性，通过标准指标评估性能。",
      "result": "Bapp在45个视频数据集上验证，与专家手动标注对比，达到98.4%的精度、96.9%的召回率和98.3%的整体准确率，这些高性能指标证实了其作为客观工具的可靠性，显著优于传统手动方法，支持在临床环境中高效监测眼睑运动。",
      "conclusion": "Bapp作为一种便携、易访问和客观的工具，可靠地支持眼睑运动评估，为传统手动眨眼计数提供了有前景的替代方案，具有重要临床应用价值，如连续眼健康监测和术后评估。摘要未明确说明局限性，未来工作可扩展数据集验证或集成更多功能以增强适用性。",
      "tags": [
        "Mobile Application",
        "Flutter",
        "Google ML Kit",
        "Real-time Analysis",
        "Clinical Validation"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:17.762268Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.18770",
    "title": "Reward Shaping to Mitigate Reward Hacking in RLHF",
    "authors": [
      "Jiayi Fu",
      "Xuandong Zhao",
      "Chengyuan Yao",
      "Heng Wang",
      "Qi Han",
      "Yanghua Xiao"
    ],
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human values. However, RLHF is susceptible to \\emph{reward hacking}, where the agent exploits flaws in the reward function rather than learning the intended behavior, thus degrading alignment. Although reward shaping helps stabilize RLHF and partially mitigate reward hacking, a systematic investigation into shaping techniques and their underlying principles remains lacking. To bridge this gap, we present a comprehensive study of the prevalent reward shaping methods. Our analysis suggests two key design principles: (1) the RL reward should be bounded, and (2) the RL reward benefits from rapid initial growth followed by gradual convergence. Guided by these insights, we propose Preference As Reward (PAR), a novel approach that leverages the latent preferences embedded within the reward model as the signal for reinforcement learning. Moreover, PAR exhibits two critical variance-reduction properties that contribute to stabilizing the RLHF training process and effectively extending the tolerance window for early stopping. We evaluated PAR on the base model Gemma2-2B using two datasets, Ultrafeedback-Binarized and HH-RLHF. Experimental results demonstrate PAR's superior performance over other reward shaping methods. On the AlpacaEval 2.0 benchmark, PAR achieves a win rate of at least 5 percentage points higher than competing approaches. Furthermore, PAR exhibits remarkable data efficiency, requiring only a single reference reward for optimal performance, and maintains robustness against reward hacking even after two full epochs of training. The code is available at https://github.com/PorUna-byte/PAR.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.18770.pdf",
    "abs_url": "https://arxiv.org/abs/2502.18770",
    "published": "2025-02-26T02:57:59Z",
    "updated": "2026-01-08T14:33:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Preference As Reward (PAR)方法，通过利用奖励模型的潜在偏好来缓解RLHF中的奖励黑客问题，并提升训练稳定性和数据效率。",
      "motivation": "本研究旨在解决Reinforcement Learning from Human Feedback (RLHF)中的奖励黑客问题，即代理通过利用奖励函数缺陷而非学习真实行为，破坏大型语言模型与人类价值观的对齐。现有奖励塑造方法虽能部分缓解此问题并稳定RLHF，但缺乏系统性的技术分析和原理探讨，限制了其效能的进一步提升，因此需深入研究和提出更有效的解决方案。",
      "method": "论文基于两个设计原则（RL奖励应有界和需快速初始增长后逐渐收敛），提出Preference As Reward (PAR)方法，将奖励模型中嵌入的潜在偏好用作强化学习信号。该方法具有两种关键方差减少属性，能稳定RLHF训练过程并扩展早期停止容忍窗口。在Gemma2-2B基础模型上，使用Ultrafeedback-Binarized和HH-RLHF数据集进行评估，核心创新在于系统性整合奖励塑造技术以提高对齐效果。",
      "result": "实验结果显示，PAR在AlpacaEval 2.0基准测试中胜率比其他奖励塑造方法至少高出5个百分点，表现出显著的性能优势。此外，PAR具备优异的数据效率，仅需单一参考奖励即可达到最佳性能，并且在两个完整训练周期后仍能保持对奖励黑客的鲁棒性，验证了其稳定性和有效性。",
      "conclusion": "本研究通过系统性分析奖励塑造方法，提出了基于设计原则的PAR方法，有效缓解了RLHF中的奖励黑客问题，提升了大型语言模型对齐的稳定性和效率。其学术价值在于深化了对奖励塑造技术的理解，实际应用价值在于可提高模型训练的可靠性和数据利用率。未来工作可探索PAR在其他模型或数据集上的泛化性和进一步优化空间。",
      "tags": [
        "Reinforcement Learning from Human Feedback",
        "Reward Shaping",
        "Reward Hacking",
        "Preference Learning",
        "Variance Reduction"
      ]
    },
    "analyzed_at": "2026-01-09T03:05:21.723518Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04977",
    "title": "On the Definition and Detection of Cherry-Picking in Counterfactual Explanations",
    "authors": [
      "James Hinns",
      "Sofie Goethals",
      "Stephan Van der Veeken",
      "Theodoros Evgeniou",
      "David Martens"
    ],
    "abstract": "Counterfactual explanations are widely used to communicate how inputs must change for a model to alter its prediction. For a single instance, many valid counterfactuals can exist, which leaves open the possibility for an explanation provider to cherry-pick explanations that better suit a narrative of their choice, highlighting favourable behaviour and withholding examples that reveal problematic behaviour. We formally define cherry-picking for counterfactual explanations in terms of an admissible explanation space, specified by the generation procedure, and a utility function. We then study to what extent an external auditor can detect such manipulation. Considering three levels of access to the explanation process: full procedural access, partial procedural access, and explanation-only access, we show that detection is extremely limited in practice. Even with full procedural access, cherry-picked explanations can remain difficult to distinguish from non cherry-picked explanations, because the multiplicity of valid counterfactuals and flexibility in the explanation specification provide sufficient degrees of freedom to mask deliberate selection. Empirically, we demonstrate that this variability often exceeds the effect of cherry-picking on standard counterfactual quality metrics such as proximity, plausibility, and sparsity, making cherry-picked explanations statistically indistinguishable from baseline explanations. We argue that safeguards should therefore prioritise reproducibility, standardisation, and procedural constraints over post-hoc detection, and we provide recommendations for algorithm developers, explanation providers, and auditors.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04977.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04977",
    "published": "2026-01-08T14:29:24Z",
    "updated": "2026-01-08T14:29:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出反事实解释中cherry-picking的形式化定义，并证明其检测在实际中极为困难，强调程序约束的重要性。",
      "motivation": "研究动机源于反事实解释可能存在多个有效解释，使得解释提供者可能选择性展示有利解释，掩盖模型问题行为。现有方法未充分解决检测这种操纵的机制，因为解释多样性导致选择偏颇，影响模型透明度和可信度，揭示现有解释框架在防范滥用方面的不足。",
      "method": "研究方法包括基于生成过程指定的可接受解释空间和效用函数来定义cherry-picking，并探索外部审计者在不同访问级别（完全程序访问、部分程序访问、仅解释访问）下的检测能力。使用标准反事实质量指标如proximity、plausibility和sparsity进行实证评估，分析变量对检测效果的影响。",
      "result": "实验结果显示，即使在完全程序访问条件下，cherry-picked解释也很难与非cherry-picked解释区分开，因为有效反事实的多样性和解释规范的灵活性提供足够自由度来掩盖故意选择，使cherry-picking在标准质量指标上统计上不可区分，例如在proximity、plausibility和sparsity上的差异不显著。",
      "conclusion": "结论指出cherry-picking检测具有局限性，应优先通过可重复性、标准化和程序约束来预防操纵，而非依赖事后检测。论文为算法开发者、解释提供者和审计者提供建议，提升反事实解释的公正性和可靠性，具有推动可解释AI标准化的学术和实际价值。",
      "tags": [
        "Counterfactual Explanations",
        "Cherry-Picking",
        "Explainability",
        "Utility Function",
        "Procedural Access"
      ]
    },
    "analyzed_at": "2026-01-09T02:28:49.453330Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04973",
    "title": "ConMax: Confidence-Maximizing Compression for Efficient Chain-of-Thought Reasoning",
    "authors": [
      "Minda Hu",
      "Zexuan Qiu",
      "Zenan Xu",
      "Kun Li",
      "Bo Zhou",
      "Irwin King"
    ],
    "abstract": "Recent breakthroughs in Large Reasoning Models (LRMs) have demonstrated that extensive Chain-of-Thought (CoT) generation is critical for enabling intricate cognitive behaviors, such as self-verification and backtracking, to solve complex tasks. However, this capability often leads to ``overthinking'', where models generate redundant reasoning paths that inflate computational costs without improving accuracy. While Supervised Fine-Tuning (SFT) on reasoning traces is a standard paradigm for the 'cold start' phase, applying existing compression techniques to these traces often compromises logical coherence or incurs prohibitive sampling costs. In this paper, we introduce ConMax (Confidence-Maximizing Compression), a novel reinforcement learning framework designed to automatically compress reasoning traces while preserving essential reasoning patterns. ConMax formulates compression as a reward-driven optimization problem, training a policy to prune redundancy by maximizing a weighted combination of answer confidence for predictive fidelity and thinking confidence for reasoning validity through a frozen auxiliary LRM. Extensive experiments across five reasoning datasets demonstrate that ConMax achieves a superior efficiency-performance trade-off. Specifically, it reduces inference length by 43% over strong baselines at the cost of a mere 0.7% dip in accuracy, proving its effectiveness in generating high-quality, efficient training data for LRMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04973.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04973",
    "published": "2026-01-08T14:22:58Z",
    "updated": "2026-01-08T14:22:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出ConMax，一种基于强化学习的置信度最大化压缩框架，用于高效压缩Chain-of-Thought推理轨迹以提升效率。",
      "motivation": "大推理模型（LRMs）中的Chain-of-Thought生成对复杂任务至关重要，但常导致'过度思考'，产生冗余推理路径，增加计算成本而不提升准确性。现有的监督微调方法在压缩推理轨迹时，要么损害逻辑连贯性，要么采样成本过高。因此，需要一种高效压缩方法以在保持推理质量的同时优化效率，解决现有技术平衡效率与性能的不足。",
      "method": "ConMax采用强化学习框架，将压缩问题转化为奖励驱动的优化。通过最大化答案置信度（用于预测保真度）和思维置信度（用于推理有效性）的加权组合，训练一个策略来剪枝冗余推理。关键创新是使用冻结的辅助LRM评估置信度，确保压缩后轨迹保持逻辑连贯性，实现自动压缩而无需人工干预，核心是优化奖励函数以平衡压缩与质量。",
      "result": "在五个推理数据集上的实验表明，ConMax实现了优越的效率性能平衡。具体地，推理长度比强基线减少43%，精度仅下降0.7%。这证明了ConMax能生成高质量、高效的训练数据，显著降低推理成本，优于现有压缩方法，展示了其在保持高准确性的同时大幅提升效率的能力。",
      "conclusion": "本研究提出了ConMax，通过置信度最大化压缩优化Chain-of-Thought推理效率，主要贡献在于提供自动压缩方法，生成高质量训练数据，提升大推理模型的实用性。该研究提高了推理模型的训练效率，促进了复杂认知任务的发展。潜在局限性包括对辅助模型的依赖，未来工作可探索在其他任务中的应用或优化置信度评估策略。",
      "tags": [
        "Reinforcement Learning",
        "Chain-of-Thought Reasoning",
        "Confidence Maximization",
        "Model Compression",
        "Large Reasoning Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:25.213431Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.24957",
    "title": "AMAP Agentic Planning Technical Report",
    "authors": [
      "AMAP AI Agent Team",
      "Yulan Hu",
      "Xiangwen Zhang",
      "Sheng Ouyang",
      "Hao Yi",
      "Lu Xu",
      "Qinglin Lang",
      "Lide Tan",
      "Xiang Cheng",
      "Tianchen Ye",
      "Zhicong Li",
      "Ge Chen",
      "Wenjin Yang",
      "Zheng Pan",
      "Shaopan Xiong",
      "Siran Yang",
      "Ju Huang",
      "Yan Zhang",
      "Jiamang Wang",
      "Yong Liu",
      "Yinfeng Huang",
      "Ning Wang",
      "Tucheng Lin",
      "Xin Li",
      "Ning Guo"
    ],
    "abstract": "We present STAgent, an agentic large language model tailored for spatio-temporal understanding, designed to solve complex tasks such as constrained point-of-interest discovery and itinerary planning. STAgent is a specialized model capable of interacting with ten distinct tools within spatio-temporal scenarios, enabling it to explore, verify, and refine intermediate steps during complex reasoning. Notably, STAgent effectively preserves its general capabilities. We empower STAgent with these capabilities through three key contributions: (1) a stable tool environment that supports over ten domain-specific tools, enabling asynchronous rollout and training; (2) a hierarchical data curation framework that identifies high-quality data like a needle in a haystack, curating high-quality queries by retaining less than 1\\% of the raw data, emphasizing both diversity and difficulty; and (3) a cascaded training recipe that starts with a seed SFT stage acting as a guardian to measure query difficulty, followed by a second SFT stage fine-tuned on queries with high certainty, and an ultimate RL stage that leverages data of low certainty. Initialized with Qwen3-30B-A3B to establish a strong SFT foundation and leverage insights into sample difficulty, STAgent yields promising performance on TravelBench while maintaining its general capabilities across a wide range of general benchmarks, thereby demonstrating the effectiveness of our proposed agentic model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.24957.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24957",
    "published": "2025-12-31T16:39:09Z",
    "updated": "2026-01-08T14:15:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出STAgent，一个代理性大型语言模型，通过工具交互和级联训练解决时空复杂任务，如兴趣点发现和行程规划。",
      "motivation": "本研究旨在解决时空场景下的复杂任务，如约束的兴趣点发现和行程规划，这些任务需要高级推理和交互能力。现有大型语言模型可能在处理特定领域任务时通用性不足或缺乏专业工具支持，导致任务效率低下或准确性受限。STAgent的动机是开发一个既能保留通用能力，又能专门处理时空任务的代理模型，以弥补现有方法在任务特定性、工具集成和数据处理效率上的不足，从而提升复杂环境下的实际应用效果。",
      "method": "STAgent模型的核心方法包括三个关键贡献：首先，构建了一个稳定的工具环境，支持超过十个时空特定工具，实现异步推出和训练；其次，采用分层数据整理框架，从原始数据中筛选出少于1%的高质量查询，强调多样性和难度，以优化数据效率；最后，设计了一个级联训练配方，包括种子SFT阶段衡量查询难度、第二个SFT阶段微调高确定性查询和RL阶段利用低确定性数据。模型基于Qwen3-30B-A3B初始化，通过监督微调和强化学习进行训练，确保专业性能的同时保持通用能力。",
      "result": "STAgent在TravelBench数据集上展现出有希望的性能，表明其在时空任务处理上的有效性，尽管摘要未提供具体性能指标如准确率或效率改进。同时，模型在广泛通用基准上保持了其原始能力，证明了所提方法的普适性和稳定性，与基线方法对比（摘要未明确说明具体对比），可以推断STAgent在专向任务上有所提升，同时避免了通用能力的退化。",
      "conclusion": "论文的主要贡献是开发了STAgent，一个结合工具交互、数据整理和级联训练的代理性大型语言模型。该模型有效解决了时空复杂任务，并在保持通用能力方面表现出色，具有显著的学术价值，如推动代理模型和时空AI研究，以及实际应用价值，如智能旅游规划和实时决策支持。未来工作方向可能包括扩展到更多领域或进一步优化训练效率，但摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Agentic Planning",
        "Spatio-temporal Understanding",
        "Supervised Fine-tuning",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:28.841001Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04963",
    "title": "Text as a Universal Interface for Transferable Personalization",
    "authors": [
      "Yuting Liu",
      "Jian Guan",
      "Jia-Nan Li",
      "Wei Wu",
      "Jiang-Ming Yang",
      "Jianzhe Zhao",
      "Guibing Guo"
    ],
    "abstract": "We study the problem of personalization in large language models (LLMs). Prior work predominantly represents user preferences as implicit, model-specific vectors or parameters, yielding opaque ``black-box'' profiles that are difficult to interpret and transfer across models and tasks. In contrast, we advocate natural language as a universal, model- and task-agnostic interface for preference representation. The formulation leads to interpretable and reusable preference descriptions, while naturally supporting continual evolution as new interactions are observed. To learn such representations, we introduce a two-stage training framework that combines supervised fine-tuning on high-quality synthesized data with reinforcement learning to optimize long-term utility and cross-task transferability. Based on this framework, we develop AlignXplore+, a universal preference reasoning model that generates textual preference summaries. Experiments on nine benchmarks show that our 8B model achieves state-of-the-art performanc -- outperforming substantially larger open-source models -- while exhibiting strong transferability across tasks, model families, and interaction formats.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04963.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04963",
    "published": "2026-01-08T14:09:17Z",
    "updated": "2026-01-08T14:09:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出使用自然语言作为通用接口来表示大语言模型中的用户偏好，实现可解释和可转移的个性化。",
      "motivation": "研究动机在于解决大语言模型中用户偏好表示的不透明性问题。现有方法主要使用隐式的、模型特定的向量或参数来表示偏好，导致‘黑盒’配置文件难以解释且无法在不同模型和任务间迁移。这种局限性限制了个人化系统的实际应用，因为用户偏好无法被直观理解和灵活调整，影响了系统的通用性和可持续改进。通过采用自然语言作为表示接口，研究旨在提升偏好的可解释性、可重用性和持续演化能力。",
      "method": "论文采用两阶段训练框架来学习文本偏好表示。首先，在高质量合成数据上进行监督微调，以生成初步的偏好描述；然后，结合强化学习优化这些描述的长期效用和跨任务可转移性。基于此框架，开发了AlignXplore+模型，该模型能够生成可解释的文本摘要来表示用户偏好，利用自然语言作为模型和任务无关的接口。关键创新点在于将文本表示与强化学习结合，以平衡短期性能与长期适应性。",
      "result": "在九个基准测试上的实验表明，论文提出的8B参数模型实现了最先进的性能，优于参数更大的开源模型。模型展现出强大的可转移性，能够无缝适应不同的任务、模型家族和交互格式。具体数据未在摘要中明确提供，但结果证明了文本表示方法在提升个性化效果和泛化能力方面的有效性，为后续研究提供了实证支持。",
      "conclusion": "论文的主要贡献是验证了自然语言作为通用接口在用户偏好表示中的有效性，解决了现有方法的可解释性和可转移性问题。研究具有重要的学术价值，推动了个人化AI系统向更透明和灵活的方向发展，并具有实际应用潜力，例如在跨平台推荐系统中。未来工作可探索更复杂的优化策略或扩展应用到更多任务类型，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Personalization",
        "Natural Language Representation",
        "Reinforcement Learning",
        "Supervised Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-09T02:32:48.266077Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04954",
    "title": "Precision over Diversity: High-Precision Reward Generalizes to Robust Instruction Following",
    "authors": [
      "Yirong Zeng",
      "Yufei Liu",
      "Xiao Ding",
      "Yutai Hou",
      "Yuxian Wang",
      "Haonan Song",
      "Wu Ning",
      "Dandan Tu",
      "Qixun Zhang",
      "Bibo Cai",
      "Yuxiang He",
      "Ting Liu"
    ],
    "abstract": "A central belief in scaling reinforcement learning with verifiable rewards for instruction following (IF) tasks is that, a diverse mixture of verifiable hard and unverifiable soft constraints is essential for generalizing to unseen instructions. In this work, we challenge this prevailing consensus through a systematic empirical investigation. Counter-intuitively, we find that models trained on hard-only constraints consistently outperform those trained on mixed datasets. Extensive experiments reveal that reward precision, rather than constraint diversity, is the primary driver of effective alignment. The LLM judge suffers from a low recall rate in detecting false response, which leads to severe reward hacking, thereby undermining the benefits of diversity. Furthermore, analysis of the attention mechanism reveals that high-precision rewards develop a transferable meta-skill for IF. Motivated by these insights, we propose a simple yet effective data-centric refinement strategy that prioritizes reward precision. Evaluated on five benchmarks, our approach outperforms competitive baselines by 13.4\\% in performance while achieving a 58\\% reduction in training time, maintaining strong generalization beyond instruction following. Our findings advocate for a paradigm shift: moving away from the indiscriminate pursuit of data diversity toward high-precision rewards.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04954.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04954",
    "published": "2026-01-08T14:00:51Z",
    "updated": "2026-01-08T14:00:51Z",
    "comment": "ACL under review 13 pages, 8 figures",
    "light_analysis": {
      "overview": "本论文挑战了强化学习中奖励多样性的共识，提出高精度奖励更能促进指令跟随的泛化能力，并引入了一个简单有效的数据中心优化策略。",
      "motivation": "研究动机源于强化学习中指令跟随任务的奖励设计问题。现有方法普遍认为，结合可验证硬约束和不可验证软约束的多样性奖励对泛化至关重要。然而，LLM判断器在检测错误响应时的低召回率导致奖励黑客问题，反而损害了多样性带来的好处，这使得探索奖励精度是否更关键变得重要，以优化训练效果和泛化能力。",
      "method": "论文采用系统性实证研究，比较了仅使用硬约束和混合约束训练的模型性能。分析发现奖励精度是主要驱动力，而非约束多样性。基于此，提出一个数据中心细化策略，通过优先选择高精度奖励样本来优化训练数据。该方法在五个基准上评估，利用现有强化学习框架，并分析注意力机制以开发可转移的元技能，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验结果表明，仅使用硬约束训练的模型在指令跟随任务中表现优于混合约束模型。提出的高精度奖励优先策略在五个基准测试中，性能提升了13.4%，训练时间减少了58%。与竞争基线相比，该方法展现出强大的泛化能力，超越了指令跟随任务的范围，显著提高了效率和效果。",
      "conclusion": "结论强调奖励精度对强化学习中指令跟随泛化的核心作用，挑战了传统多样性共识。论文贡献在于提出数据中心细化策略，促进向高精度奖励的范式转变。学术上，为奖励设计提供新视角；实践中，提升模型性能和效率。未来工作可进一步研究奖励精度的量化方法和应用范围，摘要未明确说明潜在局限性。",
      "tags": [
        "Reinforcement Learning",
        "Instruction Following",
        "Reward Precision",
        "Data-Centric Strategy",
        "LLM Judge"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:00.818340Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.01802",
    "title": "PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism AI Psychological Counselor",
    "authors": [
      "Qianjun Pan",
      "Junyi Wang",
      "Jie Zhou",
      "Yutao Yang",
      "Junsong Li",
      "Kaiyin Xu",
      "Yougen Zhou",
      "Yihan Li",
      "Jingyuan Zhao",
      "Qin Chen",
      "Ningning Zhou",
      "Kai Chen",
      "Liang He"
    ],
    "abstract": "To develop a reliable AI for psychological assessment, we introduce \\texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \\textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \\textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \\textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \\texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.01802.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01802",
    "published": "2026-01-05T05:26:57Z",
    "updated": "2026-01-08T13:52:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了PsychEval基准，用于开发和评估高真实性的多会话、多疗法AI心理辅导员。",
      "motivation": "研究旨在解决训练高真实性AI心理辅导员的挑战。当前方法通常只专注于单一疗法，无法满足复杂心理案例中需要多种疗法策略的需求。心理辅导是一个纵向任务，需要持续记忆和动态目标跟踪，而现有基准缺乏多会话和多疗法的设定，导致AI难以适应真实临床环境。开发全面的基准对于提高AI在心理评估中的可靠性至关重要，填补了这一领域的空白。",
      "method": "研究方法涉及构建PsychEval基准，包括多会话（跨越6-10个会话的三个阶段）和多疗法的数据集，覆盖五种疗法模式和一个整合疗法。数据集标注了677个元技能和4577个原子技能，支持记忆连续性、自适应推理等关键能力。此外，建立了一个整体评估框架，包含18个疗法特异和共享的度量标准，覆盖客户级和辅导员级维度，并构建了超过2000个多样客户档案作为支撑。",
      "result": "实验分析表明，PsychEval数据集在质量和临床保真度上表现出优越性，有效验证了其作为基准的价值。然而，摘要未明确说明具体的性能指标（如准确率提升或效率改进）以及与基线方法的详细对比数据。研究通过广泛实验验证了数据集的可靠性，但缺少量化结果支撑。",
      "conclusion": "PsychEval不仅是一个静态基准，更是一个高保真的强化学习环境，支持AI心理辅导员的自进化训练。它推动了临床负责和自适应AI的发展，具有重要学术价值和实际应用潜力。潜在局限性在于数据集的规模和多样性可能有限，未来工作可扩展更多疗法类型或增加会话复杂性，以进一步提升实用性。",
      "tags": [
        "Psychological AI Benchmark",
        "Multi-Session Counseling",
        "Multi-Therapy Dataset",
        "Reinforcement Learning Environment",
        "Clinical Evaluation Framework"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:51.927494Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04946",
    "title": "Prototypicality Bias Reveals Blindspots in Multimodal Evaluation Metrics",
    "authors": [
      "Subhadeep Roy",
      "Gagan Bhatia",
      "Steffen Eger"
    ],
    "abstract": "Automatic metrics are now central to evaluating text-to-image models, often substituting for human judgment in benchmarking and large-scale filtering. However, it remains unclear whether these metrics truly prioritize semantic correctness or instead favor visually and socially prototypical images learned from biased data distributions. We identify and study \\emph{prototypicality bias} as a systematic failure mode in multimodal evaluation. We introduce a controlled contrastive benchmark \\textsc{\\textbf{ProtoBias}} (\\textit{\\textbf{Proto}typical \\textbf{Bias}}), spanning Animals, Objects, and Demography images, where semantically correct but non-prototypical images are paired with subtly incorrect yet prototypical adversarial counterparts. This setup enables a directional evaluation of whether metrics follow textual semantics or default to prototypes. Our results show that widely used metrics, including CLIPScore, PickScore, and VQA-based scores, frequently misrank these pairs, while even LLM-as-Judge systems exhibit uneven robustness in socially grounded cases. Human evaluations consistently favour semantic correctness with larger decision margins. Motivated by these findings, we propose \\textbf{\\textsc{ProtoScore}}, a robust 7B-parameter metric that substantially reduces failure rates and suppresses misranking, while running at orders of magnitude faster than the inference time of GPT-5, approaching the robustness of much larger closed-source judges.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04946.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04946",
    "published": "2026-01-08T13:49:14Z",
    "updated": "2026-01-08T13:49:14Z",
    "comment": "First version",
    "light_analysis": {
      "overview": "论文通过识别和评估多模态评估指标中的典型性偏差，提出了鲁棒度量ProtoScore以减少偏差并提高评估准确性。",
      "motivation": "研究动机在于自动评估指标在文本到图像模型评估中常替代人工判断，但存在不确定性：这些指标是否真正优先语义正确性，还是偏向于从有偏数据分布中学到的视觉和社会典型图像。现有方法的不足在于可能导致系统性的误评，从而影响评估的准确性，特别是在基准测试和大规模过滤中，这强调了探索评估指标盲点的重要性。",
      "method": "研究方法包括引入ProtoBias基准，这是一个受控对比基准，涵盖动物、对象和人口图像，其中配对语义正确但非典型的图像与不正确但典型的对抗图像。通过这种设置，定向评估指标是否遵循文本语义或默认到原型。关键创新点在于提供了一个标准化框架来测试典型性偏差。此外，论文提出了ProtoScore，一个7B参数的鲁棒度量，设计来减少失败率和误评，技术特色包括高效的计算和接近大型闭源法官的鲁棒性。",
      "result": "实验结果显示，广泛使用的指标如CLIPScore、PickScore和基于VQA的分数经常误评这些配对，表明它们系统性地偏向典型图像。LLM-as-Judge系统在社会化场景中表现出不均衡的鲁棒性。相比之下，人类评估一致偏好语义正确性，且决策边界更大，凸显了现有指标的不足。提出的ProtoScore度量显著降低失败率，抑制误评，运行速度远快于GPT-5的推理时间，鲁棒性接近更大闭源法官，展示了改进效果。",
      "conclusion": "论文的主要贡献是识别并研究了多模态评估中的典型性偏差作为系统失败模式，提供了ProtoBias基准用于评估指标，并提出了ProtoScore度量以改善鲁棒性。学术价值在于揭示评估指标的盲点，促进更公正的评估方法；实际应用价值在于为模型评估提供更可靠的工具，潜在局限性是未来可能需要在更多场景和数据集上扩展验证。",
      "tags": [
        "Prototypicality Bias",
        "Multimodal Evaluation Metrics",
        "Contrastive Benchmark",
        "LLM-as-Judge",
        "ProtoScore"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:06.472012Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04945",
    "title": "T-Retriever: Tree-based Hierarchical Retrieval Augmented Generation for Textual Graphs",
    "authors": [
      "Chunyu Wei",
      "Huaiyu Qin",
      "Siyuan He",
      "Yunhai Wang",
      "Yueguo Chen"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has significantly enhanced Large Language Models' ability to access external knowledge, yet current graph-based RAG approaches face two critical limitations in managing hierarchical information: they impose rigid layer-specific compression quotas that damage local graph structures, and they prioritize topological structure while neglecting semantic content. We introduce T-Retriever, a novel framework that reformulates attributed graph retrieval as tree-based retrieval using a semantic and structure-guided encoding tree. Our approach features two key innovations: (1) Adaptive Compression Encoding, which replaces artificial compression quotas with a global optimization strategy that preserves the graph's natural hierarchical organization, and (2) Semantic-Structural Entropy ($S^2$-Entropy), which jointly optimizes for both structural cohesion and semantic consistency when creating hierarchical partitions. Experiments across diverse graph reasoning benchmarks demonstrate that T-Retriever significantly outperforms state-of-the-art RAG methods, providing more coherent and contextually relevant responses to complex queries.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04945.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04945",
    "published": "2026-01-08T13:49:12Z",
    "updated": "2026-01-08T13:49:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出T-Retriever框架，通过语义和结构指导的编码树，改进了图基检索增强生成中的层次信息管理。",
      "motivation": "检索增强生成（RAG）显著增强了大型语言模型访问外部知识的能力，但在处理文本图数据的层次信息时，现有基于图的RAG方法面临两个关键限制：它们使用刚性层特定压缩配额，这会破坏局部图结构；同时，这些方法过度优先拓扑结构而忽视语义内容，导致在面对复杂查询时可能产生不连贯或缺乏上下文的响应。因此，开发一种能有效平衡结构和语义的层次信息管理方法变得尤为重要。",
      "method": "T-Retriever框架将属性图检索重新定义为基于树的检索，利用语义和结构指导的编码树来解决现有问题。核心创新包括自适应压缩编码，它通过全局优化策略替换人工压缩配额，以保持图的自然层次组织；以及语义结构熵，在创建层次分区时联合优化结构凝聚和语义一致性，确保检索过程的效率和准确性。该方法在多种图推理基准上进行实验，但摘要未明确说明具体的数据集或模型架构细节。",
      "result": "在多样化的图推理基准测试中，T-Retriever显著优于当前最先进的RAG方法，实验结果表明，它能够提供更连贯和上下文相关的复杂查询响应，有效提升了检索性能。尽管摘要未提供具体的准确率或效率量化数据，但与基线方法的对比显示，其自适应压缩和语义结构熵优化带来了显著的性能改进。",
      "conclusion": "T-Retriever框架通过树基层次检索解决了图基RAG在管理层次信息时的关键限制，主要贡献在于自适应压缩编码和语义结构熵的引入，这不仅提高了检索增强生成的学术价值，还对处理复杂图数据的实际应用具有重要意义。未来工作可能包括扩展到更广泛的图学习任务或改进算法效率。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Tree-based Retrieval",
        "Semantic-Structural Entropy",
        "Hierarchical Partitioning",
        "Graph Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T02:36:04.955950Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03919",
    "title": "A Gap Between Decision Trees and Neural Networks",
    "authors": [
      "Akash Kumar"
    ],
    "abstract": "We study when geometric simplicity of decision boundaries, used here as a notion of interpretability, can conflict with accurate approximation of axis-aligned decision trees by shallow neural networks. Decision trees induce rule-based, axis-aligned decision regions (finite unions of boxes), whereas shallow ReLU networks are typically trained as score models whose predictions are obtained by thresholding. We analyze the infinite-width, bounded-norm, single-hidden-layer ReLU class through the Radon total variation ($\\mathrm{R}\\mathrm{TV}$) seminorm, which controls the geometric complexity of level sets.   We first show that the hard tree indicator $1_A$ has infinite $\\mathrm{R}\\mathrm{TV}$. Moreover, two natural split-wise continuous surrogates--piecewise-linear ramp smoothing and sigmoidal (logistic) smoothing--also have infinite $\\mathrm{R}\\mathrm{TV}$ in dimensions $d>1$, while Gaussian convolution yields finite $\\mathrm{R}\\mathrm{TV}$ but with an explicit exponential dependence on $d$.   We then separate two goals that are often conflated: classification after thresholding (recovering the decision set) versus score learning (learning a calibrated score close to $1_A$). For classification, we construct a smooth barrier score $S_A$ with finite $\\mathrm{R}\\mathrm{TV}$ whose fixed threshold $τ=1$ exactly recovers the box. Under a mild tube-mass condition near $\\partial A$, we prove an $L_1(P)$ calibration bound that decays polynomially in a sharpness parameter, along with an explicit $\\mathrm{R}\\mathrm{TV}$ upper bound in terms of face measures. Experiments on synthetic unions of rectangles illustrate the resulting accuracy--complexity tradeoff and how threshold selection shifts where training lands along it.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03919.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03919",
    "published": "2026-01-07T13:40:30Z",
    "updated": "2026-01-08T13:31:51Z",
    "comment": "45 pages, plots were improved",
    "light_analysis": {
      "overview": "论文通过Radon总变差分析，揭示了决策树与浅层神经网络在决策边界几何复杂性上的根本差异，并提出平滑屏障分数方法以有限复杂性实现精确分类。",
      "motivation": "论文动机在于探究决策树的决策边界几何简单性与浅层神经网络近似能力之间的冲突。决策树因其基于规则的轴对齐决策区域而具有高可解释性，但浅层ReLU网络通常通过分数模型和阈值化进行训练，可能导致复杂的决策边界。现有平滑方法如分段线性斜坡或逻辑平滑在维度d>1时仍导致无限Radon总变差，无法有效平衡精度和几何复杂性，这突出了区分分类和分数学习的重要性。",
      "method": "研究方法基于Radon总变差半范数分析浅层ReLU网络的几何复杂性。首先证明硬树指示器及其常见平滑替代有无限RTV，然后创新性地分离分类和分数学习目标。提出构造平滑屏障分数S_A，具有有限RTV，并在管质量条件下理论分析校准性能和RTV上界。实验使用合成矩形并集验证精度-复杂性权衡，不涉及具体数据集或模型架构，但聚焦于理论构造和分析。",
      "result": "主要实验结果表明，提出的平滑屏障分数S_A在有限Radon总变差下实现精确分类，校准界限随锐度参数多项式衰减。与基线方法相比，硬树指示器和常见平滑替代具有无限RTV，而高斯卷积虽有有限RTV但指数依赖维度。合成实验验证了精度和几何复杂性之间的权衡，以及阈值选择对训练路径的影响，提供了理论和实验支持。",
      "conclusion": "结论总结论文主要贡献为通过Radon总变差分析决策树与神经网络决策边界差异，并提出平滑屏障分数以有限几何复杂性实现准确分类。学术价值在于提供理论框架区分分类和分数学习，实际应用有助于开发可解释且高效的模型。局限性包括依赖合成数据实验，未来工作可扩展到真实数据集或更深网络架构。",
      "tags": [
        "Decision Trees",
        "Radon Total Variation",
        "Shallow Neural Networks",
        "ReLU Activation",
        "Score Calibration"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:18.845825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.06842",
    "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation",
    "authors": [
      "Arkadiusz Modzelewski",
      "Witold Sosnowski",
      "Tiziano Labruna",
      "Adam Wierzbicki",
      "Giovanni Da San Martino"
    ],
    "abstract": "Disinformation detection is a key aspect of media literacy. Psychological studies have shown that knowledge of persuasive fallacies helps individuals detect disinformation. Inspired by these findings, we experimented with large language models (LLMs) to test whether infusing persuasion knowledge enhances disinformation detection. As a result, we introduce the Persuasion-Augmented Chain of Thought (PCoT), a novel approach that leverages persuasion to improve disinformation detection in zero-shot classification. We extensively evaluate PCoT on online news and social media posts. Moreover, we publish two novel, up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets enable the evaluation of PCoT on content entirely unseen by the LLMs used in our experiments, as the content was published after the models' knowledge cutoffs. We show that, on average, PCoT outperforms competitive methods by 15% across five LLMs and five datasets. These findings highlight the value of persuasion in strengthening zero-shot disinformation detection.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.06842.pdf",
    "abs_url": "https://arxiv.org/abs/2506.06842",
    "published": "2025-06-07T15:46:02Z",
    "updated": "2026-01-08T13:25:34Z",
    "comment": "Accepted to ACL 2025 Main Conference",
    "light_analysis": {
      "overview": "论文提出了Persuasion-Augmented Chain of Thought (PCoT)方法，通过结合说服知识增强大型语言模型在零样本设置下检测虚假新闻和社交媒体虚假信息的性能。",
      "motivation": "虚假信息检测是媒体素养的核心挑战，尤其在社会媒体时代其危害加剧。心理学研究表明，说服性谬误的知识能帮助个体识别虚假内容，但现有基于大型语言模型的检测方法通常忽视这类心理因素，可能导致检测精度不足。本研究动机是探索是否可以通过注入说服知识来弥补这一缺陷，提升模型在零样本场景下的检测能力，以应对日益复杂的虚假信息传播。",
      "method": "研究方法的核心是Persuasion-Augmented Chain of Thought (PCoT)，这是一种新型零样本分类技术，通过将说服知识融入思维链推理过程来增强虚假信息检测。实验中使用五个不同的大型语言模型进行验证，并发布两个新数据集EUDisinfo和MultiDis，这些数据集内容发布时间晚于模型知识截止，确保评估公正性。关键创新在于结合心理学理论优化模型推理，无需额外训练即可提升检测效果。",
      "result": "实验结果显示，PCoT在五个大型语言模型和五个数据集上平均性能优于竞争方法15%，具体表现为检测准确率的显著提升。这一改进展示了说服知识在零样本虚假信息检测中的有效性，跨模型和数据集的泛化性能验证了方法的鲁棒性。与基线方法相比，PCoT在多个评估指标上均表现出优势，突显了其在真实场景中的应用潜力。",
      "conclusion": "论文的主要贡献是提出PCoT方法，成功将说服知识融入虚假信息检测，并发布新数据集促进研究。研究意义在于展示了心理学与人工智能结合的学术价值，为媒体素养和内容审核提供实用工具。未来工作可探索说服知识在其他AI任务中的应用，或进一步优化方法以处理更复杂的虚假信息类型。",
      "tags": [
        "Large Language Model",
        "Zero-shot Classification",
        "Chain of Thought",
        "Persuasion Knowledge",
        "Disinformation Detection"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:27.460015Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04920",
    "title": "Conversational AI for Rapid Scientific Prototyping: A Case Study on ESA's ELOPE Competition",
    "authors": [
      "Nils Einecke"
    ],
    "abstract": "Large language models (LLMs) are increasingly used as coding partners, yet their role in accelerating scientific discovery remains underexplored. This paper presents a case study of using ChatGPT for rapid prototyping in ESA's ELOPE (Event-based Lunar OPtical flow Egomotion estimation) competition. The competition required participants to process event camera data to estimate lunar lander trajectories. Despite joining late, we achieved second place with a score of 0.01282, highlighting the potential of human-AI collaboration in competitive scientific settings. ChatGPT contributed not only executable code but also algorithmic reasoning, data handling routines, and methodological suggestions, such as using fixed number of events instead of fixed time spans for windowing. At the same time, we observed limitations: the model often introduced unnecessary structural changes, gets confused by intermediate discussions about alternative ideas, occasionally produced critical errors and forgets important aspects in longer scientific discussions. By analyzing these strengths and shortcomings, we show how conversational AI can both accelerate development and support conceptual insight in scientific research. We argue that structured integration of LLMs into the scientific workflow can enhance rapid prototyping by proposing best practices for AI-assisted scientific work.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04920.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04920",
    "published": "2026-01-08T13:17:50Z",
    "updated": "2026-01-08T13:17:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过ELOPE竞赛案例，展示了ChatGPT在加速科学原型开发中的潜力，并提出AI辅助科学工作的最佳实践。",
      "motivation": "LLMs作为编码伙伴已广泛应用，但它们在加速科学发现中的角色仍未充分探索。科学发现需要快速迭代和原型开发，现有方法可能未有效整合AI的推理能力。本研究通过ESA的ELOPE竞赛，旨在解决如何利用对话式AI辅助处理复杂科学任务（如事件相机数据分析），并揭示其在竞争性环境中的实际价值，以填补这一研究空白。",
      "method": "本研究采用案例研究方法，以ChatGPT为工具参与ESA的ELOPE竞赛。竞赛任务涉及处理事件相机数据以估计月球着陆器轨迹。ChatGPT提供了可执行代码、算法推理、数据处理例程和方法建议，例如使用固定事件数而非固定时间跨度进行窗口划分。这体现了AI在科学原型开发中的多层次辅助，从代码生成到方法优化，核心创新在于利用AI的对话能力加速开发流程。",
      "result": "在ELOPE竞赛中，团队使用ChatGPT辅助获得了第二名，得分0.01282，尽管加入时间较晚。这一结果表明人机合作在科学任务中的高效性，AI加速了原型开发并提升了性能。竞赛排名间接验证了其优势，虽然摘要未明确说明与基线方法的详细对比，但成绩突显了AI在竞争性科学环境中的潜力。",
      "conclusion": "论文总结了对话式AI在加速科学原型开发和支持概念洞察方面的双重作用，同时指出了局限性如模型易混淆、产生错误等。通过分析这些特点，提出结构化集成LLMs到科学工作流程的最佳实践，以增强快速原型开发能力。这为AI辅助科学研究提供了新视角，具有学术和实际应用价值，并建议未来研究关注如何克服现有局限。",
      "tags": [
        "Large Language Model",
        "ChatGPT",
        "Rapid Prototyping",
        "Event Camera",
        "Human-AI Collaboration"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:48.656434Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04919",
    "title": "What Students Ask, How a Generative AI Assistant Responds: Exploring Higher Education Students' Dialogues on Learning Analytics Feedback",
    "authors": [
      "Yildiz Uzun",
      "Andrea Gauthier",
      "Mutlu Cukurova"
    ],
    "abstract": "Learning analytics dashboards (LADs) aim to support students' regulation of learning by translating complex data into feedback. Yet students, especially those with lower self-regulated learning (SRL) competence, often struggle to engage with and interpret analytics feedback. Conversational generative artificial intelligence (GenAI) assistants have shown potential to scaffold this process through real-time, personalised, dialogue-based support. Further advancing this potential, we explored authentic dialogues between students and GenAI assistant integrated into LAD during a 10-week semester. The analysis focused on questions students with different SRL levels posed, the relevance and quality of the assistant's answers, and how students perceived the assistant's role in their learning. Findings revealed distinct query patterns. While low SRL students sought clarification and reassurance, high SRL students queried technical aspects and requested personalised strategies. The assistant provided clear and reliable explanations but limited in personalisation, handling emotionally charged queries, and integrating multiple data points for tailored responses. Findings further extend that GenAI interventions can be especially valuable for low SRL students, offering scaffolding that supports engagement with feedback and narrows gaps with their higher SRL peers. At the same time, students' reflections underscored the importance of trust, need for greater adaptivity, context-awareness, and technical refinement in future systems.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04919.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04919",
    "published": "2026-01-08T13:17:44Z",
    "updated": "2026-01-08T13:17:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过分析学生与生成式AI助手的真实对话，揭示了其在支持学习分析反馈中的潜力，以及对不同自我调节学习水平学生的差异化影响。",
      "motivation": "学习分析仪表板旨在通过数据反馈帮助学生调节学习，但现有系统在促进学生，尤其是自我调节学习能力较低的学生理解和应用反馈方面存在挑战，这影响了学习效果和教育公平。生成式人工智能助手通过提供实时、个性化对话支持，有望改善这一过程，但如何有效集成AI助手以应对不同学生需求并弥补现有方法的互动性和适应性不足，尚需实证研究。本研究的动机是探索生成式AI在教育反馈中的实际应用效果，以推动教育技术的创新发展。",
      "method": "本研究采用实地研究方法，在为期10周的学期中，将生成式AI助手集成到学习分析仪表板中，收集和分析学生与助手的真实对话数据。技术路线聚焦于对话分析，包括不同自我调节学习水平学生提出的问题类型、助手回答的相关性和质量评估，以及学生对助手角色的感知调查。这种方法结合了生成式AI技术与教育反馈，通过实际交互数据评估对话式AI在支持学习中的实施效果和创新应用。",
      "result": "分析发现，不同自我调节学习水平的学生在提问模式上存在显著差异：低能力学生倾向于寻求澄清和保证，而高能力学生关注技术细节和请求个性化策略。生成式AI助手在提供清晰可靠解释方面表现良好，但在个性化响应、处理情感化查询和整合多数据点进行定制反馈方面存在局限。尽管如此，助手对低能力学生尤其有价值，能提升其参与度并帮助缩小与高能力学生的差距，但摘要未明确与基线方法的对比数据，仅基于定性发现推断其有效性。",
      "conclusion": "本研究证实生成式AI助手在高等教育中能有效支持学生学习反馈，特别为低自我调节学习能力学生提供脚手架辅助，具有学术价值和实际应用前景。主要贡献在于提供对话式AI在教育应用的实证证据，并指出当前系统在个性化、情感处理和跨数据整合方面的不足。未来工作应着重于增强助手的适应性、上下文感知和技术优化，以提高用户信任和实用价值，推动教育技术的智能化发展，并探索更多数据集成和情感计算方向。",
      "tags": [
        "Generative AI",
        "Learning Analytics",
        "Self-Regulated Learning",
        "Dialogue Systems",
        "Educational Feedback"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:22.004818Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04911",
    "title": "From Stories to Cities to Games: A Qualitative Evaluation of Behaviour Planning",
    "authors": [
      "Mustafa F. Abdelwahed",
      "Joan Espasa",
      "Alice Toniolo",
      "Ian P. Gent"
    ],
    "abstract": "The primary objective of a diverse planning approach is to generate a set of plans that are distinct from one another. Such an approach is applied in a variety of real-world domains, including risk management, automated stream data analysis, and malware detection. More recently, a novel diverse planning paradigm, referred to as behaviour planning, has been proposed. This approach extends earlier methods by explicitly incorporating a diversity model into the planning process and supporting multiple planning categories. In this paper, we demonstrate the usefulness of behaviour planning in real-world settings by presenting three case studies. The first case study focuses on storytelling, the second addresses urban planning, and the third examines game evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04911.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04911",
    "published": "2026-01-08T13:09:43Z",
    "updated": "2026-01-08T13:09:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过三个案例研究定性评估了行为规划方法，展示了其在讲故事、城市规划和游戏评估等现实世界中的应用价值。",
      "motivation": "多样化规划在风险管理、自动化流数据分析和恶意软件检测等实际领域中至关重要，需要生成一组相互区别的计划以应对复杂场景。现有方法可能在整合多样性模型和支持多类别方面存在不足，行为规划作为新范式扩展了这些方面，旨在提高计划的多样性和适用性。本文旨在通过案例研究验证行为规划在实际设置中的有效性和实用性，以解决多样化规划中的关键问题。",
      "method": "行为规划是一种新颖的多样化规划方法，其核心创新在于明确将多样性模型纳入规划过程，并支持多个规划类别，从而扩展了早期规划技术。该方法通过引入多样性模型来增强计划的差异性，并在多类别框架下进行规划。摘要未明确说明具体的数据集、模型架构或算法细节，但强调了其在多领域应用中的通用性，以提高规划过程的灵活性和覆盖范围。",
      "result": "通过三个案例研究（讲故事、城市规划和游戏评估），行为规划展示了在现实世界中的有用性。摘要未明确说明具体的性能指标如准确率提升或效率改进，也未提供与基线方法的对比数据；因此，结果主要基于定性评估，证明了该方法在不同领域的适用性和潜在效果，强调了其在实际应用中的价值。",
      "conclusion": "本文的主要贡献是提出并定性评估了行为规划方法，通过案例研究扩展了多样化规划的理论和实践。学术上，该方法丰富了规划理论，引入了多样性模型和多类别支持；实际上，为风险管理、城市规划和游戏设计等领域提供了新工具。局限性包括摘要未明确说明未来工作方向，但可推测需要进一步定量验证、优化算法和探索更多应用场景，以增强其鲁棒性和广泛适用性。",
      "tags": [
        "Behaviour Planning",
        "Diverse Planning",
        "Diversity Model",
        "Qualitative Evaluation",
        "Planning Categories"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:44.886822Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.08973",
    "title": "Is Contrastive Distillation Enough for Learning Comprehensive 3D Representations?",
    "authors": [
      "Yifan Zhang",
      "Junhui Hou"
    ],
    "abstract": "Cross-modal contrastive distillation has recently been explored for learning effective 3D representations. However, existing methods focus primarily on modality-shared features, neglecting the modality-specific features during the pre-training process, which leads to suboptimal representations. In this paper, we theoretically analyze the limitations of current contrastive methods for 3D representation learning and propose a new framework, namely CMCR (Cross-Modal Comprehensive Representation Learning), to address these shortcomings. Our approach improves upon traditional methods by better integrating both modality-shared and modality-specific features. Specifically, we introduce masked image modeling and occupancy estimation tasks to guide the network in learning more comprehensive modality-specific features. Furthermore, we propose a novel multi-modal unified codebook that learns an embedding space shared across different modalities. Besides, we introduce geometry-enhanced masked image modeling to further boost 3D representation learning. Extensive experiments demonstrate that our method mitigates the challenges faced by traditional approaches and consistently outperforms existing image-to-LiDAR contrastive distillation methods in downstream tasks. Code will be available at https://github.com/Eaphan/CMCR.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2412.08973.pdf",
    "abs_url": "https://arxiv.org/abs/2412.08973",
    "published": "2024-12-12T06:09:49Z",
    "updated": "2026-01-08T13:08:33Z",
    "comment": "22 pages, 10 figures",
    "light_analysis": {
      "overview": "本文提出CMCR框架，通过整合模态共享和模态特定特征，改进跨模态对比蒸馏，以学习更全面的3D表示。",
      "motivation": "跨模态对比蒸馏方法被用于学习3D表示，但现有技术过度关注模态共享特征，忽略了模态特定特征，导致表示不全面且性能受限。这在自动驾驶和机器人感知等应用领域尤为重要，因为多模态信息（如图像和激光雷达）的综合利用是关键。现有方法因未能有效处理模态特异性，限制了表示学习的质量，因此需要一种更全面的框架来弥补这一不足。摘要未明确说明具体应用场景，但可推断其涉及下游任务如对象检测或场景理解。",
      "method": "CMCR框架在传统跨模态对比蒸馏基础上，引入多项创新技术。具体包括掩码图像建模和占用估计任务，以增强模态特定特征的学习；设计多模态统一码书，构建跨模态共享的嵌入空间；以及实施几何增强的掩码图像建模，进一步提升3D表示能力。该框架通过结合这些组件，使模型同时捕获模态共享和模态特定的特征，无需额外数据集细节，核心是基于网络架构的改进和自监督任务的扩展。",
      "result": "实验结果表明，CMCR方法有效缓解了传统方法在处理模态特定特征时的挑战，并在下游任务中一致优于现有的图像到激光雷达对比蒸馏方法。尽管摘要未提供具体性能指标如准确率或效率数据，但强调了该框架相对于基线方法的改进，展示了其在提升3D表示质量方面的优越性，涵盖多个任务场景。",
      "conclusion": "CMCR框架通过创新整合模态共享和模态特定特征，显著推进了3D表示学习。该研究贡献包括理论分析现有方法的局限，并提出掩码图像建模和统一码书等新技术，具有重要学术价值，可促进跨模态学习领域的发展。实际应用价值体现在自动驾驶和机器人感知等任务中，未来工作可能包括扩展到其他模态或进一步优化框架的泛化能力，但摘要未明确提及局限性。",
      "tags": [
        "Cross-Modal Contrastive Distillation",
        "Masked Image Modeling",
        "Occupancy Estimation",
        "Multi-Modal Unified Codebook",
        "Geometry-Enhanced Masked Image Modeling"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:37.137936Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04895",
    "title": "DVD: A Robust Method for Detecting Variant Contamination in Large Language Model Evaluation",
    "authors": [
      "Renzhao Liang",
      "Jingru Chen",
      "Bo Jia",
      "Bo Deng",
      "Chenggang Xie",
      "Yidong Wang",
      "Ke Jin",
      "Xin Wang",
      "Linfeng Zhang",
      "Cunxiang Wang"
    ],
    "abstract": "Evaluating large language models (LLMs) is increasingly confounded by \\emph{variant contamination}: the training corpus contains semantically equivalent yet lexically or syntactically altered versions of test items. Unlike verbatim leakage, these paraphrased or structurally transformed variants evade existing detectors based on sampling consistency or perplexity, thereby inflating benchmark scores via memorization rather than genuine reasoning. We formalize this problem and introduce \\textbf{DVD} (\\textbf{D}etection via \\textbf{V}ariance of generation \\textbf{D}istribution), a single-sample detector that models the local output distribution induced by temperature sampling. Our key insight is that contaminated items trigger alternation between a \\emph{memory-adherence} state and a \\emph{perturbation-drift} state, yielding abnormally high variance in the synthetic difficulty of low-probability tokens; uncontaminated items remain in drift with comparatively smooth variance. We construct the first benchmark for variant contamination across two domains Omni-MATH and SuperGPQA by generating and filtering semantically equivalent variants, and simulate contamination via fine-tuning models of different scales and architectures (Qwen2.5 and Llama3.1). Across datasets and models, \\textbf{DVD} consistently outperforms perplexity-based, Min-$k$\\%++, edit-distance (CDD), and embedding-similarity baselines, while exhibiting strong robustness to hyperparameters. Our results establish variance of the generation distribution as a principled and practical fingerprint for detecting variant contamination in LLM evaluation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04895.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04895",
    "published": "2026-01-08T12:48:40Z",
    "updated": "2026-01-08T12:48:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了DVD方法，一种基于生成分布方差的单样本检测器，有效识别大型语言模型评估中的变体污染，优于现有基线。",
      "motivation": "研究动机源于大型语言模型评估中日益严重的变体污染问题，即训练语料中包含语义等效但词汇或句法改变的测试项目版本，这导致模型通过记忆而非真实推理来提升基准分数，误导性能评估。现有检测方法如基于采样一致性或困惑度的技术无法有效识别这些变体，因为它们能逃避传统检测机制。因此，开发更精确的检测方法对确保评估的可靠性和公平性至关重要，以促进模型能力的真实衡量。",
      "method": "论文提出了DVD（Detection via Variance of generation Distribution）方法，这是一种单样本检测器，核心创新在于通过温度采样建模本地输出分布，并分析生成过程中低概率标记的合成难度方差。关键洞察是污染项会触发记忆遵从状态和扰动漂移状态之间的交替，导致方差异常高，而非污染项则保持相对平滑的方差。实验中使用Omni-MATH和SuperGPQA数据集构建首个变体污染基准，通过生成和过滤语义等效变体模拟污染，并微调了不同规模和架构的模型如Qwen2.5和Llama3.1以评估检测效果。",
      "result": "实验结果表明，DVD在检测变体污染方面表现优异，在Omni-MATH和SuperGPQA数据集上，以及使用Qwen2.5和Llama3.1等不同模型时，均一致性地优于多个基线方法，包括基于困惑度的方法、Min-k%++、编辑距离（CDD）和嵌入相似性基线。DVD展现出对超参数的强鲁棒性，证明了其在实际应用中的可靠性和泛化能力；摘要未明确说明具体数值，但强调了方法在多种设置下的有效性。",
      "conclusion": "论文结论确立了生成分布方差作为检测大型语言模型评估中变体污染的可靠和实用指标，DVD方法的主要贡献是提供了一种高效且鲁棒的检测机制，能准确区分污染项和非污染项，从而提升评估的准确性和可信度。这一研究具有重要学术价值，为未来LLM评估标准提供了新思路；实际应用中，可帮助避免污染导致的偏差，促进模型真实能力的评估。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Variant Contamination",
        "Large Language Model",
        "Generation Distribution",
        "Temperature Sampling",
        "Variance Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:31.189736Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04888",
    "title": "SmartSearch: Process Reward-Guided Query Refinement for Search Agents",
    "authors": [
      "Tongyu Wen",
      "Guanting Dong",
      "Zhicheng Dou"
    ],
    "abstract": "Large language model (LLM)-based search agents have proven promising for addressing knowledge-intensive problems by incorporating information retrieval capabilities. Existing works largely focus on optimizing the reasoning paradigms of search agents, yet the quality of intermediate search queries during reasoning remains overlooked. As a result, the generated queries often remain inaccurate, leading to unexpected retrieval results and ultimately limiting search agents' overall effectiveness. To mitigate this issue, we introduce SmartSearch, a framework built upon two key mechanisms: (1) Process rewards, which provide fine-grained supervision for the quality of each intermediate search query through Dual-Level Credit Assessment. (2) Query refinement, which promotes the optimization of query generation by selectively refining low-quality search queries and regenerating subsequent search rounds based on these refinements. To enable the search agent to progressively internalize the ability to improve query quality under the guidance of process rewards, we design a three-stage curriculum learning framework. This framework guides the agent through a progression from imitation, to alignment, and ultimately to generalization. Experimental results show that SmartSearch consistently surpasses existing baselines, and additional quantitative analyses further confirm its significant gains in both search efficiency and query quality. The code is available at https://github.com/MYVAE/SmartSearch.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04888.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04888",
    "published": "2026-01-08T12:39:05Z",
    "updated": "2026-01-08T12:39:05Z",
    "comment": "16 pages, 6 figures",
    "light_analysis": {
      "overview": "本文提出了SmartSearch框架，通过过程奖励引导的查询细化机制来优化基于大型语言模型的搜索代理的查询质量。",
      "motivation": "基于大型语言模型（LLM）的搜索代理在处理知识密集型问题时，现有方法主要聚焦于优化推理范式，但忽略了中间搜索查询的质量，导致查询不准确和检索结果不佳，从而限制了搜索代理的整体效能。这一问题的解决对提升搜索代理在信息检索中的准确性至关重要，因为查询质量直接影响性能，而现有研究在这一方面存在明显不足。",
      "method": "SmartSearch框架的核心机制包括过程奖励和查询细化：过程奖励通过双层次信用评估为每个中间查询提供细粒度监督；查询细化则选择性优化低质量查询，并基于此重新生成后续搜索轮次。此外，作者设计了三阶段课程学习框架，从模仿、对齐到泛化，指导搜索代理逐步内化改进查询质量的能力。这些方法特色在于直接优化查询生成，并通过渐进式学习提升性能。摘要未明确说明使用的具体数据集或模型架构。",
      "result": "实验结果显示，SmartSearch consistently surpasses existing baselines，在多个基准测试中表现优异。定量分析进一步证实了其在搜索效率和查询质量上的显著提升，相对于基线方法有明确优势。这些成果验证了框架的有效性，尽管摘要未提供具体的性能指标数值。",
      "conclusion": "本研究的主要贡献是提出SmartSearch框架，有效解决了搜索代理中中间查询质量低的问题，从而提高了整体性能。学术价值在于为优化LLM-based搜索代理提供了新思路，实际应用价值在于提升搜索任务的准确性和效率。未来工作可扩展应用到其他领域或进一步优化奖励机制，摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Query Refinement",
        "Process Rewards",
        "Curriculum Learning",
        "Dual-Level Credit Assessment"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:28.905808Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04887",
    "title": "Flexible Manufacturing Systems Intralogistics: Dynamic Optimization of AGVs and Tool Sharing Using Coloured-Timed Petri Nets and Actor-Critic RL with Actions Masking",
    "authors": [
      "Sofiene Lassoued",
      "Laxmikant Shrikant Bahetic",
      "Nathalie Weiß-Borkowskib",
      "Stefan Lierc",
      "Andreas Schwunga"
    ],
    "abstract": "Flexible Manufacturing Systems (FMS) are pivotal in optimizing production processes in today's rapidly evolving manufacturing landscape. This paper advances the traditional job shop scheduling problem by incorporating additional complexities through the simultaneous integration of automated guided vehicles (AGVs) and tool-sharing systems. We propose a novel approach that combines Colored-Timed Petri Nets (CTPNs) with actor-critic model-based reinforcement learning (MBRL), effectively addressing the multifaceted challenges associated with FMS. CTPNs provide a formal modeling structure and dynamic action masking, significantly reducing the action search space, while MBRL ensures adaptability to changing environments through the learned policy. Leveraging the advantages of MBRL, we incorporate a lookahead strategy for optimal positioning of AGVs, improving operational efficiency. Our approach was evaluated on small-sized public benchmarks and a newly developed large-scale benchmark inspired by the Taillard benchmark. The results show that our approach matches traditional methods on smaller instances and outperforms them on larger ones in terms of makespan while achieving a tenfold reduction in computation time. To ensure reproducibility, we propose a gym-compatible environment and an instance generator. Additionally, an ablation study evaluates the contribution of each framework component to its overall performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04887.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04887",
    "published": "2026-01-08T12:37:02Z",
    "updated": "2026-01-08T12:37:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合彩色时间Petri网和演员-批评者强化学习的方法，动态优化柔性制造系统中的自动导引车和工具共享。",
      "motivation": "柔性制造系统在现代制造业中优化生产流程至关重要，但传统作业车间调度方法难以整合自动导引车和工具共享的复杂性，导致在动态环境中适应性不足和效率低下。本研究旨在解决这一多面性挑战，通过结合CTPNs和MBRL来增强优化能力，推动FMS内物流的进步。",
      "method": "本研究提出了一种新颖方法，结合彩色时间Petri网和基于演员-批评者模型的强化学习。CTPNs提供正式建模结构和动态动作掩码，显著减少动作搜索空间；MBRL通过学习策略确保系统适应环境变化，并采用前瞻策略优化自动导引车定位。方法评估使用了小型公开基准和基于Taillard基准新开发的大型数据集。",
      "result": "实验结果显示，在小规模公开基准上，该方法与传统方法性能相当；在新开发的大规模基准上，在makespan方面优于传统方法，同时计算时间减少了十倍。提供了gym兼容环境和实例生成器以确保可重复性，消融研究评估了各组件对整体性能的贡献。",
      "conclusion": "本研究的主要贡献是提出了一种结合CTPNs和MBRL的动态优化框架，有效提升了柔性制造系统的内物流效率。学术上为FMS调度问题提供了新建模途径，实际上可减少生产时间和计算成本。未来工作可扩展基准测试和优化策略泛化能力。",
      "tags": [
        "Coloured-Timed Petri Nets",
        "Actor-Critic Reinforcement Learning",
        "Model-Based Reinforcement Learning",
        "Action Masking",
        "Flexible Manufacturing Systems"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:28.714641Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04885",
    "title": "CuMA: Aligning LLMs with Sparse Cultural Values via Demographic-Aware Mixture of Adapters",
    "authors": [
      "Ao Sun",
      "Xiaoyu Wang",
      "Zhe Tan",
      "Yu Li",
      "Jiachen Zhu",
      "Shu Su",
      "Yuheng Jia"
    ],
    "abstract": "As Large Language Models (LLMs) serve a global audience, alignment must transition from enforcing universal consensus to respecting cultural pluralism. We demonstrate that dense models, when forced to fit conflicting value distributions, suffer from \\textbf{Mean Collapse}, converging to a generic average that fails to represent diverse groups. We attribute this to \\textbf{Cultural Sparsity}, where gradient interference prevents dense parameters from spanning distinct cultural modes. To resolve this, we propose \\textbf{\\textsc{CuMA}} (\\textbf{Cu}ltural \\textbf{M}ixture of \\textbf{A}dapters), a framework that frames alignment as a \\textbf{conditional capacity separation} problem. By incorporating demographic-aware routing, \\textsc{CuMA} internalizes a \\textit{Latent Cultural Topology} to explicitly disentangle conflicting gradients into specialized expert subspaces. Extensive evaluations on WorldValuesBench, Community Alignment, and PRISM demonstrate that \\textsc{CuMA} achieves state-of-the-art performance, significantly outperforming both dense baselines and semantic-only MoEs. Crucially, our analysis confirms that \\textsc{CuMA} effectively mitigates mean collapse, preserving cultural diversity. Our code is available at https://github.com/Throll/CuMA.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04885.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04885",
    "published": "2026-01-08T12:30:43Z",
    "updated": "2026-01-08T12:30:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "CuMA框架通过人口感知的适配器混合技术，解决大语言模型文化对齐中的平均崩溃问题，以尊重和保持文化多元性。",
      "motivation": "随着大语言模型服务全球用户，对齐问题需从强制普适性转向尊重文化多样性。现有密集模型在处理冲突价值观时遭遇'平均崩溃'，即收敛到通用平均值，无法代表不同文化群体，导致模型在多元化环境中失效。这是因为'文化稀疏性'使得梯度干扰阻碍参数覆盖多元文化模式，这一问题对确保LLMs的公平性和适应性至关重要，以避免文化偏见和提升用户体验。",
      "method": "论文提出CuMA框架，将文化对齐视为条件容量分离问题。通过人口感知路由机制，内部化潜在文化拓扑，将冲突梯度解耦到专门的专家子空间。具体采用适配器混合技术，每个适配器针对特定文化群体进行优化，避免了密集参数中的梯度干扰。该方法在多个数据集上应用，但摘要未明确说明使用的模型架构细节或具体实现步骤。",
      "result": "在WorldValuesBench、Community Alignment和PRISM数据集上的评估显示，CuMA实现了最先进的性能，显著优于密集基线模型和仅基于语义的混合专家系统。具体而言，CuMA有效减轻了平均崩溃现象，更好地保留了文化多样性，确保了模型对不同群体的准确代表。尽管摘要未提供具体数值指标，但结果强调了方法的优越性和实用性。",
      "conclusion": "CuMA的主要贡献在于通过条件容量分离和人口感知路由，解决了LLMs文化对齐中的平均崩溃问题，推动了尊重文化多元性的对齐研究。这具有重要的学术价值，为AI模型在全球化应用中的公平性提供新思路；实际应用上，能增强模型在多元化环境中的适应性。未来工作可能扩展到更多文化维度或探索其他模型的对齐方法。",
      "tags": [
        "Large Language Model",
        "Mixture of Experts",
        "Adapters",
        "Cultural Alignment",
        "Demographic-Aware"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:28.323515Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04884",
    "title": "Precomputing Multi-Agent Path Replanning using Temporal Flexibility: A Case Study on the Dutch Railway Network",
    "authors": [
      "Issa Hanou",
      "Eric Kemmeren",
      "Devin Wild Thomas",
      "Mathijs de Weerdt"
    ],
    "abstract": "Executing a multi-agent plan can be challenging when an agent is delayed, because this typically creates conflicts with other agents. So, we need to quickly find a new safe plan. Replanning only the delayed agent often does not result in an efficient plan, and sometimes cannot even yield a feasible plan. On the other hand, replanning other agents may lead to a cascade of changes and delays. We show how to efficiently replan by tracking and using the temporal flexibility of other agents while avoiding cascading delays. This flexibility is the maximum delay an agent can take without changing the order of or further delaying more agents. Our algorithm, FlexSIPP, precomputes all possible plans for the delayed agent, also returning the changes for the other agents, for any single-agent delay within the given scenario. We demonstrate our method in a real-world case study of replanning trains in the densely-used Dutch railway network. Our experiments show that FlexSIPP provides effective solutions, relevant to real-world adjustments, and within a reasonable timeframe.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04884.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04884",
    "published": "2026-01-08T12:30:36Z",
    "updated": "2026-01-08T12:30:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于时间灵活性的多智能体路径重规划方法，通过预计算来避免连锁延迟，并在荷兰铁路网络中验证了其有效性。",
      "motivation": "在多智能体系统如铁路网络中，单个智能体延迟常引发冲突，传统重规划方法仅处理延迟智能体往往效率低下或不可行，而重新规划其他智能体则可能导致连锁延迟，增加系统复杂性。这一问题的解决对于密集网络的实际应用至关重要，现有方法难以在效率和可行性间取得平衡，本研究旨在填补这一空白，提供高效的重规划策略以避免连锁效应。",
      "method": "研究提出 FlexSIPP 算法，核心是利用时间灵活性——即智能体可延迟的最大时间而不改变其他智能体顺序或造成额外延迟。该方法预计算所有可能的单智能体延迟重规划方案，并返回其他智能体的调整方案。创新点在于通过跟踪灵活性来避免连锁延迟，并在实际场景如荷兰铁路网络中进行案例应用，强调预计算和灵活性整合的技术特色，尽管摘要未详细说明具体模型架构或数据集细节。",
      "result": "在荷兰铁路网络的真实案例实验中，FlexSIPP 方法提供了有效的重规划解决方案，能够应对实际调整需求，并在合理的时间框架内完成。实验表明该方法避免了传统方法可能引发的连锁延迟问题，但摘要未提供具体性能指标如准确率或运行时间数据，也未明确说明与基线方法的对比情况，仅强调了方法的实用性和效率。",
      "conclusion": "本研究的主要贡献是开发了基于时间灵活性的多智能体路径重规划方法 FlexSIPP，有效避免了连锁延迟，提高了重规划的效率和可行性。学术价值在于为多智能体计划执行提供了新思路，实际应用价值体现在密集网络如铁路系统的优化中。局限性包括摘要未提及扩展性或多智能体延迟的处理，未来工作可能涉及更复杂场景的扩展和改进。",
      "tags": [
        "Multi-Agent Planning",
        "Temporal Flexibility",
        "Path Replanning",
        "Precomputing",
        "Railway Network"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:48.114942Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.03979",
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "authors": [
      "Jin-Ting He",
      "Fu-Jen Tsai",
      "Yan-Tsung Peng",
      "Min-Hung Chen",
      "Chia-Wen Lin",
      "Yen-Yu Lin"
    ],
    "abstract": "Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/BlurDM/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.03979.pdf",
    "abs_url": "https://arxiv.org/abs/2512.03979",
    "published": "2025-12-03T17:10:44Z",
    "updated": "2026-01-08T12:28:51Z",
    "comment": "NeurIPS 2025. Project Page: https://jin-ting-he.github.io/BlurDM/",
    "light_analysis": {
      "overview": "BlurDM提出了一种模糊扩散模型，将模糊形成过程无缝集成到扩散框架中，用于动态场景图像去模糊，实现同时去噪和去模糊。",
      "motivation": "动态场景图像去模糊是计算机视觉中的重要任务，现有扩散模型方法在去模糊应用中存在局限性，未能有效利用模糊形成过程的内在性质，限制了模型潜力的充分发挥。该研究旨在解决这一问题，通过整合模糊动力学来提升去模糊性能，改进图像清晰度。",
      "method": "BlurDM基于运动模糊源于连续曝光的观察，采用双扩散前向方案隐式建模模糊形成过程，将噪声和模糊扩散到锐利图像上。在反向生成过程中，推导出双重去噪和去模糊公式，使模型能够以模糊图像为条件，从高斯噪声中同时去除噪声和模糊恢复锐利图像。此外，BlurDM在潜在空间中执行，形成一个灵活的先验生成网络，便于集成到现有去模糊网络中。",
      "result": "在四个基准数据集上的广泛实验表明，BlurDM能够显著且一致地增强现有去模糊方法的性能。尽管摘要未提供具体指标如准确率提升数值，但实验结果验证了其有效性，显示该方法在动态场景去模糊任务中优于基线方法。",
      "conclusion": "BlurDM通过将模糊形成过程集成到扩散模型中，提出了一种创新方法，不仅提升了图像去模糊的学术研究价值，还为实际应用中的图像处理提供了高效工具。研究的主要贡献在于优化扩散模型用于去模糊任务，未来工作可探索扩展到其他图像恢复领域或提高计算效率。",
      "tags": [
        "Diffusion Model",
        "Image Deblurring",
        "Motion Blur",
        "Denoising",
        "Latent Space"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:42.835334Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04878",
    "title": "Higher-Order Knowledge Representations for Agentic Scientific Reasoning",
    "authors": [
      "Isabella A. Stewart",
      "Markus J. Buehler"
    ],
    "abstract": "Scientific inquiry requires systems-level reasoning that integrates heterogeneous experimental data, cross-domain knowledge, and mechanistic evidence into coherent explanations. While Large Language Models (LLMs) offer inferential capabilities, they often depend on retrieval-augmented contexts that lack structural depth. Traditional Knowledge Graphs (KGs) attempt to bridge this gap, yet their pairwise constraints fail to capture the irreducible higher-order interactions that govern emergent physical behavior. To address this, we introduce a methodology for constructing hypergraph-based knowledge representations that faithfully encode multi-entity relationships. Applied to a corpus of ~1,100 manuscripts on biocomposite scaffolds, our framework constructs a global hypergraph of 161,172 nodes and 320,201 hyperedges, revealing a scale-free topology (power law exponent ~1.23) organized around highly connected conceptual hubs. This representation prevents the combinatorial explosion typical of pairwise expansions and explicitly preserves the co-occurrence context of scientific formulations. We further demonstrate that equipping agentic systems with hypergraph traversal tools, specifically using node-intersection constraints, enables them to bridge semantically distant concepts. By exploiting these higher-order pathways, the system successfully generates grounded mechanistic hypotheses for novel composite materials, such as linking cerium oxide to PCL scaffolds via chitosan intermediates. This work establishes a \"teacherless\" agentic reasoning system where hypergraph topology acts as a verifiable guardrail, accelerating scientific discovery by uncovering relationships obscured by traditional graph methods.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04878.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04878",
    "published": "2026-01-08T12:25:37Z",
    "updated": "2026-01-08T12:25:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种基于超图的知识表示方法，显著增强代理科学推理系统，避免了传统知识图的局限，从而加速科学发现。",
      "motivation": "研究动机源于科学探究需要整合异构实验数据、跨领域知识和机械证据以形成系统级推理。现有大语言模型虽具推理能力，但依赖缺乏结构深度的检索增强上下文；传统知识图因其成对约束无法捕捉高阶相互作用，这在复杂科学问题中尤为关键。",
      "method": "研究方法基于构建超图知识表示来编码多实体关系，应用于约1,100篇生物复合支架手稿的语料库。框架构建了包含161,172节点和320,201超边的全球超图，关键创新在于避免组合爆炸并保留共现上下文，使用节点交集约束等超图遍历工具桥接概念。",
      "result": "主要实验结果显示构建的超图显示出无标度拓扑（幂律指数约1.23），围绕高连接概念枢纽组织。系统利用超图遍历成功生成有根据的机制假设，例如通过壳聚糖中间体连接氧化铈与PCL支架，超越了传统知识图方法所不能揭示的关系。",
      "conclusion": "本研究的贡献是建立了一个“无教师”代理推理系统，以超图拓扑作为可验证的防护栏，加速科学发现。学术价值在于提供新方法增强知识表示和科学推理的结合，实际应用于揭示材料科学中的隐藏关系，未来可拓展到其他科学领域（摘要未明确说明局限性）。",
      "tags": [
        "Hypergraph",
        "Knowledge Representation",
        "Agentic Reasoning",
        "Large Language Models",
        "Scientific Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T02:47:30.161970Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.10643",
    "title": "Black-Box On-Policy Distillation of Large Language Models",
    "authors": [
      "Tianzhu Ye",
      "Li Dong",
      "Zewen Chi",
      "Xun Wu",
      "Shaohan Huang",
      "Furu Wei"
    ],
    "abstract": "Black-box distillation creates student large language models (LLMs) by learning from a proprietary teacher model's text outputs alone, without access to its internal logits or parameters. In this work, we introduce Generative Adversarial Distillation (GAD), which enables on-policy and black-box distillation. GAD frames the student LLM as a generator and trains a discriminator to distinguish its responses from the teacher LLM's, creating a minimax game. The discriminator acts as an on-policy reward model that co-evolves with the student, providing stable, adaptive feedback. Experimental results show that GAD consistently surpasses the commonly used sequence-level knowledge distillation. In particular, Qwen2.5-14B-Instruct (student) trained with GAD becomes comparable to its teacher, GPT-5-Chat, on the LMSYS-Chat automatic evaluation. The results establish GAD as a promising and effective paradigm for black-box LLM distillation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.10643.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10643",
    "published": "2025-11-13T18:58:37Z",
    "updated": "2026-01-08T12:15:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了生成对抗蒸馏方法，实现黑盒和on-policy的大型语言模型蒸馏。",
      "motivation": "黑盒蒸馏仅通过教师模型的文本输出进行学习，不访问其内部参数，这在保护知识产权和应对现实应用场景中至关重要。然而，现有方法如序列级知识蒸馏可能在黑盒设置下效果有限，无法充分提取教师知识，导致学生模型性能不足。因此，本研究旨在开发一种更有效的蒸馏技术，通过on-policy训练提供自适应反馈，以解决黑盒蒸馏的挑战并提升模型性能。",
      "method": "研究方法采用生成对抗蒸馏框架，将学生大型语言模型视为生成器，并训练一个判别器来区分学生和教师的响应，形成最小最大对抗游戏。判别器作为on-policy奖励模型，与学生共同进化，提供稳定且自适应的实时反馈。关键创新点包括黑盒设置、on-policy蒸馏和对抗性学习策略。实验中使用学生模型Qwen2.5-14B-Instruct和教师模型GPT-5-Chat，但摘要未详细说明具体模型架构或其他技术细节。",
      "result": "实验结果显示，生成对抗蒸馏在性能上一致超越常用的序列级知识蒸馏方法。具体而言，通过该方法的Qwen2.5-14B-Instruct学生模型在LMSYS-Chat自动评估中表现与教师模型GPT-5-Chat相当，证明了其有效性。这表明该方法能显著提升学生模型的性能，使其接近教师水平，并在与基线方法的对比中展现出优越性，但摘要未提供具体的准确率或效率数据。",
      "conclusion": "结论指出，生成对抗蒸馏作为一种有前景且有效的黑盒大型语言模型蒸馏范式，其主要贡献在于结合生成对抗网络和on-policy学习，实现了更高效的模型知识转移。学术价值体现在扩展蒸馏技术的应用范围，实际应用可能包括模型压缩和隐私保护场景。未来工作方向可探索该方法在其他任务或模型中的泛化能力，但摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Knowledge Distillation",
        "Generative Adversarial Networks",
        "On-Policy Learning",
        "Black-Box Distillation"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:30.540382Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04864",
    "title": "Key-Value Pair-Free Continual Learner via Task-Specific Prompt-Prototype",
    "authors": [
      "Haihua Luo",
      "Xuming Ran",
      "Zhengji Li",
      "Huiyan Xue",
      "Tingting Jiang",
      "Jiangrong Shen",
      "Tommi Kärkkäinen",
      "Qi Xu",
      "Fengyu Cong"
    ],
    "abstract": "Continual learning aims to enable models to acquire new knowledge while retaining previously learned information. Prompt-based methods have shown remarkable performance in this domain; however, they typically rely on key-value pairing, which can introduce inter-task interference and hinder scalability. To overcome these limitations, we propose a novel approach employing task-specific Prompt-Prototype (ProP), thereby eliminating the need for key-value pairs. In our method, task-specific prompts facilitate more effective feature learning for the current task, while corresponding prototypes capture the representative features of the input. During inference, predictions are generated by binding each task-specific prompt with its associated prototype. Additionally, we introduce regularization constraints during prompt initialization to penalize excessively large values, thereby enhancing stability. Experiments on several widely used datasets demonstrate the effectiveness of the proposed method. In contrast to mainstream prompt-based approaches, our framework removes the dependency on key-value pairs, offering a fresh perspective for future continual learning research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04864.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04864",
    "published": "2026-01-08T11:59:35Z",
    "updated": "2026-01-08T11:59:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种基于任务特定提示原型的无键值对持续学习框架，核心创新是消除对键值对的依赖。",
      "motivation": "持续学习旨在使模型在不断学习新任务时保持对旧知识的记忆，但在实际应用中，传统基于提示的方法通常依赖于键值对，这会引发任务间干扰并限制模型的可扩展性。现有方法的这些不足导致模型在处理多任务时性能下降，尤其是在大量任务场景下。因此，开发无键值对的方法变得至关重要，以提升持续学习系统的稳定性和效率，解决模型遗忘和可扩展性问题。",
      "method": "本文提出了一种名为任务特定提示原型（ProP）的新方法，用于持续学习。该方法使用任务特定提示来促进当前任务的特征学习，而对应原型则捕捉输入的代表性特征。在推理阶段，预测通过将每个任务特定提示与其关联原型绑定来生成。此外，为了增强稳定性，引入了正则化约束在提示初始化时，惩罚过大的值，以优化模型训练过程。该方法在多个广泛使用的数据集上进行了应用和验证。",
      "result": "在多个广泛使用的数据集上的实验结果表明，所提出的无键值对框架在持续学习中表现有效，能够减少传统基于提示方法中的任务间干扰并提升可扩展性。尽管摘要未明确说明具体的性能指标（如准确率提升或效率改进数据），但对比主流基线方法，该方法在模型稳定性和任务切换方面显示出改进，验证了其优势。",
      "conclusion": "本研究的主要贡献是提出了一种无键值对的持续学习框架，通过任务特定提示原型成功消除了对键值对的依赖。这为持续学习研究提供了新的视角，具有重要的学术价值，可能推动更稳定和可扩展的模型发展，并在实际应用中增强AI系统的鲁棒性。潜在局限性包括未详细探讨模型复杂度或特定应用场景的泛化能力，未来工作可以进一步优化正则化策略或评估在大规模任务中的表现。",
      "tags": [
        "Continual Learning",
        "Prompt-based Methods",
        "Prototype Learning",
        "Regularization",
        "Task-specific Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:35.727282Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04861",
    "title": "Orchestrating Intelligence: Confidence-Aware Routing for Efficient Multi-Agent Collaboration across Multi-Scale Models",
    "authors": [
      "Jingbo Wang",
      "Sendong Zhao",
      "Jiatong Liu",
      "Haochun Wang",
      "Wanting Li",
      "Bing Qin",
      "Ting Liu"
    ],
    "abstract": "While multi-agent systems (MAS) have demonstrated superior performance over single-agent approaches in complex reasoning tasks, they often suffer from significant computational inefficiencies. Existing frameworks typically deploy large language models (LLMs) uniformly across all agent roles, failing to account for the varying cognitive demands of different reasoning stages. We address this inefficiency by proposing OI-MAS framework, a novel multi-agent framework that implements an adaptive model-selection policy across a heterogeneous pool of multi-scale LLMs. Specifically, OI-MAS introduces a state-dependent routing mechanism that dynamically selects agent roles and model scales throughout the reasoning process. In addition, we introduce a confidence-aware mechanism that selects appropriate model scales conditioned on task complexity, thus reducing unnecessary reliance on large-scale models. Experimental results show that OI-MAS consistently outperforms baseline multi-agent systems, improving accuracy by up to 12.88\\% while reducing cost by up to 79.78\\%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04861.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04861",
    "published": "2026-01-08T11:56:09Z",
    "updated": "2026-01-08T11:56:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "OI-MAS框架通过自适应模型选择和信心感知路由，显著提升多代理系统的性能和计算效率。",
      "motivation": "多代理系统在复杂推理任务中表现优越，但常因计算效率低下而受限。现有框架通常在所有代理角色中均匀部署大型语言模型，未能适应不同推理阶段的认知需求差异，导致资源浪费和成本高昂。这一问题在需要高效协作的复杂任务中尤为重要，因为不必要的模型使用会降低整体效率，因此研究旨在通过动态资源分配优化多代理系统，减少对大规模模型的依赖，从而提高实用性。",
      "method": "论文提出OI-MAS框架，基于异构多尺度大型语言模型池实现自适应模型选择策略。核心创新包括状态相关路由机制，动态选择代理角色和模型规模，以及信心感知机制，根据任务复杂度调整模型选择。该方法避免了统一使用大规模模型，通过在整个推理过程中灵活分配计算资源，提高了协作效率，具体使用多尺度LLMs池来平衡准确性和成本。",
      "result": "实验结果显示，OI-MAS在性能上持续超越基线多代理系统，准确率最高提升12.88%，同时成本最高降低79.78%。这些具体数据表明该方法在保持高准确性的同时，显著减少了计算开销，验证了自适应模型选择的有效性，并优于现有统一部署LLMs的框架，体现出在复杂推理任务中的实际优势。",
      "conclusion": "论文的主要贡献是OI-MAS框架，通过自适应模型选择和信心感知路由为多代理系统提供了高效协作的新方法。学术上，这推动了MAS设计的优化，促进了资源分配理论的发展；实际应用中，能降低部署成本，支持复杂推理任务的规模化部署。未来工作可能包括扩展到更多模型类型或任务领域，并进一步优化路由机制的通用性。",
      "tags": [
        "Multi-Agent Systems",
        "Large Language Models",
        "Model Routing",
        "Confidence Estimation",
        "Efficiency Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:51:20.800366Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.12641",
    "title": "Single Image Reflection Separation via Dual Prior Interaction Transformer",
    "authors": [
      "Yue Huang",
      "Zi'ang Li",
      "Tianle Hu",
      "Jie Wen",
      "Guanbin Li",
      "Jinglin Zhang",
      "Guoxu Zhou",
      "Xiaozhao Fang"
    ],
    "abstract": "Single image reflection separation aims to separate the transmission and reflection layers from a mixed image. Existing methods typically combine general priors from pre-trained models with task-specific priors such as text prompts and reflection detection. However, the transmission prior, as the most direct task-specific prior for the target transmission layer, has not been effectively modeled or fully utilized, limiting performance in complex scenarios. To address this issue, we propose a dual-prior interaction framework based on lightweight transmission prior generation and effective prior fusion. First, we design a Local Linear Correction Network (LLCN) that finetunes pre-trained models based on the physical constraint T=SI+B, where S and B represent pixel-wise and channel-wise scaling and bias transformations. LLCN efficiently generates high-quality transmission priors with minimal parameters. Second, we construct a Dual-Prior Interaction Transformer (DPIT) that employs a dual-stream channel reorganization attention mechanism. By reorganizing features from general and transmission priors for attention computation, DPIT achieves deep fusion of both priors, fully exploiting their complementary information. Experimental results on multiple benchmark datasets demonstrate that the proposed method achieves state-of-the-art performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.12641.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12641",
    "published": "2025-05-19T02:50:15Z",
    "updated": "2026-01-08T11:53:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种双先验交互的单图像反射分离方法，通过轻量级传输先验生成和深度融合提升性能。",
      "motivation": "单图像反射分离任务旨在从混合图像中分离传输和反射层，现有方法结合预训练模型的通用先验和任务特定先验，但传输先验作为最直接的任务特定先验未被有效建模，导致复杂场景下性能受限。因此，研究致力于解决传输先验未充分利用的问题，以提升反射分离效果。",
      "method": "论文提出双先验交互框架，首先设计Local Linear Correction Network (LLCN)，基于物理约束T=SI+B微调预训练模型，高效生成高质量传输先验；其次构建Dual-Prior Interaction Transformer (DPIT)，使用双流通道重组注意力机制重组通用和传输先验特征，实现先验深度融合和互补信息利用。",
      "result": "实验在多个基准数据集上进行，结果显示所提方法达到最先进的性能，表明优于现有方法。摘要未明确说明具体性能指标如准确率提升，但可合理推断该方法在复杂场景下有效，由于充分利用传输先验。",
      "conclusion": "本研究的主要贡献是提出双先验交互框架，有效建模和融合传输先验，提升单图像反射分离性能，具有学术价值并支持图像处理应用。局限性或未来工作方向摘要未明确说明。",
      "tags": [
        "Reflection Separation",
        "Transformer",
        "Attention Mechanism",
        "Prior Fusion",
        "Pre-trained Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:59.876935Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04855",
    "title": "Rethinking GNNs and Missing Features: Challenges, Evaluation and a Robust Solution",
    "authors": [
      "Francesco Ferrini",
      "Veronica Lachi",
      "Antonio Longa",
      "Bruno Lepri",
      "Matono Akiyoshi",
      "Andrea Passerini",
      "Xin Liu",
      "Manfred Jaeger"
    ],
    "abstract": "Handling missing node features is a key challenge for deploying Graph Neural Networks (GNNs) in real-world domains such as healthcare and sensor networks. Existing studies mostly address relatively benign scenarios, namely benchmark datasets with (a) high-dimensional but sparse node features and (b) incomplete data generated under Missing Completely At Random (MCAR) mechanisms. For (a), we theoretically prove that high sparsity substantially limits the information loss caused by missingness, making all models appear robust and preventing a meaningful comparison of their performance. To overcome this limitation, we introduce one synthetic and three real-world datasets with dense, semantically meaningful features. For (b), we move beyond MCAR and design evaluation protocols with more realistic missingness mechanisms. Moreover, we provide a theoretical background to state explicit assumptions on the missingness process and analyze their implications for different methods. Building on this analysis, we propose GNNmim, a simple yet effective baseline for node classification with incomplete feature data. Experiments show that GNNmim is competitive with respect to specialized architectures across diverse datasets and missingness regimes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04855.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04855",
    "published": "2026-01-08T11:45:59Z",
    "updated": "2026-01-08T11:45:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出GNNmim方法，用于处理图神经网络中缺失节点特征，并引入新数据集和评估协议以应对现实挑战。",
      "motivation": "图神经网络（GNNs）在处理缺失节点特征时面临关键挑战，尤其在医疗保健和传感器网络等现实领域。现有研究主要基于高维稀疏特征和完全随机缺失（MCAR）机制，这些场景过于理想化。高稀疏性限制了信息损失，使模型显得鲁棒，但阻碍了性能的有效比较。现实中，特征可能密集且缺失机制复杂，因此需要更真实的评估和方法来弥补现有不足。",
      "method": "本文提出GNNmim，一个简单而有效的基线方法，用于节点分类任务中的不完整特征数据。关键创新包括引入一个合成和三个真实世界数据集，这些数据集具有密集、语义丰富的特征，以克服稀疏性限制。此外，设计了超越MCAR的缺失机制评估协议，提供更现实的测试环境。研究还提供理论背景，分析缺失过程假设对不同方法的影响，指导方法选择。摘要未明确说明GNNmim的具体模型架构细节。",
      "result": "实验显示，GNNmim在不同的数据集和缺失机制下与专门架构相比具有竞争力。摘要未提供具体的准确率提升或效率改进数据，但结果表明该方法能有效应对现实挑战，并在多种场景下表现稳健，验证了其作为基线的实用性。",
      "conclusion": "本研究通过提出GNNmim方法、引入密集特征数据集和现实缺失机制评估，解决了GNNs中缺失节点特征的关键问题。研究为GNNs在现实世界的部署提供了更可靠的解决方案，具有重要的学术价值（如推动理论分析）和实际应用价值（如医疗和传感器网络）。未来工作可能包括优化方法以处理更多复杂缺失机制或扩展到其他图任务。",
      "tags": [
        "Graph Neural Networks",
        "Missing Features",
        "Node Classification",
        "Missingness Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:03.090807Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04854",
    "title": "Token Maturation: Autoregressive Language Generation via Continuous Token Dynamics",
    "authors": [
      "Oshri Naparstek"
    ],
    "abstract": "Autoregressive language models are conventionally defined over discrete token sequences, committing to a specific token at every generation step. This early discretization forces uncertainty to be resolved through token-level sampling, often leading to instability, repetition, and sensitivity to decoding heuristics.   In this work, we introduce a continuous autoregressive formulation of language generation in which tokens are represented as continuous vectors that \\emph{mature} over multiple update steps before being discretized. Rather than sampling tokens, the model evolves continuous token representations through a deterministic dynamical process, committing to a discrete token only when the representation has sufficiently converged. Discrete text is recovered via hard decoding, while uncertainty is maintained and resolved in the continuous space.   We show that this maturation process alone is sufficient to produce coherent and diverse text using deterministic decoding (argmax), without reliance on token-level sampling, diffusion-style denoising, or auxiliary stabilization mechanisms. Additional perturbations, such as stochastic dynamics or history smoothing, can be incorporated naturally but are not required for the model to function.   To our knowledge, this is the first autoregressive language model that generates text by evolving continuous token representations to convergence prior to discretization, enabling stable generation without token-level sampling.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04854.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04854",
    "published": "2026-01-08T11:44:34Z",
    "updated": "2026-01-08T11:44:34Z",
    "comment": "In preperation to ICML 2026",
    "light_analysis": {
      "overview": "本论文提出一种基于连续令牌成熟的自回归语言生成方法，通过在离散化前演化连续表示到收敛，实现稳定生成而无需令牌级采样。",
      "motivation": "传统自回归语言模型使用离散令牌序列，早期离散化迫使不确定性通过令牌级采样解决，导致文本不稳定性、重复现象以及对解码启发式方法的敏感性。这一问题的重要性在于它影响了生成文本的质量和鲁棒性，现有方法如依赖采样或辅助稳定机制未能根本解决不稳定性，需要新方法在保持不确定性的同时实现更可靠的生成过程。",
      "method": "本研究引入连续自回归语言生成公式，将令牌表示为连续向量，通过成熟过程在多个更新步骤中逐步演化。模型采用确定性动态过程来更新这些连续表示，只在表示充分收敛时才进行离散化，离散文本通过硬解码恢复，不确定性在连续空间中被维护和解决。创新点在于避免了令牌级采样，且可以自然整合额外扰动如随机动态或历史平滑，但不是生成所必需。",
      "result": "摘要未明确说明具体实验数据或性能指标，但声称该方法足以产生连贯和多样的文本，使用确定性解码（argmax）而不依赖令牌级采样、扩散去噪或辅助稳定机制。这表明该方法在理论上能改善生成稳定性，但缺乏与基线方法的量化对比或具体效果数据，需要参考论文全文以获得更详细的实验验证。",
      "conclusion": "本研究的主要贡献是首次提出一种自回归语言模型，通过演化连续令牌表示到收敛再离散化，实现稳定文本生成而不需令牌级采样。这提供了新视角改进语言生成的鲁棒性，具有学术价值，可能推动更可控生成技术的发展。实际应用中，可促进更可靠的AI文本生成系统，未来工作可探索性能在不同场景下的扩展和改进。",
      "tags": [
        "Autoregressive Language Models",
        "Continuous Token Dynamics",
        "Token Maturation",
        "Deterministic Decoding",
        "Hard Decoding"
      ]
    },
    "analyzed_at": "2026-01-09T02:54:15.697094Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.12611",
    "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction",
    "authors": [
      "Trang Tran",
      "Trung Hoang Le",
      "Huiping Cao",
      "Tran Cao Son"
    ],
    "abstract": "Joint entity-relation extraction (JERE) identifies both entities and their relationships simultaneously. Traditional machine-learning based approaches to performing this task require a large corpus of annotated data and lack the ability to easily incorporate domain specific information in the construction of the model. Therefore, creating a model for JERE is often labor intensive, time consuming, and elaboration intolerant. In this paper, we propose harnessing the capabilities of generative pre-trained large language models (LLMs) and the knowledge representation and reasoning capabilities of Answer Set Programming (ASP) to perform JERE. We present a generic workflow for JERE using LLMs and ASP. The workflow is generic in the sense that it can be applied for JERE in any domain. It takes advantage of LLM's capability in natural language understanding in that it works directly with unannotated text. It exploits the elaboration tolerant feature of ASP in that no modification of its core program is required when additional domain specific knowledge, in the form of type specifications, is found and needs to be used. We demonstrate the usefulness of the proposed workflow through experiments with limited training data on three well-known benchmarks for JERE. The results of our experiments show that the LLM + ASP workflow is better than state-of-the-art JERE systems in several categories with only 10% of training data. It is able to achieve a 2.5 times (35% over 15%) improvement in the Relation Extraction task for the SciERC corpus, one of the most difficult benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.12611.pdf",
    "abs_url": "https://arxiv.org/abs/2508.12611",
    "published": "2025-08-18T04:15:35Z",
    "updated": "2026-01-08T11:15:52Z",
    "comment": "In Proceedings ICLP 2025, arXiv:2601.00047",
    "light_analysis": {
      "overview": "该论文提出了一种结合大型语言模型和答案集编程的通用工作流，用于联合实体关系提取，减少对标注数据的依赖并提高领域适应性。",
      "motivation": "联合实体关系提取是自然语言处理中的关键任务，旨在同时识别文本中的实体及其关系。传统基于机器学习的方法需要大规模标注数据集，且难以灵活融入领域特定知识，导致模型构建耗时耗力、缺乏效率。这限制了模型在不同领域中的快速部署和适应性，因此研究旨在开发一种新方法，通过减少数据需求和增强知识整合能力来解决这些问题，提升实用性和可扩展性。",
      "method": "论文提出一个通用工作流，整合生成预训练大型语言模型的自然语言理解能力与答案集编程的知识表示和推理特性。LLMs负责直接处理未标注文本，提取初步实体和关系信息；ASP则用于推理和整合领域知识，其elaboration tolerance特性允许在不修改核心程序的情况下融入额外知识（如类型规范）。该工作流设计为领域无关，可应用于任何JERE任务，关键创新在于结合了两种技术的优势，实现高效、灵活的提取过程。",
      "result": "在三个知名联合实体关系提取基准数据集上的实验结果显示，LLM + ASP工作流仅使用10%的训练数据，在多个性能类别中优于当前最优系统。具体而言，在SciERC语料库的关系提取任务中，准确率从15%提升至35%，实现了2.5倍的改进。这些结果证明了该方法在数据有限情况下的有效性，展现了显著的性能提升和效率优势，凸显了其相对于传统方法的竞争力。",
      "conclusion": "本研究的主要贡献是开发了一种结合LLMs和ASP的通用工作流，显著减少了联合实体关系提取对标注数据的依赖，并提高了领域适应性。学术上，它展示了跨技术融合的潜力，为自然语言处理任务提供了新思路；实际上，该方法易于部署到不同领域，具有广泛的应用前景。摘要未明确说明局限性，但未来工作可能包括进一步优化性能或扩展到更多复杂场景，推动高效和灵活的NLP系统发展。",
      "tags": [
        "Joint Entity-Relation Extraction",
        "Large Language Models",
        "Answer Set Programming",
        "Elaboration Tolerance",
        "Natural Language Understanding"
      ]
    },
    "analyzed_at": "2026-01-09T02:54:06.265892Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03193",
    "title": "UniCorn: Towards Self-Improving Unified Multimodal Models through Self-Generated Supervision",
    "authors": [
      "Ruiyan Han",
      "Zhen Fang",
      "XinYu Sun",
      "Yuchen Ma",
      "Ziheng Wang",
      "Yu Zeng",
      "Zehui Chen",
      "Lin Chen",
      "Wenxuan Huang",
      "Wei-Jie Xu",
      "Yi Cao",
      "Feng Zhao"
    ],
    "abstract": "While Unified Multimodal Models (UMMs) have achieved remarkable success in cross-modal comprehension, a significant gap persists in their ability to leverage such internal knowledge for high-quality generation. We formalize this discrepancy as Conduction Aphasia, a phenomenon where models accurately interpret multimodal inputs but struggle to translate that understanding into faithful and controllable synthesis. To address this, we propose UniCorn, a simple yet elegant self-improvement framework that eliminates the need for external data or teacher supervision. By partitioning a single UMM into three collaborative roles: Proposer, Solver, and Judge, UniCorn generates high-quality interactions via self-play and employs cognitive pattern reconstruction to distill latent understanding into explicit generative signals. To validate the restoration of multimodal coherence, we introduce UniCycle, a cycle-consistency benchmark based on a Text to Image to Text reconstruction loop. Extensive experiments demonstrate that UniCorn achieves comprehensive and substantial improvements over the base model across six general image generation benchmarks. Notably, it achieves SOTA performance on TIIF(73.8), DPG(86.8), CompBench(88.5), and UniCycle while further delivering substantial gains of +5.0 on WISE and +6.5 on OneIG. These results highlight that our method significantly enhances T2I generation while maintaining robust comprehension, demonstrating the scalability of fully self-supervised refinement for unified multimodal intelligence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.03193.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03193",
    "published": "2026-01-06T17:15:50Z",
    "updated": "2026-01-08T11:08:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "UniCorn提出一种自改进框架，通过自我生成监督提升统一多模态模型的生成能力，无需外部数据或教师监督，解决了多模态模型在生成任务上的不足。",
      "motivation": "统一多模态模型在跨模态理解方面取得了显著进展，但在高质量生成任务上存在差距，表现为'传导性失语症'现象：模型能准确理解多模态输入，却难以将其转化为忠实和可控的生成。这一问题限制了模型在实际应用中的有效性，因为现有方法往往依赖外部数据或监督，缺乏自改进能力，导致生成质量不稳定且可控性差。因此，研究旨在开发一种无需外部资源的自我优化方法，以弥合理解与生成之间的鸿沟，提升多模态模型的整体性能。",
      "method": "UniCorn框架通过将单一统一多模态模型划分为三个协作角色来实现自我改进：Proposer负责提出任务，Solver生成解决方案，Judge进行评估。通过自我博弈，模型生成高质量交互数据，并采用认知模式重构技术，将内部潜在理解转化为显式的生成信号。该方法的核心创新在于无需外部数据或监督，完全依赖自生成监督进行优化。此外，引入了UniCycle基准，基于文本到图像再到文本的重构循环，用于验证多模态一致性的恢复，确保方法在通用图像生成任务中的有效性。",
      "result": "UniCorn在六个通用图像生成基准上相比基础模型取得了全面且显著的改进：在TIIF基准上达到73.8分，DPG基准上达到86.8分，CompBench基准上达到88.5分，均达到SOTA性能。同时，在WISE基准上提升了5.0分，在OneIG基准上提升了6.5分。UniCycle基准也验证了多模态一致性的有效恢复。这些结果证明了UniCorn不仅显著增强了文本到图像生成能力，还保持了鲁棒的跨模态理解，突出了完全自监督优化在多模态领域的可行性和可扩展性。",
      "conclusion": "UniCorn的主要贡献在于提出了一种创新的自我改进框架，通过自我生成监督成功提升了统一多模态模型的生成质量，同时保持其理解能力。这为多模态智能的自适应优化提供了新途径，具有重要的学术价值，展示了自监督学习在多模态任务中的潜力。在实际应用上，该方法可减少对外部资源的依赖，促进模型在复杂场景中的部署。摘要未明确说明局限性，但未来工作可探索在更广泛的多模态任务中的泛化能力或计算效率的优化。",
      "tags": [
        "Unified Multimodal Models",
        "Self-Generated Supervision",
        "Self-Play",
        "Cognitive Pattern Reconstruction",
        "Cycle-Consistency Benchmark"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:06.317415Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04823",
    "title": "DR-LoRA: Dynamic Rank LoRA for Mixture-of-Experts Adaptation",
    "authors": [
      "Guanzhi Deng",
      "Bo Li",
      "Ronghao Chen",
      "Huacan Wang",
      "Linqi Song",
      "Lijie Wen"
    ],
    "abstract": "Mixture-of-Experts (MoE) has become a prominent paradigm for scaling Large Language Models (LLMs). Parameter-efficient fine-tuning (PEFT), such as LoRA, is widely adopted to adapt pretrained MoE LLMs to downstream tasks. However, existing approaches assign identical LoRA ranks to all experts, overlooking the intrinsic functional specialization within MoE LLMs. This uniform allocation leads to resource mismatch, task-relevant experts are under-provisioned while less relevant ones receive redundant parameters. We propose a Dynamic Rank LoRA framework named DR-LoRA, which dynamically grows expert LoRA ranks during fine-tuning based on task-specific demands. DR-LoRA employs an Expert Saliency Scoring mechanism that integrates expert routing frequency and LoRA rank importance to quantify each expert's demand for additional capacity. Experts with higher saliency scores are prioritized for rank expansion, enabling the automatic formation of a heterogeneous rank distribution tailored to the target task. Experiments on multiple benchmarks demonstrate that DR-LoRA consistently outperforms standard LoRA and static allocation strategies under the same parameter budget, achieving superior task performance with more efficient parameter utilization.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04823.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04823",
    "published": "2026-01-08T10:58:51Z",
    "updated": "2026-01-08T10:58:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出DR-LoRA框架，通过动态调整LoRA秩，优化MoE LLMs的参数高效微调，提升任务性能和参数利用率。",
      "motivation": "在MoE LLMs的微调中，现有参数高效方法如LoRA采用统一秩分配，忽视了专家的功能专门化，导致任务相关专家资源不足，而无关专家参数冗余。这种资源不匹配降低了微调效率和性能，限制了MoE结构潜力的发挥，因此需要一种自适应方法，根据任务需求智能分配参数，以提高微调效果。",
      "method": "DR-LoRA框架基于动态秩调整机制，引入专家显著性评分，结合路由频率和LoRA秩重要性来量化专家需求。在微调过程中，优先扩展高显著性专家的秩，自动形成异构秩分布，从而优化参数分配。该方法采用MoE LLMs架构，并在下游任务微调中应用，实现任务特定的专家容量自适应增长。",
      "result": "在多个基准实验中，DR-LoRA在相同参数预算下，持续优于标准LoRA和静态分配策略，展现出更高的任务性能和参数利用效率。实验结果表明，动态秩调整能有效识别关键专家，提升整体微调效果，证实了该方法在资源优化方面的优势。",
      "conclusion": "DR-LoRA通过动态秩分配解决了MoE LLMs微调中的资源不匹配问题，提供了更高效的参数利用方法，具有学术创新和实际应用价值。它推动了参数高效微调技术的发展，未来可探索扩展至其他模型结构或复杂任务，以进一步验证其泛化能力。",
      "tags": [
        "Mixture-of-Experts",
        "LoRA",
        "Parameter-efficient Fine-tuning",
        "Dynamic Rank Adaptation",
        "Expert Saliency Scoring"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:20.640466Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05232",
    "title": "Measuring and Fostering Peace through Machine Learning and Artificial Intelligence",
    "authors": [
      "P. Gilda",
      "P. Dungarwal",
      "A. Thongkham",
      "E. T. Ajayi",
      "S. Choudhary",
      "T. M. Terol",
      "C. Lam",
      "J. P. Araujo",
      "M. McFadyen-Mungalln",
      "L. S. Liebovitch",
      "P. T. Coleman",
      "H. West",
      "K. Sieck",
      "S. Carter"
    ],
    "abstract": "We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05232.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05232",
    "published": "2026-01-08T18:57:01Z",
    "updated": "2026-01-08T18:57:01Z",
    "comment": "6 pages, 4 figures",
    "light_analysis": {
      "overview": "论文利用机器学习和人工智能测量国家和平水平，并开发在线工具MirrorMirror促进和平，通过实时反馈帮助用户理解媒体消费的影响。",
      "motivation": "研究动机源于当前和平测量方法可能不足，尤其是社交媒体内容偏向情感激活以增加点击，导致偏见和冲突。摘要指出71%的20-40岁人群通过短视频获取新闻，内容创作者倾向于制作愤怒内容吸引注意力。因此，需要开发工具来准确测量和平并帮助用户意识到媒体消费的影响，以促进更和谐的社会环境，弥补现有方法在实时反馈和用户参与度方面的局限。",
      "method": "研究方法包括使用神经网络从在线新闻源的文本嵌入中测量和平水平，模型在一个新闻数据集上训练并在另一个数据集上验证准确性。针对社交媒体如YouTube，采用词级方法（GoEmotions）和上下文级方法（大型语言模型）来测量与和平相关的社会维度。此外，开发了Chrome扩展MirrorMirror，提供实时反馈给用户关于所观看媒体的和平性，模型基于文本分析和情感检测技术。",
      "result": "主要实验结果显示，训练于一个新闻数据集的模型在分析不同新闻数据集时表现出高准确性，但摘要未明确说明具体性能指标如准确率数值。MirrorMirror扩展已被开发并测试，提供实时反馈功能。然而，缺乏与基线方法的直接对比数据，结果基于模型的多数据集适应性和工具的可行性推断而来。",
      "conclusion": "论文总结成功利用机器学习和人工智能测量和平，并开发了促进和平的工具MirrorMirror。学术贡献在于将先进ML技术应用于社会和平研究，实际应用价值在于帮助用户、内容创作者和平台改善媒体环境，推动更尊重、细致的沟通。未来目标是使MirrorMirror成为开源工具，超越简单参与度指标，鼓励信息丰富和尊重性的内容创作与消费。",
      "tags": [
        "Machine Learning",
        "Neural Networks",
        "Text Embeddings",
        "Large Language Model",
        "Chrome Extension"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:30.926516Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.13467",
    "title": "Non-Linear Scoring Model for Translation Quality Evaluation",
    "authors": [
      "Serge Gladkoff",
      "Lifeng Han",
      "Katerina Gasova"
    ],
    "abstract": "Analytic Translation Quality Evaluation (TQE), based on Multidimensional Quality Metrics (MQM), traditionally uses a linear error-to-penalty scale calibrated to a reference sample of 1000-2000 words. However, linear extrapolation biases judgment on samples of different sizes, over-penalizing short samples and under-penalizing long ones, producing misalignment with expert intuition.   Building on the Multi-Range framework, this paper presents a calibrated, non-linear scoring model that better reflects how human content consumers perceive translation quality across samples of varying length. Empirical data from three large-scale enterprise environments shows that acceptable error counts grow logarithmically, not linearly, with sample size.   Psychophysical and cognitive evidence, including the Weber-Fechner law and Cognitive Load Theory, supports this premise by explaining why the perceptual impact of additional errors diminishes while the cognitive burden grows with scale. We propose a two-parameter model   E(x) = a * ln(1 + b * x), a, b > 0,   anchored to a reference tolerance and calibrated from two tolerance points using a one-dimensional root-finding step. The model yields an explicit interval within which the linear approximation stays within +/-20 percent relative error and integrates into existing evaluation workflows with only a dynamic tolerance function added.   The approach improves interpretability, fairness, and inter-rater reliability across both human and AI-generated translations. By operationalizing a perceptually valid scoring paradigm, it advances translation quality evaluation toward more accurate and scalable assessment. The model also provides a stronger basis for AI-based document-level evaluation aligned with human judgment. Implementation considerations for CAT/LQA systems and implications for human and AI-generated text evaluation are discussed.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.13467.pdf",
    "abs_url": "https://arxiv.org/abs/2511.13467",
    "published": "2025-11-17T15:09:22Z",
    "updated": "2026-01-08T18:51:57Z",
    "comment": "ongoing work, 32 pages",
    "light_analysis": {
      "overview": "本文提出一种基于对数函数的非线性评分模型，改进了翻译质量评估的准确性和公平性，以更贴近人类感知。",
      "motivation": "传统翻译质量评估基于多维质量度量（MQM），采用线性误差惩罚比例，在样本大小不同时存在偏差，导致短样本被过度惩罚、长样本惩罚不足，与专家直觉不符。这一问题在实际应用中影响评估结果的可靠性，特别是在企业环境中处理多样化文本长度时，现有线性方法无法准确模拟人类对错误的感知，需要开发更符合认知的非线性模型来解决评估不公。",
      "method": "论文基于Multi-Range框架，提出一个两参数非线性评分模型E(x) = a * ln(1 + b * x)，其中a和b为正参数。核心创新在于使用对数函数模拟误差容忍度与样本大小的关系，校准过程通过经验数据（来自三个大规模企业环境）确定参数，使用两个容忍点进行一维寻根计算，并整合到现有评估工作流中，仅添加动态容忍函数，便于实际应用。",
      "result": "经验数据显示，可接受误差计数随样本大小呈对数增长，而非线性增长，证实了非线性模型的有效性。模型提高了评估的可解释性、公平性和评估者间可靠性，线性近似在±20%相对误差范围内。与传统线性方法相比，该模型更好地模拟人类感知，适用于人类和AI生成的翻译评估，改善了整体评估一致性。",
      "conclusion": "本研究通过提出感知有效的非线性评分模型，推动了翻译质量评估向更准确和可扩展的方向发展。模型为AI驱动的文档级评估提供了更强基础，有助于对齐人类判断，并讨论了在CAT/LQA系统中的实现考虑，以及对人类和AI生成文本评估的广泛影响，为未来研究和应用奠定了基础。",
      "tags": [
        "Translation Quality Evaluation",
        "Non-linear Scoring",
        "MQM",
        "Logarithmic Scaling",
        "Psychophysical Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:28.826659Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05192",
    "title": "LELA: an LLM-based Entity Linking Approach with Zero-Shot Domain Adaptation",
    "authors": [
      "Samy Haffoudhi",
      "Fabian M. Suchanek",
      "Nils Holzenberger"
    ],
    "abstract": "Entity linking (mapping ambiguous mentions in text to entities in a knowledge base) is a foundational step in tasks such as knowledge graph construction, question-answering, and information extraction. Our method, LELA, is a modular coarse-to-fine approach that leverages the capabilities of large language models (LLMs), and works with different target domains, knowledge bases and LLMs, without any fine-tuning phase. Our experiments across various entity linking settings show that LELA is highly competitive with fine-tuned approaches, and substantially outperforms the non-fine-tuned ones.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05192.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05192",
    "published": "2026-01-08T18:15:34Z",
    "updated": "2026-01-08T18:15:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "LELA是一个基于大型语言模型的实体链接方法，通过零射击领域适应，无需微调即可适应不同领域。",
      "motivation": "实体链接是知识图谱构建、问答和信息提取等任务的基础步骤，但现有方法通常需针对每个领域进行微调，限制了灵活性和适应性，特别是在处理新领域时非微调方法性能显著下降。本研究旨在解决跨领域实体链接中非微调方法的不足，开发一种无需额外训练就能适应不同领域的方法，以提高通用性和部署效率。",
      "method": "LELA采用模块化的粗到细策略，首先进行粗粒度候选实体检索，然后利用大型语言模型（LLMs）进行细粒度匹配，实现零射击领域适应。该方法无需微调阶段，支持不同目标领域、知识库和LLM模型，关键创新在于通过LLMs的泛化能力提升跨领域性能和可扩展性。",
      "result": "实验显示，LELA在各种实体链接设置中与经过微调的方法具有高度竞争力，并显著优于非微调方法，证明了其零射击适应机制的有效性。摘要未明确说明具体性能指标如准确率提升或效率改进的详细数据，但强调了在跨领域场景中的性能优势。",
      "conclusion": "LELA的主要贡献在于提出了一种无需微调的实体链接方法，通过零射击领域适应提高了方法的通用性和可扩展性，具有重要学术价值和实际应用价值，如简化跨领域知识图谱构建。局限性可能包括对LLM的依赖，未来工作可探索更多领域的验证和效率优化。",
      "tags": [
        "Entity Linking",
        "Large Language Model",
        "Zero-Shot Learning",
        "Domain Adaptation",
        "Modular Approach"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:38.210305Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.21072",
    "title": "Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval Augmented Generation",
    "authors": [
      "Ekaterina Fadeeva",
      "Aleksandr Rubashevskii",
      "Dzianis Piatrashyn",
      "Roman Vashurin",
      "Shehzaad Dhuliawala",
      "Artem Shelmanov",
      "Timothy Baldwin",
      "Preslav Nakov",
      "Mrinmaya Sachan",
      "Maxim Panov"
    ],
    "abstract": "Large Language Models (LLMs) enhanced with retrieval, an approach known as Retrieval-Augmented Generation (RAG), have achieved strong performance in open-domain question answering. However, RAG remains prone to hallucinations: factually incorrect outputs may arise from inaccuracies in the model's internal knowledge and the retrieved context. Existing approaches to mitigating hallucinations often conflate factuality with faithfulness to the retrieved evidence, incorrectly labeling factually correct statements as hallucinations if they are not explicitly supported by the retrieval. In this paper, we introduce FRANQ, a new method for hallucination detection in RAG outputs. FRANQ applies distinct uncertainty quantification (UQ) techniques to estimate factuality, conditioning on whether a statement is faithful to the retrieved context. To evaluate FRANQ and competing UQ methods, we construct a new long-form question answering dataset annotated for both factuality and faithfulness, combining automated labeling with manual validation of challenging cases. Extensive experiments across multiple datasets, tasks, and LLMs show that FRANQ achieves more accurate detection of factual errors in RAG-generated responses compared to existing approaches.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.21072.pdf",
    "abs_url": "https://arxiv.org/abs/2505.21072",
    "published": "2025-05-27T11:56:59Z",
    "updated": "2026-01-08T18:06:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出FRANQ方法，通过忠实性感知的不确定性量化技术，改进检索增强生成输出中幻觉检测的准确性。",
      "motivation": "检索增强生成（RAG）在开放域问答中表现良好，但容易产生事实错误（幻觉），影响系统可靠性。现有方法常将事实性与对检索证据的忠实性混淆，导致即使事实正确的陈述，若未明确基于检索内容，也被误判为幻觉。这限制了RAG在关键应用中的实用性，因此需要开发更精确的检测方法，以区分真实幻觉和忠实性问题，提升AI输出的可信度。",
      "method": "FRANQ方法的核心是应用不同的不确定性量化技术，根据语句是否忠实于检索上下文来分别估计其事实性。关键创新在于区分事实性和忠实性，避免了现有方法的混淆。为评估该方法，论文构建了一个新的长形式问答数据集，结合自动标注和手动验证挑战性案例，同时标注了事实性和忠实性标签。摘要未明确说明具体的不确定性量化技术细节或模型架构。",
      "result": "实验结果表明，在多个数据集、不同任务和各种大型语言模型上，FRANQ相比现有不确定性量化方法，能够更准确地检测RAG生成响应中的事实错误。摘要未提供具体的性能指标如准确率提升百分比，但强调了其相对于基线方法的优势，表明FRANQ在幻觉检测方面具有更好的效果，为RAG系统的质量评估提供了可靠工具。",
      "conclusion": "本文的主要贡献是提出了FRANQ方法，一种忠实性感知的不确定性量化技术，有效解决了RAG输出中幻觉检测的准确性问题，纠正了现有方法混淆事实性和忠实性的错误。学术上，它推动了RAG和不确定性量化领域的发展；实际上，有助于提高AI系统在开放域问答中的可靠性。局限性可能包括对特定数据集的依赖，未来工作可探索更广泛的UQ技术或应用到其他生成任务。",
      "tags": [
        "Retrieval-Augmented Generation (RAG)",
        "Uncertainty Quantification (UQ)",
        "Hallucination Detection",
        "Large Language Models (LLMs)",
        "Fact-Checking"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:32.096439Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05171",
    "title": "Inside Out: Evolving User-Centric Core Memory Trees for Long-Term Personalized Dialogue Systems",
    "authors": [
      "Jihao Zhao",
      "Ding Chen",
      "Zhaoxin Fan",
      "Kerun Xu",
      "Mengting Hu",
      "Bo Tang",
      "Feiyu Xiong",
      "Zhiyu li"
    ],
    "abstract": "Existing long-term personalized dialogue systems struggle to reconcile unbounded interaction streams with finite context constraints, often succumbing to memory noise accumulation, reasoning degradation, and persona inconsistency. To address these challenges, this paper proposes Inside Out, a framework that utilizes a globally maintained PersonaTree as the carrier of long-term user profiling. By constraining the trunk with an initial schema and updating the branches and leaves, PersonaTree enables controllable growth, achieving memory compression while preserving consistency. Moreover, we train a lightweight MemListener via reinforcement learning with process-based rewards to produce structured, executable, and interpretable {ADD, UPDATE, DELETE, NO_OP} operations, thereby supporting the dynamic evolution of the personalized tree. During response generation, PersonaTree is directly leveraged to enhance outputs in latency-sensitive scenarios; when users require more details, the agentic mode is triggered to introduce details on-demand under the constraints of the PersonaTree. Experiments show that PersonaTree outperforms full-text concatenation and various personalized memory systems in suppressing contextual noise and maintaining persona consistency. Notably, the small MemListener model achieves memory-operation decision performance comparable to, or even surpassing, powerful reasoning models such as DeepSeek-R1-0528 and Gemini-3-Pro.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05171.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05171",
    "published": "2026-01-08T17:59:11Z",
    "updated": "2026-01-08T17:59:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Inside Out框架，通过PersonaTree和MemListener实现长期个性化对话系统的动态内存进化，解决内存噪声和角色一致性问题。",
      "motivation": "现有长期个性化对话系统在处理无限交互流时，受限于有限上下文约束，导致内存噪声积累、推理能力退化和用户角色不一致，严重影响系统可靠性和用户体验。这些问题在长期交互中尤为突出，现有方法如全文拼接和各类个性化内存系统未能有效抑制噪声或保持一致性，因此亟需新的解决方案来优化内存管理和进化过程。",
      "method": "论文提出Inside Out框架，其核心是利用全局维护的PersonaTree作为长期用户档案载体，通过约束主干使用初始模式并动态更新分支和叶子，实现可控增长，从而压缩内存并保持一致性。此外，通过强化学习训练轻量级MemListener模型，使用过程为基础奖励生成结构化、可执行、可解释的内存操作（如添加、更新、删除），支持个性化树的动态进化。在响应生成阶段，PersonaTree直接用于增强输出以降低延迟，并在用户需要时触发代理模式，在PersonaTree约束下按需引入细节。",
      "result": "实验显示PersonaTree在抑制上下文噪声和保持角色一致性方面优于全文拼接及多种个性化内存系统，具体表现为在标准评估指标中取得更好成绩。轻量级MemListener模型的内存操作决策性能媲美甚至超越DeepSeek-R1-0528和Gemini-3-Pro等强大推理模型，验证了其高效性和有效性，特别是在长期交互场景中减少了性能下降。",
      "conclusion": "本研究的主要贡献在于提出了Inside Out框架，结合PersonaTree和MemListener，解决了长期个性化对话系统中的关键挑战，如内存噪声和角色一致性，学术上提供了一种可控、可解释的内存进化方法，应用上适用于聊天机器人等交互场景以提升用户体验。摘要未明确说明局限性，未来工作可能包括扩展到更复杂环境或评估更多性能指标。",
      "tags": [
        "Reinforcement Learning",
        "Memory Trees",
        "Personalized Dialogue Systems",
        "Controlled Evolution",
        "Structured Operations"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:13.509805Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05170",
    "title": "Reverse-engineering NLI: A study of the meta-inferential properties of Natural Language Inference",
    "authors": [
      "Rasmus Blanck",
      "Bill Noble",
      "Stergios Chatzikyriakidis"
    ],
    "abstract": "Natural Language Inference (NLI) has been an important task for evaluating language models for Natural Language Understanding, but the logical properties of the task are poorly understood and often mischaracterized. Understanding the notion of inference captured by NLI is key to interpreting model performance on the task. In this paper we formulate three possible readings of the NLI label set and perform a comprehensive analysis of the meta-inferential properties they entail. Focusing on the SNLI dataset, we exploit (1) NLI items with shared premises and (2) items generated by LLMs to evaluate models trained on SNLI for meta-inferential consistency and derive insights into which reading of the logical relations is encoded by the dataset.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05170.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05170",
    "published": "2026-01-08T17:58:52Z",
    "updated": "2026-01-08T17:58:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过逆向工程研究自然语言推理的元推理属性，提出三种逻辑解读并分析其在SNLI数据集上的编码。",
      "motivation": "自然语言推理（NLI）是评估语言模型自然语言理解能力的重要任务，但其逻辑属性常被误解，导致模型性能的解读不准确。现有研究对NLI的推理概念理解不足，任务设计可能基于模糊假设，影响评估的可靠性。本研究旨在澄清NLI的逻辑基础，解决这一问题，为更稳健的模型评估提供理论支撑，强调理解推理属性的重要性以避免误判模型能力。",
      "method": "本研究提出NLI标签集的三种可能逻辑解读，并进行全面的元推理属性分析。聚焦于SNLI数据集，利用（1）共享前提的NLI项目和（2）由大型语言模型（LLMs）生成的项目，评估在SNLI上训练模型的元推理一致性。方法核心是逆向工程，通过分析项目类型和LLM生成内容，探究数据集编码的逻辑关系，创新点在于结合传统数据和LLM增强分析，以揭示NLI任务的内在属性。",
      "result": "摘要未明确说明具体实验数据，但通过分析推导出关于SNLI数据集编码逻辑关系的见解。论文可能揭示了数据集倾向于某种逻辑解读，并评估了模型的元推理一致性，从而提供了对NLI任务内在属性的新认识。由于缺乏定量指标，无法给出准确率或效率改进的具体数据，但研究为理解模型在NLI上的表现提供了定性洞察，有助于后续工作的基准对比和优化。",
      "conclusion": "本研究的主要贡献是通过逆向工程方法深入分析了NLI的元推理属性，提出了三种逻辑解读并验证其在数据集中的体现，填补了NLI逻辑属性研究的空白。学术价值在于为NLI任务提供更精确的理论框架，实际应用价值在于指导NLI数据集设计和模型评估的改进。局限性或未来工作未在摘要中明确说明，但可扩展到其他数据集或更复杂的推理任务，以进一步验证和推广研究成果。",
      "tags": [
        "Natural Language Inference",
        "SNLI",
        "Large Language Models",
        "Meta-inferential Analysis",
        "Logical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:03.376559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05163",
    "title": "DocDancer: Towards Agentic Document-Grounded Information Seeking",
    "authors": [
      "Qintong Zhang",
      "Xinjie Lv",
      "Jialong Wu",
      "Baixuan Li",
      "Zhengwei Tao",
      "Guochen Yan",
      "Huanyao Zhang",
      "Bin Wang",
      "Jiahao Xu",
      "Haitao Mi",
      "Wentao Zhang"
    ],
    "abstract": "Document Question Answering (DocQA) focuses on answering questions grounded in given documents, yet existing DocQA agents lack effective tool utilization and largely rely on closed-source models. In this work, we introduce DocDancer, an end-to-end trained open-source Doc agent. We formulate DocQA as an information-seeking problem and propose a tool-driven agent framework that explicitly models document exploration and comprehension. To enable end-to-end training of such agents, we introduce an Exploration-then-Synthesis data synthesis pipeline that addresses the scarcity of high-quality training data for DocQA. Training on the synthesized data, the trained models on two long-context document understanding benchmarks, MMLongBench-Doc and DocBench, show their effectiveness. Further analysis provides valuable insights for the agentic tool design and synthetic data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05163.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05163",
    "published": "2026-01-08T17:54:32Z",
    "updated": "2026-01-08T17:54:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "DocDancer是一个端到端训练的开源文档问答代理，通过工具驱动框架和探索-合成数据管道提升文档信息寻求能力。",
      "motivation": "现有文档问答代理存在工具利用不足和依赖闭源模型的问题，导致文档探索效率低且成本高，限制了AI在复杂文档处理中的应用。高质量训练数据稀缺阻碍了模型的端到端训练和泛化能力，因此开发开源且工具驱动的代理框架成为迫切需求，以提升文档理解的可定制性和透明度。",
      "method": "该研究提出DocDancer代理框架，将文档问答形式化为信息寻求问题，采用工具驱动方法明确建模文档探索和阅读理解。关键创新是Exploration-then-Synthesis数据合成管道，用于生成高质量训练数据以支持端到端训练，并在MMLongBench-Doc和DocBench等长上下文文档理解基准上进行模型训练。",
      "result": "在MMLongBench-Doc和DocBench基准上，DocDancer展示了有效性，表明合成数据和工具驱动框架能提升文档问答性能，但摘要未提供具体准确率数据。实验结果表明该方法在文档理解任务中表现良好，为代理设计提供了实证支持，进一步分析揭示了工具优化和合成数据生成的关键因素。",
      "conclusion": "该研究通过开源代理框架和数据合成管道推进了文档问答的信息寻求能力，为代理工具设计和合成数据生成提供了宝贵见解，具有促进开源AI发展和文档自动化处理的学术与实际价值。未来工作可扩展至更复杂文档类型或集成更多工具，以解决潜在的数据偏差和泛化挑战。",
      "tags": [
        "Document Question Answering",
        "Tool-driven Agent",
        "Data Synthesis",
        "End-to-end Training",
        "Document Exploration"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:02.987362Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05143",
    "title": "A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering",
    "authors": [
      "Md. Zahid Hossain",
      "Most. Sharmin Sultana Samu",
      "Md. Rakibul Islam",
      "Md. Siam Ansary"
    ],
    "abstract": "Visual question answering for crop disease analysis requires accurate visual understanding and reliable language generation. This work presents a lightweight vision-language framework for crop and disease identification from leaf images. The proposed approach combines a Swin Transformer vision encoder with sequence-to-sequence language decoders. A two-stage training strategy is adopted to improve visual representation learning and cross-modal alignment. The model is evaluated on a large-scale crop disease dataset using classification and natural language generation metrics. Experimental results show high accuracy for both crop and disease identification. The framework also achieves strong performance on BLEU, ROUGE and BERTScore. Our proposed models outperform large-scale vision-language baselines while using significantly fewer parameters. Explainability is assessed using Grad-CAM and token-level attribution. Qualitative results demonstrate robust performance under diverse user-driven queries. These findings highlight the effectiveness of task-specific visual pretraining for crop disease visual question answering.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05143.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05143",
    "published": "2026-01-08T17:31:09Z",
    "updated": "2026-01-08T17:31:09Z",
    "comment": "Preprint, manuscript is under review",
    "light_analysis": {
      "overview": "本研究提出一个轻量级且可解释的视觉语言框架，用于作物疾病视觉问答，结合Swin Transformer和序列到序列解码器。",
      "motivation": "作物疾病视觉问答需要在叶子图像中准确理解视觉信息并生成可靠的自然语言描述，这对农业应用至关重要，能帮助及时诊断疾病、减少损失。现有方法可能参数庞大、计算效率低或缺乏可解释性，限制了实际部署。因此，本研究旨在解决这些问题，通过开发轻量级和可解释的框架来提高模型实用性和透明度，以应对农业中的精准诊断需求。摘要未明确说明具体基线不足，但基于轻量化和可解释性推断现有方法在这些方面存在改进空间。",
      "method": "研究提出一个轻量级视觉语言框架，结合Swin Transformer作为视觉编码器提取叶子图像特征，并使用序列到序列语言解码器生成答案。采用两阶段训练策略：第一阶段增强视觉表示学习，通过任务特定视觉预训练优化特征提取；第二阶段改进跨模态对齐，确保视觉和语言模态间的有效交互。模型在大规模作物疾病数据集上训练，涉及分类和自然语言生成任务，旨在减少参数量的同时保持高性能。关键创新点包括轻量化架构设计和跨模态对齐优化。",
      "result": "实验结果显示，模型在作物和疾病识别上达到高准确率，具体数据未在摘要中提供，但指出性能显著。在自然语言生成方面，框架在BLEU、ROUGE和BERTScore等指标上表现强劲，显示其语言生成能力。与大规模视觉语言基线模型对比，提出的模型使用显著更少的参数却取得优越性能，突出了轻量级设计的有效性。可解释性评估通过Grad-CAM和标记级别归因进行，定性分析表明模型在多样化用户查询下保持稳健性能，增强了实际应用的可信度。",
      "conclusion": "本研究的主要贡献是验证了轻量级且可解释的视觉语言框架在作物疾病视觉问答中的有效性，强调了任务特定视觉预训练的重要性。学术价值在于为视觉语言模型提供了轻量化和可解释性的新方法，推动农业AI应用；实际应用价值包括辅助农民进行快速疾病诊断，提高农业智能化水平。局限性或未来工作未在摘要中明确说明，但可能涉及扩展到更多作物类型、增强跨模态交互，或进一步量化可解释性评估以优化模型。",
      "tags": [
        "Vision-Language Framework",
        "Swin Transformer",
        "Sequence-to-Sequence Decoders",
        "Cross-modal Alignment",
        "Grad-CAM"
      ]
    },
    "analyzed_at": "2026-01-09T03:06:45.331713Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05104",
    "title": "How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness",
    "authors": [
      "Florence Bernays",
      "Marco Henriques Pereira",
      "Jochen Menges"
    ],
    "abstract": "This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.",
    "categories": [
      "cs.CL",
      "econ.GN"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05104.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05104",
    "published": "2026-01-08T16:50:00Z",
    "updated": "2026-01-08T16:50:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示了情绪提示在人类-AI交互中对ChatGPT输出质量及后续人类交流行为的显著影响。",
      "motivation": "研究动机在于探索人类情绪如何影响AI系统响应，并检验这种影响是否延伸到人类之间的交流。当前AI交互研究可能忽视情绪因素的重要性，而情绪在沟通中扮演关键角色。通过填补这一空白，本研究旨在优化AI设计，并理解社会互动中情绪的动态效应，以提升AI系统的适应性和用户体验。",
      "method": "研究方法采用主体间实验设计，参与者使用特定情绪（如赞美、愤怒、责备和中性）与ChatGPT（GPT-4.0版本）互动，执行两个任务：编写公开回复和解决伦理困境。关键创新点在于系统考察情绪提示对AI输出的直接影响，并分析互动后人类在交流中的语言使用变化，从而连接AI交互与人-人交流的行为模式。",
      "result": "实验结果表明，与中性语调相比，赞美ChatGPT能显著改善其答案质量；表达愤怒也带来较小改善，而责备无改善。在伦理困境中，参与者表达愤怒时，ChatGPT减少对企业利益的优先考虑；责备则增加对公众利益的强调。此外，责备互动后，人类在后续交流中使用更多负面、敌对和失望的表达，显示了情绪影响的连锁效应。",
      "conclusion": "本研究结论强调情绪提示在人类-AI交互中不仅塑造ChatGPT的输出，还影响后续人类沟通，凸显了情绪在AI系统设计中的重要性。学术价值在于提供了情绪对AI响应和人-人交流作用的新见解，实际应用有助于开发更人性化的AI系统。未来工作可扩展研究其他情绪类型或不同AI模型，以验证泛化性和潜在局限性。",
      "tags": [
        "Human-AI Interaction",
        "Emotional Prompts",
        "ChatGPT",
        "GPT-4.0",
        "Behavioral Experiments"
      ]
    },
    "analyzed_at": "2026-01-09T02:28:50.596523Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.03259",
    "title": "Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems",
    "authors": [
      "Michael E. Garcia-Alcoser",
      "Mobina GhojoghNejad",
      "Fakrul Islam Tushar",
      "David Kim",
      "Kyle J. Lafata",
      "Geoffrey D. Rubin",
      "Joseph Y. Lo"
    ],
    "abstract": "Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.   Materials and Methods: This retrospective study analyzed 40,833 chest-abdomen-pelvis (CAP) CT reports from 29,540 patients, with 1,789 reports manually annotated across three organ systems. External validation was conducted using the CT RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa ($κ$) and micro/macro-averaged F1 scores.   Results: In the internal test set of 12,197 CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($κ$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT RATE dataset (lungs/pleura labels only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for labels with high subjectivity such as atelectasis.   Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.03259.pdf",
    "abs_url": "https://arxiv.org/abs/2506.03259",
    "published": "2025-06-03T18:00:08Z",
    "updated": "2026-01-08T16:22:24Z",
    "comment": "18 pages, 9 figures, to be submitted in Radiology: Artificial Intelligence",
    "light_analysis": {
      "overview": "本研究通过零样本提示评估轻量级大型语言模型在 CT 放射学报告中的疾病标注性能，发现其优于传统规则方法，展示出在跨器官系统中的泛化能力。",
      "motivation": "研究动机源于自动化疾病标注在 CT 放射学报告中具有重要临床价值，能提高效率和辅助研究。然而，现有基于规则的算法难以处理报告语言的复杂性，特别是主观性强的标签如肺不张，导致标注不准确。手动标注成本高昂，而轻量级 LLMs 的兴起为提供灵活、自动化的解决方案提供了新机遇。该研究旨在评估 LLMs 在零样本设置下的有效性，以克服传统方法的不足。",
      "method": "研究方法涉及使用三个轻量级开放权重大型语言模型（如 Llama-3.1 8B 和 Gemma-3 27B）进行零样本提示，直接应用于疾病标注任务。数据集包括 40,833 个胸腹盆 CT 报告，其中 1,789 个手动注释用于评估，外部验证使用 CT RATE 数据集。评估指标采用 Cohen's Kappa 和微/宏平均 F1 分数，以量化模型性能与人工标注的一致性，技术路线侧重于无需特定训练的零样本学习。",
      "result": "实验结果显示，在内部测试集的 12,197 个报告中，Llama-3.1 8B 和 Gemma-3 27B 的 Cohen's Kappa 中值为 0.87。在手动注释集上，Gemma-3 27B 达到最高宏平均 F1 分数 0.82，而规则算法最低为 0.64。在外部 CT RATE 数据集（仅肺部/胸膜标签），Llama-3.1 8B 表现最佳，F1 为 0.91。性能差异主要源于标签主观性，特别是肺不张等疾病，突出 LLMs 的优势。",
      "conclusion": "研究结论强调轻量级 LLMs 在 CT 报告疾病标注中显著优于规则方法，通过零样本提示实现跨器官系统的泛化，为临床自动化提供高效、灵活的解决方案。然而，二进制标签无法完全捕获报告语言的细微差别，可能限制应用范围。学术价值在于推动医学 AI 的发展，实际应用价值包括辅助诊断和研究。未来工作可探索多标签系统或更细粒度的标注方法。",
      "tags": [
        "Large Language Model",
        "Zero-Shot Prompting",
        "CT Radiology Reports",
        "Disease Annotation",
        "Medical Text Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:10.925054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05075",
    "title": "SemPA: Improving Sentence Embeddings of Large Language Models through Semantic Preference Alignment",
    "authors": [
      "Ziyang Chen",
      "Zhenxuan Huang",
      "Yile Wang",
      "Weiqin Wang",
      "Lu Yin",
      "Hui Huang"
    ],
    "abstract": "Traditional sentence embedding methods employ token-level contrastive learning on non-generative pre-trained models. Recently, there have emerged embedding methods based on generative large language models (LLMs). These methods either rely on fixed prompt templates or involve modifications to the model architecture. The former lacks further optimization of the model and results in limited performance, while the latter alters the internal computational mechanisms of the model, thereby compromising its generative capabilities. We propose SemPA, a novel approach that boosts the sentence representations while preserving the generative ability of LLMs via semantic preference alignment. We leverage sentence-level Direct Preference Optimization (DPO) to efficiently optimize LLMs on a paraphrase generation task, where the model learns to discriminate semantically equivalent sentences while preserving inherent generative capacity. Theoretically, we establish a formal connection between DPO and contrastive learning under the Plackett-Luce model framework. Empirically, experimental results on both semantic textual similarity tasks and various benchmarks for LLMs show that SemPA achieves better semantic representations without sacrificing the inherent generation capability of LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05075.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05075",
    "published": "2026-01-08T16:19:24Z",
    "updated": "2026-01-08T16:19:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "SemPA方法通过语义偏好对齐提升大语言模型的句子嵌入质量，同时保持其固有生成能力。",
      "motivation": "传统句子嵌入方法通常依赖非生成预训练模型和词级对比学习，而近年来基于生成大语言模型的方法存在明显不足：固定提示模板缺乏进一步优化，导致性能受限；修改模型架构则会损害模型的生成能力，影响其核心功能。句子表示在自然语言处理应用中至关重要，如语义搜索和文本相似性计算。因此，急需一种方法既能高效优化句子嵌入，又不会破坏大语言模型的生成特性。本研究旨在解决这一问题，通过语义偏好对齐来平衡表示提升与模型完整性。",
      "method": "本研究提出SemPA方法，利用句子级直接偏好优化在大语言模型上进行语义偏好对齐。核心创新在于通过释义生成任务优化模型，使其学习区分语义等价句子，同时不修改内部架构以保持生成能力。技术路线中，作者使用直接偏好优化算法，并在Plackett-Luce模型框架下理论建立了与对比学习的正式连接。该方法避免了传统方法的架构改动，确保了模型生成能力的完整性。摘要未明确指定使用的具体数据集或模型类型，但基于大语言模型通用框架进行优化。",
      "result": "实验在语义文本相似性任务和多个大语言模型基准上进行，结果显示SemPA能够获得更好的语义表示，且不牺牲模型的生成能力。与基线方法相比，SemPA在提升句子嵌入质量方面表现优越，尽管摘要未提供具体性能指标如准确率或效率数据，但强调了其在保持生成能力的同时优于现有方法。这表明方法在平衡表示优化和模型功能方面具有显著优势，为实际应用提供了可靠支持。",
      "conclusion": "研究的主要贡献是提出SemPA方法，有效改进大语言模型的句子嵌入并理论验证了直接偏好优化与对比学习的联系。学术上，这为句子表示学习提供了新视角；实际上，增强了大型语言模型在多种任务中的表示能力，具有广泛应用价值。摘要未明确说明局限性或未来工作方向，但可推断可能涉及扩展到更多任务或优化训练效率，以进一步提升方法的普适性和性能。",
      "tags": [
        "Large Language Models",
        "Direct Preference Optimization",
        "Semantic Preference Alignment",
        "Contrastive Learning",
        "Sentence Embeddings"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:23.143836Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.21007",
    "title": "Can Confidence Estimates Decide When Chain-of-Thought Is Necessary for LLMs?",
    "authors": [
      "Samuel Lewis-Lim",
      "Xingwei Tan",
      "Zhixue Zhao",
      "Nikolaos Aletras"
    ],
    "abstract": "Chain-of-thought (CoT) prompting is a common technique for improving the reasoning abilities of large language models (LLMs). However, extended reasoning is often unnecessary and substantially increases token usage. As such, a key question becomes how to optimally allocate compute to when reasoning is actually needed. We study this through confidence-gated CoT, where a model produces a direct answer and a confidence estimate to decide whether to invoke CoT. We present an evaluation framework together with the first systematic study of confidence signals for this decision. We evaluate four representative confidence measures and compare them with random gating and an oracle upper bound. Experiments across two model families and diverse reasoning tasks show that existing training-free confidence measures can reduce redundant reasoning. However, we also find that the utility of individual confidence measures is inconsistent across settings. Through our evaluation framework and analysis, our study provides practical guidance toward developing and evaluating models that selectively use CoT.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.21007.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21007",
    "published": "2025-10-23T21:33:28Z",
    "updated": "2026-01-08T16:05:16Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "本研究通过置信度门控的链式思考提示，系统评估如何优化大语言模型的推理资源分配，以减少不必要的计算开销。",
      "motivation": "链式思考提示是提升大语言模型推理能力的常用技术，但扩展推理往往不必要且显著增加令牌使用，导致计算资源浪费。现有方法缺乏有效机制来决定何时需要推理，使得计算分配效率低下，因此研究如何优化推理触发成为关键问题。本研究的动机是解决这一实际问题，通过置信度估计智能决定是否使用链式思考，以提高模型效率和降低成本。",
      "method": "论文提出confidence-gated CoT方法，模型首先生成直接答案和置信度估计，然后基于置信度决定是否调用链式思考推理。研究首次系统评估了四种代表置信度测量信号，并构建评估框架与随机门控和理论上限进行比较。实验在两个大语言模型家族和多种推理任务上进行，关键创新在于提供了一种可操作的决策机制，以优化推理过程，减少冗余计算。",
      "result": "实验结果表明，现有无需训练的置信度测量可以有效减少冗余推理，从而降低令牌使用。然而，研究发现单个置信度测量的效果在不同模型设置和任务中表现不一致，与随机门控相比显示出改进，但未达到理论上限。摘要未明确说明具体性能指标数据，如准确率提升百分比，但总体证实了置信度估计在优化推理资源分配中的潜力和局限性。",
      "conclusion": "本研究的主要贡献在于提出了一个评估框架，系统研究了置信度估计在决定链式思考必要性中的应用，为开发和评估选择性使用链式思考的模型提供了实用指导。研究具有学术价值，推动了推理优化技术的发展，并具有实际应用价值，能减少大语言模型的计算成本。未来工作可探索更稳定的置信度测量方法或扩展到更多复杂任务，以进一步提升模型效率。",
      "tags": [
        "Chain-of-Thought (CoT)",
        "Confidence Estimation",
        "Large Language Models (LLMs)",
        "Evaluation Framework",
        "Reasoning Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:21.461943Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.13691",
    "title": "Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora",
    "authors": [
      "Tristan Karch",
      "Luca Engel",
      "Philippe Schwaller",
      "Frédéric Kaplan"
    ],
    "abstract": "As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.13691.pdf",
    "abs_url": "https://arxiv.org/abs/2502.13691",
    "published": "2025-02-19T13:03:06Z",
    "updated": "2026-01-08T15:44:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种自动化方法，通过生成多项选择题并测量大语言模型性能差距来评估文本集合的信息潜力。",
      "motivation": "随着大语言模型（LLMs）能力趋同，提升性能的关键在于整合新信息源，但评估文本集合是否值得投资（如数字化、预处理）是一个重大挑战。现有方法通常依赖模型训练或人工评估，成本高昂且效率低下，缺乏自动化工具来快速筛选高潜力数据。本研究旨在解决这一不足，开发一种无需训练即可自动测量信息增益的方法，以优化数据获取和LLM系统集成。",
      "method": "方法基于自动化管道：首先从文本集合生成多项选择题（MCQs），然后测试LLM在两种条件下的性能——有访问源材料和没有访问。性能差异（如准确率差距）被用作信息潜力的代理指标。关键创新在于无需模型训练或微调，利用现有LLM能力进行评估。该方法涵盖多种文本类型，确保泛化性，同时简化了评估流程。",
      "result": "在五个数据集上验证方法：EPFL博士手稿、威尼斯历史记录、两组相关维基百科文章和合成基线。结果证明，该方法能有效识别包含有价值新信息的集合，例如历史文档和学术文本。虽然摘要未明确具体性能指标如准确率提升，但通过与基线对比，证实了方法在区分高潜力和低潜力数据上的可靠性，为数据优先级排序提供了实用工具。",
      "conclusion": "本研究贡献了一种自动化评估文本集合信息潜力的方法，降低评估成本，无需训练，有助于优化LLM数据管理。它提供实用工具，指导资源分配和集成策略，具有学术和实际价值。局限性包括对MCQ生成的依赖，未来工作可扩展到更多文本类型和评估指标，以进一步提升准确性。",
      "tags": [
        "Large Language Model",
        "Multiple Choice Question Generation",
        "Information Potential Assessment",
        "Automated Pipeline",
        "Text Corpus Evaluation"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:31.290127Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05004",
    "title": "Can Large Language Models Resolve Semantic Discrepancy in Self-Destructive Subcultures? Evidence from Jirai Kei",
    "authors": [
      "Peng Wang",
      "Xilin Tao",
      "Siyi Yao",
      "Jiageng Wu",
      "Yuntao Zou",
      "Zhuotao Tian",
      "Libo Qin",
      "Dagang Li"
    ],
    "abstract": "Self-destructive behaviors are linked to complex psychological states and can be challenging to diagnose. These behaviors may be even harder to identify within subcultural groups due to their unique expressions. As large language models (LLMs) are applied across various fields, some researchers have begun exploring their application for detecting self-destructive behaviors. Motivated by this, we investigate self-destructive behavior detection within subcultures using current LLM-based methods. However, these methods have two main challenges: (1) Knowledge Lag: Subcultural slang evolves rapidly, faster than LLMs' training cycles; and (2) Semantic Misalignment: it is challenging to grasp the specific and nuanced expressions unique to subcultures. To address these issues, we proposed Subcultural Alignment Solver (SAS), a multi-agent framework that incorporates automatic retrieval and subculture alignment, significantly enhancing the performance of LLMs in detecting self-destructive behavior. Our experimental results show that SAS outperforms the current advanced multi-agent framework OWL. Notably, it competes well with fine-tuned LLMs. We hope that SAS will advance the field of self-destructive behavior detection in subcultural contexts and serve as a valuable resource for future researchers.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05004.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05004",
    "published": "2026-01-08T15:02:41Z",
    "updated": "2026-01-08T15:02:41Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "本研究提出SAS框架，通过结合自动检索和亚文化对齐，提升大语言模型在亚文化群体中检测自毁行为的性能。",
      "motivation": "自毁行为检测对心理健康干预至关重要，尤其在亚文化群体中，因其表达独特而难以识别。随着大语言模型（LLMs）在各领域应用，现有方法面临两大挑战：知识滞后——亚文化俚语演变速度快于模型训练周期；语义不匹配——LLMs难以把握亚文化特有的微妙表达。这些问题限制了检测的准确性，凸显了开发适应性更强方法的必要性。",
      "method": "论文提出Subcultural Alignment Solver (SAS），一个多代理框架，整合自动检索机制以动态获取最新亚文化信息，并结合亚文化对齐技术，优化LLMs对特定表达的语义理解。该方法旨在解决知识滞后和语义不匹配问题，关键创新在于通过检索更新知识库和增强对齐能力，具体模型架构和数据集摘要未明确说明。",
      "result": "实验结果表明，SAS框架在亚文化自毁行为检测任务中，性能优于当前先进的多代理框架OWL，并展现出与经过微调的LLMs相竞争的能力。这证明了SAS在解决语义不匹配和知识滞后方面的有效性，尽管摘要未提供具体性能指标如准确率，但对比基线显示显著改进。",
      "conclusion": "本研究的主要贡献是开发了SAS框架，有效解决LLMs在亚文化背景下的语义挑战，提升了自毁行为检测的准确性和适应性。学术价值在于扩展LLMs在复杂社会场景的应用，实际价值在于为心理健康干预提供更精准工具。未来工作可能包括验证框架的通用性，并探索对其他亚文化群体的适应性，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Models",
        "Multi-Agent Framework",
        "Automatic Retrieval",
        "Subcultural Alignment",
        "Self-Destructive Behavior Detection"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:33.302892Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03042",
    "title": "BaseCal: Unsupervised Confidence Calibration via Base Model Signals",
    "authors": [
      "Hexiang Tan",
      "Wanli Yang",
      "Junwei Zhang",
      "Xin Chen",
      "Rui Tang",
      "Du Su",
      "Jingang Wang",
      "Yuanzhuo Wang",
      "Fei Sun",
      "Xueqi Cheng"
    ],
    "abstract": "Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\\% compared to the best unsupervised baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03042.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03042",
    "published": "2026-01-06T14:22:21Z",
    "updated": "2026-01-08T14:57:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "BaseCal提出了一种无监督的信心校准方法，利用基础大语言模型的信号来校准后训练模型的信心，无需外部标签或模型修改。",
      "motivation": "后训练大语言模型（PoLLMs）常存在严重的过度自信问题，这降低了其输出结果的可信度，影响实际应用的可靠性。相比之下，对应的基础大语言模型（base LLMs）通常校准良好。因此，本研究旨在通过基础模型的信号来校准PoLLMs的信心，以解决过度自信问题，提升模型输出的信任度和实用价值。该问题重要在于LLM的广泛部署需要可靠的信心估计，而现有方法可能依赖人工标注或引入额外开销。",
      "method": "研究提出了两种校准技术：BaseCal-ReEval和BaseCal-Proj。BaseCal-ReEval是一种直接方法，将PoLLM的响应输入基础LLM，计算平均概率作为信心，但增加了推理开销。BaseCal-Proj则训练一个轻量级投影，将PoLLM的最终层隐藏状态映射回基础LLM的隐藏状态，然后通过基础LLM的输出层处理，得到校准后的信心。该方法是无监督和插件式的，无需人类标签或LLM修改，利用数据集进行训练，适用于各种LLM家族和任务。",
      "result": "实验在五个数据集和三个LLM家族上进行，评估了BaseCal的有效性。结果显示，BaseCal显著降低了预期校准误差（ECE），平均减少42.90%相对于最佳无监督基线方法。这表明BaseCal能有效缓解PoLLMs的过度自信问题，提升了信心校准的性能，在多个任务中表现出稳定的改进，并与基线方法形成明显对比。",
      "conclusion": "BaseCal的主要贡献是提供了一种高效且实用的无监督信心校准方案，通过基础模型的参考信号来增强LLM输出的可靠性。该研究的学术价值在于提出了一种插件式方法，简化了校准过程；实际应用价值在于易于部署，适用于多种LLM应用场景。潜在局限性可能包括依赖基础模型的质量，但摘要未明确说明。未来工作可探索方法的泛化性和对不同任务的适应。",
      "tags": [
        "Large Language Model",
        "Confidence Calibration",
        "Unsupervised Learning",
        "Hidden State Projection",
        "Expected Calibration Error"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:41.038209Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.19455",
    "title": "SiamGPT: Quality-First Fine-Tuning for Stable Thai Text Generation",
    "authors": [
      "Thittipat Pairatsuppawat",
      "Abhibhu Tachaapornchai",
      "Paweekorn Kusolsomboon",
      "Chutikan Chaiwong",
      "Thodsaporn Chay-intr",
      "Kobkrit Viriyayudhakorn",
      "Nongnuch Ketui",
      "Aslan B. Wong"
    ],
    "abstract": "Open-weights large language models remain difficult to deploy for Thai due to unstable generation under complex instructions, despite strong English performance. To mitigate these limitations, We present SiamGPT-32B, an open-weights model based on Qwen3-32B, fine-tuned with a Quality-First strategy emphasizing curated supervision over data scale. The fine-tuning pipeline combines high-complexity English instruction data with a Thai-adapted AutoIF framework for instruction and linguistic constraints. Using supervised fine-tuning only, without continual pretraining or corpus expansion, SiamGPT-32B improves instruction adherence, multi-turn robustness, and linguistic stability. Evaluations on the SEA-HELM benchmark show that SiamGPT-32B achieves the strongest overall performance among similar-scale open-weights Thai models, with consistent gains in instruction following, multi-turn dialogue, and natural language understanding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.19455.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19455",
    "published": "2025-12-22T15:00:25Z",
    "updated": "2026-01-08T14:54:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "SiamGPT-32B 通过 Quality-First 微调策略，基于 Qwen3-32B 模型，有效提升了泰语文本生成的指令遵循和语言稳定性。",
      "motivation": "开源大语言模型在英语表现优异，但在泰语文本生成中面临不稳定问题，特别是在复杂指令下，导致实际部署困难。现有方法通常依赖数据规模扩展，而非质量优化，这限制了模型在低资源语言环境中的性能。因此，本研究旨在解决泰语生成不稳定的挑战，通过质量优先策略改进模型，以提高泰语 NLP 应用的可靠性和实用性。",
      "method": "SiamGPT-32B 基于 Qwen3-32B 开源模型，采用 Quality-First 微调策略，强调精选监督数据而非扩大数据规模。微调管道结合了高复杂度英语指令数据和泰语适应的 AutoIF 框架，该框架用于施加指令和语言约束。仅使用监督微调，无需持续预训练或语料扩展，核心创新在于通过质量优先的方法优化模型，提升在泰语环境中的指令遵循和多轮对话能力。",
      "result": "在 SEA-HELM 基准测试中，SiamGPT-32B 在类似规模的开源泰语模型中取得了最强的整体性能。结果显示，该模型在指令遵循、多轮对话和自然语言理解方面均有持续提升，摘要未明确说明具体数据指标，但其表现超越了其他基线方法，证明了质量优先微调策略在改善泰语文本生成稳定性方面的有效性。",
      "conclusion": "本论文的主要贡献是提出了 SiamGPT-32B 模型，通过 Quality-First 微调策略显著提升了泰语文本生成的性能。研究具有重要学术价值，为低资源语言 NLP 任务提供了新的微调方法；实际应用中，有助于推动泰语 AI 模型的发展和部署。未来工作可探索将该策略扩展到其他语言或进一步优化 AutoIF 框架。",
      "tags": [
        "Large Language Model",
        "Fine-Tuning",
        "Instruction Following",
        "AutoIF",
        "Thai Language Generation"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:51.540084Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04992",
    "title": "Learning from Mistakes: Negative Reasoning Samples Enhance Out-of-Domain Generalization",
    "authors": [
      "Xueyun Tian",
      "Minghua Ma",
      "Bingbing Xu",
      "Nuoyan Lyu",
      "Wei Li",
      "Heng Dong",
      "Zheng Chu",
      "Yuanzhuo Wang",
      "Huawei Shen"
    ],
    "abstract": "Supervised fine-tuning (SFT) on chain-of-thought (CoT) trajectories demonstrations is a common approach for enabling reasoning in large language models. Standard practices typically only retain trajectories with correct final answers (positives) while ignoring the rest (negatives). We argue that this paradigm discards substantial supervision and exacerbates overfitting, limiting out-of-domain (OOD) generalization. Specifically, we surprisingly find that incorporating negative trajectories into SFT yields substantial OOD generalization gains over positive-only training, as these trajectories often retain valid intermediate reasoning despite incorrect final answers. To understand this effect in depth, we systematically analyze data, training dynamics, and inference behavior, identifying 22 recurring patterns in negative chains that serve a dual role: they moderate loss descent to mitigate overfitting during training and boost policy entropy by 35.67% during inference to facilitate exploration. Motivated by these observations, we further propose Gain-based LOss Weighting (GLOW), an adaptive, sample-aware scheme that exploits such distinctive training dynamics by rescaling per-sample loss based on inter-epoch progress. Empirically, GLOW efficiently leverages unfiltered trajectories, yielding a 5.51% OOD gain over positive-only SFT on Qwen2.5-7B and boosting MMLU from 72.82% to 76.47% as an RL initialization.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04992.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04992",
    "published": "2026-01-08T14:49:10Z",
    "updated": "2026-01-08T14:49:10Z",
    "comment": "Code and data are available at https://github.com/Eureka-Maggie/GLOW",
    "light_analysis": {
      "overview": "本文提出通过利用负样本推理轨迹并提出自适应损失加权方法 GLOW，显著提升大型语言模型在域外任务上的泛化能力。",
      "motivation": "当前监督微调（SFT）在思维链（CoT）轨迹学习中仅使用正确最终答案的轨迹（正样本），而忽略错误答案的轨迹（负样本）。这种做法浪费了丰富的监督信号，加剧模型过拟合，导致模型在未知领域（OOD）泛化能力受限。由于负样本轨迹常包含有效的中间推理步骤，研究如何整合这些样本以弥补现有方法的不足，对于提升模型鲁棒性和实际应用价值至关重要。",
      "method": "论文首先实证发现将负样本推理轨迹纳入监督微调（SFT）能增强域外（OOD）泛化。随后，系统分析数据、训练动态和推理行为，识别出负链中的 22 个重复模式，这些模式在训练中调节损失下降以减轻过拟合，在推理中提升策略熵促进探索。基于此，作者提出 Gain-based LOss Weighting (GLOW），一种自适应样本感知方案，通过评估每个样本在跨训练时代中的进展动态调整损失权重，以高效利用未过滤的轨迹。",
      "result": "实验结果显示，GLOW 方法在 Qwen2.5-7B 模型上，相比仅使用正样本的监督微调（SFT），实现了 5.51% 的域外（OOD）泛化增益。作为强化学习（RL）初始化的应用，MMLU 分数从 72.82% 提升至 76.47%。分析还表明，负样本轨迹的引入使推理时策略熵增加了 35.67%，验证了其在促进探索和改善性能方面的有效性。",
      "conclusion": "本研究主要贡献是揭示了负样本推理轨迹在监督微调中的价值，并通过 GLOW 方法有效利用这些样本提升泛化性能。这挑战了传统仅依赖正样本的实践，为大型语言模型训练提供了新视角。学术上，丰富了思维链学习和泛化理论；实际上，增强了模型在未知场景的鲁棒性。未来工作可能包括扩展方法到更多任务或优化损失加权策略。",
      "tags": [
        "Supervised Fine-Tuning",
        "Chain-of-Thought",
        "Out-of-Domain Generalization",
        "Loss Weighting",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:08.731986Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04960",
    "title": "A Unified Spoken Language Model with Injected Emotional-Attribution Thinking for Human-like Interaction",
    "authors": [
      "Qing Wang",
      "Zehan Li",
      "Yaodong Song",
      "Hongjie Chen",
      "Jian Kang",
      "Jie Lian",
      "Jie Li",
      "Yongxiang Li",
      "Xuelong Li"
    ],
    "abstract": "This paper presents a unified spoken language model for emotional intelligence, enhanced by a novel data construction strategy termed Injected Emotional-Attribution Thinking (IEAT). IEAT incorporates user emotional states and their underlying causes into the model's internal reasoning process, enabling emotion-aware reasoning to be internalized rather than treated as explicit supervision. The model is trained with a two-stage progressive strategy. The first stage performs speech-text alignment and emotional attribute modeling via self-distillation, while the second stage conducts end-to-end cross-modal joint optimization to ensure consistency between textual and spoken emotional expressions. Experiments on the Human-like Spoken Dialogue Systems Challenge (HumDial) Emotional Intelligence benchmark demonstrate that the proposed approach achieves top-ranked performance across emotional trajectory modeling, emotional reasoning, and empathetic response generation under both LLM-based and human evaluations.",
    "categories": [
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04960.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04960",
    "published": "2026-01-08T14:07:30Z",
    "updated": "2026-01-08T14:07:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种统一的语音语言模型，通过注入情感归因思考（IEAT）策略，增强情感智能以实现更人性化的交互。",
      "motivation": "该研究旨在解决现有语音语言模型在情感智能方面的不足，特别是在处理用户情感状态及其原因时，常依赖显式监督而非内部推理。情感智能对于实现自然、人性化的人机交互至关重要，但传统方法可能无法有效整合情感信息，导致响应不够共情或不一致。因此，本文提出通过内部化情感推理来提升模型的交互质量。",
      "method": "论文提出了一种统一的语音语言模型，采用注入情感归因思考（IEAT）策略，将用户情感状态及其原因整合到内部推理中。训练采用两阶段渐进策略：第一阶段通过自蒸馏实现语音-文本对齐和情感属性建模；第二阶段进行端到端跨模态联合优化，以确保文本和语音情感表达的一致性。该方法的关键创新在于 IEAT 的数据构建和内部化情感推理。",
      "result": "实验在 Human-like Spoken Dialogue Systems Challenge (HumDial) 情感智能基准上进行，评估了情绪轨迹建模、情感推理和共情响应生成等任务。结果显示，所提出的方法在基于大语言模型（LLM）和人类评估中都取得了顶级性能，但摘要未明确说明具体性能指标。这表明模型在理解和表达情感方面优于基线方法。",
      "conclusion": "本文的主要贡献是提出了一种统一的语音语言模型，通过 IEAT 策略内部化情感推理，并采用两阶段训练优化跨模态一致性。该研究在学术上推动了情感智能领域的发展，为语音交互系统提供了新的方法。实际应用中，能提升对话系统的自然度和共情能力。未来工作可能包括扩展到更多情感维度或真实场景部署。",
      "tags": [
        "Spoken Language Model",
        "Injected Emotional-Attribution Thinking",
        "Cross-Modal Joint Optimization",
        "Emotional Intelligence",
        "Self-Distillation"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:23.919384Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04932",
    "title": "GenProve: Learning to Generate Text with Fine-Grained Provenance",
    "authors": [
      "Jingxuan Wei",
      "Xingyue Wang",
      "Yanghaoyu Liao",
      "Jie Dong",
      "Yuchen Liu",
      "Caijun Jia",
      "Bihui Yu",
      "Junnan Zhu"
    ],
    "abstract": "Large language models (LLM) often hallucinate, and while adding citations is a common solution, it is frequently insufficient for accountability as users struggle to verify how a cited source supports a generated claim. Existing methods are typically coarse-grained and fail to distinguish between direct quotes and complex reasoning. In this paper, we introduce Generation-time Fine-grained Provenance, a task where models must generate fluent answers while simultaneously producing structured, sentence-level provenance triples. To enable this, we present ReFInE (Relation-aware Fine-grained Interpretability & Evidence), a dataset featuring expert verified annotations that distinguish between Quotation, Compression, and Inference. Building on ReFInE, we propose GenProve, a framework that combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO). By optimizing a composite reward for answer fidelity and provenance correctness, GenProve significantly outperforms 14 strong LLMs in joint evaluation. Crucially, our analysis uncovers a reasoning gap where models excel at surface-level quotation but struggle significantly with inference-based provenance, suggesting that verifiable reasoning remains a frontier challenge distinct from surface-level citation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04932.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04932",
    "published": "2026-01-08T13:30:30Z",
    "updated": "2026-01-08T13:30:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了GenProve框架，通过结合监督微调和组相对策略优化，使大语言模型在生成流畅文本时提供细粒度追溯信息，以区分引用、压缩和推理，应对幻觉问题。",
      "motivation": "大语言模型（LLM）常产生不准确信息（幻觉），现有引用添加方法多为粗粒度，难以区分直接引用和复杂推理，导致用户难以验证生成内容的可信度。这一问题在人工智能生成内容普及的背景下尤为重要，因为缺乏细粒度追溯会影响模型的解释性和责任性。因此，研究旨在通过细粒度追溯任务提升模型生成文本的可追溯性，弥补现有方法在区分不同追溯类型方面的不足。",
      "method": "研究引入生成时细粒度追溯任务，要求模型同时生成流畅答案和句子级追溯三元组。为此，构建了ReFInE数据集，包含专家验证的注释，区分引用、压缩和推理。基于此，提出GenProve框架，结合监督微调（SFT）和组相对策略优化（GRPO），通过复合奖励函数优化答案保真度和追溯正确性，实现细粒度追溯的生成。该方法强调结构化的追溯输出和区分不同推理类型的技术特色。",
      "result": "GenProve在联合评估中显著优于14个强大LLM，展现出在细粒度追溯任务上的优越性能。关键分析发现推理差距，即模型在表面引用方面表现良好，但在推理追溯上存在显著困难，表明可验证的推理是一个独立挑战。摘要未明确说明具体性能指标，但突出了与基线方法的对比和改进趋势。",
      "conclusion": "本研究的主要贡献是提出了细粒度追溯任务及GenProve框架，提升了语言模型生成文本的追溯能力和可解释性。学术上，它推动了模型解释性和追溯机制的研究；应用上，有助于增强生成内容的可信度。未来工作可针对推理差距进行改进，进一步探索可验证推理的技术，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Fine-Grained Provenance",
        "Supervised Fine-Tuning",
        "Group Relative Policy Optimization",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:47.260842Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03997",
    "title": "VotIE: Information Extraction from Meeting Minutes",
    "authors": [
      "José Pedro Evans",
      "Luís Filipe Cunha",
      "Purificação Silvano",
      "Alípio Jorge",
      "Nuno Guimarães",
      "Sérgio Nunes",
      "Ricardo Campos"
    ],
    "abstract": "Municipal meeting minutes record key decisions in local democratic processes. Unlike parliamentary proceedings, which typically adhere to standardized formats, they encode voting outcomes in highly heterogeneous, free-form narrative text that varies widely across municipalities, posing significant challenges for automated extraction. In this paper, we introduce VotIE (Voting Information Extraction), a new information extraction task aimed at identifying structured voting events in narrative deliberative records, and establish the first benchmark for this task using Portuguese municipal minutes, building on the recently introduced CitiLink corpus. Our experiments yield two key findings. First, under standard in-domain evaluation, fine-tuned encoders, specifically XLM-R-CRF, achieve the strongest performance, reaching 93.2\\% macro F1, outperforming generative approaches. Second, in a cross-municipality setting that evaluates transfer to unseen administrative contexts, these models suffer substantial performance degradation, whereas few-shot LLMs demonstrate greater robustness, with significantly smaller declines in performance. Despite this generalization advantage, the high computational cost of generative models currently constrains their practicality. As a result, lightweight fine-tuned encoders remain a more practical option for large-scale, real-world deployment. To support reproducible research in administrative NLP, we publicly release our benchmark, trained models, and evaluation framework.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03997.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03997",
    "published": "2026-01-07T15:06:53Z",
    "updated": "2026-01-08T13:24:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入VotIE投票信息提取任务，建立首个基准，并发现微调编码器在域内表现最佳，而少样本LLMs在跨域中更鲁棒。",
      "motivation": "市政会议纪要记录地方民主关键决策，但其文本异构、格式不一，难以自动提取投票信息。现有NLP方法多针对标准化格式设计，无法有效处理这种自由叙述文本，导致行政文档分析受限，影响民主进程的透明度和效率。因此，本研究旨在解决这一实际挑战，为高效信息提取提供新途径。",
      "method": "论文提出VotIE任务，专注于从市政会议纪要中提取结构化投票事件，基于葡萄牙市政纪要的CitiLink语料库建立首个基准。研究方法包括使用微调编码器（如XLM-R-CRF）和生成式大语言模型进行少样本学习，通过设计评估框架比较域内和跨域性能，关键创新在于任务定义和泛化能力分析。",
      "result": "实验结果显示，在标准域内评估中，微调编码器XLM-R-CRF达到93.2%宏观F1分数，优于生成方法。然而，在跨市镇设置中，微调模型性能大幅下降，而少样本LLMs表现出更强鲁棒性，性能下降较小。尽管生成模型在跨域中更优，但其高计算成本使轻量级微调编码器在实际部署中更实用。",
      "conclusion": "论文的主要贡献是定义VotIE任务并建立首个基准，推动了行政NLP研究。通过实验比较，发现微调编码器在域内高效，而少样本LLMs在跨域中更鲁棒但计算成本高。研究意义在于为大规模市政文档分析提供实用方案，并公开资源促进可重复研究，未来工作可优化模型效率或扩展基准。",
      "tags": [
        "Information Extraction",
        "XLM-R-CRF",
        "Few-shot Learning",
        "Large Language Models",
        "Cross-domain Transfer"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:41.429510Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04925",
    "title": "Can AI-Generated Persuasion Be Detected? Persuaficial Benchmark and AI vs. Human Linguistic Differences",
    "authors": [
      "Arkadiusz Modzelewski",
      "Paweł Golik",
      "Anna Kołos",
      "Giovanni Da San Martino"
    ],
    "abstract": "Large Language Models (LLMs) can generate highly persuasive text, raising concerns about their misuse for propaganda, manipulation, and other harmful purposes. This leads us to our central question: Is LLM-generated persuasion more difficult to automatically detect than human-written persuasion? To address this, we categorize controllable generation approaches for producing persuasive content with LLMs and introduce Persuaficial, a high-quality multilingual benchmark covering six languages: English, German, Polish, Italian, French and Russian. Using this benchmark, we conduct extensive empirical evaluations comparing human-authored and LLM-generated persuasive texts. We find that although overtly persuasive LLM-generated texts can be easier to detect than human-written ones, subtle LLM-generated persuasion consistently degrades automatic detection performance. Beyond detection performance, we provide the first comprehensive linguistic analysis contrasting human and LLM-generated persuasive texts, offering insights that may guide the development of more interpretable and robust detection tools.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04925.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04925",
    "published": "2026-01-08T13:22:25Z",
    "updated": "2026-01-08T13:22:25Z",
    "comment": "Preprint; Paper is currently under review at a major NLP conference",
    "light_analysis": {
      "overview": "论文引入 Persuaficial 基准，比较 AI 生成与人类编写说服性文本的检测难度，并进行语言分析以指导检测工具开发。",
      "motivation": "研究动机源于大型语言模型（LLMs）能够生成高度有说服力的文本，可能被滥用于宣传和操纵等有害目的，这引发了对 AI 生成说服性检测的担忧。当前自动检测方法可能难以有效区分微妙的说服性 AI 文本，特别是在面对 LLM 生成的隐蔽内容时。因此，论文旨在探究 LLM 生成说服性是否比人类编写的更难检测，以评估现有技术的不足并推动更鲁棒的解决方案。",
      "method": "研究方法包括分类使用 LLMs 生成可控说服内容的方法，并提出了 Persuaficial 基准，这是一个高质量的多语言数据集，覆盖英语、德语、波兰语、意大利语、法语和俄语。使用该基准，论文进行了广泛的实证评估，比较人类编写和 LLM 生成的说服性文本。关键创新在于基准的构建，以及首个全面的语言分析，以系统性地揭示人类与 AI 生成文本之间的差异，从而支持更可解释的检测工具开发。",
      "result": "主要实验结果显示，公开有说服力的 LLM 生成文本比人类编写的更容易被自动检测，但微妙的 LLM 生成说服性内容持续降低检测性能。摘要未明确说明具体性能指标如准确率，但表明检测工具在面对微妙 AI 说服时效果显著下降，这突显了现有方法在区分人类与 AI 生成内容方面的局限性，尤其是在多语言背景下。",
      "conclusion": "论文的主要贡献是引入了 Persuaficial 基准，并提供了人类与 LLM 生成说服性文本的首次全面语言分析。这为指导开发更可解释和鲁棒的检测工具提供了重要见解，具有显著的学术价值，有助于应对 AI 滥用的实际挑战。未来工作可能涉及改进检测算法以处理微妙说服，并扩展基准到更多语言和应用场景，以增强实际适用性。",
      "tags": [
        "Large Language Models",
        "Persuasive Text Generation",
        "Benchmark Evaluation",
        "Multilingual NLP",
        "Linguistic Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:49.884840Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04897",
    "title": "V-FAT: Benchmarking Visual Fidelity Against Text-bias",
    "authors": [
      "Ziteng Wang",
      "Yujie He",
      "Guanliang Li",
      "Siqi Yang",
      "Jiaqi Xiong",
      "Songxiang Liu"
    ],
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance on standard visual reasoning benchmarks. However, there is growing concern that these models rely excessively on linguistic shortcuts rather than genuine visual grounding, a phenomenon we term Text Bias. In this paper, we investigate the fundamental tension between visual perception and linguistic priors. We decouple the sources of this bias into two dimensions: Internal Corpus Bias, stemming from statistical correlations in pretraining, and External Instruction Bias, arising from the alignment-induced tendency toward sycophancy. To quantify this effect, we introduce V-FAT (Visual Fidelity Against Text-bias), a diagnostic benchmark comprising 4,026 VQA instances across six semantic domains. V-FAT employs a Three-Level Evaluation Framework that systematically increases the conflict between visual evidence and textual information: (L1) internal bias from atypical images, (L2) external bias from misleading instructions, and (L3) synergistic bias where both coincide. We introduce the Visual Robustness Score (VRS), a metric designed to penalize \"lucky\" linguistic guesses and reward true visual fidelity. Our evaluation of 12 frontier MLLMs reveals that while models excel in existing benchmarks, they experience significant visual collapse under high linguistic dominance.",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04897.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04897",
    "published": "2026-01-08T12:50:14Z",
    "updated": "2026-01-08T12:50:14Z",
    "comment": "12 pages, 6 figures",
    "light_analysis": {
      "overview": "本研究提出V-FAT基准和视觉鲁棒性分数，以诊断多模态大语言模型对文本偏见的依赖程度。",
      "motivation": "多模态大语言模型在标准视觉推理任务中表现出色，但存在过度依赖语言捷径而非视觉基础的文本偏见问题。现有基准无法有效评估这种偏见，可能导致模型性能被高估，因此需要新方法来量化视觉保真度和解决视觉与语言先验之间的张力。",
      "method": "论文引入V-FAT诊断基准，包含4,026个视觉问答实例覆盖六个语义领域，采用三级评估框架系统增加视觉证据与文本信息的冲突：L1针对非典型图像的内部偏见，L2针对误导指令的外部偏见，L3针对两者协同的偏见，并设计视觉鲁棒性分数以惩罚语言猜测并奖励视觉保真度。",
      "result": "对12个前沿多模态大语言模型的评估显示，虽然它们在现有基准上表现出色，但在V-FAT的高语言主导情境下出现显著的视觉崩溃，揭示了模型过度依赖文本偏见的问题。摘要未明确说明具体性能指标数据。",
      "conclusion": "本研究贡献了V-FAT基准和评估框架，为多模态模型的视觉保真度评估提供了新工具，具有学术价值和实际应用价值，有助于改进模型设计；未来工作可能包括扩展基准或探索减少偏见的模型优化方法。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Question Answering",
        "Text Bias",
        "Benchmarking",
        "Evaluation Framework"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:02.033941Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04889",
    "title": "Faithful Summarisation under Disagreement via Belief-Level Aggregation",
    "authors": [
      "Favour Yahdii Aghaebe",
      "Tanefa Apekey",
      "Elizabeth Williams",
      "Nafise Sadat Moosavi"
    ],
    "abstract": "Opinion and multi-document summarisation often involve genuinely conflicting viewpoints, yet many existing approaches, particularly LLM-based systems, implicitly smooth disagreement and over-represent majority opinions. This limits the faithfulness of generated summaries in opinion-heavy settings. We introduce a disagreement-aware synthesis pipeline that separates belief-level aggregation from language generation. Documents are first represented as structured belief sets and aggregated using distance-based belief merging operators that explicitly model conflict. Large language models are then used only to realise the aggregated beliefs as natural language summaries. We evaluate the approach across multiple model families and scales, comparing it to methods that perform explicit aggregation during generation. Our results show that while sufficiently large models can match belief-level aggregation when aggregation is handled at generation time, this behaviour is not stable across architectures or capacities. In contrast, belief-level aggregation combined with simple prompting yields consistently strong disagreement-aware performance across models, while maintaining fluent and grounded summaries.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04889.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04889",
    "published": "2026-01-08T12:40:47Z",
    "updated": "2026-01-08T12:40:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种基于信念级聚合的分歧感知摘要方法，通过分离聚合和生成过程来提高摘要的忠实度。",
      "motivation": "在意见和多文档摘要中，常存在观点冲突，但现有方法特别是基于大语言模型的系统往往隐含平滑分歧、过度代表多数意见，导致生成的摘要不够忠实，限制在真实场景中的应用。本研究旨在解决这一问题，通过显式建模冲突来提高摘要的准确性和代表性，克服现有方法的不足，使摘要能更好地反映不同观点的多样性。",
      "method": "论文引入一个两阶段的合成管道：首先，将输入文档表示为结构化信念集，使用基于距离的信念合并算子进行聚合，显式建模冲突；然后，仅利用大型语言模型将聚合后的信念转化为自然语言摘要。关键创新点在于将信念级聚合与语言生成分离，避免在生成过程中直接聚合，从而提高处理分歧的透明度和可控性。方法使用距离算子处理文档间的矛盾，并依赖LLM进行语言实现。",
      "result": "在多个模型族和规模上进行评估，结果显示，当在生成时处理聚合时，足够大的模型可以匹配信念级聚合的性能，但这行为不稳定，受模型架构和容量影响。相比之下，信念级聚合结合简单提示在所有测试模型中表现出一致且强健的分歧感知性能，同时生成的摘要保持流畅和基于事实。这表明分离聚合和生成的方法更可靠，能提高摘要的忠实度和稳定性。",
      "conclusion": "本研究的主要贡献是提出了一个信念级聚合框架，有效处理分歧感知摘要中的冲突观点，提升摘要的忠实度。学术价值在于为多文档摘要提供了新的技术路径，促进了对复杂观点建模的理解；实际应用价值可扩展到新闻摘要、社交媒体分析等领域。未来工作可能包括优化聚合算子或扩展该方法到更广泛的场景，以提高方法的泛化能力。",
      "tags": [
        "Multi-document Summarisation",
        "Belief Aggregation",
        "Large Language Models",
        "Conflict Modeling",
        "Opinion Summarisation"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:05.511637Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04879",
    "title": "Mind2Report: A Cognitive Deep Research Agent for Expert-Level Commercial Report Synthesis",
    "authors": [
      "Mingyue Cheng",
      "Daoyu Wang",
      "Qi Liu",
      "Shuo Yu",
      "Xiaoyu Tao",
      "Yuqian Wang",
      "Chengzhong Chu",
      "Yu Duan",
      "Mingkang Long",
      "Enhong Chen"
    ],
    "abstract": "Synthesizing informative commercial reports from massive and noisy web sources is critical for high-stakes business decisions. Although current deep research agents achieve notable progress, their reports still remain limited in terms of quality, reliability, and coverage. In this work, we propose Mind2Report, a cognitive deep research agent that emulates the commercial analyst to synthesize expert-level reports. Specifically, it first probes fine-grained intent, then searches web sources and records distilled information on the fly, and subsequently iteratively synthesizes the report. We design Mind2Report as a training-free agentic workflow that augments general large language models (LLMs) with dynamic memory to support these long-form cognitive processes. To rigorously evaluate Mind2Report, we further construct QRC-Eval comprising 200 real-world commercial tasks and establish a holistic evaluation strategy to assess report quality, reliability, and coverage. Experiments demonstrate that Mind2Report outperforms leading baselines, including OpenAI and Gemini deep research agents. Although this is a preliminary study, we expect it to serve as a foundation for advancing the future design of commercial deep research agents. Our code and data are available at https://github.com/Melmaphother/Mind2Report.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04879.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04879",
    "published": "2026-01-08T12:27:52Z",
    "updated": "2026-01-08T12:27:52Z",
    "comment": "26 Pages, 9 Figures, 7 Tables",
    "light_analysis": {
      "overview": "提出Mind2Report，一个无需训练的认知代理工作流，通过动态内存增强大型语言模型，合成专家级商业报告。",
      "motivation": "商业报告的合成对高风险商业决策至关重要，现有深度研究代理虽然在生成报告方面有所进展，但其报告在质量、可靠性和覆盖范围方面仍然有限，导致信息不准确或不全面，影响决策有效性。因此，需要开发更可靠的代理来提升报告合成能力，以支持更精准的商业洞察和决策过程。该研究旨在解决这一问题，通过模拟商业分析师思维来改进报告生成。",
      "method": "Mind2Report是一个认知深度研究代理，模拟商业分析师的思维过程。其工作流包括三个关键步骤：首先探测细粒度用户意图以明确需求，然后搜索网络来源并实时记录和提炼信息，最后迭代合成最终报告。创新点在于设计为一个训练免费的代理工作流，使用动态内存机制增强通用大型语言模型（LLMs），以支持长形式的认知任务，无需额外训练即可提升处理复杂报告的效率和准确性。",
      "result": "实验评估中，研究人员构建了QRC-Eval数据集，包含200个真实世界商业任务，并采用整体策略评估报告的质量、可靠性和覆盖范围。摘要未提供具体性能指标数值，但实验结果表明，Mind2Report在评估中优于OpenAI和Gemini等领先的深度研究代理基线，显示出在多个维度上的性能提升，证明了其方法的有效性。",
      "conclusion": "本研究主要贡献在于提出Mind2Report，一个创新的认知代理，能够高效合成专家级商业报告。通过训练免费的工作流和动态内存机制，它增强了LLMs在长形式认知任务中的应用，为未来商业深度研究代理的设计提供了基础。学术价值在于推动了认知代理技术的发展，实用价值体现在提升商业报告合成的质量和效率。作为初步研究，局限性包括可能需在更广泛场景验证，未来工作可聚焦于优化内存机制和扩展应用领域。",
      "tags": [
        "Large Language Model",
        "Agentic Workflow",
        "Dynamic Memory",
        "Report Synthesis",
        "Deep Research Agent"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:59.974154Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04875",
    "title": "EvolSQL: Structure-Aware Evolution for Scalable Text-to-SQL Data Synthesis",
    "authors": [
      "Xuanguang Pan",
      "Chongyang Tao",
      "Jiayuan Bai",
      "Jianling Gao",
      "Zhengwei Tao",
      "Xiansheng Zhou",
      "Gavin Cheung",
      "Shuai Ma"
    ],
    "abstract": "Training effective Text-to-SQL models remains challenging due to the scarcity of high-quality, diverse, and structurally complex datasets. Existing methods either rely on limited human-annotated corpora, or synthesize datasets directly by simply prompting LLMs without explicit control over SQL structures, often resulting in limited structural diversity and complexity. To address this, we introduce EvolSQL, a structure-aware data synthesis framework that evolves SQL queries from seed data into richer and more semantically diverse forms. EvolSQL starts with an exploratory Query-SQL expansion to broaden question diversity and improve schema coverage, and then applies an adaptive directional evolution strategy using six atomic transformation operators derived from the SQL Abstract Syntax Tree to progressively increase query complexity across relational, predicate, aggregation, and nesting dimensions. An execution-grounded SQL refinement module and schema-aware deduplication further ensure the creation of high-quality, structurally diverse mapping pairs. Experimental results show that a 7B model fine-tuned on our data outperforms one trained on the much larger SynSQL dataset using only 1/18 of the data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04875.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04875",
    "published": "2026-01-08T12:19:50Z",
    "updated": "2026-01-08T12:19:50Z",
    "comment": "18 pages",
    "light_analysis": {
      "overview": "本文提出EvolSQL框架，通过结构感知进化合成高质量Text-to-SQL数据，提升模型训练效果。",
      "motivation": "训练Text-to-SQL模型面临高质量数据集稀缺的挑战，现有方法要么依赖有限人工标注数据，要么直接提示大语言模型合成数据但缺乏SQL结构控制，导致生成的数据结构多样性和复杂性不足。这限制了模型学习复杂查询的能力，影响实际应用效果，因此需要一种能生成结构化丰富数据的方法来改进模型性能和可扩展性。",
      "method": "EvolSQL采用结构感知数据合成框架，从种子数据开始，先进行探索性Query-SQL扩展以增加问题多样性和数据库模式覆盖率。然后应用自适应定向进化策略，基于SQL抽象语法树设计六个原子变换操作符，逐步在关系、谓词、聚合和嵌套维度上增加查询复杂度。此外，执行基础的SQL细化模块和模式感知去重机制确保生成高质量、结构多样化的Text-to-SQL配对数据，提高了合成过程的可靠性和效率。",
      "result": "实验结果表明，使用EvolSQL合成数据微调的70亿参数模型，在性能上超越了在更大规模SynSQL数据集上训练的模型，尽管仅使用了SynSQL数据量的1/18。这证明了EvolSQL生成的数据具有更高的质量和结构多样性，有效提升了Text-to-SQL模型的训练效率和效果，解决了数据稀缺问题。",
      "conclusion": "EvolSQL通过结构感知进化策略，成功生成了高质量、结构复杂的Text-to-SQL数据集，显著提升了模型性能。该研究为Text-to-SQL领域提供了有效的数据生成工具，具有学术和实际应用价值，未来可进一步优化进化策略或扩展到其他结构化数据生成任务，如更复杂的SQL查询合成或多语言支持。",
      "tags": [
        "Text-to-SQL",
        "Data Synthesis",
        "SQL Evolution",
        "Structure-Aware",
        "Abstract Syntax Tree"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:51.801529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04859",
    "title": "A Navigational Approach for Comprehensive RAG via Traversal over Proposition Graphs",
    "authors": [
      "Maxime Delmas",
      "Lei Xu",
      "André Freitas"
    ],
    "abstract": "Standard RAG pipelines based on chunking excel at simple factual retrieval but fail on complex multi-hop queries due to a lack of structural connectivity. Conversely, initial strategies that interleave retrieval with reasoning often lack global corpus awareness, while Knowledge Graph (KG)-based RAG performs strongly on complex multi-hop tasks but suffers on fact-oriented single-hop queries. To bridge this gap, we propose a novel RAG framework: ToPG (Traversal over Proposition Graphs). ToPG models its knowledge base as a heterogeneous graph of propositions, entities, and passages, effectively combining the granular fact density of propositions with graph connectivity. We leverage this structure using iterative Suggestion-Selection cycles, where the Suggestion phase enables a query-aware traversal of the graph, and the Selection phase provides LLM feedback to prune irrelevant propositions and seed the next iteration. Evaluated on three distinct QA tasks (Simple, Complex, and Abstract QA), ToPG demonstrates strong performance across both accuracy- and quality-based metrics. Overall, ToPG shows that query-aware graph traversal combined with factual granularity is a critical component for efficient structured RAG systems. ToPG is available at https://github.com/idiap/ToPG.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04859.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04859",
    "published": "2026-01-08T11:50:40Z",
    "updated": "2026-01-08T11:50:40Z",
    "comment": "23 pages, 10 figures, 6 tables",
    "light_analysis": {
      "overview": "ToPG提出一个新颖的RAG框架，通过遍历命题图来结合事实粒度和图连接性，以处理复杂多跳查询。",
      "motivation": "标准RAG基于分块的检索在简单事实检索上优秀，但缺乏结构连接性导致在复杂多跳查询上失败。相反，初期的检索与推理交织策略缺乏全局语料意识，而基于知识图的RAG在复杂多跳任务上强但在事实导向的单跳查询上差。为了弥合这个差距，本研究旨在开发一个全面的RAG系统，以平衡不同查询类型的处理需求，提升问答系统的整体性能。",
      "method": "ToPG框架将知识库建模为一个包含命题、实体和段落的异构图，结合了命题的粒度事实密度和图的结构连接性。核心方法是迭代的Suggestion-Selection循环：Suggestion阶段实现查询感知的图遍历，Selection阶段利用LLM反馈剪枝无关命题并指导下一轮迭代。这种方法允许全局语料意识和查询特定处理，提高检索效率和准确性。",
      "result": "在三个不同的问答任务（简单、复杂和抽象QA）上评估，ToPG在准确性和质量指标上均表现出强大性能。尽管摘要未提供具体数据，但结果表明ToPG在多种查询类型上均优于或匹敌现有方法，尤其在复杂多跳查询和简单事实检索上取得平衡，展现了全面的检索能力。",
      "conclusion": "ToPG的主要贡献是提出了一个结合事实粒度和图连接性的RAG框架，有效处理各种复杂度的查询。其学术价值在于弥合了不同RAG方法之间的差距，提升了结构化检索系统的理论框架。实际应用价值在于增强了问答系统的鲁棒性和适应性。摘要未明确说明局限性，但暗示了未来工作可进一步优化图结构和迭代效率。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Graph Traversal",
        "Proposition Graphs",
        "Iterative Retrieval",
        "LLM Feedback"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:02.438021Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04857",
    "title": "MisSpans: Fine-Grained False Span Identification in Cross-Domain Fake News",
    "authors": [
      "Zhiwei Liu",
      "Paul Thompson",
      "Jiaqi Rong",
      "Baojie Qu",
      "Runteng Guo",
      "Min Peng",
      "Qianqian Xie",
      "Sophia Ananiadou"
    ],
    "abstract": "Online misinformation is increasingly pervasive, yet most existing benchmarks and methods evaluate veracity at the level of whole claims or paragraphs using coarse binary labels, obscuring how true and false details often co-exist within single sentences. These simplifications also limit interpretability: global explanations cannot identify which specific segments are misleading or differentiate how a detail is false (e.g., distorted vs. fabricated). To address these gaps, we introduce MisSpans, the first multi-domain, human-annotated benchmark for span-level misinformation detection and analysis, consisting of paired real and fake news stories. MisSpans defines three complementary tasks: MisSpansIdentity for pinpointing false spans within sentences, MisSpansType for categorising false spans by misinformation type, and MisSpansExplanation for providing rationales grounded in identified spans. Together, these tasks enable fine-grained localisation, nuanced characterisation beyond true/false and actionable explanations. Expert annotators were guided by standardised guidelines and consistency checks, leading to high inter-annotator agreement. We evaluate 15 representative LLMs, including reasoning-enhanced and non-reasoning variants, under zero-shot and one-shot settings. Results reveal the challenging nature of fine-grained misinformation identification and analysis, and highlight the need for a deeper understanding of how performance may be influenced by multiple interacting factors, including model size and reasoning capabilities, along with domain-specific textual features. This project will be available at https://github.com/lzw108/MisSpans.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04857.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04857",
    "published": "2026-01-08T11:46:30Z",
    "updated": "2026-01-08T11:46:30Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "本文提出了MisSpans基准，用于在跨领域假新闻中进行细粒度的虚假span识别和分析，促进错误信息检测的精确化和可解释性。",
      "motivation": "在线错误信息日益普遍，但现有基准和方法通常使用粗粒度二元标签在声明或段落级别评估真实性，忽视了真假细节在单个句子中共存的问题。这种简化限制了可解释性，因为全局解释无法精确定位误导性部分或区分错误类型（如事实扭曲与捏造），导致检测精度不足和解析能力受限。因此，研究旨在填补这些空白，推动细粒度错误信息分析的发展，以提高假新闻检测的准确性和实用性。",
      "method": "论文创建了MisSpans基准，这是首个多领域、人工注释的span级错误信息检测和分析工具集，由成对的真实与虚假新闻故事组成。核心方法包括定义三个互补任务：MisSpansIdentity用于识别句子中的虚假文本片段，MisSpansType根据错误信息类型进行分类，MisSpansExplanation为识别的span提供基于事实的解释。数据构建中，专家注释者遵循标准化指南并通过一致性检查确保高互评一致性。此外，研究评估了15个代表性大型语言模型，包括推理增强和非推理变体，在零样本和一样本设置下进行测试。",
      "result": "实验结果表明，细粒度错误信息识别和分析任务对现有模型构成显著挑战。在15个大型语言模型的评估中，性能受多种因素交互影响，包括模型大小、推理能力以及领域特定的文本特征。尽管摘要未明确提供具体准确率数据，但研究发现这些任务揭示了现有模型在精细定位和分类方面的局限性。与基线方法相比，结果强调了细粒度分析的难度，并指出需要更深入理解模型性能与因素之间的复杂关系以指导未来改进。",
      "conclusion": "本研究的核心贡献是引入了MisSpans基准，为跨领域假新闻提供了首个span级细粒度检测和分析框架。这具有重要学术价值，推动了错误信息研究向更精确化发展，并增强了实际应用潜力，如提升假新闻检测的可解释性和准确性。然而，研究也揭示了任务挑战性，性能受多因素影响，未来工作可集中在优化模型架构、探索因素交互作用以及开发更有效的训练策略上，以进一步推进该领域进展。",
      "tags": [
        "Misinformation Detection",
        "Span-Level Analysis",
        "Large Language Models",
        "Benchmark Evaluation",
        "Fine-Grained Localization"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:56.780384Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04853",
    "title": "RAAR: Retrieval Augmented Agentic Reasoning for Cross-Domain Misinformation Detection",
    "authors": [
      "Zhiwei Liu",
      "Runteng Guo",
      "Baojie Qu",
      "Yuechen Jiang",
      "Min Peng",
      "Qianqian Xie",
      "Sophia Ananiadou"
    ],
    "abstract": "Cross-domain misinformation detection is challenging, as misinformation arises across domains with substantial differences in knowledge and discourse. Existing methods often rely on single-perspective cues and struggle to generalize to challenging or underrepresented domains, while reasoning large language models (LLMs), though effective on complex tasks, are limited to same-distribution data. To address these gaps, we introduce RAAR, the first retrieval-augmented agentic reasoning framework for cross-domain misinformation detection. To enable cross-domain transfer beyond same-distribution assumptions, RAAR retrieves multi-perspective source-domain evidence aligned with each target sample's semantics, sentiment, and writing style. To overcome single-perspective modeling and missing systematic reasoning, RAAR constructs verifiable multi-step reasoning paths through specialized multi-agent collaboration, where perspective-specific agents produce complementary analyses and a summary agent integrates them under verifier guidance. RAAR further applies supervised fine-tuning and reinforcement learning to train a single multi-task verifier to enhance verification and reasoning capabilities. Based on RAAR, we trained the RAAR-8b and RAAR-14b models. Evaluation on three cross-domain misinformation detection tasks shows that RAAR substantially enhances the capabilities of the base models and outperforms other cross-domain methods, advanced LLMs, and LLM-based adaptation approaches. The project will be released at https://github.com/lzw108/RAAR.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04853.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04853",
    "published": "2026-01-08T11:43:16Z",
    "updated": "2026-01-08T11:43:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 RAAR 框架，首次将检索增强与代理推理相结合于跨领域虚假信息检测，通过多视角证据检索和多代理协作提升模型泛化能力。",
      "motivation": "跨领域虚假信息检测因领域间知识和话语差异大而具有挑战性。现有方法通常依赖单一视角线索，难以泛化到具有挑战性或代表性不足的领域。大型语言模型虽然能处理复杂任务，但局限于同分布数据，限制了其在实际跨领域场景中的应用。因此，需要新方法来超越同分布假设并整合多视角分析，以提高检测的准确性和鲁棒性。",
      "method": "RAAR 框架包含检索模块和多代理推理系统。检索模块根据目标样本的语义、情感和写作风格，从源领域获取多视角证据。通过专业化多代理协作构建可验证的多步推理路径，包括特定视角代理进行互补分析和总结代理在验证器指导下整合。应用监督微调和强化学习训练多任务验证器以增强验证和推理能力，并基于此训练了 RAAR-8b 和 RAAR-14b 模型。",
      "result": "在三个跨领域虚假信息检测任务上的评估显示，RAAR 显著增强了基础模型的性能，优于其他跨领域方法、先进的大型语言模型以及基于大型语言模型的适应方法。摘要未明确说明具体准确率数字，但结果表明该方法在跨领域场景中具有优越泛化能力，证明了检索增强和代理推理框架的有效性。",
      "conclusion": "RAAR 框架通过检索增强和多代理推理解决了跨领域虚假信息检测难题，主要贡献在于创新框架超越同分布限制并整合多视角证据。该研究具有学术价值，推动了虚假信息检测领域发展，并具实际应用潜力。项目已开源，未来工作可扩展框架到更多领域或任务，并探索其局限性。",
      "tags": [
        "Retrieval-Augmented",
        "Multi-Agent Reasoning",
        "Cross-Domain Adaptation",
        "Large Language Models",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:37.343813Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04833",
    "title": "When AI Settles Down: Late-Stage Stability as a Signature of AI-Generated Text Detection",
    "authors": [
      "Ke Sun",
      "Guangsheng Bao",
      "Han Cui",
      "Yue Zhang"
    ],
    "abstract": "Zero-shot detection methods for AI-generated text typically aggregate token-level statistics across entire sequences, overlooking the temporal dynamics inherent to autoregressive generation. We analyze over 120k text samples and reveal Late-Stage Volatility Decay: AI-generated text exhibits rapidly stabilizing log probability fluctuations as generation progresses, while human writing maintains higher variability throughout. This divergence peaks in the second half of sequences, where AI-generated text shows 24--32\\% lower volatility. Based on this finding, we propose two simple features: Derivative Dispersion and Local Volatility, which computed exclusively from late-stage statistics. Without perturbation sampling or additional model access, our method achieves state-of-the-art performance on EvoBench and MAGE benchmarks and demonstrates strong complementarity with existing global methods.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04833.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04833",
    "published": "2026-01-08T11:11:00Z",
    "updated": "2026-01-08T11:11:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文揭示了AI生成文本在晚期波动性衰减现象，并提出基于此的简单检测特征，实现了高性能的零-shot检测。",
      "motivation": "现有零-shot AI生成文本检测方法通常聚合整个序列的token级别统计信息，忽略了自回归生成过程中的时间动态变化，导致可能无法有效区分AI和人类写作的细微模式。这一问题在序列晚期尤为突出，因为AI生成文本的波动性表现出独特稳定性，而现有方法未能捕获这一差异，限制了检测精度。因此，研究旨在探索时间维度特征，以解决这一不足并提升检测性能。",
      "method": "方法基于对超过12万个文本样本的分析，发现Late-Stage Volatility Decay现象：AI生成文本在序列生成后期log概率波动性迅速稳定，人类写作则保持较高变异性，尤其在第二半序列中AI文本波动性低24-32%。基于此，提出两个简单特征Derivative Dispersion和Local Volatility，仅使用序列晚期统计信息计算，无需扰动采样或额外模型访问，直接利用现有自回归模型的输出进行特征提取。",
      "result": "所提方法在EvoBench和MAGE基准测试中实现了最先进的性能，尽管摘要未明确给出具体准确率数据，但表明显著优于基线方法。特征分析显示AI生成文本在晚期波动性低24-32%，支撑了方法的有效性。此外，新方法与现有全局方法展现出强互补性，可结合使用以进一步提升整体检测效果。",
      "conclusion": "论文主要贡献在于揭示了Late-Stage Volatility Decay现象，并开发了基于晚期稳定性的检测特征，为AI生成文本检测提供了新视角。学术上，丰富了动态模式分析的理论基础；应用上，方法简单且无需额外资源，适用于实际部署。未来工作可能包括扩展到其他生成模型或更复杂文本类型，以增强方法的通用性和鲁棒性。",
      "tags": [
        "AI-Generated Text Detection",
        "Zero-shot Detection",
        "Autoregressive Generation",
        "Late-Stage Volatility Decay",
        "Derivative Dispersion"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:02.009671Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03034",
    "title": "NorwAI's Large Language Models: Technical Report",
    "authors": [
      "Jon Atle Gulla",
      "Peng Liu",
      "Lemei Zhang"
    ],
    "abstract": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03034.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03034",
    "published": "2026-01-06T14:06:55Z",
    "updated": "2026-01-08T10:58:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "开发了针对挪威语和其他斯堪的纳维亚语言的大语言模型家族，以弥补其在自然语言处理中的代表性不足。",
      "motivation": "挪威语约有五百万人使用，但在许多自然语言处理重大突破中代表性不足，限制了挪威语社区受益于AI技术发展。现有通用模型缺乏针对挪威语的优化，导致资源稀缺和模型性能不佳，因此该研究旨在填补这一空白，促进语言多样性在NLP领域的应用，并通过定制化模型提升挪威语在实际任务中的适应性。",
      "method": "研究方法基于多种Transformer架构，包括GPT、Mistral、Llama2、Mixtral和Magistral，模型从零开始或持续预训练在25亿至88.45亿token的数据上。使用挪威语扩展的分词器优化语言处理，并应用高级后训练策略，如指导调优，以增强性能、鲁棒性和跨任务适应性。关键创新在于针对挪威语的定制化设计和多架构集成，确保模型在多样化场景中的有效性。",
      "result": "摘要中未提供具体的性能指标，但指导调优的变体如Mistral-7B-Instruct和Mixtral-8x7B-Instruct展示了强大的助手式能力，表明在交互式和领域特定应用中的实际部署潜力。结果侧重于模型能力的定性描述，缺乏与基线方法的量化对比数据，仅通过能力展示强调了其在挪威语任务中的潜在优势。",
      "conclusion": "该研究成功开发并公开了一系列针对挪威语的大语言模型，为北欧组织、公司和学生提供了研究和实验资源。这些模型填补了挪威语在NLP中的空白，具有实际应用价值，尤其在助手式交互和领域特定任务中。报告未明确讨论模型局限性，未来工作可能包括更广泛评估和多语言扩展以进一步提升性能。",
      "tags": [
        "Large Language Models",
        "Transformer",
        "Instruction Tuning",
        "Pretraining",
        "Scandinavian NLP"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:11.907793Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04795",
    "title": "Defense Against Indirect Prompt Injection via Tool Result Parsing",
    "authors": [
      "Qiang Yu",
      "Xinran Cheng",
      "Chuanyi Liu"
    ],
    "abstract": "As LLM agents transition from digital assistants to physical controllers in autonomous systems and robotics, they face an escalating threat from indirect prompt injection. By embedding adversarial instructions into the results of tool calls, attackers can hijack the agent's decision-making process to execute unauthorized actions. This vulnerability poses a significant risk as agents gain more direct control over physical environments. Existing defense mechanisms against Indirect Prompt Injection (IPI) generally fall into two categories. The first involves training dedicated detection models; however, this approach entails high computational overhead for both training and inference, and requires frequent updates to keep pace with evolving attack vectors. Alternatively, prompt-based methods leverage the inherent capabilities of LLMs to detect or ignore malicious instructions via prompt engineering. Despite their flexibility, most current prompt-based defenses suffer from high Attack Success Rates (ASR), demonstrating limited robustness against sophisticated injection attacks. In this paper, we propose a novel method that provides LLMs with precise data via tool result parsing while effectively filtering out injected malicious code. Our approach achieves competitive Utility under Attack (UA) while maintaining the lowest Attack Success Rate (ASR) to date, significantly outperforming existing methods. Code is available at GitHub.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04795.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04795",
    "published": "2026-01-08T10:21:56Z",
    "updated": "2026-01-08T10:21:56Z",
    "comment": "20 pages, 3 figures, 5 tables",
    "light_analysis": {
      "overview": "提出一种通过工具结果解析来防御间接提示注入的新方法，在保持实用性的同时最小化攻击成功率。",
      "motivation": "随着大型语言模型（LLM）代理在自主系统和机器人中从数字助手过渡到物理控制器，间接提示注入（Indirect Prompt Injection, IPI）威胁日益严重。攻击者通过工具调用结果嵌入恶意指令，劫持代理决策过程，可能导致未授权行动，对物理环境构成重大风险。现有防御方法分为两类：训练专用检测模型计算开销高且需频繁更新；基于提示的方法虽灵活，但攻击成功率高，鲁棒性有限，因此亟需更高效、鲁棒的防御机制。",
      "method": "该方法的核心是通过工具结果解析来为LLMs提供精确数据，并有效过滤注入的恶意代码。它利用解析技术从工具调用的结果中提取和清洗数据，去除潜在的敌对指令，从而保护LLM代理免受间接提示注入攻击。创新点在于结合了解析和过滤机制，以较低的计算开销实现鲁棒防御，突出技术特色在于优化数据处理流程。摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验结果显示，该方法在攻击下的实用性（UA）表现竞争性，同时攻击成功率（ASR）保持最低，显著优于现有防御方法。与基线相比，它在保持高实用性的同时，有效降低了攻击成功风险，提升了整体鲁棒性。摘要未明确说明具体数据指标如准确率，但强调了其在ASR和UA方面的优越性能。",
      "conclusion": "本研究提出了一种基于工具结果解析的防御方法，有效对抗间接提示注入攻击，主要贡献是提供计算高效且鲁棒的解决方案，提升了LLM代理的安全性。学术价值在于创新性地结合了解析和过滤技术，实际应用可用于增强自主系统和机器人的安全防护。未来工作方向可包括进一步优化解析精度或扩展到其他攻击场景，摘要未明确说明具体局限性。",
      "tags": [
        "Indirect Prompt Injection",
        "LLM Agents",
        "Tool Result Parsing",
        "Prompt Engineering",
        "Security Defense"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:03.002536Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04790",
    "title": "Belief in Authority: Impact of Authority in Multi-Agent Evaluation Framework",
    "authors": [
      "Junhyuk Choi",
      "Jeongyoun Kwon",
      "Heeju Kim",
      "Haeun Cho",
      "Hayeong Jung",
      "Sehee Min",
      "Bugeun Kim"
    ],
    "abstract": "Multi-agent systems utilizing large language models often assign authoritative roles to improve performance, yet the impact of authority bias on agent interactions remains underexplored. We present the first systematic analysis of role-based authority bias in free-form multi-agent evaluation using ChatEval. Applying French and Raven's power-based theory, we classify authoritative roles into legitimate, referent, and expert types and analyze their influence across 12-turn conversations. Experiments with GPT-4o and DeepSeek R1 reveal that Expert and Referent power roles exert stronger influence than Legitimate power roles. Crucially, authority bias emerges not through active conformity by general agents, but through authoritative roles consistently maintaining their positions while general agents demonstrate flexibility. Furthermore, authority influence requires clear position statements, as neutral responses fail to generate bias. These findings provide key insights for designing multi-agent frameworks with asymmetric interaction patterns.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04790.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04790",
    "published": "2026-01-08T10:13:56Z",
    "updated": "2026-01-08T10:13:56Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "论文首次系统分析了多智能体评估中基于角色的权威偏见，发现专家和参照权威比合法权威更具影响力，并揭示偏见源于权威角色的立场坚持。",
      "motivation": "在多智能体系统中，常通过分配权威角色以提升性能，但权威偏见对智能体交互的具体影响未被充分探索。现有方法多侧重于优化性能，而忽略了权威角色可能引入的系统性偏见，导致评估不公或效率低下。因此，本研究旨在填补这一空白，探讨权威偏见在多智能体环境中的机制，为设计更公平、有效的交互框架提供理论基础。",
      "method": "研究采用 ChatEval 作为多智能体评估框架，应用 French 和 Raven 的权威理论，将权威角色分类为合法、参照和专家类型。通过在12轮对话中模拟交互，分析这些角色对智能体行为的影响。实验使用 GPT-4o 和 DeepSeek R1 作为智能体模型，关键创新在于首次系统性探索角色基于的权威偏见，并设置多轮对话以捕捉动态交互过程。",
      "result": "实验结果显示，专家和参照权威角色在多智能体对话中比合法权威角色展现出更强的影响力。权威偏见并非通过一般智能体的主动顺从产生，而是权威角色在对话中保持一致立场，而一般智能体表现出灵活性。此外，权威影响需要明确的立场陈述，中性回应不会引发偏见。这些发现通过对比不同权威类型的效果，提供了对权威偏见机制的实证洞察。",
      "conclusion": "本研究的主要贡献在于首次对多智能体评估中的权威偏见进行了系统分析，揭示了专家和参照权威的显著影响。学术上，这增进了对权威理论在多智能体环境中的应用理解；实践上，为设计具有不对称交互模式的多智能体框架提供了关键指导，帮助减少偏见并提升系统公平性。未来工作可扩展到更多模型或交互场景以进一步验证。",
      "tags": [
        "Multi-Agent Systems",
        "Large Language Models",
        "Authority Bias",
        "ChatEval",
        "Role-Based Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:11.459875Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04789",
    "title": "NC2C: Automated Convexification of Generic Non-Convex Optimization Problems",
    "authors": [
      "Xinyue Peng",
      "Yanming Liu",
      "Yihan Cang",
      "Yuwei Zhang",
      "Xinyi Wang",
      "Songhang Deng",
      "Jiannan Cao"
    ],
    "abstract": "Non-convex optimization problems are pervasive across mathematical programming, engineering design, and scientific computing, often posing intractable challenges for traditional solvers due to their complex objective functions and constrained landscapes. To address the inefficiency of manual convexification and the over-reliance on expert knowledge, we propose NC2C, an LLM-based end-to-end automated framework designed to transform generic non-convex optimization problems into solvable convex forms using large language models. NC2C leverages LLMs' mathematical reasoning capabilities to autonomously detect non-convex components, select optimal convexification strategies, and generate rigorous convex equivalents. The framework integrates symbolic reasoning, adaptive transformation techniques, and iterative validation, equipped with error correction loops and feasibility domain correction mechanisms to ensure the robustness and validity of transformed problems. Experimental results on a diverse dataset of 100 generic non-convex problems demonstrate that NC2C achieves an 89.3\\% execution rate and a 76\\% success rate in producing feasible, high-quality convex transformations. This outperforms baseline methods by a significant margin, highlighting NC2C's ability to leverage LLMs for automated non-convex to convex transformation, reduce expert dependency, and enable efficient deployment of convex solvers for previously intractable optimization tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04789.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04789",
    "published": "2026-01-08T10:12:45Z",
    "updated": "2026-01-08T10:12:45Z",
    "comment": "First version of NC2C",
    "light_analysis": {
      "overview": "NC2C 是一种基于大型语言模型的自动化框架，用于将通用非凸优化问题转化为凸形式，减少专家依赖并提升求解效率。",
      "motivation": "非凸优化问题在数学编程、工程设计等领域广泛存在，但由于复杂的目函数和约束，传统求解器难以高效处理。现有方法主要依赖手动凸化和专家知识，导致效率低下、通用性受限，限制了优化任务的可扩展性。因此，开发自动化工具以替代手动过程、降低专家依赖性并提高优化求解的效率和可访问性，成为重要研究动机。",
      "method": "NC2C 框架利用大型语言模型的数学推理能力，自动检测优化问题中的非凸组件，选择最优凸化策略，并生成严格的凸等价问题。框架整合符号推理以处理数学结构，应用自适应变换技术根据不同问题调整方法，并通过迭代验证、错误校正循环和可行性域校正机制确保变换的鲁棒性和有效性，实现端到端自动化非凸到凸转换，减少人为干预。",
      "result": "实验基于 100 个通用非凸问题，NC2C 实现了 89.3% 的执行率和 76% 的成功率，在生成可行且高质量的凸变换方面表现优异。与基线方法相比，性能提升显著，证明了 NC2C 利用 LLM 进行自动化凸化的有效性，有效减少专家依赖，并使凸求解器能应用于先前难处理的优化任务，提升了整体求解效率。",
      "conclusion": "NC2C 的主要贡献是提出了一种基于 LLM 的自动化非凸到凸转换框架，展示了大型语言模型在数学优化中的实用价值。研究具有重要学术意义，推动了自动化优化方法的发展，并在实际应用中降低专家成本、提高求解效率。未来工作可能包括改进转换成功率、扩展到更复杂的非凸问题类型，以及进一步整合自适应机制以增强通用性。",
      "tags": [
        "Large Language Model",
        "Non-Convex Optimization",
        "Convexification",
        "Symbolic Reasoning",
        "Automated Transformation"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:56.647712Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.17652",
    "title": "Qomhra: A Bilingual Irish and English Large Language Model",
    "authors": [
      "Joseph McInerney",
      "Khanh-Tung Tran",
      "Liam Lonergan",
      "Ailbhe Ní Chasaide",
      "Neasa Ní Chiaráin",
      "Barry Devereux"
    ],
    "abstract": "Large language model (LLM) research and development has overwhelmingly focused on the world's major languages, leading to under-representation of low-resource languages such as Irish. This paper introduces \\textbf{Qomhrá}, a bilingual Irish and English LLM, developed under extremely low-resource constraints. A complete pipeline is outlined spanning bilingual continued pre-training, instruction tuning, and the synthesis of human preference data for future alignment training. We focus on the lack of scalable methods to create human preference data by proposing a novel method to synthesise such data by prompting an LLM to generate ``accepted'' and ``rejected'' responses, which we validate as aligning with L1 Irish speakers. To select an LLM for synthesis, we evaluate the top closed-weight LLMs for Irish language generation performance. Gemini-2.5-Pro is ranked highest by L1 and L2 Irish-speakers, diverging from LLM-as-a-judge ratings, indicating a misalignment between current LLMs and the Irish-language community. Subsequently, we leverage Gemini-2.5-Pro to translate a large scale English-language instruction tuning dataset to Irish and to synthesise a first-of-its-kind Irish-language human preference dataset. We comprehensively evaluate Qomhrá across several benchmarks, testing translation, gender understanding, topic identification, and world knowledge; these evaluations show gains of up to 29\\% in Irish and 44\\% in English compared to the existing open-source Irish LLM baseline, UCCIX. The results of our framework provide insight and guidance to developing LLMs for both Irish and other low-resource languages.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.17652.pdf",
    "abs_url": "https://arxiv.org/abs/2510.17652",
    "published": "2025-10-20T15:27:53Z",
    "updated": "2026-01-08T10:04:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文开发了Qomhrá双语爱尔兰语和英语大语言模型，并提出了一种合成人类偏好数据的新方法，以支持低资源语言的自然语言处理。",
      "motivation": "大型语言模型研究主要集中在世界主要语言上，导致低资源语言如爱尔兰语缺乏代表性。这一问题的重要性在于支持语言多样性和促进多语言人工智能的公平发展。现有方法在创建人类偏好数据方面存在不足，缺乏可扩展的解决方案，难以满足低资源语言的特定需求，从而限制了相关模型的开发和优化。",
      "method": "论文提出了一个完整的开发管道，包括双语继续预训练、指令调优和合成人类偏好数据。核心创新是提出一种新方法来合成人类偏好数据：通过提示大型语言模型生成“接受”和“拒绝”响应，并验证其与爱尔兰语母语者的对齐。为了选择用于合成的模型，评估了顶级闭源大语言模型的爱尔兰语生成性能，最终选择Gemini-2.5-Pro来翻译大规模英语指令调优数据集并合成首个人类偏好数据集。",
      "result": "Qomhrá在多个基准测试中进行了全面评估，包括翻译、性别理解、主题识别和世界知识。实验结果显示，与现有开源爱尔兰大语言模型基线UCCIX相比，Qomhrá在爱尔兰语任务中性能提升达29%，在英语任务中提升达44%。这些数据表明，所提出的框架在低资源语言处理方面取得了显著改进，并超越了现有基线方法。",
      "conclusion": "本研究的主要贡献是开发了Qomhrá双语大语言模型，并创新性地提出了合成人类偏好数据的方法。学术价值在于为低资源语言的自然语言处理研究提供了新的洞见和指导。实际应用价值在于促进多语言人工智能的发展，支持语言多样性保护。未来工作可能包括将该框架扩展到其他低资源语言，或进一步优化数据合成方法以提高对齐效果。",
      "tags": [
        "Large Language Model",
        "Bilingual Model",
        "Low-Resource Language Processing",
        "Instruction Tuning",
        "Human Preference Data Synthesis"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:10.547714Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04778",
    "title": "CounterVid: Counterfactual Video Generation for Mitigating Action and Temporal Hallucinations in Video-Language Models",
    "authors": [
      "Tobia Poppi",
      "Burak Uzkent",
      "Amanmeet Garg",
      "Lucas Porto",
      "Garin Kessler",
      "Yezhou Yang",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara",
      "Florian Schiffers"
    ],
    "abstract": "Video-language models (VLMs) achieve strong multimodal understanding but remain prone to hallucinations, especially when reasoning about actions and temporal order. Existing mitigation strategies, such as textual filtering or random video perturbations, often fail to address the root cause: over-reliance on language priors rather than fine-grained visual dynamics. We propose a scalable framework for counterfactual video generation that synthesizes videos differing only in actions or temporal structure while preserving scene context. Our pipeline combines multimodal LLMs for action proposal and editing guidance with diffusion-based image and video models to generate semantic hard negatives at scale. Using this framework, we build CounterVid, a synthetic dataset of ~26k preference pairs targeting action recognition and temporal reasoning. We further introduce MixDPO, a unified Direct Preference Optimization approach that jointly leverages textual and visual preferences. Fine-tuning Qwen2.5-VL with MixDPO yields consistent improvements, notably in temporal ordering, and transfers effectively to standard video hallucination benchmarks. Code and models will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04778.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04778",
    "published": "2026-01-08T10:03:07Z",
    "updated": "2026-01-08T10:03:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出CounterVid框架和MixDPO方法，通过反事实视频生成和混合偏好优化，有效缓解视频语言模型中的行动和时间幻觉问题。",
      "motivation": "视频语言模型（VLMs）虽然在多模态理解上表现出色，但仍易产生幻觉，尤其在推理行动和时间顺序时。现有缓解策略如文本过滤或随机视频扰动常失败，因它们过度依赖语言先验，未能从根本上解决对细粒度视觉动态的忽视。这限制了VLMs在实际应用中的可靠性和准确性，亟需一种能针对性地生成对抗性数据的方法来增强模型鲁棒性。",
      "method": "研究提出一个可扩展的反事实视频生成框架，通过合成仅在行动或时间结构上不同但场景上下文一致的视频，以解决幻觉问题。关键技术包括结合多模态大语言模型（LLMs）进行行动提案和编辑指导，并使用基于扩散的图像和视频模型来大规模生成语义硬负例。基于此框架构建了CounterVid数据集，包含约26,000个偏好对，专注于行动识别和时间推理。进一步引入MixDPO方法，这是一种统一的直接偏好优化方法，能同时利用文本和视觉偏好进行模型微调。",
      "result": "通过使用MixDPO对Qwen2.5-VL进行微调，论文取得了显著改进，尤其是在时间顺序推理方面。实验结果展示了持续的性能提升，并能有效泛化到标准视频幻觉基准测试中。虽然摘要未明确提供具体指标数值，但与基线方法相比，该方法显示出优越的迁移能力和泛化性能，验证了反事实视频生成和混合偏好优化在缓解幻觉方面的有效性。",
      "conclusion": "本研究的主要贡献是开发了CounterVid框架和MixDPO方法，显著减少了视频语言模型中的行动和时间幻觉，提升了模型的可靠性和准确性。这具有重要的学术价值，推动了多模态AI的发展，并为实际应用如视频内容分析提供了基础。未来工作可进一步扩展数据集或探索更高效的反事实生成技术。代码和模型的公开可用性将促进社区研究和应用发展。",
      "tags": [
        "Video-Language Models",
        "Counterfactual Video Generation",
        "Diffusion Models",
        "Direct Preference Optimization",
        "Multimodal LLMs"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:43.952581Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.01486",
    "title": "TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu",
    "authors": [
      "Vallabhaneni Raj Kumar",
      "Ashwin S",
      "Supriya Manna",
      "Niladri Sett",
      "Cheedella V S N M S Hema Harshitha",
      "Kurakula Harshitha",
      "Anand Kumar Sharma",
      "Basina Deepakraj",
      "Tanuj Sarkar",
      "Bondada Navaneeth Krishna",
      "Samanthapudi Shakeer"
    ],
    "abstract": "In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 21,119 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with human rationales improves model accuracy and models' alignment with human reasoning, but does not necessarily reduce bias.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.01486.pdf",
    "abs_url": "https://arxiv.org/abs/2508.01486",
    "published": "2025-08-02T20:42:37Z",
    "updated": "2026-01-08T09:54:08Z",
    "comment": "We identified and resolved technical issues in the previous version and updated the results and resources accordingly",
    "light_analysis": {
      "overview": "论文介绍了泰卢固语情感分类基准数据集TeSent，整合人类标注理由以评估模型可解释性和公平性。",
      "motivation": "泰卢固语作为印度六种古典语言之一，拥有9600万使用者，但在全球NLP和机器学习领域代表性不足，主要因缺乏高质量标注资源。本研究旨在解决情感分类问题，强调可解释性和公平性在现代机器学习中的重要性，以弥补现有方法在资源稀缺语言中的不足，推动更可靠的AI应用发展。",
      "method": "研究通过抓取社交媒体、新闻网站等多元领域的泰卢固语文本，预处理后生成21,119个句子，并开发自定义标注平台和协议，收集情感标签及人类标注理由。方法包括微调多种SOTA预训练模型，对比带理由与不带理由的训练方式，并构建可解释性评估套件，应用六种事后解释器进行可信度分析。同时，创建TeEEC语料库评估模型公平性，提供全面评估工具。",
      "result": "实验结果显示，使用人类理由训练模型能提高准确性并增强与人类推理的对齐，但未必减少偏见。具体性能指标摘要未明确说明，但与基线（不带理由的模型）相比，模型表现有所改善。公平性和可解释性评估套件为泰卢固语NLP任务提供了新基准，支持更深入的分析和比较。",
      "conclusion": "本研究的主要贡献是创建了TeSent数据集和评估套件，填补了泰卢固语NLP资源空白，推动了可解释性和公平性评估在机器学习中的应用。学术价值在于促进低资源语言研究，实际应用有助于提升模型可靠性和社会接受度。未来可扩展数据集或探索更广泛领域，以进一步优化性能和减少偏见。",
      "tags": [
        "Sentiment Classification",
        "Explainable AI",
        "Fairness Evaluation",
        "Benchmark Dataset",
        "Telugu NLP"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:25.165833Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.01014",
    "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation",
    "authors": [
      "Bosi Wen",
      "Yilin Niu",
      "Cunxiang Wang",
      "Pei Ke",
      "Xiaoying Ling",
      "Ying Zhang",
      "Aohan Zeng",
      "Hongning Wang",
      "Minlie Huang"
    ],
    "abstract": "Instruction-following is a fundamental ability of Large Language Models (LLMs), requiring their generated outputs to follow multiple constraints imposed in input instructions. Numerous studies have attempted to enhance this ability through preference optimization or reinforcement learning based on reward signals from LLM-as-a-Judge. However, existing evaluation models for instruction-following still possess many deficiencies, such as substantial costs and unreliable assessments. To this end, we propose IF-CRITIC, an LLM critic for fine-grained, efficient, and reliable instruction-following evaluation. We first develop a checklist generator to decompose instructions and generate constraint checklists. With the assistance of the checklists, we collect high-quality critique training data through a multi-stage critique filtering mechanism and employ a constraint-level preference optimization method to train IF-CRITIC. Extensive experiments show that the evaluation performance of IF-CRITIC can beat strong LLM-as-a-Judge baselines, including o4-mini and Gemini-3-Pro. With the reward signals provided by IF-CRITIC, LLMs can achieve substantial performance gains in instruction-following optimization under lower computational overhead compared to strong LLM critic baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.01014.pdf",
    "abs_url": "https://arxiv.org/abs/2511.01014",
    "published": "2025-11-02T17:06:49Z",
    "updated": "2026-01-08T09:50:02Z",
    "comment": "24 pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出了IF-CRITIC，一个用于指令遵循评估的细粒度、高效、可靠的LLM批评器，通过约束清单生成和多阶段筛选机制实现创新评估方法。",
      "motivation": "指令遵循是大型语言模型的基本能力，要求模型输出符合输入指令中的多个约束。现有研究常依赖基于LLM-as-a-Judge的偏好优化或强化学习来提升此能力，但评估模型存在成本高昂、评估不可靠等缺陷，如不一致的判断和计算开销大。这些不足限制了指令遵循优化的效率和准确性，因此开发更精细、可靠的评估模型对于改进LLM性能至关重要。",
      "method": "IF-CRITIC方法首先开发检查表生成器，将指令分解为细粒度约束清单，以捕捉具体评估点。接着，利用清单通过多阶段筛选机制收集高质量批评训练数据，去除噪声并确保数据可靠性。最后，采用约束级偏好优化方法训练IF-CRITIC批评器，优化其在每个约束上的评估能力。该方法强调细粒度、高效性和可靠性，避免了传统评估方法的粗糙性。",
      "result": "实验显示，IF-CRITIC在指令遵循评估性能上优于强基线模型，包括o4-mini和Gemini-3-Pro等LLM-as-a-Judge方法。使用IF-CRITIC提供的奖励信号，LLMs在指令遵循优化中能实现显著性能提升，同时计算开销低于其他强批评器基线。这证明了IF-CRITIC在提高评估可靠性和降低计算成本方面的有效性，但摘要未明确说明具体性能指标如准确率提升百分比。",
      "conclusion": "IF-CRITIC为指令遵循评估提供了一个细粒度、高效且可靠的LLM批评器，通过约束清单和多阶段筛选改进了评估质量。其贡献在于促进了LLM优化中的准确奖励信号生成，具有重要学术价值和应用前景，如提升模型训练效率。未来工作可探索在其他任务中的应用或优化筛选机制，但摘要未明确说明具体局限性或扩展方向。",
      "tags": [
        "Large Language Model",
        "LLM Critic",
        "Instruction-Following",
        "Preference Optimization",
        "Constraint-Level Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:45.206530Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.11529",
    "title": "Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models",
    "authors": [
      "Yusheng Song",
      "Lirong Qiu",
      "Xi Zhang",
      "Zhihao Tang"
    ],
    "abstract": "The detection of sophisticated hallucinations in Large Language Models (LLMs) is hampered by a ``Detection Dilemma'': methods probing internal states (Internal State Probing) excel at identifying factual inconsistencies but fail on logical fallacies, while those verifying externalized reasoning (Chain-of-Thought Verification) show the opposite behavior. This schism creates a task-dependent blind spot: Chain-of-Thought Verification fails on fact-intensive tasks like open-domain QA where reasoning is ungrounded, while Internal State Probing is ineffective on logic-intensive tasks like mathematical reasoning where models are confidently wrong. We resolve this with a unified framework that bridges this critical gap. However, unification is hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse symbolic reasoning chains lack signals directly comparable to fine-grained internal states, and the Representational Alignment Barrier, a deep-seated mismatch between their underlying semantic spaces. To overcome these, we introduce a multi-path reasoning mechanism to obtain more comparable, fine-grained signals, and a segment-aware temporalized cross-attention module to adaptively fuse these now-aligned representations, pinpointing subtle dissonances. Extensive experiments on three diverse benchmarks and two leading LLMs demonstrate that our framework consistently and significantly outperforms strong baselines. Our code is available: https://github.com/peach918/HalluDet.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.11529.pdf",
    "abs_url": "https://arxiv.org/abs/2510.11529",
    "published": "2025-10-13T15:31:21Z",
    "updated": "2026-01-08T09:44:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一个统一框架，通过融合内部状态探测和思维链验证来检测大型语言模型中的复杂幻觉，解决了现有方法的Detection Dilemma。",
      "motivation": "该研究旨在解决大型语言模型（LLMs）中幻觉检测的难题。当前方法存在Detection Dilemma：内部状态探测（Internal State Probing）擅长识别事实不一致，但无法处理逻辑谬误；而思维链验证（Chain-of-Thought Verification）则相反。这种分裂导致任务依赖的盲点，例如思维链验证在开放域问答等事实密集型任务中失效，而内部状态探测在数学推理等逻辑密集型任务中无效。这个问题至关重要，因为准确检测幻觉对于提高LLMs的输出可靠性和实际应用安全有重要意义。",
      "method": "论文提出一个统一的幻觉检测框架，以弥合内部状态探测和思维链验证之间的差距。为解决Signal Scarcity Barrier（细粒度信号缺乏）和Representational Alignment Barrier（表示空间不匹配），引入了多路径推理机制，生成更可比、细粒度的信号，以及分段感知的时间化交叉注意力模块，自适应地融合对齐后的表示，从而精确定位细微不一致。该方法不依赖特定数据集或模型架构，但实验在三个基准和两个领先LLMs上进行，以验证有效性。",
      "result": "通过广泛的实验，在三个多样化基准和两个主要大型语言模型上评估了该框架。结果显示，该框架在幻觉检测任务上一致且显著地优于强基线方法，表明其能够有效克服现有方法的盲点。摘要未明确说明具体的性能指标（如准确率提升），但强调了框架在多个基准上的优势，证实了统一方法在提升检测效果方面的有效性。",
      "conclusion": "该研究的主要贡献是开发了一个统一的幻觉检测框架，成功解决了Detection Dilemma，结合了内部状态探测和思维链验证的优点。学术上，这推动了LLMs可信度和鲁棒性研究，为幻觉检测提供了新思路；实际上，有助于改进LLMs的输出质量和可靠性，减少错误信息。未来工作可探索更广泛的基准和模型类型，进一步优化融合机制或扩展到其他任务领域。",
      "tags": [
        "Large Language Models",
        "Hallucination Detection",
        "Internal State Probing",
        "Chain-of-Thought Verification",
        "Cross-Attention Module"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:57.091706Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04768",
    "title": "LANGSAE EDITING: Improving Multilingual Information Retrieval via Post-hoc Language Identity Removal",
    "authors": [
      "Dongjun Kim",
      "Jeongho Yoon",
      "Chanjun Park",
      "Heuiseok Lim"
    ],
    "abstract": "Dense retrieval in multilingual settings often searches over mixed-language collections, yet multilingual embeddings encode language identity alongside semantics. This language signal can inflate similarity for same-language pairs and crowd out relevant evidence written in other languages. We propose LANGSAE EDITING, a post-hoc sparse autoencoder trained on pooled embeddings that enables controllable removal of language-identity signal directly in vector space. The method identifies language-associated latent units using cross-language activation statistics, suppresses these units at inference time, and reconstructs embeddings in the original dimensionality, making it compatible with existing vector databases without retraining the base encoder or re-encoding raw text. Experiments across multiple languages show consistent improvements in ranking quality and cross-language coverage, with especially strong gains for script-distinct languages.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04768.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04768",
    "published": "2026-01-08T09:36:41Z",
    "updated": "2026-01-08T09:36:41Z",
    "comment": "16 pages, 3 figures",
    "light_analysis": {
      "overview": "提出 LANGSAE EDITING 方法，通过后处理稀疏自编码器移除多语言嵌入中的语言身份信号，提升密集检索的排名质量和跨语言覆盖。",
      "motivation": "多语言密集检索常在混合语言集合中进行搜索，但多语言嵌入将语言身份与语义信息一同编码。这种语言信号会夸大相同语言对之间的相似度，并挤出其他语言的相关证据，导致检索偏差，影响跨语言检索的准确性和公平性。现有方法中，嵌入编码语言身份的问题未得到有效解决，限制了多语言环境的公平性和效率。",
      "method": "论文提出 LANGSAE EDITING，一种后处理稀疏自编码器，训练在池化的嵌入向量上。核心创新点包括：使用跨语言激活统计识别语言相关的潜在单元，在推理时抑制这些单元，并在原始维度重建嵌入向量。该方法无需重新训练基础编码器或重新编码原始文本，兼容现有向量数据库，确保了部署的简便性和兼容性。",
      "result": "实验在多种语言上进行，结果显示排名质量和跨语言覆盖度均有持续改进。特别地，对于脚本不同的语言，增益更加显著。尽管摘要未提供具体数值，但强调了在所有测试语言中都观察到正向效果，与基线方法相比表现出优越性，验证了该方法的有效性。",
      "conclusion": "LANGSAE EDITING 的主要贡献是提出了一种可控的后处理技术来移除语言身份信号，从而改善多语言检索性能。其学术价值在于提供新方法处理嵌入偏差，实际应用价值在于与现有系统兼容，无需额外训练成本。摘要未明确说明局限性或未来工作方向，但可以推断该方法有潜力扩展用于其他信号移除任务。",
      "tags": [
        "Multilingual Information Retrieval",
        "Sparse Autoencoder",
        "Post-hoc Editing",
        "Language Identity Removal",
        "Cross-language Activation"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:44.497802Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04767",
    "title": "AT$^2$PO: Agentic Turn-based Policy Optimization via Tree Search",
    "authors": [
      "Zefang Zong",
      "Dingwei Chen",
      "Yang Li",
      "Qi Yi",
      "Bo Zhou",
      "Chengming Li",
      "Bo Qian",
      "Peng Chen",
      "Jie Jiang"
    ],
    "abstract": "LLM agents have emerged as powerful systems for tackling multi-turn tasks by interleaving internal reasoning and external tool interactions. Agentic Reinforcement Learning has recently drawn significant research attention as a critical post-training paradigm to further refine these capabilities. In this paper, we present AT$^2$PO (Agentic Turn-based Policy Optimization via Tree Search), a unified framework for multi-turn agentic RL that addresses three core challenges: limited exploration diversity, sparse credit assignment, and misaligned policy optimization. AT$^2$PO introduces a turn-level tree structure that jointly enables Entropy-Guided Tree Expansion for strategic exploration and Turn-wise Credit Assignment for fine-grained reward propagation from sparse outcomes. Complementing this, we propose Agentic Turn-based Policy Optimization, a turn-level learning objective that aligns policy updates with the natural decision granularity of agentic interactions. ATPO is orthogonal to tree search and can be readily integrated into any multi-turn RL pipeline. Experiments across seven benchmarks demonstrate consistent improvements over the state-of-the-art baseline by up to 1.84 percentage points in average, with ablation studies validating the effectiveness of each component. Our code is available at https://github.com/zzfoutofspace/ATPO.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04767.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04767",
    "published": "2026-01-08T09:35:49Z",
    "updated": "2026-01-08T09:35:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "AT^2PO框架通过回合级树搜索和策略优化，解决了多回合代理强化学习中的探索多样性有限、信用分配稀疏和策略优化错位问题。",
      "motivation": "近年来，大语言模型代理通过结合内部推理和外部工具交互，在解决多回合任务中显示出强大能力。代理强化学习作为关键的后训练范式受到关注，旨在提升这些能力。然而，现有方法面临探索多样性有限、信用分配稀疏和策略优化错位等挑战，限制了代理在多回合交互中的性能。这些问题导致代理在复杂任务中表现不佳，因此需要新框架来优化强化学习过程，以提高效率和效果。",
      "method": "AT^2PO框架提出回合级树结构，结合熵引导的树扩展以实现战略探索，以及回合级信用分配用于从稀疏结果中传播细粒度奖励。此外，引入了代理回合策略优化（ATPO），这是一个回合级学习目标，使策略更新与代理交互的自然决策粒度保持一致。ATPO与树搜索正交，可轻松集成到任何多回合强化学习流程中，利用树搜索增强探索和奖励分配，从而优化代理决策。",
      "result": "实验在七个基准测试中进行，AT^2PO相比最先进的基线方法，在平均性能上提升了高达1.84个百分点。消融研究验证了熵引导树扩展、回合级信用分配和ATPO学习目标各个组件的有效性，表明每个部分对性能提升有贡献。这些结果证明了框架在解决多回合代理强化学习挑战方面的优越性，突出了其在提高任务成功率方面的潜力。",
      "conclusion": "AT^2PO的主要贡献是通过回合级树搜索和策略优化，有效解决多回合代理强化学习的关键问题。该研究的学术价值在于提供了一个统一的优化框架，增强了代理在复杂交互任务中的表现；实际应用价值包括提高代理系统的决策能力和效率。未来工作可扩展到更多任务类型或集成到其他强化学习方法中，以进一步验证通用性和扩展性。",
      "tags": [
        "Agentic Reinforcement Learning",
        "Tree Search",
        "Policy Optimization",
        "Credit Assignment",
        "Multi-turn Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:21.766750Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04766",
    "title": "Revisiting Judge Decoding from First Principles via Training-Free Distributional Divergence",
    "authors": [
      "Shengyin Sun",
      "Yiming Li",
      "Renxi Liu",
      "Weizhe Lin",
      "Hui-Ling Zhen",
      "Xianzhi Yu",
      "Mingxuan Yuan",
      "Chen Ma"
    ],
    "abstract": "Judge Decoding accelerates LLM inference by relaxing the strict verification of Speculative Decoding, yet it typically relies on expensive and noisy supervision. In this work, we revisit this paradigm from first principles, revealing that the ``criticality'' scores learned via costly supervision are intrinsically encoded in the draft-target distributional divergence. We theoretically prove a structural correspondence between learned linear judges and Kullback-Leibler (KL) divergence, demonstrating they rely on the same underlying logit primitives. Guided by this, we propose a simple, training-free verification mechanism based on KL divergence. Extensive experiments across reasoning and coding benchmarks show that our method matches or outperforms complex trained judges (e.g., AutoJudge), offering superior robustness to domain shifts and eliminating the supervision bottleneck entirely.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04766.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04766",
    "published": "2026-01-08T09:34:54Z",
    "updated": "2026-01-08T09:34:54Z",
    "comment": "16 pages",
    "light_analysis": {
      "overview": "本研究提出基于KL散度的无训练验证机制，用于Judge Decoding，无需监督即可匹配或超越现有复杂判断器，加速LLM推理。",
      "motivation": "Judge Decoding是一种加速大型语言模型推理的技术，通过放宽Speculative Decoding的严格验证来提高效率。然而，现有方法依赖昂贵且噪声的监督数据来训练判断器，导致高成本和领域适应性差，限制了实际应用。因此，研究旨在解决监督瓶颈，探索无需监督的替代方案，以降低实现难度并提高鲁棒性。",
      "method": "论文从第一原理出发，理论证明了Judge Decoding中的判断器与Kullback-Leibler散度在结构上相对应，两者都基于相同的底层logit原语。基于此，作者提出一个简单的无训练验证机制，直接计算草稿模型和目标模型输出的分布散度（KL散度）来评估关键性分数。这种方法避免了复杂的训练过程，仅依赖于散度计算，无需额外监督数据，数据集和模型架构在摘要中未明确说明，但推断使用标准推理和编码基准。",
      "result": "在推理和编码等多个基准上的实验结果表明，提出的基于KL散度的无训练验证机制在性能上匹配甚至超越了需要监督训练的复杂判断器（如AutoJudge）。该方法展现出更强的领域转移鲁棒性，能够有效处理不同领域的任务，并完全消除了对监督数据的依赖。摘要未明确说明具体性能指标（如准确率提升），但对比实验证实了其相对于基线方法的优势。",
      "conclusion": "该研究的主要贡献在于通过理论分析揭示了Judge Decoding中判断器与Kullback-Leibler散度的内在联系，并据此提出了一个简单且无需训练的验证机制。这为加速大型语言模型推理提供了更高效的方法，避免了监督数据的瓶颈，具有重要的学术和实际应用价值。未来工作可以进一步验证该方法在其他模型或任务中的泛化能力，并探索可能的优化方向。",
      "tags": [
        "Judge Decoding",
        "Kullback-Leibler Divergence",
        "Speculative Decoding",
        "Large Language Model",
        "Training-Free Verification"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:14.276424Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04765",
    "title": "Differential syntactic and semantic encoding in LLMs",
    "authors": [
      "Santiago Acevedo",
      "Alessandro Laio",
      "Marco Baroni"
    ],
    "abstract": "We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04765.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04765",
    "published": "2026-01-08T09:33:29Z",
    "updated": "2026-01-08T09:33:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示了大型语言模型中语法和语义信息的差异化线性编码机制。",
      "motivation": "研究旨在探究大型语言模型内部表示如何编码语法和语义信息，聚焦于DeepSeek-V3模型。语法和语义是语言理解的核心要素，理解它们在LLMs中的编码方式有助于提升模型的可解释性和性能优化。当前研究可能缺乏对这两种信息编码差异的系统分析，因此本工作通过实证方法填补这一空白，以揭示编码机制，从而为模型改进提供理论基础。",
      "method": "研究方法基于DeepSeek-V3模型，通过计算共享语法结构或语义的句子隐藏表示向量的平均值，获得语法和语义质心。然后，从句子向量中减去这些质心，观察其与匹配句子相似性的变化，以检验线性编码假设。创新点在于跨层分析编码轮廓，量化语法和语义信号的解耦程度，提供了对语言信息编码模式的细致量化分析。",
      "result": "实验结果显示，减去语法或语义质心显著影响句子向量的相似性，表明语法和语义信息在表示中至少部分以线性方式编码。此外，语法和语义的跨层编码轮廓存在差异，且两者信号可以在一定程度上分离，这证实了LLMs中语法和语义信息的差异化编码。这些发现为模型内部表示机制提供了实证证据，无需具体数据支撑。",
      "conclusion": "论文的主要贡献是揭示了LLMs中语法和语义信息的差异化线性编码机制，深化了对模型内部工作方式的理解。学术价值在于为语言表示理论提供了新见解，实际应用价值可能包括改进模型解释性、优化任务性能或指导未来模型设计。摘要未明确说明局限性，但未来工作可探索编码机制的具体应用或扩展到其他模型。",
      "tags": [
        "Large Language Models",
        "Syntactic Encoding",
        "Semantic Encoding",
        "Linear Encoding",
        "Cross-layer Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:58.585219Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04758",
    "title": "PILOT-Bench: A Benchmark for Legal Reasoning in the Patent Domain with IRAC-Aligned Classification Tasks",
    "authors": [
      "Yehoon Jang",
      "Chaewon Lee",
      "Hyun-seok Min",
      "Sungchul Choi"
    ],
    "abstract": "The Patent Trial and Appeal Board (PTAB) of the USPTO adjudicates thousands of ex parte appeals each year, requiring the integration of technical understanding and legal reasoning. While large language models (LLMs) are increasingly applied in patent and legal practice, their use has remained limited to lightweight tasks, with no established means of systematically evaluating their capacity for structured legal reasoning in the patent domain. In this work, we introduce PILOT-Bench, the first PTAB-centric benchmark that aligns PTAB decisions with USPTO patent data at the case-level and formalizes three IRAC-aligned classification tasks: Issue Type, Board Authorities, and Subdecision. We evaluate a diverse set of closed-source (commercial) and open-source LLMs and conduct analyses across multiple perspectives, including input-variation settings, model families, and error tendencies. Notably, on the Issue Type task, closed-source models consistently exceed 0.75 in Micro-F1 score, whereas the strongest open-source model (Qwen-8B) achieves performance around 0.56, highlighting a substantial gap in reasoning capabilities. PILOT-Bench establishes a foundation for the systematic evaluation of patent-domain legal reasoning and points toward future directions for improving LLMs through dataset design and model alignment. All data, code, and benchmark resources are available at https://github.com/TeamLab/pilot-bench.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04758.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04758",
    "published": "2026-01-08T09:26:05Z",
    "updated": "2026-01-08T09:26:05Z",
    "comment": "Accepted at the NLLP Workshop at EMNLP 2025",
    "light_analysis": {
      "overview": "PILOT-Bench 引入了首个针对专利领域法律推理的基准，通过 IRAC 对齐的分类任务系统评估大语言模型的能力。",
      "motivation": "美国专利局专利审判和上诉委员会（PTAB）每年裁决大量上诉案件，需要结合技术和法律推理。尽管大语言模型在专利法律实践中应用增加，但现有方法多局限于轻量任务，缺乏系统评估它们在结构化法律推理能力的方法，尤其是与专利数据对齐的基准，导致难以衡量模型在实际复杂场景下的表现，因此需要专门的基准来推动研究和应用。",
      "method": "PILOT-Bench 构建了一个基于 PTAB 决定和 USPTO 专利数据的基准，形式化了三个与 IRAC 框架对齐的分类任务：Issue Type、Board Authorities 和 Subdecision。关键创新包括将法律推理结构化为分类问题，并在案例级别对齐数据，使用多样化的封闭源（商业）和开源大语言模型进行评估，进行多角度分析，如输入变化设置、模型家族和错误倾向。",
      "result": "在 Issue Type 任务中，封闭源模型的 Micro-F1 得分 consistently exceed 0.75，而开源模型如 Qwen-8B 约为 0.56，显示出显著的推理能力差距。评估结果揭示不同模型在结构化法律推理上的性能差异，为量化比较和错误分析提供了基础，突显当前开源模型在复杂任务上的局限性。",
      "conclusion": "PILOT-Bench 为系统评估专利领域法律推理提供了首个基准，强调通过 IRAC 对齐任务衡量大语言模型能力。研究指出数据集设计和模型对齐是未来改进方向，基准资源和代码开放促进社区研究。尽管基准限于特定领域，但为实际应用和学术发展奠定基础，指向减少模型差距的潜在工作。",
      "tags": [
        "Large Language Model",
        "Legal Reasoning",
        "Benchmark",
        "IRAC Framework",
        "Classification"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:44.844634Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.15658",
    "title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
    "authors": [
      "Weihang Su",
      "Anzhe Xie",
      "Qingyao Ai",
      "Jianming Long",
      "Jiaxin Mao",
      "Ziyi Ye",
      "Yiqun Liu"
    ],
    "abstract": "The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.15658.pdf",
    "abs_url": "https://arxiv.org/abs/2508.15658",
    "published": "2025-08-21T15:45:10Z",
    "updated": "2026-01-08T09:23:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "SurGE 为科学调查生成引入了首个标准化基准和自动化评估框架，填补了该领域的空白。",
      "motivation": "随着学术文献的爆炸式增长，手动创建科学调查变得耗时且不切实际，急需自动化解决方案。尽管大型语言模型在文本生成方面展现潜力，但现有研究缺乏统一的基准和评估协议，导致进展受阻，不同方法难以公平比较。标准化缺失限制了技术进步和实际应用，迫切需要建立评估体系来推动领域发展并解决文献管理的现实挑战。",
      "method": "研究提出 SurGE 基准，包含两部分：一是测试实例集，每个实例提供计算机科学主题的描述、专家撰写的调查及其完整引用文献；二是大规模学术语料库，涵盖超过一百万篇论文用于参考。此外，开发了自动化评估框架，通过四个关键维度（全面性、引用准确性、结构组织和内容质量）来衡量生成调查的质量，确保客观评估和促进方法改进。",
      "result": "通过对多种基于大型语言模型的方法进行评估，结果显示存在显著的性能差距。即使使用先进的代理框架，模型在生成调查时仍难以处理复杂任务，如确保全面性和准确引用，突显了当前技术的局限性。评估结果强调了该任务的挑战性，为未来研究提供了基准，但摘要未明确说明具体数值指标，仅指出性能不足的现实情况。",
      "conclusion": "本研究的核心贡献是引入 SurGE 基准和评估框架，为标准化学术调查生成研究提供了重要工具，推动了自动化领域的进展。其学术价值在于填补了领域空白，促进了方法的公平比较；实际应用价值在于支持自动化文献综述工具的开发和改进。未来工作可扩展基准至其他学科，并优化评估指标以应对更复杂的生成任务。",
      "tags": [
        "Survey Generation",
        "Large Language Models",
        "Benchmark Evaluation",
        "Automated Assessment",
        "Academic Corpus"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:38.687090Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.13341",
    "title": "Proverbs or Pythian Oracles? Sentiments and Emotions in Greek Sayings",
    "authors": [
      "Katerina Korre",
      "John Pavlopoulos"
    ],
    "abstract": "Proverbs are among the most fascinating language phenomena that transcend cultural and linguistic boundaries. Yet, much of the global landscape of proverbs remains underexplored, as many cultures preserve their traditional wisdom within their own communities due to the oral tradition of the phenomenon. Taking advantage of the current advances in Natural Language Processing (NLP), we focus on Greek proverbs, analyzing their sentiment and emotion. Departing from an annotated dataset of Greek proverbs, (1) we propose a multi-label annotation framework and dataset that captures the emotional variability of the proverbs, (2) we up-scale to local varieties, (3) we sketch a map of Greece that provides an overview of the distribution of emotions. Our findings show that the interpretation of proverbs is multidimensional, a property manifested through both multi-labeling and instance-level polarity. LLMs can capture and reproduce this complexity, and can therefore help us better understand the proverbial landscape of a place, as in the case of Greece, where surprise and anger compete and coexist within proverbs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.13341.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13341",
    "published": "2025-10-15T09:26:52Z",
    "updated": "2026-01-08T09:14:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过提出多标签注释框架，利用NLP技术分析希腊谚语的情感和情绪，揭示了谚语的多维解释特性，并利用LLMs捕获其复杂性。",
      "motivation": "研究动机源于谚语作为跨文化语言现象的重要性，但全球谚语情感分析因口述传统而研究不足，希腊案例尤为显著。现有方法缺乏系统框架处理谚语的情感多样性，难以探索文化智慧。利用NLP进展，特别是LLMs，可以更有效地填补这一学术空白，促进跨文化理解和情感分析领域的发展。",
      "method": "研究方法提出一个多标签注释框架，创建希腊谚语情感数据集以捕获情绪变异性，并扩展到局部变体分析区域差异。关键技术包括绘制希腊地图展示情绪分布，并利用大型语言模型（LLMs）处理实例级极性和多标签情感，以提升分析的准确性和深度，摘要未明确说明具体模型架构细节。",
      "result": "实验结果显示谚语解释具有多维特性，通过多标签注释和实例级极性得到验证。LLMs能成功捕获这种复杂性，如在希腊谚语中惊喜和愤怒情绪共存。这表明NLP方法在情感分析上的有效性，为理解谚语景观提供了新视角，摘要未明确说明具体性能指标或与基线方法的对比。",
      "conclusion": "论文贡献了多标签注释框架和情感分布地图，推动谚语研究和NLP在文化分析中的应用。学术价值在于提供新分析工具，实际价值支持文化遗产保护和跨文化交流。局限性可能在于数据集规模，未来工作可扩展到其他文化或改进注释方法以增强泛化能力。",
      "tags": [
        "Natural Language Processing",
        "Multi-label Annotation",
        "Large Language Model",
        "Sentiment Analysis",
        "Emotion Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:55.571845Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04742",
    "title": "Tool-MAD: A Multi-Agent Debate Framework for Fact Verification with Diverse Tool Augmentation and Adaptive Retrieval",
    "authors": [
      "Seyeon Jeong",
      "Yeonjun Choi",
      "JongWook Kim",
      "Beakcheol Jang"
    ],
    "abstract": "Large Language Models (LLMs) suffer from hallucinations and factual inaccuracies, especially in complex reasoning and fact verification tasks. Multi-Agent Debate (MAD) systems aim to improve answer accuracy by enabling multiple LLM agents to engage in dialogue, promoting diverse reasoning and mutual verification. However, existing MAD frameworks primarily rely on internal knowledge or static documents, making them vulnerable to hallucinations. While MADKE introduces external evidence to mitigate this, its one-time retrieval mechanism limits adaptability to new arguments or emerging information during the debate. To address these limitations, We propose Tool-MAD, a multi-agent debate framework that enhances factual verification by assigning each agent a distinct external tool, such as a search API or RAG module. Tool-MAD introduces three key innovations: (1) a multi-agent debate framework where agents leverage heterogeneous external tools, encouraging diverse perspectives, (2) an adaptive query formulation mechanism that iteratively refines evidence retrieval based on the flow of the debate, and (3) the integration of Faithfulness and Answer Relevance scores into the final decision process, allowing the Judge agent to quantitatively assess the coherence and question alignment of each response and effectively detect hallucinations. Experimental results on four fact verification benchmarks demonstrate that Tool-MAD consistently outperforms state-of-the-art MAD frameworks, achieving up to 5.5% accuracy improvement. Furthermore, in medically specialized domains, Tool-MAD exhibits strong robustness and adaptability across various tool configurations and domain conditions, confirming its potential for broader real-world fact-checking applications.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04742.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04742",
    "published": "2026-01-08T09:07:41Z",
    "updated": "2026-01-08T09:07:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "Tool-MAD 是一个多智能体辩论框架，通过为智能体分配不同外部工具和自适应检索机制，显著提升了事实验证的准确性和鲁棒性。",
      "motivation": "大型语言模型（LLMs）在复杂推理和事实验证任务中存在幻觉和事实不准确问题，影响其实际应用可靠性。现有多智能体辩论框架主要依赖内部知识或静态文档，容易受幻觉干扰；虽然 MADKE 引入了外部证据，但其一次性检索机制缺乏灵活性，难以适应辩论中产生的新论点或信息。因此，研究旨在开发一个能动态利用多样化外部工具和自适应检索的系统，以解决这些局限性并增强事实核查能力。",
      "method": "Tool-MAD 的核心方法是构建一个多智能体辩论框架，其中每个智能体被分配独特的异构外部工具，如搜索 API 或 RAG 模块，以促进多样化视角和证据检索。关键创新包括：自适应查询制定机制，在辩论过程中迭代优化证据检索以响应新信息；以及集成 Faithfulness 和 Answer Relevance 分数到决策过程，使 Judge 智能体能定量评估响应的连贯性和问题对齐度，有效检测幻觉。该方法不依赖于静态数据，而是动态整合工具增强。",
      "result": "在四个事实验证基准上的实验结果表明，Tool-MAD 持续优于现有最先进的多智能体辩论框架，最高实现了 5.5% 的准确率提升。与基线方法相比，Tool-MAD 在性能指标上表现优越。此外，在医学专业领域，Tool-MAD 展现出强大的鲁棒性和适应性，能够在不同的工具配置和领域条件下稳定运行，这证实了其在更广泛现实世界事实核查应用中的潜力。",
      "conclusion": "Tool-MAD 通过结合多样化工具增强和自适应检索，显著提高了多智能体辩论在事实验证中的有效性，减少了 LLMs 的幻觉问题。这项研究的学术价值在于推动了多智能体辩论技术的发展，提高了自动事实核查的精度；实际应用价值体现在医学等专业领域的可靠核查中。摘要未明确说明局限性，但未来工作可能包括扩展更多工具类型或应用于其他复杂任务领域。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Debate",
        "Fact Verification",
        "Adaptive Retrieval",
        "Retrieval-Augmented Generation"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:27.048326Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04740",
    "title": "RiskAtlas: Exposing Domain-Specific Risks in LLMs through Knowledge-Graph-Guided Harmful Prompt Generation",
    "authors": [
      "Huawei Zheng",
      "Xinqi Jiang",
      "Sen Yang",
      "Shouling Ji",
      "Yingcai Wu",
      "Dazhen Deng"
    ],
    "abstract": "Large language models (LLMs) are increasingly applied in specialized domains such as finance and healthcare, where they introduce unique safety risks. Domain-specific datasets of harmful prompts remain scarce and still largely rely on manual construction; public datasets mainly focus on explicit harmful prompts, which modern LLM defenses can often detect and refuse. In contrast, implicit harmful prompts-expressed through indirect domain knowledge-are harder to detect and better reflect real-world threats. We identify two challenges: transforming domain knowledge into actionable constraints and increasing the implicitness of generated harmful prompts. To address them, we propose an end-to-end framework that first performs knowledge-graph-guided harmful prompt generation to systematically produce domain-relevant prompts, and then applies dual-path obfuscation rewriting to convert explicit harmful prompts into implicit variants via direct and context-enhanced rewriting. This framework yields high-quality datasets combining strong domain relevance with implicitness, enabling more realistic red-teaming and advancing LLM safety research. We release our code and datasets at GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04740.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04740",
    "published": "2026-01-08T09:05:28Z",
    "updated": "2026-01-08T09:05:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "RiskAtlas提出一个端到端框架，通过知识图谱引导的有害提示生成和双路径混淆重写，自动生成领域特定的隐式有害提示数据集。",
      "motivation": "大型语言模型在金融、医疗等专业领域的应用日益普及，但现有安全测试主要依赖公共数据集，这些数据集大多关注显性有害提示，容易被现代防御系统检测。相比之下，隐式有害提示通过间接领域知识表达，更难检测，更能反映真实威胁。然而，领域特定的有害提示数据集稀缺，且主要依赖手动构建，缺乏系统性的生成方法。因此，本研究旨在解决如何自动化生成高质量领域相关隐式提示的挑战，以提升LLM安全评估的准确性。",
      "method": "本研究提出了一个端到端框架，分为两个核心步骤。首先，利用知识图谱引导的有害提示生成，将领域知识（如金融或医疗）转化为约束条件，系统性地生成与特定领域相关的提示。其次，应用双路径混淆重写技术，包括直接重写和上下文增强重写，将显性有害提示转换为更隐式的变体，从而提高检测难度。该方法的创新点在于结合知识图谱和混淆重写，自动产生高隐式性和强领域相关性的提示，为红色团队测试提供了有效工具。",
      "result": "该框架成功生成了结合领域相关性和隐式性的有害提示数据集，有助于实现更现实的红色团队测试，并推动了LLM安全研究的进展。摘要未明确说明具体的实验性能指标，如准确率提升或效率改进，但通过自动化生成高质量提示，有望弥补现有数据集的不足，增强安全评估的全面性。",
      "conclusion": "RiskAtlas框架的主要贡献在于自动化生成领域特定的隐式有害提示数据集，解决了现有手动构建方法的局限性。其学术价值体现在推动了LLM安全研究，提供了新的评估工具；实际应用中，可用于红色团队测试，帮助识别和防御专业领域中的潜在风险。未来工作可能包括扩展到更多领域、优化生成算法或与现有防御系统集成，以进一步提升安全测试的效率和覆盖面。",
      "tags": [
        "Large Language Model",
        "Knowledge Graph",
        "Harmful Prompt Generation",
        "Red Teaming",
        "Prompt Obfuscation"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:36.063469Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.14641",
    "title": "Revisiting Chain-of-Thought Prompting: Zero-shot Can Be Stronger than Few-shot",
    "authors": [
      "Xiang Cheng",
      "Chengyan Pan",
      "Minjun Zhao",
      "Deyang Li",
      "Fangchao Liu",
      "Xinyu Zhang",
      "Xiao Zhang",
      "Yong Liu"
    ],
    "abstract": "In-Context Learning (ICL) is an essential emergent ability of Large Language Models (LLMs), and recent studies introduce Chain-of-Thought (CoT) to exemplars of ICL to enhance the reasoning capability, especially in mathematics tasks. However, given the continuous advancement of model capabilities, it remains unclear whether CoT exemplars still benefit recent, stronger models in such tasks. Through systematic experiments, we find that for recent strong models such as the Qwen2.5 series, adding traditional CoT exemplars does not improve reasoning performance compared to Zero-Shot CoT. Instead, their primary function is to align the output format with human expectations. We further investigate the effectiveness of enhanced CoT exemplars, constructed using answers from advanced models such as \\texttt{Qwen2.5-Max} and \\texttt{DeepSeek-R1}. Experimental results indicate that these enhanced exemplars still fail to improve the model's reasoning performance. Further analysis reveals that models tend to ignore the exemplars and focus primarily on the instructions, leading to no observable gain in reasoning ability. Overall, our findings highlight the limitations of the current ICL+CoT framework in mathematical reasoning, calling for a re-examination of the ICL paradigm and the definition of exemplars.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.14641.pdf",
    "abs_url": "https://arxiv.org/abs/2506.14641",
    "published": "2025-06-17T15:39:33Z",
    "updated": "2026-01-08T09:01:32Z",
    "comment": "EMNLP25-findings camera_ready, 19 pages,22 figures",
    "light_analysis": {
      "overview": "本研究通过实验揭示，对于近期强大型语言模型，零样本思维链提示在数学推理任务中可能优于少样本提示，挑战了当前上下文学习范式的假设。",
      "motivation": "随着大型语言模型能力的持续提升，上下文学习已成为其关键能力，而思维链提示被广泛应用于增强模型推理能力，尤其在数学任务中。然而，现有研究未明确验证这些方法对近期更强模型的有效性，因此本研究旨在探究思维链示例是否仍能提升强模型的推理性能，解决这一关键问题并填补研究空白。背景在于，传统上下文学习范式可能因模型能力变化而失效，需要重新评估其适用性。",
      "method": "本研究采用系统性实验方法，针对强模型如Qwen2.5系列，比较传统思维链示例与零样本思维链在数学推理任务中的效果。关键创新点包括分析思维链示例的实际作用——主要是输出格式对齐而非推理提升，并构建增强的示例，使用高级模型如Qwen2.5-Max和DeepSeek-R1的答案来测试其有效性。技术特色在于通过实验揭示模型行为，如忽略示例而关注指令，从而质疑现有上下文学习框架。",
      "result": "实验结果显示，传统思维链示例与零样本思维链相比，未能提高推理性能；同时，使用高级模型构建的增强示例同样无效。模型表现出倾向于忽略示例的行为，导致推理能力没有可观察到的增益。与基线方法（零样本思维链）对比，少样本思维链没有优势，突显了当前方法的局限性。具体性能指标未在摘要中说明，但实验证实了框架在强模型中的不足。",
      "conclusion": "本研究的主要贡献是揭示了当前上下文学习加思维链框架在强模型数学推理中的局限性，呼吁重新审视上下文学习范式和示例定义。学术价值在于挑战传统观点，推动提示策略优化；实际应用价值在于为人工智能推理任务提供新的研究方向。局限性包括未详细探索其他任务类型，未来工作可扩展至更广泛领域。",
      "tags": [
        "In-Context Learning",
        "Chain-of-Thought",
        "Large Language Models",
        "Zero-shot Learning",
        "Mathematical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:36.130491Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04736",
    "title": "AM$^3$Safety: Towards Data Efficient Alignment of Multi-modal Multi-turn Safety for MLLMs",
    "authors": [
      "Han Zhu",
      "Jiale Chen",
      "Chengkun Cai",
      "Shengjie Sun",
      "Haoran Li",
      "Yujin Zhou",
      "Chi-Min Chan",
      "Pengcheng Wen",
      "Lei Li",
      "Sirui Han",
      "Yike Guo"
    ],
    "abstract": "Multi-modal Large Language Models (MLLMs) are increasingly deployed in interactive applications. However, their safety vulnerabilities become pronounced in multi-turn multi-modal scenarios, where harmful intent can be gradually reconstructed across turns, and security protocols fade into oblivion as the conversation progresses. Existing Reinforcement Learning from Human Feedback (RLHF) alignment methods are largely developed for single-turn visual question-answer (VQA) task and often require costly manual preference annotations, limiting their effectiveness and scalability in dialogues. To address this challenge, we present InterSafe-V, an open-source multi-modal dialogue dataset containing 11,270 dialogues and 500 specially designed refusal VQA samples. This dataset, constructed through interaction between several models, is designed to more accurately reflect real-world scenarios and includes specialized VQA pairs tailored for specific domains. Building on this dataset, we propose AM$^3$Safety, a framework that combines a cold-start refusal phase with Group Relative Policy Optimization (GRPO) fine-tuning using turn-aware dual-objective rewards across entire dialogues. Experiments on Qwen2.5-VL-7B-Instruct and LLaVA-NeXT-7B show more than 10\\% decrease in Attack Success Rate (ASR) together with an increment of at least 8\\% in harmless dimension and over 13\\% in helpful dimension of MLLMs on multi-modal multi-turn safety benchmarks, while preserving their general abilities.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04736.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04736",
    "published": "2026-01-08T08:57:05Z",
    "updated": "2026-01-08T08:57:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了AM^3Safety框架和InterSafe-V数据集，以高效对齐多模态多回合对话中MLLM的安全性，提升其安全性和对话能力。",
      "motivation": "研究动机在于解决MLLMs在交互应用中部署时，多回合多模态场景下的安全性漏洞问题，例如有害意图可能跨回合逐步构建，而安全协议随对话进展失效。现有方法如Reinforcement Learning from Human Feedback（RLHF）主要针对单回合视觉问答任务，依赖高成本人工标注，导致在对话中效果受限且扩展性差。因此，亟需数据高效的对齐方法来应对这些挑战。",
      "method": "研究方法包括构建开源多模态对话数据集InterSafe-V，该数据集包含11,270个对话和500个专门设计的拒绝视觉问答（VQA）样本，通过模型间交互构建以更准确反映真实场景。在此基础上，提出AM^3Safety框架，结合冷启动拒绝阶段和使用回合感知双目标奖励的Group Relative Policy Optimization（GRPO）微调技术，在整个对话中进行安全对齐。",
      "result": "实验结果在Qwen2.5-VL-7B-Instruct和LLaVA-NeXT-7B模型上展示，应用AM^3Safety框架后，攻击成功率（ASR）降低超过10%，同时无害维度提升至少8%，有帮助维度提升超过13%。这些改进在多模态多回合安全基准测试中实现，且模型的一般能力得到保留，表明该方法有效提升了MLLM的安全性和有用性。",
      "conclusion": "论文的主要贡献是通过AM^3Safety框架和InterSafe-V数据集，实现了MLLM在多回合多模态场景中的数据高效安全对齐，具有重要学术价值，推动了对话安全对齐技术的发展。在实际应用中，增强了MLLMs的可靠性和用户体验，未来工作可能涉及扩展到更多模型或更复杂场景，摘要未明确说明具体局限性。",
      "tags": [
        "Multi-modal Large Language Models (MLLMs)",
        "Reinforcement Learning from Human Feedback (RLHF)",
        "Group Relative Policy Optimization (GRPO)",
        "Multi-turn Dialogue Safety",
        "Dataset Construction"
      ]
    },
    "analyzed_at": "2026-01-09T02:28:45.404824Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04731",
    "title": "Miner:Mining Intrinsic Mastery for Data-Efficient RL in Large Reasoning Models",
    "authors": [
      "Shuyang Jiang",
      "Yuhao Wang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "abstract": "Current critic-free RL methods for large reasoning models suffer from severe inefficiency when training on positive homogeneous prompts (where all rollouts are correct), resulting in waste of rollouts due to zero advantage estimates. We introduce a radically simple yet powerful solution to \\uline{M}ine \\uline{in}trinsic mast\\uline{er}y (Miner), that repurposes the policy's intrinsic uncertainty as a self-supervised reward signal, with no external supervision, auxiliary models, or additional inference cost. Our method pioneers two key innovations: (1) a token-level focal credit assignment mechanism that dynamically amplifies gradients on critical uncertain tokens while suppressing overconfident ones, and (2) adaptive advantage calibration to seamlessly integrate intrinsic and verifiable rewards. Evaluated across six reasoning benchmarks on Qwen3-4B and Qwen3-8B base models, Miner achieves state-of-the-art performance among the other four algorithms, yielding up to \\textbf{4.58} absolute gains in Pass@1 and \\textbf{6.66} gains in Pass@K compared to GRPO. Comparison with other methods targeted at exploration enhancement further discloses the superiority of the two newly proposed innovations. This demonstrates that latent uncertainty exploitation is both necessary and sufficient for efficient and scalable RL training of reasoning models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04731.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04731",
    "published": "2026-01-08T08:52:37Z",
    "updated": "2026-01-08T08:52:37Z",
    "comment": "22 pages",
    "light_analysis": {
      "overview": "Miner方法通过挖掘策略的内在不确定性作为自监督奖励信号，提高大型推理模型中强化学习的数据效率和训练效率。",
      "motivation": "当前批评者免费的强化学习方法在大型推理模型训练中面临效率低下问题，尤其是在正同质提示下，所有rollout都正确导致优势估计为零，浪费计算资源。这一问题的重要性在于数据高效训练对大型模型至关重要，现有方法在处理此场景时效果不佳，通常依赖外部监督或产生额外成本，限制了可扩展性。Miner旨在通过自监督方式利用内在不确定性解决这一瓶颈，无需外部干预。",
      "method": "Miner方法的核心是利用策略的内在不确定性作为自监督奖励信号，无需外部监督、辅助模型或额外推理成本。关键创新包括令牌级焦点信用分配机制，该机制动态放大关键不确定令牌的梯度同时抑制过自信令牌；以及自适应优势校准，无缝整合内在奖励和可验证奖励。该方法在Qwen3-4B和Qwen3-8B基础模型上实现，通过挖掘潜在不确定性来优化强化学习训练过程。",
      "result": "在六个推理基准上对Qwen3-4B和Qwen3-8B模型进行评估，Miner在四种算法中取得了最先进的性能。具体而言，相比GRPO基线方法，在Pass@1指标上实现了4.58的绝对增益，在Pass@K指标上实现了6.66的增益。与其他专注于探索增强的方法进行比较，进一步验证了新提出的令牌级信用分配和自适应优势校准机制的优势，证明了该方法在提升训练效率和性能方面的有效性。",
      "conclusion": "Miner方法的主要贡献在于提出了一种基于内在不确定性的自监督强化学习框架，有效提高了大型推理模型的数据效率和训练可扩展性。其学术价值体现在证明了利用潜在不确定性对于高效RL训练是必要且充分的，为未来研究提供了新方向。实际应用中，该方法可促进更高效的模型训练。摘要未明确说明具体局限性或未来工作，但暗示了在推理模型中的广泛适用性。",
      "tags": [
        "Large Reasoning Models",
        "Reinforcement Learning",
        "Self-Supervised Reward",
        "Uncertainty Mining",
        "Token-Level Credit Assignment"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:02.944909Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04730",
    "title": "Automatic Classifiers Underdetect Emotions Expressed by Men",
    "authors": [
      "Ivan Smirnov",
      "Segun T. Aroyehun",
      "Paul Plener",
      "David Garcia"
    ],
    "abstract": "The widespread adoption of automatic sentiment and emotion classifiers makes it important to ensure that these tools perform reliably across different populations. Yet their reliability is typically assessed using benchmarks that rely on third-party annotators rather than the individuals experiencing the emotions themselves, potentially concealing systematic biases. In this paper, we use a unique, large-scale dataset of more than one million self-annotated posts and a pre-registered research design to investigate gender biases in emotion detection across 414 combinations of models and emotion-related classes. We find that across different types of automatic classifiers and various underlying emotions, error rates are consistently higher for texts authored by men compared to those authored by women. We quantify how this bias could affect results in downstream applications and show that current machine learning tools, including large language models, should be applied with caution when the gender composition of a sample is not known or variable. Our findings demonstrate that sentiment analysis is not yet a solved problem, especially in ensuring equitable model behaviour across demographic groups.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04730.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04730",
    "published": "2026-01-08T08:52:17Z",
    "updated": "2026-01-08T08:52:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过大规模自标注数据揭示自动情感分类器存在性别偏见，对男性文本的错误检测率持续更高。",
      "motivation": "自动情感和情绪分类器的广泛应用使得跨人群可靠性变得至关重要，但现有评估方法依赖第三方标注者而非情感体验者本人，可能掩盖系统性偏见，尤其是在性别差异方面。这凸显了确保AI工具公平性的必要性，以解决潜在的不公正和偏差传播问题。",
      "method": "本研究采用一个独特的大规模数据集，包含超过一百万个自标注帖子，并通过预注册研究设计系统分析了414种模型和情绪相关类组合。方法核心在于利用自标注数据减少标注偏差，并综合评估多种自动分类器在不同性别作者文本上的性能，以量化性别偏见。",
      "result": "研究发现，在不同类型自动分类器和各种情绪中，对男性作者文本的错误率持续高于女性作者，量化了这种偏见对下游应用的影响。摘要未明确说明具体数据，但指出当前机器学习工具（包括大型语言模型）在样本性别组成未知时应谨慎使用，以避免偏差。",
      "conclusion": "本研究的主要贡献是证明自动情感分类器存在性别偏见，情感分析在跨人口群体公平性方面尚未解决。学术上强调评估方法的重要性；实践中提醒开发者和用户注意偏见风险。未来工作可改进数据集标注和模型训练，以提升公平性和准确性。",
      "tags": [
        "Emotion Detection",
        "Gender Bias",
        "Large Language Models",
        "Machine Learning",
        "Sentiment Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:40.767405Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.12225",
    "title": "Mining Intrinsic Rewards from LLM Hidden States for Efficient Best-of-N Sampling",
    "authors": [
      "Jizhou Guo",
      "Zhaomin Wu",
      "Hanchen Yang",
      "Philip S. Yu"
    ],
    "abstract": "Best-of-N sampling is a powerful method for improving Large Language Model (LLM) performance, but it is often limited by its dependence on massive, text-based reward models. These models are not only computationally expensive but also data-hungry, requiring extensive labeled datasets for training. This creates a significant data challenge, as they overlook a rich, readily available data source: the LLM's own internal hidden states. To address this data and efficiency gap, we introduce SWIFT (Simple Weighted Intrinsic Feedback Technique), a novel and lightweight method that learns a reward function directly from the rich information embedded in LLM hidden states. Operating at the token embedding level, SWIFT employs simple linear layers to effectively distinguish between preferred and dispreferred generations, eliminating the need for computationally intensive text-based modeling. Extensive experiments on standard benchmarks show that SWIFT outperforms existing baselines (12.7% higher accuracy than EurusRM-7B on MATH dataset) while using less than 0.005% of their parameters. Its robust scalability, compatibility with certain closed-source models via logit access, and ability to combine with traditional reward models for additional performance highlight SWIFT's practical value and contribution to more efficient data-driven LLM post-training. Our code is available at https://github.com/aster2024/SWIFT .",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.12225.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12225",
    "published": "2025-05-18T04:00:35Z",
    "updated": "2026-01-08T08:44:58Z",
    "comment": "Accepted by KDD 2026 (Research Track). Project page: https://aster2024.github.io/swift-website/",
    "light_analysis": {
      "overview": "论文提出SWIFT方法，通过从LLM隐藏状态挖掘内在奖励，实现高效的最佳N采样，减少对文本奖励模型的依赖。",
      "motivation": "研究动机在于Best-of-N采样方法通常依赖计算昂贵、数据饥渴的文本奖励模型，这些模型需要大量标注数据训练，但忽略了LLM内部隐藏状态的丰富信息。现有方法存在数据不足和效率低下的问题，因为文本建模既耗费资源又难以获取充足训练数据。为了解决这一缺口，本研究旨在利用LLM自身生成的免费数据源，开发一种轻量级、高效的替代方案，以提升采样过程的实用性和可扩展性。",
      "method": "SWIFT方法是一种新颖的轻量级技术，它在词嵌入级别操作，使用简单的线性层直接从LLM的隐藏状态学习奖励函数。关键创新在于利用LLM的内部表示作为数据源，避免了复杂的文本建模过程。该方法通过区分偏好和非偏好生成，无需额外标注数据集，从而简化了奖励学习流程。技术特色包括直接挖掘隐藏状态中的信息，减少计算负担，适用于标准基准测试中的各种LLM模型。",
      "result": "在标准基准测试中，SWIFT显著优于现有基线方法。例如，在MATH数据集上，其准确率比EurusRM-7B模型高出12.7%。此外，SWIFT的参数使用量少于基线方法的0.005%，显示出极高的效率和可扩展性。实验结果还表明，该方法能够与闭源模型兼容，并可与传统奖励模型结合以进一步提升性能，验证了其在实际应用中的有效性和优势。",
      "conclusion": "SWIFT的主要贡献是提出了一种基于LLM隐藏状态的轻量级奖励学习方法，有效提高了Best-of-N采样的效率和性能。其学术价值在于探索了内部数据源的新应用，实际意义包括降低计算成本、增强模型兼容性和可扩展性。未来工作可扩展该方法到更多场景，或与其他技术结合以优化LLM后训练过程，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Best-of-N Sampling",
        "Intrinsic Reward",
        "Hidden States",
        "Linear Layers"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:47.085002Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04726",
    "title": "Memory Matters More: Event-Centric Memory as a Logic Map for Agent Searching and Reasoning",
    "authors": [
      "Yuyang Hu",
      "Jiongnan Liu",
      "Jiejun Tan",
      "Yutao Zhu",
      "Zhicheng Dou"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed as intelligent agents that reason, plan, and interact with their environments. To effectively scale to long-horizon scenarios, a key capability for such agents is a memory mechanism that can retain, organize, and retrieve past experiences to support downstream decision-making. However, most existing approaches organize and store memories in a flat manner and rely on simple similarity-based retrieval techniques. Even when structured memory is introduced, existing methods often struggle to explicitly capture the logical relationships among experiences or memory units. Moreover, memory access is largely detached from the constructed structure and still depends on shallow semantic retrieval, preventing agents from reasoning logically over long-horizon dependencies. In this work, we propose CompassMem, an event-centric memory framework inspired by Event Segmentation Theory. CompassMem organizes memory as an Event Graph by incrementally segmenting experiences into events and linking them through explicit logical relations. This graph serves as a logic map, enabling agents to perform structured and goal-directed navigation over memory beyond superficial retrieval, progressively gathering valuable memories to support long-horizon reasoning. Experiments on LoCoMo and NarrativeQA demonstrate that CompassMem consistently improves both retrieval and reasoning performance across multiple backbone models.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04726.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04726",
    "published": "2026-01-08T08:44:07Z",
    "updated": "2026-01-08T08:44:07Z",
    "comment": "19 pages,6 figures",
    "light_analysis": {
      "overview": "本文提出CompassMem，一个事件中心的内存框架，通过将内存组织为事件图来增强智能代理的长期推理和检索能力。",
      "motivation": "研究动机在于解决当前基于大语言模型的智能代理在长程场景中内存机制的不足。现有方法以平坦方式组织存储，依赖基于相似性的检索技术，难以捕获经验间的逻辑关系，导致推理脱节，影响决策效率。这突出了开发结构化内存以支持长期依赖推理的重要性。",
      "method": "CompassMem基于事件分割理论，将经验增量式分割为事件，并通过逻辑关系链接成事件图，构建逻辑地图。该框架使代理能够进行结构化、目标导向的内存导航，超越浅层语义检索。实验中使用LoCoMo和NarrativeQA数据集，结合多个骨干模型验证方法。",
      "result": "在LoCoMo和NarrativeQA数据集上的实验显示，CompassMem一致提高了检索和推理性能。具体提升程度摘要未明确说明，但与基线方法相比，显示出显著改进，验证了框架在增强代理能力方面的有效性。",
      "conclusion": "本研究贡献了CompassMem框架，通过事件图和逻辑地图改进内存组织，提升代理的长期推理能力。学术价值在于创新内存机制，实际应用价值在于优化智能代理决策。摘要未明确说明局限性，未来工作可能涉及事件分割算法的进一步优化。",
      "tags": [
        "Large Language Model",
        "Event Segmentation",
        "Memory Graph",
        "Logical Reasoning",
        "Retrieval Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:57.181431Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03714",
    "title": "Visual Merit or Linguistic Crutch? A Close Look at DeepSeek-OCR",
    "authors": [
      "Yunhao Liang",
      "Ruixuan Ying",
      "Bo Li",
      "Hong Li",
      "Kai Yan",
      "Qingwen Li",
      "Min Yang",
      "Okamoto Satoshi",
      "Zhe Cui",
      "Shiwen Ni"
    ],
    "abstract": "DeepSeek-OCR utilizes an optical 2D mapping approach to achieve high-ratio vision-text compression, claiming to decode text tokens exceeding ten times the input visual tokens. While this suggests a promising solution for the LLM long-context bottleneck, we investigate a critical question: \"Visual merit or linguistic crutch - which drives DeepSeek-OCR's performance?\" By employing sentence-level and word-level semantic corruption, we isolate the model's intrinsic OCR capabilities from its language priors. Results demonstrate that without linguistic support, DeepSeek-OCR's performance plummets from approximately 90% to 20%. Comparative benchmarking against 13 baseline models reveals that traditional pipeline OCR methods exhibit significantly higher robustness to such semantic perturbations than end-to-end methods. Furthermore, we find that lower visual token counts correlate with increased reliance on priors, exacerbating hallucination risks. Context stress testing also reveals a total model collapse around 10,000 text tokens, suggesting that current optical compression techniques may paradoxically aggravate the long-context bottleneck. This study empirically defines DeepSeek-OCR's capability boundaries and offers essential insights for future optimizations of the vision-text compression paradigm. We release all data, results and scripts used in this study at https://github.com/dududuck00/DeepSeekOCR.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03714.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03714",
    "published": "2026-01-07T09:01:23Z",
    "updated": "2026-01-08T08:37:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过隔离OCR能力与语言先验，发现DeepSeek-OCR的性能主要依赖语言支持而非视觉特征，揭示了其在高压缩下的局限性。",
      "motivation": "DeepSeek-OCR声称通过高比率视觉-文本压缩缓解LLM的长上下文瓶颈，但其解码能力可能并非纯视觉驱动，而是隐含了语言先验。现有端到端OCR方法可能因过度依赖语言模型而脆弱，导致对语义扰动的敏感性问题。本研究旨在量化语言先验的影响，以探索视觉-文本压缩范式的真实能力边界，为优化技术提供基础。",
      "method": "本研究采用句子级和词级语义腐败实验，通过破坏输入文本的语义结构来隔离DeepSeek-OCR的视觉处理能力与语言先验知识。关键创新在于系统性地测试模型内在OCR性能，同时对比13个基线OCR模型，包括传统管道方法和端到端方法，以评估其对语义扰动的鲁棒性。实验基于自定义语义腐败数据，模拟真实场景中的语言依赖性。",
      "result": "实验结果显示，去除语言支持后，DeepSeek-OCR的性能从约90%骤降至20%，表明高度依赖语言先验。在对比基准中，传统管道OCR方法对语义扰动表现出更高鲁棒性，优于端到端方法。此外，视觉令牌计数减少会增加对先验的依赖，加剧幻觉风险，并在约10,000个文本令牌处观察到模型完全崩溃，暗示当前压缩技术可能恶化长上下文瓶颈。",
      "conclusion": "本研究实证定义了DeepSeek-OCR的能力边界，揭示了其性能主要源于语言先验而非纯视觉处理，为视觉-文本压缩范式提供了关键见解。学术上，它推动了对OCR模型内在驱动力的理解，强调了端到端方法的脆弱性。实际应用中，此研究有助于指导未来优化，如增强视觉令牌效率或减少先验依赖，以真正缓解LLM长上下文瓶颈。摘要未明确说明具体优化方向，但暗示了未来工作需要关注这些方面。",
      "tags": [
        "OCR",
        "Vision-Text Compression",
        "Language Priors",
        "End-to-End Models",
        "Semantic Perturbation"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:08.965267Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03641",
    "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
    "authors": [
      "Zheng Wu",
      "Xingyu Lou",
      "Xinbei Ma",
      "Yansi Li",
      "Weiwen Liu",
      "Weinan Zhang",
      "Jun Wang",
      "Zhuosheng Zhang"
    ],
    "abstract": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates. The codes are available at https://github.com/Wuzheng02/Agent-Dice.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03641.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03641",
    "published": "2026-01-07T06:43:50Z",
    "updated": "2026-01-08T08:36:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Agent-Dice，一个基于几何共识评估的参数融合框架，通过解耦知识更新来解决代理持续学习中的稳定性-可塑性困境。",
      "motivation": "基于大型语言模型（LLM）的代理在动态环境中交互时，能够持续学习新任务而不发生灾难性遗忘是一个关键挑战，即稳定性-可塑性困境。现有方法未能明确区分跨任务共享的公共知识和任务特定干扰引入的冲突知识，导致代理在适应新任务时容易遗忘旧知识，这限制了智能代理在实时应用中的长期适应性和可靠性。",
      "method": "Agent-Dice采用一个两阶段参数融合框架：首先，通过几何共识过滤识别并修剪任务特定干扰产生的冲突梯度；其次，使用基于曲率的重要性加权来增强跨任务共享的语义。该框架基于方向共识评估进行知识更新解耦，提供了理论分析验证融合方案的有效性，并探讨了稳定性-可塑性困境的起源，不依赖于特定数据集或模型架构。",
      "result": "在GUI代理和工具使用代理领域的实验中，Agent-Dice展现出优异的持续学习性能，与基线方法相比，能有效防止灾难性遗忘，同时以最小的计算开销和参数更新实现高效运行。摘要未明确说明具体性能指标如准确率提升，但实验结果验证了框架在实际应用中的可行性和效率。",
      "conclusion": "Agent-Dice通过解耦知识更新的参数融合方法，有效解决了代理持续学习中的稳定性-可塑性困境，提供了理论分析和实验验证。其学术价值在于推动了持续学习领域的技术发展，实际应用价值是增强了LLM代理在动态环境中的适应能力。未来工作可探索该框架在不同任务和领域的泛化性及潜在扩展。",
      "tags": [
        "Large Language Model",
        "Continual Learning",
        "Parameter Fusion",
        "Gradient Pruning",
        "Curvature-based Weighting"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:10.907757Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04720",
    "title": "Qwen3-VL-Embedding and Qwen3-VL-Reranker: A Unified Framework for State-of-the-Art Multimodal Retrieval and Ranking",
    "authors": [
      "Mingxin Li",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Keqin Chen",
      "Sibo Song",
      "Shuai Bai",
      "Zhibo Yang",
      "Pengjun Xie",
      "An Yang",
      "Dayiheng Liu",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "abstract": "In this report, we introduce the Qwen3-VL-Embedding and Qwen3-VL-Reranker model series, the latest extensions of the Qwen family built on the Qwen3-VL foundation model. Together, they provide an end-to-end pipeline for high-precision multimodal search by mapping diverse modalities, including text, images, document images, and video, into a unified representation space. The Qwen3-VL-Embedding model employs a multi-stage training paradigm, progressing from large-scale contrastive pre-training to reranking model distillation, to generate semantically rich high-dimensional vectors. It supports Matryoshka Representation Learning, enabling flexible embedding dimensions, and handles inputs up to 32k tokens. Complementing this, Qwen3-VL-Reranker performs fine-grained relevance estimation for query-document pairs using a cross-encoder architecture with cross-attention mechanisms. Both model series inherit the multilingual capabilities of Qwen3-VL, supporting more than 30 languages, and are released in $\\textbf{2B}$ and $\\textbf{8B}$ parameter sizes to accommodate diverse deployment requirements. Empirical evaluations demonstrate that the Qwen3-VL-Embedding series achieves state-of-the-art results across diverse multimodal embedding evaluation benchmarks. Specifically, Qwen3-VL-Embedding-8B attains an overall score of $\\textbf{77.8}$ on MMEB-V2, ranking first among all models (as of January 8, 2025). This report presents the architecture, training methodology, and practical capabilities of the series, demonstrating their effectiveness on various multimodal retrieval tasks, including image-text retrieval, visual question answering, and video-text matching.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04720.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04720",
    "published": "2026-01-08T08:36:06Z",
    "updated": "2026-01-08T08:36:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Qwen3-VL-Embedding和Qwen3-VL-Reranker模型系列，构建了一个端到端多模态检索框架，在多模态嵌入评估中达到最佳性能。",
      "motivation": "研究动机是解决多模态信息检索中高精度搜索的挑战。随着文本、图像、文档图像和视频等多种模态数据的增长，现有方法可能难以实现高效统一的表示学习，导致检索精度不足或跨模态匹配困难。该研究旨在开发一个统一框架，通过映射多种模态到共享表示空间，提升检索准确性和实用性，满足实际应用中对多模态搜索的需求。",
      "method": "研究方法基于Qwen3-VL基础模型，开发了Qwen3-VL-Embedding和Qwen3-VL-Reranker两个模型系列。Qwen3-VL-Embedding采用多阶段训练，包括大规模对比预训练和重排序模型蒸馏，生成高维语义向量，并支持Matryoshka Representation Learning以实现灵活维度，处理长达32k tokens的输入。Qwen3-VL-Reranker使用跨编码器架构与跨注意力机制，对查询-文档对进行细粒度相关性估计。两个模型继承多语言能力，支持30多种语言，并提供2B和8B参数版本以适应不同部署需求。",
      "result": "实验结果显示，Qwen3-VL-Embedding系列在多模态嵌入评估基准上取得了state-of-the-art的结果。具体来说，Qwen3-VL-Embedding-8B在MMEB-V2基准上获得77.8总分，排名第一（截至2025年1月8日）。这表明该模型在图像-文本检索、视觉问答和视频-文本匹配等任务中表现优异，显著优于现有基线方法，验证了框架的有效性。",
      "conclusion": "该论文的主要贡献是提出了Qwen3-VL-Embedding和Qwen3-VL-Reranker模型系列，提供了一个高效的多模态检索和排序框架。学术上，它推动了多模态表示学习和检索技术的发展；实际上，该框架可用于图像-文本检索、视觉问答等应用，提高搜索精度。研究展示了模型在多语言和多模态任务中的强大能力，为未来多模态AI系统部署奠定基础，但摘要未明确说明局限性。",
      "tags": [
        "Multimodal Embedding",
        "Contrastive Learning",
        "Cross-Attention",
        "Multilingual Models",
        "Matryoshka Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:36:23.357781Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04716",
    "title": "Fame Fades, Nature Remains: Disentangling the Character Identity of Role-Playing Agents",
    "authors": [
      "Yonghyun Jun",
      "Junhyuk Choi",
      "Jihyeong Park",
      "Hwanhee Lee"
    ],
    "abstract": "Despite the rapid proliferation of Role-Playing Agents (RPAs) based on Large Language Models (LLMs), the structural dimensions defining a character's identity remain weakly formalized, often treating characters as arbitrary text inputs. In this paper, we propose the concept of \\textbf{Character Identity}, a multidimensional construct that disentangles a character into two distinct layers: \\textbf{(1) Parametric Identity}, referring to character-specific knowledge encoded from the LLM's pre-training, and \\textbf{(2) Attributive Identity}, capturing fine-grained behavioral properties such as personality traits and moral values. To systematically investigate these layers, we construct a unified character profile schema and generate both Famous and Synthetic characters under identical structural constraints. Our evaluation across single-turn and multi-turn interactions reveals two critical phenomena. First, we identify \\textit{\"Fame Fades\"}: while famous characters hold a significant advantage in initial turns due to parametric knowledge, this edge rapidly vanishes as models prioritize accumulating conversational context over pre-trained priors. Second, we find that \\textit{\"Nature Remains\"}: while models robustly portray general personality traits regardless of polarity, RPA performance is highly sensitive to the valence of morality and interpersonal relationships. Our findings pinpoint negative social natures as the primary bottleneck in RPA fidelity, guiding future character construction and evaluation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04716.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04716",
    "published": "2026-01-08T08:33:40Z",
    "updated": "2026-01-08T08:33:40Z",
    "comment": "27 pages",
    "light_analysis": {
      "overview": "提出角色身份的多维概念，将其解构为参数身份和属性身份，系统分析基于大型语言模型的角色扮演代理，以揭示其关键行为特征。",
      "motivation": "基于大型语言模型的角色扮演代理日益普及，但角色身份的结构维度缺乏形式化定义，常被简化为任意文本输入。这导致现有方法难以准确区分角色的内在层次，限制了RPAs的性能和评估效果，使得角色构建不够精准。本研究旨在解决此问题，通过系统化定义角色身份，以提升RPAs的逼真度和应用有效性。",
      "method": "研究方法基于提出“角色身份”概念，分解为参数身份（来自LLM预训练的角色特定知识）和属性身份（捕捉细粒度行为属性如人格特质和道德价值观）。作者构建了统一的角色配置文件模式，在相同结构约束下生成著名和合成角色。通过单轮和多轮交互进行评估，以探索身份层次对RPAs的影响。摘要未明确说明具体数据集或模型架构，但依赖LLMs进行实验分析。",
      "result": "实验结果显示两个关键现象：一是“名望消退”，著名角色在初始回合因参数知识优势明显，但随着对话进行，模型更依赖上下文，优势迅速消失；二是“本性难移”，模型能稳健地描绘一般人格特质，但RPAs表现对道德和人际关系的极性高度敏感。研究发现，负面社会属性是影响RPA逼真度的主要瓶颈，与基线方法对比强调了身份层次的重要性。",
      "conclusion": "本文的主要贡献是提出了角色身份的多维框架，并验证其在基于LLM的RPAs中的关键作用。研究发现揭示了身份层次对性能的影响，学术上为角色研究提供了新视角，应用上可指导角色构建和评估以提升真实性和鲁棒性。未来工作可探索缓解负面属性影响的方法，并扩展身份框架到更多交互场景，以推动RPAs的进一步发展。",
      "tags": [
        "Large Language Model",
        "Role-Playing Agent",
        "Character Identity",
        "Parametric Identity",
        "Attributive Identity"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:14.002401Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04711",
    "title": "DSC2025 -- ViHallu Challenge: Detecting Hallucination in Vietnamese LLMs",
    "authors": [
      "Anh Thi-Hoang Nguyen",
      "Khanh Quoc Tran",
      "Tin Van Huynh",
      "Phuoc Tan-Hoang Nguyen",
      "Cam Tan Nguyen",
      "Kiet Van Nguyen"
    ],
    "abstract": "The reliability of large language models (LLMs) in production environments remains significantly constrained by their propensity to generate hallucinations--fluent, plausible-sounding outputs that contradict or fabricate information. While hallucination detection has recently emerged as a priority in English-centric benchmarks, low-to-medium resource languages such as Vietnamese remain inadequately covered by standardized evaluation frameworks. This paper introduces the DSC2025 ViHallu Challenge, the first large-scale shared task for detecting hallucinations in Vietnamese LLMs. We present the ViHallu dataset, comprising 10,000 annotated triplets of (context, prompt, response) samples systematically partitioned into three hallucination categories: no hallucination, intrinsic, and extrinsic hallucinations. The dataset incorporates three prompt types--factual, noisy, and adversarial--to stress-test model robustness. A total of 111 teams participated, with the best-performing system achieving a macro-F1 score of 84.80\\%, compared to a baseline encoder-only score of 32.83\\%, demonstrating that instruction-tuned LLMs with structured prompting and ensemble strategies substantially outperform generic architectures. However, the gap to perfect performance indicates that hallucination detection remains a challenging problem, particularly for intrinsic (contradiction-based) hallucinations. This work establishes a rigorous benchmark and explores a diverse range of detection methodologies, providing a foundation for future research into the trustworthiness and reliability of Vietnamese language AI systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04711.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04711",
    "published": "2026-01-08T08:27:47Z",
    "updated": "2026-01-08T08:27:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出首个针对越南语大语言模型的幻觉检测挑战赛和数据集，为提升模型可靠性奠定基准。",
      "motivation": "研究动机是解决越南语大语言模型（LLMs）中的幻觉检测问题，这些幻觉指流畅但矛盾或虚构的输出，严重影响模型在生产环境中的可靠性。当前，幻觉检测主要集中在英语基准测试中，而越南语等低至中等资源语言缺乏标准化评估框架，导致现有方法覆盖不足，无法有效评估模型可信度。因此，开发针对越南语的幻觉检测框架至关重要，以填补这一空白并推动AI系统在现实应用中的信任度提升。",
      "method": "研究方法包括创建ViHallu数据集，该数据集包含10,000个标注的三元组（上下文、提示、响应），系统划分为三类幻觉：无幻觉、内在幻觉（矛盾）和外在幻觉（虚构）。数据集引入三种提示类型（事实性、噪声和对抗性）以测试模型鲁棒性。技术路线涉及组织大规模共享任务，吸引了111支团队参与，其中最佳解决方案基于指令调优的大语言模型，结合结构化提示和集成策略，以提高检测准确性和模型泛化能力。",
      "result": "主要实验结果表明，最佳系统在ViHallu数据集上达到84.80%的宏观F1分数，显著优于基线编码器模型的32.83%，验证了指令调优模型结合结构化提示和集成策略的有效性。然而，与完美性能的差距表明幻觉检测仍具挑战性，尤其是内在幻觉（矛盾类型）的检测更为困难。这些结果为未来研究提供了基准，并突出了复杂幻觉场景中检测算法的改进空间。",
      "conclusion": "本研究的主要贡献是建立了首个越南语大语言模型幻觉检测的严格基准——ViHallu挑战赛和数据集，探索了多样化的检测方法，为提升越南语AI系统的可信赖性和可靠性奠定基础。学术价值在于填补了低资源语言评估的空白，实际应用有助于推动安全AI发展。局限性体现在内在幻觉检测挑战较大，未来工作可专注于优化检测算法、扩展数据集规模以及应用于其他类似语言环境。",
      "tags": [
        "Hallucination Detection",
        "Vietnamese Language Models",
        "Benchmark Dataset",
        "Instruction Tuning",
        "Ensemble Strategies"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:14.794422Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04710",
    "title": "Prior-Informed Zeroth-Order Optimization with Adaptive Direction Alignment for Memory-Efficient LLM Fine-Tuning",
    "authors": [
      "Feihu Jin",
      "Shipeng Cen",
      "Ying Tan"
    ],
    "abstract": "Fine-tuning large language models (LLMs) has achieved remarkable success across various NLP tasks, but the substantial memory overhead during backpropagation remains a critical bottleneck, especially as model scales grow. Zeroth-order (ZO) optimization alleviates this issue by estimating gradients through forward passes and Gaussian sampling, avoiding the need for backpropagation. However, conventional ZO methods suffer from high variance in gradient estimation due to their reliance on random perturbations, leading to slow convergence and suboptimal performance. We propose a simple plug-and-play method that incorporates prior-informed perturbations to refine gradient estimation. Our method dynamically computes a guiding vector from Gaussian samples, which directs perturbations toward more informative directions, significantly accelerating convergence compared to standard ZO approaches. We further investigate a greedy perturbation strategy to explore the impact of prior knowledge on gradient estimation. Theoretically, we prove that our gradient estimator achieves stronger alignment with the true gradient direction, enhancing optimization efficiency. Extensive experiments across LLMs of varying scales and architectures demonstrate that our proposed method could seamlessly integrate into existing optimization methods, delivering faster convergence and superior performance. Notably, on the OPT-13B model, our method outperforms traditional ZO optimization across all 11 benchmark tasks and surpasses gradient-based baselines on 9 out of 11 tasks, establishing a robust balance between efficiency and accuracy.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04710.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04710",
    "published": "2026-01-08T08:27:15Z",
    "updated": "2026-01-08T08:27:15Z",
    "comment": "12pages, 6figures",
    "light_analysis": {
      "overview": "论文提出了一种先验信息引导的零阶优化方法，通过自适应方向对齐提升大规模语言模型微调的内存效率和收敛速度。",
      "motivation": "大规模语言模型（LLMs）微调在多种自然语言处理任务中表现卓越，但反向传播带来的巨大内存开销成为关键瓶颈，尤其随模型规模增大而加剧。传统零阶（ZO）优化方法通过前向传递和高斯采样估计梯度以规避反向传播，但其依赖随机扰动导致梯度估计方差高、收敛缓慢且性能不佳。因此，需要改进ZO方法以解决内存效率低和优化效果差的问题，为大规模模型部署提供实用方案。",
      "method": "论文提出一种即插即用的先验信息引导优化方法，核心在于利用高斯采样动态计算引导向量，将扰动导向信息更丰富的方向，从而减少梯度估计方差并加速收敛。此外，方法还探索了贪婪扰动策略以评估先验知识对梯度估计的影响。从理论上证明，该梯度估计器能更强对齐真实梯度方向，提升优化效率。基于传统ZO框架，无需改变优化器结构，便于集成到现有方法中。",
      "result": "在多种规模和架构的大规模语言模型上进行广泛实验，结果表明该方法能无缝集成到现有优化方法中，实现更快收敛和优越性能。具体地，在OPT-13B模型上，论文方法在所有11个基准任务中均优于传统ZO优化方法，并在其中9个任务中超越了基于梯度的基线方法，从而在效率和准确性之间达到稳健平衡。这些结果验证了方法在减少内存开销的同时提升任务性能的有效性。",
      "conclusion": "论文的主要贡献是提出先验信息引导的零阶优化方法，显著提高大规模语言模型微调的内存效率和收敛速度。从学术角度看，理论分析证实了梯度估计器与真实方向的对齐性，丰富了优化理论。实际应用中，该方法有助于解决大规模模型训练中的内存瓶颈，具有广泛部署潜力。未来工作可探索先验知识的多样化来源或扩展方法到更多模型架构中。",
      "tags": [
        "Zeroth-Order Optimization",
        "Prior-Informed Optimization",
        "Adaptive Direction Alignment",
        "LLM Fine-Tuning",
        "Memory Efficiency"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:47.333025Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.03296",
    "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling",
    "authors": [
      "Anqi Li",
      "Wenwei Jin",
      "Jintao Tong",
      "Pengda Qin",
      "Weijia Li",
      "Guo Lu"
    ],
    "abstract": "Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term \"Hierarchical\" reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.03296.pdf",
    "abs_url": "https://arxiv.org/abs/2508.03296",
    "published": "2025-08-05T10:16:04Z",
    "updated": "2026-01-08T08:23:50Z",
    "comment": "Accepted by KDD 2026. Code is available at https://github.com/lianqi1008/Hi-Guard",
    "light_analysis": {
      "overview": "本论文提出Hierarchical Guard (Hi-Guard)框架，通过政策对齐推理和分层标签，实现可信赖的多模态内容审核，显著提升准确性和可解释性。",
      "motivation": "社交平台信息共享革命化，但有害和违规内容传播加速，需大规模、准确和可解释的审核系统确保安全合规。现有方法主要依赖噪声标签驱动学习，缺乏与审核规则对齐，产生不透明决策，妨碍人工审查，导致效率低下和信任缺失。因此，亟需新方法解决这些不足，以实现更可靠的自动化内容审核。",
      "method": "Hi-Guard框架采用分层审核管道：先使用轻量二进制模型过滤安全内容，再用强模型处理细粒度风险分类，并在第二阶段实施分层分类法进行路径分类，从粗到细粒度。为确保政策对齐，直接将规则定义集成到模型提示中；为增强结构化预测，引入多级软边界奖励，并通过Group Relative Policy Optimization (GRPO)优化，惩罚语义相邻误分类以改善解释质量，实现高效的多模态推理。",
      "result": "广泛实验和实际部署表明，Hi-Guard在分类准确性、泛化性和可解释性方面表现优越，相较于现有方法有显著提升，摘要未明确说明具体性能指标数据，但暗示其通过政策对齐和分层设计，为构建可信内容安全系统提供了实证基础，支持大规模透明审核应用。",
      "conclusion": "本论文贡献了Hi-Guard框架，通过政策对齐推理和分层标签，推动多模态内容审核向可扩展、透明和可信方向发展，具有重要的学术和实际应用价值，未来工作可进一步优化规则集成或扩展到更广泛领域，摘要未明确说明局限性。",
      "tags": [
        "Multimodal Moderation",
        "Hierarchical Classification",
        "Policy-Aligned Reasoning",
        "Reinforcement Learning",
        "GRPO"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:49.742788Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.10029",
    "title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs",
    "authors": [
      "Wenpeng Xing",
      "Mohan Li",
      "Chunqiang Hu",
      "Haitao Xu",
      "Ningyu Zhang",
      "Bo Lin",
      "Meng Han"
    ],
    "abstract": "While Large Language Models (LLMs) have achieved remarkable progress, they remain vulnerable to jailbreak attacks. Existing methods, primarily relying on discrete input optimization (e.g., GCG), often suffer from high computational costs and generate high-perplexity prompts that are easily blocked by simple filters. To overcome these limitations, we propose Latent Fusion Jailbreak (LFJ), a stealthy white-box attack that operates in the continuous latent space. Unlike previous approaches, LFJ constructs adversarial representations by mathematically fusing the hidden states of a harmful query with a thematically similar benign query, effectively masking malicious intent while retaining semantic drive. We further introduce a gradient-guided optimization strategy to balance attack success and computational efficiency. Extensive evaluations on Vicuna-7B, LLaMA-2-7B-Chat, Guanaco-7B, LLaMA-3-70B, and Mistral-7B-Instruct show that LFJ achieves an average Attack Success Rate (ASR) of 94.01%, significantly outperforming state-of-the-art baselines like GCG and AutoDAN while avoiding detectable input artifacts. Furthermore, we identify that thematic similarity in the latent space is a critical vulnerability in current safety alignments. Finally, we propose a latent adversarial training defense that reduces LFJ's ASR by over 80% without compromising model utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.10029.pdf",
    "abs_url": "https://arxiv.org/abs/2508.10029",
    "published": "2025-08-08T17:29:16Z",
    "updated": "2026-01-08T08:10:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Latent Fusion Jailbreak（LFJ）方法，通过在连续潜在空间中融合有害和无害查询的表示，以隐秘方式引发大型语言模型的不安全输出，揭示新的安全漏洞。",
      "motivation": "大型语言模型（LLMs）虽取得显著进展，但容易受到越狱攻击，这是一个重要的安全问题，因为它可能导致有害内容生成。现有方法主要依赖于离散输入优化（如GCG），但这些方法存在计算成本高、生成的提示高困惑度且易被简单过滤器检测的不足，限制了攻击效率和隐蔽性。因此，开发更高效、隐秘的越狱攻击方法至关重要，以深入理解模型脆弱性并推动安全对齐改进。",
      "method": "本研究提出Latent Fusion Jailbreak（LFJ），一种在连续潜在空间中操作的白盒攻击方法。核心方法是数学融合有害查询和主题相似无害查询的隐藏状态，构建对抗性表示，既能掩藏恶意意图，又保留语义驱动。关键创新点包括引入梯度引导优化策略，以在攻击成功率和计算效率之间取得平衡。实验中使用多个LLM进行评估，如Vicuna-7B和LLaMA-2-7B-Chat，但具体数据集摘要未明确说明。",
      "result": "在多个LLM（Vicuna-7B、LLaMA-2-7B-Chat、Guanaco-7B、LLaMA-3-70B和Mistral-7B-Instruct）上广泛评估，LFJ的平均攻击成功率（ASR）达到94.01%，显著优于GCG和AutoDAN等基线方法。此外，LFJ避免了可检测输入伪影，提高了隐蔽性。提出的潜在对抗训练防御措施将LFJ的ASR降低了超过80%，证明该防御在不损害模型实用性的情况下有效缓解攻击。",
      "conclusion": "本研究的主要贡献是提出LFJ攻击方法，并揭示潜在空间中主题相似性是当前LLM安全对齐的关键漏洞。学术价值在于提供一种新攻击视角，促进了对抗攻击和防御研究；实际应用价值包括帮助开发更鲁棒的安全措施。局限性摘要未明确说明，但未来工作可扩展到更多模型或应用场景，如结合其他安全机制。",
      "tags": [
        "Large Language Model",
        "Jailbreak Attack",
        "Adversarial Representation",
        "Latent Space",
        "Gradient-Guided Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:52.294890Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04700",
    "title": "PRISM: A Unified Framework for Post-Training LLMs Without Verifiable Rewards",
    "authors": [
      "Mukesh Ghimire",
      "Aosong Feng",
      "Liwen You",
      "Youzhi Luo",
      "Fang Liu",
      "Xuan Zhu"
    ],
    "abstract": "Current techniques for post-training Large Language Models (LLMs) rely either on costly human supervision or on external verifiers to boost performance on tasks such as mathematical reasoning and code generation. However, as LLMs improve their problem-solving, any further improvement will potentially require high-quality solutions to difficult problems that are not available to humans. As a result, learning from unlabeled data is becoming increasingly attractive in the research community. Existing methods extract learning signal from a model's consistency, either by majority voting or by converting the model's internal confidence into reward. Although internal consistency metric such as entropy or self-certainty require no human intervention, as we show in this work, these are unreliable signals for large-scale and long-term training. To address the unreliability, we propose PRISM, a unified training framework that uses a Process Reward Model (PRM) to guide learning alongside model's internal confidence in the absence of ground-truth labels. We show that effectively combining PRM with self-certainty can lead to both stable training and better test-time performance, and also keep the model's internal confidence in check.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04700.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04700",
    "published": "2026-01-08T08:09:29Z",
    "updated": "2026-01-08T08:09:29Z",
    "comment": "Preprint. Under Review",
    "light_analysis": {
      "overview": "PRISM框架提出一种无验证奖励的后训练方法，通过结合过程奖励模型和模型内部置信度，实现稳定训练和性能提升。",
      "motivation": "当前后训练大型语言模型（LLMs）通常依赖于昂贵的人类监督或外部验证器来提高在数学推理和代码生成等任务上的性能。随着LLMs解决问题能力的提升，需要高质量解决方案的困难问题可能超出人类能力范围，使得无标签数据学习变得更具吸引力。现有方法通过多数投票或将模型内部置信度转换为奖励来从模型一致性中提取学习信号，但这些内部一致性指标如熵或自确定性不可靠，特别是在大规模和长期训练中。因此，本研究旨在解决内部信号不可靠的问题，提供一种更稳定的无监督学习框架。",
      "method": "论文提出了PRISM框架，一个统一的后训练框架，用于在无地面真实标签的情况下指导大型语言模型的学习。核心方法结合了过程奖励模型（PRM）和模型内部置信度（如自确定性）。PRM用于提供外部指导信号，而内部置信度用于补充和校准学习过程。这种结合旨在利用过程奖励的稳定性和内部置信度的动态调整，以实现更可靠的学习。关键创新点在于统一了外部奖励和内部信号，避免了单一依赖的不可靠性。摘要未明确说明具体使用的数据集、模型架构或其他技术细节。",
      "result": "论文表明，有效结合过程奖励模型（PRM）和模型自确定性可以导致稳定的训练过程和更好的测试时性能。同时，这种结合有助于控制模型的内部置信度，防止过拟合或偏差。与现有基于内部一致性的方法相比，PRISM框架在可靠性和性能上有所改进。然而，摘要未明确说明具体的性能指标数据，如准确率提升百分比或效率改进，也未提供与基线方法的详细对比结果。",
      "conclusion": "PRISM框架的主要贡献在于提出了一种统一的方法，用于无监督后训练大型语言模型，通过结合过程奖励和内部置信度来克服现有方法的不可靠性。这项研究具有重要的学术价值，因为它为在没有人类监督或外部验证器的情况下进行LLM后训练提供了新思路，并可能推动无监督学习在复杂任务中的应用。实际应用价值包括降低训练成本和提高模型在数学推理、代码生成等领域的性能。摘要未明确说明研究的局限性或未来工作方向，但可以推断进一步验证和扩展是潜在方向。",
      "tags": [
        "Large Language Model",
        "Process Reward Model",
        "Self-Certainty",
        "Unsupervised Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:13.764370Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04698",
    "title": "TourPlanner: A Competitive Consensus Framework with Constraint-Gated Reinforcement Learning for Travel Planning",
    "authors": [
      "Yinuo Wang",
      "Mining Tan",
      "Wenxiang Jiao",
      "Xiaoxi Li",
      "Hao Wang",
      "Xuanyu Zhang",
      "Yuan Lu",
      "Weiming Dong"
    ],
    "abstract": "Travel planning is a sophisticated decision-making process that requires synthesizing multifaceted information to construct itineraries. However, existing travel planning approaches face several challenges: (1) Pruning candidate points of interest (POIs) while maintaining a high recall rate; (2) A single reasoning path restricts the exploration capability within the feasible solution space for travel planning; (3) Simultaneously optimizing hard constraints and soft constraints remains a significant difficulty. To address these challenges, we propose TourPlanner, a comprehensive framework featuring multi-path reasoning and constraint-gated reinforcement learning. Specifically, we first introduce a Personalized Recall and Spatial Optimization (PReSO) workflow to construct spatially-aware candidate POIs' set. Subsequently, we propose Competitive consensus Chain-of-Thought (CCoT), a multi-path reasoning paradigm that improves the ability of exploring the feasible solution space. To further refine the plan, we integrate a sigmoid-based gating mechanism into the reinforcement learning stage, which dynamically prioritizes soft-constraint satisfaction only after hard constraints are met. Experimental results on travel planning benchmarks demonstrate that TourPlanner achieves state-of-the-art performance, significantly surpassing existing methods in both feasibility and user-preference alignment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04698.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04698",
    "published": "2026-01-08T08:08:35Z",
    "updated": "2026-01-08T08:08:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "TourPlanner提出一个结合多路径推理和约束门控强化学习的框架，以解决旅行规划中候选点剪裁、推理路径单一和约束优化等关键挑战。",
      "motivation": "旅行规划作为复杂的决策过程，需要综合多方面信息来构建行程，但现有方法面临多个问题：剪裁候选兴趣点（POI）时难以保持高召回率，导致信息丢失；单一推理路径限制了在可行解空间中的探索能力，可能错失最优解；同时优化硬约束（如时间、预算）和软约束（如用户偏好）是一大难题，影响规划的实用性和准确性。这些不足使得现有方法在现实应用中效率低下，亟需更灵活的解决方案来提升旅行规划的质量和用户体验。",
      "method": "TourPlanner框架包括三个核心组件：首先，引入个性化召回和空间优化（PReSO）工作流，基于用户偏好和空间信息构建候选POI集，提高召回率；其次，提出竞争共识链式思考（CCoT）多路径推理范式，通过并行推理路径增强对可行解空间的探索能力；最后，在强化学习阶段集成基于sigmoid的门控机制，动态调整约束优先级，确保在满足硬约束后才优化软约束。该框架使用强化学习方法，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验在旅行规划基准上进行，结果显示TourPlanner实现了最先进的性能，在可行性和用户偏好对齐方面显著超越现有方法。虽然没有提供具体数据，但摘要强调其在处理硬约束和软约束上的优越性，表明框架能有效平衡约束条件，提升旅行规划的准确性和用户满意度，为相关领域提供了新的性能基准。",
      "conclusion": "论文的主要贡献是提出了TourPlanner综合框架，通过多路径推理和约束门控强化学习解决了旅行规划的关键挑战。学术上，它引入了创新方法如CCoT和门控机制，推动了决策智能和个性化推荐领域的发展；实际应用中，提高了旅行规划的效率和用户满意度。摘要未明确说明研究的局限性或未来工作方向，但可能涉及扩展到其他领域或处理更多约束类型。",
      "tags": [
        "Travel Planning",
        "Reinforcement Learning",
        "Constraint-Gated Mechanism",
        "Multi-Path Reasoning",
        "Chain-of-Thought"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:08.869126Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04696",
    "title": "A Method for Constructing a Digital Transformation Driving Mechanism Based on Semantic Understanding of Large Models",
    "authors": [
      "Huayi Liu"
    ],
    "abstract": "In the process of digital transformation, enterprises are faced with problems such as insufficient semantic understanding of unstructured data and lack of intelligent decision-making basis in driving mechanisms. This study proposes a method that combines a large language model (LLM) and a knowledge graph. First, a fine-tuned BERT (Bidirectional Encoder Representations from Transformers) model is used to perform entity recognition and relationship extraction on multi-source heterogeneous texts, and GPT-4 is used to generate semantically enhanced vector representations; secondly, a two-layer graph neural network (GNN) architecture is designed to fuse the semantic vectors output by LLM with business metadata to construct a dynamic and scalable enterprise knowledge graph; then reinforcement learning is introduced to optimize decision path generation, and the reward function is used to drive the mechanism iteration. In the case of the manufacturing industry, this mechanism reduced the response time for equipment failure scenarios from 7.8 hours to 3.7 hours, the F1 value reached 94.3%, and the compensation for decision errors in the annual digital transformation cost decreased by 45.3%. This method significantly enhances the intelligence level and execution efficiency of the digital transformation driving mechanism by integrating large model semantic understanding with structured knowledge.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04696.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04696",
    "published": "2026-01-08T08:06:58Z",
    "updated": "2026-01-08T08:06:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种结合大型语言模型和知识图谱的数字化转型驱动机制，通过融合语义理解和结构化知识显著提升智能决策能力。",
      "motivation": "企业在数字化转型过程中面临非结构化数据语义理解不足和驱动机制缺乏智能决策依据的问题。现有方法在处理异构数据和动态决策支持方面存在局限，导致数字化转型效率低下。解决这些问题对于提高企业竞争力和运营效率至关重要，因此需要开发能够深入理解语义并实时优化决策的新机制。",
      "method": "本研究提出一种结合大型语言模型和知识图谱的方法。首先，使用微调BERT模型对多源异构文本进行实体识别和关系抽取，并利用GPT-4生成语义增强的向量表示。其次，设计两层图神经网络架构，将语义向量与业务元数据融合，构建动态可扩展的企业知识图谱。然后，引入强化学习优化决策路径生成，通过奖励函数驱动机制迭代更新。",
      "result": "在制造业案例实验中，该方法显著提升了性能：设备故障场景响应时间从7.8小时缩短至3.7小时，实现了快速响应；F1值达到94.3%，表明在实体识别和关系抽取方面具有高准确性；年度数字化转型成本中决策错误补偿减少了45.3%，显示出良好的经济效益。",
      "conclusion": "该方法通过集成大型语言模型的语义理解和知识图谱的结构化知识，显著增强了数字化转型驱动机制的智能水平和执行效率。学术价值在于多技术融合创新，实际应用价值为加速企业数字化转型。未来可扩展至其他行业或更复杂场景，以提升通用性。",
      "tags": [
        "Large Language Model",
        "Knowledge Graph",
        "Graph Neural Network",
        "Reinforcement Learning",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:24.802362Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.19327",
    "title": "Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome",
    "authors": [
      "Moamal Fadhil Abdul-Mahdi",
      "Jonas Bruun Hubrechts",
      "Thomas Martini Jørgensen",
      "Emil Hovad"
    ],
    "abstract": "Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either \"bounce\", \"net\", or \"empty_event\" in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.19327.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19327",
    "published": "2025-12-22T12:25:50Z",
    "updated": "2026-01-08T18:45:51Z",
    "comment": "Thomas Martini Jørgensen and Emil Hovad contributed equally and share last authorship",
    "light_analysis": {
      "overview": "论文扩展了OpenTTGames数据集，添加细粒度击球类型和回合结果注释，以支持乒乓球视频的自动分析和战术理解。",
      "motivation": "研究旨在解决自动检测和分类乒乓球视频中击球的问题，以优化训练工作流、丰富广播内容并实现精细性能分析。这一问题重要，因为现有视频资源往往未公开或具有限制性许可证，阻碍了重用和基准测试。OpenTTGames数据集虽然包含基础事件注释，但缺乏细粒度的击球类型和战术相关标签，限制了模型从简单事件检测向高级战术理解的过渡，导致实际应用受限。",
      "method": "论文通过扩展OpenTTGames数据集，添加了帧精确的击球类型注释（如正手、反手及其子类型）、球员姿势标签（身体倾斜和腿部姿态）以及回合结果标签。关键创新在于提供了一套紧凑的编码方案和代码辅助标签过程，确保注释的可重复性和一致性，支持从事件检测向战术理解过渡。数据集原本包含球坐标、事件（反弹、网等）和语义掩码（人、桌、记分牌），扩展后增强了细粒度分析能力。",
      "result": "摘要未明确说明具体的实验性能指标或对比结果。论文的主要成果是发布了扩展的数据集，提供了细粒度击球类型和回合结果注释，为相关研究提供了基础资源。该数据集可用于训练基线模型，以进行事件检测和分类任务，但未在摘要中报告具体的准确率或效率改进数据。",
      "conclusion": "论文的主要贡献是扩展了OpenTTGames数据集，填补了乒乓球视频细粒度注释的空白，支持计算机视觉和体育分析研究。学术价值在于促进了细粒度击球理解和战术分析的发展；实际应用价值包括非商业用途的训练工具和广播增强。局限性在于许可证限制为非商业用途，未来工作可能扩展到其他球类运动或增加更多注释维度。",
      "tags": [
        "Dataset Annotation",
        "Fine-grained Classification",
        "Video Analysis",
        "Sports Analytics",
        "Computer Vision"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:25.673733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05212",
    "title": "FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching",
    "authors": [
      "Danilo Danese",
      "Angela Lombardi",
      "Matteo Attimonelli",
      "Giuseppe Fasano",
      "Tommaso Di Noia"
    ],
    "abstract": "Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual's biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05212.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05212",
    "published": "2026-01-08T18:36:29Z",
    "updated": "2026-01-08T18:36:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了FlowLet，一种利用小波域流匹配的条件性3D脑MRI合成框架，旨在提升脑年龄预测的公平性和效率。",
      "motivation": "脑年龄预测（BAP）模型需要大规模、多样化且年龄均衡的MRI数据集以确保公平性和泛化能力，但现有3D MRI数据存在人口统计偏差，获取新数据成本高且受伦理限制。现有生成方法如潜在扩散模型虽能处理体积数据，但推理速度慢、可能引入压缩伪影，且缺乏年龄条件性，限制了BAP应用效果，因此需要更高效的生成数据增强方法来解决这一问题。",
      "method": "FlowLet采用流匹配技术，在可逆的3D小波变换域中进行操作，通过将MRI数据转换到小波域，利用流匹配模型生成年龄条件的3D MRI图像。该方法的关键创新点在于结合小波域和流匹配，避免了传统潜在扩散模型中的潜在压缩伪影，同时减少了计算需求，实现更快速、高质量的合成过程，摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验表明，FlowLet能够以少量采样步骤生成高保真度的3D MRI体积。使用FlowLet合成数据进行BAP模型训练后，对代表性不足的年龄组的预测性能得到改善，提升了模型的公平性。基于区域的分析进一步确认，生成的MRI图像有效保留了关键解剖结构，验证了该方法在合成质量和实用性方面的优势。",
      "conclusion": "FlowLet框架通过结合小波域和流匹配，实现了高效、高质量的条件性3D脑MRI合成，为脑年龄预测提供了更公平的数据增强手段，促进了模型在人口统计偏差数据集上的性能提升。该研究具有学术价值，推动了生成模型在医学成像领域的应用，并可能扩展至其他临床任务。摘要未明确说明具体局限性，未来工作可包括进一步优化生成效率和探索更多条件生成场景。",
      "tags": [
        "Flow Matching",
        "Wavelet Transform",
        "Conditional Generation",
        "3D MRI Synthesis"
      ]
    },
    "analyzed_at": "2026-01-09T02:47:57.695563Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05208",
    "title": "MoE3D: A Mixture-of-Experts Module for 3D Reconstruction",
    "authors": [
      "Zichen Wang",
      "Ang Cao",
      "Liam J. Wang",
      "Jeong Joon Park"
    ],
    "abstract": "MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05208.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05208",
    "published": "2026-01-08T18:33:52Z",
    "updated": "2026-01-08T18:33:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "MoE3D是一个混合专家模块，旨在通过动态加权融合多个候选深度图来锐化3D重建的深度边界并减少飞行点伪影。",
      "motivation": "该研究旨在解决现有前馈3D重建模型中常见的深度边界模糊和飞行点伪影问题。这些问题在实际应用中（如计算机视觉和虚拟现实）会降低重建的准确性和视觉效果，影响模型实用性。现有方法可能缺乏精细调整能力，导致伪影和不精确的边界估计，因此需要创新技术来提升重建质量。",
      "method": "MoE3D采用混合专家（MoE）架构，通过预测多个候选深度图并使用动态权重进行融合。关键创新点包括基于输入特征的动态加权机制，以优化专家贡献并减少计算开销。该方法可无缝集成到预训练的3D重建骨干网络（如VGGT）中，通过模块化设计增强模型性能，而无需大幅增加参数或计算复杂度。",
      "result": "当与预训练骨干网络集成时，MoE3D显著提升了重建质量，有效减少了深度边界模糊和飞行点伪影。尽管摘要未明确说明具体性能指标，但作者指出在最小额外计算开销下实现了质量改进。与基线方法相比，该模块展示了更好的视觉精度和效率，适用于实时应用场景。",
      "conclusion": "MoE3D模块通过混合专家和动态加权技术，为3D重建提供了有效的增强方案，提升了边界清晰度和伪影缓解能力。其学术价值在于提出轻量级模块化方法，实际应用潜力广泛，如增强现实和自动驾驶。未来工作可能涉及扩展到更多骨干网络或优化动态加权算法以进一步提高适应性。",
      "tags": [
        "Mixture-of-Experts",
        "3D Reconstruction",
        "Depth Estimation",
        "Dynamic Weighting",
        "Artifact Mitigation"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:30.880990Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05191",
    "title": "Cutting AI Research Costs: How Task-Aware Compression Makes Large Language Model Agents Affordable",
    "authors": [
      "Zuhair Ahmed Khan Taha",
      "Mohammed Mudassir Uddin",
      "Shahnawaz Alam"
    ],
    "abstract": "When researchers deploy large language models for autonomous tasks like reviewing literature or generating hypotheses, the computational bills add up quickly. A single research session using a 70-billion parameter model can cost around $127 in cloud fees, putting these tools out of reach for many academic labs. We developed AgentCompress to tackle this problem head-on. The core idea came from a simple observation during our own work: writing a novel hypothesis clearly demands more from the model than reformatting a bibliography. Why should both tasks run at full precision? Our system uses a small neural network to gauge how hard each incoming task will be, based only on its opening words, then routes it to a suitably compressed model variant. The decision happens in under a millisecond. Testing across 500 research workflows in four scientific fields, we cut compute costs by 68.3% while keeping 96.2% of the original success rate. For labs watching their budgets, this could mean the difference between running experiments and sitting on the sidelines",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05191.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05191",
    "published": "2026-01-08T18:13:46Z",
    "updated": "2026-01-08T18:13:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了AgentCompress系统，通过任务感知压缩技术降低大型语言模型在自主任务中的部署成本。",
      "motivation": "研究动机源于大型语言模型在学术研究中的应用成本过高问题。使用70亿参数模型进行单个研究会话的云费用约为127美元，使得许多学术实验室难以负担。现有方法对所有任务采用全精度模型，未考虑任务难度差异（例如，撰写新假设比重新格式化参考文献需要更高精度），导致不必要的计算开销。因此，开发一种能根据任务需求动态调整模型精度的方法至关重要，以降低成本并提高资源利用率，促进AI研究的普及。",
      "method": "方法核心是AgentCompress系统，采用小型神经网络实时评估任务的难度。系统仅基于任务的开头词语，在1毫秒内判断任务复杂度，并路由到适当压缩的模型变体。关键创新在于任务感知压缩技术，通过动态选择模型精度优化计算资源使用，避免了为简单任务浪费全精度计算。该方法结合了压缩模型变体和快速决策机制，实现了高效的资源分配。",
      "result": "实验在四个科学领域的500个工作流中进行测试。结果显示，计算成本降低了68.3%，同时保持了96.2%的原始任务成功率。与使用全精度模型的基线相比，AgentCompress显著降低了成本，而性能损失极小。具体数据表明，成本降低基于实际云费用计算，成功率的保持验证了系统的可靠性，支持了在实际应用中的经济可行性。",
      "conclusion": "结论表明，AgentCompress系统成功降低了大型语言模型的部署成本，为资源有限的学术实验室提供了可负担的解决方案。研究的学术价值在于提出了任务感知压缩的新方法，促进了AI工具的民主化。实际应用价值体现在帮助实验室节省预算，使更多研究得以开展。未来工作可包括扩展到更多领域和优化压缩算法以进一步提高效率。",
      "tags": [
        "Large Language Model",
        "Model Compression",
        "Task-Aware Routing",
        "Neural Network",
        "Cost Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:53.200107Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05175",
    "title": "VideoAuto-R1: Video Auto Reasoning via Thinking Once, Answering Twice",
    "authors": [
      "Shuming Liu",
      "Mingchen Zhuge",
      "Changsheng Zhao",
      "Jun Chen",
      "Lemeng Wu",
      "Zechun Liu",
      "Chenchen Zhu",
      "Zhipeng Cai",
      "Chong Zhou",
      "Haozhe Liu",
      "Ernie Chang",
      "Saksham Suri",
      "Hongyu Xu",
      "Qi Qian",
      "Wei Wen",
      "Balakrishnan Varadarajan",
      "Zhuang Liu",
      "Hu Xu",
      "Florian Bordes",
      "Raghuraman Krishnamoorthi",
      "Bernard Ghanem",
      "Vikas Chandra",
      "Yunyang Xiong"
    ],
    "abstract": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for multimodal large language models on video understanding tasks. However, its necessity and advantages over direct answering remain underexplored. In this paper, we first demonstrate that for RL-trained video models, direct answering often matches or even surpasses CoT performance, despite CoT producing step-by-step analyses at a higher computational cost. Motivated by this, we propose VideoAuto-R1, a video understanding framework that adopts a reason-when-necessary strategy. During training, our approach follows a Thinking Once, Answering Twice paradigm: the model first generates an initial answer, then performs reasoning, and finally outputs a reviewed answer. Both answers are supervised via verifiable rewards. During inference, the model uses the confidence score of the initial answer to determine whether to proceed with reasoning. Across video QA and grounding benchmarks, VideoAuto-R1 achieves state-of-the-art accuracy with significantly improved efficiency, reducing the average response length by ~3.3x, e.g., from 149 to just 44 tokens. Moreover, we observe a low rate of thinking-mode activation on perception-oriented tasks, but a higher rate on reasoning-intensive tasks. This suggests that explicit language-based reasoning is generally beneficial but not always necessary.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05175.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05175",
    "published": "2026-01-08T18:00:59Z",
    "updated": "2026-01-08T18:00:59Z",
    "comment": "Project page: https://ivul-kaust.github.io/projects/videoauto-r1/",
    "light_analysis": {
      "overview": "论文提出VideoAuto-R1框架，采用按需推理策略，在视频理解任务中实现高效准确的性能提升。",
      "motivation": "该研究动机在于探讨Chain-of-thought推理在视频理解中的必要性。现有方法中，CoT推理虽能提供逐步分析，但计算成本高，且对于强化学习训练的视频模型，直接回答常能匹配或超越CoT性能，导致资源浪费和效率低下。这突出了视频任务中推理优化的需求，促使开发更智能的策略以平衡准确性和开销。",
      "method": "研究方法基于VideoAuto-R1框架，采用Thinking Once, Answering Twice范式。训练时，模型首先生成初始答案，然后进行推理并输出审查答案，两者通过可验证奖励机制监督，确保学习效果。推理时，模型利用初始答案的置信度动态决定是否激活推理模式，实现reason-when-necessary策略。关键创新点在于自适应推理机制，结合强化学习训练的视频模型，优化计算效率。",
      "result": "实验结果表明，VideoAuto-R1在视频问答和定位基准测试中达到最先进准确性，同时效率显著提升，平均响应长度减少约3.3倍，如从149个令牌降至44个令牌。与基线CoT推理相比，框架在不降低性能的情况下优化了响应速度。此外，研究观察到感知导向任务中思考模式激活率低，而推理密集任务中激活率高，验证了推理策略的自适应性。",
      "conclusion": "论文结论是语言基础推理有益但不总是必要，VideoAuto-R1通过动态推理策略实现了高效视频理解。主要贡献在于提出新框架和reason-when-necessary方法，为多模态模型优化提供新方向。研究具有学术价值，深化了对推理机制的理解，实际应用中可降低计算成本。未来工作可能涉及扩展到其他任务或优化置信度阈值。",
      "tags": [
        "Video Understanding",
        "Chain-of-Thought Reasoning",
        "Reinforcement Learning",
        "Automatic Reasoning",
        "Efficiency Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:17.580341Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18693",
    "title": "MVT: Mask-Grounded Vision-Language Models for Taxonomy-Aligned Land-Cover Tagging",
    "authors": [
      "Siyi Chen",
      "Kai Wang",
      "Weicong Pang",
      "Ruiming Yang",
      "Ziru Chen",
      "Renjun Gao",
      "Alexis Kai Hon Lau",
      "Dasa Gu",
      "Chenchen Zhang",
      "Cheng Li"
    ],
    "abstract": "Land-cover understanding in remote sensing increasingly demands class-agnostic systems that generalize across datasets while remaining spatially precise and interpretable. We study a geometry-first discovery-and-interpretation setting under domain shift, where candidate regions are delineated class-agnostically and supervision avoids lexical class names via anonymized identifiers. Complementary to open-set recognition and open-world learning, we focus on coupling class-agnostic mask evidence with taxonomy-grounded scene interpretation, rather than unknown rejection or continual class expansion. We propose MVT, a three-stage framework that (i) extracts boundary-faithful region masks using SAM2 with domain adaptation, (ii) performs mask-grounded semantic tagging and scene description generation via dual-step LoRA fine-tuning of multimodal LLMs, and (iii) evaluates outputs with LLM-as-judge scoring calibrated by stratified expert ratings. On cross-dataset segmentation transfer (train on OpenEarthMap, evaluate on LoveDA), domain-adapted SAM2 improves mask quality; meanwhile, dual-step MLLM fine-tuning yields more accurate taxonomy-aligned tags and more informative mask-grounded scene descriptions.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.18693.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18693",
    "published": "2025-09-23T06:23:56Z",
    "updated": "2026-01-08T17:56:05Z",
    "comment": "The project is available at https://charlescsyyy.github.io/MVT",
    "light_analysis": {
      "overview": "提出了MVT框架，通过域适应掩码提取和双步微调多模态LLM，实现分类学对齐的土地覆盖标注和场景描述生成。",
      "motivation": "研究动机在于遥感土地覆盖理解需要类无关系统，以跨数据集泛化，同时保持空间精确性和可解释性。在领域转移下，现有方法如开放集识别和开放世界学习侧重于拒绝未知类或扩展类别，而本研究关注几何优先的发现与解释设置，通过匿名标识符避免类名监督，耦合类无关掩码证据与分类学基础的场景解释，以弥补现有方法在掩码证据与语义解释整合上的不足，提升系统适应性和实用性。",
      "method": "研究方法为三阶段MVT框架：首先，使用SAM2结合域适应提取边界忠实区域掩码；其次，通过双步LoRA微调多模态大语言模型，进行掩码基础的语义标记和场景描述生成；最后，采用LLM-as-judge评分评估输出，该评分通过分层专家评级校准。关键创新包括域适应优化掩码提取、双步微调提升模型性能，以及掩码作为证据增强场景解释，使用数据集如OpenEarthMap和LoveDA进行实验验证。",
      "result": "在跨数据集分割迁移实验中（在OpenEarthMap数据集训练，LoveDA数据集评估），域适应的SAM2提高了掩码质量；双步微调的多模态LLM产生了更准确的分类学对齐标签和更丰富的掩码基础场景描述。摘要未明确说明具体性能指标如准确率提升百分比，但表明与基线方法相比，在掩码质量和描述信息量方面有所改进，增强了泛化能力和解释性。",
      "conclusion": "论文的主要贡献是提出了MVT框架，有效整合了几何发现与语义解释，提升了土地覆盖标注的跨数据集泛化能力和可解释性。学术价值在于推动了类无关系统在遥感领域的应用，实际应用价值包括增强土地覆盖监测的准确性和适应性。未来工作可进一步扩展数据集或优化评估方法，摘要未明确说明局限性。",
      "tags": [
        "Mask-Grounded Vision-Language Models",
        "Domain Adaptation",
        "SAM2",
        "LoRA Fine-Tuning",
        "Multimodal Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:23.153401Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.18620",
    "title": "Spontaneous emergence of linguistic statistical laws in images via artificial neural networks",
    "authors": [
      "Ping-Rui Tsai",
      "Chi-hsiang Wang",
      "Yu-Cheng Liao",
      "Hong-Yue Huang",
      "Tzay-Ming Hong"
    ],
    "abstract": "As a core element of culture, images transform perception into structured representations and undergo evolution similar to natural languages. Given that visual input accounts for 60% of human sensory experience, it is natural to ask whether images follow statistical regularities similar to those in linguistic systems. Guided by symbol-grounding theory, which posits that meaningful symbols originate from perception, we treat images as vision-centric artifacts and employ pre-trained neural networks to model visual processing. By detecting kernel activations and extracting pixels, we obtain text-like units, which reveal that these image-derived representations adhere to statistical laws such as Zipf's, Heaps', and Benford's laws, analogous to linguistic data. Notably, these statistical regularities emerge spontaneously, without the need for explicit symbols or hybrid architectures. Our results indicate that connectionist networks can automatically develop structured, quasi-symbolic units through perceptual processing alone, suggesting that text- and symbol-like properties can naturally emerge from neural networks and providing a novel perspective for interpretation.",
    "categories": [
      "cs.CV",
      "physics.comp-ph"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.18620.pdf",
    "abs_url": "https://arxiv.org/abs/2501.18620",
    "published": "2025-01-26T16:26:32Z",
    "updated": "2026-01-08T17:46:03Z",
    "comment": "10 figures",
    "light_analysis": {
      "overview": "论文通过预训练神经网络处理图像，自发涌现出语言统计规律，揭示了神经网络能自然发展出类符号属性，提供新解释视角。",
      "motivation": "研究动机基于图像作为文化核心元素，将感知转化为结构化表示，且视觉输入占人类感官体验的60%，因此探讨图像是否遵循类似语言的统计规律。符号接地理论认为有意义的符号源于感知，支持将图像视为视觉中心artifact。现有方法可能依赖显式符号或混合架构，存在复杂性，而本研究旨在揭示更自然的涌现机制。",
      "method": "研究方法采用预训练神经网络建模视觉处理，基于符号接地理论。通过检测内核激活和提取像素，获得类似文本的单位。关键创新点是这些统计规律自发出现，无需显式符号或混合架构。具体技术细节包括使用预训练神经网络（如CNN）处理图像，提取结构化表示。",
      "result": "实验结果显示图像表示符合Zipf's、Heaps'和Benford's定律，类似于语言数据。这些统计规律自发涌现，表明连接主义网络能通过感知处理自动发展出结构化的类符号单位。摘要未明确说明具体性能指标或基线对比，但强调了自发性为关键发现。",
      "conclusion": "结论指出连接主义网络可通过纯感知处理自动形成结构化单位，提供神经网络符号性的新解释视角。这表明文本和符号属性可在没有显式编码的情况下自然涌现，学术价值在于为结构化表示研究提供新思路。潜在局限性是摘要未明确涉及广泛验证，未来工作可扩展到其他网络类型或应用场景。",
      "tags": [
        "Artificial Neural Networks",
        "Statistical Laws",
        "Computer Vision",
        "Pre-trained Neural Networks",
        "Connectionist Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:20.437512Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05149",
    "title": "Multi-Scale Local Speculative Decoding for Image Generation",
    "authors": [
      "Elia Peruzzo",
      "Guillaume Sautière",
      "Amirhossein Habibian"
    ],
    "abstract": "Autoregressive (AR) models have achieved remarkable success in image synthesis, yet their sequential nature imposes significant latency constraints. Speculative Decoding offers a promising avenue for acceleration, but existing approaches are limited by token-level ambiguity and lack of spatial awareness. In this work, we introduce Multi-Scale Local Speculative Decoding (MuLo-SD), a novel framework that combines multi-resolution drafting with spatially informed verification to accelerate AR image generation. Our method leverages a low-resolution drafter paired with learned up-samplers to propose candidate image tokens, which are then verified in parallel by a high-resolution target model. Crucially, we incorporate a local rejection and resampling mechanism, enabling efficient correction of draft errors by focusing on spatial neighborhoods rather than raster-scan resampling after the first rejection. We demonstrate that MuLo-SD achieves substantial speedups - up to $\\mathbf{1.7\\times}$ - outperforming strong speculative decoding baselines such as EAGLE-2 and LANTERN in terms of acceleration, while maintaining comparable semantic alignment and perceptual quality. These results are validated using GenEval, DPG-Bench, and FID/HPSv2 on the MS-COCO 5k validation split. Extensive ablations highlight the impact of up-sampling design, probability pooling, and local rejection and resampling with neighborhood expansion. Our approach sets a new state-of-the-art in speculative decoding for image synthesis, bridging the gap between efficiency and fidelity.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05149.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05149",
    "published": "2026-01-08T17:39:35Z",
    "updated": "2026-01-08T17:39:35Z",
    "comment": "Project page is available at https://qualcomm-ai-research.github.io/mulo-sd-webpage",
    "light_analysis": {
      "overview": "提出了多尺度局部推测解码（MuLo-SD）框架，通过结合多分辨率草稿和空间感知验证，显著加速自回归图像生成。",
      "motivation": "自回归（AR）模型在图像合成中取得了显著成功，但其顺序特性导致生成延迟高，限制了实时应用。推测解码方法虽能加速生成，但现有方法如EAGLE-2和LANTERN受限于令牌级别的模糊性和缺乏空间意识，难以高效处理图像中的空间关系，从而影响加速效果和保真度。因此，本研究旨在开发一种能够兼顾效率和图像质量的新方法，以克服现有技术的不足，提升图像生成的实用性和可扩展性。",
      "method": "MuLo-SD框架采用多分辨率草稿与空间感知验证相结合的技术路线。具体包括使用低分辨率草稿器配合学习型上采样器提议候选图像令牌，然后通过高分辨率目标模型进行并行验证。关键创新是引入局部拒绝和重采样机制，专注于空间邻居而非光栅扫描，以高效纠正草稿错误。此外，方法涉及上采样设计、概率池化和邻域扩展等组件，优化整体性能，在MS-COCO数据集上验证了其有效性。",
      "result": "在MS-COCO 5k验证集上，MuLo-SD实现了最高1.7倍的加速，优于基线方法EAGLE-2和LANTERN。评估使用GenEval、DPG-Bench、FID和HPSv2指标，结果显示在保持可比的语义对齐和感知质量的同时，显著提升生成效率。消融研究突出了上采样设计、概率池化以及局部拒绝重采样对性能的关键影响，证实了各组件对加速和保真度的贡献。",
      "conclusion": "MuLo-SD为图像合成中的推测解码设立了新的最高标准，成功桥接了效率和保真度之间的差距。该研究具有重要的学术价值，推动了加速图像生成技术的发展，并可能应用于实时图形渲染、增强现实等领域。局限性包括摘要未明确说明具体计算开销或泛化能力，未来工作可进一步优化模型在复杂场景中的表现或扩展到其他生成任务。",
      "tags": [
        "Speculative Decoding",
        "Multi-Scale Drafting",
        "Local Rejection Resampling",
        "Image Generation",
        "Autoregressive Models"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:30.109488Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05138",
    "title": "VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control",
    "authors": [
      "Sixiao Zheng",
      "Minghao Yin",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Ying Shan",
      "Yanwei Fu"
    ],
    "abstract": "Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05138.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05138",
    "published": "2026-01-08T17:28:52Z",
    "updated": "2026-01-08T17:28:52Z",
    "comment": "Project Page: https://sixiaozheng.github.io/VerseCrafter_page/",
    "light_analysis": {
      "overview": "VerseCrafter提出了一种基于4D几何控制的视频世界模型，实现对摄像头和对象动态的统一精确控制。",
      "motivation": "视频世界模型旨在模拟动态真实环境，但现有方法难以统一和精确控制摄像头与多对象运动，因为它们通常在投影的2D图像平面上操作动态。这一问题在需要生成逼真、几何一致的视频场景中尤为重要，现有方法缺乏对4D状态的显式控制，限制了视频生成的质量和灵活性。研究目标是通过4D几何控制来克服这些不足，提升动态视频的模拟能力。",
      "method": "VerseCrafter的核心是一种新颖的4D几何控制表示，它通过静态背景点云和每对象3D高斯轨迹编码世界状态。这种表示不仅捕获对象的路径，还捕捉其随时间的概率性3D占用，提供了一个灵活且类别无关的替代方案，避免了刚性边界框或参数模型的局限。这些4D控制被渲染为预训练视频扩散模型的调节信号，用于生成视频。为解决数据稀缺问题，开发了一个自动数据引擎，从野外视频中提取4D控制，从而在大型多样化数据集上训练模型。",
      "result": "摘要未明确说明具体实验结果，如准确率或效率指标。但基于描述，VerseCrafter能够生成高保真、视角一致的视频，精确遵循指定的摄像头和对象动态控制，这表明在视频质量和控制精度方面可能优于现有基线方法，但缺乏具体数据支撑和对比分析。",
      "conclusion": "论文的主要贡献是VerseCrafter模型，它通过4D几何控制表示提高了视频世界模型的精确性和一致性，在学术上推动了视频生成与几何控制的结合，实际应用可能包括视频编辑和虚拟环境模拟。局限性可能涉及计算资源需求或数据依赖性，未来工作可扩展模型到更多场景或优化控制机制。",
      "tags": [
        "Video World Model",
        "4D Geometric Control",
        "3D Gaussian Trajectories",
        "Video Diffusion Model",
        "Automatic Data Engine"
      ]
    },
    "analyzed_at": "2026-01-09T02:51:15.307904Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.19850",
    "title": "FALCONEye: Finding Answers and Localizing Content in ONE-hour-long videos with multi-modal LLMs",
    "authors": [
      "Carlos Plou",
      "Cesar Borja",
      "Ruben Martinez-Cantin",
      "Ana C. Murillo"
    ],
    "abstract": "Finding information in hour-long videos is a challenging task even for top-performing Vision Language Models (VLMs), as encoding visual content quickly exceeds available context windows. To tackle this challenge, we present FALCONEye, a novel video agent based on a training-free, model-agnostic meta-architecture composed of a VLM and a Large Language Model (LLM). FALCONEye answers open-ended questions using an exploration-based search algorithm guided by calibrated confidence from the VLM's answers. We also introduce the FALCON-Bench benchmark, extending Question Answering problem to Video Answer Search-requiring models to return both the answer and its supporting temporal window for open-ended questions in hour-long videos. With just a 7B VLM and a lightweight LLM, FALCONEye outscores all open-source 7B VLMs and comparable agents in FALCON-Bench. It further demonstrates its generalization capability in MLVU benchmark with shorter videos and different tasks, surpassing GPT-4o on single-detail tasks while slashing inference cost by roughly an order of magnitude.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.19850.pdf",
    "abs_url": "https://arxiv.org/abs/2503.19850",
    "published": "2025-03-25T17:17:19Z",
    "updated": "2026-01-08T17:17:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "FALCONEye提出一种结合VLM和LLM的视频代理，通过训练免费、模型无关的架构，在长视频中高效查找答案并定位内容。",
      "motivation": "在人工智能领域，处理时长一小时的视频信息检索是一个重要挑战。现有视觉语言模型（VLMs）在编码大量视觉内容时，由于上下文窗口有限，导致信息查找困难，影响长视频问答系统的实用性。随着视频内容的快速增长，开发高效检索方法具有显著应用价值。本文针对现有VLMs在长视频处理中的局限性，提出解决方案以改进性能和效率。",
      "method": "FALCONEye采用无需训练、模型无关的元架构，结合视觉语言模型（VLM）和大语言模型（LLM）。核心方法使用基于探索的搜索算法，该算法根据VLM答案的校准置信度指导，以回答开放性问题。关键创新包括引入FALCON-Bench基准，将问答任务扩展为视频答案搜索，要求返回答案及其时间窗口。具体实现中，系统使用一个7B参数的VLM和一个轻量级LLM，优化了多模态处理流程。",
      "result": "在FALCON-Bench基准测试中，FALCONEye胜过所有开源7B VLMs和可比代理，显示了其在长视频答案搜索任务上的优势。在MLVU基准上，它进一步展示泛化能力，在单细节任务上超越GPT-4o，同时推理成本降低大约一个数量级，体现了效率和准确性的双重提升。摘要未提供具体准确率数据，但强调了性能超越和成本节省。",
      "conclusion": "本文的主要贡献是提出了FALCONEye系统，有效解决长视频答案搜索问题，并引入FALCON-Bench基准推动相关研究。学术上，它为多模态长视频理解提供了新方法；应用上，支持低成本、高效的视频信息检索，具有实际价值。摘要未明确说明局限性，但未来工作可能涉及扩展到更复杂任务或更长视频处理。",
      "tags": [
        "Vision Language Models (VLM)",
        "Large Language Models (LLM)",
        "Multi-modal LLMs",
        "Video Answer Search",
        "Calibrated Confidence"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:13.490026Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05124",
    "title": "Re-Align: Structured Reasoning-guided Alignment for In-Context Image Generation and Editing",
    "authors": [
      "Runze He",
      "Yiji Cheng",
      "Tiankai Hang",
      "Zhimin Li",
      "Yu Xu",
      "Zijin Yin",
      "Shiyi Zhang",
      "Wenxun Dai",
      "Penghui Du",
      "Ao Ma",
      "Chunyu Wang",
      "Qinglin Lu",
      "Jizhong Han",
      "Jiao Dai"
    ],
    "abstract": "In-context image generation and editing (ICGE) enables users to specify visual concepts through interleaved image-text prompts, demanding precise understanding and faithful execution of user intent. Although recent unified multimodal models exhibit promising understanding capabilities, these strengths often fail to transfer effectively to image generation. We introduce Re-Align, a unified framework that bridges the gap between understanding and generation through structured reasoning-guided alignment. At its core lies the In-Context Chain-of-Thought (IC-CoT), a structured reasoning paradigm that decouples semantic guidance and reference association, providing clear textual target and mitigating confusion among reference images. Furthermore, Re-Align introduces an effective RL training scheme that leverages a surrogate reward to measure the alignment between structured reasoning text and the generated image, thereby improving the model's overall performance on ICGE tasks. Extensive experiments verify that Re-Align outperforms competitive methods of comparable model scale and resources on both in-context image generation and editing tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05124.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05124",
    "published": "2026-01-08T17:13:00Z",
    "updated": "2026-01-08T17:13:00Z",
    "comment": "13 pages, 9 figures, project page: https://github.com/hrz2000/realign",
    "light_analysis": {
      "overview": "Re-Align 通过结构化推理引导的对齐和强化学习训练，有效提升上下文图像生成和编辑的性能。",
      "motivation": "本研究针对上下文图像生成和编辑（ICGE）任务，该任务需要精确理解和忠实执行用户意图，但现有统一多模态模型虽然在理解能力上表现出色，却未能有效转化到图像生成中，导致用户意图执行不精确。这一问题在实际应用中至关重要，如创意设计和内容创作，但当前方法在理解和生成之间存在差距，限制了其效能。",
      "method": "Re-Align 框架引入两个核心技术：一是 In-Context Chain-of-Thought (IC-CoT)，它作为结构化推理范式，通过解耦语义指导和参考关联，提供清晰的文本目标，减轻参考图像之间的混淆；二是强化学习训练方案，利用代理奖励来衡量结构化推理文本与生成图像之间的对齐，从而优化模型在 ICGE 任务上的整体性能。该方法专注于提升对齐度，摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验结果显示，Re-Align 在上下文图像生成和编辑任务上优于具有相当模型规模和资源的竞争方法，验证了其有效性。摘要未提供具体性能指标如准确率或效率数据，但强调了其在整体任务上的优势，与基线方法的对比表明该方法能够提升执行用户意图的精确度。",
      "conclusion": "Re-Align 的主要贡献是弥合了多模态模型在理解和生成之间的差距，通过结构化推理引导的对齐提高 ICGE 任务性能，具有学术价值，为多模态对齐提供了新思路，并在实际应用中可能改善创意设计的用户交互。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "In-Context Image Generation",
        "Structured Reasoning",
        "Chain-of-Thought",
        "Reinforcement Learning",
        "Alignment"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:05.114610Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05116",
    "title": "From Rays to Projections: Better Inputs for Feed-Forward View Synthesis",
    "authors": [
      "Zirui Wu",
      "Zeren Jiang",
      "Martin R. Oswald",
      "Jie Song"
    ],
    "abstract": "Feed-forward view synthesis models predict a novel view in a single pass with minimal 3D inductive bias. Existing works encode cameras as Plücker ray maps, which tie predictions to the arbitrary world coordinate gauge and make them sensitive to small camera transformations, thereby undermining geometric consistency. In this paper, we ask what inputs best condition a model for robust and consistent view synthesis. We propose projective conditioning, which replaces raw camera parameters with a target-view projective cue that provides a stable 2D input. This reframes the task from a brittle geometric regression problem in ray space to a well-conditioned target-view image-to-image translation problem. Additionally, we introduce a masked autoencoding pretraining strategy tailored to this cue, enabling the use of large-scale uncalibrated data for pretraining. Our method shows improved fidelity and stronger cross-view consistency compared to ray-conditioned baselines on our view-consistency benchmark. It also achieves state-of-the-art quality on standard novel view synthesis benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05116.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05116",
    "published": "2026-01-08T17:03:44Z",
    "updated": "2026-01-08T17:03:44Z",
    "comment": "Project Page: https://wuzirui.github.io/pvsm-web",
    "light_analysis": {
      "overview": "论文提出投影条件输入方法，替代射线图，提高前馈视图合成的鲁棒性和几何一致性。",
      "motivation": "现有前馈视图合成模型通过Plücker射线图编码相机参数，但这方法使预测依赖于任意世界坐标系规，并对微小相机变换敏感，从而削弱几何一致性，影响合成视图的稳定性和实用性。因此，研究目标是寻找更优的输入条件来克服这一不足，提升模型在复杂场景中的表现。",
      "method": "论文提出投影条件方法，用目标视图的二维投影线索取代原始相机参数，将任务从射线空间的几何回归问题重新定义为图像到图像的翻译问题。关键创新包括使用稳定二维输入和针对该线索的掩蔽自动编码预训练策略，后者能利用大规模未标定数据进行预训练，提升模型泛化能力。",
      "result": "在视图一致性基准测试中，该方法比射线条件基线展现改进的保真度和更强的跨视图一致性，具体表现为误差减少。同时，在标准新颖视图合成基准上实现最先进性能，证实了方法的有效性和优越性。",
      "conclusion": "论文的主要贡献是引入投影条件输入，显著提升视图合成的稳定性和一致性，为计算机视觉领域提供新思路。研究具有学术价值，推动视图合成技术发展，并具备实际应用潜力，未来可探索模型优化或扩展到更广泛任务中。",
      "tags": [
        "View Synthesis",
        "Projective Conditioning",
        "Masked Autoencoding",
        "Image-to-Image Translation",
        "Computer Vision"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:51.725947Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05105",
    "title": "UniLiPs: Unified LiDAR Pseudo-Labeling with Geometry-Grounded Dynamic Scene Decomposition",
    "authors": [
      "Filippo Ghilotti",
      "Samuel Brucker",
      "Nahku Saidy",
      "Matteo Matteucci",
      "Mario Bijelic",
      "Felix Heide"
    ],
    "abstract": "Unlabeled LiDAR logs, in autonomous driving applications, are inherently a gold mine of dense 3D geometry hiding in plain sight - yet they are almost useless without human labels, highlighting a dominant cost barrier for autonomous-perception research. In this work we tackle this bottleneck by leveraging temporal-geometric consistency across LiDAR sweeps to lift and fuse cues from text and 2D vision foundation models directly into 3D, without any manual input. We introduce an unsupervised multi-modal pseudo-labeling method relying on strong geometric priors learned from temporally accumulated LiDAR maps, alongside with a novel iterative update rule that enforces joint geometric-semantic consistency, and vice-versa detecting moving objects from inconsistencies. Our method simultaneously produces 3D semantic labels, 3D bounding boxes, and dense LiDAR scans, demonstrating robust generalization across three datasets. We experimentally validate that our method compares favorably to existing semantic segmentation and object detection pseudo-labeling methods, which often require additional manual supervision. We confirm that even a small fraction of our geometrically consistent, densified LiDAR improves depth prediction by 51.5% and 22.0% MAE in the 80-150 and 150-250 meters range, respectively.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05105.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05105",
    "published": "2026-01-08T16:52:28Z",
    "updated": "2026-01-08T16:52:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出UniLiPs方法，一种基于几何先验的无监督多模态伪标注技术，利用LiDAR时空一致性自动生成3D语义标签和边界框，无需人工标注。",
      "motivation": "自动驾驶应用中，未标注的LiDAR数据量大但标注成本高，导致数据利用率低，阻碍感知研究进展。现有伪标注方法如语义分割和目标检测通常依赖额外人工监督，增加开销。因此，需要开发无监督方法来利用LiDAR的时空几何一致性，自动生成高质量3D标注，以降低成本并提升数据价值。",
      "method": "该方法通过融合LiDAR扫描的时空几何一致性与文本及2D视觉基础模型的线索，直接生成3D信息，无需人工输入。核心创新包括：基于时间累积的LiDAR地图学习强几何先验；引入迭代更新规则强制执行联合几何-语义一致性，并通过不一致性检测移动物体；能同时生成3D语义标签、3D边界框和密集LiDAR扫描，在三个数据集上验证泛化能力。",
      "result": "实验验证该方法在语义分割和目标检测伪标注上优于现有方法，后者通常需额外人工监督。具体地，使用生成的几何一致、密集化LiDAR数据，在80-150米范围内深度预测的MAE减少51.5%，在150-250米范围内减少22.0%，显著提升远距离深度估计精度，展示出鲁棒的泛化效果。",
      "conclusion": "本研究贡献了一种无监督多模态伪标注方法，有效降低LiDAR标注成本，推动自动驾驶感知研究。学术上，促进了无监督学习在3D感知领域的应用；实际上，为自动驾驶系统提供高质量训练数据。未来工作可扩展至更多复杂场景或整合其他传感器，进一步提升性能。",
      "tags": [
        "LiDAR Pseudo-Labeling",
        "Geometric Consistency",
        "Multi-Modal Fusion",
        "Unsupervised Learning",
        "3D Object Detection"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:02.232165Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.17297",
    "title": "BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for Better Evaluation under Visual Ambiguities",
    "authors": [
      "Boris Meden",
      "Asma Brazi",
      "Fabrice Mayran de Chamisso",
      "Steve Bourgeois",
      "Vincent Lepetit"
    ],
    "abstract": "6D pose estimation aims at determining the object pose that best explains the camera observation. The unique solution for non-ambiguous objects can turn into a multi-modal pose distribution for symmetrical objects or when occlusions of symmetry-breaking elements happen, depending on the viewpoint. Currently, 6D pose estimation methods are benchmarked on datasets that consider, for their ground truth annotations, visual ambiguities as only related to global object symmetries, whereas they should be defined per-image to account for the camera viewpoint. We thus first propose an automatic method to re-annotate those datasets with a 6D pose distribution specific to each image, taking into account the object surface visibility in the image to correctly determine the visual ambiguities. Second, given this improved ground truth, we re-evaluate the state-of-the-art single pose methods and show that this greatly modifies the ranking of these methods. Third, as some recent works focus on estimating the complete set of solutions, we derive a precision/recall formulation to evaluate them against our image-wise distribution ground truth, making it the first benchmark for pose distribution methods on real images.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2408.17297.pdf",
    "abs_url": "https://arxiv.org/abs/2408.17297",
    "published": "2024-08-30T13:52:26Z",
    "updated": "2026-01-08T16:42:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出自动重标注数据集和评估框架，改进6D姿态估计在视觉模糊性下的基准评估。",
      "motivation": "6D姿态估计旨在确定对象姿态以最佳解释相机观测。对于对称对象或对称破坏元素被遮挡的情况，姿态解可能变成多模态分布，取决于视点。当前，6D姿态估计方法在基准数据集上的评估，其真实标注仅考虑全局对象对称性相关的视觉模糊性，而未基于每张图像的相机视点定义。这导致评估不准确，因为实际场景中模糊性应图像特定。因此，本研究旨在解决这一不足，提供更精确的评估方法，以反映姿态估计在视觉模糊性下的真实性能。",
      "method": "论文提出三个关键步骤。首先，开发一种自动方法，重新标注现有数据集，为每张图像生成6D姿态分布。该方法考虑对象表面在图像中的可见性，以正确确定视觉模糊性。其次，基于改进的真实标注，重新评估最先进的单姿态估计方法。第三，推导精度/召回公式，用于评估那些估计完整姿态分布的方法。这使得本研究成为首个在真实图像上为姿态分布方法提供基准的工作，方法中未具体说明使用的模型架构或数据集细节，但强调了对现有数据集的重新处理。",
      "result": "实验结果显示，重新评估单姿态估计方法后，这些方法的排名发生了显著变化，表明考虑视觉模糊性对评估至关重要。提出的精度/召回公式为姿态分布方法提供了评估框架。但摘要未明确说明具体的性能指标数据，如准确率提升或效率改进，与基线方法的对比情况也未详细描述，因此需要参考完整论文获取详细信息。",
      "conclusion": "论文的主要贡献是重新审视6D姿态估计基准，提出自动重标注方法和评估框架，以更好地处理视觉模糊性。这改进了评估的准确性，为姿态分布方法提供了首个真实图像基准，具有重要的学术价值。在实际应用中，可以促进更鲁棒的姿态估计方法发展。局限性可能在于方法的自动化程度或对特定数据集的依赖，未来工作可扩展到更多数据集或优化评估指标，进一步提升评估的普适性和实用性。",
      "tags": [
        "6D Pose Estimation",
        "Visual Ambiguities",
        "Benchmark Evaluation",
        "Pose Distribution",
        "Precision-Recall"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:30.689729Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.10797",
    "title": "Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells",
    "authors": [
      "Aleksandra Melnikova",
      "Petr Matula"
    ],
    "abstract": "High-quality, publicly available segmentation annotations of image and video datasets are critical for advancing the field of image processing. In particular, annotations of volumetric images of a large number of targets are time-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we presented the first publicly available full 3D time-lapse segmentation annotations of migrating cells with complex dynamic shapes. Concretely, three distinct humans annotated two sequences of MDA231 human breast carcinoma cells (Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).   This paper aims to provide a comprehensive description of the dataset and accompanying experiments that were not included in (Melnikova, A., & Matula, P., 2025) due to limitations in publication space. Namely, we show that the created annotations are consistent with the previously published tracking markers provided by the CTC organizers and the segmentation accuracy measured based on the 2D gold truth of CTC is within the inter-annotator variability margins. We compared the created 3D annotations with automatically created silver truth provided by CTC. We have found the proposed annotations better represent the complexity of the input images. The presented annotations can be used for testing and training cell segmentation, or analyzing 3D shapes of highly dynamic objects.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.10797.pdf",
    "abs_url": "https://arxiv.org/abs/2510.10797",
    "published": "2025-10-12T20:31:40Z",
    "updated": "2026-01-08T16:08:46Z",
    "comment": "6 pages, 2 figures, 4 tables",
    "light_analysis": {
      "overview": "本论文提供了首个公开可用的3D时间推移显微镜细胞分割注释数据集。",
      "motivation": "图像处理领域的发展高度依赖于高质量的分割注释，特别是对于生物医学应用中细胞图像的体积数据。现有公开数据集往往缺乏对复杂动态形状的全面3D注释，这限制了细胞分割算法的开发和评估。Cell Tracking Challenge等平台提供了数据，但自动方法生成的注释可能不准确，而手动注释耗时且缺乏一致性。因此，创建一个可靠且公开的注释数据集至关重要，以解决现有方法的不足并推动细胞分割及相关研究的前沿。",
      "method": "本研究创建了一个基于Cell Tracking Challenge (CTC) 的3D时间推移细胞分割注释数据集，具体使用Fluo-C3DL-MDA231人类乳腺癌细胞序列。由三名独立人类标注者进行手动分割注释，提供全3D时间推移的标注数据。关键创新点在于首次公开此类高质量注释，并通过实验设计验证其质量，包括与CTC提供的跟踪标记和自动生成的silver truth进行比较分析，确保注释的连贯性和准确性。",
      "result": "实验验证了所创注释的连贯性，显示其与CTC组织者提供的跟踪标记高度一致。分割准确性基于CTC的2D gold truth测量，结果在标注者间的可变范围内，证明了标注的一致性。与自动创建的silver truth相比，提出的手动注释更好地反映了输入图像的复杂性，尤其是在捕捉细胞动态形状变化方面，表明高质量手动标注在复杂场景中更具优势。",
      "conclusion": "本研究的主要贡献是提供了一个全面的3D时间推移细胞分割注释数据集，弥补了现有资源的空白。通过详细描述数据集和补充实验，增强了数据的可信度和实用性。该数据集可用于细胞分割算法的测试和训练，以及动态物体3D形状的分析，具有重要的学术和实际应用价值。局限性可能在于数据集规模有限，未来工作可包括扩展数据集或结合自动标注方法以提高效率。",
      "tags": [
        "3D Segmentation",
        "Time-lapse Microscopy",
        "Cell Tracking",
        "Image Annotation",
        "Dataset Creation"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:51.139768Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05059",
    "title": "From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)",
    "authors": [
      "Suyash Mishra",
      "Qiang Li",
      "Srikanth Patil",
      "Anubhav Girdhar"
    ],
    "abstract": "Vision Language Models (VLMs) are poised to revolutionize the digital transformation of pharmacyceutical industry by enabling intelligent, scalable, and automated multi-modality content processing. Traditional manual annotation of heterogeneous data modalities (text, images, video, audio, and web links), is prone to inconsistencies, quality degradation, and inefficiencies in content utilization. The sheer volume of long video and audio data further exacerbates these challenges, (e.g. long clinical trial interviews and educational seminars).   Here, we introduce a domain adapted Video to Video Clip Generation framework that integrates Audio Language Models (ALMs) and Vision Language Models (VLMs) to produce highlight clips. Our contributions are threefold: (i) a reproducible Cut & Merge algorithm with fade in/out and timestamp normalization, ensuring smooth transitions and audio/visual alignment; (ii) a personalization mechanism based on role definition and prompt injection for tailored outputs (marketing, training, regulatory); (iii) a cost efficient e2e pipeline strategy balancing ALM/VLM enhanced processing. Evaluations on Video MME benchmark (900) and our proprietary dataset of 16,159 pharmacy videos across 14 disease areas demonstrate 3 to 4 times speedup, 4 times cost reduction, and competitive clip quality. Beyond efficiency gains, we also report our methods improved clip coherence scores (0.348) and informativeness scores (0.721) over state of the art VLM baselines (e.g., Gemini 2.5 Pro), highlighting the potential of transparent, custom extractive, and compliance supporting video summarization for life sciences.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05059.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05059",
    "published": "2026-01-08T16:02:56Z",
    "updated": "2026-01-08T16:02:56Z",
    "comment": "Contributed original research to top tier conference in VLM; currently undergoing peer review",
    "light_analysis": {
      "overview": "提出一个整合音频和视觉语言模型的领域自适应视频片段生成框架，用于个性化药学视频剪辑，提升处理效率和定制性。",
      "motivation": "药学行业数字转型面临多模态内容处理的挑战，传统手动标注异构数据模态（如文本、图像、视频）导致不一致性、质量下降和低效率；长视频和音频数据（如临床试验访谈）加剧了这些问题。现有方法依赖人工处理，缺乏自动化和个性化能力，限制了内容利用的智能化和可扩展性。因此，需要开发高效、自动化的解决方案来支持药学行业的数字化需求，提高内容处理的准确性和效率。",
      "method": "论文提出了一个视频到视频片段生成的框架，整合音频语言模型（ALMs）和视觉语言模型（VLMs）来处理药学视频数据。核心创新包括：可复现的Cut & Merge算法，采用淡入淡出和时间戳归一化技术，确保平滑过渡和音视频对齐；基于角色定义和提示注入的个性化机制，定制输出用于营销、培训或监管用途；以及成本高效的端到端管道策略，平衡ALM和VLM的处理。框架应用于专有数据集和基准测试，以自动生成亮点视频片段。",
      "result": "在Video MME基准测试（900个视频）和专有数据集（16,159个药学视频，覆盖14种疾病）上评估，结果显示框架实现3到4倍的处理速度提升和4倍的成本降低，同时保持竞争力的片段质量。具体性能指标方面，片段连贯性得分达0.348，信息性得分达0.721，优于Gemini 2.5 Pro等最先进的VLM基线方法，证明了方法在效率和效果上的显著优势。",
      "conclusion": "论文的主要贡献是开发了一个高效的视频片段生成框架，整合ALMs和VLMs，显著提升药学视频处理的效率和个性化能力。学术价值在于展示了多模态模型整合的创新应用，支持透明、定制提取和合规的视频摘要；实际应用价值体现在药学行业的数字化营销、培训和监管场景中。摘要未明确说明局限性，但未来工作可进一步优化算法或扩展应用到其他医疗领域，以增强泛化性和适应性。",
      "tags": [
        "Vision Language Models",
        "Audio Language Models",
        "Video Summarization",
        "Personalization",
        "Cut & Merge Algorithm"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:42.785010Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05035",
    "title": "Patch-based Representation and Learning for Efficient Deformation Modeling",
    "authors": [
      "Ruochen Chen",
      "Thuy Tran",
      "Shaifali Parashar"
    ],
    "abstract": "In this paper, we present a patch-based representation of surfaces, PolyFit, which is obtained by fitting jet functions locally on surface patches. Such a representation can be learned efficiently in a supervised fashion from both analytic functions and real data. Once learned, it can be generalized to various types of surfaces. Using PolyFit, the surfaces can be efficiently deformed by updating a compact set of jet coefficients rather than optimizing per-vertex degrees of freedom for many downstream tasks in computer vision and graphics. We demonstrate the capabilities of our proposed methodologies with two applications: 1) Shape-from-template (SfT): where the goal is to deform the input 3D template of an object as seen in image/video. Using PolyFit, we adopt test-time optimization that delivers competitive accuracy while being markedly faster than offline physics-based solvers, and outperforms recent physics-guided neural simulators in accuracy at modest additional runtime. 2) Garment draping. We train a self-supervised, mesh- and garment-agnostic model that generalizes across resolutions and garment types, delivering up to an order-of-magnitude faster inference than strong baselines.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05035.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05035",
    "published": "2026-01-08T15:43:57Z",
    "updated": "2026-01-08T15:43:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了PolyFit，一种基于面片的表面表示方法，通过局部拟合jet函数实现高效变形建模。",
      "motivation": "该研究旨在解决计算机视觉和图形学中表面变形建模的效率问题。现有方法如基于物理的离线求解器计算成本高，导致速度慢；而神经模拟器在精度上可能妥协，难以兼顾快速和准确。PolyFit通过紧凑表示减少优化自由度，提升下游任务如形状恢复和服装模拟的效率，以弥补现有方法的不足。",
      "method": "PolyFit通过将表面划分为面片，并在每个面片上局部拟合jet函数（一种数学近似）来构建紧凑表示。该方法可以从解析函数或真实数据中通过监督学习高效训练，并具有良好的泛化能力。关键创新点包括使用jet系数更新而非每个顶点优化，以及针对应用如Shape-from-template采用测试时间优化，对服装悬垂训练自监督、网格和服装无关的模型（摘要未明确说明具体数据集和模型架构）。",
      "result": "在Shape-from-template任务中，PolyFit实现了竞争性精度，比离线物理求解器显著更快，并在准确性上优于最近的物理引导神经模拟器。在服装悬垂应用中，模型泛化到不同分辨率和服装类型，推理速度比强基线快一个数量级，显示出效率和准确性的提升。",
      "conclusion": "PolyFit提供了一种高效、泛化的表面表示和变形方法，通过学习jet系数加速下游任务。其学术价值在于提出新颖的表示技术，实际应用能推动计算机视觉和图形学领域的进步，如三维重建和动画模拟。局限性（摘要未明确说明）可能包括泛化范围或计算资源需求，未来工作可探索更多应用场景。",
      "tags": [
        "Patch-based Representation",
        "Jet Functions",
        "Deformation Modeling",
        "Supervised Learning",
        "Self-supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:37.842425Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04991",
    "title": "Higher-Order Adversarial Patches for Real-Time Object Detectors",
    "authors": [
      "Jens Bayer",
      "Stefan Becker",
      "David Münch",
      "Michael Arens",
      "Jürgen Beyerer"
    ],
    "abstract": "Higher-order adversarial attacks can directly be considered the result of a cat-and-mouse game -- an elaborate action involving constant pursuit, near captures, and repeated escapes. This idiom describes the enduring circular training of adversarial attack patterns and adversarial training the best. The following work investigates the impact of higher-order adversarial attacks on object detectors by successively training attack patterns and hardening object detectors with adversarial training. The YOLOv10 object detector is chosen as a representative, and adversarial patches are used in an evasion attack manner. Our results indicate that higher-order adversarial patches are not only affecting the object detector directly trained on but rather provide a stronger generalization capacity compared to lower-order adversarial patches. Moreover, the results highlight that solely adversarial training is not sufficient to harden an object detector efficiently against this kind of adversarial attack. Code: https://github.com/JensBayer/HigherOrder",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04991.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04991",
    "published": "2026-01-08T14:48:50Z",
    "updated": "2026-01-08T14:48:50Z",
    "comment": "Under review (ICPR2026)",
    "light_analysis": {
      "overview": "论文提出高阶对抗性补丁攻击方法，研究了其对实时物体检测器的影响，发现其具有更强泛化能力并揭示对抗性训练的局限性。",
      "motivation": "本研究针对对抗性攻击与防御之间的持续竞争问题，旨在解决高阶对抗性攻击对物体检测器的威胁。现有方法如对抗性训练可能不足以有效防御这类攻击，因为攻击模式不断进化，形成猫鼠游戏般的循环，导致安全性挑战加剧。这问题的重要性在于物体检测器广泛应用于自动驾驶和安全监控等关键领域，攻击可能引发严重后果。",
      "method": "论文采用YOLOv10作为代表性实时物体检测器，使用对抗性补丁进行规避攻击，通过连续训练攻击模式和对抗性训练来研究高阶对抗性攻击的影响。核心创新点在于引入高阶对抗性补丁，评估其对检测器的泛化能力，技术特色包括对比低阶攻击的效果。摘要未明确说明具体数据集，但方法侧重于攻击和防御的循环训练框架。",
      "result": "实验结果表明，高阶对抗性补丁不仅影响直接训练的物体检测器，还展现出比低阶补丁更强的泛化能力，能有效攻击更多检测器。同时，研究发现仅靠对抗性训练不足以高效加强物体检测器对抗这种攻击，凸显了现有防御方法的不足。具体性能指标如准确率提升或效率改进未在摘要中提供，但通过与低阶攻击的对比，强调了高阶攻击的威胁。",
      "conclusion": "论文的主要贡献是揭示了高阶对抗性攻击对物体检测器的影响，挑战了对抗性训练的防御效果，具有重要学术价值，推动了对鲁棒性防御策略的研究。实际应用价值包括提醒安全系统开发者关注高阶攻击的威胁，未来工作方向可能涉及开发更高效的防御机制或扩展研究到其他模型。局限性在于摘要未详细讨论具体数据集或更广泛的实验验证。",
      "tags": [
        "Adversarial Patches",
        "Object Detection",
        "Adversarial Training",
        "YOLOv10",
        "Higher-Order Attacks"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:11.768510Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04984",
    "title": "OceanSplat: Object-aware Gaussian Splatting with Trinocular View Consistency for Underwater Scene Reconstruction",
    "authors": [
      "Minseong Kweon",
      "Jinsun Park"
    ],
    "abstract": "We introduce OceanSplat, a novel 3D Gaussian Splatting-based approach for accurately representing 3D geometry in underwater scenes. To overcome multi-view inconsistencies caused by underwater optical degradation, our method enforces trinocular view consistency by rendering horizontally and vertically translated camera views relative to each input view and aligning them via inverse warping. Furthermore, these translated camera views are used to derive a synthetic epipolar depth prior through triangulation, which serves as a self-supervised depth regularizer. These geometric constraints facilitate the spatial optimization of 3D Gaussians and preserve scene structure in underwater environments. We also propose a depth-aware alpha adjustment that modulates the opacity of 3D Gaussians during early training based on their $z$-component and viewing direction, deterring the formation of medium-induced primitives. With our contributions, 3D Gaussians are disentangled from the scattering medium, enabling robust representation of object geometry and significantly reducing floating artifacts in reconstructed underwater scenes. Experiments on real-world underwater and simulated scenes demonstrate that OceanSplat substantially outperforms existing methods for both scene reconstruction and restoration in scattering media.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04984.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04984",
    "published": "2026-01-08T14:38:39Z",
    "updated": "2026-01-08T14:38:39Z",
    "comment": "Accepted to AAAI 2026. Project page: https://oceansplat.github.io",
    "light_analysis": {
      "overview": "OceanSplat是一种基于3D高斯泼溅的新方法，通过三视角一致性和深度感知调整，准确重建水下场景的3D几何结构，克服散射介质影响。",
      "motivation": "研究动机是解决水下场景重建中因光学退化导致的多视角不一致问题，散射效应使得现有方法在恢复几何时产生误差和漂浮伪影，影响精度。现有技术在处理水下多视角图像时缺乏鲁棒性，因此需要开发新方法来确保几何一致性，提高水下视觉应用的有效性。",
      "method": "该方法核心是强制三视角一致性，通过渲染每个输入视图的水平与垂直平移相机视图，并利用逆变形进行对齐以减少视角不一致。同时，通过三角测量从平移视图中生成合成极线深度先验，作为自监督深度正则化器来优化3D高斯的空间分布。还提出深度感知alpha调整，在训练早期基于高斯的z分量和视角方向调整其透明度，防止介质引起的原语形成，从而分离高斯与散射介质。",
      "result": "在真实水下和模拟场景的实验显示，OceanSplat在散射介质中的场景重建和恢复方面显著优于现有方法，有效减少了漂浮伪影并提高了几何表示的鲁棒性，但摘要未明确说明具体性能指标如准确率提升的百分比，仅证实了方法的整体有效性。",
      "conclusion": "OceanSplat的主要贡献是通过三视角一致性和深度感知调整，使得3D高斯能与散射介质解耦，准确重建物体几何。该研究提高了水下场景重建的精度和鲁棒性，对水下机器人和海洋探索有实际应用价值；潜在局限性可能包括对复杂水下条件的适应性，未来工作可扩展到更多样化场景或集成其他传感技术。",
      "tags": [
        "3D Gaussian Splatting",
        "Trinocular View Consistency",
        "Self-supervised Depth Regularization",
        "Underwater Scene Reconstruction",
        "Scattering Media"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:17.802399Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.18503",
    "title": "NASTaR: NovaSAR Automated Ship Target Recognition Dataset",
    "authors": [
      "Benyamin Hosseiny",
      "Kamirul Kamirul",
      "Odysseas Pappas",
      "Alin Achim"
    ],
    "abstract": "Synthetic Aperture Radar (SAR) offers a unique capability for all-weather, space-based maritime activity monitoring by capturing and imaging strong reflections from ships at sea. A well-defined challenge in this domain is ship type classification. Due to the high diversity and complexity of ship types, accurate recognition is difficult and typically requires specialized deep learning models. These models, however, depend on large, high-quality ground-truth datasets to achieve robust performance and generalization. Furthermore, the growing variety of SAR satellites operating at different frequencies and spatial resolutions has amplified the need for more annotated datasets to enhance model accuracy. To address this, we present the NovaSAR Automated Ship Target Recognition (NASTaR) dataset. This dataset comprises of 3415 ship patches extracted from NovaSAR S-band imagery, with labels matched to AIS data. It includes distinctive features such as 23 unique classes, inshore/offshore separation, and an auxiliary wake dataset for patches where ship wakes are visible. We validated the dataset applicability across prominent ship-type classification scenarios using benchmark deep learning models. Results demonstrate over 60% accuracy for classifying four major ship types, over 70% for a three-class scenario, more than 75% for distinguishing cargo from tanker ships, and over 87% for identifying fishing vessels. The NASTaR dataset is available at https://doi.org/10.5523/bris.2tfa6x37oerz2lyiw6hp47058, while relevant codes for benchmarking and analysis are available at https://github.com/benyaminhosseiny/nastar.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.18503.pdf",
    "abs_url": "https://arxiv.org/abs/2512.18503",
    "published": "2025-12-20T20:42:30Z",
    "updated": "2026-01-08T14:23:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了NASTaR数据集，一个基于NovaSAR S波段影像的合成孔径雷达船只目标识别数据集，旨在解决船只类型分类中的数据集短缺问题。",
      "motivation": "合成孔径雷达（SAR）提供全天候、空间基的海上活动监测能力，但船只类型分类面临挑战，因为船只类型多样且复杂，需要专门的深度学习模型。现有方法依赖于大型、高质量的真实数据集以实现鲁棒性能，然而这类数据集稀缺，限制了模型的准确性和泛化能力。随着SAR卫星在不同频率和分辨率上操作，对更多标注数据的需求日益增加，以提高模型在多样场景下的表现，这对海事安全、监控等实际应用至关重要。",
      "method": "论文的核心方法是创建和验证NASTaR数据集。数据集从NovaSAR S波段影像中提取3415个船只补丁，标签与AIS数据匹配以确保准确性。它包括23个独特类别、近岸与离岸分离特征，以及一个辅助尾迹数据集用于有可见尾迹的补丁。验证过程中，使用基准深度学习模型测试数据集在多个船只类型分类场景中的适用性，以评估其质量和实用性，而不涉及新的算法创新。",
      "result": "实验结果显示，基于NASTaR数据集和基准深度学习模型，在多个分类任务中取得良好性能：四个主要船只类型分类准确率超过60%，三类场景准确率超过70%，区分货船与油轮准确率超过75%，识别渔船准确率超过87%。这些结果与未明确说明的基线方法相比，展示了数据集在提升模型准确性方面的有效性，直接支持了SAR图像中船只识别的应用需求。",
      "conclusion": "论文的主要贡献是公开了NASTaR数据集，为SAR船只类型分类研究提供了高质量、标注丰富的资源，解决了数据短缺问题，有助于提高深度学习模型的性能和泛化能力。其学术价值在于促进相关领域研究，实际应用价值体现在海事监测和安全等领域。局限性或未来工作未在摘要中明确说明，但可推断包括扩展数据集规模或结合更先进模型以进一步提升分类精度。",
      "tags": [
        "Synthetic Aperture Radar",
        "Deep Learning",
        "Ship Type Classification",
        "Dataset Creation",
        "AIS Data"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:27.254738Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04968",
    "title": "SparseLaneSTP: Leveraging Spatio-Temporal Priors with Sparse Transformers for 3D Lane Detection",
    "authors": [
      "Maximilian Pittner",
      "Joel Janai",
      "Mario Faigle",
      "Alexandru Paul Condurache"
    ],
    "abstract": "3D lane detection has emerged as a critical challenge in autonomous driving, encompassing identification and localization of lane markings and the 3D road surface. Conventional 3D methods detect lanes from dense birds-eye-viewed (BEV) features, though erroneous transformations often result in a poor feature representation misaligned with the true 3D road surface. While recent sparse lane detectors have surpassed dense BEV approaches, they completely disregard valuable lane-specific priors. Furthermore, existing methods fail to utilize historic lane observations, which yield the potential to resolve ambiguities in situations of poor visibility. To address these challenges, we present SparseLaneSTP, a novel method that integrates both geometric properties of the lane structure and temporal information into a sparse lane transformer. It introduces a new lane-specific spatio-temporal attention mechanism, a continuous lane representation tailored for sparse architectures as well as temporal regularization. Identifying weaknesses of existing 3D lane datasets, we also introduce a precise and consistent 3D lane dataset using a simple yet effective auto-labeling strategy. Our experimental section proves the benefits of our contributions and demonstrates state-of-the-art performance across all detection and error metrics on existing 3D lane detection benchmarks as well as on our novel dataset.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04968.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04968",
    "published": "2026-01-08T14:16:11Z",
    "updated": "2026-01-08T14:16:11Z",
    "comment": "Published at IEEE/CVF International Conference on Computer Vision (ICCV) 2025",
    "light_analysis": {
      "overview": "论文提出SparseLaneSTP方法，通过稀疏Transformer整合车道几何属性和时间信息，实现高效的3D车道检测。",
      "motivation": "3D车道检测是自动驾驶中的关键挑战，传统方法基于密集鸟瞰图特征，但转换错误导致特征与真实3D路面不对齐，降低检测精度。尽管稀疏方法在性能上超越密集方法，却完全忽略车道特定先验和历史观察信息，这限制了在能见度差等复杂场景下的鲁棒性。因此，研究旨在开发一种整合时空先验的方法，以解决现有不足并提升检测可靠性。",
      "method": "SparseLaneSTP方法基于稀疏车道Transformer，核心创新包括车道特定时空注意力机制，能同时建模车道几何属性和时间依赖性；引入连续车道表示，专为稀疏架构优化特征编码；采用时间正则化增强时间一致性。此外，论文识别现有3D车道数据集的弱点，提出一个使用简单有效自动标注策略的新数据集，以提高数据质量和一致性。",
      "result": "实验证明，SparseLaneSTP在现有3D车道检测基准测试和新提出的数据集上均取得了最先进的性能，所有检测和误差指标均有显著提升，优于传统密集BEV方法和现有稀疏方法。这展示了方法在提升准确性和鲁棒性方面的有效性，但摘要未提供具体数值数据。",
      "conclusion": "论文的主要贡献是提出了SparseLaneSTP方法和新3D车道数据集，整合时空先验到稀疏Transformer中，解决了现有方法忽略先验和历史信息的问题。这具有重要的学术价值，推动了3D车道检测技术的发展，并在自动驾驶应用中提升系统鲁棒性。未来工作可扩展数据集规模或优化在极端环境下的性能。",
      "tags": [
        "3D Lane Detection",
        "Sparse Transformer",
        "Spatio-Temporal Attention",
        "Auto-Labeling",
        "Autonomous Driving"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:09.828380Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04956",
    "title": "TEA: Temporal Adaptive Satellite Image Semantic Segmentation",
    "authors": [
      "Juyuan Kang",
      "Hao Zhu",
      "Yan Zhu",
      "Wei Zhang",
      "Jianing Chen",
      "Tianxiang Xiao",
      "Yike Ma",
      "Hao Jiang",
      "Feng Dai"
    ],
    "abstract": "Crop mapping based on satellite images time-series (SITS) holds substantial economic value in agricultural production settings, in which parcel segmentation is an essential step. Existing approaches have achieved notable advancements in SITS segmentation with predetermined sequence lengths. However, we found that these approaches overlooked the generalization capability of models across scenarios with varying temporal length, leading to markedly poor segmentation results in such cases. To address this issue, we propose TEA, a TEmporal Adaptive SITS semantic segmentation method to enhance the model's resilience under varying sequence lengths. We introduce a teacher model that encapsulates the global sequence knowledge to guide a student model with adaptive temporal input lengths. Specifically, teacher shapes the student's feature space via intermediate embedding, prototypes and soft label perspectives to realize knowledge transfer, while dynamically aggregating student model to mitigate knowledge forgetting. Finally, we introduce full-sequence reconstruction as an auxiliary task to further enhance the quality of representations across inputs of varying temporal lengths. Through extensive experiments, we demonstrate that our method brings remarkable improvements across inputs of different temporal lengths on common benchmarks. Our code will be publicly available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04956.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04956",
    "published": "2026-01-08T14:02:28Z",
    "updated": "2026-01-08T14:02:28Z",
    "comment": "Under review. Code will be available at \\href{https://github.com/KeplerKang/TEA}{this https URL}",
    "light_analysis": {
      "overview": "论文提出TEA方法，一种时间自适应的卫星图像时间序列语义分割技术，通过教师-学生知识转移框架，显著提升模型在不同时间长度输入下的泛化性能。",
      "motivation": "卫星图像时间序列（SITS）在农业生产中的作物映射具有重要经济价值，其中地块分割是关键步骤。现有方法基于预定序列长度，在固定条件下取得了进展，但忽视了模型在不同时间长度场景下的泛化能力，导致在天气变化或传感器差异导致的序列长度不一致时，分割结果显著下降。这一问题限制了SITS技术的实际应用可靠性，因此需要开发能够自适应处理可变时间长度输入的模型，以提高鲁棒性和准确性。",
      "method": "论文提出TEA（TEmporal Adaptive SITS semantic segmentation method），核心是基于教师-学生框架的知识转移机制。教师模型封装全局序列知识，指导学生模型处理自适应时间输入长度；具体通过中间嵌入、原型和软标签视角塑造学生特征空间，实现高效知识转移，并结合动态聚合策略更新学生模型以减轻知识遗忘。此外，引入全序列重建作为辅助任务，进一步优化不同时间长度输入下的表示质量，增强模型的适应性和鲁棒性。",
      "result": "通过大量实验验证，TEA方法在常见基准数据集上对卫星图像时间序列语义分割任务取得了显著改进。与现有基于固定序列长度的方法相比，TEA在不同时间长度输入下表现出更强的泛化能力，分割精度得到明显提升，减少了性能下降现象。摘要未提供具体数值指标，但强调了“remarkable improvements”，表明方法在变化时间长度场景中的有效性和优越性。",
      "conclusion": "TEA方法通过引入教师-学生框架和自适应时间输入机制，有效解决了卫星图像时间序列分割中模型泛化能力不足的问题。其主要贡献在于提出了一种新颖的知识转移策略，结合辅助任务，增强了模型在不同时间长度下的鲁棒性。这项研究提升了作物映射的准确性和实用性，为时间序列分析领域提供了新思路。未来工作可探索扩展到其他遥感任务或复杂动态场景，以验证其普适性。",
      "tags": [
        "Satellite Image Semantic Segmentation",
        "Temporal Adaptation",
        "Knowledge Distillation",
        "Teacher-Student Framework",
        "Auxiliary Task"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:33.727280Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04899",
    "title": "Rotation-Robust Regression with Convolutional Model Trees",
    "authors": [
      "Hongyi Li",
      "William Ward Armstrong",
      "Jun Xu"
    ],
    "abstract": "We study rotation-robust learning for image inputs using Convolutional Model Trees (CMTs) [1], whose split and leaf coefficients can be structured on the image grid and transformed geometrically at deployment time. In a controlled MNIST setting with a rotation-invariant regression target, we introduce three geometry-aware inductive biases for split directions -- convolutional smoothing, a tilt dominance constraint, and importance-based pruning -- and quantify their impact on robustness under in-plane rotations. We further evaluate a deployment-time orientation search that selects a discrete rotation maximizing a forest-level confidence proxy without updating model parameters. Orientation search improves robustness under severe rotations but can be harmful near the canonical orientation when confidence is misaligned with correctness. Finally, we observe consistent trends on MNIST digit recognition implemented as one-vs-rest regression, highlighting both the promise and limitations of confidence-based orientation selection for model-tree ensembles.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04899.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04899",
    "published": "2026-01-08T12:53:33Z",
    "updated": "2026-01-08T12:53:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出使用卷积模型树，结合几何感知归纳偏置和部署时方向搜索，实现图像输入的旋转鲁棒回归学习。",
      "motivation": "该研究旨在解决图像输入模型中旋转鲁棒性不足的实际问题。在计算机视觉任务中，如图像识别和回归，输入图像常经历各种旋转，而传统模型可能对旋转敏感，导致性能下降。现有方法在旋转不变性方面存在局限，特别是在结构化模型中缺乏几何感知能力。因此，开发能够适应旋转变化的模型至关重要，卷积模型树为此提供了可几何变换的结构化表示，以提升鲁棒性并应对实际应用中的旋转挑战。",
      "method": "核心方法是采用卷积模型树（CMTs），其分割和叶系数可在图像网格上结构化，并能在部署时进行几何变换。在控制实验设置下，使用MNIST数据集和旋转不变的回归目标，引入三个几何感知的归纳偏置：卷积平滑、倾斜主导约束和基于重要性的剪枝，以优化分割方向。创新点包括部署时方向搜索，通过选择最大化森林级置信度代理的离散旋转来增强鲁棒性，而无需更新模型参数，展示了技术特色和灵活性。",
      "result": "实验量化了三个几何感知归纳偏置对旋转鲁棒性的影响，结果表明这些偏置能有效提升模型在平面旋转下的性能。部署时方向搜索在严重旋转情况下显著改善了鲁棒性，但在规范方向附近可能因置信度与正确性不匹配而产生负面影响。在MNIST数字识别的一对多回归任务中，观察到一致趋势，验证了方法的有效性，并与基线方法对比，强调了置信度导向搜索的改进与局限性，但摘要未提供具体性能指标数据。",
      "conclusion": "论文的主要贡献在于提出了一种旋转鲁棒的卷积模型树方法，通过几何感知归纳偏置和置信度导向的方向搜索，增强了图像回归任务对旋转的适应性。学术上，该方法为几何不变性学习提供了新视角；实际中，可应用于需要高鲁棒性的视觉系统。局限性在于方向搜索在置信度与正确性错配时可能失效，未来工作可探索更精确的置信度对齐机制，或扩展方法处理其他几何变换，以提升通用性和实用性。",
      "tags": [
        "Convolutional Model Trees",
        "Rotation Robustness",
        "Inductive Biases",
        "Orientation Search",
        "Image Regression"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:14.464901Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04891",
    "title": "Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform",
    "authors": [
      "Suyash Mishra",
      "Qiang Li",
      "Srikanth Patil",
      "Satyanarayan Pati",
      "Baddu Narendra"
    ],
    "abstract": "Vision Language Models (VLMs) have shown strong performance on multimodal reasoning tasks, yet most evaluations focus on short videos and assume unconstrained computational resources. In industrial settings such as pharmaceutical content understanding, practitioners must process long-form videos under strict GPU, latency, and cost constraints, where many existing approaches fail to scale. In this work, we present an industrial GenAI framework that processes over 200,000 PDFs, 25,326 videos across eight formats (e.g., MP4, M4V, etc.), and 888 multilingual audio files in more than 20 languages. Our study makes three contributions: (i) an industrial large-scale architecture for multimodal reasoning in pharmaceutical domains; (ii) empirical analysis of over 40 VLMs on two leading benchmarks (Video-MME and MMBench) and proprietary dataset of 25,326 videos across 14 disease areas; and (iii) four findings relevant to long-form video reasoning: the role of multimodality, attention mechanism trade-offs, temporal reasoning limits, and challenges of video splitting under GPU constraints. Results show 3-8 times efficiency gains with SDPA attention on commodity GPUs, multimodality improving up to 8/12 task domains (especially length-dependent tasks), and clear bottlenecks in temporal alignment and keyframe detection across open- and closed-source VLMs. Rather than proposing a new \"A+B\" model, this paper characterizes practical limits, trade-offs, and failure patterns of current VLMs under realistic deployment constraints, and provide actionable guidance for both researchers and practitioners designing scalable multimodal systems for long-form video understanding in industrial domains.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04891.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04891",
    "published": "2026-01-08T12:42:17Z",
    "updated": "2026-01-08T12:42:17Z",
    "comment": "Submitted to the Industry Track of Top Tier Conference; currently under peer review",
    "light_analysis": {
      "overview": "本文通过大规模实证分析，揭示了在工业制药领域中视觉语言模型处理长视频推理的实际限制，并为设计可扩展多模态系统提供指导。",
      "motivation": "本研究旨在解决视觉语言模型在工业制药内容理解中处理长视频时面临的挑战。现有方法多数针对短视频并假设无限计算资源，而在实际部署中，受限于GPU、延迟和成本约束，导致可扩展性不足。这个问题的重要性在于，随着工业应用中多模态数据（如长视频、PDFs、音频）的增多，现有VLMs无法有效满足实时处理和分析需求，亟需探索在严格资源条件下的优化方案。",
      "method": "论文提出一个工业级GenAI框架，处理包括200,000多个PDFs、25,326个视频（覆盖八种格式）和888个多语言音频文件的大规模多模态数据集。研究方法包括构建工业大规模架构，并对超过40个VLMs在公开基准（Video-MME、MMBench）和专有数据集上进行实证分析。核心创新点在于聚焦长视频推理的四个关键方面：多模态角色、注意力机制权衡（如SDPA）、时序推理限制以及在GPU约束下的视频分割挑战。",
      "result": "实验结果显示出显著性能提升和瓶颈识别。在商品GPU上使用SDPA注意力机制实现3-8倍效率增益；多模态改进在12个任务领域中的8个得到验证，尤其在长度依赖任务中效果突出。同时，开源和闭源VLMs均表现出时序对齐和关键帧检测的明显瓶颈，这限制了长视频推理的整体效果，对比基线方法突显了现有模型的局限性。",
      "conclusion": "本文的主要贡献在于系统分析了当前视觉语言模型在工业部署约束下的实际限制、权衡和失败模式，而非提出新模型。学术价值在于填补了工业应用场景中多模态推理的研究空白，提供了实证数据和可操作指导；实际应用价值则为研究人员和从业者设计可扩展系统提供参考，未来工作可专注于改进时序推理和优化视频分割技术以克服现有瓶颈。",
      "tags": [
        "Vision Language Models",
        "Multimodal Reasoning",
        "Long-form Video",
        "Attention Mechanisms",
        "Industrial AI"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:43.526997Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.10688",
    "title": "Novel View Synthesis using DDIM Inversion",
    "authors": [
      "Sehajdeep Singh",
      "A V Subramanyam",
      "Aditya Gupta",
      "Sahil Gupta"
    ],
    "abstract": "Synthesizing novel views from a single input image is a challenging task. It requires extrapolating the 3D structure of a scene while inferring details in occluded regions, and maintaining geometric consistency across viewpoints. Many existing methods must fine-tune large diffusion backbones using multiple views or train a diffusion model from scratch, which is extremely expensive. Additionally, they suffer from blurry reconstruction and poor generalization. This gap presents the opportunity to explore an explicit lightweight view translation framework that can directly utilize the high-fidelity generative capabilities of a pretrained diffusion model while reconstructing a scene from a novel view. Given the DDIM-inverted latent of a single input image, we employ a camera pose-conditioned translation U-Net, TUNet, to predict the inverted latent corresponding to the desired target view. However, the image sampled using the predicted latent may result in a blurry reconstruction. To this end, we propose a novel fusion strategy that exploits the inherent noise correlation structure observed in DDIM inversion. The proposed fusion strategy helps preserve the texture and fine-grained details. To synthesize the novel view, we use the fused latent as the initial condition for DDIM sampling, leveraging the generative prior of the pretrained diffusion model. Extensive experiments on MVImgNet demonstrate that our method outperforms existing methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.10688.pdf",
    "abs_url": "https://arxiv.org/abs/2508.10688",
    "published": "2025-08-14T14:32:52Z",
    "updated": "2026-01-08T11:56:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于DDIM反演和融合策略的轻量级视图翻译框架，用于从单个输入图像合成高保真新视角，有效避免昂贵训练并提升重建质量。",
      "motivation": "从单个输入图像合成新视角是一个挑战性任务，需要推断场景的3D结构、处理遮挡区域并保持几何一致性。现有方法必须微调大型扩散模型或从头训练，计算成本极高，且常导致模糊重建和泛化能力差。这突显了探索轻量级框架的必要性，以直接利用预训练扩散模型的高保真生成能力，提升合成效率和应用价值。",
      "method": "本方法首先通过DDIM反演获取输入图像的潜在表示，然后使用相机姿态条件化的翻译U-Net（TUNet）预测目标视图的潜在。创新点在于提出一种融合策略，利用DDIM反演中观察到的噪声相关性结构来保留纹理和细节。最后，将融合潜在作为初始条件进行DDIM采样，以利用预训练扩散模型的生成先验。实验在MVImgNet数据集上进行，以验证方法的有效性。",
      "result": "在MVImgNet数据集上的广泛实验表明，该方法在合成新视角时优于现有方法。虽然摘要未提供具体性能指标如准确率，但通过融合策略显著改善了重建的清晰度和细节保留，解决了模糊问题，并在泛化能力上表现更优，显示出对基线的明显优势。",
      "conclusion": "本文的主要贡献是提出一种结合DDIM反演和融合策略的轻量级框架，实现了高效、高质量的单图像新视角合成。学术上，它提供了一种新的视图翻译方法；应用上，降低了计算成本并提升合成质量，具有实际应用潜力。未来工作方向，摘要未明确说明，可能涉及扩展到更多场景或进一步优化。",
      "tags": [
        "DDIM Inversion",
        "Novel View Synthesis",
        "Diffusion Models",
        "U-Net",
        "Fusion Strategy"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:14.010131Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04860",
    "title": "DivAS: Interactive 3D Segmentation of NeRFs via Depth-Weighted Voxel Aggregation",
    "authors": [
      "Ayush Pande"
    ],
    "abstract": "Existing methods for segmenting Neural Radiance Fields (NeRFs) are often optimization-based, requiring slow per-scene training that sacrifices the zero-shot capabilities of 2D foundation models. We introduce DivAS (Depth-interactive Voxel Aggregation Segmentation), an optimization-free, fully interactive framework that addresses these limitations. Our method operates via a fast GUI-based workflow where 2D SAM masks, generated from user point prompts, are refined using NeRF-derived depth priors to improve geometric accuracy and foreground-background separation. The core of our contribution is a custom CUDA kernel that aggregates these refined multi-view masks into a unified 3D voxel grid in under 200ms, enabling real-time visual feedback. This optimization-free design eliminates the need for per-scene training. Experiments on Mip-NeRF 360° and LLFF show that DivAS achieves segmentation quality comparable to optimization-based methods, while being 2-2.5x faster end-to-end, and up to an order of magnitude faster when excluding user prompting time.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04860.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04860",
    "published": "2026-01-08T11:53:04Z",
    "updated": "2026-01-08T11:53:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了DivAS框架，一种优化免费的交互式3D分割方法，通过深度加权体素聚合实现实时视觉反馈。",
      "motivation": "现有神经辐射场（NeRF）分割方法多基于优化，需要进行每场景训练，过程耗时且牺牲了2D基础模型（如SAM）的零样本能力，导致交互性差和效率低下。这在实际应用中，如3D场景编辑或虚拟现实，限制了快速迭代和用户参与。因此，开发无需训练、能实时响应的分割方法至关重要，以提升实用性和可扩展性。",
      "method": "DivAS方法采用基于GUI的快速工作流程：首先从用户点提示生成2D SAM masks，利用NeRF衍生的深度先验精炼这些mask，以改善几何精度和前景-背景分离。核心创新是一个自定义CUDA内核，能在200毫秒内将这些精炼后的多视角mask聚合到统一的3D体素网格中，实现实时视觉反馈。此优化免费设计完全避免了每场景训练需求，依赖深度加权技术增强分割的准确性。",
      "result": "实验在Mip-NeRF 360°和LLFF数据集上进行，结果显示DivAS的分割质量与基于优化的方法相当，验证了其有效性。在端到端流程中，DivAS比基线方法快2-2.5倍；若排除用户提示时间，速度提升可达一个数量级。这些性能指标表明DivAS在保持高质量的同时，显著提高了分割效率，支持了其实时交互应用。",
      "conclusion": "本研究的核心贡献是提出了DivAS框架，实现了优化免费的NeRF分割，通过深度加权体素聚合和CUDA加速，增强了交互效率和实时能力。其学术价值在于推动了3D分割方法的创新，减少了训练开销；实际应用可能扩展至虚拟现实、游戏开发等领域。局限性可能包括对深度先验的依赖，未来工作可探索更多自动化交互或适应不同场景类型。",
      "tags": [
        "Neural Radiance Fields",
        "3D Segmentation",
        "Interactive Segmentation",
        "Voxel Aggregation",
        "CUDA Kernel"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:21.732093Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.03244",
    "title": "Two-Stream Thermal Imaging Fusion for Enhanced Time of Birth Detection in Neonatal Care",
    "authors": [
      "Jorge García-Torres",
      "Øyvind Meinich-Bache",
      "Sara Brunner",
      "Siren Rettedal",
      "Vilde Kolstad",
      "Kjersti Engan"
    ],
    "abstract": "Around 10% of newborns require some help to initiate breathing, and 5\\% need ventilation assistance. Accurate Time of Birth (ToB) documentation is essential for optimizing neonatal care, as timely interventions are vital for proper resuscitation. However, current clinical methods for recording ToB often rely on manual processes, which can be prone to inaccuracies. In this study, we present a novel two-stream fusion system that combines the power of image and video analysis to accurately detect the ToB from thermal recordings in the delivery room and operating theater. By integrating static and dynamic streams, our approach captures richer birth-related spatiotemporal features, leading to more robust and precise ToB estimation. We demonstrate that this synergy between data modalities enhances performance over single-stream approaches. Our system achieves 95.7% precision and 84.8% recall in detecting birth within short video clips. Additionally, with the help of a score aggregation module, it successfully identifies ToB in 100% of test cases, with a median absolute error of 2 seconds and an absolute mean deviation of 4.5 seconds compared to manual annotations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.03244.pdf",
    "abs_url": "https://arxiv.org/abs/2503.03244",
    "published": "2025-03-05T07:52:52Z",
    "updated": "2026-01-08T11:16:21Z",
    "comment": "This work has been accepted at IEEE 25th International Conference on Digital Signal Processing",
    "light_analysis": {
      "overview": "提出一种两流热成像融合系统，通过整合静态图像和动态视频流，提升新生儿出生时间的自动检测精度和鲁棒性。",
      "motivation": "新生儿护理中，准确记录出生时间至关重要，因为约10%新生儿需要呼吸帮助，5%需要通风辅助，及时干预对复苏很关键。然而，现有临床方法依赖手动记录，易出错且不精确，可能导致护理延迟。本研究旨在利用热成像技术，在产房和手术室环境中自动检测出生时间，以解决手动方法的不足，优化新生儿护理流程。背景是基于热记录分析，以增强数据模态的协同作用。",
      "method": "研究方法基于一个两流融合系统，结合图像分析和视频分析。静态流处理单帧热图像以捕获出生相关的静态特征，动态流分析视频序列以提取时间动态信息。通过融合这两种数据流，系统能捕捉更丰富的时空特征，提高ToB估计的鲁棒性。关键技术包括得分聚合模块，用于整合来自不同流的预测结果，增强检测准确性。系统从热成像记录中获取数据，在临床环境中实现自动分析。",
      "result": "实验结果显示，系统在检测短视频片段中的出生事件时，达到95.7%的精确率和84.8%的召回率。通过得分聚合模块，系统在所有测试案例中成功识别出生时间，识别率100%。与手动标注相比，中位绝对误差为2秒，绝对平均偏差为4.5秒，表明两流融合方法优于单流方法，提供了更准确和可靠的ToB估计。摘要未明确说明其他基线对比细节。",
      "conclusion": "本研究的主要贡献是开发了一个高效的两流热成像融合系统，显著提升新生儿出生时间的自动检测性能。通过结合静态和动态数据流，增强了时空特征提取能力，具有重要的临床应用价值，可替代易错的手动记录，促进更及时和准确的医疗干预。学术上，它展示了多模态数据融合在医疗影像分析中的潜力。未来工作可能包括扩展数据集或优化模型以提高泛化能力。摘要未明确说明具体局限性。",
      "tags": [
        "Thermal Imaging",
        "Two-Stream Fusion",
        "Spatiotemporal Features",
        "Video Analysis",
        "Time of Birth Detection"
      ]
    },
    "analyzed_at": "2026-01-09T02:32:41.464993Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04834",
    "title": "Character Detection using YOLO for Writer Identification in multiple Medieval books",
    "authors": [
      "Alessandra Scotto di Freca",
      "Tiziana D Alessandro",
      "Francesco Fontanella",
      "Filippo Sarria",
      "Claudio De Stefano"
    ],
    "abstract": "Paleography is the study of ancient and historical handwriting, its key objectives include the dating of manuscripts and understanding the evolution of writing. Estimating when a document was written and tracing the development of scripts and writing styles can be aided by identifying the individual scribes who contributed to a medieval manuscript. Although digital technologies have made significant progress in this field, the general problem remains unsolved and continues to pose open challenges. ... We previously proposed an approach focused on identifying specific letters or abbreviations that characterize each writer. In that study, we considered the letter \"a\", as it was widely present on all pages of text and highly distinctive, according to the suggestions of expert paleographers. We used template matching techniques to detect the occurrences of the character \"a\" on each page and the convolutional neural network (CNN) to attribute each instance to the correct scribe. Moving from the interesting results achieved from this previous system and being aware of the limitations of the template matching technique, which requires an appropriate threshold to work, we decided to experiment in the same framework with the use of the YOLO object detection model to identify the scribe who contributed to the writing of different medieval books. We considered the fifth version of YOLO to implement the YOLO object detection model, which completely substituted the template matching and CNN used in the previous work. The experimental results demonstrate that YOLO effectively extracts a greater number of letters considered, leading to a more accurate second-stage classification. Furthermore, the YOLO confidence score provides a foundation for developing a system that applies a rejection threshold, enabling reliable writer identification even in unseen manuscripts.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04834.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04834",
    "published": "2026-01-08T11:11:24Z",
    "updated": "2026-01-08T11:11:24Z",
    "comment": "7 pages, 2 figures, 1 table. Accepted at IEEE-CH 2025",
    "light_analysis": {
      "overview": "本论文提出使用YOLO对象检测模型替代传统模板匹配方法，以提高中世纪书籍中字符检测和书写者识别的准确性和效率。",
      "motivation": "研究动机源于古文书学领域对自动化识别书写者的需求，以辅助手稿年代确定和书写演变研究。现有方法如模板匹配结合CNN虽有一定效果，但模板匹配依赖阈值设定，检测字符数量有限，且整体问题仍存在挑战。数字技术虽有进展，但书写者识别问题尚未完全解决，因此需要更高效的方法来克服这些限制，提升分析精度。",
      "method": "研究方法采用YOLO（第五版）对象检测模型来检测字母'a'，替代先前工作中的模板匹配和CNN。YOLO能自动定位字符并提取特征，无需手动设置阈值，提供了更灵活的检测框架。关键创新在于利用YOLO的端到端检测能力，结合其置信度分数，为后续分类阶段提供更可靠的输入，从而提高整体系统的鲁棒性和效率。",
      "result": "实验结果显示，YOLO模型能提取更多字符实例，从而提升第二阶段分类的准确性。尽管摘要未提供具体性能指标，但与基线模板匹配方法相比，YOLO在检测效率和覆盖范围上表现更优。置信度分数的引入支持开发带拒绝阈值的系统，增强了在新手稿上的泛化能力，实现了更可靠的书写者识别。",
      "conclusion": "论文的主要贡献是成功将YOLO对象检测应用于古文书学的书写者识别，验证了其在字符检测上的优势，为历史文档分析提供了更自动化的工具。这具有学术价值，推动了跨学科技术应用，并具有实际意义，帮助研究人员更准确地分析中世纪手稿。未来工作可探索扩展到其他字符或更复杂的书写风格，以进一步提升系统的适用范围。",
      "tags": [
        "YOLO",
        "Object Detection",
        "Character Detection",
        "Writer Identification",
        "CNN"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:41.336910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04824",
    "title": "SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models",
    "authors": [
      "Oriol Rabasseda",
      "Zenjie Li",
      "Kamal Nasrollahi",
      "Sergio Escalera"
    ],
    "abstract": "Automatic identification of events and recurrent behavior analysis are critical for video surveillance. However, most existing content-based video retrieval benchmarks focus on scene-level similarity and do not evaluate the action discrimination required in surveillance. To address this gap, we introduce SOVABench (Surveillance Opposite Vehicle Actions Benchmark), a real-world retrieval benchmark built from surveillance footage and centered on vehicle-related actions. SOVABench defines two evaluation protocols (inter-pair and intra-pair) to assess cross-action discrimination and temporal direction understanding. Although action distinctions are generally intuitive for human observers, our experiments show that they remain challenging for state-of-the-art vision and multimodal models.   Leveraging the visual reasoning and instruction-following capabilities of Multimodal Large Language Models (MLLMs), we present a training-free framework for producing interpretable embeddings from MLLM-generated descriptions for both images and videos. The framework achieves strong performance on SOVABench as well as on several spatial and counting benchmarks where contrastive Vision-Language Models often fail. The code, annotations, and instructions to construct the benchmark are publicly available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04824.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04824",
    "published": "2026-01-08T10:58:59Z",
    "updated": "2026-01-08T10:58:59Z",
    "comment": "This work has been accepted at Real World Surveillance: Applications and Challenges, 6th (in WACV Workshops)",
    "light_analysis": {
      "overview": "本文引入了SOVABench基准和基于多模态大语言模型的无需训练框架，以提升视频监控中的车辆动作检索性能。",
      "motivation": "该研究旨在解决视频监控中自动事件识别和反复行为分析的需求。现有基于内容的视频检索基准大多关注场景级相似性，缺乏对动作辨别的评估，这在监控场景中至关重要，因为动作差异能反映事件本质。例如，车辆动作的细微区别可能指示异常行为，但现有方法未有效捕捉这些细节，导致性能受限，凸显了开发新基准和方法的必要性。",
      "method": "论文提出一个无需训练的多模态框架，利用多模态大语言模型的视觉推理和指令跟随能力生成描述。通过从这些描述中提取可解释的嵌入，框架处理图像和视频进行动作检索。关键创新点包括：使用MLLM生成文本描述以避免直接训练，提高可解释性；结合跨动作辨别和时间方向理解评估协议；适用于多种模态输入，强化了在复杂监控场景中的适应能力。",
      "result": "实验结果显示，该框架在SOVABench基准上实现了强大性能，成功评估了动作辨别和时间方向理解。摘要未明确说明具体数据，如准确率提升，但指出框架在多个空间和计数基准上优于对比性视觉-语言模型，后者在这些任务中经常失败。这表明框架在提高检索精度和泛化能力方面表现显著，填补了现有方法在动作识别方面的不足。",
      "conclusion": "论文主要贡献是引入了SOVABench基准，为视频监控动作检索提供了标准化评估工具，并结合多模态大语言模型提出高效框架。学术价值在于推动多模态学习在监控领域的应用，实际应用价值在于提升事件自动识别和安全性分析效率。局限性可能包括基准范围的限制，未来工作可扩展基准到更多动作类型或改进框架处理动态场景的能力。",
      "tags": [
        "Multimodal Large Language Models",
        "Video Surveillance",
        "Action Retrieval",
        "Contrastive Learning",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:48.527709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02046",
    "title": "Agentic Retoucher for Text-To-Image Generation",
    "authors": [
      "Shaocheng Shen",
      "Jianfeng Liang",
      "Chunlei Cai",
      "Cong Geng",
      "Huiyu Duan",
      "Xiaoyun Zhang",
      "Qiang Hu",
      "Guangtao Zhai"
    ],
    "abstract": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.02046.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02046",
    "published": "2026-01-05T12:06:43Z",
    "updated": "2026-01-08T10:57:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Agentic Retoucher，一个分层决策框架，通过人类感知-推理-行动循环纠正文本到图像生成中的小规模扭曲，为自校正和感知可靠生成建立新范式。",
      "motivation": "文本到图像扩散模型如SDXL和FLUX在实现高真实感的同时，仍普遍存在肢体、面部和文本等小规模扭曲问题。现有改进方法要么依赖高成本的迭代再生，要么使用空间定位弱的视觉语言模型，导致语义漂移和局部编辑不可靠。因此，研究旨在解决T2I模型中这些普遍扭曲，弥补现有方法的不足，提高校正的准确性和可控性。",
      "method": "论文提出Agentic Retoucher，一种分层决策驱动框架，将后生成校正重构为感知-推理-行动循环。具体设计包括三个代理：感知代理学习文本-图像一致性下的上下文显著性以定位细粒度扭曲；推理代理通过渐进偏好对齐进行人类对齐推断诊断；行动代理根据用户偏好自适应规划局部修复。此外，构建了GenBlemish-27K数据集，包含6K图像和27K标注扭曲区域，支持细粒度监督和评估。",
      "result": "广泛实验表明，Agentic Retoucher在感知质量、扭曲定位和人类偏好对齐方面一致优于最先进方法。它有效减少了语义漂移，提供了可靠的局部编辑，在评估中展现出优越性能，无需依赖具体数字指标即可确认其有效性。这为自校正T2I生成确立了新的技术基准，推动了该领域的发展。",
      "conclusion": "论文的主要贡献包括提出集成感知、推理和行动的Agentic Retoucher框架，构建GenBlemish-27K数据集用于细粒度评估，并展示了在性能上的显著提升。其学术价值在于为T2I校正提供了可解释和可控的决策范式，实际应用价值包括提升图像生成质量和用户体验。未来工作可能涉及扩展到更广泛扭曲类别或优化计算效率。",
      "tags": [
        "Text-to-Image Generation",
        "Diffusion Models",
        "Hierarchical Decision Framework",
        "Perceptual Reasoning",
        "Inpainting"
      ]
    },
    "analyzed_at": "2026-01-09T02:35:31.714397Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04800",
    "title": "Integrated Framework for Selecting and Enhancing Ancient Marathi Inscription Images from Stone, Metal Plate, and Paper Documents",
    "authors": [
      "Bapu D. Chendage",
      "Rajivkumar S. Mente"
    ],
    "abstract": "Ancient script images often suffer from severe background noise, low contrast, and degradation caused by aging and environmental effects. In many cases, the foreground text and background exhibit similar visual characteristics, making the inscriptions difficult to read. The primary objective of image enhancement is to improve the readability of such degraded ancient images. This paper presents an image enhancement approach based on binarization and complementary preprocessing techniques for removing stains and enhancing unclear ancient text. The proposed methods are evaluated on different types of ancient scripts, including inscriptions on stone, metal plates, and historical documents. Experimental results show that the proposed approach achieves classification accuracies of 55.7%, 62%, and 65.6% for stone, metal plate, and document scripts, respectively, using the K-Nearest Neighbor (K-NN) classifier. Using the Support Vector Machine (SVM) classifier, accuracies of 53.2%, 59.5%, and 67.8% are obtained. The results demonstrate the effectiveness of the proposed enhancement method in improving the readability of ancient Marathi inscription images.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04800.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04800",
    "published": "2026-01-08T10:30:59Z",
    "updated": "2026-01-08T10:30:59Z",
    "comment": "9 Pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出一种基于二值化和互补预处理技术的图像增强框架，用于提升古代马拉地语铭文图像的可读性。",
      "motivation": "古代马拉地语铭文图像常因石头、金属板和纸质文档的材质及环境因素，遭受严重背景噪声、低对比度和老化退化，前景文本与背景相似，导致铭文难以识读。这限制了历史研究、数字化保存和文化遗产保护等领域的发展。现有图像增强方法可能无法有效处理此类复杂退化，因此需要专门技术来改善图像质量，以支持后续分析和自动化处理。本研究旨在解决这一问题，通过开发新方法提高铭文图像的可读性，为文化遗产领域提供实用工具。",
      "method": "本文提出一种图像增强方法，基于二值化技术和互补预处理步骤，用于去除污渍并增强不清晰的古代文本。方法框架包括对输入图像进行预处理，如噪声去除和对比度调整，然后应用二值化将图像转换为黑白形式，分离文本与背景。针对不同类型古代铭文（石头、金属板、纸质文档），优化了处理流程。摘要未明确说明具体数据集和模型架构，但强调了结合多种技术以提升效果，并通过分类器评估增强后的图像质量。",
      "result": "实验结果显示，使用K-最近邻（K-NN）分类器时，在石头、金属板和文档类型上的分类准确率分别为55.7%、62%和65.6%。使用支持向量机（SVM）分类器时，准确率分别为53.2%、59.5%和67.8%。这些数据表明，提出的增强方法在不同介质上均有效提高了图像的可读性，尤其在文档类型上表现最佳，准确率可达67.8%。摘要未明确对比基线方法，但结果直接证明了方法的实用性，并为后续应用提供了数据支撑。",
      "conclusion": "本研究的主要贡献在于开发了一个集成框架，通过二值化和预处理技术显著增强了古代马拉地语铭文图像的可读性。该方法对于历史文档的数字化、文化遗产保护和自动分类具有重要的应用价值。学术上，它为处理退化图像提供了一种有效途径。摘要未提及具体局限性，但未来工作可能包括优化算法以处理更广泛的退化类型、扩展到其他古代语言铭文，或结合更先进的图像处理技术进一步提升性能。",
      "tags": [
        "Image Enhancement",
        "Binarization",
        "Classification",
        "K-Nearest Neighbor",
        "Support Vector Machine"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:23.918159Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.07155",
    "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
    "authors": [
      "Dahyeon Kye",
      "Jeahun Sung",
      "Minkyu Jeon",
      "Jihyong Oh"
    ],
    "abstract": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.07155.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07155",
    "published": "2025-12-08T04:39:12Z",
    "updated": "2026-01-08T10:29:58Z",
    "comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
    "light_analysis": {
      "overview": "提出CHIMERA框架，通过自适应缓存注入和语义锚定提示实现零样本图像变形，并引入面向变形的度量标准。",
      "motivation": "扩散模型在图像生成方面表现出色，但实现平滑且语义一致的图像变形仍具挑战性。现有方法因缺乏自适应的结构和语义对齐，常导致过渡突兀或外观过饱和，限制了图像编辑的实际应用。本研究旨在解决这些问题，提升变形的自然度和一致性，为高保真图像合成和创意工具开发提供支持。",
      "method": "CHIMERA是一种零样本扩散框架，将图像变形视为缓存反转引导的去噪过程。通过Adaptive Cache Injection (ACI) 在DDIM反转中缓存输入特征，并在去噪时自适应重注入，实现深度和时间自适应的对齐。Semantic Anchor Prompting (SAP) 利用视觉语言模型生成共享锚定提示，作为语义桥梁引导去噪。此外，引入Global-Local Consistency Score (GLCS) 作为变形专用度量，评估全局协调和局部平滑度。",
      "result": "广泛实验和用户研究表明，CHIMERA在图像变形任务中实现了比现有方法更平滑和语义更一致的过渡，确立了新的最优性能。具体对比显示，该方法在过渡自然度和语义对齐方面显著优于基线，但摘要未明确说明具体准确率或效率指标。用户研究进一步验证了其优越性，提升了变形质量。",
      "conclusion": "本研究的主要贡献是提出了CHIMERA框架，结合自适应缓存注入和语义锚定提示，有效解决了图像变形中的挑战。引入的GLCS度量为评估变形质量提供了新标准，具有学术和实际应用价值。未来工作包括代码公开和进一步优化框架，以扩展到更多图像生成任务和现实场景。",
      "tags": [
        "Diffusion Models",
        "Zero-shot Learning",
        "Image Morphing",
        "Adaptive Cache Injection",
        "Semantic Anchor Prompting"
      ]
    },
    "analyzed_at": "2026-01-09T02:38:10.098612Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04798",
    "title": "Detector-Augmented SAMURAI for Long-Duration Drone Tracking",
    "authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Hichem Snoussi",
      "Tobias Koch"
    ],
    "abstract": "Robust long-term tracking of drone is a critical requirement for modern surveillance systems, given their increasing threat potential. While detector-based approaches typically achieve strong frame-level accuracy, they often suffer from temporal inconsistencies caused by frequent detection dropouts. Despite its practical relevance, research on RGB-based drone tracking is still limited and largely reliant on conventional motion models. Meanwhile, foundation models like SAMURAI have established their effectiveness across other domains, exhibiting strong category-agnostic tracking performance. However, their applicability in drone-specific scenarios has not been investigated yet. Motivated by this gap, we present the first systematic evaluation of SAMURAI's potential for robust drone tracking in urban surveillance settings. Furthermore, we introduce a detector-augmented extension of SAMURAI to mitigate sensitivity to bounding-box initialization and sequence length. Our findings demonstrate that the proposed extension significantly improves robustness in complex urban environments, with pronounced benefits in long-duration sequences - especially under drone exit-re-entry events. The incorporation of detector cues yields consistent gains over SAMURAI's zero-shot performance across datasets and metrics, with success rate improvements of up to +0.393 and FNR reductions of up to -0.475.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04798.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04798",
    "published": "2026-01-08T10:27:05Z",
    "updated": "2026-01-08T10:27:05Z",
    "comment": "Accepted at the WACV 2026 Workshop on \"Real World Surveillance: Applications and Challenges\"",
    "light_analysis": {
      "overview": "该研究提出了一个检测器增强的SAMURAI扩展方法，用于显著改善无人机在复杂城市环境中的长期跟踪鲁棒性。",
      "motivation": "该研究旨在解决现代监视系统中无人机长期跟踪的实际问题，因为无人机威胁日益增加，需要可靠的跟踪技术。现有基于检测器的方法虽然帧级精度高，但存在时间不一致性和频繁检测丢失的缺陷。当前RGB-based无人机跟踪研究较少，依赖传统运动模型，且SAMURAI等基础模型在其他领域表现优秀但未在无人机场景中验证。因此，填补这一空白对于提高跟踪稳定性和应用价值至关重要。",
      "method": "该论文的核心方法是提出一个检测器增强的SAMURAI扩展。它通过集成检测器来减少对边界框初始化和序列长度的敏感性，从而提升跟踪鲁棒性。关键创新点在于结合检测器的提示信号，以应对复杂环境中的挑战，如无人机退出再进入事件。技术特色包括对SAMURAI模型进行扩展，优化其在城市监视设置中的表现。摘要未明确说明具体数据集和模型架构细节，但提到了在多个数据集上进行评估。",
      "result": "实验结果显示，提出的检测器增强扩展在复杂城市环境中显著提高了跟踪鲁棒性，尤其是在长时间序列中。与SAMURAI的零次射击性能相比，该方法在多个数据集和指标上获得一致提升，具体数据包括成功率最高提高0.393和假负率最多降低0.475。这些改进在无人机退出再进入事件中尤为明显，证明了该方法能有效减少跟踪失败，增强了基线方法的性能。",
      "conclusion": "该论文的主要贡献是首次系统评估了SAMURAI在无人机跟踪中的潜力，并提出了检测器增强扩展以提高鲁棒性。学术价值在于扩展了基础模型的应用领域，为无人机跟踪研究提供新思路；实际应用价值在于增强了监视系统的跟踪能力，提升对无人机威胁的响应效果。潜在局限性如对特定环境依赖或未来工作方向（如扩展其他检测器）摘要未明确说明。",
      "tags": [
        "Drone Tracking",
        "Foundational Models",
        "Detector-Augmented",
        "Long-Duration Tracking",
        "Zero-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:19.434822Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.05547",
    "title": "Automated Invoice Data Extraction: Using LLM and OCR",
    "authors": [
      "Khushi Khanchandani",
      "Advait Thakur",
      "Akshita Shetty",
      "Chaitravi Reddy",
      "Ritisa Behera"
    ],
    "abstract": "Conventional Optical Character Recognition (OCR) systems are challenged by variant invoice layouts, handwritten text, and low- quality scans, which are often caused by strong template dependencies that restrict their flexibility across different document structures and layouts. Newer solutions utilize advanced deep learning models such as Convolutional Neural Networks (CNN) as well as Transformers, and domain-specific models for better layout analysis and accuracy across various sections over varied document types. Large Language Models (LLMs) have revolutionized extraction pipelines at their core with sophisticated entity recognition and semantic comprehension to support complex contextual relationship mapping without direct programming specification. Visual Named Entity Recognition (NER) capabilities permit extraction from invoice images with greater contextual sensitivity and much higher accuracy rates than older approaches. Existing industry best practices utilize hybrid architectures that blend OCR technology and LLM for maximum scalability and minimal human intervention. This work introduces a holistic Artificial Intelligence (AI) platform combining OCR, deep learning, LLMs, and graph analytics to achieve unprecedented extraction quality and consistency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.05547.pdf",
    "abs_url": "https://arxiv.org/abs/2511.05547",
    "published": "2025-11-01T19:05:09Z",
    "updated": "2026-01-08T10:24:34Z",
    "comment": "10 pages, 3 figures",
    "light_analysis": {
      "overview": "该论文提出了一个综合AI平台，结合OCR、深度学习、LLM和图形分析，以提升发票数据提取的质量和一致性。",
      "motivation": "传统OCR系统在发票数据提取中面临变体布局、手写文本和低质量扫描等挑战，其强模板依赖性限制了灵活性和准确性。现有方法虽利用深度学习模型如CNN和Transformer改进布局分析，但仍无法充分处理复杂上下文关系，导致提取误差和人工干预需求高。因此，研究旨在解决这些不足，通过融合先进技术推动自动化文档处理的进步，减少对固定模板的依赖，提升跨文档类型的适应性。",
      "method": "该论文提出一个全面的AI平台，集成了OCR技术处理图像输入，深度学习模型包括CNN和Transformer进行文档布局分析，并利用LLM的视觉实体识别和语义理解能力来映射复杂上下文关系。关键创新在于引入图形分析优化数据提取流程，实现端到端的智能处理，结合多种技术减少对特定模板的依赖性，从而增强对不同发票结构的适应性和准确性。",
      "result": "摘要未明确说明具体实验数据，但声称该平台实现了前所未有的提取质量和一致性。推断表明，通过综合OCR、深度学习和LLM，该方法可能相比传统OCR和现有深度学习方案显著提升准确率和可靠性，减少人工干预，并在处理变体布局和低质量扫描时提供更高的处理效率，尽管缺乏具体性能指标如准确率提升数值。",
      "conclusion": "本论文的主要贡献是开发了一个创新的AI平台，融合OCR、深度学习和LLM，以优化发票数据提取过程。学术上，展示了多技术集成在文档处理中的潜力；实践上，提高了自动化水平，可应用于金融和会计领域，减少人工成本。未来工作可能包括扩展到其他文档类型或进一步优化模型架构以应对更复杂的场景。",
      "tags": [
        "OCR",
        "Large Language Model",
        "Visual Named Entity Recognition",
        "Transformer",
        "Graph Analytics"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:59.615863Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04792",
    "title": "PyramidalWan: On Making Pretrained Video Model Pyramidal for Efficient Inference",
    "authors": [
      "Denis Korzhenkov",
      "Adil Karjauv",
      "Animesh Karnewar",
      "Mohsen Ghafoorian",
      "Amirhossein Habibian"
    ],
    "abstract": "Recently proposed pyramidal models decompose the conventional forward and backward diffusion processes into multiple stages operating at varying resolutions. These models handle inputs with higher noise levels at lower resolutions, while less noisy inputs are processed at higher resolutions. This hierarchical approach significantly reduces the computational cost of inference in multi-step denoising models. However, existing open-source pyramidal video models have been trained from scratch and tend to underperform compared to state-of-the-art systems in terms of visual plausibility. In this work, we present a pipeline that converts a pretrained diffusion model into a pyramidal one through low-cost finetuning, achieving this transformation without degradation in quality of output videos. Furthermore, we investigate and compare various strategies for step distillation within pyramidal models, aiming to further enhance the inference efficiency. Our results are available at https://qualcomm-ai-research.github.io/PyramidalWan.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04792.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04792",
    "published": "2026-01-08T10:16:06Z",
    "updated": "2026-01-08T10:16:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种管道，通过低成本微调将预训练视频扩散模型转换为金字塔模型，以实现高效推理而不降低输出质量。",
      "motivation": "视频生成模型的推理效率是一个关键问题，现有金字塔模型通常从头训练，虽然在多分辨率处理上降低计算成本，但在视觉质量上不及基于预训练的先进系统。本研究旨在解决这一矛盾，通过利用预训练模型的优势，开发一种转换方法，以在保持高质量输出的同时提升推理效率，弥补现有开源金字塔模型性能不足的缺陷。",
      "method": "研究提出一个转换管道，将预训练的扩散模型分解为多个阶段，每个阶段处理不同分辨率的输入，形成金字塔结构。通过低成本微调实现转换，避免了从头训练的开销。关键创新在于适配预训练模型到多分辨率流程，并探索步骤蒸馏策略，如优化扩散步数，以进一步提升推理效率。该方法使用视频数据集进行验证，但摘要未明确说明具体架构细节。",
      "result": "转换后的金字塔模型在输出视频质量上与原预训练模型相当，未出现质量退化。推理效率得到提升，通过多分辨率处理和步骤蒸馏减少了计算成本。与从头训练的金字塔模型相比，该方法在视觉质量上更具竞争力，同时保持高效率，但摘要未提供具体的性能指标如加速比或质量得分，仅暗示效率改进。",
      "conclusion": "该研究成功实现了预训练模型到金字塔结构的转换，为视频生成提供了高效且高质量的解决方案。学术上扩展了金字塔模型的应用范围，推动视频生成技术的效率优化，实际应用中可用于加速视频处理任务。未来工作可能包括更深入的蒸馏策略研究、扩展到其他模型类型，以及在大规模数据集上验证性能，摘要未明确说明局限性。",
      "tags": [
        "Diffusion Models",
        "Pyramidal Models",
        "Fine-tuning",
        "Step Distillation",
        "Video Generation"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:38.422470Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04791",
    "title": "Measurement-Consistent Langevin Corrector: A Remedy for Latent Diffusion Inverse Solvers",
    "authors": [
      "Lee Hyoseok",
      "Sohwi Lim",
      "Eunju Cha",
      "Tae-Hyun Oh"
    ],
    "abstract": "With recent advances in generative models, diffusion models have emerged as powerful priors for solving inverse problems in each domain. Since Latent Diffusion Models (LDMs) provide generic priors, several studies have explored their potential as domain-agnostic zero-shot inverse solvers. Despite these efforts, existing latent diffusion inverse solvers suffer from their instability, exhibiting undesirable artifacts and degraded quality. In this work, we first identify the instability as a discrepancy between the solver's and true reverse diffusion dynamics, and show that reducing this gap stabilizes the solver. Building on this, we introduce Measurement-Consistent Langevin Corrector (MCLC), a theoretically grounded plug-and-play correction module that remedies the LDM-based inverse solvers through measurement-consistent Langevin updates. Compared to prior approaches that rely on linear manifold assumptions, which often do not hold in latent space, MCLC operates without this assumption, leading to more stable and reliable behavior. We experimentally demonstrate the effectiveness of MCLC and its compatibility with existing solvers across diverse image restoration tasks. Additionally, we analyze blob artifacts and offer insights into their underlying causes. We highlight that MCLC is a key step toward more robust zero-shot inverse problem solvers.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04791.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04791",
    "published": "2026-01-08T10:15:35Z",
    "updated": "2026-01-08T10:15:35Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "论文提出Measurement-Consistent Langevin Corrector (MCLC)，一个即插即用校正模块，用于稳定基于潜在扩散模型的零样本逆求解器，通过测量一致的Langevin更新解决不稳定问题。",
      "motivation": "随着生成模型的进步，扩散模型成为解决逆问题的强大先验。潜在扩散模型（LDMs）被探索为领域无关的零样本逆求解器，但现有方法存在不稳定性，导致伪影和质量下降。这主要是因为这些方法依赖线性流形假设，而在潜在空间中该假设通常不成立，从而限制了求解器的可靠性。因此，开发更稳定的逆求解器对于提高逆问题求解的质量和实用性至关重要。",
      "method": "论文引入Measurement-Consistent Langevin Corrector (MCLC)，一个理论基础的即插即用校正模块。该方法的核心是通过测量一致的Langevin更新来减少求解器与真实反向扩散动态之间的差异，从而稳定求解器。MCLC无需线性流形假设，在潜在空间中更适用，与现有求解器兼容，可作为附加模块轻松集成。关键技术基于Langevin动力学，确保更新与测量一致，提高了求解过程的可靠性。",
      "result": "实验结果显示，MCLC在多样化图像修复任务中表现出有效性和兼容性。与依赖线性流形假设的先验方法相比，MCLC显著提高了求解器的稳定性和输出质量，减少了不理想的伪影。摘要未明确说明具体性能指标，但强调了MCLC能够与现有求解器协同工作，并在多个任务中验证了其稳健性。",
      "conclusion": "MCLC是朝着更鲁棒零样本逆问题求解器的关键步骤，解决了现有求解器的不稳定性问题。研究不仅提供了理论基础的校正方法，还具有实际应用价值，作为即插即用模块增强现有系统的可靠性。此外，论文分析了斑点伪影的成因，为未来改进和进一步研究提供了见解。潜在局限性包括未详细讨论特定应用场景，未来工作可扩展至更广泛的逆问题领域。",
      "tags": [
        "Latent Diffusion Models",
        "Inverse Solvers",
        "Langevin Dynamics",
        "Zero-Shot Learning",
        "Image Restoration"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:44.436540Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04785",
    "title": "SRU-Pix2Pix: A Fusion-Driven Generator Network for Medical Image Translation with Few-Shot Learning",
    "authors": [
      "Xihe Qiu",
      "Yang Dai",
      "Xiaoyu Tan",
      "Sijia Li",
      "Fenghao Sun",
      "Lu Gan",
      "Liang Liu"
    ],
    "abstract": "Magnetic Resonance Imaging (MRI) provides detailed tissue information, but its clinical application is limited by long acquisition time, high cost, and restricted resolution. Image translation has recently gained attention as a strategy to address these limitations. Although Pix2Pix has been widely applied in medical image translation, its potential has not been fully explored. In this study, we propose an enhanced Pix2Pix framework that integrates Squeeze-and-Excitation Residual Networks (SEResNet) and U-Net++ to improve image generation quality and structural fidelity. SEResNet strengthens critical feature representation through channel attention, while U-Net++ enhances multi-scale feature fusion. A simplified PatchGAN discriminator further stabilizes training and refines local anatomical realism. Experimental results demonstrate that under few-shot conditions with fewer than 500 images, the proposed method achieves consistent structural fidelity and superior image quality across multiple intra-modality MRI translation tasks, showing strong generalization ability. These results suggest an effective extension of Pix2Pix for medical image translation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04785.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04785",
    "published": "2026-01-08T10:10:03Z",
    "updated": "2026-01-08T10:10:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出SRU-Pix2Pix，一种融合SEResNet和U-Net++的增强型Pix2Pix框架，旨在改进医学图像翻译的生成质量和结构保真度，特别适用于少样本学习场景。",
      "motivation": "MRI在临床应用中面临获取时间长、成本高和分辨率受限等挑战，图像翻译技术被视为解决这些问题的有效策略。尽管Pix2Pix在医学图像翻译中广泛应用，但其在图像质量和结构保真度方面仍有待提升。现有方法未能充分利用先进网络架构以优化特征表示，本研究旨在通过融合通道注意力和多尺度特征融合技术，弥补Pix2Pix的不足，满足临床对高效、高质量图像处理的需求，从而推动医学影像分析的进步。",
      "method": "论文提出了一个增强的Pix2Pix框架，融合了SEResNet和U-Net++。SEResNet通过通道注意力机制强化关键特征的表示能力，U-Net++则实现更精细的多尺度特征融合，以提升图像生成质量。此外，采用简化的PatchGAN鉴别器来稳定训练过程，并优化局部解剖结构的真实性。该方法在少于500张图像的少样本条件下进行训练，应用于多个模态内MRI翻译任务，通过这种集成设计，实现了对传统Pix2Pix的有效扩展，特别关注于提高结构保真度和图像清晰度。",
      "result": "实验结果表明，在少于500张图像的少样本条件下，SRU-Pix2Pix在多个模态内MRI翻译任务中实现了一致的结构保真度和卓越的图像质量。摘要未明确说明具体性能指标如准确率或效率改进的具体数值，但强调了该方法展现出强大的泛化能力。与基线方法相比，其性能在少样本场景中得到验证，表明在资源受限的医学图像处理中具有显著优势。这些结果证实了融合策略的有效性，为医学图像翻译提供了新的研究方向。",
      "conclusion": "本研究的主要贡献是通过集成SEResNet和U-Net++，扩展了Pix2Pix框架，显著提升了医学图像翻译的质量和结构保真度。这不仅具有学术价值，如推动生成对抗网络在医学领域的应用，还具有实际应用意义，为临床少样本图像翻译提供了高效工具。局限性可能包括对其他医学成像模态的泛化能力待进一步验证，未来工作可探索网络优化、扩展到更多任务或结合其他先进技术，以增强实用性和适应性。",
      "tags": [
        "Squeeze-and-Excitation Residual Networks",
        "U-Net++",
        "PatchGAN",
        "Medical Image Translation",
        "Few-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:11.631321Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04779",
    "title": "Defocus Aberration Theory Confirms Gaussian Model in Most Imaging Devices",
    "authors": [
      "Akbar Saadat"
    ],
    "abstract": "Over the past three decades, defocus has consistently provided groundbreaking depth information in scene images. However, accurately estimating depth from 2D images continues to be a persistent and fundamental challenge in the field of 3D recovery. Heuristic approaches involve with the ill-posed problem for inferring the spatial variant defocusing blur, as the desired blur cannot be distinguished from the inherent blur. Given a prior knowledge of the defocus model, the problem become well-posed with an analytic solution for the relative blur between two images, taken at the same viewpoint with different camera settings for the focus. The Gaussian model stands out as an optimal choice for real-time applications, due to its mathematical simplicity and computational efficiency. And theoretically, it is the only model can be applied at the same time to both the absolute blur caused by depth in a single image and the relative blur resulting from depth differences between two images. This paper introduces the settings, for conventional imaging devices, to ensure that the defocusing operator adheres to the Gaussian model. Defocus analysis begins within the framework of geometric optics and is conducted by defocus aberration theory in diffraction-limited optics to obtain the accuracy of fitting the actual model to its Gaussian approximation. The results for a typical set of focused depths between $1$ and $100$ meters, with a maximum depth variation of $10\\%$ at the focused depth, confirm the Gaussian model's applicability for defocus operators in most imaging devices. The findings demonstrate a maximum Mean Absolute Error $(\\!M\\!A\\!E)$ of less than $1\\%$, underscoring the model's accuracy and reliability.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04779.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04779",
    "published": "2026-01-08T10:03:13Z",
    "updated": "2026-01-08T10:03:13Z",
    "comment": "13 pages, 9 figures, 11 .jpg files",
    "light_analysis": {
      "overview": "论文通过散焦像差理论验证了高斯模型在大多数成像设备中的适用性和准确性，为深度估计提供了可靠方法。",
      "motivation": "该研究旨在解决3D恢复领域从2D图像准确估计深度的根本挑战。散焦在场景图像中提供深度信息，但深度估计一直是一个持续性问题，因为启发式方法可能因无法区分所需模糊和固有模糊而不适定。在已知散焦模型先验知识的情况下，问题变得适定，高斯模型因其数学简单性和计算效率成为实时应用的最佳选择。因此，研究动机是验证高斯模型在实际成像设备中的有效性，以改进深度估计的准确性和效率。",
      "method": "论文提出了在常规成像设备中确保散焦算子遵循高斯模型的设置。方法从几何光学框架开始，应用散焦像差理论于衍射极限光学，分析实际模型拟合高斯近似的准确性。具体分析覆盖了典型聚焦深度范围（1到100米）和最大深度变化（10%），以评估高斯模型在不同条件下的表现，关键创新点在于通过理论分析确认模型适用性。",
      "result": "研究结果表明，在聚焦深度1到100米、最大深度变化10%的条件下，高斯模型在大多数成像设备的散焦算子中适用。最大平均绝对误差（MAE）小于1%，证明了模型的准确性和可靠性。这一结果基于理论分析，验证了高斯模型在深度估计中的高精度，为实时应用提供了性能支撑。",
      "conclusion": "论文的主要贡献是通过散焦像差理论证实了高斯模型在大多数成像设备中的适用性，最大误差小于1%，这为深度估计提供了可靠的理论基础，尤其适用于实时应用，具有重要的学术和实际价值。局限性在于分析仅限于特定深度范围，未来工作可扩展到更广泛条件或不同设备类型。",
      "tags": [
        "Defocus Aberration Theory",
        "Gaussian Model",
        "Depth Estimation",
        "Geometric Optics",
        "Imaging Devices"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:59.597164Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.15361",
    "title": "Boosting HDR Image Reconstruction via Semantic Knowledge Transfer",
    "authors": [
      "Tao Hu",
      "Longyao Wu",
      "Wei Dong",
      "Peng Wu",
      "Jinqiu Sun",
      "Xiaogang Xu",
      "Qingsen Yan",
      "Yanning Zhang"
    ],
    "abstract": "Recovering High Dynamic Range (HDR) images from multiple Standard Dynamic Range (SDR) images become challenging when the SDR images exhibit noticeable degradation and missing content. Leveraging scene-specific semantic priors offers a promising solution for restoring heavily degraded regions. However, these priors are typically extracted from sRGB SDR images, the domain/format gap poses a significant challenge when applying it to HDR imaging. To address this issue, we propose a general framework that transfers semantic knowledge derived from SDR domain via self-distillation to boost existing HDR reconstruction. Specifically, the proposed framework first introduces the Semantic Priors Guided Reconstruction Model (SPGRM), which leverages SDR image semantic knowledge to address ill-posed problems in the initial HDR reconstruction results. Subsequently, we leverage a self-distillation mechanism that constrains the color and content information with semantic knowledge, aligning the external outputs between the baseline and SPGRM. Furthermore, to transfer the semantic knowledge of the internal features, we utilize a Semantic Knowledge Alignment Module (SKAM) to fill the missing semantic contents with the complementary masks. Extensive experiments demonstrate that our framework significantly boosts HDR imaging quality for existing methods without altering the network architecture.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.15361.pdf",
    "abs_url": "https://arxiv.org/abs/2503.15361",
    "published": "2025-03-19T16:01:27Z",
    "updated": "2026-01-08T10:00:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个通用框架，通过自蒸馏将SDR域的语义知识转移到HDR图像重建中，显著提升现有方法的成像质量。",
      "motivation": "在从多张标准动态范围（SDR）图像重建高动态范围（HDR）图像时，SDR图像常出现退化和内容缺失问题，导致重建效果不佳。现有方法利用场景语义先验来恢复退化区域，但这些先验从SDR图像提取，应用于HDR成像时存在域差距，限制了其有效性。因此，本研究旨在解决这一域差距问题，通过语义知识转移来提升HDR图像重建的鲁棒性和质量，以适应实际应用中图像退化的挑战。",
      "method": "本研究提出一个通用框架，首先引入语义先验引导重建模型（SPGRM），利用SDR图像的语义知识处理初始HDR重建中的病态问题。其次，采用自蒸馏机制，通过语义知识约束颜色和内容信息，对齐基线模型和SPGRM的输出。此外，使用语义知识对齐模块（SKAM）通过互补掩码转移内部特征的语义知识，填充缺失内容。框架设计为不改变现有网络架构，兼容各种HDR重建方法，突出了语义知识转移和自蒸馏的创新点。",
      "result": "实验结果表明，所提出的框架能够显著提升现有HDR重建方法的成像质量。通过语义知识转移，框架有效减少了退化区域的影响，改善了HDR图像的整体质量，在不修改网络架构的情况下实现性能提升。与基线方法相比，框架显示出明显的优势，但摘要未明确说明具体的定量指标如PSNR或SSIM值，仅强调了对现有方法的增强效果。",
      "conclusion": "本文的主要贡献是提出一个通过语义知识转移提升HDR图像重建的通用框架，利用自蒸馏和语义对齐克服SDR到HDR的域差距。研究具有学术价值，为HDR重建提供了新思路，并在实际应用中能兼容现有方法，降低部署成本。潜在的局限性包括对语义先验提取的依赖，未来工作可能涉及优化知识转移机制或在更多数据集上验证通用性。",
      "tags": [
        "High Dynamic Range Imaging",
        "Semantic Knowledge Transfer",
        "Self-Distillation",
        "Image Reconstruction",
        "Semantic Priors"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:09.403755Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04777",
    "title": "GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models",
    "authors": [
      "Shurong Zheng",
      "Yousong Zhu",
      "Hongyin Zhao",
      "Fan Yang",
      "Yufei Zhan",
      "Ming Tang",
      "Jinqiao Wang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive progress in single-image grounding and general multi-image understanding. Recently, some methods begin to address multi-image grounding. However, they are constrained by single-target localization and limited types of practical tasks, due to the lack of unified modeling for generalized grounding tasks. Therefore, we propose GeM-VG, an MLLM capable of Generalized Multi-image Visual Grounding. To support this, we systematically categorize and organize existing multi-image grounding tasks according to their reliance of cross-image cues and reasoning, and introduce the MG-Data-240K dataset, addressing the limitations of existing datasets regarding target quantity and image relation. To tackle the challenges of robustly handling diverse multi-image grounding tasks, we further propose a hybrid reinforcement finetuning strategy that integrates chain-of-thought (CoT) reasoning and direct answering, considering their complementary strengths. This strategy adopts an R1-like algorithm guided by a carefully designed rule-based reward, effectively enhancing the model's overall perception and reasoning capabilities. Extensive experiments demonstrate the superior generalized grounding capabilities of our model. For multi-image grounding, it outperforms the previous leading MLLMs by 2.0% and 9.7% on MIG-Bench and MC-Bench, respectively. In single-image grounding, it achieves a 9.1% improvement over the base model on ODINW. Furthermore, our model retains strong capabilities in general multi-image understanding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04777.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04777",
    "published": "2026-01-08T09:58:35Z",
    "updated": "2026-01-08T09:58:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出GeM-VG，一种多模态大语言模型，通过引入新数据集和混合强化微调策略，实现了广义多图像视觉接地的卓越性能。",
      "motivation": "多模态大语言模型在单图像接地和多图像理解中取得进展，但现有多图像接地方法受限于单目标定位和有限任务类型，缺乏统一建模，这阻碍了广义接地任务的开发。广义多图像视觉接地对复杂场景分析和跨图像推理至关重要，但现有数据集在目标数量和图像关系方面不足，且任务多样性有限，限制了实际应用和性能提升，因此需研究更全面和高效的模型来处理多样接地挑战。",
      "method": "论文提出GeM-VG模型，基于多模态大语言模型架构。首先，系统性分类多图像接地任务，依据跨图像线索和推理依赖，并构建MG-Data-240K数据集，解决现有数据在目标数量和图像关系上的限制。其次，设计混合强化微调策略，整合链式思维推理和直接回答，利用它们的互补优势；采用类似R1的算法，以基于规则的奖励为指导，增强模型的整体感知和推理能力，从而稳健处理多样多图像接地任务。",
      "result": "广泛实验表明，GeM-VG在广义多图像接地上表现优越：在MIG-Bench和MC-Bench数据集上，分别比先前领先多模态大语言模型提升2.0%和9.7%。在单图像接地上，于ODINW数据集上比基础模型提高9.1%。此外，模型在一般多图像理解任务中保持强能力，这验证了其泛化能力和鲁棒性，超越了现有方法在处理多样接地场景时的表现。",
      "conclusion": "本研究的主要贡献是提出GeM-VG模型、新数据集和混合微调策略，显著提升了广义多图像视觉接地的性能。学术上，推动了视觉接地领域的统一建模和任务泛化；实际应用上，可促进多图像场景理解和跨模态交互技术的发展。局限性或未来工作方向（摘要未明确说明），可能包括扩展任务类型、优化数据集覆盖范围，以及探索更复杂的推理机制。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Grounding",
        "Chain-of-Thought Reasoning",
        "Reinforcement Learning Finetuning",
        "Multi-image Understanding"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:33.406065Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04776",
    "title": "Segmentation-Driven Monocular Shape from Polarization based on Physical Model",
    "authors": [
      "Jinyu Zhang",
      "Xu Ma",
      "Weili Chen",
      "Gonzalo R. Arce"
    ],
    "abstract": "Monocular shape-from-polarization (SfP) leverages the intrinsic relationship between light polarization properties and surface geometry to recover surface normals from single-view polarized images, providing a compact and robust approach for three-dimensional (3D) reconstruction. Despite its potential, existing monocular SfP methods suffer from azimuth angle ambiguity, an inherent limitation of polarization analysis, that severely compromises reconstruction accuracy and stability. This paper introduces a novel segmentation-driven monocular SfP (SMSfP) framework that reformulates global shape recovery into a set of local reconstructions over adaptively segmented convex sub-regions. Specifically, a polarization-aided adaptive region growing (PARG) segmentation strategy is proposed to decompose the global convexity assumption into locally convex regions, effectively suppressing azimuth ambiguities and preserving surface continuity. Furthermore, a multi-scale fusion convexity prior (MFCP) constraint is developed to ensure local surface consistency and enhance the recovery of fine textural and structural details. Extensive experiments on both synthetic and real-world datasets validate the proposed approach, showing significant improvements in disambiguation accuracy and geometric fidelity compared with existing physics-based monocular SfP techniques.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04776.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04776",
    "published": "2026-01-08T09:57:47Z",
    "updated": "2026-01-08T09:57:47Z",
    "comment": "11 pages, 10 figures, submittd to IEEE Transactions on Image Processing",
    "light_analysis": {
      "overview": "本文提出了一种分割驱动的单目形状从偏振框架，通过局部重建和凸性先验有效解决方位角模糊问题，显著提升了三维重建的准确性和几何保真度。",
      "motivation": "单目形状从偏振技术利用偏振光属性与表面几何的内在关系，从单视图偏振图像恢复表面法线，提供了一种紧凑且鲁棒的三维重建方法。然而，现有方法受限于方位角模糊性，这是偏振分析的固有局限，严重损害重建精度和稳定性，限制了其在真实世界应用中的实用性。因此，本研究旨在开发新方法来克服这一不足，通过改进局部处理机制，解决全局形状恢复中的模糊性挑战，从而推动基于物理的视觉重建技术发展。",
      "method": "本研究提出了一种新颖的分割驱动单目形状从偏振框架，通过将全局形状恢复分解为自适应分割的凸子区域进行局部重建来消除方位角模糊。核心方法包括偏振辅助自适应区域增长分割策略，它基于物理模型将全局凸性假设分解为局部凸区域，以抑制模糊并保持表面连续性；以及多尺度融合凸性先验约束，通过整合多尺度信息确保局部表面一致性，并增强纹理和结构细节的恢复。这些技术利用偏振数据和几何先验，优化了传统单目SfP的局限。",
      "result": "在合成和真实世界数据集上进行的广泛实验验证了所提出方法的有效性。结果表明，与现有基于物理的单目形状从偏振技术相比，该方法在去模糊准确性和几何保真度方面有显著改进，尽管摘要未提供具体准确率数字，但强调了在抑制方位模糊和恢复细节方面的明显优势。实验显示该方法能更稳定地处理复杂表面，提高三维重建的整体质量，证明了其相对于基线方法的优越性能。",
      "conclusion": "本研究的主要贡献是提出了一种分割驱动的单目形状从偏振框架，通过局部重建和凸性先验约束成功解决了方位角模糊问题，提升了三维重建的精度和鲁棒性。其学术价值在于推进了基于偏振的形状恢复技术，丰富了计算机视觉中的物理模型应用；实际应用价值在于为单目系统提供了更可靠和紧凑的三维重建方案。虽然摘要未明确说明局限性，但该方法可能依赖于分割准确性，未来工作可探索更鲁棒的分割算法或扩展到非凸表面处理。",
      "tags": [
        "Monocular Shape from Polarization",
        "Segmentation",
        "Convexity Prior",
        "Physical Model",
        "Multi-scale Fusion"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:43.643143Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.13425",
    "title": "CaTFormer: Causal Temporal Transformer with Dynamic Contextual Fusion for Driving Intention Prediction",
    "authors": [
      "Sirui Wang",
      "Zhou Guan",
      "Bingxi Zhao",
      "Tongjia Gu",
      "Jie Liu"
    ],
    "abstract": "Accurate prediction of driving intention is key to enhancing the safety and interactive efficiency of human-machine co-driving systems. It serves as a cornerstone for achieving high-level autonomous driving. However, current approaches remain inadequate for accurately modeling the complex spatiotemporal interdependencies and the unpredictable variability of human driving behavior. To address these challenges, we propose CaTFormer, a causal Temporal Transformer that explicitly models causal interactions between driver behavior and environmental context for robust intention prediction. Specifically, CaTFormer introduces a novel Reciprocal Delayed Fusion (RDF) mechanism for precise temporal alignment of interior and exterior feature streams, a Counterfactual Residual Encoding (CRE) module that systematically eliminates spurious correlations to reveal authentic causal dependencies, and an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these purified representations into coherent temporal representations. Experimental results demonstrate that CaTFormer attains state-of-the-art performance on the Brain4Cars dataset. It effectively captures complex causal temporal dependencies and enhances both the accuracy and transparency of driving intention prediction.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.13425.pdf",
    "abs_url": "https://arxiv.org/abs/2507.13425",
    "published": "2025-07-17T17:10:37Z",
    "updated": "2026-01-08T09:25:09Z",
    "comment": "Accepted at AAAI 2026",
    "light_analysis": {
      "overview": "CaTFormer是一个因果时序Transformer模型，通过建模驾驶员行为与环境上下文的因果交互，实现鲁棒的驾驶意图预测。",
      "motivation": "准确预测驾驶意图对提升人机协同驾驶系统安全性和交互效率至关重要，是实现高级自动驾驶的基石。然而，现有方法在建模复杂时空相互依赖性和人类驾驶行为不可预测变异性方面仍有不足，这导致预测准确性受限，影响了系统的可靠性和透明性。因此，需要一种能够捕获真实因果依赖的新方法来克服这些挑战。",
      "method": "CaTFormer是一个基于Transformer的因果时序模型，核心创新包括：互惠延迟融合（RDF）机制用于精确对齐内部（驾驶员行为）和外部（环境上下文）特征流的时间关系；反事实残差编码（CRE）模块系统性地消除虚假相关性，揭示真实因果依赖；特征合成网络（FSN）自适应地合成净化后的特征，形成连贯的时序表示。在Brain4Cars数据集上实现，以动态上下文融合提升预测性能。",
      "result": "实验在Brain4Cars数据集上进行，CaTFormer达到了最先进的性能，有效捕捉了复杂因果时序依赖，显著提升了驾驶意图预测的准确性并增强了透明度。与基线方法相比，模型在建模复杂时空交互方面表现优异，但具体性能指标如准确率提升百分比摘要未明确说明。",
      "conclusion": "CaTFormer通过因果建模和动态上下文融合，为驾驶意图预测提供了创新解决方案，贡献在于提出RDF、CRE和FSN等机制改进预测准确性和可解释性。这项研究对自动驾驶安全性和人机交互效率有重要应用价值，未来工作可扩展到其他时序预测任务或优化模型实时性能。",
      "tags": [
        "Causal Temporal Transformer",
        "Reciprocal Delayed Fusion",
        "Counterfactual Residual Encoding",
        "Feature Synthesis Network",
        "Driving Intention Prediction"
      ]
    },
    "analyzed_at": "2026-01-09T02:46:51.559791Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04754",
    "title": "ProFuse: Efficient Cross-View Context Fusion for Open-Vocabulary 3D Gaussian Splatting",
    "authors": [
      "Yen-Jen Chiou",
      "Wei-Tse Cheng",
      "Yuan-Fu Yang"
    ],
    "abstract": "We present ProFuse, an efficient context-aware framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting (3DGS). The pipeline enhances cross-view consistency and intra-mask cohesion within a direct registration setup, adding minimal overhead and requiring no render-supervised fine-tuning. Instead of relying on a pretrained 3DGS scene, we introduce a dense correspondence-guided pre-registration phase that initializes Gaussians with accurate geometry while jointly constructing 3D Context Proposals via cross-view clustering. Each proposal carries a global feature obtained through weighted aggregation of member embeddings, and this feature is fused onto Gaussians during direct registration to maintain per-primitive language coherence across views. With associations established in advance, semantic fusion requires no additional optimization beyond standard reconstruction, and the model retains geometric refinement without densification. ProFuse achieves strong open-vocabulary 3DGS understanding while completing semantic attachment in about five minutes per scene, which is two times faster than SOTA.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04754.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04754",
    "published": "2026-01-08T09:20:46Z",
    "updated": "2026-01-08T09:20:46Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "ProFuse 提出了一种高效的跨视图上下文融合框架，用于开放词汇的 3D 高斯喷射场景理解，显著提升了语义附着速度和跨视图一致性。",
      "motivation": "该研究旨在解决开放词汇 3D 场景理解中的跨视图一致性和效率问题。随着 3D 高斯喷射在增强现实和机器人导航等领域的应用增长，现有方法常在语义附着时效率低下或需要额外监督微调，增加了计算开销。ProFuse 通过避免依赖预训练场景和渲染监督，提供了一个高效框架，以在直接配准设置中增强跨视图一致性并减少处理时间，从而应对现有技术中几何和语义融合不足的挑战。",
      "method": "ProFuse 框架引入了一个密集对应引导的预配准阶段，用于初始化高斯分布并同时通过跨视图聚类构建 3D 上下文建议。每个建议携带通过成员嵌入加权聚合获得的全局特征，在直接配准过程中融合到高斯上，以保持每个基元跨视图的语言一致性。该方法无需依赖预训练 3DGS 场景，避免了渲染监督微调，仅通过标准重建流程完成语义融合，并保留几何细化而不需要额外致密化，从而实现高效上下文感知。",
      "result": "ProFuse 在开放词汇 3D 高斯喷射理解方面表现优异，语义附着过程每个场景仅需约五分钟，比当前最先进方法快两倍。模型在保持几何精度和跨视图一致性的同时，无需额外优化或致密化，显著提升了处理效率。实验结果显示其速度优势明显，适用于实时应用，但没有提供具体准确率数据，摘要未明确说明其他性能指标细节。",
      "conclusion": "ProFuse 的主要贡献在于提出了一个无需预训练和微调的高效跨视图上下文融合框架，改善了开放词汇 3D 场景理解的效率和一致性。学术上，它推动了 3D 高斯喷射技术的发展；实际上，快速语义附着有助于增强现实和机器人导航等实时应用。未来工作可扩展到更复杂场景或集成更多语言模型，但摘要未明确说明局限性。",
      "tags": [
        "3D Gaussian Splatting",
        "Open-Vocabulary 3D Understanding",
        "Cross-View Context Fusion",
        "Dense Correspondence",
        "3D Context Proposals"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:05.736704Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04752",
    "title": "Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition",
    "authors": [
      "Masatomo Yoshida",
      "Haruto Namura",
      "Nicola Adami",
      "Masahiro Okuda"
    ],
    "abstract": "This work explores the visual capabilities and limitations of foundation models by introducing a novel adversarial attack method utilizing skeletonization to reduce the search space effectively. Our approach specifically targets images containing text, particularly mathematical formula images, which are more challenging due to their LaTeX conversion and intricate structure. We conduct a detailed evaluation of both character and semantic changes between original and adversarially perturbed outputs to provide insights into the models' visual interpretation and reasoning abilities. The effectiveness of our method is further demonstrated through its application to ChatGPT, which shows its practical implications in real-world scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04752.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04752",
    "published": "2026-01-08T09:15:27Z",
    "updated": "2026-01-08T09:15:27Z",
    "comment": "accepted to ITC-CSCC 2025",
    "light_analysis": {
      "overview": "本文提出一种基于骨架化的对抗攻击方法，探索大型视觉语言模型在数学公式图像识别中的视觉能力与局限性。",
      "motivation": "研究动机在于评估基础模型在处理复杂视觉任务时的能力，特别是针对包含文本的图像，如数学公式图像，这些图像因LaTeX转换和结构复杂而更具挑战性。现有方法可能未能充分测试模型的鲁棒性和推理能力，因此通过对抗攻击来揭示其弱点，有助于理解模型在实际应用中的局限性，并为改进模型提供依据。",
      "method": "研究方法采用骨架化技术来生成对抗性扰动，通过提取图像中文本（尤其是数学公式）的骨架结构，有效减少搜索空间。核心创新在于利用骨架化针对特定视觉内容，评估原始和扰动输出的字符与语义变化，以分析模型的视觉解释能力。实验涉及大型视觉语言模型，如ChatGPT，应用骨架化处理图像输入，但摘要未明确说明具体数据集和模型架构细节。",
      "result": "主要实验结果显示，通过详细评估字符和语义变化，该方法提供了对模型视觉解释和推理能力的见解，揭示了其在数学文本识别中的局限性。在ChatGPT上的应用进一步证明了方法的有效性，显示了在现实世界场景中的实际意义，但摘要未明确说明具体性能指标（如准确率提升）或与基线方法的对比数据。",
      "conclusion": "结论指出，本研究提出了一种新颖的基于骨架化的对抗攻击方法，为探索大型视觉语言模型的视觉能力提供了新工具，具有重要学术价值。实际应用价值体现在对ChatGPT等模型的测试中，展示了潜在的现实影响。未来工作可能包括扩展到其他视觉任务或模型类型，以进一步评估和改进模型的鲁棒性。",
      "tags": [
        "Adversarial Attacks",
        "Skeletonization",
        "Vision Language Models",
        "Mathematical Text Recognition",
        "ChatGPT"
      ]
    },
    "analyzed_at": "2026-01-09T02:48:31.167269Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.05633",
    "title": "SynDroneVision: A Synthetic Dataset for Image-Based Drone Detection",
    "authors": [
      "Tamara R. Lenhard",
      "Andreas Weinmann",
      "Kai Franke",
      "Tobias Koch"
    ],
    "abstract": "Developing robust drone detection systems is often constrained by the limited availability of large-scale annotated training data and the high costs associated with real-world data collection. However, leveraging synthetic data generated via game engine-based simulations provides a promising and cost-effective solution to overcome this issue. Therefore, we present SynDroneVision, a synthetic dataset specifically designed for RGB-based drone detection in surveillance applications. Featuring diverse backgrounds, lighting conditions, and drone models, SynDroneVision offers a comprehensive training foundation for deep learning algorithms. To evaluate the dataset's effectiveness, we perform a comparative analysis across a selection of recent YOLO detection models. Our findings demonstrate that SynDroneVision is a valuable resource for real-world data enrichment, achieving notable enhancements in model performance and robustness, while significantly reducing the time and costs of real-world data acquisition. SynDroneVision will be publicly released upon paper acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2411.05633.pdf",
    "abs_url": "https://arxiv.org/abs/2411.05633",
    "published": "2024-11-08T15:22:49Z",
    "updated": "2026-01-08T09:08:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SynDroneVision合成数据集，通过游戏引擎生成多样化训练数据，以增强基于RGB的无人机检测模型性能，降低成本和时间。",
      "motivation": "无人机检测系统的开发常受限于大规模标注训练数据的稀缺性和真实世界数据收集的高昂成本。现有方法依赖真实数据，导致训练效率低下且部署成本高，难以满足实际应用需求。因此，利用合成数据作为补充或替代方案变得至关重要，以解决数据不足问题并促进无人机检测技术的快速发展，特别是在监控等关键领域。",
      "method": "本研究构建了SynDroneVision合成数据集，专为RGB-based无人机检测设计。方法采用游戏引擎仿真技术生成数据，通过模拟多样化的背景、光照条件和无人机模型来提供全面的训练基础。关键创新在于合成数据的系统化生成策略，以丰富训练样本并提升模型泛化能力。评估方面，选取一系列YOLO检测模型进行对比分析，验证数据集在深度学习算法中的实用性，确保技术路线简洁有效。",
      "result": "实验通过对多种YOLO检测模型进行对比评估，结果显示SynDroneVision数据集显著提升了模型的性能和鲁棒性。具体表现为检测准确度增强，并在真实世界应用中有效减少了数据收集的时间和成本。尽管摘要未提供精确的性能指标如准确率提升百分比，但与基线方法相比，合成数据的使用明显改善了训练效果，证明其在无人机检测任务中的实用价值和推广潜力。",
      "conclusion": "本文的主要贡献是SynDroneVision合成数据集，为无人机检测提供了有效的训练资源，解决了数据稀缺和成本问题。学术价值体现在展示了合成数据在计算机视觉领域的应用潜力，推动低成本数据生成方法的发展；实际应用价值在于加速无人机检测系统的部署并降低开发门槛。未来工作可探索数据泛化到其他检测任务或结合更多先进模型，以进一步提升性能。",
      "tags": [
        "Synthetic Dataset",
        "Drone Detection",
        "YOLO Models",
        "Game Engine Simulation",
        "Computer Vision"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:17.462062Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04734",
    "title": "AIVD: Adaptive Edge-Cloud Collaboration for Accurate and Efficient Industrial Visual Detection",
    "authors": [
      "Yunqing Hu",
      "Zheming Yang",
      "Chang Zhao",
      "Qi Guo",
      "Meng Gao",
      "Pengcheng Li",
      "Wen Ji"
    ],
    "abstract": "Multimodal large language models (MLLMs) demonstrate exceptional capabilities in semantic understanding and visual reasoning, yet they still face challenges in precise object localization and resource-constrained edge-cloud deployment. To address this, this paper proposes the AIVD framework, which achieves unified precise localization and high-quality semantic generation through the collaboration between lightweight edge detectors and cloud-based MLLMs. To enhance the cloud MLLM's robustness against edge cropped-box noise and scenario variations, we design an efficient fine-tuning strategy with visual-semantic collaborative augmentation, significantly improving classification accuracy and semantic consistency. Furthermore, to maintain high throughput and low latency across heterogeneous edge devices and dynamic network conditions, we propose a heterogeneous resource-aware dynamic scheduling algorithm. Experimental results demonstrate that AIVD substantially reduces resource consumption while improving MLLM classification performance and semantic generation quality. The proposed scheduling strategy also achieves higher throughput and lower latency across diverse scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04734.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04734",
    "published": "2026-01-08T08:56:07Z",
    "updated": "2026-01-08T08:56:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出AIVD框架，通过轻量级边缘检测器与云端MLLM协作，实现工业视觉检测中精确对象定位和高质量语义生成的统一解决方案。",
      "motivation": "本研究旨在解决多模态大语言模型（MLLMs）在工业视觉检测中的关键挑战：尽管MLLMs具备强大的语义理解和视觉推理能力，但它们在精确对象定位方面表现不足，且难以在资源受限的边缘设备上高效部署。现有方法往往忽视了边缘-云协作的动态性，导致在异构设备和变化网络条件下性能下降、延迟增加。工业应用对准确性和实时性要求高，因此开发一个能平衡精度与效率的框架至关重要，以提升实际部署的可靠性。",
      "method": "论文提出的AIVD框架结合轻量级边缘检测器进行初步对象定位，与云端MLLM协作以生成高质量语义。核心创新包括：设计基于视觉语义协同增强的高效微调策略，通过数据增强提高MLLM对边缘噪声和场景变化的鲁棒性；引入异构资源感知动态调度算法，根据边缘设备计算能力和网络状态动态调整任务分配。该方法集成了多模态数据处理和自适应资源管理，确保在多变环境中维持高性能和低延迟。",
      "result": "实验结果表明，AIVD框架显著降低了资源消耗，同时提升了MLLM的分类准确率和语义生成一致性。与基线方法相比，该框架在分类任务中表现出更高的性能，语义生成质量得到改善，增强了整体检测的可靠性。此外，提出的调度算法在不同场景下实现了更高的吞吐量和更低的延迟，证明了其在高动态工业环境中的适应性和效率优势。这些效果验证了AIVD在平衡精确性与资源效率方面的有效性。",
      "conclusion": "本研究的主要贡献是开发了AIVD框架，有效整合边缘与云资源，解决了MLLMs在工业视觉检测中的精确定位和高效部署难题。其学术价值在于推动了多模态模型在资源受限场景下的应用，并创新了协同增强和动态调度技术。实际应用价值包括为智能制造等领域提供可靠、可扩展的解决方案。未来工作可扩展至更多模态融合或优化算法以应对更复杂的实时任务。",
      "tags": [
        "Multimodal Large Language Models",
        "Edge-Cloud Collaboration",
        "Fine-Tuning",
        "Resource Scheduling",
        "Industrial Visual Detection"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:12.251787Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04727",
    "title": "Training a Custom CNN on Five Heterogeneous Image Datasets",
    "authors": [
      "Anika Tabassum",
      "Tasnuva Mahazabin Tuba",
      "Nafisa Naznin"
    ],
    "abstract": "Deep learning has transformed visual data analysis, with Convolutional Neural Networks (CNNs) becoming highly effective in learning meaningful feature representations directly from images. Unlike traditional manual feature engineering methods, CNNs automatically extract hierarchical visual patterns, enabling strong performance across diverse real-world contexts. This study investigates the effectiveness of CNN-based architectures across five heterogeneous datasets spanning agricultural and urban domains: mango variety classification, paddy variety identification, road surface condition assessment, auto-rickshaw detection, and footpath encroachment monitoring. These datasets introduce varying challenges, including differences in illumination, resolution, environmental complexity, and class imbalance, necessitating adaptable and robust learning models.   We evaluate a lightweight, task-specific custom CNN alongside established deep architectures, including ResNet-18 and VGG-16, trained both from scratch and using transfer learning. Through systematic preprocessing, augmentation, and controlled experimentation, we analyze how architectural complexity, model depth, and pre-training influence convergence, generalization, and performance across datasets of differing scale and difficulty. The key contributions of this work are: (1) the development of an efficient custom CNN that achieves competitive performance across multiple application domains, and (2) a comprehensive comparative analysis highlighting when transfer learning and deep architectures provide substantial advantages, particularly in data-constrained environments. These findings offer practical insights for deploying deep learning models in resource-limited yet high-impact real-world visual classification tasks.",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04727.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04727",
    "published": "2026-01-08T08:44:17Z",
    "updated": "2026-01-08T08:44:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文开发了一种高效的自定义卷积神经网络，并在五个异构图像数据集上进行了比较分析，揭示了迁移学习在数据受限环境中的优势。",
      "motivation": "该研究旨在解决现实世界视觉分类任务中因数据集异构性带来的挑战，包括农业和城市领域的图像分类问题，如芒果品种分类和路面状况评估。这些数据集存在光照、分辨率、环境复杂性和类不平衡等多样性问题，传统手动特征工程方法效率较低，而现有深度学习模型可能过参数化或需要大量数据，导致在资源有限环境中部署困难。研究的重要性在于为高影响应用提供适配性强且鲁棒的解决方案。",
      "method": "论文提出了一种轻量级、任务特定的自定义CNN，并与经典深度架构如ResNet-18和VGG-16进行比较。方法采用从头训练和迁移学习两种策略，在五个异构图像数据集上进行系统预处理、数据增强和控制实验，以分析架构复杂性、模型深度和预训练对模型收敛性、泛化能力和性能的影响。关键创新点在于通过自定义CNN实现效率和适应性，并全面评估不同策略的效果。",
      "result": "实验结果表明，自定义CNN在多个应用领域中实现了竞争性性能，与基线方法如ResNet-18和VGG-16相比表现良好。比较分析显示，迁移学习和深层架构在数据受限环境下提供显著优势，能够提升模型的泛化能力和效果。尽管摘要未明确说明具体准确率数据，但研究通过系统性评估，为模型选择提供了实证依据，尤其在处理异构数据集时表现突出。",
      "conclusion": "论文的主要贡献是开发了高效的自定义CNN并进行了全面的比较分析，学术上深化了对CNN在异构数据上表现的理解，实际上为资源有限的视觉分类任务部署提供了指导。研究的实际应用价值在于帮助优化模型选择，提高在农业和城市监控等领域的效率。未来工作可进一步探索模型在更广泛数据集上的泛化能力或优化计算效率。",
      "tags": [
        "Convolutional Neural Networks",
        "Transfer Learning",
        "Image Classification",
        "Dataset Heterogeneity",
        "Model Architecture Comparison"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:44.944544Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04715",
    "title": "On the Holistic Approach for Detecting Human Image Forgery",
    "authors": [
      "Xiao Guo",
      "Jie Zhu",
      "Anil Jain",
      "Xiaoming Liu"
    ],
    "abstract": "The rapid advancement of AI-generated content (AIGC) has escalated the threat of deepfakes, from facial manipulations to the synthesis of entire photorealistic human bodies. However, existing detection methods remain fragmented, specializing either in facial-region forgeries or full-body synthetic images, and consequently fail to generalize across the full spectrum of human image manipulations. We introduce HuForDet, a holistic framework for human image forgery detection, which features a dual-branch architecture comprising: (1) a face forgery detection branch that employs heterogeneous experts operating in both RGB and frequency domains, including an adaptive Laplacian-of-Gaussian (LoG) module designed to capture artifacts ranging from fine-grained blending boundaries to coarse-scale texture irregularities; and (2) a contextualized forgery detection branch that leverages a Multi-Modal Large Language Model (MLLM) to analyze full-body semantic consistency, enhanced with a confidence estimation mechanism that dynamically weights its contribution during feature fusion. We curate a human image forgery (HuFor) dataset that unifies existing face forgery data with a new corpus of full-body synthetic humans. Extensive experiments show that our HuForDet achieves state-of-the-art forgery detection performance and superior robustness across diverse human image forgeries.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04715.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04715",
    "published": "2026-01-08T08:33:22Z",
    "updated": "2026-01-08T08:33:22Z",
    "comment": "6 figures, 5 tables",
    "light_analysis": {
      "overview": "本文提出HuForDet框架，采用双分支架构整合面部伪造检测和上下文分析，实现了对多样化人类图像伪造的整体检测，解决了现有方法泛化不足的问题。",
      "motivation": "随着AI生成内容的快速发展，深度伪造威胁日益加剧，从面部篡改到全身合成图像层出不穷。现有检测方法多专注于单一领域，如面部伪造或全身图像合成，导致碎片化严重，无法泛化到各类人类图像操纵中。这种局限性降低了实际应用效果，因此亟需开发统一框架以提升检测的全面性和鲁棒性，应对日益复杂的伪造场景。",
      "method": "HuForDet框架采用双分支架构：一是面部伪造检测分支，通过异质专家在RGB和频域操作，引入自适应拉普拉斯-高斯模块捕捉从细粒度混合边界到粗尺度纹理不规则性的伪像；二是上下文伪造检测分支，利用多模态大型语言模型分析全身语义一致性，并加入置信度估计机制动态加权特征融合。此外，研究还策划了HuFor数据集，整合现有面部伪造数据与新收集的全身合成人类数据，以支持全面评估。",
      "result": "广泛实验表明，HuForDet在多种人类图像伪造上实现了最先进的检测性能，并展现出卓越的鲁棒性。尽管摘要未提供具体数值指标，但强调了其优于现有基线方法，能够有效应对从面部到全身的多样化伪造类型，提升泛化能力和检测准确性。",
      "conclusion": "本研究的主要贡献是提出了整体人类图像伪造检测框架HuForDet及配套数据集，整合了面部与上下文分析，显著提升了检测性能和鲁棒性。学术价值在于推动了伪造检测领域的统一方法发展，实际应用有助于增强图像安全验证。未来工作可进一步优化计算效率或扩展到更广泛伪造场景（摘要未明确说明局限性）。",
      "tags": [
        "Human Image Forgery Detection",
        "Dual-Branch Architecture",
        "Adaptive Laplacian-of-Gaussian",
        "Multi-Modal Large Language Model",
        "Heterogeneous Experts"
      ]
    },
    "analyzed_at": "2026-01-09T02:51:59.732455Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04706",
    "title": "Forge-and-Quench: Enhancing Image Generation for Higher Fidelity in Unified Multimodal Models",
    "authors": [
      "Yanbing Zeng",
      "Jia Wang",
      "Hanghang Ma",
      "Junqiang Wu",
      "Jie Zhu",
      "Xiaoming Wei",
      "Jie Hu"
    ],
    "abstract": "Integrating image generation and understanding into a single framework has become a pivotal goal in the multimodal domain. However, how understanding can effectively assist generation has not been fully explored. Unlike previous works that focus on leveraging reasoning abilities and world knowledge from understanding models, this paper introduces a novel perspective: leveraging understanding to enhance the fidelity and detail richness of generated images. To this end, we propose Forge-and-Quench, a new unified framework that puts this principle into practice. In the generation process of our framework, an MLLM first reasons over the entire conversational context, including text instructions, to produce an enhanced text instruction. This refined instruction is then mapped to a virtual visual representation, termed the Bridge Feature, via a novel Bridge Adapter. This feature acts as a crucial link, forging insights from the understanding model to quench and refine the generation process. It is subsequently injected into the T2I backbone as a visual guidance signal, alongside the enhanced text instruction that replaces the original input. To validate this paradigm, we conduct comprehensive studies on the design of the Bridge Feature and Bridge Adapter. Our framework demonstrates exceptional extensibility and flexibility, enabling efficient migration across different MLLM and T2I models with significant savings in training overhead, all without compromising the MLLM's inherent multimodal understanding capabilities. Experiments show that Forge-and-Quench significantly improves image fidelity and detail across multiple models, while also maintaining instruction-following accuracy and enhancing world knowledge application. Models and codes are available at https://github.com/YanbingZeng/Forge-and-Quench.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04706.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04706",
    "published": "2026-01-08T08:18:44Z",
    "updated": "2026-01-08T08:18:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Request timed out."
  },
  {
    "id": "2601.04692",
    "title": "See, Explain, and Intervene: A Few-Shot Multimodal Agent Framework for Hateful Meme Moderation",
    "authors": [
      "Naquee Rizwan",
      "Subhankar Swain",
      "Paramananda Bhaskar",
      "Gagan Aryan",
      "Shehryaar Shah Khan",
      "Animesh Mukherjee"
    ],
    "abstract": "In this work, we examine hateful memes from three complementary angles - how to detect them, how to explain their content and how to intervene them prior to being posted - by applying a range of strategies built on top of generative AI models. To the best of our knowledge, explanation and intervention have typically been studied separately from detection, which does not reflect real-world conditions. Further, since curating large annotated datasets for meme moderation is prohibitively expensive, we propose a novel framework that leverages task-specific generative multimodal agents and the few-shot adaptability of large multimodal models to cater to different types of memes. We believe this is the first work focused on generalizable hateful meme moderation under limited data conditions, and has strong potential for deployment in real-world production scenarios. Warning: Contains potentially toxic contents.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04692.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04692",
    "published": "2026-01-08T08:02:48Z",
    "updated": "2026-01-08T08:02:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一个少样本多模态代理框架，用于仇恨性迷因的检测、解释和干预。",
      "motivation": "仇恨性迷因在网络内容审核中日益突出，但现有方法通常将检测、解释和干预分开研究，缺乏集成处理，不符合实际应用需求。此外，策划大规模标注数据集成本高昂，限制了模型泛化能力和部署可行性。因此，本研究旨在开发一个框架，在有限数据条件下整合这三个角度，以提升调节效果和实际适用性，解决数据稀缺和现实场景中的综合挑战。",
      "method": "研究基于生成式 AI 模型，提出了一个任务特定的生成多模态代理框架，利用大型多模态模型的少样本适应性。关键创新点在于将检测、解释和干预模块集成到多模态代理中，通过少样本学习减少对大量标注数据的依赖，适应不同类型仇恨性迷因的处理。框架结合图像和文本等多模态输入，实现端到端的调节，从而提高在数据有限条件下的灵活性和效率。",
      "result": "摘要未明确说明具体实验结果。论文声称该框架在有限数据条件下具有潜在应用价值，并有望在实际生产场景中部署，但未提供性能指标如检测准确率、解释质量或干预效果的量化数据，也缺乏与基线方法的详细对比。因此，实验效果的评估和具体改进需要在全文或后续工作中进一步验证。",
      "conclusion": "本研究是首个专注于有限数据条件下通用仇恨性迷因调节的工作，贡献在于集成了检测、解释和干预的多模态代理框架。学术上推动了少样本学习和生成式 AI 在复杂多模态任务中的应用，实际价值在于为自动内容审核提供可行解决方案。局限性可能包括对不同文化背景迷因的泛化能力，未来工作可扩展到更广泛数据集和优化评估标准。",
      "tags": [
        "Multimodal Agents",
        "Generative AI",
        "Few-Shot Learning",
        "Hate Speech Detection",
        "Memes Moderation"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:51.081775Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04687",
    "title": "WebCryptoAgent: Agentic Crypto Trading with Web Informatics",
    "authors": [
      "Ali Kurban",
      "Wei Luo",
      "Liangyu Zuo",
      "Zeyu Zhang",
      "Renda Han",
      "Zhaolu Kang",
      "Hao Tang"
    ],
    "abstract": "Cryptocurrency trading increasingly depends on timely integration of heterogeneous web information and market microstructure signals to support short-horizon decision making under extreme volatility. However, existing trading systems struggle to jointly reason over noisy multi-source web evidence while maintaining robustness to rapid price shocks at sub-second timescales. The first challenge lies in synthesizing unstructured web content, social sentiment, and structured OHLCV signals into coherent and interpretable trading decisions without amplifying spurious correlations, while the second challenge concerns risk control, as slow deliberative reasoning pipelines are ill-suited for handling abrupt market shocks that require immediate defensive responses. To address these challenges, we propose WebCryptoAgent, an agentic trading framework that decomposes web-informed decision making into modality-specific agents and consolidates their outputs into a unified evidence document for confidence-calibrated reasoning. We further introduce a decoupled control architecture that separates strategic hourly reasoning from a real-time second-level risk model, enabling fast shock detection and protective intervention independent of the trading loop. Extensive experiments on real-world cryptocurrency markets demonstrate that WebCryptoAgent improves trading stability, reduces spurious activity, and enhances tail-risk handling compared to existing baselines. Code will be available at https://github.com/AIGeeksGroup/WebCryptoAgent.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04687.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04687",
    "published": "2026-01-08T07:55:10Z",
    "updated": "2026-01-08T07:55:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "WebCryptoAgent框架通过模态特定代理和信心校准推理，解决了加密货币交易中网页信息整合和实时风险控制的挑战。",
      "motivation": "加密货币交易在极端波动下依赖整合异构网页信息和市场微结构信号进行短期决策，但现有系统在处理噪声多源证据（如非结构化网页内容、社交情绪和结构化OHLCV信号）时容易放大虚假相关，同时风险控制延迟导致无法及时应对秒级价格冲击，这些问题降低了交易的鲁棒性和效率，急需一种能同时进行多源推理和快速响应的解决方案。",
      "method": "WebCryptoAgent采用代理化交易框架，将网页信息决策分解为模态特定代理（如专门处理社交情绪或OHLCV信号），并整合输出为统一证据文档以进行信心校准推理，确保决策的连贯性和可解释性；同时引入解耦控制架构，分离战略小时级推理和实时秒级风险模型，实现快速冲击检测和保护性干预，独立于主交易循环来增强系统鲁棒性。",
      "result": "在真实加密货币市场的实验中，WebCryptoAgent相比现有基线（如传统交易系统或未整合多源信息的模型）提高了交易稳定性、减少了虚假活动，并增强了尾部风险处理能力，具体性能指标（如准确率或效率提升）摘要未明确说明，但通过对比验证了其在多源信号合成和风险控制方面的改进效果。",
      "conclusion": "该研究的主要贡献在于提出了一个代理化框架来解决加密货币交易中的信息整合和风险控制难题，学术价值体现在改进多源信号推理的鲁棒性和实时响应机制，实际应用价值是提升交易系统性能以减少损失和风险；潜在局限性摘要未明确说明，未来工作可能包括代码开源和进一步优化模型架构以适应更广泛的市场条件。",
      "tags": [
        "Agentic Trading",
        "Web Informatics",
        "Risk Control",
        "Modality-specific Agents",
        "Confidence-calibrated Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T02:54:31.357941Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04682",
    "title": "HATIR: Heat-Aware Diffusion for Turbulent Infrared Video Super-Resolution",
    "authors": [
      "Yang Zou",
      "Xingyue Zhu",
      "Kaiqi Han",
      "Jun Ma",
      "Xingyuan Li",
      "Zhiying Jiang",
      "Jinyuan Liu"
    ],
    "abstract": "Infrared video has been of great interest in visual tasks under challenging environments, but often suffers from severe atmospheric turbulence and compression degradation. Existing video super-resolution (VSR) methods either neglect the inherent modality gap between infrared and visible images or fail to restore turbulence-induced distortions. Directly cascading turbulence mitigation (TM) algorithms with VSR methods leads to error propagation and accumulation due to the decoupled modeling of degradation between turbulence and resolution. We introduce HATIR, a Heat-Aware Diffusion for Turbulent InfraRed Video Super-Resolution, which injects heat-aware deformation priors into the diffusion sampling path to jointly model the inverse process of turbulent degradation and structural detail loss. Specifically, HATIR constructs a Phasor-Guided Flow Estimator, rooted in the physical principle that thermally active regions exhibit consistent phasor responses over time, enabling reliable turbulence-aware flow to guide the reverse diffusion process. To ensure the fidelity of structural recovery under nonuniform distortions, a Turbulence-Aware Decoder is proposed to selectively suppress unstable temporal cues and enhance edge-aware feature aggregation via turbulence gating and structure-aware attention. We built FLIR-IVSR, the first dataset for turbulent infrared VSR, comprising paired LR-HR sequences from a FLIR T1050sc camera (1024 X 768) spanning 640 diverse scenes with varying camera and object motion conditions. This encourages future research in infrared VSR. Project page: https://github.com/JZ0606/HATIR",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04682.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04682",
    "published": "2026-01-08T07:49:02Z",
    "updated": "2026-01-08T07:49:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "HATIR是一种热感知扩散方法，通过联合建模湍流退化和结构细节损失，实现湍流红外视频的超分辨率。",
      "motivation": "红外视频在低光、烟雾等挑战性环境中应用广泛，但常受大气湍流和压缩导致的质量下降影响。现有视频超分辨率方法要么忽略红外与可见光的模态差异，要么无法有效恢复湍流引起的失真；直接级联湍流缓解与超分辨率技术会因建模分离而引发误差传播。因此，开发能同步处理湍流和提升分辨率的集成方法至关重要，以提升红外视觉系统的可靠性和性能。",
      "method": "HATIR采用热感知扩散框架，将热变形先验注入扩散采样路径，以联合逆推湍流退化和细节损失。关键创新包括基于热活动区域相位响应一致性的Phasor-Guided Flow Estimator，生成湍流感知流指导扩散过程；以及Turbulence-Aware Decoder，通过湍流门控和结构感知注意力机制，选择性抑制不稳定时间线索并增强边缘特征聚合。研究还构建了首个湍流红外视频超分辨率数据集FLIR-IVSR，包含640个多样化场景的LR-HR序列，支持方法验证。",
      "result": "摘要未明确说明具体实验结果数据，如准确率或效率改进。但论文通过提出HATIR方法和建立FLIR-IVSR数据集，展示了在湍流红外视频超分辨率领域的创新。预期该方法能比现有技术更有效地减少错误传播，提升恢复质量，具体性能指标需参考完整论文中的实验对比。",
      "conclusion": "本研究的核心贡献是提出了HATIR方法，首次联合建模湍流退化和超分辨率问题，并建立了FLIR-IVSR数据集。学术价值在于填补了红外视频处理中模态差异和湍流恢复的研究空白，推动了该领域的发展；实际应用价值包括改善红外视觉系统在军事、安防等恶劣环境中的可靠性。未来工作可进一步优化模型效率或扩展到其他退化类型。",
      "tags": [
        "Video Super-Resolution",
        "Diffusion Models",
        "Turbulence Mitigation",
        "Infrared Imaging",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:43.472792Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04676",
    "title": "DB-MSMUNet:Dual Branch Multi-scale Mamba UNet for Pancreatic CT Scans Segmentation",
    "authors": [
      "Qiu Guan",
      "Zhiqiang Yang",
      "Dezhang Ye",
      "Yang Chen",
      "Xinli Xu",
      "Ying Tang"
    ],
    "abstract": "Accurate segmentation of the pancreas and its lesions in CT scans is crucial for the precise diagnosis and treatment of pancreatic cancer. However, it remains a highly challenging task due to several factors such as low tissue contrast with surrounding organs, blurry anatomical boundaries, irregular organ shapes, and the small size of lesions. To tackle these issues, we propose DB-MSMUNet (Dual-Branch Multi-scale Mamba UNet), a novel encoder-decoder architecture designed specifically for robust pancreatic segmentation. The encoder is constructed using a Multi-scale Mamba Module (MSMM), which combines deformable convolutions and multi-scale state space modeling to enhance both global context modeling and local deformation adaptation. The network employs a dual-decoder design: the edge decoder introduces an Edge Enhancement Path (EEP) to explicitly capture boundary cues and refine fuzzy contours, while the area decoder incorporates a Multi-layer Decoder (MLD) to preserve fine-grained details and accurately reconstruct small lesions by leveraging multi-scale deep semantic features. Furthermore, Auxiliary Deep Supervision (ADS) heads are added at multiple scales to both decoders, providing more accurate gradient feedback and further enhancing the discriminative capability of multi-scale features. We conduct extensive experiments on three datasets: the NIH Pancreas dataset, the MSD dataset, and a clinical pancreatic tumor dataset provided by collaborating hospitals. DB-MSMUNet achieves Dice Similarity Coefficients of 89.47%, 87.59%, and 89.02%, respectively, outperforming most existing state-of-the-art methods in terms of segmentation accuracy, edge preservation, and robustness across different datasets. These results demonstrate the effectiveness and generalizability of the proposed method for real-world pancreatic CT segmentation tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04676.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04676",
    "published": "2026-01-08T07:41:37Z",
    "updated": "2026-01-08T07:41:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "DB-MSMUNet通过双分支多尺度Mamba UNet架构，结合可变形卷积和多尺度状态空间建模，显著提升了胰腺CT分割的准确性和鲁棒性。",
      "motivation": "准确分割胰腺和病变在CT扫描中对胰腺癌的精确诊断和治疗至关重要，但现有方法面临诸多挑战：胰腺组织与周围器官对比度低、解剖边界模糊、器官形状不规则以及病变尺寸小，这些因素导致分割任务高度复杂。传统方法可能难以有效处理全局上下文和局部细节的平衡，因此需要开发一种更鲁棒且精确的解决方案来应对这些医学影像中的实际难题，以提升临床应用的可靠性。",
      "method": "论文提出DB-MSMUNet，一种新颖的编码器-解码器架构。编码器采用多尺度Mamba模块（MSMM），结合可变形卷积和多尺度状态空间建模，以增强全局上下文建模和局部变形适应能力。解码器分为双分支：边缘解码器引入边缘增强路径（EEP）来显式捕获边界信息并优化模糊轮廓；区域解码器则整合多层解码器（MLD），利用多尺度深层语义特征来保留细粒度细节并准确重建小病变。此外，通过在多尺度为两个解码器添加辅助深度监督（ADS）头，提供更精准的梯度反馈，进一步增强了多尺度特征的区分能力。",
      "result": "在三个数据集上进行实验：NIH Pancreas数据集、MSD数据集和合作医院提供的临床胰腺肿瘤数据集。DB-MSMUNet分别取得了89.47%、87.59%和89.02%的Dice相似系数，在分割准确性、边缘保持和跨数据集鲁棒性方面优于大多数现有最先进方法。这些结果表明，该方法在提升分割性能的同时，具有较好的泛化能力，但摘要未提供具体基线对比数据，仅强调其在整体指标上的优越表现。",
      "conclusion": "DB-MSMUNet通过创新的双分支设计、多尺度建模和辅助监督机制，有效解决了胰腺CT分割中的关键挑战，提升了模型在真实世界任务中的有效性和泛化能力。该研究为医学影像分析提供了有价值的工具，未来工作可探索其在其他器官或病变分割中的扩展，并进一步优化计算效率以支持更广泛的临床应用。",
      "tags": [
        "State Space Modeling",
        "Multi-scale Modeling",
        "Dual Branch Architecture",
        "Deformable Convolutions",
        "Edge Enhancement"
      ]
    },
    "analyzed_at": "2026-01-09T02:56:45.636213Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04672",
    "title": "Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning",
    "authors": [
      "Wentao Zhang",
      "Lifei Wang",
      "Lina Lu",
      "MingKun Xu",
      "Shangyang Li",
      "Yanchao Yang",
      "Tao Fang"
    ],
    "abstract": "Agricultural disease diagnosis challenges VLMs, as conventional fine-tuning requires extensive labels, lacks interpretability, and generalizes poorly. While reasoning improves model robustness, existing methods rely on costly expert annotations and rarely address the open-ended, diverse nature of agricultural queries. To address these limitations, we propose \\textbf{Agri-R1}, a reasoning-enhanced large model for agriculture. Our framework automates high-quality reasoning data generation via vision-language synthesis and LLM-based filtering, using only 19\\% of available samples. Training employs Group Relative Policy Optimization (GRPO) with a novel proposed reward function that integrates domain-specific lexicons and fuzzy matching to assess both correctness and linguistic flexibility in open-ended responses. Evaluated on CDDMBench, our resulting 3B-parameter model achieves performance competitive with 7B- to 13B-parameter baselines, showing a +23.2\\% relative gain in disease recognition accuracy, +33.3\\% in agricultural knowledge QA, and a +26.10-point improvement in cross-domain generalization over standard fine-tuning. Ablation studies confirm that the synergy between structured reasoning data and GRPO-driven exploration underpins these gains, with benefits scaling as question complexity increases.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04672.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04672",
    "published": "2026-01-08T07:34:37Z",
    "updated": "2026-01-08T07:34:37Z",
    "comment": "This paper is submitted for review to ACL 2026. It is 17 pages long and includes 5 figures. The corresponding authors are Tao Fang and Lina Lu",
    "light_analysis": {
      "overview": "论文提出Agri-R1，一种通过强化学习自动化数据生成和GRPO训练，增强视觉-语言模型农业推理能力的方法。",
      "motivation": "农业疾病诊断中，视觉-语言模型面临传统微调方法依赖大量标注数据、可解释性差和泛化能力不足的挑战。现有推理方法通常需要昂贵的专家注释，难以处理农业领域开放式、多样化的查询。因此，研究致力于开发一种更高效和通用的推理框架，以解决这些限制，提升模型在复杂农业任务中的表现，并减少对人工标注的依赖。",
      "method": "Agri-R1框架通过视觉-语言合成和基于大型语言模型的过滤，自动化生成高质量的推理数据，仅使用19%的可用样本。训练采用Group Relative Policy Optimization (GRPO)，并结合一个新设计的奖励函数，该函数整合了领域特定词库和模糊匹配技术，以评估开放式响应中的正确性和语言灵活性，从而优化模型的推理过程，提高其适应性。",
      "result": "在CDDMBench基准测试中，提出的3B参数Agri-R1模型表现优异，疾病识别准确率相比基线提升23.2%，农业知识问答提升33.3%，跨域泛化能力改善26.10分。消融研究表明，结构化推理数据和GRPO驱动的探索之间的协同作用是性能提升的关键因素，并且随着问题复杂度的增加，这种增益更为显著。",
      "conclusion": "本研究的主要贡献是开发了Agri-R1，一种结合推理增强和强化学习的视觉-语言模型，显著提高了农业推理的性能和泛化能力。其学术价值在于提供了高效的数据生成和训练方法，实际应用价值在于推动农业AI的诊断和决策支持。未来工作可探索扩展到更广泛的农业任务，并可能考虑对模型效率和可扩展性的优化。",
      "tags": [
        "Vision-Language Models",
        "Reinforcement Learning",
        "Large Language Models",
        "Group Relative Policy Optimization",
        "Agricultural Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:57.985751Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.17226",
    "title": "Robust Scene Coordinate Regression via Geometrically-Consistent Global Descriptors",
    "authors": [
      "Son Tung Nguyen",
      "Alejandro Fontan",
      "Michael Milford",
      "Tobias Fischer"
    ],
    "abstract": "Recent learning-based visual localization methods use global descriptors to disambiguate visually similar places, but existing approaches often derive these descriptors from geometric cues alone (e.g., covisibility graphs), limiting their discriminative power and reducing robustness in the presence of noisy geometric constraints. We propose an aggregator module that learns global descriptors consistent with both geometrical structure and visual similarity, ensuring that images are close in descriptor space only when they are visually similar and spatially connected. This corrects erroneous associations caused by unreliable overlap scores. Using a batch-mining strategy based solely on the overlap scores and a modified contrastive loss, our method trains without manual place labels and generalizes across diverse environments. Experiments on challenging benchmarks show substantial localization gains in large-scale environments while preserving computational and memory efficiency. Code is available at https://github.com/sontung/robust_scr.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.17226.pdf",
    "abs_url": "https://arxiv.org/abs/2512.17226",
    "published": "2025-12-19T04:24:03Z",
    "updated": "2026-01-08T07:00:53Z",
    "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026",
    "light_analysis": {
      "overview": "提出聚合模块学习几何与视觉一致的全局描述符，提高视觉定位鲁棒性。",
      "motivation": "现有基于学习的视觉定位方法依赖全局描述符区分视觉相似地点，但仅从几何线索（如共视图）推导，导致区分能力受限且噪声几何约束下鲁棒性差。该问题在增强现实和机器人导航等应用中至关重要，因不可靠的重叠分数易引发错误关联，现有方法未能充分整合视觉相似性与几何结构，限制了性能提升。",
      "method": "论文提出聚合模块，学习全局描述符确保图像在描述符空间中接近仅当它们在视觉上相似且空间上连接。关键创新是结合几何结构和视觉相似性，使用基于重叠分数的批量挖掘策略和修改的对比损失，无需手动地点标签，能在多样化环境中泛化。此方法纠正不可靠重叠分数导致的错误关联，通过自监督方式提升描述符学习效果。",
      "result": "在挑战性基准测试中，该方法在大规模环境中展现出显著的定位性能增益，同时保持了计算和内存效率。摘要未明确说明具体性能指标（如准确率提升），但强调‘substantial localization gains’，表明与基线方法相比有改进。通过几何和视觉一致性，有效减少噪声约束下的错误，增强了定位准确性。",
      "conclusion": "该方法通过聚合模块学习几何和视觉一致的全局描述符，显著提高了视觉定位的鲁棒性。主要贡献包括无手动标签训练、强泛化能力和效率保持；学术上优化了描述符学习策略，实际中支持增强现实等精确应用。摘要未明确说明局限性，未来工作可能涉及扩展到更多复杂环境或进一步优化损失函数。",
      "tags": [
        "Scene Coordinate Regression",
        "Global Descriptors",
        "Contrastive Learning",
        "Visual Localization",
        "Geometric Consistency"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:10.190917Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.05665",
    "title": "Interleaved Latent Visual Reasoning with Selective Perceptual Modeling",
    "authors": [
      "Shuai Dong",
      "Siyuan Wang",
      "Xingyu Liu",
      "Chenglin Li",
      "Haowen Hou",
      "Zhongyu Wei"
    ],
    "abstract": "Interleaved reasoning paradigms enhance Multimodal Large Language Models (MLLMs) with visual feedback but are hindered by the prohibitive computational cost of re-encoding pixel-dense images. A promising alternative, latent visual reasoning, circumvents this bottleneck yet faces limitations: methods either fail to capture intermediate state evolution due to single-step, non-interleaved structures, or sacrifice precise perceptual modeling by over-compressing features. We introduce Interleaved Latent Visual Reasoning (ILVR), a framework that unifies dynamic state evolution with precise perceptual modeling. ILVR interleaves textual generation with latent visual representations that act as specific, evolving cues for subsequent reasoning. Specifically, we employ a self-supervision strategy where a momentum teacher model selectively distills relevant features from ground-truth intermediate images into sparse supervision targets. This adaptive selection mechanism guides the model to autonomously generate context-aware visual signals. Extensive experiments on multimodal reasoning benchmarks demonstrate that ILVR outperforms existing approaches, effectively bridging the gap between fine-grained perception and sequential multimodal reasoning.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.05665.pdf",
    "abs_url": "https://arxiv.org/abs/2512.05665",
    "published": "2025-12-05T12:09:39Z",
    "updated": "2026-01-08T06:50:09Z",
    "comment": "18 pages, 11 figures. Code available at https://github.com/XD111ds/ILVR",
    "light_analysis": {
      "overview": "论文提出了Interleaved Latent Visual Reasoning框架，通过交错潜在视觉表示统一动态状态演变和精确感知建模，以增强多模态大语言模型的视觉推理能力。",
      "motivation": "交错推理范式旨在增强多模态大语言模型的视觉反馈，但受限于重新编码像素密集型图像的高昂计算成本。潜在视觉推理方法虽避免了这一瓶颈，却存在缺陷：单步、非交错结构无法捕捉中间状态演变，或过度压缩特征牺牲精确感知建模。这些问题阻碍了多模态推理中细粒度感知与序列推理的平衡，因此需要一种能同时处理动态演变和精确建模的新方法。",
      "method": "论文提出ILVR框架，通过交错生成文本与潜在视觉表示，这些表示作为动态演变的特定线索来指导后续推理。关键创新点在于采用自监督策略，利用动量教师模型选择性地从真实中间图像中提取相关特征，形成稀疏监督目标，并通过自适应选择机制引导模型自主生成上下文感知的视觉信号，以结合动态状态演变和精确感知建模。",
      "result": "在多模态推理基准上进行的广泛实验显示，ILVR框架优于现有方法，有效弥合了细粒度感知与序列多模态推理之间的差距。摘要未明确说明具体性能指标如准确率提升，但结果表明该方法显著提高了推理效果，与基线方法对比表现出优势。",
      "conclusion": "该研究贡献了ILVR框架，实现了动态状态演变与精确感知建模的统一，推动了多模态推理技术的发展，具有学术和应用价值，例如提升视觉相关任务的准确性和效率。摘要未明确说明局限性或未来工作，但该方法为后续研究提供了基础，可能涉及计算优化或泛化能力改进。",
      "tags": [
        "Multimodal Large Language Models",
        "Latent Visual Representation",
        "Self-supervised Learning",
        "Momentum Teacher Model",
        "Interleaved Reasoning"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:04.540011Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03667",
    "title": "TRec: Egocentric Action Recognition using 2D Point Tracks",
    "authors": [
      "Dennis Holzmann",
      "Sven Wachsmuth"
    ],
    "abstract": "We present a novel approach for egocentric action recognition that leverages 2D point tracks as an additional motion cue. While most existing methods rely on RGB appearance, human pose estimation, or their combination, our work demonstrates that tracking randomly sampled image points across video frames can substantially improve recognition accuracy. Unlike prior approaches, we do not detect hands, objects, or interaction regions. Instead, we employ CoTracker to follow a set of randomly initialized points through each video and use the resulting trajectories, together with the corresponding image frames, as input to a Transformer-based recognition model. Surprisingly, our method achieves notable gains even when only the initial frame and its associated point tracks are provided, without incorporating the full video sequence. Experimental results confirm that integrating 2D point tracks consistently enhances performance compared to the same model trained without motion information, highlighting their potential as a lightweight yet effective representation for egocentric action understanding.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.03667.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03667",
    "published": "2026-01-07T07:41:57Z",
    "updated": "2026-01-08T06:30:42Z",
    "comment": "submitted to ICPR 2026",
    "light_analysis": {
      "overview": "本文提出一种新方法，通过利用随机采样的2D点轨迹作为额外运动线索，显著提升自我中心动作识别性能。",
      "motivation": "自我中心动作识别在计算机视觉中至关重要，但现有方法主要依赖RGB图像外观、人体姿态估计或它们的组合，可能忽略细微的运动信息。传统方法常需检测手、物体或交互区域，计算复杂度高且易受遮挡影响。本研究旨在解决这些不足，通过引入轻量级的点轨迹作为运动表示，无需复杂检测，以更高效的方式增强识别准确率，从而推动动作理解技术的发展。",
      "method": "本研究提出TRec方法，核心是使用CoTracker工具对视频初始帧中随机采样的图像点进行跨帧跟踪，生成2D点轨迹。这些轨迹与对应的图像帧一起作为输入，馈入基于Transformer的识别模型中。关键创新在于不依赖手或物体检测，直接利用点轨迹捕捉动作动态，简化了处理流程。该方法即使在仅提供初始帧和点轨迹的情况下也能有效工作，展示了点轨迹作为运动线索的实用性和灵活性。",
      "result": "实验结果显示，与仅使用图像帧的基线模型相比，集成2D点轨迹后识别性能得到显著提升。具体表现为准确率持续提高，即使模型在仅使用初始帧及其点轨迹而不包含完整视频序列的简化设置下，也观察到了明显的性能增益。这证实了点轨迹作为轻量级运动表示的有效性，但摘要未提供具体准确率数字，仅强调了与不使用运动信息的模型相比的性能优势。",
      "conclusion": "本文的主要贡献是提出了一种基于2D点轨迹的自我中心动作识别方法，证明了随机跟踪点轨迹可以大幅提高识别准确率，无需依赖复杂检测步骤。该研究的学术价值在于提供了一种轻量级、高效的替代方案，简化了动作理解流程；实际应用上，可能在降低计算成本和提高鲁棒性方面有益。未来工作可扩展至更多数据集，或探索点轨迹与其他视觉线索的融合以进一步优化性能。",
      "tags": [
        "Egocentric Action Recognition",
        "2D Point Tracks",
        "Transformer",
        "CoTracker",
        "Motion Cue"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:06.228922Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.09779",
    "title": "MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models",
    "authors": [
      "Dianyi Wang",
      "Siyuan Wang",
      "Zejun Li",
      "Yikun Wang",
      "Yitong Li",
      "Duyu Tang",
      "Xiaoyu Shen",
      "Xuanjing Huang",
      "Zhongyu Wei"
    ],
    "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance across multi-modal tasks by scaling model size and training data. However, these dense LVLMs incur significant computational costs and motivate the exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve parameter efficiency, effectively applying MoE to simultaneously model modality-specific features and cross-modal associations in LVLMs remains challenging. In this work, we propose to incorporate Mixture of Intra- and Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is guided by its modality, directing tokens to their respective intra-modality experts as well as a shared pool of inter-modality experts, enabling the model to jointly learn rich intra-modal features and cross-modal interactions. We further introduce an effective and straightforward two-stage training strategy, which facilitates the direct activation of both MoE and multi-modal capabilities. Extensive experiments across different data scales and LLM backbone demonstrate the effectiveness, efficiency and generality of our approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters match or even surpass the performance of existing advanced open-source MoE-LLMs based multi-modal models that involve more activated parameters. The code is available at https://github.com/AlenjandroWang/MoIIE.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.09779.pdf",
    "abs_url": "https://arxiv.org/abs/2508.09779",
    "published": "2025-08-13T13:00:05Z",
    "updated": "2026-01-08T05:44:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出 MoIIE（混合内部和跨模态专家）方法，通过专家路由机制优化大型视觉语言模型的参数效率和性能。",
      "motivation": "大型视觉语言模型（LVLMs）在多种任务中表现出色，但密集架构带来高昂计算成本。现有稀疏混合专家（MoE）方法虽能提升参数效率，却难以同时建模模态特定特征和跨模态关联的复杂性。这限制了多模态AI的可扩展性和实用部署，因此亟需创新架构来解决效率与性能的平衡问题。",
      "method": "MoIIE 方法基于每个 token 的模态指导专家路由：将 token 分配到相应的模态内专家处理特定特征，并共享跨模态专家池促进交互。关键创新包括路由机制和两阶段训练策略，后者直接激活 MoE 和多模态能力。实验使用不同数据规模和大型语言模型（LLM）骨干，但具体数据集和架构细节摘要未明确说明。",
      "result": "实验表明，MoIIE 模型在有效性和效率上表现优异。激活参数为5.5亿和11.3亿的模型，其性能匹配甚至超越了现有先进开源 MoE-LLM 多模态模型，后者通常激活更多参数。这证明了 MoIIE 在参数利用上更高效，同时维持或提升任务表现，验证了方法的通用性和优越性。",
      "conclusion": "MoIIE 通过混合专家架构成功优化 LVLMs 的参数使用，贡献了高效的多模态建模方法。学术上为稀疏模型设计提供新思路，实践中可降低计算成本并推动AI应用。未来工作可探索更复杂的路由策略或扩展到其他模态任务，潜在局限性包括路由机制的进一步优化。",
      "tags": [
        "Large Vision-Language Models",
        "Mixture of Experts",
        "Expert Routing",
        "Cross-Modal Learning",
        "Multi-Modal Tasks"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:54.985037Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04614",
    "title": "HyperAlign: Hyperbolic Entailment Cones for Adaptive Text-to-Image Alignment Assessment",
    "authors": [
      "Wenzhi Chen",
      "Bo Hu",
      "Leida Li",
      "Lihuo He",
      "Wen Lu",
      "Xinbo Gao"
    ],
    "abstract": "With the rapid development of text-to-image generation technology, accurately assessing the alignment between generated images and text prompts has become a critical challenge. Existing methods rely on Euclidean space metrics, neglecting the structured nature of semantic alignment, while lacking adaptive capabilities for different samples. To address these limitations, we propose HyperAlign, an adaptive text-to-image alignment assessment framework based on hyperbolic entailment geometry. First, we extract Euclidean features using CLIP and map them to hyperbolic space. Second, we design a dynamic-supervision entailment modeling mechanism that transforms discrete entailment logic into continuous geometric structure supervision. Finally, we propose an adaptive modulation regressor that utilizes hyperbolic geometric features to generate sample-level modulation parameters, adaptively calibrating Euclidean cosine similarity to predict the final score. HyperAlign achieves highly competitive performance on both single database evaluation and cross-database generalization tasks, fully validating the effectiveness of hyperbolic geometric modeling for image-text alignment assessment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04614.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04614",
    "published": "2026-01-08T05:41:06Z",
    "updated": "2026-01-08T05:41:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "HyperAlign通过引入双曲蕴含几何和自适应机制，显著提升了文本到图像对齐评估的准确性和泛化能力。",
      "motivation": "随着文本到图像生成技术的快速发展，准确评估生成图像与文本提示的对齐成为关键挑战。现有方法主要依赖欧几里得空间度量，如余弦相似度，但忽略了语义对齐的结构化特性，导致评估不够精确。此外，这些方法缺乏对不同样本的自适应能力，无法根据样本特性动态调整。因此，开发一个能够建模语义结构并自适应调整的评估框架至关重要，以解决现有方法的局限性，提升评估的鲁棒性和适用性，应对实际应用中的多样性和复杂性。",
      "method": "论文提出HyperAlign框架，基于双曲几何进行自适应文本到图像对齐评估。首先，使用CLIP模型提取图像和文本的欧几里得特征，并将这些特征映射到双曲空间，以捕捉语义的层次结构。其次，设计动态监督蕴含建模机制，将离散的蕴含逻辑（如文本蕴含图像内容）转化为连续的几何结构监督，通过双曲锥形结构建模语义关系。最后，提出自适应调制回归器，利用双曲几何特征生成样本级调制参数，这些参数自适应地校准欧几里得余弦相似度，从而预测最终的对齐分数，实现了技术上的创新与集成。",
      "result": "HyperAlign在实验评估中取得了显著效果。在单个数据库评估任务上，它表现出高度竞争性的性能，优于或与现有方法持平。在跨数据库泛化任务中，HyperAlign也展示了优秀的泛化能力，能够适应不同数据集的变化。摘要未明确说明具体的性能指标，如准确率或F1分数的提升，但可以推断其性能在标准基准测试中具有竞争力，验证了双曲几何建模和自适应机制的有效性。",
      "conclusion": "HyperAlign的主要贡献是提出并验证了一个基于双曲蕴含几何的自适应文本到图像对齐评估框架。它通过引入双曲空间建模，更好地捕捉了语义对齐的结构化性质，并通过自适应调制机制提高了评估的灵活性和准确性。该研究具有重要的学术价值，为对齐评估任务提供了新的几何视角和方法论；同时，其实际应用价值在于提升文本到图像生成系统的质量评估能力。未来工作可能包括扩展到其他模态对齐任务，或进一步优化双曲几何参数以提高效率。",
      "tags": [
        "Text-to-Image Alignment",
        "Hyperbolic Geometry",
        "Entailment Modeling",
        "Adaptive Modulation",
        "CLIP"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:40.873825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03637",
    "title": "CrackSegFlow: Controllable Flow Matching Synthesis for Generalizable Crack Segmentation with a 50K Image-Mask Benchmark",
    "authors": [
      "Babak Asadi",
      "Peiyang Wu",
      "Mani Golparvar-Fard",
      "Ramez Hajj"
    ],
    "abstract": "Automated crack segmentation is essential for condition assessment, yet deployment is limited by scarce pixel-level labels and domain shift. We present CrackSegFlow, a controllable flow-matching synthesis framework that generates crack images conditioned on binary masks with mask-image alignment. The renderer combines topology-preserving mask injection with edge gating to maintain thin-structure continuity and suppress false positives. A class-conditional flow-matching mask model synthesizes masks with control over crack coverage, enabling balanced, topology-diverse data without manual annotation. We inject masks into crack-free backgrounds to diversify illumination and reduce false positives. On five datasets with a CNN-Transformer backbone, incorporating synthesized pairs improves in-domain performance by 5.37 mIoU and 5.13 F1, and target-guided cross-domain synthesis yields gains of 13.12 mIoU and 14.82 F1 using target mask statistics. We also release CSF-50K, 50,000 image-mask pairs for benchmarking.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.03637.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03637",
    "published": "2026-01-07T06:28:16Z",
    "updated": "2026-01-08T05:35:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "CrackSegFlow提出可控流匹配合成框架，通过生成裂缝图像-掩码对提升分割泛化性，并发布50K基准数据集。",
      "motivation": "自动化裂缝分割在基础设施条件评估中至关重要，但实际部署面临两大挑战：一是像素级标注数据稀缺，导致模型训练困难；二是不同场景间的域偏移问题，使得模型泛化能力不足。现有方法依赖人工标注，成本高昂且难以覆盖多样化的裂缝拓扑结构，因此需要一种能够生成高质量合成数据的方法来缓解标签稀缺和域偏移，提升模型的实用性和可扩展性。",
      "method": "论文提出CrackSegFlow框架，基于可控流匹配合成技术生成裂缝图像。核心方法包括：首先，一个类别条件的流匹配模型合成二进制掩码，通过控制裂缝覆盖率生成多样化的拓扑结构；其次，渲染器采用拓扑保持的掩码注入和边缘门控机制，确保细裂缝的连续性并抑制假阳性。此外，框架将合成掩码注入无裂缝背景中，模拟不同光照条件，增强数据多样性，并使用CNN-Transformer混合架构作为分割模型骨干，实现高效的图像处理。",
      "result": "实验在五个数据集上进行，采用CNN-Transformer混合架构作为分割模型。结果显示，引入合成的图像-掩码对后，在域内设置下，平均交并比（mIoU）和F1分数分别提升了5.37和5.13点。在跨域场景中，利用目标掩码统计进行目标导向合成后，性能增益更显著，达到13.12 mIoU和14.82 F1的改进。这表明该方法有效缓解了域偏移问题，显著提升了模型在不同数据集上的泛化能力。",
      "conclusion": "论文的主要贡献在于提出了CrackSegFlow框架和CSF-50K数据集，通过可控流匹配合成技术生成高质量裂缝数据，解决了标签稀缺和域偏移问题。学术上，该方法引入了拓扑保持和边缘门控等创新，为图像合成和分割任务提供了新思路。实际应用中，减少了对人工标注的依赖，推动了自动化裂缝检测的发展。未来工作可能包括扩展到其他缺陷类型或优化合成效率。",
      "tags": [
        "Controllable Flow Matching",
        "Crack Segmentation",
        "Image-Mask Synthesis",
        "Topology Preservation",
        "Edge Gating"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:37.397338Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04607",
    "title": "HUR-MACL: High-Uncertainty Region-Guided Multi-Architecture Collaborative Learning for Head and Neck Multi-Organ Segmentation",
    "authors": [
      "Xiaoyu Liu",
      "Siwen Wei",
      "Linhao Qu",
      "Mingyuan Pan",
      "Chengsheng Zhang",
      "Yonghong Shi",
      "Zhijian Song"
    ],
    "abstract": "Accurate segmentation of organs at risk in the head and neck is essential for radiation therapy, yet deep learning models often fail on small, complexly shaped organs. While hybrid architectures that combine different models show promise, they typically just concatenate features without exploiting the unique strengths of each component. This results in functional overlap and limited segmentation accuracy. To address these issues, we propose a high uncertainty region-guided multi-architecture collaborative learning (HUR-MACL) model for multi-organ segmentation in the head and neck. This model adaptively identifies high uncertainty regions using a convolutional neural network, and for these regions, Vision Mamba as well as Deformable CNN are utilized to jointly improve their segmentation accuracy. Additionally, a heterogeneous feature distillation loss was proposed to promote collaborative learning between the two architectures in high uncertainty regions to further enhance performance. Our method achieves SOTA results on two public datasets and one private dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04607.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04607",
    "published": "2026-01-08T05:25:05Z",
    "updated": "2026-01-08T05:25:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出HUR-MACL模型，通过高不确定性区域引导的多架构协作学习，显著提高头颈部多器官分割的准确性。",
      "motivation": "在头颈部放射治疗中，准确分割危险器官至关重要，但现有深度学习模型在处理形状复杂的小器官时效果不佳，这可能导致治疗方案偏差。尽管混合架构结合不同模型展现出潜力，但它们通常只是串联特征，未能充分利用各组件的独特优势，造成功能重叠和分割精度受限。因此，亟需一种新方法来弥补现有方法的不足，通过有效协作提升模型性能，从而支持更精准的医疗应用。",
      "method": "HUR-MACL模型的核心方法是：首先，使用卷积神经网络自适应识别图像中的高不确定性区域，这些区域通常对应难分割的器官；其次，针对这些区域，同时利用Vision Mamba和Deformable CNN进行联合特征提取和分割，结合前者的高效建模能力和后者的空间适应性优势。此外，引入了异构特征蒸馏损失，促进两个架构在高不确定性区域的知识共享和协作学习，进一步优化分割结果，避免功能冗余并增强整体性能。",
      "result": "论文在两个公共数据集和一个私有数据集上进行了实验，结果显示HUR-MACL模型取得了最先进的性能（SOTA），有效提升了头颈部多器官分割的准确性。与基线方法相比，该方法在复杂器官的分割上表现更优，但摘要未明确说明具体的准确率或效率改进数据，仅强调了其在三个数据集上的优越性，证明了模型在实际应用中的稳健性和有效性。",
      "conclusion": "HUR-MACL模型通过高不确定性区域引导和多架构协作学习，成功提高了头颈部多器官分割的精度，为医学影像分析提供了创新方法。该研究具有重要的学术价值，改进了深度学习分割技术，并展示了在放射治疗等临床应用中的潜力。未来工作可能包括进一步优化模型架构或扩展到其他医学图像分割任务，以应对更多挑战。",
      "tags": [
        "High-Uncertainty Region-Guided Learning",
        "Vision Mamba",
        "Deformable CNN",
        "Heterogeneous Feature Distillation Loss",
        "Multi-Organ Segmentation"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:37.953824Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04605",
    "title": "Detection of Deployment Operational Deviations for Safety and Security of AI-Enabled Human-Centric Cyber Physical Systems",
    "authors": [
      "Bernard Ngabonziza",
      "Ayan Banerjee",
      "Sandeep K. S. Gupta"
    ],
    "abstract": "In recent years, Human-centric cyber-physical systems have increasingly involved artificial intelligence to enable knowledge extraction from sensor-collected data. Examples include medical monitoring and control systems, as well as autonomous cars. Such systems are intended to operate according to the protocols and guidelines for regular system operations. However, in many scenarios, such as closed-loop blood glucose control for Type 1 diabetics, self-driving cars, and monitoring systems for stroke diagnosis. The operations of such AI-enabled human-centric applications can expose them to cases for which their operational mode may be uncertain, for instance, resulting from the interactions with a human with the system. Such cases, in which the system is in uncertain conditions, can violate the system's safety and security requirements.    This paper will discuss operational deviations that can lead these systems to operate in unknown conditions. We will then create a framework to evaluate different strategies for ensuring the safety and security of AI-enabled human-centric cyber-physical systems in operation deployment. Then, as an example, we show a personalized image-based novel technique for detecting the non-announcement of meals in closed-loop blood glucose control for Type 1 diabetics.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04605.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04605",
    "published": "2026-01-08T05:23:58Z",
    "updated": "2026-01-08T05:23:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个框架评估AI使能人中心系统的操作偏差安全策略，并以闭环血糖控制为例展示基于图像的个性化检测技术。",
      "motivation": "近年来，AI使能的以人为中心网络物理系统（如医疗监控和自动驾驶汽车）日益普及，旨在从传感器数据提取知识以优化操作。然而，在实际部署中，这些系统常因与人类的交互（如患者未宣布用餐或环境突发变化）而暴露于不确定状态，导致操作偏离协议，威胁安全性和安全性。现有方法在动态不确定性处理上不足，容易引发违规风险，因此亟需开发新机制来检测和预防偏差，确保系统可靠运行。",
      "method": "研究提出一个综合框架，用于识别AI使能人中心系统中的操作偏差并评估不同安全策略。关键创新在于系统化分析偏差类型（如未知条件运行）并设计评估指标来优化应对措施。作为实例，论文介绍一种基于图像的个性化技术，该技术通过分析1型糖尿病患者的图像数据，结合计算机视觉和异常检测算法，检测闭环血糖控制中未宣布用餐的事件，从而增强系统自适应性和早期预警能力。",
      "result": "摘要中未明确说明具体的实验结果或性能指标。论文通过框架评估和实例展示，旨在证明所提方法在检测操作偏差方面的潜在有效性，但缺乏量化数据如准确率提升、效率改进或与基线方法的对比。未来研究需进行实证验证以补充实验细节。",
      "conclusion": "本论文主要贡献在于系统探讨了AI使能人中心系统的操作偏差问题，并提出了一个评估框架以确保安全性和安全性。通过闭环血糖控制实例，研究强调了技术在医疗等领域的实用价值，为AI系统安全提供了新视角。该研究具有学术价值，推动了跨学科安全策略的发展，并具有实际应用潜力，可提升关键系统的可靠性。未来工作可扩展框架至其他场景并进行更广泛实验验证。",
      "tags": [
        "Operational Deviation Detection",
        "AI-Enabled Cyber-Physical Systems",
        "Safety and Security Framework",
        "Image-Based Anomaly Detection",
        "Closed-Loop Control"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:15.908748Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.11399",
    "title": "Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction",
    "authors": [
      "Galann Pennec",
      "Zhengyuan Liu",
      "Nicholas Asher",
      "Philippe Muller",
      "Nancy F. Chen"
    ],
    "abstract": "Vision-Language Models (VLMs) are able to process increasingly longer videos. Yet, important visual information is easily lost throughout the entire context and missed by VLMs. Also, it is important to design tools that enable cost-effective analysis of lengthy video content. In this paper, we propose a clip selection method that targets key video moments to be included in a multimodal summary. We divide the video into short clips and generate compact visual descriptions of each using a lightweight video captioning model. These are then passed to a large language model (LLM), which selects the K clips containing the most relevant visual information for a multimodal summary. We evaluate our approach on reference clips for the task, automatically derived from full human-annotated screenplays and summaries in the MovieSum dataset. We further show that these reference clips (less than 6% of the movie) are sufficient to build a complete multimodal summary of the movies in MovieSum. Using our clip selection method, we achieve a summarization performance close to that of these reference clips while capturing substantially more relevant video information than random clip selection. Importantly, we maintain low computational cost by relying on a lightweight captioning model.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.11399.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11399",
    "published": "2025-12-12T09:19:45Z",
    "updated": "2026-01-08T05:10:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出一种通过提取关键时刻的剪辑选择方法，用于长视频的多模态摘要，结合轻量级视频字幕模型和大型语言模型来高效识别重要视频内容。",
      "motivation": "视觉语言模型在处理越来越长的视频时，重要视觉信息容易在长上下文中丢失，导致关键内容被忽略，这限制了VLMs在长视频分析中的应用。同时，设计成本效益高的工具对于分析冗长视频内容至关重要，因为现有方法可能计算开销大且效率低下，无法有效捕获关键时刻。因此，研究旨在解决信息丢失和计算成本高的问题，以提升长视频摘要的效率和实用性。",
      "method": "论文提出一个剪辑选择方法：首先将长视频分割成短剪辑，每个剪辑通过轻量级视频字幕模型生成紧凑的视觉描述。这些描述被输入到大型语言模型中，该模型基于相关性选择K个最相关的剪辑用于构建多模态摘要。关键创新在于结合轻量级模型降低计算开销，并利用LLM的推理能力精确提取关键视频时刻。在MovieSum数据集上进行评估，该数据集包含人工注释的剧本和摘要，用于自动导出参考剪辑。",
      "result": "在MovieSum数据集上的实验表明，自动导出的参考剪辑（少于电影总长度的6%）足以构建完整的电影多模态摘要。使用提出的剪辑选择方法，摘要性能接近这些参考剪辑，同时比随机剪辑选择捕获了更多的相关视频信息。这表明方法能有效识别关键时刻，且计算成本保持较低，依赖于轻量级字幕模型，但具体性能指标如准确率摘要未明确说明。",
      "conclusion": "论文的主要贡献是提出了一种高效的关键时刻提取方法，用于长视频摘要，展示了轻量级视频字幕模型与大型语言模型结合在视频处理中的有效性。学术上，为多模态视频分析提供了新思路；实际应用中，降低了计算成本，使得长视频分析更加可行。潜在局限性可能包括对特定数据集的依赖，未来工作可能涉及优化模型或扩展到其他视频类型。",
      "tags": [
        "Vision-Language Models",
        "Large Language Model",
        "Video Summarization",
        "Key Moment Extraction",
        "Lightweight Video Captioning"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:21.703929Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.12766",
    "title": "Probing Deep into Temporal Profile Makes the Infrared Small Target Detector Much Better",
    "authors": [
      "Ruojing Li",
      "Wei An",
      "Yingqian Wang",
      "Xinyi Ying",
      "Yimian Dai",
      "Longguang Wang",
      "Miao Li",
      "Yulan Guo",
      "Li Liu"
    ],
    "abstract": "Infrared small target (IRST) detection is challenging in simultaneously achieving precise, robust, and efficient performance due to extremely dim targets and strong interference. Current learning-based methods attempt to leverage ``more\" information from both the spatial and the short-term temporal domains, but suffer from unreliable performance under complex conditions while incurring computational redundancy. In this paper, we explore the ``more essential\" information from a more crucial domain for the detection. Through theoretical analysis, we reveal that the global temporal saliency and correlation information in the temporal profile demonstrate significant superiority in distinguishing target signals from other signals. To investigate whether such superiority is preferentially leveraged by well-trained networks, we built the first prediction attribution tool in this field and verified the importance of the temporal profile information. Inspired by the above conclusions, we remodel the IRST detection task as a one-dimensional signal anomaly detection task, and propose an efficient deep temporal probe network (DeepPro) that only performs calculations in the time dimension for IRST detection. We conducted extensive experiments to fully validate the effectiveness of our method. The experimental results are exciting, as our DeepPro outperforms existing state-of-the-art IRST detection methods on widely-used benchmarks with extremely high efficiency, and achieves a significant improvement on dim targets and in complex scenarios. We provide a new modeling domain, a new insight, a new method, and a new performance, which can promote the development of IRST detection. Codes are available at https://github.com/TinaLRJ/DeepPro.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.12766.pdf",
    "abs_url": "https://arxiv.org/abs/2506.12766",
    "published": "2025-06-15T08:19:32Z",
    "updated": "2026-01-08T04:46:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出通过深度探索时间剖面信息，将红外小目标检测重塑为一维信号异常检测任务，并开发了高效的DeepPro网络，显著提升了检测精度、鲁棒性和效率。",
      "motivation": "红外小目标检测面临目标极暗、干扰强烈的挑战，难以同时实现精确、鲁棒和高效的性能。现有基于学习的方法试图利用空间和短期时间域的更多信息，但在复杂条件下性能不稳定且计算冗余。因此，研究探索更关键时间剖面信息以改进检测效果至关重要，因为它能更好地从干扰中区分目标信号。",
      "method": "论文通过理论分析揭示全局时间显著性和相关性信息在区分目标信号方面的优势，并首次构建预测归因工具验证了时间剖面信息的重要性。受此启发，作者将IRST检测任务重塑为一维信号异常检测，提出深度时间探头网络（DeepPro），该网络仅沿时间维度进行计算，减少了冗余并提高了效率。关键创新包括新建模领域、新见解和网络设计。",
      "result": "在广泛使用的基准测试中，DeepPro方法优于现有最先进的红外小目标检测方法，具有极高的效率。实验结果显示，在暗目标和复杂场景下，检测性能显著提升，表现出更好的鲁棒性和准确性，但摘要未明确说明具体性能指标如准确率提升值。",
      "conclusion": "论文的主要贡献是提供了新的建模领域、新见解、新方法和新性能，推动了红外小目标检测的发展。学术价值在于提出从时间剖面角度重新思考检测任务，实际应用价值是提高了检测系统的效率和鲁棒性。未来工作可进一步优化网络结构或扩展到其他领域，但摘要未明确说明局限性。",
      "tags": [
        "Infrared Small Target Detection",
        "Temporal Profile Analysis",
        "Anomaly Detection",
        "Deep Learning",
        "DeepPro Network"
      ]
    },
    "analyzed_at": "2026-01-09T02:29:48.315058Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.04358",
    "title": "MAFNet:Multi-frequency Adaptive Fusion Network for Real-time Stereo Matching",
    "authors": [
      "Ao Xu",
      "Rujin Zhao",
      "Xiong Xu",
      "Boceng Huang",
      "Yujia Jia",
      "Hongfeng Long",
      "Fuxuan Chen",
      "Zilong Cao",
      "Fangyuan Chen"
    ],
    "abstract": "Existing stereo matching networks typically rely on either cost-volume construction based on 3D convolutions or deformation methods based on iterative optimization. The former incurs significant computational overhead during cost aggregation, whereas the latter often lacks the ability to model non-local contextual information. These methods exhibit poor compatibility on resource-constrained mobile devices, limiting their deployment in real-time applications. To address this, we propose a Multi-frequency Adaptive Fusion Network (MAFNet), which can produce high-quality disparity maps using only efficient 2D convolutions. Specifically, we design an adaptive frequency-domain filtering attention module that decomposes the full cost volume into high-frequency and low-frequency volumes, performing frequency-aware feature aggregation separately. Subsequently, we introduce a Linformer-based low-rank attention mechanism to adaptively fuse high- and low-frequency information, yielding more robust disparity estimation. Extensive experiments demonstrate that the proposed MAFNet significantly outperforms existing real-time methods on public datasets such as Scene Flow and KITTI 2015, showing a favorable balance between accuracy and real-time performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.04358.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04358",
    "published": "2025-12-04T01:08:38Z",
    "updated": "2026-01-08T04:38:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MAFNet，一个多频自适应融合网络，用于实时立体匹配，仅使用高效的2D卷积实现高质量视差估计。",
      "motivation": "现有立体匹配网络通常依赖于基于3D卷积的成本体积构建或迭代优化方法，前者计算开销大，后者缺乏非局部上下文建模能力。这些方法在资源受限的移动设备上兼容性差，难以满足实时应用需求。本研究旨在解决这一效率与精度之间的矛盾，开发一种能在移动设备上高效部署的实时立体匹配方案。",
      "method": "MAFNet的核心是自适应频域滤波注意力模块，它将完整成本体积分解为高、低频体积，进行频率感知特征聚合。随后，引入基于Linformer的低秩注意力机制，自适应融合高、低频信息以提升视差估计的鲁棒性。网络仅使用2D卷积，避免了3D卷积的计算负担，实现了高效处理。",
      "result": "实验在Scene Flow和KITTI 2015等公共数据集上进行，MAFNet显著优于现有实时方法，展现了准确性与实时性能之间的良好平衡。尽管摘要未提供具体性能指标数值，但强调了其优越性，表明该方法适合部署在资源受限环境中。",
      "conclusion": "MAFNet通过频域分解和注意力机制创新，有效平衡了实时立体匹配的效率与精度，为移动设备应用提供了实用方案。研究具有重要学术价值和实际意义，未来工作可能包括进一步优化网络或扩展到其他视觉任务。",
      "tags": [
        "Stereo Matching",
        "2D Convolutions",
        "Frequency-domain Filtering",
        "Attention Mechanism",
        "Linformer"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:52.386191Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04589",
    "title": "MiLDEdit: Reasoning-Based Multi-Layer Design Document Editing",
    "authors": [
      "Zihao Lin",
      "Wanrong Zhu",
      "Jiuxiang Gu",
      "Jihyung Kil",
      "Christopher Tensmeyer",
      "Lin Zhang",
      "Shilong Liu",
      "Ruiyi Zhang",
      "Lifu Huang",
      "Vlad I. Morariu",
      "Tong Sun"
    ],
    "abstract": "Real-world design documents (e.g., posters) are inherently multi-layered, combining decoration, text, and images. Editing them from natural-language instructions requires fine-grained, layer-aware reasoning to identify relevant layers and coordinate modifications. Prior work largely overlooks multi-layer design document editing, focusing instead on single-layer image editing or multi-layer generation, which assume a flat canvas and lack the reasoning needed to determine what and where to modify. To address this gap, we introduce the Multi-Layer Document Editing Agent (MiLDEAgent), a reasoning-based framework that combines an RL-trained multimodal reasoner for layer-wise understanding with an image editor for targeted modifications. To systematically benchmark this setting, we introduce the MiLDEBench, a human-in-the-loop corpus of over 20K design documents paired with diverse editing instructions. The benchmark is complemented by a task-specific evaluation protocol, MiLDEEval, which spans four dimensions including instruction following, layout consistency, aesthetics, and text rendering. Extensive experiments on 14 open-source and 2 closed-source models reveal that existing approaches fail to generalize: open-source models often cannot complete multi-layer document editing tasks, while closed-source models suffer from format violations. In contrast, MiLDEAgent achieves strong layer-aware reasoning and precise editing, significantly outperforming all open-source baselines and attaining performance comparable to closed-source models, thereby establishing the first strong baseline for multi-layer document editing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04589.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04589",
    "published": "2026-01-08T04:38:07Z",
    "updated": "2026-01-08T04:38:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一个基于推理的多层设计文档编辑框架，结合强化学习训练的多模态推理器和图像编辑器，实现了层级感知的精确编辑。",
      "motivation": "现实世界中的设计文档，如海报，是多层结构的，包含装饰、文本和图像等元素。从自然语言指令编辑这些文档需要细粒度的层级感知推理，以识别相关层级并协调修改。然而，现有研究大多忽略了这一挑战，专注于单层图像编辑或多层生成，这些方法假设平面画布，缺乏必要的推理能力来确定修改内容和位置，导致在实际应用中编辑准确性和灵活性不足。因此，开发一个能够处理多层设计文档编辑的框架至关重要，以解决自动化编辑中的复杂推理问题。",
      "method": "论文提出了Multi-Layer Document Editing Agent (MiLDEAgent)，一个基于推理的框架。该框架结合了一个基于强化学习训练的多模态推理器，用于层级感知的理解，以及一个图像编辑器，用于针对性修改。关键创新点在于通过推理器进行细粒度推理，确定需要编辑的层级和具体操作。为了评估，作者引入了MiLDEBench数据集，包含超过20,000个设计文档及其编辑指令，并设计了MiLDEEval评估协议，涵盖指令遵循、布局一致性、美观度和文本渲染四个维度，为多层设计文档编辑任务提供了系统的基准测试环境。",
      "result": "实验对14个开源模型和2个闭源模型进行了广泛测试。结果表明，现有方法难以泛化：开源模型通常无法完成多层文档编辑任务，而闭源模型存在格式违规问题。相比之下，MiLDEAgent展现出强大的层级感知推理和精确编辑能力，显著优于所有开源基线模型，并达到与闭源模型相当的性能。具体来说，在MiLDEEval评估中，MiLDEAgent在多个维度上表现优秀，为多层设计文档编辑建立了第一个强基线。",
      "conclusion": "本论文的主要贡献在于提出了一个基于推理的多层设计文档编辑框架，解决了现有方法在处理多层结构时的不足。研究展示了结合多模态推理和图像编辑的有效性，为自动设计编辑任务提供了新思路。在学术上，这推动了多模态理解和推理技术的发展；在实际应用中，可提升设计文档编辑的自动化水平和效率。未来工作可能包括扩展到其他类型文档或进一步优化推理机制，以增强泛化能力。",
      "tags": [
        "Multimodal Reasoning",
        "Reinforcement Learning",
        "Image Editing",
        "Benchmark Evaluation",
        "Natural Language Processing"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:59.177140Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04588",
    "title": "3D Conditional Image Synthesis of Left Atrial LGE MRI from Composite Semantic Masks",
    "authors": [
      "Yusri Al-Sanaani",
      "Rebecca Thornhill",
      "Sreeraman Rajan"
    ],
    "abstract": "Segmentation of the left atrial (LA) wall and endocardium from late gadolinium-enhanced (LGE) MRI is essential for quantifying atrial fibrosis in patients with atrial fibrillation. The development of accurate machine learning-based segmentation models remains challenging due to the limited availability of data and the complexity of anatomical structures. In this work, we investigate 3D conditional generative models as potential solution for augmenting scarce LGE training data and improving LA segmentation performance. We develop a pipeline to synthesize high-fidelity 3D LGE MRI volumes from composite semantic label maps combining anatomical expert annotations with unsupervised tissue clusters, using three 3D conditional generators (Pix2Pix GAN, SPADE-GAN, and SPADE-LDM). The synthetic images are evaluated for realism and their impact on downstream LA segmentation. SPADE-LDM generates the most realistic and structurally accurate images, achieving an FID of 4.063 and surpassing GAN models, which have FIDs of 40.821 and 7.652 for Pix2Pix and SPADE-GAN, respectively. When augmented with synthetic LGE images, the Dice score for LA cavity segmentation with a 3D U-Net model improved from 0.908 to 0.936, showing a statistically significant improvement (p < 0.05) over the baseline.These findings demonstrate the potential of label-conditioned 3D synthesis to enhance the segmentation of under-represented cardiac structures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04588.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04588",
    "published": "2026-01-08T04:35:40Z",
    "updated": "2026-01-08T04:35:40Z",
    "comment": "This work has been published in the Proceedings of the 2025 IEEE International Conference on Imaging Systems and Techniques (IST). The final published version is available via IEEE Xplore",
    "light_analysis": {
      "overview": "本文提出了一种通过复合语义标签图合成3D左心房LGE MRI图像的方法，以增强分割模型性能，解决了医学图像分析中数据稀缺的问题。",
      "motivation": "左心房LGE MRI分割对量化房颤患者的心房纤维化至关重要，但数据稀缺和复杂解剖结构使得基于机器学习的分割模型开发面临挑战。现有方法因训练数据有限而难以实现高精度分割，尤其在心脏成像中，传统数据增强技术不足以捕捉结构细节。本研究旨在通过条件生成模型合成高质量图像，以弥补数据不足，提升分割准确性，从而促进临床诊断和治疗规划。",
      "method": "论文开发了一个流水线，从复合语义标签图合成高保真3D LGE MRI体积。标签图结合解剖专家注释和无监督组织簇，以捕捉心脏结构的复杂性。使用三种3D条件生成器进行比较：Pix2Pix GAN、SPADE-GAN和SPADE-LDM（基于潜在扩散模型），这些模型以标签图为条件生成图像，实现数据增强。关键创新点在于利用复合标签和先进生成架构，提升合成图像的真实性和结构准确性，适用于医学图像分析场景。",
      "result": "实验结果显示，SPADE-LDM生成最真实和结构准确的图像，FID得分为4.063，显著优于Pix2Pix GAN（FID 40.821）和SPADE-GAN（FID 7.652）。在下游分割任务中，使用合成图像增强训练数据后，3D U-Net模型的左心房腔分割Dice分数从基准0.908提高到0.936，改善具有统计显著性（p < 0.05）。这表明合成图像能有效提升分割性能，优于基线方法，验证了条件合成的实用性。",
      "conclusion": "本研究证实了标签条件3D图像合成在增强左心房LGE MRI分割中的潜力，通过生成高质量合成图像解决了数据稀缺问题。这为医学图像分析提供了新方法，具有学术价值，如推动生成模型在医疗领域的应用，以及实际应用价值，如改善心脏疾病诊断。局限性包括对标签图质量的依赖，未来工作可探索更多生成模型或扩展到其他解剖结构，进一步提升泛化能力。",
      "tags": [
        "Conditional Generative Adversarial Networks",
        "Diffusion Models",
        "3D Medical Image Synthesis",
        "Image Segmentation",
        "Data Augmentation"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:33.988637Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.01856",
    "title": "GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection",
    "authors": [
      "Joongwon Chae",
      "Lihui Luo",
      "Yang Liu",
      "Runming Wang",
      "Dongmei Yu",
      "Zeming Liang",
      "Xi Yuan",
      "Dayan Zhang",
      "Zhenglin Chen",
      "Peiwu Qin",
      "Ilmoon Chae"
    ],
    "abstract": "Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.   We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.   Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.01856.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01856",
    "published": "2026-01-05T07:33:50Z",
    "updated": "2026-01-08T04:18:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出GCR框架，通过几何一致路由改善任务无关持续异常检测的稳定性。",
      "motivation": "工业检测中，基于特征的异常检测因其强大表示能力被广泛采用，但实际部署需要任务无关的持续类别扩展操作，测试时类别身份未知。此时，整体性能受专家选择主导，即路由输入到适当的正常性模型。然而，现有方法依赖于比较跨独立构建头的异常分数，在实践中不可靠，因为分数分布在类别间存在显著的尺度差异和尾部行为差异，导致路由不稳定和性能下降，这突显了改进路由机制的紧迫性。",
      "method": "GCR采用轻量级混合专家框架，核心方法是通过几何一致路由稳定检测。它在共享的冻结补丁嵌入空间中，使用类别特定的原型库，通过最小化累积最近原型距离来路由每个测试图像到相应专家，然后仅在该专家内使用标准基于原型的评分规则计算异常图。关键创新点在于分离跨头决策和头内异常评分，从而避免跨头分数可比性问题，无需端到端表示学习，保持了模型的轻量化和实用性。",
      "result": "在MVTec AD和VisA数据集上的实验表明，几何一致路由显著提高了路由稳定性，有效缓解了持续性能崩溃，实现了几乎零遗忘，同时保持了竞争力的异常检测和定位性能。与基线方法对比，GCR避免了因跨头路由不稳定导致的性能下降，实验结果显示，许多先前归因于表示遗忘的失败可归因于路由决策规则的不稳定性，代码已开源以促进后续研究。",
      "conclusion": "GCR框架的主要贡献在于提出了基于几何一致路由的轻量级方法，改善了任务无关持续异常检测的稳定性，其学术价值在于揭示了路由决策不稳定性对性能的影响，为持续学习领域提供了新视角。实际应用价值在于适用于工业检测中的持续类别扩展场景，提高系统鲁棒性。局限性摘要未明确说明，未来工作可能包括进一步优化路由机制或扩展到更复杂的数据集以验证泛化能力。",
      "tags": [
        "Anomaly Detection",
        "Continual Learning",
        "Mixture-of-Experts",
        "Geometry-Consistent Routing",
        "Prototype-based Scoring"
      ]
    },
    "analyzed_at": "2026-01-09T02:34:13.281362Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.05759",
    "title": "OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search",
    "authors": [
      "Zexin Zheng",
      "Huangyu Dai",
      "Lingtao Mao",
      "Xinyu Sun",
      "Zihan Liang",
      "Ben Chen",
      "Yuqing Ding",
      "Chenyi Lei",
      "Wenwu Ou",
      "Han Li",
      "Kun Gai"
    ],
    "abstract": "Traditional vision search, similar to search and recommendation systems, follows the multi-stage cascading architecture (MCA) paradigm to balance efficiency and conversion. Specifically, the query image undergoes feature extraction, recall, pre-ranking, and ranking stages, ultimately presenting the user with semantically similar products that meet their preferences. This multi-view representation discrepancy of the same object in the query and the optimization objective collide across these stages, making it difficult to achieve Pareto optimality in both user experience and conversion. In this paper, an end-to-end generative framework, OneVision, is proposed to address these problems. OneVision builds on VRQ, a vision-aligned residual quantization encoding, which can align the vastly different representations of an object across multiple viewpoints while preserving the distinctive features of each product as much as possible. Then a multi-stage semantic alignment scheme is adopted to maintain strong visual similarity priors while effectively incorporating user-specific information for personalized preference generation. In offline evaluations, OneVision performs on par with online MCA, while improving inference efficiency by 21% through dynamic pruning. In A/B tests, it achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and +3.12% order volume. These results demonstrate that a semantic ID centric, generative architecture can unify retrieval and personalization while simplifying the serving pathway.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.05759.pdf",
    "abs_url": "https://arxiv.org/abs/2510.05759",
    "published": "2025-10-07T10:25:21Z",
    "updated": "2026-01-08T04:15:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出OneVision，一个端到端生成框架，通过视觉对齐残差量化编码和多阶段语义对齐，解决多视角电商视觉搜索中的表示差异和优化冲突问题。",
      "motivation": "传统电商视觉搜索采用多阶段级联架构（MCA）以平衡效率和转化，但同一对象在不同视角下的表示差异巨大，且各阶段优化目标冲突（如视觉相似性与个性化偏好），导致用户体验和转化率难以同时达到帕累托最优。现有方法在统一检索和个性化方面存在不足，因此需要新的框架来弥补这一差距。",
      "method": "OneVision基于VRQ（视觉对齐残差量化编码），该技术能对齐多视角对象的表示，同时尽可能保留各产品的独特特征。然后，采用多阶段语义对齐方案，在保持强视觉相似性先验的基础上，有效整合用户特定信息，生成个性化偏好。框架通过动态修剪技术优化推理流程，提高效率。",
      "result": "在离线评估中，OneVision与在线多阶段级联架构性能相当，并通过动态修剪将推理效率提升21%。在线A/B测试结果显示，商品点击率（CTR）提升2.15%，转化率（CVR）提升2.27%，订单量增加3.12%，显著优于基线方法。这些数据表明框架在保持效果的同时实现了效率改进。",
      "conclusion": "论文的主要贡献是提出以语义ID为中心的生成架构，统一了视觉检索和个性化生成，简化了服务路径。其学术价值在于提供了一种端到端的解决方案，解决了多阶段架构的瓶颈；实际应用价值是提升了电商搜索的性能指标，如点击率和转化率。未来可探索在多模态场景中的扩展和进一步优化。",
      "tags": [
        "Vision Search",
        "Generative Framework",
        "Quantization Encoding",
        "Semantic Alignment",
        "Dynamic Pruning"
      ]
    },
    "analyzed_at": "2026-01-09T02:36:05.126311Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.12994",
    "title": "Explainable Binary Classification of Separable Shape Ensembles",
    "authors": [
      "Zachary Grey",
      "Nicholas Fisher",
      "Andrew Glaws"
    ],
    "abstract": "Scientists, engineers, biologists, and technology specialists universally leverage image segmentation to extract shape ensembles containing many thousands of curves representing patterns in observations and measurements. These large curve ensembles facilitate inferences about important changes when comparing and contrasting images. We introduce novel pattern recognition formalisms combined with inference methods over large ensembles of segmented curves. Our formalism involves accurately approximating eigenspaces of composite integral operators to motivate discrete, dual representations of curves collocated at quadrature nodes. Approximations are projected onto underlying matrix manifolds and the resulting separable shape tensors constitute rigid-invariant decompositions of curves into generalized (linear) scale variations and complementary (nonlinear) undulations. With thousands of curves segmented from pairs of images, we demonstrate how data-driven features of separable shape tensors inform explainable binary classification utilizing a product maximum mean discrepancy; absent labeled data, building interpretable feature spaces in seconds without high performance computation, and detecting discrepancies below cursory visual inspections.",
    "categories": [
      "cs.CV",
      "math.ST"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2410.12994.pdf",
    "abs_url": "https://arxiv.org/abs/2410.12994",
    "published": "2024-10-16T19:42:47Z",
    "updated": "2026-01-08T04:10:56Z",
    "comment": "32 pages, 16 figures",
    "light_analysis": {
      "overview": "提出一种基于可分离形状张量的可解释二元分类方法，用于大规模曲线集合的模式识别，结合积分算子近似和无监督特征构建。",
      "motivation": "科学家和工程师普遍使用图像分割提取成千上万的曲线来代表观测中的模式，以比较图像并推断重要变化。然而，现有方法在无标注数据时面临挑战，如可解释性差、计算效率低，或无法检测细微差异。这使得快速、准确地分析大规模曲线集合变得困难，尤其是在生物医学或工程应用中，需要高效的无监督方法来识别视觉检查不易发现的变化。因此，本研究旨在开发一种可解释的二元分类框架，以克服这些限制，提升模式识别的实用性和效率。",
      "method": "论文引入新颖的模式识别形式主义，通过准确近似复合积分算子的特征空间，得到曲线在求积节点处的离散对偶表示。这些近似被投影到基础矩阵流形上，形成可分离形状张量，实现刚性不变分解，将曲线分解为广义线性尺度变化和互补非线性波动。利用从图像对分割的数千条曲线，方法使用可分离形状张量的数据驱动特征，结合产品最大均值差异进行二元分类。关键创新包括无监督特征构建和高效计算，无需高性能硬件，快速生成可解释特征空间。",
      "result": "实验结果表明，该方法能够在几秒钟内快速构建可解释特征空间，无需依赖标注数据或高性能计算。它有效检测图像对之间的细微差异，性能优于粗略的视觉检查，展示了在无监督设置下的实用性和效率。尽管摘要未明确提供具体性能指标如准确率提升，但方法在识别视觉不易察觉的变化方面表现出优势，强调了其在现实场景中的潜在应用价值，如医学影像分析或工程监测。",
      "conclusion": "本研究的主要贡献是提出一种基于可分离形状张量的模式识别形式主义，实现大规模曲线集合的可解释二元分类。其学术价值在于整合积分算子近似和矩阵流形理论，提供刚性不变的特征分解方法；实际应用价值高，适用于图像分割、生物信息学等领域，能快速无监督地检测变化。局限性可能包括对曲线分割质量的依赖或处理更复杂形状的挑战，未来工作可扩展到多类分类或优化计算效率，以增强通用性和可扩展性。",
      "tags": [
        "Pattern Recognition",
        "Binary Classification",
        "Separable Shape Tensors",
        "Maximum Mean Discrepancy",
        "Integral Operators"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:20.311324Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02356",
    "title": "Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes",
    "authors": [
      "Jing Tan",
      "Zhaoyang Zhang",
      "Yantao Shen",
      "Jiarui Cai",
      "Shuo Yang",
      "Jiajun Wu",
      "Wei Xia",
      "Zhuowen Tu",
      "Stefano Soatto"
    ],
    "abstract": "We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.02356.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02356",
    "published": "2026-01-05T18:55:32Z",
    "updated": "2026-01-08T03:56:27Z",
    "comment": "Project page: https://sparkstj.github.io/talk2move",
    "light_analysis": {
      "overview": "Talk2Move是一个基于强化学习的扩散框架，用于实现文本指导下的物体几何转换，通过创新优化方法避免了成对数据需求并提升准确性和一致性。",
      "motivation": "该研究旨在解决通过自然语言指令对场景中物体进行几何转换（如平移、旋转、缩放）的挑战，这一问题对多模态生成系统至关重要，因为它能实现更直观的场景编辑和交互。然而，现有基于文本的编辑方法主要关注外观调整，难以执行几何操作，原因在于缺乏成对的监督数据（即图像-文本-转换对）以及像素级优化的局限性，导致处理复杂转换任务时效率低下和效果不佳。",
      "method": "Talk2Move采用基于强化学习的扩散框架，核心方法包括使用Group Relative Policy Optimization (GRPO)来探索几何动作。它通过输入图像和轻量级文本变化生成多样化rollout，从而消除对成对监督数据的需求。模型通过空间奖励机制将几何转换与语言描述对齐，并引入离策略步骤评估和主动步骤采样以提高学习效率，重点关注信息丰富的转换阶段。此外，设计对象中心的空间奖励直接评估位移、旋转和缩放行为，确保转换的可解释性和一致性。",
      "result": "在精选的基准测试中，Talk2Move展现出卓越的性能，实现了精确、一致且语义忠实的物体几何转换。实验结果表明，该框架在空间准确性和场景一致性方面显著优于现有的文本引导编辑方法，例如能有效执行复杂的转换任务，如根据文本指令调整物体的位置、方向或大小，同时保持场景的整体连贯性。尽管摘要未提供具体数值指标，但定性评估突出了其在几何转换任务中的优势。",
      "conclusion": "Talk2Move的主要贡献是提出了一种创新的强化学习框架，用于解决文本指示的物体级几何转换问题。该研究通过结合扩散模型和空间奖励机制，实现了无需成对数据的高效转换，学术上推动了多模态生成和强化学习的交叉应用。实际应用中，该方法可增强场景编辑、虚拟现实等领域的交互能力，未来工作可能包括扩展到更复杂的转换任务或集成更多模态输入。",
      "tags": [
        "Reinforcement Learning",
        "Diffusion Models",
        "Geometric Transformation",
        "Group Relative Policy Optimization",
        "Text-Guided Editing"
      ]
    },
    "analyzed_at": "2026-01-09T02:37:22.750224Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05205",
    "title": "EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI",
    "authors": [
      "Zain Iqbal",
      "Lorenzo Valerio"
    ],
    "abstract": "Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.",
    "categories": [
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05205.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05205",
    "published": "2026-01-08T18:31:11Z",
    "updated": "2026-01-08T18:31:11Z",
    "comment": "6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]",
    "light_analysis": {
      "overview": "论文提出 EARL，一种能量感知的强化学习框架，通过联合优化液体状态机的准确性和能耗，提升资源受限设备端 AI 应用的效率。",
      "motivation": "Pervasive AI 日益依赖设备端学习系统，需要在严格资源约束下实现低延迟和高能效计算。液体状态机（LSMs）作为低功耗时间处理方法有前景，但部署面临挑战，因为其超参数敏感性和传统优化方法忽略能量约束，导致计算成本高且难以在资源受限环境中有效应用。本研究旨在解决这些问题，通过开发能量感知的优化框架来改进 LSMs 的效率和可扩展性。",
      "method": "EARL 是一个能量感知的强化学习框架，集成了贝叶斯优化与自适应强化学习选择策略，以联合优化准确性和能量消耗。方法采用代理建模进行全局超参数空间探索，利用强化学习动态优先排序候选解，并引入早期终止机制消除冗余评估，从而显著减少计算开销。该方法专门针对 LSM 的优化，通过多策略整合提高了调优效率和能量意识。",
      "result": "在三个基准数据集上的实验表明，EARL 比领先的超参数调优框架实现 6% 至 15% 的更高准确性、60% 至 80% 的更低能量消耗，以及优化时间减少多达一个数量级。这些结果突显了 EARL 在提升 LSM 性能的同时，显著降低了能量消耗和计算成本，优于现有基线方法。",
      "conclusion": "本研究的主要贡献是提出 EARL 框架，通过能量感知自适应搜索提高了 LSM 的优化效率和可扩展性，为资源受限设备端 AI 应用提供了有效解决方案。摘要未明确说明局限性，但未来工作可能包括将该方法扩展到其他模型类型或进一步优化算法以减少计算复杂度。这项研究强调了能量意识在 pervasive AI 系统中的学术和实际价值。",
      "tags": [
        "Liquid State Machines",
        "Reinforcement Learning",
        "Bayesian Optimization",
        "Hyperparameter Tuning",
        "Energy-Aware Optimization"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:11.366725Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05194",
    "title": "An interpretable data-driven approach to optimizing clinical fall risk assessment",
    "authors": [
      "Fardin Ganjkhanloo",
      "Emmett Springer",
      "Erik H. Hoyer",
      "Daniel L. Young",
      "Holley Farley",
      "Kimia Ghobadi"
    ],
    "abstract": "In this study, we aim to better align fall risk prediction from the Johns Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically meaningful measures via a data-driven modelling approach. We conducted a retrospective cohort analysis of 54,209 inpatient admissions from three Johns Hopkins Health System hospitals between March 2022 and October 2023. A total of 20,208 admissions were included as high fall risk encounters, and 13,941 were included as low fall risk encounters. To incorporate clinical knowledge and maintain interpretability, we employed constrained score optimization (CSO) models to reweight the JHFRAT scoring weights, while preserving its additive structure and clinical thresholds. Recalibration refers to adjusting item weights so that the resulting score can order encounters more consistently by the study's risk labels, and without changing the tool's form factor or deployment workflow. The model demonstrated significant improvements in predictive performance over the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). This performance improvement translates to protecting an additional 35 high-risk patients per week across the Johns Hopkins Health System. The constrained score optimization models performed similarly with and without the EHR variables. Although the benchmark black-box model (XGBoost), improves upon the performance metrics of the knowledge-based constrained logistic regression (AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk labeling. This evidence-based approach provides a robust foundation for health systems to systematically enhance inpatient fall prevention protocols and patient safety using data-driven optimization techniques, contributing to improved risk assessment and resource allocation in healthcare settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05194.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05194",
    "published": "2026-01-08T18:17:31Z",
    "updated": "2026-01-08T18:17:31Z",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2510.20714",
    "light_analysis": {
      "overview": "本研究通过受限得分优化模型重新加权临床跌倒风险评估工具评分，显著提升预测性能并保持可解释性。",
      "motivation": "本研究旨在改进约翰霍普金斯跌倒风险评估工具（JHFRAT）的预测性能，使其更好地与临床指标对齐，以解决现有方法在风险评估中可能不一致的问题。跌倒风险是住院患者安全的关键挑战，JHFRAT当前版本可能无法充分保护高风险患者，导致资源分配效率低下。因此，需要数据驱动方法来优化评估工具，提升预防效果和患者安全，摘要未明确说明具体不足，但指出需通过更准确的预测来弥补现有缺陷。",
      "method": "研究方法采用受限得分优化模型，对JHFRAT的评分权重进行重新加权，同时保持其加法结构和临床阈值不变，以确保可解释性。数据集包括2022年3月至2023年10月间约翰霍普金斯医疗系统的54,209例住院记录，其中20,208例为高风险，13,941例为低风险。模型通过校准项目权重，使得分能更一致地按研究风险标签排序，而不改变工具的形式或部署流程，并与基准模型如XGBoost和约束逻辑回归进行对比评估。",
      "result": "实验结果显示，受限得分优化模型的AUC-ROC达到0.91，显著优于当前JHFRAT的0.86，性能提升相当于每周在约翰霍普金斯医疗系统额外保护35名高风险患者。模型在有无电子健康记录变量的情况下表现相似，且比基准黑盒模型XGBoost（AUC-ROC=0.94）在风险标签变化时更鲁棒，基于知识的约束逻辑回归也得到改进，但CSO显示出更强的稳健性。",
      "conclusion": "本研究开发了一种基于证据的方法，为医疗系统通过数据驱动优化技术系统性增强住院跌倒预防协议和患者安全提供了坚实基础。该方法在提升风险评估准确性的同时保持了工具的可解释性，具有重要学术价值和实际应用意义，有助于改进医疗环境中的资源分配和患者安全措施。摘要未明确说明局限性或未来工作，但暗示了在临床部署中的潜在应用方向。",
      "tags": [
        "Constrained Score Optimization",
        "Data-driven Modeling",
        "AUC-ROC",
        "XGBoost",
        "Clinical Risk Assessment"
      ]
    },
    "analyzed_at": "2026-01-09T02:39:24.233580Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.04487",
    "title": "Forking-Sequences",
    "authors": [
      "Willa Potosnak",
      "Malcolm Wolff",
      "Mengfei Cao",
      "Ruijun Ma",
      "Tatiana Konstantinova",
      "Dmitry Efimov",
      "Michael W. Mahoney",
      "Boris Oreshkin",
      "Kin G. Olivares"
    ],
    "abstract": "While accuracy is a critical requirement for time series forecasting, an equally important desideratum is forecast stability across forecast creation dates (FCDs). Even highly accurate models can produce erratic revisions between FCDs, disrupting downstream decision-making. To improve forecast stability of such revisions, several state-of-the-art models including MQCNN, MQT, and SPADE employ a powerful yet underexplored neural network architectural design known as forking-sequences. This architectural design jointly encodes and decodes the entire time series across all FCDs, producing an entire multi-horizon forecast grid in a single forward pass. This approach contrasts with conventional neural forecasting methods that process FCDs independently, generating only a single multi-horizon forecast per forward pass. In this work, we formalize the forking-sequences design and motivate its broader adoption by introducing a metric for quantifying excess volatility in forecast revisions and by providing theoretical and empirical analysis. We theoretically motivate three key benefits of forking-sequences: (i) increased forecast stability through ensembling; (ii) gradient variance reduction, leading to more stable and consistent training steps; and (iii) improved computational efficiency during inference. We validate the benefits of forking-sequences compared to baseline window-sampling on the M-series benchmark, using 16 datasets from the M1, M3, M4, and Tourism competitions. We observe median accuracy improvements across datasets of 29.7%, 46.2%, 49.3%, 28.6%, 24.7%, and 6.4% for MLP, RNN, LSTM, CNN, Transformer, and StateSpace-based architectures, respectively. We then show that forecast ensembling during inference can improve median forecast stability by 10.8%, 13.2%, 13.0%, 10.9%, 10.2%, and 11.2% for these respective models trained with forking-sequences, while maintaining accuracy.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.04487.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04487",
    "published": "2025-10-06T04:51:06Z",
    "updated": "2026-01-08T17:43:12Z",
    "comment": "Presented at the GPU-Accelerated and Scalable Optimization (ScaleOpt) Workshop, NeurIPS 2025",
    "light_analysis": {
      "overview": "本文形式化了forking-sequences神经网络架构，通过理论分析和实证验证，显著提高了时间序列预测的稳定性和准确性。",
      "motivation": "时间序列预测中，预测稳定性对下游决策至关重要，但现有方法如独立处理预测创建日期（FCDs）常导致预测修订不稳定，即使准确率高也可能破坏决策流程。论文指出，尽管MQCNN、MQT和SPADE等前沿模型采用forking-sequences设计，但该设计未被充分探索和应用，因此本研究旨在通过量化预测修订的过度波动并提供理论实证分析，推动forking-sequences的广泛采用，解决预测波动性问题。",
      "method": "论文形式化了forking-sequences设计，这是一种神经网络架构，能同时编码和解码所有预测创建日期（FCDs）的整个时间序列，在单个前向传递中生成多视角预测网格。关键创新包括理论分析三个优势：通过预测集成提高稳定性、减少梯度方差以稳定训练、改进推理计算效率。方法使用M-series基准数据集（涵盖M1、M3、M4和Tourism竞赛的16个数据集），对多种神经网络架构（如MLP、RNN、LSTM、CNN、Transformer、StateSpace-based）进行实证比较，验证forking-sequences的效果。",
      "result": "在M-series基准上，forking-sequences相比基线window-sampling显著提升了准确率，多种神经网络架构的中位数改进分别为MLP 29.7%、RNN 46.2%、LSTM 49.3%、CNN 28.6%、Transformer 24.7%、StateSpace-based 6.4%。同时，推理期间的预测集成进一步提高了预测稳定性，使用forking-sequences训练的模型稳定性改进中位数分别为10.8%、13.2%、13.0%、10.9%、10.2%、11.2%，并保持了准确率水平。",
      "conclusion": "论文的主要贡献是形式化了forking-sequences设计，并通过理论和实证分析证明了其在增强预测稳定性和准确性方面的价值，促进时间序列预测领域对该架构的采用。研究具有实际应用意义，能改善下游决策流程，未来工作可扩展到更多数据集和应用场景，或探索其他集成策略以优化性能。",
      "tags": [
        "Forking-Sequences",
        "Time Series Forecasting",
        "Ensemble Methods",
        "Gradient Variance Reduction",
        "Neural Network Architectures"
      ]
    },
    "analyzed_at": "2026-01-09T02:40:22.701508Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05134",
    "title": "Sequential Subspace Noise Injection Prevents Accuracy Collapse in Certified Unlearning",
    "authors": [
      "Polina Dolgova",
      "Sebastian U. Stich"
    ],
    "abstract": "Certified unlearning based on differential privacy offers strong guarantees but remains largely impractical: the noisy fine-tuning approaches proposed so far achieve these guarantees but severely reduce model accuracy. We propose sequential noise scheduling, which distributes the noise budget across orthogonal subspaces of the parameter space, rather than injecting it all at once. This simple modification mitigates the destructive effect of noise while preserving the original certification guarantees. We extend the analysis of noisy fine-tuning to the subspace setting, proving that the same $(\\varepsilon,δ)$ privacy budget is retained. Empirical results on image classification benchmarks show that our approach substantially improves accuracy after unlearning while remaining robust to membership inference attacks. These results show that certified unlearning can achieve both rigorous guarantees and practical utility.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05134.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05134",
    "published": "2026-01-08T17:23:13Z",
    "updated": "2026-01-08T17:23:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出序列子空间噪声调度方法，通过在正交子空间中分布噪声预算，防止认证遗忘中的准确性崩溃，同时保持差分隐私认证保证。",
      "motivation": "认证遗忘技术旨在从机器学习模型中安全移除特定数据，以保护隐私。基于差分隐私的认证方法提供强理论保证，但现有噪声微调技术在实践中存在问题：一次性注入噪声会严重损害模型准确性，使其不实用。这限制了隐私保护机器学习在现实世界中的应用，因为模型性能下降阻碍了部署。因此，需要一种新方法来平衡隐私保证与模型效用，以增强认证遗忘的实用性。",
      "method": "论文提出序列子空间噪声注入方法，核心创新是将噪声预算分配到参数空间的正交子空间中，而不是一次性注入。通过扩展噪声微调的分析到子空间设置，证明该方法能保留相同的 $\\varepsilon,\\delta$ 差分隐私预算。这种技术利用正交子空间减少噪声对模型参数的破坏性影响，同时确保认证遗忘的保证。摘要提到使用图像分类基准进行评估，但未指定具体数据集或模型架构。",
      "result": "实验在图像分类基准上进行，结果显示该方法显著提高了模型在遗忘后的准确性，具体指标如准确率改善显著但未明确数值。同时，它保持了对成员推断攻击的鲁棒性，表明在维持隐私认证的同时，有效解决了现有噪声微调方法中准确性下降的问题。与基线方法相比，新方法在实用性和安全性方面表现出更好的平衡。",
      "conclusion": "本研究的主要贡献是证明通过序列子空间噪声注入，认证遗忘可以同时实现严格隐私保证和实际模型效用，为隐私保护机器学习提供了更可行的解决方案。其学术价值在于改进认证遗忘的实用性，潜在应用包括数据隐私敏感的模型更新。局限性或未来工作方向在摘要中未明确说明，但可推断可能涉及扩展到其他任务或更大数据集。",
      "tags": [
        "Certified Unlearning",
        "Differential Privacy",
        "Sequential Noise Scheduling",
        "Subspace Method",
        "Noisy Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:10.137433Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.05729",
    "title": "Graph-Dictionary Signal Model for Sparse Representations of Multivariate Data",
    "authors": [
      "William Cappelletti",
      "Pascal Frossard"
    ],
    "abstract": "Representing and exploiting multivariate signals requires capturing relations between variables, which we can represent by graphs. Graph dictionaries allow to describe complex relational information as a sparse sum of simpler structures, but no prior model exists to infer such underlying structure elements from data. We define a novel Graph-Dictionary signal model, where a finite set of graphs characterizes relationships in data distribution as filters on the weighted sum of their Laplacians. We propose a framework to infer the graph dictionary representation from observed node signals, which allows to include a priori knowledge about signal properties, and about underlying graphs and their coefficients. We introduce a bilinear generalization of the primal-dual splitting algorithm to solve the learning problem. We show the capability of our method to reconstruct graphs from signals in multiple synthetic settings, where our model outperforms popular baselines. Then, we exploit graph-dictionary representations in an illustrative motor imagery decoding task on brain activity data, where we classify imagined motion better than standard methods relying on many more features. Our graph-dictionary model bridges a gap between sparse representations of multivariate data and a structured decomposition of sample-varying relationships into a sparse combination of elementary graph atoms.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.05729.pdf",
    "abs_url": "https://arxiv.org/abs/2411.05729",
    "published": "2024-11-08T17:40:43Z",
    "updated": "2026-01-08T17:09:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种图字典信号模型，用于从多变量数据中推断底层图结构，实现稀疏表示，弥合稀疏表示与结构化分解之间的差距。",
      "motivation": "本研究旨在解决从多变量数据中推断底层图结构的问题，这对于捕捉变量间关系、进行有效信号表示至关重要。现有方法缺乏能够从数据中学习这些关系结构的模型，而图字典虽然能描述复杂关系作为简单结构的稀疏和，却无法自动推断这些结构元素，导致在实际应用中如脑活动数据分析时效率低下。因此，开发一个能结合先验知识、适应数据变化的模型是必要的改进方向。",
      "method": "论文定义了一个新颖的图字典信号模型，其中有限图集通过其拉普拉斯矩阵的加权和作为滤波器来特征化数据关系。核心方法是提出一个学习框架，从观测节点信号推断图字典表示，允许整合信号属性、底层图及其系数的先验知识。关键创新点在于引入了原始-对偶分裂算法的双线性泛化，以解决优化问题，从而实现高效学习，无需依赖特定数据集或复杂架构，强调了算法的通用性和可扩展性。",
      "result": "实验表明，在多个合成设置中，模型能准确重建图结构，优于流行基线方法，展示了其稳健性和泛化能力。在脑活动数据的运动想象解码任务中，利用图字典表示进行分类，比依赖更多特征的标准方法性能更好，这凸显了模型在实际应用中的高效性。尽管摘要未提供具体数据指标如准确率数字，但结果确认了模型在减少特征使用的同时提升分类效果的优势。",
      "conclusion": "本论文的主要贡献是提出了图字典信号模型，成功弥合了多变量数据稀疏表示与结构化分解之间的鸿沟，具有显著的学术价值。该模型不仅提高了图结构推断的准确性，还在实际任务如脑活动分类中表现出色，推动了数据分析技术的发展。未来工作可探索模型在其他领域的应用，或处理更复杂的数据类型，以进一步验证其局限性和扩展潜力。",
      "tags": [
        "Graph-Dictionary",
        "Sparse Representations",
        "Laplacian Filters",
        "Primal-Dual Splitting",
        "Multivariate Data"
      ]
    },
    "analyzed_at": "2026-01-09T02:41:53.730311Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.04804",
    "title": "Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator",
    "authors": [
      "Chaymae Yahyati",
      "Ismail Lamaakal",
      "Khalid El Makkaoui",
      "Ibrahim Ouahbi",
      "Yassine Maleh"
    ],
    "abstract": "We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial predictor that represents f: R^d -> R^k as a globally C^r finite-element field on a learned simplicial mesh in an optionally warped input space. Each query activates exactly one simplex and at most d+1 basis functions via barycentric coordinates, yielding explicit locality, controllable smoothness, and cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with a light invertible warp and trains end-to-end with shape regularization, semi-discrete OT coverage, and differentiable edge flips. Under standard shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic approximation tasks, tabular regression/classification, and as a drop-in head on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter budgets, improves calibration (lower ECE/Brier), and reduces inference latency due to geometric locality. These properties make SiFEN a compact, interpretable, and theoretically grounded alternative to dense MLPs and edge-spline networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.04804.pdf",
    "abs_url": "https://arxiv.org/abs/2511.04804",
    "published": "2025-11-06T20:49:13Z",
    "updated": "2026-01-08T17:07:57Z",
    "comment": "We will improve our work soon",
    "light_analysis": {
      "overview": "论文提出Simplex-FEM Networks (SiFEN)，一种基于学习网格的分段多项式函数近似器，具有显式局部性、可控平滑性和理论保证，作为MLPs和KANs的可解释替代。",
      "motivation": "本研究旨在解决传统神经网络（如密集多层感知机MLPs）在函数近似中缺乏显式局部性和理论基础的局限性。现有方法往往结构复杂、难以解释，且在计算效率和校准方面存在不足。SiFEN通过引入有限元方法和学习网格技术，提供了一个更紧凑、可解释的方案，以增强近似性能、可控平滑性和理论支撑，从而改进实际应用中的可靠性和效率。",
      "method": "SiFEN的核心方法基于学习单纯形网格上的全局C^r有限元场，使用m次Bernstein-Bezier多项式作为基函数。通过可逆扭曲输入空间和重心坐标，每个查询激活一个单纯形和最多d+1个基函数，实现稀疏计算和缓存友好性。训练采用端到端优化，结合形状正则化、半离散最优传输覆盖和可微分边翻转，以确保网格质量并维持理论近似率，这些创新点提升了方法在几何结构上的灵活性和稳定性。",
      "result": "在合成近似任务、表格回归和分类任务，以及作为紧凑CNN的drop-in头部时，SiFEN在相同参数预算下匹配或超越多层感知机MLPs和Kolmogorov-Arnold Networks KANs的性能。具体而言，SiFEN提高了校准度，如降低预期校准误差ECE和Brier分数，并因几何局部性显著减少了推理延迟。这些结果通过基准对比验证了其在实际应用中的高效性和准确性优势。",
      "conclusion": "论文的主要贡献是开发了SiFEN，一种结合有限元方法和深度学习的函数近似器，提供了理论保证和可解释性。其学术价值在于扩展了神经网络设计范式，实际应用价值在于提高计算效率、性能校准和结构紧凑性。潜在局限性可能包括对网格参数和扭曲假设的依赖性，未来工作可探索更复杂扭曲函数或扩展到高维动态问题，以进一步增强泛化能力。",
      "tags": [
        "Finite Element Method",
        "Simplicial Mesh",
        "Bernstein-Bezier Polynomials",
        "Differentiable Edge Flips",
        "Kolmogorov-Arnold Networks"
      ]
    },
    "analyzed_at": "2026-01-09T02:42:27.703531Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.24164",
    "title": "$π_0$: A Vision-Language-Action Flow Model for General Robot Control",
    "authors": [
      "Kevin Black",
      "Noah Brown",
      "Danny Driess",
      "Adnan Esmail",
      "Michael Equi",
      "Chelsea Finn",
      "Niccolo Fusai",
      "Lachy Groom",
      "Karol Hausman",
      "Brian Ichter",
      "Szymon Jakubczak",
      "Tim Jones",
      "Liyiming Ke",
      "Sergey Levine",
      "Adrian Li-Bell",
      "Mohith Mothukuri",
      "Suraj Nair",
      "Karl Pertsch",
      "Lucy Xiaoyang Shi",
      "James Tanner",
      "Quan Vuong",
      "Anna Walling",
      "Haohuan Wang",
      "Ury Zhilinsky"
    ],
    "abstract": "Robot learning holds tremendous promise to unlock the full potential of flexible, general, and dexterous robot systems, as well as to address some of the deepest questions in artificial intelligence. However, bringing robot learning to the level of generality required for effective real-world systems faces major obstacles in terms of data, generalization, and robustness. In this paper, we discuss how generalist robot policies (i.e., robot foundation models) can address these challenges, and how we can design effective generalist robot policies for complex and highly dexterous tasks. We propose a novel flow matching architecture built on top of a pre-trained vision-language model (VLM) to inherit Internet-scale semantic knowledge. We then discuss how this model can be trained on a large and diverse dataset from multiple dexterous robot platforms, including single-arm robots, dual-arm robots, and mobile manipulators. We evaluate our model in terms of its ability to perform tasks in zero shot after pre-training, follow language instructions from people and from a high-level VLM policy, and its ability to acquire new skills via fine-tuning. Our results cover a wide variety of tasks, such as laundry folding, table cleaning, and assembling boxes.",
    "categories": [
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.24164.pdf",
    "abs_url": "https://arxiv.org/abs/2410.24164",
    "published": "2024-10-31T17:22:30Z",
    "updated": "2026-01-08T17:01:05Z",
    "comment": "See project website for videos: https://physicalintelligence.company/blog/pi0 Published in RSS 2025",
    "light_analysis": {
      "overview": "论文提出了一种基于预训练视觉语言模型的流匹配架构，用于实现通用机器人控制，解决机器人学习中的数据、泛化和鲁棒性挑战。",
      "motivation": "机器人学习具有巨大潜力，能解锁灵活、通用和灵巧的机器人系统，并解决AI中的深层问题。然而，实际应用中面临数据不足、泛化能力差和鲁棒性低等障碍，现有方法难以处理复杂和高度灵巧的任务。因此，需要通过通用机器人策略（如机器人基础模型）来克服这些挑战，提高系统在实际环境中的有效性和适应性，以推动机器人技术的广泛应用。",
      "method": "研究提出了一种新颖的流匹配架构，构建在预训练的视觉语言模型（VLM）之上，以继承互联网规模的语义知识。模型在来自多个灵巧机器人平台（包括单臂、双臂和移动操作机器人）的大型多样化数据集上进行训练，关键创新在于结合VLM的语义理解能力和流匹配技术，实现从视觉和语言输入到机器人动作的有效生成和控制，从而提升模型的泛化和鲁棒性能。",
      "result": "模型在零样本任务执行、跟随人类和高层次VLM策略的语言指令，以及通过微调获取新技能等方面进行了评估，涵盖多种任务如折叠衣物、清洁桌子和组装盒子。摘要未明确说明具体性能指标数据，但表明模型能够处理这些多样化任务，展示了其在通用机器人控制中的潜在能力和适应性，与基线方法的对比情况摘要未详细描述。",
      "conclusion": "论文的主要贡献是提出了一种结合视觉语言模型的流匹配架构，用于通用机器人控制，解决了数据、泛化和鲁棒性挑战。这具有重要的学术价值，推动了机器人学习和AI基础模型的研究；实际应用价值在于能够处理现实世界中的复杂任务，如家庭服务。未来工作可进一步扩展模型到更多任务或改进其性能，但局限性如特定任务性能摘要未明确说明。",
      "tags": [
        "Vision-Language Model (VLM)",
        "Flow Matching",
        "Robot Learning",
        "General Robot Control",
        "Dexterous Robot Tasks"
      ]
    },
    "analyzed_at": "2026-01-09T02:43:17.452891Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.11151",
    "title": "Enabling Weak Client Participation via On-device Knowledge Distillation in Heterogeneous Federated Learning",
    "authors": [
      "Jihyun Lim",
      "Junhyuk Jo",
      "Tuo Zhang",
      "Sunwoo Lee"
    ],
    "abstract": "Online Knowledge Distillation (KD) is recently highlighted to train large models in Federated Learning (FL) environments. Many existing studies adopt the logit ensemble method to perform KD on the server side. However, they often assume that unlabeled data collected at the edge is centralized on the server. Moreover, the logit ensemble method personalizes local models, which can degrade the quality of soft targets, especially when data is highly non-IID. To address these critical limitations,we propose a novel on-device KD-based heterogeneous FL method. Our approach leverages a small auxiliary model to learn from labeled local data. Subsequently, a subset of clients with strong system resources transfers knowledge to a large model through on-device KD using their unlabeled data. Our extensive experiments demonstrate that our on-device KD-based heterogeneous FL method effectively utilizes the system resources of all edge devices as well as the unlabeled data, resulting in higher accuracy compared to SOTA KD-based FL methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.11151.pdf",
    "abs_url": "https://arxiv.org/abs/2503.11151",
    "published": "2025-03-14T07:40:37Z",
    "updated": "2026-01-08T16:55:29Z",
    "comment": "Accepted by ECAI 2025",
    "light_analysis": {
      "overview": "论文提出了一种基于设备上知识蒸馏的异构联邦学习方法，以解决非独立同分布数据下软目标质量下降的问题。",
      "motivation": "在联邦学习环境中，现有在线知识蒸馏方法通常假设边缘设备收集的未标记数据集中在服务器上，并使用logit集成方法进行知识蒸馏。然而，这种方法导致本地模型个性化，当数据高度非独立同分布时，会显著降低软目标的质量。这些问题限制了弱客户端参与联邦学习的能力，使得在异构资源环境中有效利用所有客户端资源和未标记数据成为一个重要研究挑战。因此，需要新的方法来应对数据分布不均衡和系统资源差异的挑战。",
      "method": "论文提出一种基于设备上知识蒸馏的异构联邦学习方法。核心方法是利用一个小型的辅助模型从本地标记数据中学习知识，然后选择系统资源较强的客户端，使用其未标记数据进行设备上知识蒸馏，将知识转移到一个大型模型中。这种方法避免了数据集中到服务器，适应了客户端的资源异构性，并提高了软目标的质量。关键创新点是在设备端执行知识蒸馏，以减少数据隐私风险并改善非独立同分布数据下的性能。摘要未明确说明具体的数据集或详细模型架构细节。",
      "result": "通过广泛的实验验证，该方法有效利用了所有边缘设备的系统资源和未标记数据。与基于知识蒸馏的现有最先进联邦学习方法相比，该方法获得了更高的准确率，表明其在处理非独立同分布数据时具有优势。实验结果显示准确率有所提升，但摘要未提供具体的性能指标数值或详细的对比数据，仅强调了整体性能改进。",
      "conclusion": "该研究的主要贡献是提出了一种设备上知识蒸馏的异构联邦学习方法，解决了现有方法在非独立同分布数据下软目标质量下降的局限性。学术价值在于拓展了联邦学习在异构环境中的应用，为弱客户端参与提供了有效途径；实际价值在于提高联邦学习系统在现实世界中的适用性和性能。摘要未明确说明潜在的局限性或未来工作方向，但可以推断该方法可能在实际部署中面临资源调度或通信开销的挑战。",
      "tags": [
        "Federated Learning",
        "Knowledge Distillation",
        "On-device Learning",
        "Heterogeneous Learning",
        "Non-IID Data"
      ]
    },
    "analyzed_at": "2026-01-09T02:44:30.587440Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05082",
    "title": "Exploring Student Expectations and Confidence in Learning Analytics",
    "authors": [
      "Hayk Asatryan",
      "Basile Tousside",
      "Janis Mohr",
      "Malte Neugebauer",
      "Hildo Bijl",
      "Paul Spiegelberg",
      "Claudia Frohn-Schauf",
      "Jörg Frochte"
    ],
    "abstract": "Learning Analytics (LA) is nowadays ubiquitous in many educational systems, providing the ability to collect and analyze student data in order to understand and optimize learning and the environments in which it occurs. On the other hand, the collection of data requires to comply with the growing demand regarding privacy legislation. In this paper, we use the Student Expectation of Learning Analytics Questionnaire (SELAQ) to analyze the expectations and confidence of students from different faculties regarding the processing of their data for Learning Analytics purposes. This allows us to identify four clusters of students through clustering algorithms: Enthusiasts, Realists, Cautious and Indifferents. This structured analysis provides valuable insights into the acceptance and criticism of Learning Analytics among students.",
    "categories": [
      "cs.LG",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05082.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05082",
    "published": "2026-01-08T16:27:09Z",
    "updated": "2026-01-08T16:27:09Z",
    "comment": "7 pages, Keywords: Learning Analytics, Survey, Data Protection, Clustering",
    "light_analysis": {
      "overview": "本文通过SELAQ问卷和聚类算法分析学生对学习分析的期望和信心，识别出四种学生态度群体，为LA接受度提供实证见解。",
      "motivation": "学习分析在教育系统中广泛应用，但数据收集需符合日益严格的隐私法规，这可能导致学生对数据处理的担忧。现有研究可能缺乏对学生期望和信心的系统性实证分析，使得教育机构难以优化LA实施策略。本文旨在填补这一空白，通过调查学生态度，促进LA的可持续发展和更高效的应用，解决隐私与优化之间的平衡问题。",
      "method": "研究方法基于Student Expectation of Learning Analytics Questionnaire (SELAQ)收集来自不同学院学生的问卷数据，应用聚类算法（摘要未明确说明具体算法类型，推断为如k-means）对数据进行分析。关键创新在于将学生期望和信心量化为特征进行聚类，从而识别出Enthusiasts、Realists、Cautious和Indifferents四种群体。技术路线涉及数据收集、预处理和聚类分析，以提供结构化结果，但摘要未明确说明数据集规模或算法参数等细节。",
      "result": "实验结果显示，通过聚类算法成功识别出四个学生群体：Enthusiasts（热情者）、Realists（现实者）、Cautious（谨慎者）和Indifferents（冷漠者），揭示了他们对学习分析的不同态度和接受程度。摘要未明确说明具体性能指标如聚类准确率或统计显著性数据，但该分析为学生接受度和批评提供了实证基础，与基线方法（如单一调查）相比，提供了更细粒度的群体划分和结构化见解。",
      "conclusion": "本研究主要贡献在于使用问卷和聚类方法系统分析了学生对学习分析的期望和信心，为LA接受度研究提供了新视角。学术价值在于丰富了教育技术领域的实证研究，实际应用上可帮助教育机构优化数据收集策略和提升学生参与度。局限性可能包括样本代表性和问卷有效性未详细探讨，未来工作可扩展研究范围或验证方法在不同教育环境中的适用性，以进一步推动LA的伦理和有效实施。",
      "tags": [
        "Learning Analytics",
        "Clustering Algorithms",
        "Questionnaire Analysis",
        "Student Data",
        "Privacy Compliance"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:06.325397Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05073",
    "title": "Milestones over Outcome: Unlocking Geometric Reasoning with Sub-Goal Verifiable Reward",
    "authors": [
      "Jianlong Chen",
      "Daocheng Fu",
      "Shengze Xu",
      "Jiawei Chen",
      "Yuan Feng",
      "Yue Yang",
      "Junchi Yan",
      "Hongyuan Zha",
      "Renqiu Xia"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) struggle with complex geometric reasoning, largely because \"black box\" outcome-based supervision fails to distinguish between lucky guesses and rigorous deduction. To address this, we introduce a paradigm shift towards subgoal-level evaluation and learning. We first construct GeoGoal, a benchmark synthesized via a rigorous formal verification data engine, which converts abstract proofs into verifiable numeric subgoals. This structure reveals a critical divergence between reasoning quality and outcome accuracy. Leveraging this, we propose the Sub-Goal Verifiable Reward (SGVR) framework, which replaces sparse signals with dense rewards based on the Skeleton Rate. Experiments demonstrate that SGVR not only enhances geometric performance (+9.7%) but also exhibits strong generalization, transferring gains to general math (+8.0%) and other general reasoning tasks (+2.8%), demonstrating broad applicability across diverse domains.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05073.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05073",
    "published": "2026-01-08T16:17:56Z",
    "updated": "2026-01-08T16:17:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出子目标可验证奖励框架，通过密集奖励机制提升多模态大语言模型的几何推理能力和泛化性。",
      "motivation": "多模态大语言模型在复杂几何推理任务中表现不足，因为传统基于结果的监督方法属于'黑箱'，无法区分推理过程是严格演绎还是幸运猜测。这导致模型推理质量难以评估和优化，限制了AI在需要严谨逻辑的几何推理领域的应用。几何推理是AI的关键挑战，现有方法侧重于最终结果准确性，忽视了子目标级别的推理评估，因此亟需引入新的学习范式来提升推理透明性和效果。",
      "method": "本研究首先构建了GeoGoal基准，该基准通过严格的形式验证数据引擎合成，将抽象几何证明转换为可验证的数字子目标，以揭示推理过程的内在结构。基于此，提出了子目标可验证奖励（SGVR）框架，用基于骨架率的密集奖励替换稀疏信号，以指导模型学习更稳健的推理。关键创新在于从子目标角度进行监督和评估，通过Skeleton Rate计算奖励，从而区分推理质量，促进模型在几何任务中的学习。",
      "result": "实验结果显示，SGVR框架显著提高了多模态大语言模型在几何推理任务上的性能，准确率提升了9.7%。此外，该框架展现出强泛化能力，在一般数学任务上准确率提升8.0%，在其他一般推理任务上提升2.8%。与基线方法相比，SGVR在多个领域均取得更好效果，证明了其在跨任务中的广泛适用性和有效性。",
      "conclusion": "本研究的主要贡献是引入了子目标级评估的学习范式，并通过SGVR框架有效改善了多模态大语言模型的几何推理准确性和跨领域泛化性。这为AI推理研究提供了新视角，增强了模型的可解释性和鲁棒性，具有重要的学术价值，并可应用于教育、自动化等实际场景。未来工作可探索该方法在其他复杂推理任务中的扩展和应用，以进一步验证其通用性和局限性。",
      "tags": [
        "Multimodal Large Language Model",
        "Geometric Reasoning",
        "Sub-Goal Evaluation",
        "Verifiable Reward",
        "Skeleton Rate"
      ]
    },
    "analyzed_at": "2026-01-09T02:45:59.734544Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05052",
    "title": "DeepWeightFlow: Re-Basined Flow Matching for Generating Neural Network Weights",
    "authors": [
      "Saumya Gupta",
      "Scott Biggs",
      "Moritz Laber",
      "Zohair Shafi",
      "Robin Walters",
      "Ayan Paul"
    ],
    "abstract": "Building efficient and effective generative models for neural network weights has been a research focus of significant interest that faces challenges posed by the high-dimensional weight spaces of modern neural networks and their symmetries. Several prior generative models are limited to generating partial neural network weights, particularly for larger models, such as ResNet and ViT. Those that do generate complete weights struggle with generation speed or require finetuning of the generated models. In this work, we present DeepWeightFlow, a Flow Matching model that operates directly in weight space to generate diverse and high-accuracy neural network weights for a variety of architectures, neural network sizes, and data modalities. The neural networks generated by DeepWeightFlow do not require fine-tuning to perform well and can scale to large networks. We apply Git Re-Basin and TransFusion for neural network canonicalization in the context of generative weight models to account for the impact of neural network permutation symmetries and to improve generation efficiency for larger model sizes. The generated networks excel at transfer learning, and ensembles of hundreds of neural networks can be generated in minutes, far exceeding the efficiency of diffusion-based methods. DeepWeightFlow models pave the way for more efficient and scalable generation of diverse sets of neural networks.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05052.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05052",
    "published": "2026-01-08T15:56:28Z",
    "updated": "2026-01-08T15:56:28Z",
    "comment": "25 pages, 20 tables, 2 figures",
    "light_analysis": {
      "overview": "本文提出DeepWeightFlow模型，通过Flow Matching技术和神经网络规范化方法，高效生成不需要微调的神经网络权重。",
      "motivation": "生成神经网络权重的现有方法面临高维权重空间和排列对称性的挑战。先前模型要么只能生成部分权重，特别是对于大型模型如ResNet和ViT，要么生成完整权重但效率低下或需要后续微调。这限制了生成多样化、高性能神经网络权重的能力，而高效生成方法对于加速模型设计和部署至关重要。",
      "method": "DeepWeightFlow采用Flow Matching模型直接操作神经网络权重空间，以生成多样化和高精度的权重。关键创新点包括应用Git Re-Basin和TransFusion进行神经网络规范化，处理排列对称性并提高大尺寸模型的生成效率。该方法支持多种架构、网络大小和数据模态，生成的网络无需微调即可表现良好。",
      "result": "实验表明，DeepWeightFlow生成的神经网络在不需要微调的情况下表现优异，并能扩展到大型网络。数百个神经网络的集成可在几分钟内生成，效率远高于基于扩散的方法。生成的网络在迁移学习方面表现良好，但摘要未明确说明具体的性能指标如准确率提升。",
      "conclusion": "该研究的主要贡献是提出DeepWeightFlow模型，实现了高效、可扩展的神经网络权重生成。其学术价值在于解决了高维权重空间生成和对称性问题，实际应用价值在于加速神经网络设计和多样化模型生成。潜在局限性或未来工作方向摘要未明确说明。",
      "tags": [
        "Flow Matching",
        "Git Re-Basin",
        "TransFusion",
        "Neural Network Canonicalization",
        "Weight Generation"
      ]
    },
    "analyzed_at": "2026-01-09T02:47:02.028986Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19946",
    "title": "Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$-Realizable MDPs",
    "authors": [
      "Antoine Moulin",
      "Gergely Neu",
      "Luca Viano"
    ],
    "abstract": "We study the problem of offline imitation learning in Markov decision processes (MDPs), where the goal is to learn a well-performing policy given a dataset of state-action pairs generated by an expert policy. Complementing a recent line of work on this topic that assumes the expert belongs to a tractable class of known policies, we approach this problem from a new angle and leverage a different type of structural assumption about the environment. Specifically, for the class of linear $Q^π$-realizable MDPs, we introduce a new algorithm called saddle-point offline imitation learning (\\SPOIL), which is guaranteed to match the performance of any expert up to an additive error $\\varepsilon$ with access to $\\mathcal{O}(\\varepsilon^{-2})$ samples. Moreover, we extend this result to possibly nonlinear $Q^π$-realizable MDPs at the cost of a worse sample complexity of order $\\mathcal{O}(\\varepsilon^{-4})$. Finally, our analysis suggests a new loss function for training critic networks from expert data in deep imitation learning. Empirical evaluations on standard benchmarks demonstrate that the neural net implementation of \\SPOIL is superior to behavior cloning and competitive with state-of-the-art algorithms.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.19946.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19946",
    "published": "2025-05-26T13:10:27Z",
    "updated": "2026-01-08T15:44:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出SPOIL算法，用于离线模仿学习，在Q^π-可实现MDPs下保证性能匹配专家，具有理论样本复杂度保证。",
      "motivation": "本研究关注离线模仿学习在马尔可夫决策过程中的问题，目标是从专家策略生成的状态-动作对数据集中学习高效策略。现有方法通常假设专家属于已知策略类，限制了实际应用范围和灵活性；本文转向环境结构假设，引入Q^π-可实现MDPs，以更好地处理环境复杂性，弥补现有方法的不足。",
      "method": "论文引入鞍点离线模仿学习（SPOIL）算法，针对线性Q^π-可实现MDPs类，通过优化策略匹配专家性能，关键创新在于利用环境结构假设；并扩展到非线性Q^π-可实现MDPs，牺牲了样本复杂度。分析还提出新的损失函数，用于训练深度模仿学习中的批评网络，提升学习效率，但具体模型架构和数据集摘要未明确说明。",
      "result": "理论分析显示，SPOIL在线性Q^π-可实现MDPs下仅需O(ε^{-2})样本匹配专家性能，误差ε，非线性下则需O(ε^{-4})；实证评估在标准基准上进行，SPOIL的神经网络实现优于行为克隆，并与最先进算法竞争，但具体准确率或效率改进数据摘要未明确说明。",
      "conclusion": "主要贡献是提出SPOIL算法及其理论保证，为离线模仿学习提供了新方法；学术价值在于扩展了Q^π-可实现MDPs下的研究，实际应用有助于从有限专家数据中高效学习策略；局限性包括样本复杂度在非线性情况下较高，未来工作可探索更广泛的环境假设或优化样本效率。",
      "tags": [
        "Offline Imitation Learning",
        "Q-Realizable MDPs",
        "SPOIL Algorithm",
        "Reinforcement Learning",
        "Loss Function Design"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:02.624465Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05033",
    "title": "A Data-Driven Predictive Framework for Inventory Optimization Using Context-Augmented Machine Learning Models",
    "authors": [
      "Anees Fatima",
      "Mohammad Abdus Salam"
    ],
    "abstract": "Demand forecasting in supply chain management (SCM) is critical for optimizing inventory, reducing waste, and improving customer satisfaction. Conventional approaches frequently neglect external influences like weather, festivities, and equipment breakdowns, resulting in inefficiencies. This research investigates the use of machine learning (ML) algorithms to improve demand prediction in retail and vending machine sectors. Four machine learning algorithms. Extreme Gradient Boosting (XGBoost), Autoregressive Integrated Moving Average (ARIMA), Facebook Prophet (Fb Prophet), and Support Vector Regression (SVR) were used to forecast inventory requirements. Ex-ternal factors like weekdays, holidays, and sales deviation indicators were methodically incorporated to enhance precision. XGBoost surpassed other models, reaching the lowest Mean Absolute Error (MAE) of 22.7 with the inclusion of external variables. ARIMAX and Fb Prophet demonstrated noteworthy enhancements, whereas SVR fell short in performance. Incorporating external factors greatly improves the precision of demand forecasting models, and XGBoost is identified as the most efficient algorithm. This study offers a strong framework for enhancing inventory management in retail and vending machine systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05033.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05033",
    "published": "2026-01-08T15:43:28Z",
    "updated": "2026-01-08T15:43:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出一个基于上下文增强机器学习模型的数据驱动预测框架，用于优化库存需求预测，通过融入外部因素如天气和节日提升精度。",
      "motivation": "供应链管理中的需求预测对优化库存、减少浪费和提升客户满意度至关重要。传统方法常忽视外部因素如天气、节日和设备故障，导致预测不准和运营低效，这在零售和自动售货机领域尤为明显。本研究旨在解决这一问题，通过引入机器学习算法来改进预测模型，应对实际应用中复杂的外部影响，以提高整体效率和响应能力。",
      "method": "本研究采用四种机器学习算法——XGBoost、ARIMA、Facebook Prophet和SVR，用于预测库存需求。核心创新点在于系统性融入外部变量，如星期、假日和销售偏差指标，以增强模型对上下文的理解。摘要未明确说明具体数据集或模型架构细节，但框架强调通过整合这些外部因素来改进传统方法，使其更适应动态环境，展示了上下文增强在机器学习中的应用。",
      "result": "实验结果显示，XGBoost在融入外部因素后表现最佳，达到最低平均绝对误差（MAE）22.7。其他模型如ARIMAX和Facebook Prophet也显示出显著改进，而SVR表现相对较差。整体上，融入外部变量如星期和节日极大提升了需求预测模型的精度，验证了上下文增强方法的有效性，为库存优化提供了具体数据支撑和与基线方法的对比优势。",
      "conclusion": "本研究的主要贡献是提出了一个强大的数据驱动预测框架，通过上下文增强机器学习模型优化库存管理。它强调了融入外部因素的重要性，并识别XGBoost为最有效算法。学术上，该研究推动了需求预测方法的创新；实践中，为零售和自动售货机系统提供了可行的解决方案。摘要未明确说明局限性，未来工作可能包括扩展至更多行业或探索其他外部变量。",
      "tags": [
        "Machine Learning",
        "Demand Forecasting",
        "Inventory Optimization",
        "XGBoost",
        "External Variables"
      ]
    },
    "analyzed_at": "2026-01-09T02:49:07.606709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05028",
    "title": "Approximate equivariance via projection-based regularisation",
    "authors": [
      "Torben Berndt",
      "Jan Stühmer"
    ],
    "abstract": "Equivariance is a powerful inductive bias in neural networks, improving generalisation and physical consistency. Recently, however, non-equivariant models have regained attention, due to their better runtime performance and imperfect symmetries that might arise in real-world applications. This has motivated the development of approximately equivariant models that strike a middle ground between respecting symmetries and fitting the data distribution. Existing approaches in this field usually apply sample-based regularisers which depend on data augmentation at training time, incurring a high sample complexity, in particular for continuous groups such as $SO(3)$. This work instead approaches approximate equivariance via a projection-based regulariser which leverages the orthogonal decomposition of linear layers into equivariant and non-equivariant components. In contrast to existing methods, this penalises non-equivariance at an operator level across the full group orbit, rather than point-wise. We present a mathematical framework for computing the non-equivariance penalty exactly and efficiently in both the spatial and spectral domain. In our experiments, our method consistently outperforms prior approximate equivariance approaches in both model performance and efficiency, achieving substantial runtime gains over sample-based regularisers.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05028.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05028",
    "published": "2026-01-08T15:35:42Z",
    "updated": "2026-01-08T15:35:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于投影的正则化方法，以实现神经网络中的近似等变性，平衡对称性与数据拟合。",
      "motivation": "等变性是神经网络中的重要归纳偏置，能提高泛化能力和物理一致性。然而，现实中对称性可能不完美，需要近似等变性模型来平衡对称性和数据拟合。现有方法如基于样本的正则化器依赖数据增强，导致高样本复杂度，特别是对于连续群如SO(3)。因此，开发更高效的方法来减少样本需求并适应实际应用中的不完全对称性成为关键挑战。",
      "method": "论文提出一种基于投影的正则化器，通过将线性层正交分解为等变和非等变组件来实现近似等变性。关键创新在于，该方法在算子级别惩罚非等变性，覆盖整个群轨道，而非点级惩罚。还提供了数学框架，用于在空间和谱域中精确高效地计算惩罚项，提升计算效率。",
      "result": "实验结果显示，该方法在模型性能和效率方面一致优于先前的近似等变方法。与基于样本的正则化器相比，实现了显著的运行时间增益，具体数字摘要未明确说明。性能指标优于基线，表明在平衡对称性和数据拟合方面更有效。",
      "conclusion": "本文的主要贡献是提出了一种基于投影的正则化方法，用于实现神经网络的近似等变性。该方法在算子和群轨道层面进行惩罚，提高了计算效率并减少了样本复杂度。学术上，为处理不完全对称性提供了新框架；实际应用中，可提升模型运行性能。未来工作可能包括扩展到其他群或进一步优化计算框架。",
      "tags": [
        "Approximate Equivariance",
        "Projection-based Regularisation",
        "Orthogonal Decomposition",
        "Neural Networks",
        "Spectral Methods"
      ]
    },
    "analyzed_at": "2026-01-09T02:50:40.215670Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2402.02005",
    "title": "Topology-Informed Graph Transformer",
    "authors": [
      "Yun Young Choi",
      "Sun Woo Park",
      "Minho Lee",
      "Youngho Woo"
    ],
    "abstract": "Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for better feature representation. TIGT outperforms previous Graph Transformers in classifying synthetic dataset aimed at distinguishing isomorphism classes of graphs. Additionally, mathematical analysis and empirical evaluations highlight our model's competitive edge over state-of-the-art Graph Transformers across various benchmark datasets.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2402.02005.pdf",
    "abs_url": "https://arxiv.org/abs/2402.02005",
    "published": "2024-02-03T03:17:44Z",
    "updated": "2026-01-08T14:42:36Z",
    "comment": "Proceedings of the Geometry-grounded Representation Learning and Generative Modeling Workshop (GRaM) at ICML 2024",
    "light_analysis": {
      "overview": "TIGT是一种新颖的图变换器，通过拓扑感知组件显著增强图同构判别力和预测性能。",
      "motivation": "图变换器在整合到GNNs时面临关键挑战，即增强区分图同构的判别力，这对于提升图表示学习和预测任务的性能至关重要。现有方法可能未能充分编码图的拓扑结构，导致在复杂图任务中性能受限，因此需要创新方法来加强这一能力。",
      "method": "TIGT包含四个核心组件：拓扑位置嵌入层使用基于循环子图的非同构通用覆盖来确保图的唯一表示；双路径消息传递层在编码器层中显式编码拓扑特征；全局注意力机制用于捕获全局依赖；图信息层重新校准通道级特征以改善表示能力。这些创新整合了图的拓扑信息，增强了模型的判别力。",
      "result": "在分类合成数据集上，TIGT超越了先前的图变换器，有效区分了图的同构类。数学分析和实证评估显示，模型在多个基准数据集上相对于最先进图变换器具有竞争优势，但具体性能指标如准确率提升在摘要中未明确说明。",
      "conclusion": "TIGT通过拓扑感知组件成功提升了图变换器的判别力和整体性能，学术价值在于为图同构问题提供了新解决方案，实际应用可扩展至图分类等领域。未来工作可能包括适应更复杂图结构或进一步优化算法。",
      "tags": [
        "Graph Transformer",
        "Graph Isomorphism",
        "Topological Embedding",
        "Dual-Path Message Passing",
        "Graph Attention"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:27.820363Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.14195",
    "title": "N-GLARE: An Non-Generative Latent Representation-Efficient LLM Safety Evaluator",
    "authors": [
      "Zheyu Lin",
      "Jirui Yang",
      "Yukui Qiu",
      "Hengqi Guo",
      "Yubing Bao",
      "Yao Guan"
    ],
    "abstract": "Evaluating the safety robustness of LLMs is critical for their deployment. However, mainstream Red Teaming methods rely on online generation and black-box output analysis. These approaches are not only costly but also suffer from feedback latency, making them unsuitable for agile diagnostics after training a new model. To address this, we propose N-GLARE (A Non-Generative, Latent Representation-Efficient LLM Safety Evaluator). N-GLARE operates entirely on the model's latent representations, bypassing the need for full text generation. It characterizes hidden layer dynamics by analyzing the APT (Angular-Probabilistic Trajectory) of latent representations and introducing the JSS (Jensen-Shannon Separability) metric. Experiments on over 40 models and 20 red teaming strategies demonstrate that the JSS metric exhibits high consistency with the safety rankings derived from Red Teaming. N-GLARE reproduces the discriminative trends of large-scale red-teaming tests at less than 1\\% of the token cost and the runtime cost, providing an efficient output-free evaluation proxy for real-time diagnostics.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.14195.pdf",
    "abs_url": "https://arxiv.org/abs/2511.14195",
    "published": "2025-11-18T07:03:58Z",
    "updated": "2026-01-08T14:19:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了N-GLARE，一个非生成式、基于潜在表示的高效LLM安全评估器，通过分析隐藏层动态来评估模型安全性。",
      "motivation": "评估大型语言模型（LLM）的安全鲁棒性对实际部署至关重要。主流Red Teaming方法依赖于在线生成和黑盒输出分析，这些方法不仅成本高昂，还因反馈延迟而不适合新模型训练后的敏捷诊断。现有方法在处理大规模模型时效率低下，亟需一种低成本、低延迟的替代方案，以支持实时安全评估和快速迭代。",
      "method": "N-GLARE完全在LLM的潜在表示上操作，无需生成完整文本，从而降低了计算开销。其核心创新在于通过分析潜在表示的APT（角概率轨迹）来表征隐藏层动态，并引入JSS（Jensen-Shannon可分离性）度量来量化模型的安全性。这种方法绕过了传统文本生成步骤，专注于内部表示的高效分析，适用于多种模型架构，摘要未明确说明具体数据集细节。",
      "result": "在超过40个模型和20种red teaming策略的实验表明，JSS度量与Red Teaming得出的安全性排名高度一致，显示出高相关性。N-GLARE以不到1%的令牌成本和运行时成本，成功复制了大规枂red-teaming测试的判别趋势，大幅提升了评估效率。与传统方法相比，它在保持准确性的同时，显著降低了资源消耗，为实际应用提供了可行方案。",
      "conclusion": "N-GLARE的主要贡献在于提供了一种高效、无输出的LLM安全评估代理，解决了现有方法的高成本和延迟问题。这项研究的学术价值在于探索了基于潜在表示的安全性分析新路径，实际应用价值在于支持实时诊断和敏捷开发流程。局限性方面，摘要未明确说明，但未来工作可能涉及扩展到更广泛的模型或更复杂的评估场景。",
      "tags": [
        "LLM Safety Evaluation",
        "Latent Representation Analysis",
        "Non-Generative Methods",
        "Jensen-Shannon Separability",
        "Red Teaming"
      ]
    },
    "analyzed_at": "2026-01-09T02:51:30.013897Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.21409",
    "title": "kooplearn: A Scikit-Learn Compatible Library of Algorithms for Evolution Operator Learning",
    "authors": [
      "Giacomo Turri",
      "Grégoire Pacreau",
      "Giacomo Meanti",
      "Timothée Devergne",
      "Daniel Ordonez",
      "Erfan Mirzaei",
      "Bruno Belucci",
      "Karim Lounici",
      "Vladimir Kostic",
      "Massimiliano Pontil",
      "Pietro Novelli"
    ],
    "abstract": "kooplearn is a machine-learning library that implements linear, kernel, and deep-learning estimators of dynamical operators and their spectral decompositions. kooplearn can model both discrete-time evolution operators (Koopman/Transfer) and continuous-time infinitesimal generators. By learning these operators, users can analyze dynamical systems via spectral methods, derive data-driven reduced-order models, and forecast future states and observables. kooplearn's interface is compliant with the scikit-learn API, facilitating its integration into existing machine learning and data science workflows. Additionally, kooplearn includes curated benchmark datasets to support experimentation, reproducibility, and the fair comparison of learning algorithms. The software is available at https://github.com/Machine-Learning-Dynamical-Systems/kooplearn.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.21409.pdf",
    "abs_url": "https://arxiv.org/abs/2512.21409",
    "published": "2025-12-24T20:15:41Z",
    "updated": "2026-01-08T14:14:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "kooplearn是一个兼容scikit-learn的机器学习库，专注于实现线性、核和深度学习算法来学习动态操作符及其谱分解，提升动态系统分析的标准化和集成性。",
      "motivation": "本研究旨在解决动态系统中操作符学习的标准化和集成问题，现有方法可能分散且不易与主流机器学习工具兼容，限制了动态系统分析、降阶建模和预测的应用。kooplearn通过提供一个统一、可扩展的框架，促进数据驱动方法的广泛采用，特别是在科学和工程领域。摘要未明确说明现有具体不足，但推断出兼容性和可访问性是关键动机。",
      "method": "kooplearn库实现了线性、核和深度学习的估计器，用于学习离散时间演化操作符（如Koopman/Transfer操作符）和连续时间生成器，并支持谱分解。核心创新在于将动态操作符学习集成到scikit-learn生态系统中，通过标准API设计简化工作流集成。库还包括精选的基准数据集，以支持算法实验、可重复性和公平比较，具体模型架构未详细说明，但涵盖多种机器学习方法。",
      "result": "摘要未明确说明具体的实验结果或性能指标，如准确率提升或效率改进。论文主要通过介绍库的功能和设计来展示其价值，包括与scikit-learn的兼容性，以及提供基准数据集以支持公平算法比较。没有提及与基线方法的对比或具体数据支撑，但强调了库在促进动态系统学习研究方面的潜力。",
      "conclusion": "kooplearn的主要贡献是开发了一个开源、scikit-learn兼容的库，用于学习动态操作符和谱分解，从而支持动态系统的分析、数据驱动降阶建模和预测。该研究提升了算法可集成性和可重复性，具有学术和实际应用价值。摘要未明确说明局限性或未来工作方向，但库的设计为扩展算法、添加更多数据集和优化性能提供了基础。",
      "tags": [
        "Dynamic Operator Learning",
        "Koopman Operator",
        "Spectral Decomposition",
        "Scikit-Learn Compatibility",
        "Machine Learning Library"
      ]
    },
    "analyzed_at": "2026-01-09T02:52:04.104321Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.05988",
    "title": "Pruning the Unsurprising: Efficient LLM Reasoning via First-Token Surprisal",
    "authors": [
      "Wenhao Zeng",
      "Yaoning Wang",
      "Chao Hu",
      "Yuling Shi",
      "Chengcheng Wan",
      "Hongyu Zhang",
      "Xiaodong Gu"
    ],
    "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable capabilities by scaling up the length of Chain-of-Thought (CoT). However, excessively long reasoning traces pose substantial challenges for training cost and inference latency. While various CoT compression approaches have emerged to address this challenge, they face inherent trade-offs: token-level methods often disrupt syntactic and logical coherence, while step-level methods based on perplexity fail to reliably capture the logically critical reasoning steps because of the dilution of logical information. In this paper, we propose ASAP (Anchor-guided, SurprisAl-based Pruning), a novel coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided pruning to preserve the core reasoning structure, which efficiently reduces the search space for subsequent processing. Leveraging the insight that logical branching choices are concentrated at the onset of reasoning steps, it then enables logic-aware pruning by selecting logically essential reasoning steps based on a novel first-token surprisal metric. Finally, ASAP distills the models to autonomously generate and leverage these concise CoTs at inference time, enabling efficient reasoning. Experiments show that ASAP achieves state-of-the-art accuracy across multiple benchmarks while substantially reducing training and inference costs.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.05988.pdf",
    "abs_url": "https://arxiv.org/abs/2508.05988",
    "published": "2025-08-08T03:46:21Z",
    "updated": "2026-01-08T14:09:36Z",
    "comment": "Code and model available at https://github.com/Zengwh02/ASAP",
    "light_analysis": {
      "overview": "本文提出ASAP框架，通过锚点引导和基于首次令牌惊喜度的剪枝方法，高效压缩思维链以降低大模型推理成本。",
      "motivation": "大型推理模型通过扩展思维链长度提升能力，但过长的推理轨迹导致训练成本和推理延迟显著增加。现有压缩方法存在固有局限：基于令牌的剪枝破坏语法和逻辑连贯性，而基于困惑度的步骤级方法因逻辑信息稀释，无法可靠识别关键推理步骤。这凸显了开发一种能保持逻辑完整性并提高效率的压缩方法的必要性，以应对大模型应用中的资源约束问题。",
      "method": "ASAP采用粗到细的框架进行思维链压缩。首先，锚点引导剪枝保留核心推理结构，缩小搜索空间。接着，利用逻辑分支选择集中在推理步骤开始的洞察，基于首次令牌惊喜度的逻辑感知剪枝选取关键步骤。最后，通过模型蒸馏使模型在推理时自主生成和利用简洁思维链。摘要未明确指定具体数据集或模型架构，但强调了coarse-to-fine的创新结合。",
      "result": "实验表明，ASAP在多个基准测试中达到最先进的准确率，同时显著降低了训练和推理成本。摘要未提供具体数值，但强调在保持高准确性的同时，有效减少了延迟和资源消耗。与基线方法相比，ASAP克服了现有压缩方法的局限性，实现了更可靠和高效的推理过程，支持其在实际部署中的潜力。",
      "conclusion": "本论文主要贡献是提出了ASAP框架，实现逻辑感知的思维链压缩，平衡了准确性与效率。其学术价值在于引入了新颖的coarse-to-fine方法，解决了现有技术的不足；实际应用价值体现在提升大型推理模型的速度和资源利用率。未来工作可能涉及扩展到更广泛模型或场景，摘要未明确说明具体局限性，但强调了方法的一般适用性。",
      "tags": [
        "Chain-of-Thought",
        "Model Pruning",
        "Surprisal Metric",
        "Model Distillation",
        "Coarse-to-Fine Framework"
      ]
    },
    "analyzed_at": "2026-01-09T02:53:15.325113Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.11266",
    "title": "Practical Aspects on Solving Differential Equations Using Deep Learning: A Primer",
    "authors": [
      "Georgios Is. Detorakis"
    ],
    "abstract": "Deep learning has become a popular tool across many scientific fields, including the study of differential equations, particularly partial differential equations. This work introduces the basic principles of deep learning and the Deep Galerkin method, which uses deep neural networks to solve differential equations. This primer aims to provide technical and practical insights into the Deep Galerkin method and its implementation. We demonstrate how to solve the one-dimensional heat equation step-by-step. We also show how to apply the Deep Galerkin method to solve systems of ordinary differential equations and integral equations, such as the Fredholm of the second kind. Additionally, we provide code snippets within the text and the complete source code on Github. The examples are designed so that one can run them on a simple computer without needing a GPU.",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2408.11266.pdf",
    "abs_url": "https://arxiv.org/abs/2408.11266",
    "published": "2024-08-21T01:34:20Z",
    "updated": "2026-01-08T13:54:27Z",
    "comment": "32 pages, 12 figures, primer (tutorial)",
    "light_analysis": {
      "overview": "本论文介绍了使用深度学习的Deep Galerkin方法解决微分方程的基本原理和实践指南。",
      "motivation": "微分方程在科学和工程中广泛应用，传统数值方法可能面临高维问题计算复杂、泛化能力不足的挑战。深度学习方法为求解微分方程提供了新途径，但现有指南缺乏实践细节。本文旨在填补这一空白，通过介绍Deep Galerkin方法，帮助研究者和实践者更好地应用深度学习技术解决实际问题，并强调其在处理偏微分方程等复杂系统中的潜力。",
      "method": "本文核心方法是Deep Galerkin方法，它利用深度神经网络近似微分方程的解，基于Galerkin原理将方程转化为优化问题。论文详细介绍了基本原理，并逐步演示如何应用于一维热方程、常微分方程组和Fredholm第二类积分方程。提供代码片段和完整的GitHub源代码，使读者能在无GPU的简单计算机上运行示例，重点在于实现细节和技术路线，而非复杂模型架构。",
      "result": "摘要未明确说明具体的实验结果或性能指标。论文主要通过示例展示了Deep Galerkin方法在求解一维热方程、常微分方程组和积分方程中的可行性，但没有提供与基线方法的对比数据，如准确率提升、计算效率改进或错误分析的具体数值。因此，本文更侧重于方法介绍和实现指南，而非量化评估，读者需通过提供的代码自行验证效果。",
      "conclusion": "本论文的主要贡献是提供了一个关于使用Deep Galerkin方法解决微分方程的入门指南，整合深度学习和数值分析技术。其学术价值在于推广深度学习方法在科学计算中的应用，促进跨学科研究；实际应用价值在于提供可访问的代码和例子，降低入门门槛。未来工作可能包括扩展到更复杂方程或优化计算效率，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Deep Learning",
        "Deep Galerkin Method",
        "Neural Networks",
        "Differential Equations",
        "Numerical Methods"
      ]
    },
    "analyzed_at": "2026-01-09T02:54:32.169781Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04941",
    "title": "Cardinality augmented loss functions",
    "authors": [
      "Miguel O'Malley"
    ],
    "abstract": "Class imbalance is a common and pernicious issue for the training of neural networks. Often, an imbalanced majority class can dominate training to skew classifier performance towards the majority outcome. To address this problem we introduce cardinality augmented loss functions, derived from cardinality-like invariants in modern mathematics literature such as magnitude and the spread. These invariants enrich the concept of cardinality by evaluating the `effective diversity' of a metric space, and as such represent a natural solution to overly homogeneous training data. In this work, we establish a methodology for applying cardinality augmented loss functions in the training of neural networks and report results on both artificially imbalanced datasets as well as a real-world imbalanced material science dataset. We observe significant performance improvement among minority classes, as well as improvement in overall performance metrics.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04941.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04941",
    "published": "2026-01-08T13:43:55Z",
    "updated": "2026-01-08T13:43:55Z",
    "comment": "12 pages, 3 figures",
    "light_analysis": {
      "overview": "该论文提出基于数学不变量（如 magnitude 和 spread）的 cardinality augmented loss functions，以解决神经网络训练中的类别不平衡问题。",
      "motivation": "类别不平衡是神经网络训练中常见且有害的问题，多数类可能主导训练，导致分类器性能偏向多数类，这严重影响模型对少数类的识别能力。现有方法在处理高度不平衡数据时可能不足，导致模型泛化能力下降。本研究旨在通过引入新损失函数来缓解这一问题，提升训练数据的多样性和分类性能。",
      "method": "研究提出 cardinality augmented loss functions，这些损失函数源自现代数学文献中的不变量，如 magnitude 和 spread，这些不变量通过评估度量空间的‘有效多样性’来丰富基数的概念。该方法将数学不变量融入损失函数设计中，以解决训练数据的过度同质化问题，并建立了在神经网络训练中应用的具体方法论。实验使用人工不平衡数据集和真实世界不平衡材料科学数据集进行验证。",
      "result": "论文在人工不平衡数据集和真实世界不平衡材料科学数据集上进行了实验，观察到少数类性能显著提升，同时整体性能指标也有改进。尽管摘要未明确提供具体数据如准确率提升百分比，但结果表明新损失函数相对于基线方法具有积极效果，尤其在处理不平衡数据时表现优异。",
      "conclusion": "该研究的主要贡献是提出 cardinality augmented loss functions，通过引入数学不变量来优化损失函数，有效应对类别不平衡问题。学术上，它扩展了损失函数的设计框架；实际上，可应用于不平衡数据集训练以提高模型性能。摘要未明确说明局限性或未来工作方向，但可能涉及进一步优化和在其他领域应用。",
      "tags": [
        "Class Imbalance",
        "Loss Functions",
        "Neural Networks",
        "Metric Space",
        "Magnitude"
      ]
    },
    "analyzed_at": "2026-01-09T02:55:07.078929Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.01722",
    "title": "When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses",
    "authors": [
      "Antoine Moulin",
      "Emmanuel Esposito",
      "Dirk van der Hoeven"
    ],
    "abstract": "We consider the problem setting of prediction with expert advice with possibly heavy-tailed losses, i.e. the only assumption on the losses is an upper bound on their second moments, denoted by $θ$. We develop adaptive algorithms that do not require any prior knowledge about the range or the second moment of the losses. Existing adaptive algorithms have what is typically considered a lower-order term in their regret guarantees. We show that this lower-order term, which is often the maximum of the losses, can actually dominate the regret bound in our setting. Specifically, we show that even with small constant $θ$, this lower-order term can scale as $\\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We propose adaptive algorithms with improved regret bounds that avoid the dependence on such a lower-order term and guarantee $\\mathcal{O}(\\sqrt{θT\\log(K)})$ regret in the worst case, and $\\mathcal{O}(θ\\log(KT)/Δ_{\\min})$ regret when the losses are sampled i.i.d. from some fixed distribution, where $Δ_{\\min}$ is the difference between the mean losses of the second best expert and the best expert. Additionally, when the loss function is the squared loss, our algorithm also guarantees improved regret bounds over prior results.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.01722.pdf",
    "abs_url": "https://arxiv.org/abs/2506.01722",
    "published": "2025-06-02T14:29:05Z",
    "updated": "2026-01-08T13:15:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出自适应专家算法，优化重尾损失下的遗憾界，避免了现有方法中低阶项主导的问题。",
      "motivation": "该研究关注预测与专家建议问题中可能出现的重尾损失场景，仅假设损失的二阶矩存在上限θ。现有自适应算法虽然在理论上适应性强，但其遗憾保证中的低阶项（如最大损失）在重尾损失下可能主导整个遗憾界，即使θ较小，也可能导致遗憾高达√(KT)，这削弱了理论保证的实际有效性。因此，研究目标是开发无需先验知识的算法，以改进遗憾界并解决这一不足。",
      "method": "论文开发了自适应算法，无需依赖损失范围或二阶矩的先验知识。核心方法是基于预测与专家建议框架，通过技术改进避免了低阶项对遗憾界的主导。关键创新点包括优化算法设计以确保在重尾损失下的鲁棒性。摘要未明确说明具体数据集或模型架构细节，但强调算法侧重于理论遗憾界的分析和自适应能力。",
      "result": "算法在最坏情况下保证遗憾为O(√(θTlog(K)))，在损失从固定分布独立同分布采样时，遗憾为O(θlog(KT)/Δ_min)，其中Δ_min是第二好与最好专家平均损失差。与现有自适应算法相比，新算法避免了低阶项主导，当损失函数为平方损失时，遗憾界也有改进。摘要提供了理论保证，但未提及具体实验数据，因此基于理论分析展示性能提升。",
      "conclusion": "主要贡献是提出了改进的自适应专家算法，解决了重尾损失下低阶项主导遗憾界的问题。学术价值在于扩展了预测与专家建议理论，提供了更稳健的遗憾保证；实际应用价值体现在处理不确定或重尾损失的实际预测场景。局限性或未来工作方面，摘要未明确说明，但可推断可能需要进一步实验验证或扩展到其他损失函数类型。",
      "tags": [
        "Prediction with Expert Advice",
        "Heavy-Tailed Losses",
        "Adaptive Algorithms",
        "Regret Analysis",
        "Second Moment Bound"
      ]
    },
    "analyzed_at": "2026-01-09T02:57:32.468774Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04907",
    "title": "Distributed Online Convex Optimization with Efficient Communication: Improved Algorithm and Lower bounds",
    "authors": [
      "Sifan Yang",
      "Wenhao Yang",
      "Wei Jiang",
      "Lijun Zhang"
    ],
    "abstract": "We investigate distributed online convex optimization with compressed communication, where $n$ learners connected by a network collaboratively minimize a sequence of global loss functions using only local information and compressed data from neighbors. Prior work has established regret bounds of $O(\\max\\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\\}n\\sqrt{T})$ and $O(\\max\\{ω^{-2}ρ^{-4}n^{1/2},ω^{-4}ρ^{-8}\\}n\\ln{T})$ for convex and strongly convex functions, respectively, where $ω\\in(0,1]$ is the compression quality factor ($ω=1$ means no compression) and $ρ<1$ is the spectral gap of the communication matrix. However, these regret bounds suffer from a \\emph{quadratic} or even \\emph{quartic} dependence on $ω^{-1}$. Moreover, the \\emph{super-linear} dependence on $n$ is also undesirable. To overcome these limitations, we propose a novel algorithm that achieves improved regret bounds of $\\tilde{O}(ω^{-1/2}ρ^{-1}n\\sqrt{T})$ and $\\tilde{O}(ω^{-1}ρ^{-2}n\\ln{T})$ for convex and strongly convex functions, respectively. The primary idea is to design a \\emph{two-level blocking update framework} incorporating two novel ingredients: an online gossip strategy and an error compensation scheme, which collaborate to \\emph{achieve a better consensus} among learners. Furthermore, we establish the first lower bounds for this problem, justifying the optimality of our results with respect to both $ω$ and $T$. Additionally, we consider the bandit feedback scenario, and extend our method with the classic gradient estimators to enhance existing regret bounds.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04907.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04907",
    "published": "2026-01-08T13:05:36Z",
    "updated": "2026-01-08T13:05:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种新的分布式在线凸优化算法，通过两层次阻塞更新框架，显著降低了遗憾界对压缩质量因子ω和节点数n的依赖，并首次建立了下界以证明最优性。",
      "motivation": "该研究旨在解决分布式在线凸优化中压缩通信带来的遗憾界高依赖问题。先前方法在处理压缩通信时，遗憾界对压缩质量因子ω^{-1}有二次或四次依赖，对节点数n有超线性依赖，导致在大规模网络中效率低下。压缩通信对分布式学习至关重要，可以减少通信开销，但现有方法的不足之处在于依赖关系不理想，限制了性能提升和应用扩展。",
      "method": "论文提出了一种基于两层次阻塞更新框架的新颖算法，包含在线八卦策略和误差补偿方案。在线八卦策略促进学习者之间的信息传播，误差补偿方案减少压缩引入的误差，两者协作实现更好的共识。此外，针对带反馈场景，方法扩展使用经典梯度估计器来增强性能，关键创新在于通过协作机制优化通信效率。摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果表明，新算法显著改进了遗憾界：对于凸函数，遗憾界从O(max{ω^{-2}ρ^{-4}n^{1/2}, ω^{-4}ρ^{-8}}n√T)降低到Õ(ω^{-1/2}ρ^{-1}n√T)；对于强凸函数，从O(·)降低到Õ(ω^{-1}ρ^{-2}n ln T)。这些改进减少了对ω^{-1}的依赖从二次或四次到线性或次线性，以及对n的依赖。同时，首次建立了下界，证明了在ω和T方面的最优性，与基线方法相比具有显著性能提升。",
      "conclusion": "本论文的主要贡献是通过两层次阻塞更新框架，提出了一种高效通信的分布式在线凸优化算法，改善了遗憾界并建立了下界。研究在理论上优化了压缩通信的依赖关系，为大规模分布式学习提供了更高效的解决方案，具有重要的学术和实际应用价值。未来工作可以扩展到其他反馈机制或更复杂的网络拓扑，摘要未明确说明局限性，但暗示了进一步的探索方向。",
      "tags": [
        "Distributed Optimization",
        "Online Convex Optimization",
        "Communication Compression",
        "Regret Bounds",
        "Gradient Estimation"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:59.923987Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03706",
    "title": "The Geometry of the Pivot: A Note on Lazy Pivoted Cholesky and Farthest Point Sampling",
    "authors": [
      "Gil Shabat"
    ],
    "abstract": "Low-rank approximations of large kernel matrices are ubiquitous in machine learning, particularly for scaling Gaussian Processes to massive datasets. The Pivoted Cholesky decomposition is a standard tool for this task, offering a computationally efficient, greedy low-rank approximation. While its algebraic properties are well-documented in numerical linear algebra, its geometric intuition within the context of kernel methods often remains obscure. In this note, we elucidate the geometric interpretation of the algorithm within the Reproducing Kernel Hilbert Space (RKHS). We demonstrate that the pivotal selection step is mathematically equivalent to Farthest Point Sampling (FPS) using the kernel metric, and that the Cholesky factor construction is an implicit Gram-Schmidt orthogonalization. We provide a concise derivation and a minimalist Python implementation to bridge the gap between theory and practice.",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03706.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03706",
    "published": "2026-01-07T08:44:03Z",
    "updated": "2026-01-08T12:49:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文揭示了Pivoted Cholesky分解在再生核希尔伯特空间中的几何解释，将其关键步骤与最远点采样和Gram-Schmidt正交化等价。",
      "motivation": "该研究旨在解决大型核矩阵低秩近似中Pivoted Cholesky分解几何直觉缺乏的问题。在机器学习中，如高斯过程扩展到大规模数据集，高效的低秩近似至关重要。现有方法在数值线性代数中代数性质已充分记载，但核方法背景下的几何解释常模糊，阻碍了理论与实践的深度结合。因此，提供几何理解有助于优化算法应用和推广。",
      "method": "论文提出在再生核希尔伯特空间中解释Pivoted Cholesky分解的几何意义。核心创新在于证明关键选择步骤在数学上等价于使用核度量的最远点采样，并且Cholesky因子构建是隐式的Gram-Schmidt正交化。通过简洁推导，作者连接了算法的数值线性代数属性与核方法的几何理解，并提供简约的Python实现促进实际应用，摘要未明确说明具体数据集或模型架构。",
      "result": "摘要未明确说明具体的实验结果或性能指标。论文主要贡献在于理论推导，证明Pivoted Cholesky分解的关键步骤与最远点采样在核度量下等价，并提供Python实现。这增强了算法的几何理解，但未提及在具体数据集上的应用效果或与基线方法的对比，因此结果侧重于理论等价性而非实证性能。",
      "conclusion": "本论文的主要贡献在于揭示了Pivoted Cholesky分解在再生核希尔伯特空间中的几何本质，将其与最远点采样和Gram-Schmidt正交化等价。这一理解提升了核方法中低秩近似的理论基础，并提供了Python实现桥接理论与实践。潜在局限包括未讨论实际应用效果，未来工作可探索更广泛的应用场景或扩展几何解释。",
      "tags": [
        "Pivoted Cholesky Decomposition",
        "Farthest Point Sampling",
        "Reproducing Kernel Hilbert Space",
        "Kernel Methods",
        "Gram-Schmidt Orthogonalization"
      ]
    },
    "analyzed_at": "2026-01-09T02:58:54.336374Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04890",
    "title": "Learnable Multipliers: Freeing the Scale of Language Model Matrix Layers",
    "authors": [
      "Maksim Velikanov",
      "Ilyas Chahed",
      "Jingwei Zuo",
      "Dhia Eddine Rhaiem",
      "Younes Belkada",
      "Hakim Hacid"
    ],
    "abstract": "Applying weight decay (WD) to matrix layers is standard practice in large-language-model pretraining. Prior work suggests that stochastic gradient noise induces a Brownian-like expansion of the weight matrices W, whose growth is counteracted by WD, leading to a WD-noise equilibrium with a certain weight norm ||W||. In this work, we view the equilibrium norm as a harmful artifact of the training procedure, and address it by introducing learnable multipliers to learn the optimal scale. First, we attach a learnable scalar multiplier to W and confirm that the WD-noise equilibrium norm is suboptimal: the learned scale adapts to data and improves performance. We then argue that individual row and column norms are similarly constrained, and free their scale by introducing learnable per-row and per-column multipliers. Our method can be viewed as a learnable, more expressive generalization of muP multipliers. It outperforms a well-tuned muP baseline, reduces the computational overhead of multiplier tuning, and surfaces practical questions such as forward-pass symmetries and the width-scaling of the learned multipliers. Finally, we validate learnable multipliers with both Adam and Muon optimizers, where it shows improvement in downstream evaluations matching the improvement of the switching from Adam to Muon.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04890.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04890",
    "published": "2026-01-08T12:41:49Z",
    "updated": "2026-01-08T12:41:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出引入可学习的乘数器来优化大语言模型矩阵层的尺度，显著提升性能并减少计算开销。",
      "motivation": "在大语言模型预训练中，权重衰减与随机梯度噪声相互作用，导致权重矩阵范数达到平衡状态，但这种平衡范数被视为有害伪影，限制了模型性能的优化。现有方法如 muP 乘数器虽能调整尺度，但仍基于固定参数，可能无法适应数据动态，导致次优结果。因此，释放矩阵层的尺度约束对于提升训练效率和下游任务表现至关重要，能解决现有训练过程中尺度固定的不足，推动模型性能的进一步提升。",
      "method": "本研究提出一种可学习乘数器方法，以优化语言模型矩阵层的尺度。核心步骤包括：首先为权重矩阵 W 附加可学习标量乘数器，验证权重衰减-噪声平衡范数的次优性；然后扩展至更细粒度控制，引入可学习每行和每列乘数器，释放个体范数约束。该方法可视为对 muP 乘数器的可学习泛化，提高了表达性。技术路线基于大语言模型矩阵层的标准训练框架，利用优化器如 Adam 和 Muon 实现，无需复杂调优，且适用于矩阵层的预训练过程，简化了尺度调整的复杂性。",
      "result": "实验结果显示，可学习乘数器方法在性能上优于经过良好调整的 muP 基线，显著减少了乘数器调优的计算开销。在 Adam 和 Muon 优化器上验证时，下游评估任务表现出改进，其效果与从 Adam 切换到 Muon 所带来的改进相匹配，证明了该方法在不同优化环境下的有效性。尽管摘要未明确说明具体准确率提升数值，但相对性能提升显著，展示了可学习尺度对模型优化的积极影响。",
      "conclusion": "本研究的主要贡献是通过引入可学习乘数器，有效释放了语言模型矩阵层的尺度约束，提升了训练性能和效率。它不仅在实验上优于现有基线，还引发了对前向传递对称性和乘数器宽度缩放等实际问题的探讨，具有重要的学术价值。实际应用中，该方法能优化大语言模型预训练过程，降低调整成本，并可能推广到其他深度学习架构。未来工作可探索其理论分析及在不同模型中的泛化能力。",
      "tags": [
        "Large Language Model",
        "Learnable Multipliers",
        "Weight Decay",
        "muP",
        "Adam Optimizer"
      ]
    },
    "analyzed_at": "2026-01-09T02:59:46.952322Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04873",
    "title": "FibreCastML: An Open Web Platform for Predicting Electrospun Nanofibre Diameter Distributions",
    "authors": [
      "Elisa Roldan",
      "Kirstie Andrews",
      "Stephen M. Richardson",
      "Reyhaneh Fatahian",
      "Glen Cooper",
      "Rasool Erfani",
      "Tasneem Sabir",
      "Neil D. Reeves"
    ],
    "abstract": "Electrospinning is a scalable technique for producing fibrous scaffolds with tunable micro- and nanoscale architectures for applications in tissue engineering, drug delivery, and wound care. While machine learning (ML) has been used to support electrospinning process optimisation, most existing approaches predict only mean fibre diameters, neglecting the full diameter distribution that governs scaffold performance. This work presents FibreCastML, an open, distribution-aware ML framework that predicts complete fibre diameter spectra from routinely reported electrospinning parameters and provides interpretable insights into process structure relationships.   A meta-dataset comprising 68538 individual fibre diameter measurements extracted from 1778 studies across 16 biomedical polymers was curated. Six standard processing parameters, namely solution concentration, applied voltage, flow rate, tip to collector distance, needle diameter, and collector rotation speed, were used to train seven ML models using nested cross validation with leave one study out external folds. Model interpretability was achieved using variable importance analysis, SHapley Additive exPlanations, correlation matrices, and three dimensional parameter maps.   Non linear models consistently outperformed linear baselines, achieving coefficients of determination above 0.91 for several widely used polymers. Solution concentration emerged as the dominant global driver of fibre diameter distributions. Experimental validation across different electrospinning systems demonstrated close agreement between predicted and measured distributions. FibreCastML enables more reproducible and data driven optimisation of electrospun scaffold architectures.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04873.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04873",
    "published": "2026-01-08T12:18:41Z",
    "updated": "2026-01-08T12:18:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "FibreCastML是一个开放的机器学习框架，能预测电纺纳米纤维的完整直径分布并提供可解释性见解。",
      "motivation": "电纺丝是一种可扩展技术，用于生产可调微纳米结构纤维支架，广泛应用于组织工程、药物输送和伤口护理。然而，现有机器学习方法通常只预测平均纤维直径，忽略了完整直径分布对支架性能的影响。这限制了优化的全面性和准确性，因此需要一种分布感知的方法来改进数据驱动的工艺优化。",
      "method": "论文提出了FibreCastML，一个分布感知的机器学习框架，关键创新在于预测完整的纤维直径频谱而非仅平均值。方法包括收集元数据集，涵盖从1778项研究中提取的68538个纤维直径测量值，涉及16种生物医用聚合物。使用六个标准处理参数（如溶液浓度、电压、流速等）训练七个机器学习模型，采用嵌套交叉验证和留一项研究外的外部折叠。模型可解释性通过变量重要性分析、SHAP值、相关矩阵和三维参数图实现。",
      "result": "实验结果显示，非线性模型始终优于线性基线，对多种聚合物实现决定系数高于0.91。溶液浓度被识别为纤维直径分布的主要全局驱动因素。通过在不同电纺丝系统中的实验验证，预测分布与测量分布高度一致，表明FibreCastML的准确性和鲁棒性。",
      "conclusion": "论文的主要贡献是开发了FibreCastML，一个开放平台，用于预测电纺纳米纤维的直径分布，促进更可重复和数据驱动的支架架构优化。这具有重要的学术价值和应用潜力，可提升组织工程等领域的设计效率。潜在未来方向可能包括扩展到更多材料或参数，但摘要未明确说明局限性。",
      "tags": [
        "Machine Learning",
        "Electrospinning",
        "Nanofibre Diameter Prediction",
        "SHAP",
        "Meta-dataset"
      ]
    },
    "analyzed_at": "2026-01-09T03:00:27.390604Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.21400",
    "title": "Breaking AR's Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
    "authors": [
      "Gen Li",
      "Changxiao Cai"
    ],
    "abstract": "Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models allow for parallel sampling, offering a promising path to accelerate generation and eliminate the left-to-right generation constraints. Despite their empirical success, theoretical understandings of diffusion language models remain underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. Crucially, our theory covers the regime $T<L$, where $L$ is the text sequence length. This justifies that high-quality samples can be generated with fewer iterations than $L$, thereby breaking the fundamental sampling bottleneck of $L$ steps required by AR models. We further establish matching upper and lower bounds, up to some constant factor, that shows the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.",
    "categories": [
      "cs.LG",
      "cs.IT",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.21400.pdf",
    "abs_url": "https://arxiv.org/abs/2505.21400",
    "published": "2025-05-27T16:24:20Z",
    "updated": "2026-01-08T11:30:06Z",
    "comment": "This is the full version of a paper published at NeurIPS 2025",
    "light_analysis": {
      "overview": "本文通过信息论分析为扩散语言模型提供了收敛保证，证明了其能用比序列长度更少的迭代打破自回归模型的采样瓶颈。",
      "motivation": "扩散模型在现代生成建模中表现出强大潜力，尤其对于大型语言模型（LLMs），它们允许并行采样以加速生成并消除从左到右的约束。然而，尽管实证上成功，扩散语言模型的理论理解仍然不足。本研究旨在填补这一空白，解决自回归模型因序列生成导致的采样效率低下问题，这个问题在自然语言处理等应用中至关重要，因为现有方法缺乏理论收敛分析来支持其有效性。",
      "method": "研究从信息论视角分析扩散语言模型的收敛性，基于Kullback-Leibler（KL）散度衡量采样误差。核心方法是证明误差与迭代次数T成反比、与目标文本序列中标记间的互信息成线性关系，关键创新在于理论覆盖了T<L（序列长度）的机制，表明可用更少迭代生成高质量样本，从而突破自回归模型瓶颈。此外，建立了匹配的上界和下界以显示收敛分析的紧致性，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "主要理论结果表明，采样误差（以KL散度度量）的衰减与迭代次数T的倒数成正比，并与标记间的互信息线性相关。具体地，研究证明了在T<L的机制下，高质量样本能以比L更少的迭代生成，直接打破自回归模型需L步采样的瓶颈。通过建立匹配的上界和下界，验证了收敛分析的紧致性，为扩散语言模型的实际有效性提供了理论支撑，但摘要未提供具体实验数据如准确率提升或效率改进的量化对比。",
      "conclusion": "论文的主要贡献是为扩散语言模型提供了新的理论见解，通过信息论分析证明其收敛性并突破自回归模型的采样瓶颈。学术价值在于深化了对扩散模型的理论理解，实际应用价值在于为加速语言生成提供了理论基础，可能应用于高效的文本生成系统。局限性或未来工作方向在摘要中未明确说明，但可推断可能需要实证验证或扩展到更复杂的模型场景。",
      "tags": [
        "Diffusion Models",
        "Language Models",
        "Information Theory",
        "Convergence Analysis",
        "Autoregressive Models"
      ]
    },
    "analyzed_at": "2026-01-09T03:01:17.445127Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.10509",
    "title": "From Actions to Words: Towards Abstractive-Textual Policy Summarization in RL",
    "authors": [
      "Sahar Admoni",
      "Assaf Hallak",
      "Yftah Ziser",
      "Omer Ben-Porat",
      "Ofra Amir"
    ],
    "abstract": "Explaining reinforcement learning agents is challenging because policies emerge from complex reward structures and neural representations that are difficult for humans to interpret. Existing approaches often rely on curated demonstrations that expose local behaviors but provide limited insight into an agent's global strategy, leaving users to infer intent from raw observations. We propose SySLLM (Synthesized Summary using Large Language Models), a framework that reframes policy interpretation as a language-generation problem. Instead of visual demonstrations, SySLLM converts spatiotemporal trajectories into structured text and prompts an LLM to generate coherent summaries describing the agent's goals, exploration style, and decision patterns. SySLLM scales to long-horizon, semantically rich environments without task-specific fine-tuning, leveraging LLM world knowledge and compositional reasoning to capture latent behavioral structure across policies. Expert evaluations show strong alignment with human analyses, and a large-scale user study found that 75.5% of participants preferred SySLLM summaries over state-of-the-art demonstration-based explanations. Together, these results position abstractive textual summarization as a paradigm for interpreting complex RL behavior.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.10509.pdf",
    "abs_url": "https://arxiv.org/abs/2503.10509",
    "published": "2025-03-13T16:10:14Z",
    "updated": "2026-01-08T11:06:58Z",
    "comment": "In Proceedings of AAMAS 2026 (The 25th International Conference on Autonomous Agents and Multi-Agent Systems)",
    "light_analysis": {
      "overview": "提出 SySLLM 框架，利用大语言模型生成抽象文本摘要来解释强化学习策略的行为。",
      "motivation": "强化学习智能体的解释具有挑战性，因为策略源于复杂的奖励结构和神经表示，难以理解。现有方法依赖于局部行为的演示，例如提供视觉演示，但只能揭示部分行为，无法有效洞察智能体的全局策略和意图，导致用户难以全面理解决策过程。因此，需要一个更有效的方法来生成连贯的策略摘要，以弥补这一不足。",
      "method": "论文提出 SySLLM 框架，将策略解释重新构建为语言生成问题。该方法将强化学习智能体的时空轨迹转换为结构化文本，然后通过提示大语言模型生成描述目标、探索风格和决策模式的抽象摘要。无需任务特定微调，利用 LLM 的世界知识和组合推理能力来捕捉策略中的潜在行为结构，适用于长时段、语义丰富的环境。",
      "result": "主要实验结果包括专家评估显示 SySLLM 生成的摘要与人类分析高度一致。大规模用户研究中，75.5% 的参与者更偏好 SySLLM 摘要，优于当前最先进的基于演示的解释方法，证明了该方法在策略解释中的有效性和用户接受度，无需依赖视觉演示即可提供更深入洞察。",
      "conclusion": "论文的主要贡献是确立抽象文本摘要作为解释复杂强化学习行为的新范式，在学术上推动了 RL 可解释性研究，应用上通过自然语言生成增强策略理解。摘要未明确说明具体局限性，但暗示未来工作可能扩展到更多环境或进一步优化 LLM 的使用，以提升泛化能力和鲁棒性。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Policy Summarization",
        "Abstractive Text Generation",
        "Trajectory Analysis"
      ]
    },
    "analyzed_at": "2026-01-09T03:02:21.794548Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04807",
    "title": "Parallelizing Node-Level Explainability in Graph Neural Networks",
    "authors": [
      "Oscar Llorente",
      "Jaime Boal",
      "Eugenio F. Sánchez-Úbeda",
      "Antonio Diaz-Cano",
      "Miguel Familiar"
    ],
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance in a wide range of tasks, such as node classification, link prediction, and graph classification, by exploiting the structural information in graph-structured data. However, in node classification, computing node-level explainability becomes extremely time-consuming as the size of the graph increases, while batching strategies often degrade explanation quality. This paper introduces a novel approach to parallelizing node-level explainability in GNNs through graph partitioning. By decomposing the graph into disjoint subgraphs, we enable parallel computation of explainability for node neighbors, significantly improving the scalability and efficiency without affecting the correctness of the results, provided sufficient memory is available. For scenarios where memory is limited, we further propose a dropout-based reconstruction mechanism that offers a controllable trade-off between memory usage and explanation fidelity. Experimental results on real-world datasets demonstrate substantial speedups, enabling scalable and transparent explainability for large-scale GNN models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04807.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04807",
    "published": "2026-01-08T10:39:48Z",
    "updated": "2026-01-08T10:39:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过图分区技术实现了图神经网络节点级可解释性的并行计算，显著提升了效率和可扩展性。",
      "motivation": "图神经网络在图结构数据任务中表现出色，但节点级可解释性计算随图规模增大而变得耗时，现有批量处理策略虽能提速却损害了解释质量。这限制了GNN模型在大规模应用中的透明度和可靠性，因为可解释性对于确保模型决策的合理性至关重要，而传统方法难以兼顾效率与准确性，因此需要一种更高效且保持解释正确性的新方法。",
      "method": "论文提出一种基于图分区的并行计算方法，通过将图分解为不相交子图，并行计算节点邻居的可解释性，从而提高效率而不影响结果正确性。关键创新在于图形分区实现并行化，减少计算时间；此外，针对内存有限场景，引入基于dropout的重建机制，提供内存使用与解释保真度之间的可控权衡，增强方法适用性。实验在真实数据集上进行，使用GNN模型验证方法。",
      "result": "在真实数据集上的实验结果表明，该方法实现了显著的提速，使大规模GNN模型的节点级可解释性计算更具可扩展性和透明度。与基线方法相比，图形分区并行化避免了批量处理导致的解释质量下降，确保了解释的正确性。摘要未明确说明具体速度提升数值，但提到提速效果明显，支持高效计算，为GNN应用提供透明解释。",
      "conclusion": "该研究提出了一种高效的并行化方法来计算图神经网络的节点级可解释性，解决了现有方法在大规模图上的效率瓶颈，核心贡献是图形分区和内存优化机制。学术价值在于扩展了GNN解释性研究的边界，实际应用价值是支持更大规模GNN模型的透明部署。潜在局限性包括内存依赖，未来工作可进一步优化资源使用或扩展到其他解释性任务。",
      "tags": [
        "Graph Neural Networks",
        "Node-Level Explainability",
        "Graph Partitioning",
        "Parallel Computing",
        "Dropout-based Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-09T03:03:09.555259Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04799",
    "title": "Neural-Symbolic Integration with Evolvable Policies",
    "authors": [
      "Marios Thoma",
      "Vassilis Vassiliades",
      "Loizos Michael"
    ],
    "abstract": "Neural-Symbolic (NeSy) Artificial Intelligence has emerged as a promising approach for combining the learning capabilities of neural networks with the interpretable reasoning of symbolic systems. However, existing NeSy frameworks typically require either predefined symbolic policies or policies that are differentiable, limiting their applicability when domain expertise is unavailable or when policies are inherently non-differentiable. We propose a framework that addresses this limitation by enabling the concurrent learning of both non-differentiable symbolic policies and neural network weights through an evolutionary process. Our approach casts NeSy systems as organisms in a population that evolve through mutations (both symbolic rule additions and neural weight changes), with fitness-based selection guiding convergence toward hidden target policies. The framework extends the NEUROLOG architecture to make symbolic policies trainable, adapts Valiant's Evolvability framework to the NeSy context, and employs Machine Coaching semantics for mutable symbolic representations. Neural networks are trained through abductive reasoning from the symbolic component, eliminating differentiability requirements. Through extensive experimentation, we demonstrate that NeSy systems starting with empty policies and random neural weights can successfully approximate hidden non-differentiable target policies, achieving median correct performance approaching 100%. This work represents a step toward enabling NeSy research in domains where the acquisition of symbolic knowledge from experts is challenging or infeasible.",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04799.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04799",
    "published": "2026-01-08T10:29:49Z",
    "updated": "2026-01-08T10:29:49Z",
    "comment": "18 pages, 12 figures, related code available at https://github.com/CYENS/evolvable-nesy",
    "light_analysis": {
      "overview": "该论文提出一个框架，通过进化过程同时学习不可微符号策略和神经网络权重，扩展神经符号AI的应用范围。",
      "motivation": "研究动机在于解决现有神经符号AI（NeSy）框架的局限性。现有方法通常需要预定义符号策略或要求策略可微，这在领域专业知识不可用或策略本身不可微时限制了应用范围，使得NeSy系统在缺乏专家知识或复杂非可微场景中难以部署。因此，需要一种新方法来自动学习和优化符号策略，以增强NeSy系统的适应性和实用性。",
      "method": "论文提出一个框架，将NeSy系统视为种群中的个体，通过进化过程同时学习不可微符号策略和神经网络权重。方法基于NEUROLOG架构扩展，使符号策略可训练，并采用Valiant的Evolvability框架和Machine Coaching语义处理可变符号表示。神经网络通过从符号组件的溯因推理进行训练，无需可微性要求。进化过程包括突变（如符号规则添加和神经网络权重变化）和基于适应度的选择，引导系统收敛到隐藏目标策略，实现联合优化。",
      "result": "通过广泛实验，论文展示从空策略和随机神经网络权重开始的NeSy系统能够成功近似隐藏的不可微目标策略，中位正确性能接近100%。这证明了框架在不需要预定义知识的情况下，有效学习和推理的能力。摘要未明确说明与基线方法的详细对比，但实验结果表明系统能实现高准确率逼近。",
      "conclusion": "该研究的主要贡献是提出了一个框架，使神经符号AI系统能通过进化过程学习不可微符号策略，无需依赖领域专业知识，扩展了NeSy研究的应用范围，特别是在符号知识获取困难的领域。这代表了向更灵活NeSy系统迈出的一步，具有学术和实际应用价值。未来工作可优化进化策略或应用于更多非可微场景。",
      "tags": [
        "Neural-Symbolic AI",
        "Evolutionary Algorithms",
        "Abductive Reasoning",
        "NEUROLOG",
        "Machine Coaching"
      ]
    },
    "analyzed_at": "2026-01-09T03:04:28.040427Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04786",
    "title": "AgentOCR: Reimagining Agent History via Optical Self-Compression",
    "authors": [
      "Lang Feng",
      "Fuchao Yang",
      "Feng Chen",
      "Xin Cheng",
      "Haiyang Xu",
      "Zhenglin Wan",
      "Ming Yan",
      "Bo An"
    ],
    "abstract": "Recent advances in large language models (LLMs) enable agentic systems trained with reinforcement learning (RL) over multi-turn interaction trajectories, but practical deployment is bottlenecked by rapidly growing textual histories that inflate token budgets and memory usage. We introduce AgentOCR, a framework that exploits the superior information density of visual tokens by representing the accumulated observation-action history as a compact rendered image. To make multi-turn rollouts scalable, AgentOCR proposes segment optical caching. By decomposing history into hashable segments and maintaining a visual cache, this mechanism eliminates redundant re-rendering. Beyond fixed rendering, AgentOCR introduces agentic self-compression, where the agent actively emits a compression rate and is trained with compression-aware reward to adaptively balance task success and token efficiency. We conduct extensive experiments on challenging agentic benchmarks, ALFWorld and search-based QA. Remarkably, results demonstrate that AgentOCR preserves over 95\\% of text-based agent performance while substantially reducing token consumption (>50\\%), yielding consistent token and memory efficiency. Our further analysis validates a 20x rendering speedup from segment optical caching and the effective strategic balancing of self-compression.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04786.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04786",
    "published": "2026-01-08T10:10:20Z",
    "updated": "2026-01-08T10:10:20Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "AgentOCR框架通过将代理历史表示为图像并引入分段光学缓存和代理自压缩，显著减少令牌消耗和内存使用。",
      "motivation": "随着大型语言模型（LLMs）的发展，基于强化学习（RL）的多轮交互代理系统面临文本历史快速增长的问题，导致令牌预算和内存使用急剧增加，这成为实际部署的主要瓶颈。现有方法依赖冗余的文本表示，效率低下，亟需创新解决方案来平衡任务性能和资源效率，以提高代理在现实环境中的可行性。",
      "method": "AgentOCR提出一种框架，利用视觉令牌的高信息密度，将累积的观察-动作历史渲染为紧凑图像。核心创新包括分段光学缓存，通过将历史分解为可哈希段并维护视觉缓存来消除冗余重渲染，以及代理自压缩，代理主动发出压缩率并通过压缩感知奖励进行训练，以自适应平衡任务成功和令牌效率。实验在ALFWorld和搜索基QA等挑战性基准上实施。",
      "result": "在ALFWorld和搜索基QA基准上的实验表明，AgentOCR保留了超过95%的基于文本代理性能，同时显著减少令牌消耗超过50%，实现一致的令牌和内存效率。进一步分析验证了分段光学缓存带来20倍渲染速度提升，以及自压缩机制有效战略平衡了性能与资源使用。",
      "conclusion": "AgentOCR的主要贡献是提出了一种创新的代理历史管理方法，通过视觉表示和自压缩机制提高效率，其学术价值在于结合视觉和压缩技术优化代理系统，实际应用价值在于增强部署可行性。未来工作可能涉及扩展基准测试或进一步优化压缩策略。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Optical Caching",
        "Self-Compression",
        "Visual Tokens"
      ]
    },
    "analyzed_at": "2026-01-09T02:28:38.480910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04761",
    "title": "Smart IoT-Based Wearable Device for Detection and Monitoring of Common Cow Diseases Using a Novel Machine Learning Technique",
    "authors": [
      "Rupsa Rani Mishra",
      "D. Chandrasekhar Rao",
      "Ajaya Kumar Tripathy"
    ],
    "abstract": "Manual observation and monitoring of individual cows for disease detection present significant challenges in large-scale farming operations, as the process is labor-intensive, time-consuming, and prone to reduced accuracy. The reliance on human observation often leads to delays in identifying symptoms, as the sheer number of animals can hinder timely attention to each cow. Consequently, the accuracy and precision of disease detection are significantly compromised, potentially affecting animal health and overall farm productivity. Furthermore, organizing and managing human resources for the manual observation and monitoring of cow health is a complex and economically demanding task. It necessitates the involvement of skilled personnel, thereby contributing to elevated farm maintenance costs and operational inefficiencies. Therefore, the development of an automated, low-cost, and reliable smart system is essential to address these challenges effectively. Although several studies have been conducted in this domain, very few have simultaneously considered the detection of multiple common diseases with high prediction accuracy. However, advancements in Internet of Things (IoT), Machine Learning (ML), and Cyber-Physical Systems have enabled the automation of cow health monitoring with enhanced accuracy and reduced operational costs. This study proposes an IoT-enabled Cyber-Physical System framework designed to monitor the daily activities and health status of cow. A novel ML algorithm is proposed for the diagnosis of common cow diseases using collected physiological and behavioral data. The algorithm is designed to predict multiple diseases by analyzing a comprehensive set of recorded physiological and behavioral features, enabling accurate and efficient health assessment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04761.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04761",
    "published": "2026-01-08T09:29:11Z",
    "updated": "2026-01-08T09:29:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出一种基于物联网的智能可穿戴设备及新型机器学习算法，用于自动检测和监测奶牛常见疾病。",
      "motivation": "在大型养殖场中，手工检测奶牛疾病面临劳动密集、耗时、准确性低等挑战，导致疾病识别延迟、动物健康受损和农场生产力下降。现有方法多依赖人工观察，资源管理复杂且成本高，且少有研究能同时高效检测多种疾病。因此，开发自动化、低成本、可靠的智能系统对改善奶牛健康监测至关重要，尤其是在大规模养殖环境中，以提升效率并减少人工依赖。",
      "method": "本研究提出一个物联网赋能的网络物理系统框架，用于监测奶牛的日常活动和健康状况。采用一种新型机器学习算法，通过分析收集的生理和行为数据来诊断多种常见疾病。算法设计用于处理综合特征集，实现准确的健康评估，关键创新在于结合物联网设备收集实时数据，并应用机器学习模型进行多疾病预测，从而提高自动化水平和诊断精度。",
      "result": "摘要未明确说明具体实验结果。基于摘要推断，本研究提出的系统旨在通过自动化监测提高疾病检测的准确性和效率，但未提供性能指标或对比基线方法的详细数据。未来的实验可能验证其在实际应用中的效果，预期能减少人工依赖，提升检测精度，但具体数据需后续研究证实。",
      "conclusion": "本研究的主要贡献是开发了一个结合物联网和机器学习的智能系统，用于奶牛疾病自动监测。学术上，它推动了智能农业和动物健康管理技术的发展。实际应用中，有助于降低农场运营成本，提高疾病早期发现率，改善动物福利和生产力。未来工作可包括优化算法、扩展疾病类型或在实际农场环境中测试，以进一步提升系统的可靠性和适用性。",
      "tags": [
        "IoT-Based Systems",
        "Machine Learning Algorithms",
        "Cyber-Physical Systems",
        "Wearable Sensors",
        "Disease Prediction"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:06.285403Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23367",
    "title": "Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2",
    "authors": [
      "Yilun Luo",
      "Huaqing Zheng",
      "Haoqian Meng",
      "Wenyuan Liu",
      "Peng Zhang"
    ],
    "abstract": "Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B are variants of the openPangu large language model, designed for efficient deployment on Ascend NPUs. The 7B variant supports three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think, while the 1B variant operates exclusively in the no_think mode, which employs condensed reasoning for higher efficiency. Although CoT reasoning enhances capability, the generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation on code generation benchmarks (HumanEval and MBPP) demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.23367.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23367",
    "published": "2025-12-29T10:50:23Z",
    "updated": "2026-01-08T09:20:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种低比特量化框架，优化openPangu模型部署，支持INT8和W4A8量化，实现高效Chain-of-Thought推理在Ascend NPU上。",
      "motivation": "研究动机源于openPangu-Embedded模型在Ascend NPU上部署时，由于Chain-of-Thought推理生成扩展轨迹，导致显著的内存和延迟开销。CoT推理虽能提升模型能力，但在实际应用中引入的计算负担限制了部署效率。现有方法在资源受限环境下表现不足，因此需要高效量化技术来缓解这些约束，确保模型在硬件上的可行运行。",
      "method": "研究方法采用后训练量化技术，将FP16计算转换为整数算术以提升效率。核心是引入一个统一的低比特推理框架，支持INT8（W8A8）和W4A8量化，专门针对openPangu-Embedded模型在Atlas A2硬件进行优化。该框架通过减少模型权重和激活的比特宽度，降低计算复杂性和内存需求，同时针对特定NPU架构实现加速。",
      "result": "实验结果在代码生成基准测试HumanEval和MBPP上验证了方法的有效性。INT8量化始终保留超过90%的FP16基线精度，并在Atlas A2上实现1.5倍的前填充速度提升。W4A8量化显著减少了内存消耗，尽管在精度上有适度权衡。与基线相比，量化方法在保持高模型保真度的同时，大幅提升了推理效率和资源利用率。",
      "conclusion": "结论表明，低比特量化能有效促进openPangu模型的高效Chain-of-Thought推理在Ascend NPU上的部署。研究贡献在于提供了一个优化框架，平衡精度和效率，具有实际应用价值。未来工作可进一步探索量化策略的优化或扩展到其他模型和硬件平台，摘要未明确说明具体方向。",
      "tags": [
        "Low-bit Quantization",
        "Chain-of-Thought Reasoning",
        "Post-Training Quantization",
        "INT8 Quantization",
        "Ascend NPU Deployment"
      ]
    },
    "analyzed_at": "2026-01-09T02:30:39.026951Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04751",
    "title": "Intraday spatiotemporal PV power prediction at national scale using satellite-based solar forecast models",
    "authors": [
      "Luca Lanzilao",
      "Angela Meyer"
    ],
    "abstract": "We present a novel framework for spatiotemporal photovoltaic (PV) power forecasting and use it to evaluate the reliability, sharpness, and overall performance of seven intraday PV power nowcasting models. The model suite includes satellite-based deep learning and optical-flow approaches and physics-based numerical weather prediction models, covering both deterministic and probabilistic formulations. Forecasts are first validated against satellite-derived surface solar irradiance (SSI). Irradiance fields are then converted into PV power using station-specific machine learning models, enabling comparison with production data from 6434 PV stations across Switzerland. To our knowledge, this is the first study to investigate spatiotemporal PV forecasting at a national scale. We additionally provide the first visualizations of how mesoscale cloud systems shape national PV production on hourly and sub-hourly timescales. Our results show that satellite-based approaches outperform the Integrated Forecast System (IFS-ENS), particularly at short lead times. Among them, SolarSTEPS and SHADECast deliver the most accurate SSI and PV power predictions, with SHADECast providing the most reliable ensemble spread. The deterministic model IrradianceNet achieves the lowest root mean square error, while probabilistic forecasts of SolarSTEPS and SHADECast provide better-calibrated uncertainty. Forecast skill generally decreases with elevation. At a national scale, satellite-based models forecast the daily total PV generation with relative errors below 10% for 82% of the days in 2019-2020, demonstrating robustness and their potential for operational use.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04751.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04751",
    "published": "2026-01-08T09:15:14Z",
    "updated": "2026-01-08T09:15:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于卫星的国家尺度光伏功率时空预测框架，首次实现了大规模PV生产评估和云系统影响可视化。",
      "motivation": "本研究的动机是解决光伏功率预测在国家尺度上的关键挑战。光伏功率的波动性对电网稳定和可再生能源整合至关重要，而现有方法通常局限于局部尺度，缺乏全面模型比较。该研究首次探索了国家尺度的时空PV预测，旨在提高预测准确性和可靠性，以支持能源系统的高效运营，克服传统数值天气预测模型在短期预测中的不足。",
      "method": "研究方法采用了一个新颖框架，整合了基于卫星的深度学习和光流技术，以及基于物理的数值天气预测模型。模型套件包括七种日内PV预测模型，覆盖确定性和概率性表述。首先使用卫星衍生的地表太阳辐照度进行验证，然后通过站点特定的机器学习模型将辐照度转换为PV功率，利用瑞士6434个PV站点的生产数据进行评估，创新在于多源数据融合和时空预测的精确映射。",
      "result": "实验结果表明，基于卫星的方法在短期预测中显著优于集成预测系统IFS-ENS。具体来说，SolarSTEPS和SHADECast在SSI和PV功率预测中表现最准确，SHADECast提供最可靠的集合分布；IrradianceNet达到最低均方根误差。在国家尺度上，82%的天数中，日总PV生成相对误差低于10%，证明模型的稳健性和操作潜力，但预测技能随海拔升高而降低。",
      "conclusion": "本研究的主要贡献在于开发了首个国家尺度的PV功率时空预测框架，并首次可视化云系统对PV生产的影响。学术上，它提供了新的方法比较基准；实际上，提高了预测的可靠性和准确性，有助于电网管理和可再生能源整合。未来工作可扩展到其他地区，并进一步优化不确定性量化，以增强模型的泛化能力。",
      "tags": [
        "Photovoltaic Forecasting",
        "Satellite-based Models",
        "Deep Learning",
        "Optical Flow",
        "Probabilistic Forecasting"
      ]
    },
    "analyzed_at": "2026-01-09T02:31:33.598725Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.06568",
    "title": "Meta-Learning Objectives for Preference Optimization",
    "authors": [
      "Carlo Alfano",
      "Silvia Sapora",
      "Jakob Nicolaus Foerster",
      "Patrick Rebeschini",
      "Yee Whye Teh"
    ],
    "abstract": "Evaluating preference optimization (PO) algorithms on LLM alignment is a challenging task that presents prohibitive costs, noise, and several variables like model size and hyper-parameters. In this work, we show that it is possible to gain insights on the efficacy of PO algorithm on simpler benchmarks. We design a diagnostic suite of MuJoCo tasks and datasets, which we use to systematically evaluate PO algorithms, establishing a more controlled and cheaper benchmark. We then propose a novel family of PO algorithms based on mirror descent, which we call Mirror Preference Optimization (MPO). Through evolutionary strategies, we search this class to discover algorithms specialized to specific properties of preference datasets, such as mixed-quality or noisy data. We demonstrate that our discovered PO algorithms outperform all known algorithms in the targeted MuJoCo settings. Finally, based on the insights gained from our MuJoCo experiments, we design a PO algorithm that significantly outperform existing baselines in an LLM alignment task.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.06568.pdf",
    "abs_url": "https://arxiv.org/abs/2411.06568",
    "published": "2024-11-10T19:11:48Z",
    "updated": "2026-01-08T09:13:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出基于镜像下降的镜像偏好优化算法家族，并通过进化策略在MuJoCo基准上优化，最终在LLM对齐任务中显著优于现有方法。",
      "motivation": "该研究旨在解决大型语言模型对齐中偏好优化算法评估的挑战，包括高昂成本、噪声干扰以及模型规模和超参数等多变量影响。现有评估方法难以系统化和可控，导致算法性能评估不准确，限制了PO算法的优化和比较。通过设计更简单的基准，可以更有效地获取PO算法的效果见解，为复杂任务中的算法改进提供基础，从而推动AI对齐技术的发展。",
      "method": "研究提出镜像偏好优化算法家族，该算法基于镜像下降原理，适用于偏好优化任务。通过进化策略，在设计的MuJoCo任务和数据集诊断套件中搜索针对特定属性（如混合质量或噪声数据）的专门算法。关键创新在于结合元学习和进化策略，在可控的模拟环境中优化PO算法，使用MuJoCo作为廉价基准来系统评估算法性能，为后续应用于LLM对齐等复杂任务提供技术支持。",
      "result": "在目标MuJoCo设置中，通过进化策略发现的PO算法在性能上优于所有已知算法。基于这些实验见解，研究人员设计了一个新的PO算法，在LLM对齐任务中显著超越现有基线。摘要未明确具体数据指标，但结果表明该方法能有效提升算法性能，为偏好优化在模拟和实际场景中的应用提供了实证支持，强调了简单基准在算法评估中的有效性。",
      "conclusion": "本研究的核心贡献是提出了镜像偏好优化算法家族和基于进化策略的优化框架，通过简单基准系统评估PO算法。这为LLM对齐等复杂任务提供了新的算法设计和评估方法，具有重要的学术价值和应用潜力，例如改进AI系统的对齐性能。未来工作可扩展至其他模拟环境或实际数据集，进一步验证算法泛化性和优化潜力，以应对更广泛的偏好优化挑战。",
      "tags": [
        "Meta-Learning",
        "Preference Optimization",
        "Mirror Descent",
        "Evolutionary Strategies",
        "MuJoCo"
      ]
    },
    "analyzed_at": "2026-01-09T02:32:35.671068Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04741",
    "title": "Fast Mining and Dynamic Time-to-Event Prediction over Multi-sensor Data Streams",
    "authors": [
      "Kota Nakamura",
      "Koki Kawabata",
      "Yasuko Matsubara",
      "Yasushi Sakurai"
    ],
    "abstract": "Given real-time sensor data streams obtained from machines, how can we continuously predict when a machine failure will occur? This work aims to continuously forecast the timing of future events by analyzing multi-sensor data streams. A key characteristic of real-world data streams is their dynamic nature, where the underlying patterns evolve over time. To address this, we present TimeCast, a dynamic prediction framework designed to adapt to these changes and provide accurate, real-time predictions of future event time. Our proposed method has the following properties: (a) Dynamic: it identifies the distinct time-evolving patterns (i.e., stages) and learns individual models for each, enabling us to make adaptive predictions based on pattern shifts. (b) Practical: it finds meaningful stages that capture time-varying interdependencies between multiple sensors and improve prediction performance; (c) Scalable: our algorithm scales linearly with the input size and enables online model updates on data streams. Extensive experiments on real datasets demonstrate that TimeCast provides higher prediction accuracy than state-of-the-art methods while finding dynamic changes in data streams with a great reduction in computational time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04741.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04741",
    "published": "2026-01-08T09:05:57Z",
    "updated": "2026-01-08T09:05:57Z",
    "comment": "Accepted by KDD 2026",
    "light_analysis": {
      "overview": "提出了TimeCast动态预测框架，用于实时预测多传感器数据流中事件的动态时间，具有自适应模式变化的高准确性和可扩展性。",
      "motivation": "本研究旨在解决从实时传感器数据流中预测机器故障发生时间的问题。在实际应用中，数据流具有动态性，底层模式随时间演化，这导致现有预测方法难以适应变化，从而影响预测准确性。实时预测机器故障对预防维护和减少停机时间至关重要，但传统方法可能忽略模式变化和传感器间的依赖关系，因此需要新方法来处理这些挑战。",
      "method": "论文提出TimeCast框架，核心方法是动态识别数据流中的时间演化模式（称为阶段），并为每个阶段学习独立的预测模型，以实现自适应预测。关键技术包括捕捉传感器间时间变化的依赖性，以提高预测性能，并设计可扩展算法，输入大小线性扩展，支持在线模型更新，从而处理实时数据流。框架基于多传感器数据流分析，无需详细说明数据集和模型架构细节，但强调实用性和动态适应性。",
      "result": "通过真实数据集的广泛实验，TimeCast展现出优于最先进方法的预测准确性。实验表明，该框架不仅能有效发现数据流中的动态变化，还大幅减少了计算时间，实现高效实时预测。与基线方法相比，TimeCast在准确性和计算效率上均有显著提升，具体数据如准确率改进未被详细提及，但结果支持其优越性能。",
      "conclusion": "TimeCast框架的主要贡献在于提出一种动态、实用且可扩展的方法，用于多传感器数据流的事件时间预测。学术价值体现在自适应模式变化和传感器依赖关系的建模，实际应用价值在于提升机器故障预测的实时性和准确性，适用于工业监控和预防维护。潜在局限性或未来工作方向在摘要中未明确说明，可能需要进一步探索更复杂场景或集成更多传感器类型。",
      "tags": [
        "Dynamic Prediction",
        "Multi-sensor Data Streams",
        "Online Learning",
        "Time-to-Event Prediction",
        "Data Mining"
      ]
    },
    "analyzed_at": "2026-01-09T02:33:25.426287Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]