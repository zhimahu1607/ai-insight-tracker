[
  {
    "id": "2601.20861",
    "title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs",
    "authors": [
      "Immanuel Abdi",
      "Akshat Gupta",
      "Micah Mok",
      "Alexander Lu",
      "Nicholas Lee",
      "Gopala Anumanchipalli"
    ],
    "abstract": "One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20861.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20861",
    "published": "2026-01-28T18:59:34Z",
    "updated": "2026-01-28T18:59:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文揭示了进化策略（ES）在大型语言模型（LLM）中引发灾难性遗忘，并通过对比更新特性解释其原因。",
      "motivation": "该研究旨在解决当前AI系统缺乏部署后持续学习能力的问题。持续学习系统的实施面临内存需求大的挑战，因为传统梯度下降算法（如GRPO）在训练大型语言模型时需要大量内存，限制了在线学习的发展。进化策略（ES）作为无梯度替代方案，虽在特定任务中表现良好，但其在持续学习中的遗忘行为未知，因此需评估其实际适用性以弥补现有方法的不足。",
      "method": "论文提出了一种综合分析方法，通过比较进化策略（ES）和梯度优化算法（GRPO），评估在增加更新步骤时模型的遗忘曲线。关键创新点包括测量性能变化和遗忘动态，并分析更新的稀疏性和ℓ2范数以探究遗忘原因。具体技术路线涉及在数学和推理任务上进行实验，使用ES和GRPO在相同计算预算下训练模型，收集性能数据并量化更新步骤的影响。",
      "result": "实验结果显示，在数学和推理任务上，进化策略（ES）在相同计算预算下能达到接近梯度优化算法（GRPO）的性能水平。然而，当更新步骤增加时，ES伴随显著的灾难性遗忘，导致先前能力大幅下降，限制了其在在线训练中的应用。与GRPO相比，ES更新更不稀疏，ℓ2范数高数个数量级，这解释了二者在遗忘曲线上的明显差异。",
      "conclusion": "该论文的主要贡献是揭示了进化策略（ES）在大型语言模型中导致灾难性遗忘的现象，并阐明其源于更新的非稀疏性和高范数。研究意义在于强调了无梯度算法在持续学习中的局限性，对AI系统自适应学习的实际应用具有警示作用。未来工作可致力于改进ES以减轻遗忘问题，例如通过优化更新机制，从而提升其在在线学习场景的适用性。",
      "tags": [
        "Evolutionary Strategies",
        "Large Language Models",
        "Catastrophic Forgetting",
        "Gradient-Free Optimization",
        "Continual Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:38.127389Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20858",
    "title": "When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation",
    "authors": [
      "David Tan",
      "Pinzhen Chen",
      "Josef van Genabith",
      "Koel Dutta Chowdhury"
    ],
    "abstract": "Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to \"uncontaminated\" languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz's FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20858.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20858",
    "published": "2026-01-28T18:56:21Z",
    "updated": "2026-01-28T18:56:21Z",
    "comment": "5 pages of content, 15 total. 5 figures, 12 tables total. Accepted to EACL 2026 main conference. Code can be found here: github.com/Mr-Ao-25/cross-ling-contamination",
    "light_analysis": {
      "overview": "本文揭示了大语言模型在机器翻译评估中的跨方向污染问题，并提出通过命名实体替换有效探测记忆的方法。",
      "motivation": "该研究旨在解决大语言模型因基准污染导致的评估失真问题。在机器翻译中，模型可能记忆训练数据而非真正泛化，这在多语言环境下尤为严重，污染可能跨语言传播，影响未见翻译方向的性能评估。现有方法难以检测这种污染，导致误导性结果，强调了开发有效探测技术的重要性，以提升评估的准确性和可靠性。",
      "method": "论文使用FLORES-200翻译基准作为诊断工具，研究两个7-8B指令调优的多语言大语言模型：Bloomz（在FLORES上训练，存在污染）和Llama（作为未污染对照组）。核心方法涉及分析跨方向污染现象，即目标侧记忆如何人为提升未见翻译方向的性能。通过实施源侧扰动实验，如改述和命名实体替换，检验记忆的持续性，以识别污染效应。",
      "result": "实验结果证实了Bloomz模型存在FLORES基准污染，导致翻译评估分数膨胀，并展示了跨方向污染显著提升未见翻译方向的性能。进一步分析表明，尽管尝试改述等扰动，记忆引用回忆仍持续；但命名实体替换导致BLEU分数一致下降，这验证了该方法作为探测记忆的有效手段，为污染模型提供了可量化的检测指标。",
      "conclusion": "论文的主要贡献在于揭示跨方向污染对机器翻译评估的误导性影响，并提出命名实体替换作为有效记忆探测方法。这项研究提高了对基准污染的认识，具有重要学术价值，有助于更准确地评估模型泛化能力。未来工作可扩展到其他污染类型或开发更全面的缓解策略，以推动评估方法的改进。",
      "tags": [
        "Large Language Model",
        "Machine Translation Evaluation",
        "Benchmark Contamination",
        "Cross-Direction Memorization",
        "Named Entity Replacement"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:19.956725Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20857",
    "title": "FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models",
    "authors": [
      "Hongyu Zhou",
      "Zisen Shao",
      "Sheng Miao",
      "Pan Wang",
      "Dongfeng Bai",
      "Bingbing Liu",
      "Yiyi Liao"
    ],
    "abstract": "Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20857.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20857",
    "published": "2026-01-28T18:56:03Z",
    "updated": "2026-01-28T18:56:03Z",
    "comment": "Our project page is at https://xdimlab.github.io/freefix",
    "light_analysis": {
      "overview": "FreeFix 是一种免微调方法，通过预训练图像扩散模型优化 3D 高斯飞溅，提升外推视图的渲染质量和泛化能力，突破现有方法的权衡限制。",
      "motivation": "论文旨在解决 3D 高斯飞溅和神经辐射场在外推视图渲染时性能下降的问题，这源于这些方法对密集输入的依赖。现有方法利用扩散模型提供监督，但面临泛化与保真度的权衡：微调扩散模型以去除伪影可能提高保真度，但容易过拟合；而免微调方法虽保持泛化，保真度较低。该问题对提升稀疏输入条件下的 3D 重建真实感和鲁棒性至关重要。",
      "method": "FreeFix 的核心是一种免微调方法，利用预训练图像扩散模型增强 3D 高斯飞溅的外推渲染。关键技术包括交错的 2D-3D 优化策略，通过图像扩散模型进行一致性优化，避免使用成本高的视频扩散模型。此外，论文提出逐像素置信掩模，用于识别渲染中的不确定区域，实现针对性改进，从而提高整体质量和多帧一致性。",
      "result": "在多个数据集上的实验表明，FreeFix 改善了多帧渲染的一致性，其性能在保真度方面与或超过基于微调的方法，同时保持了强大的泛化能力，避免了过拟合问题。尽管摘要未提供具体数值指标，但结果突出了该方法在平衡泛化和保真度方面的有效性，优于传统基线方法。",
      "conclusion": "论文的主要贡献是提出 FreeFix，一种免微调的扩散模型方法，有效突破了 3D 渲染中泛化与保真度的权衡。其学术价值在于为 3D 视图合成提供了一种高效、鲁棒的优化策略，实际应用价值体现在提升稀疏输入条件下的渲染质量。未来工作可能包括进一步优化策略或扩展到更广泛的 3D 重建场景。",
      "tags": [
        "3D Gaussian Splatting",
        "Diffusion Models",
        "Fine-Tuning-Free",
        "2D-3D Refinement",
        "Confidence Mask"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:01.204525Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20856",
    "title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models",
    "authors": [
      "Sebastiano Monti",
      "Carlo Nicolini",
      "Gianni Pellegrini",
      "Jacopo Staiano",
      "Bruno Lepri"
    ],
    "abstract": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20856.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20856",
    "published": "2026-01-28T18:56:00Z",
    "updated": "2026-01-28T18:56:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SokoBench基准以系统评估大型语言模型在长期规划中的能力，并揭示其在超过25步规划时的性能退化。",
      "motivation": "尽管大型语言模型在复杂推理任务中表现出色，但它们的长期规划能力尚未得到充分研究，这在诸如自动化和机器人等实际应用中至关重要。现有研究缺少系统性评估，无法明确识别模型在长时域规划中的缺陷，因此本工作旨在填补这一空白，通过设计新基准来深入探讨和量化这些限制。",
      "method": "本研究提出了基于Sokoban谜题的SokoBench基准，该基准经过简化以隔离长期规划问题，避免了状态持久性的干扰。核心方法包括利用Planning Domain Definition Language (PDDL) 工具进行解析、验证和求解，以辅助大型推理模型进行评估，从而分析其规划能力在复杂场景下的表现。",
      "result": "实验结果显示出规划性能的显著下降：当Sokoban谜题需要超过25步解决时，模型性能一致降低，表明前向规划能力存在基本约束。通过集成PDDL工具，性能获得适度改进，但与基线模型相比提升有限，强调了固有架构的局限性可能无法仅通过扩展测试方法解决。",
      "conclusion": "本研究的主要贡献是通过SokoBench基准揭示了大型语言模型在长期规划中的架构约束，学术上提供了新的评估工具，实际应用中对优化模型设计具有指导意义。局限性包括可能依赖特定基准，未来工作可探索更广泛的规划任务或改进模型架构以克服这些限制。",
      "tags": [
        "Large Language Models",
        "Long-Horizon Planning",
        "Sokoban Benchmark",
        "PDDL Tools",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:32.468638Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20854",
    "title": "Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation",
    "authors": [
      "Aníbal Silva",
      "Moisés Santos",
      "André Restivo",
      "Carlos Soares"
    ],
    "abstract": "Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20854.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20854",
    "published": "2026-01-28T18:54:27Z",
    "updated": "2026-01-28T18:54:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过实证研究在变分自编码器中集成Transformers的不同位置，以改进表格数据生成，揭示了保真度与多样性的权衡及其线性关系特性。",
      "motivation": "表格数据生成是生成模型的一个挑战性领域。标准变分自编码器通常基于多层感知机架构，难以有效建模特征间的关系，特别是处理混合数据类型时，这限制了生成质量和多样性。相比之下，Transformers通过注意力机制能更好地捕捉复杂特征交互，为解决这一问题提供了新思路。本研究旨在通过集成Transformers来弥补现有VAE方法的不足，提升表格数据生成的性能。",
      "method": "本论文提出了一种实证研究方法，探索将Transformers集成到变分自编码器的不同组件中，重点关注在潜在表示和decoder表示中定位Transformers的效果。研究在57个来自OpenML CC18套件的数据集上进行实验，以评估不同集成策略的影响。关键创新点在于系统分析Transformer放置位置对VAE性能的作用，技术特色包括利用Transformers的注意力机制来增强特征建模能力，但摘要未明确说明具体模型架构或训练细节。",
      "result": "实验得出两个主要结论：首先，将Transformers定位到利用潜在和decoder表示时，会导致生成数据的保真度与多样性之间出现权衡关系。其次，观察到Transformers在所有组件中的连续块之间具有高度相似性；特别地，在解码器中，Transformer的输入与输出关系近似线性。摘要未提供具体性能指标如准确率提升，但通过多数据集实验验证了这些现象，并与基线方法（标准VAE）相比，揭示了集成Transformers带来的潜在优势。",
      "conclusion": "本研究的主要贡献在于通过实证分析，阐明了在变分自编码器中集成Transformers的位置对表格数据生成的影响，提供了关于保真度-多样性权衡和线性关系的见解。学术价值在于为VAE和Transformers在生成建模中的应用提供了新视角，实际应用价值包括优化表格数据生成模型的设计。局限性包括未深入探讨具体优化策略，未来工作可探索更高效的Transformer集成方法或结合其他技术以进一步提升性能。",
      "tags": [
        "Variational Autoencoder",
        "Transformers",
        "Tabular Data Generation",
        "Attention Mechanism",
        "Generative Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:44.236410Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20852",
    "title": "C3Box: A CLIP-based Class-Incremental Learning Toolbox",
    "authors": [
      "Hao Sun",
      "Da-Wei Zhou"
    ],
    "abstract": "Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20852.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20852",
    "published": "2026-01-28T18:52:36Z",
    "updated": "2026-01-28T18:52:36Z",
    "comment": "The code is available at https://github.com/LAMDA-CL/C3Box",
    "light_analysis": {
      "overview": "本文提出C3Box，一个基于CLIP的类增量学习工具箱，通过统一多种方法解决现有研究的分散性和可复现性问题，提供一个标准化基准平台。",
      "motivation": "传统机器学习系统在动态数据流中面临灾难性遗忘挑战，类增量学习（CIL）旨在使模型持续学习新类别同时保留旧知识，以提升系统适应性。随着CLIP等预训练模型在泛化和语义对齐方面展现出潜力，其在CIL中的应用成为研究热点。然而，现有基于CLIP的CIL方法分散于不同代码库，配置不一致，严重阻碍了公平比较、实验复现和实际部署，因此亟需一个标准化工具来整合这些方法，推动领域进步。",
      "method": "C3Box是一个模块化、全面的Python工具箱，将传统CIL方法、基于视觉Transformer（ViT）的CIL方法以及先进CLIP基CIL方法整合到统一的CLIP框架中。该工具箱继承了PyCIL的流线型设计，采用基于JSON的配置系统和标准化执行管道，简化实验设置和运行流程。关键创新在于提供一致的代码环境，降低工程开销，并支持可复现实验，从而将C3Box打造成类增量学习研究的可靠基准平台，适用于多样化的任务和数据集。",
      "result": "摘要未明确说明具体实验结果，如准确率提升或效率改进等性能指标。然而，C3Box的主要成果体现在提供统一的执行环境和标准化配置，解决了现有方法配置分散的问题。通过建立可靠基准平台，它促进了不同CIL方法的公平比较和可复现性评估，降低了实验复杂度。未来研究可通过该工具快速验证方法有效性，但论文未提供与其他工具箱或方法的直接性能对比数据，因此其影响更多体现在工具实用性和社区采纳上。",
      "conclusion": "本研究的主要贡献是开发了C3Box，一个基于CLIP的类增量学习工具箱，通过整合多种方法并标准化配置，解决了领域内方法的分散性和可复现性问题。C3Box具有学术价值，为CIL研究提供了一个可靠的基准平台，促进了公平比较和实验验证，同时其开源代码和用户友好设计增强了实际应用潜力。潜在局限性在于工具箱可能需扩展以覆盖更多新兴方法，未来工作可优化框架并集成更多先进技术，以支持更广泛的研究需求。",
      "tags": [
        "CLIP",
        "Class-Incremental Learning",
        "Toolbox",
        "ViT",
        "PyCIL"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:47.855913Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20848",
    "title": "Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation",
    "authors": [
      "Weixin Chen",
      "Li Chen",
      "Yuhan Zhao"
    ],
    "abstract": "Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20848.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20848",
    "published": "2026-01-28T18:48:43Z",
    "updated": "2026-01-28T18:48:43Z",
    "comment": "Accepted to WWW 2026 Workshop on HCRS (Oral Presentation)",
    "light_analysis": {
      "overview": "Cofair是一个单训练框架，实现推荐系统中的训练后动态公平性控制，无需为不同公平性要求重新训练。",
      "motivation": "推荐系统中的公平性已成为关键研究课题，现有公平感知方法通常在训练时固定公平性要求，导致训练后灵活性不足，无法应对现实场景中多样利益相关者随时间变化的需求。为不同公平性要求频繁重新训练成本高且不切实际，现有方法缺乏动态调整能力，限制了推荐系统的实用性和适应性，因此需要一种灵活的后训练公平性控制方案来解决这一局限性。摘要未明确说明具体案例，但强调了动态公平性的重要性，以减轻系统偏见和提升用户体验。",
      "method": "Cofair框架的核心方法包括一个共享表示层，结合公平性条件适配器模块生成针对不同公平性水平的用户嵌入，以及一个用户级正则化项确保用户层面的单调公平性改进。关键创新在于通过对抗目标理论上界于人口统计均等（Demographic Parity），正则化项强制渐进公平性，实现单一训练支持多公平性条件的动态调整。该框架可应用于多种骨干模型，无需为每个新公平性要求重新训练，提高了效率和技术通用性。",
      "result": "在多个数据集和骨干模型上的实验表明，Cofair框架能提供动态公平性控制，在不同公平性水平下产生可比或优于最先进基线的公平性-准确性曲线。与基线方法相比，框架无需重新训练即可适应新公平性需求，显著提高了灵活性和性能。摘要未提供具体性能指标如准确率提升数字，但通过定性对比强调了其有效性，支持推荐系统中高效公平性调整的实际应用。",
      "conclusion": "该论文的主要贡献是提出了Cofair框架，实现推荐系统中训练后的动态公平性控制，学术上扩展了公平性方法，提供灵活调整机制，实际应用中减少重新训练开销，适应多变需求。潜在局限性包括对特定公平性度量的依赖，摘要未明确说明未来工作方向，但可能涉及扩展到更多公平性定义或更复杂场景，进一步提升系统普适性和用户体验。",
      "tags": [
        "Recommendation Systems",
        "Dynamic Fairness",
        "Adapter Modules",
        "Regularization",
        "Demographic Parity"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:44.999290Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20847",
    "title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion",
    "authors": [
      "Willams de Lima Costa",
      "Thifany Ketuli Silva de Souza",
      "Jonas Ferreira Silva",
      "Carlos Gabriel Bezerra Pereira",
      "Bruno Reis Vila Nova",
      "Leonardo Silvino Brito",
      "Rafael Raider Leoni",
      "Juliano Silva",
      "Valter Ferreira",
      "Sibele Miguel Soares Neto",
      "Samantha Uehara",
      "Daniel Giacomo",
      "João Marcelo Teixeira",
      "Veronica Teichrieb",
      "Cristiano Coelho de Araújo"
    ],
    "abstract": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20847.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20847",
    "published": "2026-01-28T18:46:29Z",
    "updated": "2026-01-28T18:46:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种融合相机和IMU传感器的多模态框架及ROAD数据集，显著增强了道路表面分类的鲁棒性和泛化能力。",
      "motivation": "道路表面分类是环境感知预测维护系统的关键技术，但现有方法由于传感模态有限和数据集缺乏环境多样性（如不同光照、天气和表面条件），在广泛操作条件下泛化能力差。这个问题的重要性在于现实环境变化巨大，现有技术无法可靠适应这些变化，导致系统在实际应用中性能下降。摘要指出，当前的基准数据集缺乏可变性，限制了方法的泛化评估。",
      "method": "论文提出了一种多模态框架，通过结合廉价相机和IMU传感器进行数据融合。核心技术包括一个轻量级的双向跨注意力模块，用于高效融合图像和惯性测量，以及一个自适应门控层，以在领域漂移下动态调整模态贡献。针对现有数据集的不足，引入了ROAD数据集，包含三个子集：真实世界多模态记录（同步RGB-IMU数据）、纯视觉子集（用于评估挑战性照明条件）和合成子集（用于研究分布外泛化），以全面测试鲁棒性。",
      "result": "实验结果显示，该方法在PVS基准测试中比先前最优方法提升了1.4个百分点，在ROAD多模态子集上提升了11.6个百分点。框架在少数类上表现出更高的F1分数，并在挑战性视觉条件如夜间、大雨和混合表面转换中保持了稳定的性能。与基线方法相比，这些改进验证了其鲁棒性和泛化优势。摘要未明确说明具体效率数据，但突出了准确性提升和条件适应性。",
      "conclusion": "本研究的主要贡献在于提出了一种结合廉价相机和IMU传感器与多模态注意力机制的框架和综合数据集，为道路表面理解提供了可扩展且鲁棒的基础。特别适用于环境多变和成本受限的地区，具有重要的实际应用价值。学术上，它推动了多模态融合和鲁棒分类技术的发展。摘要未明确说明局限性或未来方向，但暗示了数据集的扩展和应用场景的拓展潜力。",
      "tags": [
        "Road Surface Classification",
        "Multimodal Fusion",
        "Camera-IMU Sensor Fusion",
        "Attention Mechanisms",
        "Dataset Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:40.583728Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20845",
    "title": "PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting",
    "authors": [
      "Olaf Yunus Laitinen Imanov",
      "Derya Umut Kulali",
      "Taner Yilmaz"
    ],
    "abstract": "Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20845.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20845",
    "published": "2026-01-28T18:45:45Z",
    "updated": "2026-01-28T18:45:45Z",
    "comment": "5 pages; 2 figures; 7 tables",
    "light_analysis": {
      "overview": "本文提出PatchFormer模型，通过分层掩码重建自监督预训练和轻量级适配器迁移学习，实现了零样本时间序列多步预测的显著性能提升。",
      "motivation": "时间序列预测在气候、能源、医疗保健和金融等多个领域有广泛应用，但现有方法通常依赖领域专家进行特征工程，并需要为每个任务准备大量标记数据，这限制了模型的通用性和部署效率。为了解决这一问题，本研究旨在开发一个基础模型，以减少对特定领域知识和标注数据的依赖，实现更高效、准确的跨领域预测。",
      "method": "PatchFormer采用基于补丁的方法分割时间序列，学习多尺度时间表示，并通过分层结构聚合不同时间尺度的信息。预训练阶段使用掩码重建策略，结合动态掩码和优化目标，以同时提高局部准确性和全局一致性；随后应用跨领域知识蒸馏增强泛化能力。迁移学习时，使用轻量级适配器进行任务适应，减少了模型调整的计算开销。",
      "result": "实验在24个基准数据集（涵盖天气、能源、交通、金融和医疗保健领域）上进行，PatchFormer在零样本多步预测中达到最先进水平，平均平方误差相对于基线降低27.3%，同时任务特定训练数据需求减少94%。模型处理长度512序列的速度是全序列Transformer的3.8倍，并在预训练数据量达到1000亿点时表现出对数线性扩展。",
      "conclusion": "PatchFormer的主要贡献在于结合分层掩码重建和跨领域迁移学习，为时间序列预测提供了一种高效的基础模型方法，显著减少了领域特定工程和数据需求。其学术价值在于推动了自监督学习在时间序列领域的应用，实际应用潜力体现在多领域预测任务中提高准确性和效率。未来工作可探索更广泛的领域适应性和计算优化，以进一步扩展模型应用范围。",
      "tags": [
        "Time Series Foundation Model",
        "Hierarchical Masked Reconstruction",
        "Cross-Domain Transfer Learning",
        "Zero-Shot Forecasting",
        "Lightweight Adapters"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:31.953255Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20844",
    "title": "$\\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval",
    "authors": [
      "Zihao Wang",
      "Hang Yin",
      "Lihui Liu",
      "Hanghang Tong",
      "Yangqiu Song",
      "Ginny Wong",
      "Simon See"
    ],
    "abstract": "This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of \"distances\" or \"similarities,\" including the $\\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20844.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20844",
    "published": "2026-01-28T18:45:43Z",
    "updated": "2026-01-28T18:45:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文理论推导了最小可嵌入维度的紧界限，揭示基于嵌入的检索限制主要源于可学习性挑战而非几何约束。",
      "motivation": "研究动机在于探索嵌入子集成员到向量空间所需的最小维度（MED），以优化基于嵌入的top-k检索效率。现有方法可能忽视理论限制，导致计算资源浪费或维度选择不当。通过分析MED，可以区分几何约束和可学习性因素，为改进嵌入算法提供理论基础，解决检索系统中的效率和性能问题。",
      "method": "研究方法包括理论推导和数值模拟。理论部分使用数学分析推导MED的紧界限，覆盖多种距离度量如ℓ2距离、内积和余弦相似性。模拟部分在更可实现的情境中实施，假设子集嵌入为所含元素嵌入的质心，以验证MED与元素数量的对数依赖关系，确保方法具有实际可行性。",
      "result": "主要实验结果包括理论推导的MED紧界限，并通过数值模拟支持。模拟中，在质心嵌入假设下，MED与元素数量表现出对数依赖关系，表明在实际应用中嵌入维度可低于预期。这一发现验证了理论界限，揭示了检索限制更多来自可学习性挑战，而非空间几何约束。",
      "conclusion": "结论指出，研究揭示了基于嵌入的检索限制主要源于可学习性问题，而非几何约束，为未来算法设计提供重要指导。学术上，它建立了理论框架用于优化嵌入维度；应用上，有助于开发高效检索系统。潜在局限性包括模拟假设的简化，未来工作可扩展至更复杂场景或实际数据集。",
      "tags": [
        "Embedding-based Retrieval",
        "Set Membership Embedding",
        "Distance Metrics",
        "Theoretical Bounds",
        "Numerical Simulation"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:49.967867Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20843",
    "title": "Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)",
    "authors": [
      "Saurav Prateek"
    ],
    "abstract": "This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20843.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20843",
    "published": "2026-01-28T18:45:39Z",
    "updated": "2026-01-28T18:45:39Z",
    "comment": "11 pages, 6 figures, 2 tables, source code: https://github.com/SauravP97/deep-researcher-reflect-evolve/",
    "light_analysis": {
      "overview": "论文提出了一种结合顺序研究计划反思和候选交叉算法的Deep Researcher架构，以生成高质量的复杂主题研究报告。",
      "motivation": "研究动机是解决现有并行扩展范式在生成研究报告时的固有局限性，如知识孤岛问题，这导致效率和知识整合性不足。生成复杂博士水平主题的详细研究报告需要动态自适应能力，但现有方法（如并行自一致性范式）可能缺乏全局上下文维护，影响报告质量和效率，因此设计更智能的架构至关重要。",
      "method": "方法包括Sequential Research Plan Refinement via Reflection，允许代理在运行时维护集中化的Global Research Context，以回顾进展、推理计划并动态调整；Candidates Crossover算法部署多个LLM候选（基于Gemini 2.5 Pro模型）以不同参数探索大搜索空间，然后合成结果；最后通过One Shot Report Generation生成统一叙事的最终报告，关键创新在于顺序反思和候选交叉的协同作用。",
      "result": "在DeepResearch Bench（一个包含100个博士级研究任务的全球基准）上评估，Deep Researcher架构获得总体得分46.21，显著超越了Claude Researcher、Nvidia AIQ Research Assistant、Perplexity Research、Kimi Researcher和Grok Deeper Search等领先研究代理，且略优于先前工作Static DRA，验证了顺序扩展范式比并行自一致性范式具有更优性能。",
      "conclusion": "论文的主要贡献是证明了Sequential Scaling范式在AI研究代理中的优越性，通过创新方法提升了研究报告生成的准确性和效率，具有学术价值在于推动AI辅助研究技术，实际应用可用于自动化学术写作；未来工作可探索更多自适应机制或扩展到其他领域，局限性摘要未明确说明。",
      "tags": [
        "Sequential Planning",
        "LLM Candidates Crossover",
        "Reflection in AI",
        "Research Report Generation",
        "Gemini 2.5 Pro"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:24.249895Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20838",
    "title": "Reward Models Inherit Value Biases from Pretraining",
    "authors": [
      "Brian Christian",
      "Jessica A. F. Thompson",
      "Elle Michelle Yang",
      "Vincent Adam",
      "Hannah Rose Kirk",
      "Christopher Summerfield",
      "Tsvetomira Dumbalska"
    ],
    "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20838.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20838",
    "published": "2026-01-28T18:40:29Z",
    "updated": "2026-01-28T18:40:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示了奖励模型从预训练大型语言模型中继承价值偏见，并强调了预训练阶段对齐的关键作用。",
      "motivation": "研究动机聚焦于奖励模型如何受其基础预训练语言模型影响，这一问题在使大型语言模型与人类价值观对齐中至关重要。由于奖励模型在强化学习和AI安全中扮演核心角色，但现有研究主要关注预训练和后训练的语言模型本身，忽视了奖励模型可能从初始化过程中继承的偏见。这导致了对AI系统偏见来源的理解不足，可能影响伦理对齐和安全应用。通过探索偏见性质和程度，本研究旨在填补这一空白，为改进对齐方法提供理论基础。",
      "method": "研究方法包括对10个领先开源权重奖励模型进行实证分析，使用验证的心理语言学语料库评估‘Big Two’心理学维度（机构性和社群性）。关键创新点在于追溯到指令微调和预训练模型的对数概率差异，并将其公式化为隐式奖励模型以推导可用的奖励分数。此外，通过消融实验控制偏好数据源和数量来训练奖励模型，以验证偏见的来源和持久性。技术特色是结合心理学理论和机器学习方法，分析模型架构（如基于Llama和Gemma的奖励模型）对价值偏见的影响，而不依赖额外的训练数据。",
      "result": "实验结果显示，基于Llama的奖励模型显著偏好‘机构性’，而基于Gemma的则偏好‘社群性’，即使使用相同的偏好数据和微调过程，这种差异依然存在。对数概率差异可转化为隐式奖励分数，并展现出相同的偏见模式。消融实验进一步证实偏见具有高重复性和持久性，表明预训练表示对奖励模型输出的影响远超出预期，与基线假设（即奖励模型完全代表人类偏好）形成对比。尽管摘要未提供具体数值，但描述强调了偏见的显著性和稳定性。",
      "conclusion": "论文主要贡献是揭示了奖励模型继承预训练语言模型价值偏见的机制，这强调了在AI开发中预训练阶段安全和对齐努力的重要性。该研究具有实际应用价值，为开源开发者提供指导，表明选择基础模型时需综合考虑价值观和性能，而不仅仅是技术指标。学术上，它扩展了对AI系统偏见来源的理解，并提出未来工作可探索更多价值维度或开发缓解偏见的方法，尽管摘要未明确说明局限性，但暗示了进一步研究以增强对齐鲁棒性的需求。",
      "tags": [
        "Reward Models",
        "Large Language Models",
        "Pretraining",
        "Value Alignment",
        "Psychometrics"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:44.373938Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20835",
    "title": "Open-Vocabulary Functional 3D Human-Scene Interaction Generation",
    "authors": [
      "Jie Liu",
      "Yu Sun",
      "Alpar Cseke",
      "Yao Feng",
      "Nicolas Heron",
      "Michael J. Black",
      "Yan Zhang"
    ],
    "abstract": "Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as \"sitting on a sofa'', while supporting fine-grained functional human-scene interactions, e.g., \"increasing the room temperature''. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20835.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20835",
    "published": "2026-01-28T18:34:25Z",
    "updated": "2026-01-28T18:34:25Z",
    "comment": "18 pages",
    "light_analysis": {
      "overview": "FunHSI框架通过功能感知推理和视觉语言模型，实现无需训练的开放词汇任务驱动功能性3D人-场景交互生成。",
      "motivation": "该研究旨在解决3D人类与场景功能性交互生成的开放问题，应用于具身AI、机器人学和交互内容创建。现有方法常缺乏对对象功能和人-场景接触的显式推理，导致生成交互不真实或功能错误，影响实际应用效果，如虚拟环境中的真实模拟和机器人任务规划。因此，研究需引入功能感知机制以提升交互合理性和正确性，解决现有方法的不足。",
      "method": "FunHSI框架首先通过功能感知接触推理识别场景中的功能元素，重建其3D几何并利用接触图建模高级交互。接着，使用视觉语言模型从开放词汇任务提示合成执行任务的图像，并估计提议的3D身体和手部姿态。最后，通过阶段优化细化身体配置，确保物理合理性和功能正确性。关键创新包括无需训练的设计、功能驱动的交互生成，以及结合视觉语言模型的多模态推理。",
      "result": "实验结果表明，FunHSI在多样化室内外场景中能一致生成功能正确且物理合理的人-场景交互。与现有方法相比，它不仅支持一般交互如“坐在沙发上”，还实现细粒度功能交互如“增加室温”。摘要未明确说明具体性能指标如准确率，但强调其在实际应用中的有效性，展示了在功能正确性和物理合理性方面的改进。",
      "conclusion": "该研究的主要贡献是提出了FunHSI框架，通过功能感知推理和视觉语言模型实现开放词汇任务驱动的功能性交互生成。这提升了交互的真实性和功能性，具有重要学术价值和实际应用意义，尤其在具身AI、机器人学和虚拟内容创建领域。未来工作可扩展至更复杂交互类型和场景，或探索更强的泛化能力和实时优化。",
      "tags": [
        "Open-Vocabulary",
        "Functionality Reasoning",
        "Vision-Language Models",
        "3D Human-Scene Interaction",
        "Contact Graph Modeling"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:48.947690Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20834",
    "title": "Linear representations in language models can change dramatically over a conversation",
    "authors": [
      "Andrew Kyle Lampinen",
      "Yuxuan Li",
      "Eghbal Hosseini",
      "Sangnie Bhardwaj",
      "Murray Shanahan"
    ],
    "abstract": "Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20834.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20834",
    "published": "2026-01-28T18:33:17Z",
    "updated": "2026-01-28T18:33:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究发现语言模型中的线性表示在对话过程中会剧烈变化，揭示了模型适应上下文的动态机制，挑战了传统静态表示假设。",
      "motivation": "语言模型的内部表示常包含对应高层次概念的线性方向，但现有研究多假设这些表示是静态的，忽略了在动态对话场景中的演化。这可能导致可解释性工具和引导方法在实际应用中出现偏差，因为对话上下文可能显著改变表示的性质。本研究旨在探究表示在对话中的动态变化，以理解模型如何响应上下文，并评估对现有方法的潜在影响。摘要未明确说明所有动机，但基于内容推断该问题对改进模型可解释性和控制至关重要。",
      "method": "论文通过模拟对话实验，分析语言模型表示沿线性维度的动态演化。核心方法包括研究表示在对话开始和结束时的变化，测试内容依赖性（如对话相关与通用信息），并验证稳健性：在不同模型家族和层中观察变化，以及使用回放对话脚本（无需策略对话）和对比科幻故事上下文。关键创新是关注表示演化的时序性和上下文敏感性，未提及具体数据集或模型架构，摘要未明确说明。",
      "result": "主要实验结果显示，线性表示在对话中剧烈变化：例如，事实性信息可变为非事实性，反之亦然。变化具有内容依赖性，对话相关信息易变而通用信息稳定；这些现象在能区分事实性的维度上依然稳健，且跨模型和层一致。表示变化无需实时对话，回放脚本也能触发类似变化，但上下文明确标记为科幻时适应较弱。引导实验表明，沿表示方向进行控制的效果随对话点不同而异，凸显了动态适应性。摘要未提供具体数值数据，但描述了定性对比结果。",
      "conclusion": "论文结论指出，语言模型表示在对话中的动态变化挑战了静态解释方法，可能导致误导性可解释性工具。主要贡献是揭示了表示演化的机制，强调考虑上下文适应性的重要性，这推动了对模型内部机制的理解。学术价值在于为可解释性和引导研究提供新视角，实际应用可能促进动态适应工具的开发。未来工作可进一步探究表示变化的原因，并设计更稳健的模型控制方法。摘要提及潜在局限性如静态假设的不足，并指向新研究方向。",
      "tags": [
        "Linear Representations",
        "Language Models",
        "Conversational Dynamics",
        "Interpretability",
        "Context Adaptation"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:53.485047Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20831",
    "title": "MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents",
    "authors": [
      "Vishnu Sashank Dorbala",
      "Dinesh Manocha"
    ],
    "abstract": "Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20831.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20831",
    "published": "2026-01-28T18:31:17Z",
    "updated": "2026-01-28T18:31:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MemCtrl框架，利用多模态大语言模型作为具身代理的在线内存控制器，通过可训练内存头动态管理内存，提升任务完成效率。",
      "motivation": "具身代理在严格内存和计算约束下在线操作时，现有内存系统如RAG常将内存视为离线存储空间，这导致效率低下，不适合动态环境。Foundation models依赖上下文学习，但上下文窗口有限，无法有效处理长序列决策，因此需要在线内存管理方法来优化资源使用，增强代理的实时决策能力。",
      "method": "MemCtrl框架采用多模态大语言模型作为基础，引入一个可训练的内存头μ，作为门机制在探索过程中决定保留、更新或丢弃观察和反思。μ通过两种方式训练：一是基于离线专家策略，二是通过在线强化学习，以实现自适应内存剪枝。在EmbodiedBench基准数据集上应用，增强低性能MLLMs，测试在线内存管理效果。",
      "result": "实验显示，在EmbodiedBench基准的多个子集上，μ增强的MLLMs相比原始模型，整体任务完成能力平均提升约16%，特定指令子集提升超过20%。与基线方法对比，显著改善性能，定性分析还表明在处理长且复杂的指令类型时表现更优，验证了内存管理的有效性。",
      "conclusion": "MemCtrl框架成功解决了具身代理在线内存管理的挑战，通过MLLMs增强和可训练内存头，实现高效内存优化，提升了任务完成率。研究推动了多模态语言模型在机器人学中的应用，并为资源受限环境下的智能系统设计提供新方法，未来可探索μ的泛化能力和更广泛的应用场景。",
      "tags": [
        "Multimodal Large Language Models",
        "Reinforcement Learning",
        "Memory Pruning",
        "Embodied Agents",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:53.756328Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20829",
    "title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning",
    "authors": [
      "Minwu Kim",
      "Safal Shrestha",
      "Keith Ross"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20829.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20829",
    "published": "2026-01-28T18:29:21Z",
    "updated": "2026-01-28T18:29:21Z",
    "comment": "16 pages",
    "light_analysis": {
      "overview": "提出 failure-prefix conditioning 方法，通过基于失败前缀的训练，有效提升大型语言模型在饱和问题上的推理能力。",
      "motivation": "当前基于可验证奖励的强化学习（RLVR）在训练大型语言模型的推理能力时，当问题变得饱和后，训练常停滞不前。这是因为在标准训练滚动中，模型很少遇到信息性失败，导致学习信号不足。现有方法的不足在于难以有效利用这些罕见失败，限制了训练效率和模型性能的提升。本研究旨在解决饱和问题下的训练停滞问题，以提高模型在复杂推理任务中的适应性和效果。",
      "method": "本研究提出 failure-prefix conditioning 方法，核心思想是基于罕见错误推理轨迹的前缀来重分配训练探索。具体而言，不是从原始问题开始训练，而是使用从错误轨迹中提取的前缀作为起点，让模型更频繁地暴露于易失败状态。关键创新点在于通过这种条件化训练，增加对信息性失败的访问性。摘要未明确说明使用的具体数据集和模型架构，但可推断基于 RLVR 框架和大型语言模型。迭代方法在训练中刷新失败前缀，以进一步优化性能。",
      "result": "实验结果显示，failure-prefix conditioning 在性能增益上与在中等难度问题上的训练相当，同时保持了标记效率。与基线方法对比，该方法在饱和问题上表现出改进，减少了在误导性失败前缀下的性能退化。迭代方法的应用在训练性能达到平台期后，解锁了额外的增益。尽管存在对正确早期推理遵从性的轻微权衡，但整体鲁棒性得到增强，提高了训练效果。",
      "conclusion": "本研究的主要贡献是提出了 failure-prefix conditioning 方法，为扩展 RLVR 在饱和问题上的训练提供了有效途径。该方法通过优化失败信号的利用，提高了训练效率和模型鲁棒性。学术价值在于改进了强化学习在推理任务中的应用，实际应用价值有助于提升大型语言模型在复杂问题上的表现。局限性包括对早期推理遵从性的轻微影响，未来工作可以探索更优的迭代策略以进一步优化性能。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards (RLVR)",
        "Large Language Model (LLM)",
        "Failure-Prefix Conditioning",
        "Saturated Problems",
        "Reasoning Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:28.137733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20815",
    "title": "GNN Explanations that do not Explain and How to find Them",
    "authors": [
      "Steve Azzolin",
      "Stefano Teso",
      "Bruno Lepri",
      "Andrea Passerini",
      "Sagar Malhotra"
    ],
    "abstract": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20815.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20815",
    "published": "2026-01-28T18:05:17Z",
    "updated": "2026-01-28T18:05:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文识别了自我可解释图神经网络（SE-GNN）解释可能与推理无关的关键失败，并提出一种新的忠实度指标来可靠地检测这种退化解释。",
      "motivation": "自我可解释图神经网络（SE-GNN）的解释对于理解模型内部机制和检测敏感属性滥用至关重要，但现有研究表明这些解释可能不准确且具有误导性，缺乏对其失败案例的系统分析，导致模型解释不可靠，无法有效审计，从而影响AI系统的可信度和公平性。本研究旨在填补这一空白，通过分析失败案例来提升解释的可靠性。",
      "method": "本研究识别了SE-GNN解释中的关键失败，即解释可能与标签推理过程无关，并进行理论分析，证明许多SE-GNNs可以在达到最优真实风险的同时产生退化解释，同时指出现有忠实度指标的局限性。在此基础上，我们引入了一种新的忠实度指标，旨在恶意和自然设置下都能可靠地标记退化解释为不忠实，研究涉及实证分析和代码实现，但摘要未明确说明使用的具体数据集或模型架构。",
      "result": "实验结果表明，许多SE-GNNs在达到最佳真实风险的同时能够产生退化解释，而多数忠实度指标无法识别这些失败模式。退化解释既可通过恶意植入以隐藏敏感属性使用，也可自然出现，突显了可靠审计的紧迫性。新提出的忠实度指标在恶意和自然场景中均能可靠地将退化解释标记为不忠实，代码已公开提供，但摘要未提供具体性能指标数据。",
      "conclusion": "本研究的主要贡献是揭示了SE-GNN解释可能无关推理的关键失败，并提出了一种新的忠实度指标来增强解释的可靠性，这提升了模型审计能力，具有重要的学术价值，为可信AI研究提供了新方向。潜在局限性包括摘要未明确说明新指标的具体适用范围或未来工作方向，但可以进一步探索其在更广泛AI模型中的应用。",
      "tags": [
        "Graph Neural Networks",
        "Explainable AI",
        "Faithfulness Metrics",
        "Model Auditing"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:33.188612Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20803",
    "title": "Structured Semantic Information Helps Retrieve Better Examples for In-Context Learning in Few-Shot Relation Extraction",
    "authors": [
      "Aunabil Chakma",
      "Mihai Surdeanu",
      "Eduardo Blanco"
    ],
    "abstract": "This paper presents several strategies to automatically obtain additional examples for in-context learning of one-shot relation extraction. Specifically, we introduce a novel strategy for example selection, in which new examples are selected based on the similarity of their underlying syntactic-semantic structure to the provided one-shot example. We show that this method results in complementary word choices and sentence structures when compared to LLM-generated examples. When these strategies are combined, the resulting hybrid system achieves a more holistic picture of the relations of interest than either method alone. Our framework transfers well across datasets (FS-TACRED and FS-FewRel) and LLM families (Qwen and Gemma). Overall, our hybrid selection method consistently outperforms alternative strategies and achieves state-of-the-art performance on FS-TACRED and strong gains on a customized FewRel subset.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20803.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20803",
    "published": "2026-01-28T17:48:58Z",
    "updated": "2026-01-28T17:48:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于结构化语义信息选择例子的混合方法，用于提高少样本关系提取中上下文学习的性能。",
      "motivation": "该研究旨在解决少样本关系提取中上下文学习时自动获取有效例子的问题。关系提取是自然语言处理的关键任务，在数据稀缺的少样本场景下，传统方法或单纯依赖大型语言模型生成的例子可能因单词选择和句子结构单一而无法提供足够的多样性，导致模型性能受限。本文的动机是通过结合结构化信息来优化例子选择，以提升学习效果和鲁棒性。",
      "method": "论文提出了一种基于句法语义结构相似性的例子选择策略，通过分析底层结构来选取与给定单样本例子相似的新例子。该方法与大型语言模型生成的例子互补，并结合成混合系统，以提供更全面的关系视图。关键创新在于利用结构化语义信息进行精准选择，实验中使用FS-TACRED和FS-FewRel数据集，并在Qwen和Gemma等LLM家族上进行测试，验证其迁移能力。",
      "result": "混合选择方法在FS-TACRED数据集上达到了最先进的性能，并在自定义FewRel子集上表现出显著增益。与单独使用LLM生成例子或其他替代策略相比，该方法一致更优，且在不同数据集和LLM家族上展示了良好的迁移性。具体性能指标如准确率提升摘要未明确说明，但强调了总体效果的优越性和强健性。",
      "conclusion": "论文的主要贡献是提出了一种结合结构化语义信息的混合例子选择方法，有效增强了上下文学习的全面性和性能。研究为少样本学习提供了新策略，具有学术价值，并可应用于关系提取等实际任务中，以改善数据稀缺场景下的模型表现。局限性或未来工作摘要未明确说明，但可能涉及扩展到更多领域或优化结构相似性计算。",
      "tags": [
        "In-Context Learning",
        "Few-Shot Learning",
        "Relation Extraction",
        "Semantic Structure",
        "Example Selection"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:46.257080Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20802",
    "title": "Reinforcement Learning via Self-Distillation",
    "authors": [
      "Jonas Hübotter",
      "Frederike Lübeck",
      "Lejs Behric",
      "Anton Baumann",
      "Marco Bagatella",
      "Daniel Marta",
      "Ido Hakimi",
      "Idan Shenfeld",
      "Thomas Kleine Buening",
      "Carlos Guestrin",
      "Andreas Krause"
    ],
    "abstract": "Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model's ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20802.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20802",
    "published": "2026-01-28T17:45:12Z",
    "updated": "2026-01-28T17:45:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Self-Distillation Policy Optimization (SDPO)方法，通过自我蒸馏利用文本反馈优化强化学习，以解决信用分配问题并提高效率。",
      "motivation": "当前强化学习方法在可验证领域（如代码生成）中仅依赖标量奖励学习，导致严重的信用分配瓶颈，限制了模型性能提升。许多环境（如编译器错误）提供丰富的文本反馈（如运行时错误），解释失败原因，但现有方法如RLVR未能充分利用这些信息，导致学习效率低下和模型优化不足。这个问题重要，因为有效利用文本反馈可以克服奖励稀疏性，提升在复杂任务（如科学推理和编程）中的学习效果。",
      "method": "SDPO 方法将标记化的文本反馈转换为密集学习信号，无需外部教师或显式奖励模型。其核心创新在于将当前模型基于反馈视为自我教师，并通过蒸馏机制将其反馈通知的下一个令牌预测回策略参数。技术特色包括自我蒸馏过程和利用大语言模型的内部能力回顾性地在上下文中识别错误。该方法在数据集LiveCodeBench v6上实施，针对科学推理、工具使用和竞争性编程任务，通过优化策略参数，直接从反馈中学习，简化了传统强化学习的信用分配。",
      "result": "在LiveCodeBench v6的任务中，SDPO 表现出优于强基线RLVR的样本效率和最终准确率，提高了学习效果。在标准RLVR环境中，SDPO 也通过使用成功尝试作为失败尝试的隐式反馈来提升性能。在困难二元奖励任务上，测试时应用SDPO加速了发现过程，以比最佳k采样或多轮对话少3倍的尝试达到相同的发现概率，具体准确率指标摘要未明确说明，但总体表现显著改进。",
      "conclusion": "本研究的主要贡献是形式化了带丰富反馈的强化学习设置，并提出了SDPO方法来有效利用文本反馈。学术上，SDPO 改进了信用分配机制，增强了学习效率和模型泛化能力；实际应用价值体现在代码生成、数学求解等领域，能加速任务解决。局限性方面摘要未明确说明，未来工作可能包括扩展至更多环境或优化反馈处理，进一步提升泛化性和鲁棒性。",
      "tags": [
        "Reinforcement Learning",
        "Self-Distillation",
        "Policy Optimization",
        "Tokenized Feedback",
        "Credit Assignment"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:01.306590Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20800",
    "title": "Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical & Dynamic Search Spaces",
    "authors": [
      "Kaito Baba",
      "Yoshihiko Ozaki",
      "Shuhei Watanabe"
    ],
    "abstract": "We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20800.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20800",
    "published": "2026-01-28T17:44:36Z",
    "updated": "2026-01-28T17:44:36Z",
    "comment": "16 pages, 9 figures",
    "light_analysis": {
      "overview": "论文提出conditional PED-ANOVA框架，有效估计条件搜索空间中的超参数重要性，解决了现有方法无法处理超参数条件依赖的局限性。",
      "motivation": "超参数优化在机器学习中至关重要，但在条件搜索空间中，超参数的存在或域可能依赖于其他超参数，例如在层次或动态结构中，这使得传统方法如PED-ANOVA难以适用，因为它假设固定的搜索空间。现有HPI估计器在条件设置中可能产生误导性或不解释性的结果，限制了实际应用的有效性。因此，需要一种新方法，能够准确评估条件超参数的重要性，以支持更复杂的模型调优任务。",
      "method": "本文提出conditional PED-ANOVA框架，通过引入条件超参数重要性概念，并推导出一个闭式估计器，来准确反映条件搜索空间中的超参数激活和域变化。该方法扩展了原始的PED-ANOVA，使其能够处理超参数之间的条件依赖关系，从而在层次和动态搜索空间中提供有效的HPI估计。关键创新包括基于条件结构的统计学建模，摘要未明确说明具体数据集或模型架构细节，但框架设计为通用性强，适用于多种超参数优化场景。",
      "result": "实验表明，现有HPI估计器的简单适应在条件设置中往往产生误导性或不解释性的重要性估计，而condPED-ANOVA则能一致地提供有意义的重要性值，这些值准确反映了底层条件结构。尽管摘要未明确说明具体性能数据如准确率提升或效率改进，但结果验证了该方法在条件搜索空间中的有效性和优越性，与基线方法相比，condPED-ANOVA在估计超参数重要性方面表现出更高的准确性和可解释性。",
      "conclusion": "本文的主要贡献是提出了condPED-ANOVA框架，为条件搜索空间中的超参数重要性估计提供了原则性解决方案，这具有重要的学术价值，因为它扩展了超参数优化的理论和方法，并具有实际应用价值，可应用于复杂机器学习模型的调优，尤其是在具有条件依赖的场景中。摘要未明确说明研究的局限性或未来工作方向，但该研究为后续工作奠定了基础，可能包括扩展到更广泛的搜索空间类型或集成到自动化机器学习流程中。",
      "tags": [
        "Hyperparameter Importance (HPI)",
        "PED-ANOVA",
        "Conditional Search Spaces",
        "Hyperparameter Optimization",
        "Hierarchical Search Spaces"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:01.545709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20796",
    "title": "Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers",
    "authors": [
      "Yiran Huang",
      "Karsten Roth",
      "Quentin Bouniot",
      "Wenjia Xu",
      "Zeynep Akata"
    ],
    "abstract": "Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20796.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20796",
    "published": "2026-01-28T17:37:28Z",
    "updated": "2026-01-28T17:37:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文揭示了现代Transformer在多模态上下文学习中的学习不对称性，并提供了理解其机制的实验基础和受控测试平台。",
      "motivation": "Transformer-based multimodal large language models（MLLMs）展现出上下文学习（ICL）能力，但跨模态关联信息的机制尚不清晰。本研究旨在探究Transformer如何从in-context示例中学习跨模态关联，以解决多模态AI中核心能力理解不足的问题。现有研究多关注单模态ICL，缺乏对多模态场景下机制的系统分析，因此本研究通过受控实验填补这一空白。",
      "method": "研究采用小型Transformer在合成分类任务上进行受控实验，精确操纵数据统计（如多样性）和模型架构（如Rotary Position Embeddings，RoPE）。首先回顾单模态ICL核心原则，扩展至多模态设置，分析跨模态学习动态。关键创新点包括利用RoPE探索ICL的数据复杂性阈值，并通过机制分析揭示归纳式标签复制机制。",
      "result": "实验发现，在单模态ICL中，RoPE提高了数据复杂性阈值。多模态实验中，当主模态预训练数据多样性高时，次模态仅需极低数据复杂性即可引发ICL，显示出学习不对称性。机制分析表明，两种设置均依赖从匹配示例复制标签的归纳式机制；多模态训练能精炼并扩展跨模态电路，未提供具体数值，但与单模态ICL形成对比。",
      "conclusion": "研究为理解现代Transformer的多模态ICL提供了机制基础，引入受控测试平台支持未来研究，具有学术价值，有助于优化多模态AI模型设计。潜在局限性包括实验基于合成任务，未来工作可扩展至真实数据集和更复杂场景。",
      "tags": [
        "Transformer",
        "In-Context Learning",
        "Multimodal Learning",
        "Rotary Position Embeddings",
        "Circuit Dynamics"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:56.411932Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20791",
    "title": "FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models",
    "authors": [
      "Haonan Zhong",
      "Wei Song",
      "Tingxu Han",
      "Maurice Pagnucco",
      "Jingling Xue",
      "Yang Song"
    ],
    "abstract": "Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.   Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20791.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20791",
    "published": "2026-01-28T17:29:53Z",
    "updated": "2026-01-28T17:29:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "FairT2V 提出一种无需训练的偏差减轻框架，通过中和提示嵌入减少文本到视频扩散模型中的性别偏见。",
      "motivation": "文本到视频（T2V）扩散模型虽进展迅速，但人口统计学偏见，尤其是性别偏见，尚未被充分探索，可能导致生成内容不公平并影响实际应用。现有方法多需微调或复杂处理，缺乏有效的training-free解决方案。本研究旨在填补这一空白，通过分析发现偏见主要源于预训练文本编码器的隐含性别关联，强调问题的重要性和研究紧迫性。",
      "method": "FairT2V 基于anchor-based spherical geodesic transformations中和提示嵌入，以减轻编码器引起的偏见同时保留语义。为保持时间连贯性，仅在早期身份形成步骤中通过动态去噪调度应用偏差减轻。此外，提出视频级公平性评估协议，结合VideoLLM-based reasoning和人类验证，确保全面评估效果。该方法无需微调，直接处理文本嵌入层。",
      "result": "在Open-Sora模型上的实验显示，FairT2V显著减少跨职业的人口统计学偏见，如性别偏见，同时对视频质量影响最小。摘要未明确提供具体性能指标数据如准确率提升，但强调了效果的大幅改善和与基线方法的对比。评估表明该方法有效降低了偏见程度。",
      "conclusion": "FairT2V的主要贡献在于提出一种无需训练的偏差减轻框架，有效缓解T2V模型中的偏见问题。该研究具有学术价值，推动了公平AI领域的发展，并为实际应用提供了可行方案。未来工作可扩展到其他偏见类型或更广泛的模型，进一步提升泛化性和效率。",
      "tags": [
        "Text-to-Video Diffusion Models",
        "Fairness Debiasing",
        "Training-Free Methods",
        "Gender Bias",
        "VideoLLM-based Evaluation"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:38.581502Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20789",
    "title": "SERA: Soft-Verified Efficient Repository Agents",
    "authors": [
      "Ethan Shen",
      "Danny Tormoen",
      "Saurabh Shah",
      "Ali Farhadi",
      "Tim Dettmers"
    ],
    "abstract": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20789.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20789",
    "published": "2026-01-28T17:27:08Z",
    "updated": "2026-01-28T17:27:08Z",
    "comment": "21 main pages, 7 pages appendix",
    "light_analysis": {
      "overview": "SERA提出了一种高效训练编码代理的方法，通过Soft Verified Generation和监督微调，实现低成本针对私有代码库的特殊化。",
      "motivation": "本研究旨在解决open-weight编码代理针对私有代码库训练成本高的问题。理论上，这类代理可编码特定仓库信息以提升性能，但在现实应用中，训练复杂性和高昂费用使这一优势难以发挥。现有方法如强化学习和合成数据技术效率低下，限制了实际部署。因此，开发高效、廉价的训练方法至关重要，以推动开源编码代理的实用化，特别是在需要保护私密代码的场景中。",
      "method": "SERA采用Soft Verified Generation（SVG）技术，从单个代码库生成数千个合成轨迹作为训练数据。核心创新在于SVG能低成本、大规模地生成高质量轨迹，结合监督微调（SFT）训练编码代理，而无需昂贵的强化学习或复杂合成数据方法。该方法使用公开代码库数据集进行扩展，生成了超过200,000个合成轨迹，以分析训练规律。技术特色包括高效轨迹生成和开源模型架构，支持针对私有代码库的定制化。",
      "result": "SERA在完全开源模型中达到state-of-the-art性能，匹配前沿开放权重模型如Devstral-Small-2。实验显示，创建SERA模型比强化学习便宜26倍，比先前合成数据方法便宜57倍，同时保持同等效果。基于SVG生成的数据集，论文提供了缩放定律的详细分析、消融研究和混杂因素探讨。这些结果证明了SERA在降低训练成本的同时提升性能，为开源编码代理的实际应用奠定了基础。",
      "conclusion": "论文主要贡献在于提出SERA方法，显著降低训练成本并提升针对私有代码库的特殊化能力。其学术价值体现在加速开源编码代理研究，实际应用价值包括支持企业私密代码库的高效管理。潜在局限性或未来工作可能涉及扩展到更广泛的代码类型或优化SVG技术，但摘要未明确说明具体细节。整体上，研究推动了开源模型的实用化，展示了其在专业场景中的优势。",
      "tags": [
        "Supervised Finetuning (SFT)",
        "Synthetic Data Generation",
        "Code Repository Specialization",
        "Open-source Coding Agents",
        "Soft Verified Generation (SVG)"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:25.467379Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20784",
    "title": "REASON: Accelerating Probabilistic Logical Reasoning for Scalable Neuro-Symbolic Intelligence",
    "authors": [
      "Zishen Wan",
      "Che-Kai Liu",
      "Jiayi Qian",
      "Hanchen Yang",
      "Arijit Raychowdhury",
      "Tushar Krishna"
    ],
    "abstract": "Neuro-symbolic AI systems integrate neural perception with symbolic reasoning to enable data-efficient, interpretable, and robust intelligence beyond purely neural models. Although this compositional paradigm has shown superior performance in domains such as reasoning, planning, and verification, its deployment remains challenging due to severe inefficiencies in symbolic and probabilistic inference. Through systematic analysis of representative neuro-symbolic workloads, we identify probabilistic logical reasoning as the inefficiency bottleneck, characterized by irregular control flow, low arithmetic intensity, uncoalesced memory accesses, and poor hardware utilization on CPUs and GPUs.   This paper presents REASON, an integrated acceleration framework for probabilistic logical reasoning in neuro-symbolic AI. REASON introduces a unified directed acyclic graph representation that captures common structure across symbolic and probabilistic models, coupled with adaptive pruning and regularization. At the architecture level, REASON features a reconfigurable, tree-based processing fabric optimized for irregular traversal, symbolic deduction, and probabilistic aggregation. At the system level, REASON is tightly integrated with GPU streaming multiprocessors through a programmable interface and multi-level pipeline that efficiently orchestrates compositional execution. Evaluated across six neuro-symbolic workloads, REASON achieves 12-50x speedup and 310-681x energy efficiency over desktop and edge GPUs under TSMC 28 nm node. REASON enables real-time probabilistic logical reasoning, completing end-to-end tasks in 0.8 s with 6 mm2 area and 2.12 W power, demonstrating that targeted acceleration of probabilistic logical reasoning is critical for practical and scalable neuro-symbolic AI and positioning REASON as a foundational system architecture for next-generation cognitive intelligence.",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20784.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20784",
    "published": "2026-01-28T17:17:21Z",
    "updated": "2026-01-28T17:17:21Z",
    "comment": "16 pages, 13 figures, 5 tables, 2026 IEEE International Symposium on High-Performance Computer Architecture (HPCA)",
    "light_analysis": {
      "overview": "论文提出了REASON框架，通过加速概率逻辑推理实现可扩展的神经符号AI。",
      "motivation": "神经符号AI系统融合神经感知与符号推理，以实现高效、可解释、鲁棒的智能，但受限于符号和概率推理的效率瓶颈。现有方法在CPU和GPU上存在不规则控制流、低算术强度、内存访问不连续和硬件利用率低等问题，导致部署困难。本研究旨在解决这些效率问题，推动神经符号AI的实际应用，解决推理缓慢带来的可扩展性挑战。",
      "method": "REASON框架采用统一的有向无环图表示，捕捉符号和概率模型的共同结构，并结合自适应剪枝和正则化。架构层面设计了可重构的树基处理结构，优化不规则遍历、符号推理和概率聚合。系统层面通过可编程接口和多级管道与GPU流多处理器紧密集成，实现高效组合执行，针对神经符号工作负载定制。",
      "result": "在六个神经符号工作负载上，REASON相比桌面和边缘GPU实现了12-50倍加速和310-681倍能效提升（基于TSMC 28 nm节点）。它能够完成实时概率逻辑推理，端到端任务在0.8秒内完成，占用面积6 mm²，功耗2.12 W，显著优于基线方法，展示了高效性和实用性。",
      "conclusion": "REASON成功加速概率逻辑推理，为实用和可扩展的神经符号AI奠定基础。它作为下一代认知智能的关键系统架构，展示了目标加速的重要性，并有望推动相关领域的应用，但摘要未明确说明未来工作方向或潜在局限性。",
      "tags": [
        "Probabilistic Logical Reasoning",
        "Neuro-Symbolic AI",
        "GPU Acceleration",
        "Reconfigurable Architecture",
        "Directed Acyclic Graph"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:44.880531Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20775",
    "title": "Active Learning for Decision Trees with Provable Guarantees",
    "authors": [
      "Arshia Soltani Moakhar",
      "Tanapoom Laoaron",
      "Faraz Ghahremani",
      "Kiarash Banihashem",
      "MohammadTaghi Hajiaghayi"
    ],
    "abstract": "This paper advances the theoretical understanding of active learning label complexity for decision trees as binary classifiers. We make two main contributions. First, we provide the first analysis of the disagreement coefficient for decision trees-a key parameter governing active learning label complexity. Our analysis holds under two natural assumptions required for achieving polylogarithmic label complexity, (i) each root-to-leaf path queries distinct feature dimensions, and (ii) the input data has a regular, grid-like structure. We show these assumptions are essential, as relaxing them leads to polynomial label complexity. Second, we present the first general active learning algorithm for binary classification that achieves a multiplicative error guarantee, producing a $(1+ε)$-approximate classifier. By combining these results, we design an active learning algorithm for decision trees that uses only a polylogarithmic number of label queries in the dataset size, under the stated assumptions. Finally, we establish a label complexity lower bound, showing our algorithm's dependence on the error tolerance $ε$ is close to optimal.",
    "categories": [
      "cs.LG",
      "cs.CC",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20775.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20775",
    "published": "2026-01-28T17:02:25Z",
    "updated": "2026-01-28T17:02:25Z",
    "comment": "10 pages, 43 pages with appendix, ICLR 2026, Conference URL: https://openreview.net/forum?id=NOkjJPJIit",
    "light_analysis": {
      "overview": "本文首次分析了决策树的不一致系数，并提出了一个具有乘法误差保证的主动学习算法，实现了多对数标签复杂度。",
      "motivation": "主动学习旨在减少监督学习中的标注成本，但对于决策树作为二元分类器，其标签复杂度的理论理解不足。现有方法可能缺乏严格的理论保证，特别是在不一致系数分析和高效算法设计方面。本研究致力于填补这一空白，通过分析关键参数并提供具有近似保证的算法，以在多对数复杂度下实现高效学习。假设的引入是必要的，因为放松假设会导致多项式复杂度，突显了问题的重要性和现有方法的局限性。",
      "method": "本研究首先分析了决策树的不一致系数，这是控制主动学习标签复杂度的关键参数。分析基于两个自然假设：每个根到叶路径查询不同特征维度，输入数据具有规则网格结构。在此基础上，提出了一个通用的主动学习算法用于二元分类，该算法产生一个(1+ε)-近似分类器，具有乘法误差保证。算法结合不一致系数分析，使用主动查询策略，在假设下仅需多对数标签查询。关键创新包括首次理论分析和具有近似保证的算法设计，模型基于决策树分类器。",
      "result": "在所述假设下，算法仅需数据集大小的多对数标签查询数，显著提高了效率。建立了标签复杂度下界，显示算法对误差容忍度ε的依赖接近最优。与基线情况对比，当放松假设时，标签复杂度变为多项式，验证了假设的必要性和算法的理论优势。结果证明算法在多对数复杂度下实现了高效分类，但摘要未明确说明具体实验数据集或准确率提升数据。",
      "conclusion": "本研究的主要贡献是首次分析了决策树的不一致系数，并设计了具有乘法误差保证的主动学习算法。学术价值在于推进了主动学习的理论框架，为决策树分类提供了严格保证。实际应用价值在于减少标注成本，促进高效机器学习。局限性在于依赖特定假设，未来工作可探索放松假设、扩展到其他模型或应用于现实世界数据集。",
      "tags": [
        "Active Learning",
        "Decision Trees",
        "Label Complexity",
        "Disagreement Coefficient",
        "Approximation Guarantee"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:16.841369Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20774",
    "title": "When More Data Doesn't Help: Limits of Adaptation in Multitask Learning",
    "authors": [
      "Steve Hanneke",
      "Mingyue Xu"
    ],
    "abstract": "Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.   In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20774.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20774",
    "published": "2026-01-28T17:00:11Z",
    "updated": "2026-01-28T17:00:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过建立更强的不可可能性结果，揭示了多任务学习中即使每任务有大量数据也无法保证适应性的统计极限，超越了现有理论。",
      "motivation": "多任务学习利用相关任务的数据共享提升性能，但在统计上存在局限性。现有研究如arXiv:2006.15785表明，当每任务样本量有限且无分布信息时，基于样本聚合的算法无法保证最优风险。然而，该工作仅覆盖有限样本情况，本文旨在探索更大样本量下的适应性问题，以理解多任务学习的根本挑战，强调问题的重要性在于实践中算法可能过度依赖数据量而忽略理论约束。",
      "method": "摘要未明确说明具体实验方法，但基于理论分析框架。论文通过统计建模和数学证明，建立了一个不可可能性定理，扩展了arXiv:2006.15785的结果。关键创新在于将适应性的极限从有限样本量推广到任意大样本量的情况，突出了多任务学习中分布信息的关键作用。由于是理论性研究，未提及具体数据集或模型架构，但强调了一般化论证。",
      "result": "论文的主要结果是证明了多任务学习中适应性的更强不可可能性定理。结果显示，即使每任务的样本量任意大，也无法保证算法能实现最优风险，从而无法克服多任务学习的困难。这一结果与基线方法arXiv:2006.15785相比更一般化，提供了更深的统计极限理解，但没有提供具体数值指标，如准确率或效率，因为它是理论性发现。",
      "conclusion": "本文总结了多任务学习的适应性存在根本性统计极限，即使数据丰富也无法完全克服，强调在算法设计中考虑分布信息的重要性。学术价值在于深化了对多任务学习理论的理解，为未来研究提供新方向；实际应用价值在于提醒实践者注意数据量的局限性。潜在局限性包括理论假设可能限制实际场景，未来工作可探索最优适应性的概念以改进框架。",
      "tags": [
        "Multitask Learning",
        "Statistical Limits",
        "Adaptation",
        "Impossibility Theorem",
        "Theoretical Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:01.447145Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20773",
    "title": "Smoothing the Black-Box: Signed-Distance Supervision for Black-Box Model Copying",
    "authors": [
      "Rubén Jiménez",
      "Oriol Pujol"
    ],
    "abstract": "Deployed machine learning systems must continuously evolve as data, architectures, and regulations change, often without access to original training data or model internals. In such settings, black-box copying provides a practical refactoring mechanism, i.e. upgrading legacy models by learning replicas from input-output queries alone. When restricted to hard-label outputs, copying turns into a discontinuous surface reconstruction problem from pointwise queries, severely limiting the ability to recover boundary geometry efficiently. We propose a distance-based copying (distillation) framework that replaces hard-label supervision with signed distances to the teacher's decision boundary, converting copying into a smooth regression problem that exploits local geometry. We develop an $α$-governed smoothing and regularization scheme with Hölder/Lipschitz control over the induced target surface, and introduce two model-agnostic algorithms to estimate signed distances under label-only access. Experiments on synthetic problems and UCI benchmarks show consistent improvements in fidelity and generalization accuracy over hard-label baselines, while enabling distance outputs as uncertainty-related signals for black-box replicas.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20773.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20773",
    "published": "2026-01-28T17:00:04Z",
    "updated": "2026-01-28T17:00:04Z",
    "comment": "27 pages",
    "light_analysis": {
      "overview": "该论文提出了一种基于有符号距离监督的黑盒模型复制框架，通过平滑回归问题有效恢复决策边界，提高了复制效率和几何恢复能力。",
      "motivation": "实际部署的机器学习系统需要持续更新以适应数据、架构和法规变化，但往往无法访问原始训练数据或模型内部结构。黑盒复制通过仅使用输入-输出查询来学习复制品，是一种实用的重构机制。然而，现有方法基于硬标签监督，导致复制问题变得不连续，无法高效恢复决策边界几何，限制了模型升级的准确性和效率，因此需要引入更有效的监督机制来改善复制过程。",
      "method": "论文提出距离基的复制框架，使用有符号距离到教师模型决策边界的监督替代硬标签，将问题转化为平滑回归以利用局部几何。开发了α控制的平滑和正则化方案，对目标表面施加Hölder/Lipschitz控制，确保平滑性和可控性。此外，引入两种模型无关算法，在仅标签访问条件下估计有符号距离，提高了方法的通用性和可扩展性，适用于多样化的黑盒场景。",
      "result": "在合成问题和UCI基准测试上的实验表明，该方法在模型复制的保真度和泛化精度上相对于硬标签基线有持续改进。距离输出不仅提升了复制准确性，还能作为不确定性相关的信号，增强黑盒复制品的实用性和可靠性。摘要未明确说明具体数值，但强调了改进的一致性和额外益处。",
      "conclusion": "该研究通过有符号距离监督，将黑盒复制问题平滑化，显著提高了复制效率和几何恢复能力，为模型升级提供了一种无需原始数据访问的方法。学术上，它为决策边界恢复和模型复制领域贡献了新思路；实际应用中，能帮助系统适应变化。未来工作可探索在更复杂环境中的扩展应用，以及进一步优化距离估计算法和正则化策略。",
      "tags": [
        "Black-Box Model Copying",
        "Signed-Distance Supervision",
        "Distance-Based Distillation",
        "Model-Agnostic Algorithms"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:56.280106Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20772",
    "title": "COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI",
    "authors": [
      "Shakhyar Gogoi"
    ],
    "abstract": "COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon behavior under fully autoregressive inference, a critical requirement for edge deployment where prediction errors accumulate over time. Experiments on non-stationary synthetic time-series data demonstrate that COMET-SG1 achieves competitive short-horizon accuracy while exhibiting significantly reduced long-horizon drift compared to MLP, LSTM, and k-nearest neighbor baselines. With a compact parameter footprint and operations compatible with fixed-point arithmetic, COMET-SG1 provides a practical and interpretable approach for stable autoregressive prediction in edge and embedded AI applications.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20772.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20772",
    "published": "2026-01-28T16:59:56Z",
    "updated": "2026-01-28T16:59:56Z",
    "comment": "Preprint. Submitted to an IEEE conference. 6 pages, 6 figures, 2 tables",
    "light_analysis": {
      "overview": "COMET-SG1是一种轻量级、稳定性优先的自回归回归模型，专为边缘和嵌入式AI时间序列预测设计，显著减少长期预测漂移。",
      "motivation": "该研究旨在解决边缘和嵌入式AI系统中时间序列预测的稳定性和效率问题。现有方法如循环神经网络和变压器模型在资源受限环境中可能计算开销大，且长期自回归推理中预测误差易累积，影响系统可靠性。在边缘部署中，确保有界长期行为至关重要，以避免误差随时间增长，因此开发轻量级、稳定性优先的模型具有重要实际意义。",
      "method": "论文提出COMET-SG1模型，采用线性行为空间编码来映射时间序列特征，通过内存锚定机制估计状态转换，并使用确定性状态更新确保预测稳定性。该方法不同于传统RNNs或transformers，其结构优先考虑在完全自回归推理下的有界行为，从而减少长期漂移。模型设计紧凑，参数占用小，并兼容定点算术，便于在资源受限的边缘设备上高效部署。",
      "result": "实验在非平稳合成时间序列数据上进行，COMET-SG1在短期预测中达到与基线（MLP、LSTM、k-NN）相当的准确率。更重要的是，与基线相比，COMET-SG1显著减少了长期预测漂移，证明了其在长期稳定性上的优势。模型通过紧凑设计和定点兼容操作，为边缘应用提供了高效解决方案，但摘要未提供具体数值指标。",
      "conclusion": "COMET-SG1的主要贡献是提供了一种轻量级、稳定性优先的自回归回归方法，适用于边缘和嵌入式AI的时间序列预测。其学术价值在于改进长期预测稳定性，减少误差累积；实际应用价值在于高效部署于资源受限设备，提升系统可靠性。未来工作可能包括在真实数据集上验证模型性能，并探索扩展应用到更复杂场景。",
      "tags": [
        "Autoregressive Model",
        "Edge AI",
        "Time-Series Prediction",
        "Lightweight Model",
        "Fixed-Point Arithmetic"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:33.662279Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20765",
    "title": "Less is More: Clustered Cross-Covariance Control for Offline RL",
    "authors": [
      "Nan Qiao",
      "Sheng Yue",
      "Shuning Wang",
      "Yongheng Deng",
      "Ju Ren"
    ],
    "abstract": "A fundamental challenge in offline reinforcement learning is distributional shift. Scarce data or datasets dominated by out-of-distribution (OOD) areas exacerbate this issue. Our theoretical analysis and experiments show that the standard squared error objective induces a harmful TD cross covariance. This effect amplifies in OOD areas, biasing optimization and degrading policy learning. To counteract this mechanism, we develop two complementary strategies: partitioned buffer sampling that restricts updates to localized replay partitions, attenuates irregular covariance effects, and aligns update directions, yielding a scheme that is easy to integrate with existing implementations, namely Clustered Cross-Covariance Control for TD (C^4). We also introduce an explicit gradient-based corrective penalty that cancels the covariance induced bias within each update. We prove that buffer partitioning preserves the lower bound property of the maximization objective, and that these constraints mitigate excessive conservatism in extreme OOD areas without altering the core behavior of policy constrained offline reinforcement learning. Empirically, our method showcases higher stability and up to 30% improvement in returns over prior methods, especially with small datasets and splits that emphasize OOD areas.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20765.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20765",
    "published": "2026-01-28T16:55:04Z",
    "updated": "2026-01-28T16:55:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出C^4方法，通过分区缓冲采样和纠正惩罚来缓解离线强化学习中的分布偏移，提升性能和稳定性。",
      "motivation": "离线强化学习中，分布偏移是一个根本挑战，特别是在数据稀少或分布外区域主导时。标准平方误差目标导致有害的TD交叉协方差，在OOD区域被放大，引起优化偏差和政策学习退化，现有方法难以有效处理这一问题。这在实际应用中尤为重要，因为离线RL常用于从静态数据学习，偏差可能导致策略失败。",
      "method": "论文开发了两种互补策略：一是分区缓冲采样，将数据集划分为本地化重放分区，限制更新以减少不规则协方差效应并对齐更新方向，形成易于集成的C^4方案。二是显式的梯度纠正惩罚，在每个更新中直接消除协方差诱导的偏差。证明缓冲分区保留最大化目标的下界性质，约束在不改变核心行为下减轻OOD区域的过度保守性。",
      "result": "实验表明，C^4方法比先前方法具有更高稳定性，并在回报上实现高达30%的提升。特别是在小数据集和强调OOD区域的拆分中，性能显著优于基线离线强化学习方法，证明了其在处理分布偏移方面的有效性。",
      "conclusion": "论文主要贡献是提出C^4方法，通过对抗TD交叉协方差改进了离线强化学习的稳定性和性能。学术上揭示了协方差机制并提供解决方案；实际上适用于数据稀缺和OOD场景。未来工作可探索更广泛适应性或与其他技术结合，潜在局限性可能包括对数据分布的依赖。",
      "tags": [
        "Offline Reinforcement Learning",
        "TD Learning",
        "Cross-Covariance Control",
        "Clustered Sampling",
        "Gradient Penalty"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:10.145601Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20757",
    "title": "Persona Prompting as a Lens on LLM Social Reasoning",
    "authors": [
      "Jing Yang",
      "Moritz Hechtbauer",
      "Elisabeth Khalilov",
      "Evelyn Luise Brinkmann",
      "Vera Schmitt",
      "Nils Feldhus"
    ],
    "abstract": "For socially sensitive tasks like hate speech detection, the quality of explanations from Large Language Models (LLMs) is crucial for factors like user trust and model alignment. While Persona prompting (PP) is increasingly used as a way to steer model towards user-specific generation, its effect on model rationales remains underexplored. We investigate how LLM-generated rationales vary when conditioned on different simulated demographic personas. Using datasets annotated with word-level rationales, we measure agreement with human annotations from different demographic groups, and assess the impact of PP on model bias and human alignment. Our evaluation across three LLMs results reveals three key findings: (1) PP improving classification on the most subjective task (hate speech) but degrading rationale quality. (2) Simulated personas fail to align with their real-world demographic counterparts, and high inter-persona agreement shows models are resistant to significant steering. (3) Models exhibit consistent demographic biases and a strong tendency to over-flag content as harmful, regardless of PP. Our findings reveal a critical trade-off: while PP can improve classification in socially-sensitive tasks, it often comes at the cost of rationale quality and fails to mitigate underlying biases, urging caution in its application.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20757.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20757",
    "published": "2026-01-28T16:41:17Z",
    "updated": "2026-01-28T16:41:17Z",
    "comment": "9 Pages, EACL main",
    "light_analysis": {
      "overview": "本文通过研究Persona Prompting对大型语言模型社会推理的影响，揭示了其在改进分类任务时可能损害推理质量且未能减轻模型偏见的权衡。",
      "motivation": "在社会敏感任务如仇恨言论检测中，大型语言模型的解释质量对用户信任和模型对齐至关重要，直接影响实际应用的社会影响。现有方法如Persona Prompting虽用于引导模型生成用户特定内容，但其对模型推理的影响尚未充分探索，导致模型可能隐藏偏见或对齐问题，因此研究这一不足对提升模型社会可靠性具有重要意义。摘要未明确说明更广泛的背景研究，但强调了社会敏感任务中解释质量的核心挑战。",
      "method": "本研究采用Persona Prompting技术，通过模拟不同人口统计角色作为提示条件来引导大型语言模型的生成。核心创新在于使用带词级推理标注的数据集，测量模型生成的推理与不同人口群体人类标注的一致性，并评估PP对模型偏见和人类对齐的影响。关键技术细节包括使用三个大型语言模型进行实验，以及设计系统性的评估框架来量化理性质量和偏见程度。摘要未明确说明具体模型架构或数据集名称。",
      "result": "主要实验结果包括三个关键发现：首先，Persona Prompting在最主观任务（仇恨言论检测）中改进分类准确性，但降低了推理质量。其次，模拟角色未能与现实世界人口统计对齐，高角色间一致性表明模型对显著引导有抵抗力。最后，模型展示了一致的人口偏见和强烈过度标记内容为有害的倾向，与PP无关。这些结果揭示了性能提升与推理质量之间的直接权衡，以及PP未能有效减轻模型潜在的偏见和不对齐问题。",
      "conclusion": "论文的主要贡献在于揭示Persona Prompting在社会敏感任务应用中存在关键权衡：虽可能改进分类性能，但常以推理质量为代价，且无法缓解模型的内在偏见。这强调了谨慎使用PP的重要性，具有学术价值在于深化对LLM社会推理机制的理解，实际应用价值体现在促进更负责任的人工智能部署。未来工作方向包括改进PP方法以减少负面影响，例如开发更有效的对齐技术。摘要未明确说明具体局限性。",
      "tags": [
        "Persona Prompting",
        "Large Language Models",
        "Social Reasoning",
        "Hate Speech Detection",
        "Model Bias"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:07.438911Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20756",
    "title": "Supervised Guidance Training for Infinite-Dimensional Diffusion Models",
    "authors": [
      "Elizabeth L. Baker",
      "Alexander Denker",
      "Jes Frellsen"
    ],
    "abstract": "Score-based diffusion models have recently been extended to infinite-dimensional function spaces, with uses such as inverse problems arising from partial differential equations. In the Bayesian formulation of inverse problems, the aim is to sample from a posterior distribution over functions obtained by conditioning a prior on noisy observations. While diffusion models provide expressive priors in function space, the theory of conditioning them to sample from the posterior remains open. We address this, assuming that either the prior lies in the Cameron-Martin space, or is absolutely continuous with respect to a Gaussian measure. We prove that the models can be conditioned using an infinite-dimensional extension of Doob's $h$-transform, and that the conditional score decomposes into an unconditional score and a guidance term. As the guidance term is intractable, we propose a simulation-free score matching objective (called Supervised Guidance Training) enabling efficient and stable posterior sampling. We illustrate the theory with numerical examples on Bayesian inverse problems in function spaces. In summary, our work offers the first function-space method for fine-tuning trained diffusion models to accurately sample from a posterior.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20756.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20756",
    "published": "2026-01-28T16:39:39Z",
    "updated": "2026-01-28T16:39:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Supervised Guidance Training方法，首次在无限维函数空间中实现扩散模型的条件化，以准确采样后验分布。",
      "motivation": "在贝叶斯反问题中，需要从后验分布采样以解决如偏微分方程引起的复杂问题。扩散模型在函数空间中作为先验表达力强，但现有方法缺乏将扩散模型条件化以采样后验的理论支持。这一问题的重要性在于函数空间反问题在科学计算中广泛存在，而理论缺失限制了扩散模型的应用潜力，导致现有方法无法高效处理无限维设置下的后验推断。",
      "method": "研究假设先验位于Cameron-Martin空间或相对于高斯测度绝对连续。核心方法是基于Doob's h-变换的无限维扩展，证明条件分数可分解为无条件分数和指导项。由于指导项不可计算，提出了Supervised Guidance Training这一无模拟分数匹配目标，实现高效稳定的训练。关键创新包括理论证明和实用训练方案，但摘要未明确说明具体的数据集、模型架构或实验设置细节。",
      "result": "理论通过数值例子在函数空间贝叶斯反问题中得到验证，证明了Supervised Guidance Training方法能够实现高效稳定的后验采样。然而，摘要未提供具体的性能指标数据，如准确率提升、效率改进数值或与基线方法的对比结果，因此实验效果的具体支撑在摘要中未明确说明，需参考全文获取更多细节。",
      "conclusion": "本研究的主要贡献是提供了首个函数空间方法，用于微调扩散模型以采样后验分布，解决了无限维扩散模型条件化的理论开放问题。学术价值在于扩展了扩散模型在贝叶斯推断中的应用范围，实际应用价值体现在反问题求解中。未来工作可能包括将该方法推广到更广泛领域、优化计算效率或探索更多理论扩展，但摘要未明确提及局限性。",
      "tags": [
        "Diffusion Models",
        "Score-based Methods",
        "Function Spaces",
        "Bayesian Inverse Problems",
        "Supervised Guidance Training"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:30.346348Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20753",
    "title": "GraphAllocBench: A Flexible Benchmark for Preference-Conditioned Multi-Objective Policy Learning",
    "authors": [
      "Zhiheng Jiang",
      "Yunzhe Wang",
      "Ryan Marr",
      "Ellen Novoseller",
      "Benjamin T. Files",
      "Volkan Ustun"
    ],
    "abstract": "Preference-Conditioned Policy Learning (PCPL) in Multi-Objective Reinforcement Learning (MORL) aims to approximate diverse Pareto-optimal solutions by conditioning policies on user-specified preferences over objectives. This enables a single model to flexibly adapt to arbitrary trade-offs at run-time by producing a policy on or near the Pareto front. However, existing benchmarks for PCPL are largely restricted to toy tasks and fixed environments, limiting their realism and scalability. To address this gap, we introduce GraphAllocBench, a flexible benchmark built on a novel graph-based resource allocation sandbox environment inspired by city management, which we call CityPlannerEnv. GraphAllocBench provides a rich suite of problems with diverse objective functions, varying preference conditions, and high-dimensional scalability. We also propose two new evaluation metrics -- Proportion of Non-Dominated Solutions (PNDS) and Ordering Score (OS) -- that directly capture preference consistency while complementing the widely used hypervolume metric. Through experiments with Multi-Layer Perceptrons (MLPs) and graph-aware models, we show that GraphAllocBench exposes the limitations of existing MORL approaches and paves the way for using graph-based methods such as Graph Neural Networks in complex, high-dimensional combinatorial allocation tasks. Beyond its predefined problem set, GraphAllocBench enables users to flexibly vary objectives, preferences, and allocation rules, establishing it as a versatile and extensible benchmark for advancing PCPL. Code: https://anonymous.4open.science/r/GraphAllocBench",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20753.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20753",
    "published": "2026-01-28T16:36:37Z",
    "updated": "2026-01-28T16:36:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "GraphAllocBench 是一个针对偏好条件多目标策略学习的灵活基准，基于图形资源分配环境，并提出新评估指标 PNDS 和 OS。",
      "motivation": "在多目标强化学习中，偏好条件策略学习旨在通过用户指定的偏好近似多样化帕累托最优解，使单一模型能灵活适应不同权衡。现有基准大多局限于玩具任务和固定环境，缺乏真实性和可扩展性，限制了该领域的发展和应用。因此，需要更丰富、可扩展的基准来评估和改进现有方法，以推动复杂现实任务中的技术进步。",
      "method": "论文提出 GraphAllocBench 基准，基于图形资源分配沙盒环境 CityPlannerEnv，模拟城市管理场景，提供多样化目标函数和偏好条件。核心创新包括图形环境的高维可扩展性，以及两个新评估指标——PNDS 和 OS，这些指标直接捕获偏好一致性，并补充了广泛使用的超体积指标。实验中使用多层感知机和图形感知模型来验证基准的有效性。",
      "result": "通过实验，GraphAllocBench 暴露了现有多目标强化学习方法的局限性，并显示出图形方法如 Graph Neural Networks 在复杂高维组合分配任务中的潜力。摘要未明确说明具体性能数据，但实验表明该基准能有效评估不同方法，为未来研究提供参考框架，并与现有基准形成对比。",
      "conclusion": "GraphAllocBench 的主要贡献是提供一个多功能、可扩展的基准，推动偏好条件策略学习的研究。其学术价值在于引入新环境和评估指标，实际应用价值在于支持城市管理等现实任务中的资源分配决策。未来工作可扩展基准的问题集，并进一步优化图形模型的性能。",
      "tags": [
        "Preference-Conditioned Policy Learning",
        "Multi-Objective Reinforcement Learning",
        "Graph Neural Networks",
        "Benchmark Evaluation",
        "Resource Allocation"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:28.766517Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20747",
    "title": "Like a Therapist, But Not: Reddit Narratives of AI in Mental Health Contexts",
    "authors": [
      "Elham Aghakhani",
      "Rezvaneh Rezapour"
    ],
    "abstract": "Large language models (LLMs) are increasingly used for emotional support and mental health-related interactions outside clinical settings, yet little is known about how people evaluate and relate to these systems in everyday use. We analyze 5,126 Reddit posts from 47 mental health communities describing experiential or exploratory use of AI for emotional support or therapy. Grounded in the Technology Acceptance Model and therapeutic alliance theory, we develop a theory-informed annotation framework and apply a hybrid LLM-human pipeline to analyze evaluative language, adoption-related attitudes, and relational alignment at scale. Our results show that engagement is shaped primarily by narrated outcomes, trust, and response quality, rather than emotional bond alone. Positive sentiment is most strongly associated with task and goal alignment, while companionship-oriented use more often involves misaligned alliances and reported risks such as dependence and symptom escalation. Overall, this work demonstrates how theory-grounded constructs can be operationalized in large-scale discourse analysis and highlights the importance of studying how users interpret language technologies in sensitive, real-world contexts.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20747.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20747",
    "published": "2026-01-28T16:23:00Z",
    "updated": "2026-01-28T16:23:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "研究Reddit上AI在心理健康语境中的用户叙事，通过理论驱动的分析框架揭示用户评价机制，操作化理论基础以进行大规模话语分析。",
      "motivation": "大语言模型（LLMs）在非临床环境中越来越常用于情感支持和心理健康交互，但人们对日常使用中如何评价和关联这些系统的了解甚少。现有方法可能侧重于技术性能，而忽视了用户主观体验和真实世界中的动态关系。因此，本研究旨在填补这一空白，探索用户在实际场景中如何评估AI系统，尤其是在敏感的心理健康领域，以识别采纳态度和关系对齐的关键因素，强调理解日常使用评价的重要性。",
      "method": "本研究分析5,126个来自47个心理健康社区的Reddit帖子，描述AI用于情感支持或治疗的体验性使用。基于技术接受模型和治疗联盟理论，开发了一个理论驱动的注释框架。关键创新在于应用混合LLM-人类管道，自动化与人工结合，大规模分析评价性语言、采纳相关态度和关系对齐。使用了具体数据集（Reddit帖子）和理论模型指导，以揭示用户行为模式，技术路线结合了理论建模和自动化处理。",
      "result": "结果显示，用户参与主要由叙述结果、信任和响应质量塑造，而不是情感纽带。积极情绪与任务和目标对齐最强相关，表明功能性对齐比情感连接更重要。以伴侣为导向的使用更常涉及错位的联盟，并报告了风险如依赖和症状恶化。这些结果基于大规模话语分析，提供了实证支持，但具体数值指标摘要未明确说明，对比基线方法未详细提及，但强调了理论构建的操作化效果。",
      "conclusion": "本研究贡献在于展示了如何在大规模话语分析中操作化理论基础的构建，如技术接受模型和治疗联盟理论。它强调了研究用户如何在敏感现实语境中解释语言技术的重要性，为AI在心理健康领域的应用提供了实证基础。实际价值在于帮助设计更安全、有效的AI支持系统，避免潜在风险。局限性可能包括数据来源有限，未来工作可扩展到其他平台或进行纵向跟踪研究，以深化理解用户动态。",
      "tags": [
        "Large Language Models",
        "Technology Acceptance Model",
        "therapeutic alliance theory",
        "hybrid LLM-human pipeline",
        "discourse analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:33.732620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20745",
    "title": "HESTIA: A Hessian-Guided Differentiable Quantization-Aware Training Framework for Extremely Low-Bit LLMs",
    "authors": [
      "Guoan Wang",
      "Feiyu Wang",
      "Zongwei Lv",
      "Yikun Zong",
      "Tong Yang"
    ],
    "abstract": "As large language models (LLMs) continue to scale, deployment is increasingly bottlenecked by the memory wall, motivating a shift toward extremely low-bit quantization. However, most quantization-aware training (QAT) methods apply hard rounding and the straight-through estimator (STE) from the beginning of the training, which prematurely discretizes the optimization landscape and induces persistent gradient mismatch between latent weights and quantized weights, hindering effective optimization of quantized models. To address this, we propose Hestia, a Hessian-guided differentiable QAT framework for extremely low-bit LLMs, which replaces the rigid step function with a temperature-controlled softmax relaxation to maintain gradient flow early in training while progressively hardening quantization. Furthermore, Hestia leverages a tensor-wise Hessian trace metric as a lightweight curvature signal to drive fine-grained temperature annealing, enabling sensitivity-aware discretization across the model. Evaluations on Llama-3.2 show that Hestia consistently outperforms existing ternary QAT baselines, yielding average zero-shot improvements of 5.39% and 4.34% for the 1B and 3B models. These results indicate that Hessian-guided relaxation effectively recovers representational capacity, establishing a more robust training path for 1.58-bit LLMs. The code is available at https://github.com/hestia2026/Hestia.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20745.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20745",
    "published": "2026-01-28T16:22:42Z",
    "updated": "2026-01-28T16:22:42Z",
    "comment": "13 pages, 2 figures",
    "light_analysis": {
      "overview": "本文提出了Hestia框架，通过Hessian引导的可微分量化感知训练，优化极低比特大型语言模型的训练。",
      "motivation": "随着大型语言模型的规模不断增大，模型部署受到内存瓶颈的限制，因此极低比特量化成为关键研究方向。然而，现有量化感知训练方法通常采用硬四舍五入和直通估计器，从训练初期就引入离散化，导致优化景观过早固化，潜在权重与量化权重之间产生持续梯度不匹配，严重阻碍了量化模型的性能优化和收敛。这一问题在高比特压缩中尤为突出，需要更灵活的量化策略来维持梯度流动并改善训练稳定性。",
      "method": "Hestia框架的核心方法是使用温度控制的softmax松弛函数替代传统的硬阶梯函数，在训练早期保持连续性和梯度流动，然后通过渐进硬化实现量化。关键创新在于利用张量级Hessian迹作为轻量级曲率信号，动态调整温度参数，实现敏感度感知的离散化，使模型各部分能根据其重要性进行差异化量化。实验基于Llama-3.2模型架构，采用三元量化（1.58位）进行评估，框架设计考虑了计算效率和实际部署需求。",
      "result": "在Llama-3.2模型上的实验结果表明，Hestia框架在极低比特量化任务中显著优于现有基准方法。具体来说，1B和3B模型在零样本任务上的平均性能提升分别达到5.39%和4.34%。与传统的三元量化感知训练基线相比，Hestia通过Hessian引导的松弛策略，有效减少了梯度不匹配，从而恢复了模型的表示能力，为1.58位大型语言模型建立了一条更稳健的训练路径，性能改进稳定且显著。",
      "conclusion": "本论文的主要贡献是提出了Hestia框架，通过结合Hessian引导和可微分量化，改善了极低比特大型语言模型的训练效果。学术上，该方法为量化感知训练提供了新思路，通过软松弛和曲率感知优化，克服了传统方法的局限性。实际应用中，Hestia有助于在资源受限环境下部署高性能LLMs，促进模型压缩技术的发展。未来研究可探索该框架在不同模型架构和量化级别上的泛化能力，以及进一步优化计算开销。",
      "tags": [
        "Quantization-Aware Training",
        "Hessian-guided Optimization",
        "Differentiable Quantization",
        "Softmax Relaxation",
        "Temperature Annealing"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:46.733111Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20742",
    "title": "Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification",
    "authors": [
      "Xin Jin",
      "Jinming Liu",
      "Yuntao Wei",
      "Junyan Lin",
      "Zhicheng Wang",
      "Jianguo Huang",
      "Xudong Yang",
      "Yanxiao Liu",
      "Wenjun Zeng"
    ],
    "abstract": "\"Compression Tells Intelligence\", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20742.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20742",
    "published": "2026-01-28T16:18:20Z",
    "updated": "2026-01-28T16:18:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过统一视觉编码与视觉令牌技术，揭示了压缩效率与模型性能的关联，并预测了未来发展方向。",
      "motivation": "研究动机源于压缩效率在多模态大模型（如MLLMs）性能中的关键作用，传统视觉编码基于信息论已广泛应用，而新兴视觉令牌技术在生成模型中追求类似目标，但两者缺乏统一优化视角。现有方法可能未充分探索其内在联系，限制了压缩技术在AI任务中的高效应用，因此论文旨在整合这两种技术以优化压缩效率和模型性能的权衡。",
      "method": "论文首先综述了视觉编码和视觉令牌技术两大技术家族，然后从优化角度提出统一公式连接它们，核心方法包括分析相似性、讨论压缩效率与模型性能的权衡本质，并基于框架合成双向见解。关键创新在于提供理论框架预测下一代技术，摘要未明确说明具体数据集或模型架构，方法侧重于概念分析和理论推导。",
      "result": "实验结果显示任务导向令牌技术在多模态LLMs、AI生成内容和具身AI等任务中具有潜力，表明统一方法可促进高效智能处理，但摘要未提供具体性能指标如准确率提升或效率改进数据，也未明确与基线方法的对比，因此实验效果需进一步验证。",
      "conclusion": "论文的主要贡献在于统一视觉编码和视觉令牌技术，为理解压缩在AI中的作用提供了新视角，学术价值是填补传统编码与现代AI技术的理论空白，推动优化方法发展，实际应用可能引领高效令牌技术标准化，未来工作包括深入实验验证和技术标准化探索。",
      "tags": [
        "Visual Coding",
        "Visual Token Technology",
        "Multimodal Large Language Models",
        "Optimization",
        "AI-generated Content"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:14.698637Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20738",
    "title": "SA-PEF: Step-Ahead Partial Error Feedback for Efficient Federated Learning",
    "authors": [
      "Dawit Kiros Redie",
      "Reza Arablouei",
      "Stefan Werner"
    ],
    "abstract": "Biased gradient compression with error feedback (EF) reduces communication in federated learning (FL), but under non-IID data, the residual error can decay slowly, causing gradient mismatch and stalled progress in the early rounds. We propose step-ahead partial error feedback (SA-PEF), which integrates step-ahead (SA) correction with partial error feedback (PEF). SA-PEF recovers EF when the step-ahead coefficient $α=0$ and step-ahead EF (SAEF) when $α=1$. For non-convex objectives and $δ$-contractive compressors, we establish a second-moment bound and a residual recursion that guarantee convergence to stationarity under heterogeneous data and partial client participation. The resulting rates match standard non-convex Fed-SGD guarantees up to constant factors, achieving $O((η,η_0TR)^{-1})$ convergence to a variance/heterogeneity floor with a fixed inner step size. Our analysis reveals a step-ahead-controlled residual contraction $ρ_r$ that explains the observed acceleration in the early training phase. To balance SAEF's rapid warm-up with EF's long-term stability, we select $α$ near its theory-predicted optimum. Experiments across diverse architectures and datasets show that SA-PEF consistently reaches target accuracy faster than EF.",
    "categories": [
      "cs.LG",
      "cs.DC",
      "eess.SP",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20738.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20738",
    "published": "2026-01-28T16:10:49Z",
    "updated": "2026-01-28T16:10:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出步前部分误差反馈（SA-PEF）方法，通过整合步前校正和部分误差反馈，解决了联邦学习在非IID数据下梯度压缩残差误差衰减缓慢的问题，提高了训练效率。",
      "motivation": "联邦学习中使用梯度压缩和误差反馈可减少通信开销，但在非独立同分布（non-IID）数据下，残差误差衰减缓慢，导致梯度不匹配和早期训练停滞。现有方法如标准误差反馈在异构数据场景下表现不佳，限制了模型收敛速度和实际应用效率。这一问题在联邦学习中尤为重要，因为通信瓶颈和非IID数据是常见挑战，影响训练过程的稳定性和性能。",
      "method": "论文提出步前部分误差反馈（SA-PEF）方法，整合步前校正和部分误差反馈，其中步前系数α控制校正程度：当α=0时恢复标准误差反馈，α=1时恢复步前误差反馈。针对非凸目标和δ-压缩器，理论分析建立二阶矩界和残差递归，证明在异构数据及部分客户端参与下收敛到平稳点。关键创新在于引入步前控制的残差收缩因子ρ_r，优化α以平衡早期加速与长期稳定，方法适用于多样联邦学习场景。",
      "result": "实验结果表明，SA-PEF在多种网络架构和数据集上相比标准误差反馈能更快达到目标准确率，例如在非凸设置下实现加速收敛。理论分析显示收敛率与非凸Fed-SGD保证匹配，达到O((η,η_0TR)^{-1})收敛到方差/异质性下限，步前控制残差收缩ρ_r解释了早期训练阶段的加速现象。与基线方法对比，SA-PEF在处理非IID数据时表现出更高的效率，减少训练停滞。",
      "conclusion": "论文的主要贡献是提出SA-PEF方法，结合理论分析和实验验证，解决了联邦学习中非IID数据下的收敛问题。学术价值在于提供了收敛保证和加速机制的解释，实际应用价值是提高联邦学习训练速度和通信效率，尤其在异构数据场景中。未来工作可能涉及扩展方法到其他压缩器或优化参数选择策略，进一步提升鲁棒性。",
      "tags": [
        "Federated Learning",
        "Gradient Compression",
        "Error Feedback",
        "Non-convex Optimization",
        "Convergence Theory"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:08.938214Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20735",
    "title": "Implementing Metric Temporal Answer Set Programming",
    "authors": [
      "Arvid Becker",
      "Pedro Cabalar",
      "Martin Diéguez",
      "Susana Hahn",
      "Javier Romero",
      "Torsten Schaub"
    ],
    "abstract": "We develop a computational approach to Metric Answer Set Programming (ASP) to allow for expressing quantitative temporal constraints, like durations and deadlines. A central challenge is to maintain scalability when dealing with fine-grained timing constraints, which can significantly exacerbate ASP's grounding bottleneck. To address this issue, we leverage extensions of ASP with difference constraints, a simplified form of linear constraints, to handle time-related aspects externally. Our approach effectively decouples metric ASP from the granularity of time, resulting in a solution that is unaffected by time precision.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20735.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20735",
    "published": "2026-01-28T16:07:54Z",
    "updated": "2026-01-28T16:07:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种度量Answer Set Programming的计算方法，利用差异约束外部处理时间约束，解决了细粒度时间约束下的可扩展性问题。",
      "motivation": "研究动机在于解决度量Answer Set Programming中表达定量时间约束时的可扩展性挑战。现有方法在处理细粒度时间约束（如持续时间和截止日期）时，会加剧ASP的接地瓶颈，导致计算效率低下。这一问题在需要精确时间建模的实际应用中尤为重要，因此开发新方法以克服瓶颈对提升时间约束建模的实用性至关重要。",
      "method": "论文采用扩展Answer Set Programming的方法，结合差异约束这一简化线性约束，将时间相关方面外部化处理。核心创新在于通过解耦度量ASP与时间粒度，避免接地瓶颈。该方法具体涉及利用ASP扩展来处理时间约束，使得时间精度不影响计算过程，从而提升可扩展性。摘要未明确说明使用的数据集或模型架构细节。",
      "result": "研究结果显示，所提出的方法有效解决了度量ASP中细粒度时间约束的可扩展性问题。通过外部化时间处理，避免了接地瓶颈，使得解决方案不受时间精度影响，从而提升了处理效率。然而，摘要未明确提供具体的性能指标如准确率提升或与基线方法的对比数据，但暗示了方法在解耦时间粒度方面的优势。",
      "conclusion": "本研究贡献了一种创新的度量Answer Set Programming方法，通过差异约束外部处理时间约束，解决了可扩展性问题。该研究具有重要学术价值，为时间约束建模提供了新途径，并提升了ASP的实用性。实际应用中，可用于需要精确时间约束的领域。未来工作可能包括方法优化和扩展至更复杂的时间约束场景。",
      "tags": [
        "Metric Answer Set Programming",
        "Difference Constraints",
        "Temporal Constraints",
        "Linear Constraints",
        "ASP Extensions"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:26.546081Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20732",
    "title": "Continual GUI Agents",
    "authors": [
      "Ziwei Liu",
      "Borui Kang",
      "Hangjie Yuan",
      "Zixiang Zhao",
      "Wei Li",
      "Yifan Zhu",
      "Tao Feng"
    ],
    "abstract": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20732.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20732",
    "published": "2026-01-28T16:06:31Z",
    "updated": "2026-01-28T16:06:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了GUI-Anchoring in Flux (GUI-AiF)框架，通过新奖励机制实现GUI代理在动态环境中的稳定持续学习。",
      "motivation": "数字环境持续变化，新GUI数据不断引入新域或分辨率，导致在静态数据上训练的代理性能下降。现有方法无法在GUI分布随时间变化时保持稳定基础，因为用户界面交互点和区域多样且动态，这使得代理过度依赖静态线索，如固定坐标或元素比例。这限制了GUI代理在长期应用中的有效性，因此需要开发适应动态环境的持续学习方法。",
      "method": "本文引入了GUI-Anchoring in Flux (GUI-AiF)，一个基于强化微调的框架，旨在稳定GUI代理的持续学习。核心创新点在于两种新奖励：Anchoring Point Reward in Flux (APR-iF)和Anchoring Region Reward in Flux (ARR-iF)。这些奖励引导代理对齐变化的交互点和区域，防止现有奖励策略过度适应静态基础线索。该方法通过强化学习微调代理，使用设计的奖励来处理分布变化，无需具体数据集或模型架构细节。",
      "result": "广泛实验表明，GUI-AiF在持续学习任务中超越最先进的基线方法。尽管摘要未提供具体性能指标数据，但结果展示了GUI-AiF在稳定代理表现方面的优越性，有效缓解了分布变化导致的性能下降。与现有基线相比，该方法通过奖励机制显著提升了适应性，为GUI代理的持续学习树立了新标杆。",
      "conclusion": "本文建立了首个GUI代理的持续学习框架，展示了强化微调在该领域的潜力。主要贡献在于解决了动态环境中代理适应性问题，具有学术价值，为未来持续学习研究提供了基础。潜在局限性可能包括奖励设计的特定依赖，未来工作可扩展到更广泛的GUI场景或与其他学习方法结合。",
      "tags": [
        "Continual Learning",
        "GUI Agents",
        "Reinforcement Fine-Tuning",
        "Anchoring Rewards"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:33.624174Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20731",
    "title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks",
    "authors": [
      "Mae Sosto",
      "Delfina Sol Martinez Pandiani",
      "Laura Hollink"
    ],
    "abstract": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20731.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20731",
    "published": "2026-01-28T16:06:04Z",
    "updated": "2026-01-28T16:06:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过句完成任务研究大型语言模型如何复制性别和性取向的社会规范偏见，并量化了偏见在不同模型类型中的差异。",
      "motivation": "研究动机是探究大型语言模型在文本生成中是否反映社会规范偏见，特别是异性恋顺性别规范。这个问题重要，因为偏见可能导致模型输出不公平，影响实际应用。现有方法可能缺乏系统性测量偏见的框架，未充分量化不同模型在句完成任务中的表示失衡。因此，本文旨在开发多维指标来评估偏见，为改进模型公平性提供基础，解决LLMs潜在的社会影响问题。",
      "method": "研究方法基于英语句完成任务，将表示失衡操作化为四个可测量维度：情感、尊重、毒性和预测多样性。研究比较了三种主题类别（queer-marked、non-queer-marked、unmarked）和不同类型的语言模型，包括掩码语言模型和自回归语言模型。关键创新点在于通过系统化的量化指标来测量偏见，并分析模型架构（如开放访问与封闭访问）对偏见表现的影响。摘要未明确说明具体数据集，但实验涉及标准的语言模型评估框架。",
      "result": "实验结果显示，掩码语言模型对queer-marked主题产生最不利的情感、更高毒性和更负面的尊重。自回归语言模型部分减轻了这些偏见模式，表明模型类型影响偏见程度。此外，封闭访问的自回归语言模型对unmarked主题产生更多有害输出，提示偏见可能在不同模型间重新分配。这些发现量化了LLMs复制社会规范偏见的差异，并突出了模型特征的关键作用，但摘要未提供具体性能指标数据。",
      "conclusion": "结论指出，大型语言模型确实复制了社会规范假设，但偏见的形式和程度强烈依赖于特定模型特性，可能导致偏见重新分配而非消除。该研究贡献于理解模型偏见机制，为开发更公平的语言模型提供理论依据和实用指导。学术价值在于量化分析模型偏见，实际应用有助于促进AI伦理和公平性。未来工作可探索模型优化方法以减少偏见，并扩展到其他社会维度进行分析。",
      "tags": [
        "Large Language Model",
        "Masked Language Model",
        "Autoregressive Language Model",
        "Bias Detection",
        "Sentence Completion Task"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:04.287075Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20730",
    "title": "AgentLongBench: A Controllable Long Benchmark For Long-Contexts Agents via Environment Rollouts",
    "authors": [
      "Shicheng Fang",
      "Yuxin Wang",
      "XiaoRan Liu",
      "Jiahao Lu",
      "Chuanyuan Tan",
      "Xinchi Chen",
      "Yining Zheng. Xuanjing Huang",
      "Xipeng Qiu"
    ],
    "abstract": "The evolution of Large Language Models (LLMs) into autonomous agents necessitates the management of extensive, dynamic contexts. Current benchmarks, however, remain largely static, relying on passive retrieval tasks that fail to simulate the complexities of agent-environment interaction, such as non-linear reasoning and iterative feedback. To address this, we introduce \\textbf{AgentLongBench}, which evaluates agents through simulated environment rollouts based on Lateral Thinking Puzzles. This framework generates rigorous interaction trajectories across knowledge-intensive and knowledge-free scenarios. Experiments with state-of-the-art models and memory systems (32K to 4M tokens) expose a critical weakness: while adept at static retrieval, agents struggle with the dynamic information synthesis essential for workflows. Our analysis indicates that this degradation is driven by the minimum number of tokens required to resolve a query. This factor explains why the high information density inherent in massive tool responses poses a significantly greater challenge than the memory fragmentation typical of long-turn dialogues.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20730.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20730",
    "published": "2026-01-28T16:05:44Z",
    "updated": "2026-01-28T16:05:44Z",
    "comment": "26 pages",
    "light_analysis": {
      "overview": "本论文的核心贡献是提出了AgentLongBench，一个基于环境滚动的新型可控长基准，用于评估长上下文代理的动态交互能力。",
      "motivation": "研究动机源于大型语言模型进化为自主代理时需要管理扩展的动态上下文。当前基准主要是静态的，依赖于被动检索任务，无法有效模拟代理与环境交互的复杂性，如非线性推理和迭代反馈。这种局限性导致现有方法难以评估代理在实际工作流中的表现，从而阻碍了代理系统的发展，因此需要一个更动态的评估框架来弥补这一不足。",
      "method": "研究方法引入了AgentLongBench框架，通过基于横向思维谜题的模拟环境滚动来评估代理。该框架生成严格的交互轨迹，覆盖知识密集和知识自由两种场景，以模拟真实世界的代理交互。关键创新在于使用环境滚动来评估代理在动态上下文中的表现，从而弥补传统静态基准的不足。实验中使用了先进模型和内存系统（32K到4M令牌），但具体数据集和模型架构摘要未明确说明。",
      "result": "主要实验结果表明，代理在静态检索任务中表现良好，但在动态信息合成方面存在显著困难。实验使用了32K到4M令牌的内存系统，暴露了代理在处理高信息密度的大规模工具响应时的挑战。分析表明，性能退化是由解决查询所需的最小令牌数驱动的，这解释了为什么大规模工具响应比长轮对话中的内存碎片化更具挑战性。与现有静态基准相比，AgentLongBench揭示了代理在动态环境中的弱点，但具体性能指标如准确率提升摘要未明确说明。",
      "conclusion": "本论文的主要贡献是提出了AgentLongBench，为评估长上下文代理的动态交互能力提供了一个新基准。学术价值在于它提供了更真实的评估框架，揭示了代理在动态信息合成中的不足。实际应用价值有助于指导代理系统的改进，特别是在处理复杂工作流时。未来工作方向可能包括优化代理的内存管理或开发更先进的交互策略，但具体局限性和未来方向摘要未明确说明。",
      "tags": [
        "Large Language Models",
        "Autonomous Agents",
        "Long-Context Benchmarking",
        "Environment Rollouts",
        "Lateral Thinking Puzzles"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:46.201623Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20729",
    "title": "Deep Semi-Supervised Survival Analysis for Predicting Cancer Prognosis",
    "authors": [
      "Anchen Sun",
      "Zhibin Chen",
      "Xiaodong Cai"
    ],
    "abstract": "The Cox Proportional Hazards (PH) model is widely used in survival analysis. Recently, artificial neural network (ANN)-based Cox-PH models have been developed. However, training these Cox models with high-dimensional features typically requires a substantial number of labeled samples containing information about time-to-event. The limited availability of labeled data for training often constrains the performance of ANN-based Cox models. To address this issue, we employed a deep semi-supervised learning (DSSL) approach to develop single- and multi-modal ANN-based Cox models based on the Mean Teacher (MT) framework, which utilizes both labeled and unlabeled data for training. We applied our model, named Cox-MT, to predict the prognosis of several types of cancer using data from The Cancer Genome Atlas (TCGA). Our single-modal Cox-MT models, utilizing TCGA RNA-seq data or whole slide images, significantly outperformed the existing ANN-based Cox model, Cox-nnet, using the same data set across four types of cancer considered. As the number of unlabeled samples increased, the performance of Cox-MT significantly improved with a given set of labeled data. Furthermore, our multi-modal Cox-MT model demonstrated considerably better performance than the single-modal model. In summary, the Cox-MT model effectively leverages both labeled and unlabeled data to significantly enhance prediction accuracy compared to existing ANN-based Cox models trained solely on labeled data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20729.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20729",
    "published": "2026-01-28T16:04:56Z",
    "updated": "2026-01-28T16:04:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Cox-MT模型，结合深度半监督学习与Cox-PH框架，有效利用未标注数据显著提升癌症预后预测准确率。",
      "motivation": "生存分析中，Cox比例风险模型广泛用于预测事件时间，但基于人工神经网络的Cox模型（如Cox-nnet）训练需大量标注样本，标注数据获取困难且成本高，这在高维特征场景下尤其限制模型性能。现有方法主要依赖标注数据，未充分利用未标注数据，导致预测准确性受限，而癌症预后预测对医疗决策至关重要，因此需开发能缓解标注数据不足问题的新方法。",
      "method": "研究采用深度半监督学习方法，基于Mean Teacher框架开发Cox-MT模型，该模型结合标注和未标注数据进行训练，通过一致性正则化增强泛化能力。具体技术路线包括构建单模态模型（使用TCGA的RNA-seq数据或全切片图像）和多模态模型（融合多种数据类型），模型架构以人工神经网络扩展Cox-PH模型，优化损失函数以处理高维特征并有效利用未标注样本提升学习效率。",
      "result": "在The Cancer Genome Atlas数据集上的实验表明，单模态Cox-MT模型使用RNA-seq数据或全切片图像时，在预测多种癌症预后方面显著优于基准模型Cox-nnet。随着未标注样本数量增加，给定标注数据下模型性能进一步提升，同时多模态Cox-MT模型通过融合不同数据模态实现了比单模态更好的预测效果，证明了半监督学习和多模态策略在提升生存分析准确性上的有效性。",
      "conclusion": "本研究的主要贡献是提出了Cox-MT模型，通过深度半监督学习框架成功解决了生存分析中标注数据不足的瓶颈，学术上推动了半监督学习在生存分析领域的应用，实际应用中为癌症预后提供了更准确的预测工具。未来工作可扩展模型到其他疾病类型或更多数据模态，并进一步探索模型优化和泛化能力。",
      "tags": [
        "Survival Analysis",
        "Deep Semi-Supervised Learning",
        "Mean Teacher",
        "Cox Proportional Hazards",
        "Multi-modal Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:05.730593Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20720",
    "title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction",
    "authors": [
      "Matej Halinkovic",
      "Nina Masarykova",
      "Alexey Vinel",
      "Marek Galinski"
    ],
    "abstract": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20720.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20720",
    "published": "2026-01-28T15:53:32Z",
    "updated": "2026-01-28T15:53:32Z",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "light_analysis": {
      "overview": "Li-ViP3D++通过查询门控可变形融合实现了端到端的摄像头-LiDAR多模态融合，提升了自动驾驶感知和轨迹预测的鲁棒性。",
      "motivation": "自动驾驶领域从原始传感器数据进行端到端感知和轨迹预测是关键需求，但现有模块化管道限制了信息流动并可能放大上游误差。尽管查询基础的端到端感知-预测模型有所改进，但摄像头和LiDAR在查询空间中的互补性尚未充分探索。现有融合方法常依赖启发式对齐和离散选择步骤，导致信息利用不足并引入偏差，这限制了系统整体性能的提升，因此开发更有效的多模态融合方案至关重要。",
      "method": "论文提出Li-ViP3D++框架，引入查询门控可变形融合技术，在查询空间内集成多视图RGB和LiDAR数据。具体技术包括：通过掩码注意力跨摄像头和特征层聚合图像证据；利用可学习的每查询偏移进行完全可微的BEV采样提取LiDAR上下文；并应用查询条件门控自适应加权每个代理的视觉和几何线索。该架构在nuScenes数据集上联合优化检测、跟踪和多假设轨迹预测，实现了端到端训练。",
      "result": "在nuScenes数据集上，Li-ViP3D++显著提升性能，EPA达到0.335，mAP为0.502，同时误报率降低至0.147。运行时间也从之前的145.91毫秒减少到139.82毫秒，表明它在检测质量、鲁棒性和效率方面优于先前的Li-ViP3D变体，实现了更高的准确性和更低的错误率。",
      "conclusion": "本研究的主要贡献是提出了查询门控可变形融合方法，有效结合摄像头和LiDAR优势，增强了端到端感知-预测系统的鲁棒性。学术上，它验证了查询空间完全可微融合的潜力；实际应用中，可提升自动驾驶部署能力。摘要未明确说明局限性或未来工作，但可能涉及进一步优化融合策略或扩展到更复杂场景。",
      "tags": [
        "Query-Based Models",
        "Deformable Attention",
        "Camera-LiDAR Fusion",
        "End-to-End Learning",
        "Trajectory Prediction"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:11.307760Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20714",
    "title": "Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions",
    "authors": [
      "Raul de la Rosa",
      "Ivana Dusparic",
      "Nicolas Cardozo"
    ],
    "abstract": "Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments to learning and exploration hyperparameters, MORPHIN adapts agents to changes in both the reward function and on-the-fly expansions of the agent's action space, while preserving prior policy knowledge to prevent catastrophic forgetting. We validate our approach using a Gridworld benchmark and a traffic signal control simulation. The results demonstrate that MORPHIN achieves superior convergence speed and continuous adaptation compared to a standard Q-learning baseline, improving learning efficiency by up to 1.7x.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20714.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20714",
    "published": "2026-01-28T15:46:51Z",
    "updated": "2026-01-28T15:46:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了MORPHIN自适应Q学习框架，使强化学习代理能在非平稳环境中在线适应奖励函数和动作空间变化，无需完全重新训练。",
      "motivation": "在实际应用中，强化学习代理常面临非平稳环境的挑战，特别是奖励函数变化或动作空间扩展时，标准Q学习方法难以适应，需要完全重新训练导致效率低下。问题重要性在于现实世界如交通控制频繁变化，代理需持续适应。现有方法缺乏处理概念漂移的机制，容易导致灾难性遗忘，影响长期学习性能，因此本研究旨在提升代理的适应能力和鲁棒性。",
      "method": "MORPHIN框架基于Q-learning，集成概念漂移检测和动态超参数调整。核心创新在于实时监控环境变化，如奖励函数偏移或动作空间扩展，并动态调整学习率和探索参数等超参数。这种方法使代理能快速适应新条件，同时保留先验策略知识防止灾难性遗忘。使用的验证数据集包括Gridworld基准和交通信号控制模拟，以评估方法的有效性。",
      "result": "实验结果显示，MORPHIN在Gridworld基准和交通信号控制模拟中，相比标准Q-learning基线，实现了更快的收敛速度和更优的连续适应能力。具体数据表明，学习效率提高达1.7倍，MORPHIN能迅速调整策略适应变化，而基线方法则需要更多资源和时间进行重新训练，验证了其在动态环境中的高效适应性。",
      "conclusion": "本研究的主要贡献是提出了MORPHIN自适应Q学习框架，有效解决了强化学习代理在非平稳环境中的适应问题。学术上，该方法集成概念漂移检测和动态调整，丰富了自适应RL领域研究。实际应用中，可提升交通控制等系统的实时适应性。局限性可能包括对其他环境变化的泛化能力，未来工作可扩展至更复杂任务或多代理场景。",
      "tags": [
        "Reinforcement Learning",
        "Q-learning",
        "Concept Drift Detection",
        "Dynamic Adaptation"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:58.070661Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20704",
    "title": "Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs",
    "authors": [
      "Melika Mobini",
      "Vincent Holst",
      "Floriano Tori",
      "Andres Algaba",
      "Vincent Ginis"
    ],
    "abstract": "Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\\approx$ 0.60) despite cleanly rejecting the random baseline ($\\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\\approx$ 0.83, and GNNs with embedding node features achieve 93\\% test accuracy on GPT vs.\\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\\ Claude $\\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20704.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20704",
    "published": "2026-01-28T15:37:31Z",
    "updated": "2026-01-28T15:37:31Z",
    "comment": "34 pages, 20 figures. Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "论文通过嵌入和图神经网络检测LLM生成的参考文献，发现其在语义上留下可检测的指纹，而结构特征不足以有效区分。",
      "motivation": "大型语言模型在学术领域被广泛应用于生成参考文献列表，但如何区分这些列表与人类创建的列表成为一个实际问题。这一问题的重要性在于，如果LLM生成的引用难以检测，可能影响学术诚信和引用准确性。现有方法可能侧重于分析引用图的结构特征，如中心性或聚类，但摘要指出仅靠结构特征区分效果有限，需要探索更有效的检测手段。",
      "method": "研究从SciSciNet数据集选取10,000篇焦点论文构建成对引用图，包括真实引用和GPT-4o生成的引用，并添加一个字段匹配的随机基线以保留出度和字段分布但打破潜在结构。方法的核心是比较结构节点特征（如度、中心性、聚类、边计数）与3072维标题/摘要嵌入特征。使用随机森林对图级聚合特征进行分析，并结合图神经网络处理节点特征，以评估不同特征的区分能力。",
      "result": "实验结果显示，仅使用结构特征区分GPT生成与真实引用的随机森林准确率约60%，而对随机基线的拒绝准确率可达89-92%。嵌入特征显著提升区分度：聚合嵌入的随机森林准确率达到约83%，而图神经网络结合嵌入节点特征在GPT与真实引用分类上实现93%的测试准确率。此外，复制实验使用Claude Sonnet 4.5和其他嵌入模型验证了结果的鲁棒性，随机森林对Claude与真实引用的区分准确率约77%。",
      "conclusion": "研究的主要贡献是揭示LLM生成的参考文献在引用拓扑结构上模拟人类，但在语义内容上存在可检测的偏差。这强调了检测和去偏置应关注内容信号而非全局图结构，对学术诚信和自动引用生成具有实际应用价值。虽然摘要未明确说明局限性，但未来工作可能涉及优化检测算法或扩展至更广泛的LLM和数据集，以进一步提高检测效果。",
      "tags": [
        "Large Language Model",
        "Graph Neural Network",
        "Embeddings",
        "Random Forest",
        "Citation Graphs"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:26.939907Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20696",
    "title": "Enterprise Resource Planning Using Multi-type Transformers in Ferro-Titanium Industry",
    "authors": [
      "Samira Yazdanpourmoghadam",
      "Mahan Balal Pour",
      "Vahid Partovi Nia"
    ],
    "abstract": "Combinatorial optimization problems such as the Job-Shop Scheduling Problem (JSP) and Knapsack Problem (KP) are fundamental challenges in operations research, logistics, and eterprise resource planning (ERP). These problems often require sophisticated algorithms to achieve near-optimal solutions within practical time constraints. Recent advances in deep learning have introduced transformer-based architectures as promising alternatives to traditional heuristics and metaheuristics. We leverage the Multi-Type Transformer (MTT) architecture to address these benchmarks in a unified framework. We present an extensive experimental evaluation across standard benchmark datasets for JSP and KP, demonstrating that MTT achieves competitive performance on different size of these benchmark problems. We showcase the potential of multi-type attention on a real application in Ferro-Titanium industry. To the best of our knowledge, we are the first to apply multi-type transformers in real manufacturing.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20696.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20696",
    "published": "2026-01-28T15:28:48Z",
    "updated": "2026-01-28T15:28:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文首次提出并应用多类型Transformer架构，在Ferro-Titanium工业的企业资源规划中解决组合优化问题。",
      "motivation": "该研究旨在解决企业资源规划中的组合优化问题，如Job-Shop Scheduling Problem和Knapsack Problem，这些是运营研究、物流和ERP的基础挑战，对工业效率至关重要。传统启发式或元启发式方法在复杂或大规模问题上可能效率不足，难以在时间限制内找到近优解。深度学习的发展提供了新途径，特别是Transformer架构能捕捉复杂关系，因此研究探索多类型Transformer作为替代方案，以提高求解能力。",
      "method": "论文提出使用Multi-Type Transformer架构，在统一框架中处理Job-Shop Scheduling Problem和Knapsack Problem基准问题。关键创新点在于多类型注意力机制，可能通过适应不同类型输入来增强模型性能。研究基于Transformer架构，并利用标准基准数据集进行实验评估，模型设计可能针对制造业数据进行了调整，以处理ERP中的复杂约束和优化目标。",
      "result": "实验在标准基准数据集上进行，结果显示Multi-Type Transformer在不同规模的Job-Shop Scheduling Problem和Knapsack Problem问题上表现出竞争性性能。摘要未明确说明具体性能指标，如准确率或效率改进，也未提供与基线方法的详细对比数据，但基于实验描述，MTT的性能可被视为与现有方法相当或有优势。",
      "conclusion": "论文主要贡献是展示了Multi-Type Transformer在组合优化问题中的有效性，并首次应用于Ferro-Titanium工业的真实制造业场景，推动深度学习在企业资源规划中的应用。研究具有学术价值，为AI优化方法提供了新思路，实际应用价值在于提高工业资源规划效率。潜在局限性包括未详细讨论模型扩展性，未来工作可能涉及优化模型性能、扩展到其他行业或探索更复杂的数据类型。",
      "tags": [
        "Multi-Type Transformer",
        "Combinatorial Optimization",
        "Job-Shop Scheduling Problem",
        "Knapsack Problem",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:45.275292Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20694",
    "title": "Is Pure Exploitation Sufficient in Exogenous MDPs with Linear Function Approximation?",
    "authors": [
      "Hao Liang",
      "Jiayu Cheng",
      "Sean R. Sinclair",
      "Yali Du"
    ],
    "abstract": "Exogenous MDPs (Exo-MDPs) capture sequential decision-making where uncertainty comes solely from exogenous inputs that evolve independently of the learner's actions. This structure is especially common in operations research applications such as inventory control, energy storage, and resource allocation, where exogenous randomness (e.g., demand, arrivals, or prices) drives system behavior. Despite decades of empirical evidence that greedy, exploitation-only methods work remarkably well in these settings, theory has lagged behind: all existing regret guarantees for Exo-MDPs rely on explicit exploration or tabular assumptions. We show that exploration is unnecessary. We propose Pure Exploitation Learning (PEL) and prove the first general finite-sample regret bounds for exploitation-only algorithms in Exo-MDPs. In the tabular case, PEL achieves $\\widetilde{O}(H^2|Ξ|\\sqrt{K})$. For large, continuous endogenous state spaces, we introduce LSVI-PE, a simple linear-approximation method whose regret is polynomial in the feature dimension, exogenous state space, and horizon, independent of the endogenous state and action spaces. Our analysis introduces two new tools: counterfactual trajectories and Bellman-closed feature transport, which together allow greedy policies to have accurate value estimates without optimism. Experiments on synthetic and resource-management tasks show that PEL consistently outperforming baselines. Overall, our results overturn the conventional wisdom that exploration is required, demonstrating that in Exo-MDPs, pure exploitation is enough.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20694.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20694",
    "published": "2026-01-28T15:23:50Z",
    "updated": "2026-01-28T15:23:50Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "JSON 解析失败: 结构化输出解析失败 (尝试 4 次): Invalid json output: {\n  \"overview\": \"该论文证明在外生马尔可夫决策过程（Exo-MDPs）中纯利用学习足够有效，并提出首个有限样本遗憾边界，颠覆了传统探索必须的观念。\",\n  \"motivation\": \"Exogenous MDPs（Exo-MDPs）在操作研究领域广泛应用，如库存控制和资源分配，其中外生随机性（如需求或价格）主导系统行为。尽管经验证据表明贪婪、纯利用方法在这些场景中表现良好，但现有理论依赖显式探索或表格状态假设，限制了理论支持。本研究旨在填补这一理论空白，探索纯利用学习的可行性，以解决实际应用中的序列决策问题。\",\n  \"method\": \"论文提出Pure Exploitation Learning（PEL）算法，针对Exo-MDPs实现纯利用学习。在表格情况下，PEL的遗憾边界为 $\\widetilde{O}(H^2|Ξ|\\sqrt{K})$；对于连续内生状态空间，引入LSVI-PE方法，使用线性函数逼近，其遗憾仅与特征维度、外生状态空间和时域相关，独立于内生状态和动作空间。关键创新工具包括反事实轨迹和Bellman-闭特征传输，确保贪婪策略能准确估计价值而无须乐观估计。\",\n  \"result\": \"在合成和资源管理任务上的实验显示，PEL consistently outperforming baselines，验证了纯利用学习的有效性。摘要未明确说明具体准确率或效率改进的数据，但实验结果表明PEL在性能上超越基线方法，证实了理论分析中纯利用学习在外生MDPs中的优势。\",\n  \"conclusion\": \"本研究证明在外生MDPs中，纯利用学习足够有效，首次提供有限样本遗憾边界，填补了理论空白。这具有重要学术价值，为操作研究中的实际应用（如库存控制）提供了理论支持，可能启示未来工作扩展到更复杂场景或研究局限性。摘要未明确说明潜在局限性或具体未来方向。\",\n  \"tags\": [\"Exogenous MDPs\", \"Pure Exploitation Learning\", \"Linear Function Approximation\", \"Regret Analysis\", \"Counterfactual Trajectories\"]\n}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
  },
  {
    "id": "2601.20692",
    "title": "Optimal Transport Group Counterfactual Explanations",
    "authors": [
      "Enrique Valero-Leal",
      "Bernd Bischl",
      "Pedro Larrañaga",
      "Concha Bielza",
      "Giuseppe Casalicchio"
    ],
    "abstract": "Group counterfactual explanations find a set of counterfactual instances to explain a group of input instances contrastively. However, existing methods either (i) optimize counterfactuals only for a fixed group and do not generalize to new group members, (ii) strictly rely on strong model assumptions (e.g., linearity) for tractability or/and (iii) poorly control the counterfactual group geometry distortion. We instead learn an explicit optimal transport map that sends any group instance to its counterfactual without re-optimization, minimizing the group's total transport cost. This enables generalization with fewer parameters, making it easier to interpret the common actionable recourse. For linear classifiers, we prove that functions representing group counterfactuals are derived via mathematical optimization, identifying the underlying convex optimization type (QP, QCQP, ...). Experiments show that they accurately generalize, preserve group geometry and incur only negligible additional transport cost compared to baseline methods. If model linearity cannot be exploited, our approach also significantly outperforms the baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20692.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20692",
    "published": "2026-01-28T15:22:20Z",
    "updated": "2026-01-28T15:22:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出学习一个显式最优传输映射，用于生成群组反事实解释，实现泛化并最小化传输成本。",
      "motivation": "群组反事实解释旨在为一组输入实例找到对比性解释集，以揭示模型决策。现有方法存在不足：一是仅优化固定群组，无法泛化到新成员；二是依赖线性等强模型假设以保证可解性；三是难以控制群组几何结构的失真。因此，需要一种能泛化、参数少且易于解释的改进方法，以克服这些局限并提高实际应用的灵活性。",
      "method": "论文提出学习一个显式的最优传输映射，该映射能直接将任何群组实例发送到其反事实，无需重新优化，从而最小化总传输成本。对于线性分类器，通过数学优化推导群组反事实函数，并识别其属于特定凸优化类型（如二次规划或二次约束二次规划）。即使模型不具备线性性，该方法仍能有效应用，保持泛化能力并减少几何失真。",
      "result": "实验结果表明，该方法能准确泛化到新群组成员，有效保持群组几何结构，且相比基线方法仅产生可忽略的额外传输成本。在模型不具备线性性的情况下，该方法仍显著优于基线，展示了鲁棒的性能提升和较低的计算开销，验证了其在实际应用中的有效性。",
      "conclusion": "本论文的主要贡献是提出基于最优传输的群组反事实解释方法，实现泛化和几何保持。学术价值在于为线性分类器提供数学优化基础，增强解释性；实际应用价值在于通过减少参数和易于解释的映射，促进行动建议的制定。未来工作可能涉及扩展到非线性模型或更广泛的应用场景，以进一步验证其普适性。",
      "tags": [
        "Optimal Transport",
        "Counterfactual Explanations",
        "Group Counterfactuals",
        "Convex Optimization",
        "Linear Classifiers"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:10.226764Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20689",
    "title": "Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework",
    "authors": [
      "Xinyue Li",
      "Zhichao Zhang",
      "Zhiming Xu",
      "Shubo Xu",
      "Xiongkuo Min",
      "Yitong Chen",
      "Guangtao Zhai"
    ],
    "abstract": "Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20689.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20689",
    "published": "2026-01-28T15:15:17Z",
    "updated": "2026-01-28T15:15:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了标签高效的LEAF框架，通过从多模态大语言模型蒸馏感知质量先验到轻量回归器，降低图像质量评估中的人类标注需求。",
      "motivation": "多模态大语言模型在图像质量评估任务中展现了强大能力，但适应这些模型计算成本高，且严重依赖大量平均意见分数注释。核心瓶颈在于MOS尺度校准，而非模型的感知能力。现有方法面临标注成本高、实用性受限的挑战，因此需要开发标签高效的框架，以减少注释负担并提升IQA在资源有限场景中的可行性。",
      "method": "LEAF框架通过解耦感知与校准，从MLLM教师模型蒸馏感知质量先验到轻量学生回归器。关键创新包括使用点状质量判断和成对偏好比较进行密集监督，并评估决策可靠性；学生通过联合蒸馏学习教师的感知模式，并在小规模MOS子集上进行校准，以对齐人类标注。该方法减少了训练中对大规模注释的依赖，实现了高效的知识迁移。",
      "result": "在用户生成和AI生成的图像质量评估基准测试中，LEAF方法显著降低了人类标注需求，同时保持了与平均意见分数的强对齐相关性。实验结果表明，该方法在减少注释量的情况下，仍能维持高效的评估性能，优于依赖大规模注释的传统MLLM方法，突显了标签高效框架的有效性和实用性。摘要未提供具体数值，但强调了性能的显著改进。",
      "conclusion": "LEAF框架的主要贡献是提出了一种标签高效的图像质量评估方法，通过解耦感知与校准，从MLLM蒸馏知识，大幅减少了对MOS注释的依赖。学术价值在于创新性地结合了蒸馏和校准策略，推动IQA领域向轻量化发展；实际应用中，有助于在有限标注预算下部署轻量级IQA模型。未来工作可探索在更多数据集上的泛化能力或优化校准机制。",
      "tags": [
        "Multimodal Large Language Models",
        "Image Quality Assessment",
        "Distillation",
        "Regression",
        "Calibration"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:20.291494Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20687",
    "title": "Positive-Unlabeled Reinforcement Learning Distillation for On-Premise Small Models",
    "authors": [
      "Zhiqiang Kou",
      "Junyang Chen",
      "Xin-Qiang Cai",
      "Xiaobo Xia",
      "Ming-Kun Xie",
      "Dong-Dong Wu",
      "Biao Liu",
      "Yuheng Jia",
      "Xin Geng",
      "Masashi Sugiyama",
      "Tat-Seng Chua"
    ],
    "abstract": "Due to constraints on privacy, cost, and latency, on-premise deployment of small models is increasingly common. However, most practical pipelines stop at supervised fine-tuning (SFT) and fail to reach the reinforcement learning (RL) alignment stage. The main reason is that RL alignment typically requires either expensive human preference annotation or heavy reliance on high-quality reward models with large-scale API usage and ongoing engineering maintenance, both of which are ill-suited to on-premise settings. To bridge this gap, we propose a positive-unlabeled (PU) RL distillation method for on-premise small-model deployment. Without human-labeled preferences or a reward model, our method distills the teacher's preference-optimization capability from black-box generations into a locally trainable student. For each prompt, we query the teacher once to obtain an anchor response, locally sample multiple student candidates, and perform anchor-conditioned self-ranking to induce pairwise or listwise preferences, enabling a fully local training loop via direct preference optimization or group relative policy optimization. Theoretical analysis justifies that the induced preference signal by our method is order-consistent and concentrates on near-optimal candidates, supporting its stability for preference optimization. Experiments demonstrate that our method achieves consistently strong performance under a low-cost setting.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20687.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20687",
    "published": "2026-01-28T15:14:50Z",
    "updated": "2026-01-28T15:14:50Z",
    "comment": "22 pages, 8 figures, 7 tables",
    "light_analysis": {
      "overview": "提出正未标注强化学习蒸馏方法，使本地小模型在无需人类偏好标注或奖励模型的情况下实现强化学习对齐。",
      "motivation": "随着隐私、成本和延迟约束增加，小模型在本地部署日益普及，但强化学习对齐通常依赖昂贵人类偏好标注或高成本奖励模型，这些方法需要大规模API使用和持续工程维护，不适合资源受限的本地环境。本研究旨在弥合监督微调与强化学习对齐之间的差距，为小模型本地部署提供高效、低成本的解决方案，解决现有方法在现实部署中的不适用性问题。",
      "method": "本研究提出正未标注强化学习蒸馏方法，无需人类标注偏好或奖励模型，从黑盒教师模型蒸馏偏好优化能力到本地可训练学生模型。核心步骤包括：为每个提示查询教师一次获取锚定响应，在本地采样多个学生候选响应，通过锚定条件的自排名诱导成对或列表偏好，然后使用直接偏好优化或组相对策略优化进行完全本地训练。关键创新在于实现自监督对齐，避免外部依赖，适合低成本设置。",
      "result": "实验结果表明，在低成本设置下，该方法实现了一致强性能。摘要未提供具体性能指标（如准确率提升百分比），但理论分析证实诱导的偏好信号顺序一致并集中于近最优候选，支持其稳定性。与基线方法相比，可能显著降低资源消耗和成本，但对比细节摘要未明确说明。",
      "conclusion": "本研究主要贡献是提出了正未标注强化学习蒸馏方法，解决了小模型本地部署中强化学习对齐的挑战。学术价值在于发展了新的自监督对齐技术，实际应用价值在于降低成本、增强隐私保护，促进模型在本地环境中的高效部署。未来工作可能探索更复杂场景或方法扩展，但局限性摘要未明确说明。",
      "tags": [
        "Positive-Unlabeled Learning",
        "Reinforcement Learning Distillation",
        "Direct Preference Optimization",
        "Group Relative Policy Optimization",
        "On-Premise Deployment"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:38.707819Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20686",
    "title": "MuRAL-CPD: Active Learning for Multiresolution Change Point Detection",
    "authors": [
      "Stefano Bertolasi",
      "Diego Carrera",
      "Diego Stucchi",
      "Pasqualina Fragneto",
      "Luigi Amedeo Bianchi"
    ],
    "abstract": "Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20686.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20686",
    "published": "2026-01-28T15:14:37Z",
    "updated": "2026-01-28T15:14:37Z",
    "comment": "Presented at 2025 IEEE International Conference on Data Mining (ICDM), to appear in the Proceedings",
    "light_analysis": {
      "overview": "本文提出 MuRAL-CPD，一种结合主动学习和多分辨率分析的新型半监督变化点检测方法，旨在提高检测准确性和可解释性。",
      "motivation": "变化点检测在时间序列分析中至关重要，但传统方法主要依赖无监督技术，缺乏对任务特定变化定义的适应性，且无法利用用户知识。这导致检测结果可能不符合实际需求，尤其在复杂场景下表现不佳。研究动机是解决传统方法灵活性不足的问题，通过引入用户反馈来优化模型，以满足实际应用中更精准的检测需求。",
      "method": "MuRAL-CPD 采用小波基的多分辨率分解来检测时间序列中多个时间尺度的变化点。核心创新在于整合主动学习，通过用户反馈迭代优化关键超参数，如阈值或模型参数。该方法使模型能动态对齐用户对变化的定义，增强自适应性。技术特色包括半监督框架和多分辨率分析的结合，使用户能在有限监督下高效指导模型训练。",
      "result": "实验在多个真实世界数据集上进行，结果显示 MuRAL-CPD 相比最先进方法更有效，特别是在监督信号有限的情况下。它提高了变化点检测的准确性，例如在用户反馈辅助下可能减少误报率。摘要未明确说明具体性能数据如准确率提升百分比，但强调了模型在复杂场景中的优越性，表明了方法在实际应用中的潜力。",
      "conclusion": "研究的主要贡献是提出了一个半监督变化点检测框架，通过主动学习和多分辨率分析提升模型性能。学术价值在于扩展了用户交互在时间序列分析中的应用，实际应用价值在于改善工业监控或科学研究中的变化检测任务。局限性可能包括用户反馈的获取成本，未来工作可探索自动化反馈机制或扩展到更多数据类型。",
      "tags": [
        "Change Point Detection",
        "Active Learning",
        "Multiresolution Analysis",
        "Wavelet Decomposition",
        "Semi-supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:22.620779Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20680",
    "title": "Online Density-Based Clustering for Real-Time Narrative Evolution Monitorin",
    "authors": [
      "Ostap Vykhopen",
      "Viktoria Skorik",
      "Maxim Tereschenko",
      "Veronika Solopova"
    ],
    "abstract": "Automated narrative intelligence systems for social media monitoring face significant scalability challenges when processing continuous data streams using traditional batch clustering algorithms. We investigate the replacement of HDBSCAN (offline clustering) with online (streaming/incremental) clustering methods in a production narrative report generation pipeline. The proposed system employs a three-stage architecture (data collection, modeling, dashboard generation) that processes thousands of multilingual social media documents daily. While HDBSCAN excels at discovering hierarchical density-based clusters and handling noise, its batch-only nature necessitates complete retraining for each time window, resulting in memory constraints, computational inefficiency, and inability to adapt to evolving narratives in real-time. This work evaluates a bunch of online clustering algorithms across dimensions of cluster quality preservation, computational efficiency, memory footprint, and integration compatibility with existing workflows. We propose evaluation criteria that balance traditional clustering metrics (Silhouette Coefficient, Davies-Bouldin Index) with narrative metrics (narrative distinctness, contingency and variance). Our methodology includes sliding-window simulations on historical datasets from Ukraine information space, enabling comparative analysis of algorithmic trade-offs in realistic operational contexts. This research addresses a critical gap between batch-oriented topic modeling frameworks and the streaming nature of social media monitoring, with implications for computational social science, crisis informatics, and narrative surveillance systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20680.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20680",
    "published": "2026-01-28T15:07:30Z",
    "updated": "2026-01-28T15:07:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出在线密度聚类方法，用于实时监控社交媒体叙事演变，并综合评估了多种在线聚类算法。",
      "motivation": "社交媒体监控中的自动化叙事情报系统在处理连续数据流时面临可扩展性挑战，传统批量聚类算法如HDBSCAN虽能有效发现层次密度聚类和处理噪声，但需完整重新训练每个时间窗口，导致内存限制、计算效率低下，且无法实时适应叙事演变。这一问题对实时危机信息学和叙事监控至关重要，因为现有方法不适应流式数据，限制了系统的实时响应能力和社会科学应用。",
      "method": "研究采用三阶段架构，包括数据收集、建模和仪表板生成，每日处理数千个多语言社交媒体文档。核心方法是评估一系列在线聚类算法，并提出平衡传统聚类指标（如Silhouette Coefficient和Davies-Bouldin Index）与叙事指标（如叙事独特性、连续性和方差）的评估标准。通过滑动窗口模拟在乌克兰信息空间的历史数据集上进行比较分析，以评估算法在实际操作环境中的权衡。",
      "result": "摘要中未明确说明具体实验结果数据，如准确率提升等。论文评估了在线聚类算法在聚类质量保持、计算效率、内存占用和与现有工作流集成兼容性等维度上的表现，并通过滑动窗口模拟进行对比分析，但未提供具体数值结果，旨在识别适合替代HDBSCAN的在线方法。",
      "conclusion": "该研究解决了批处理主题建模框架与社交媒体监控流式特性之间的关键差距，主要贡献在于提出在线密度聚类方法，并综合评估算法，为实时叙事监控提供解决方案。研究对计算社会科学、危机信息学和叙事监控系统有重要价值，可推动实时数据分析领域的发展。未来工作可优化算法性能或扩展到其他流式应用场景。",
      "tags": [
        "Online Clustering",
        "Density-Based Clustering",
        "HDBSCAN",
        "Narrative Monitoring",
        "Stream Processing"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:43.097063Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20679",
    "title": "ShieldedCode: Learning Robust Representations for Virtual Machine Protected Code",
    "authors": [
      "Mingqiao Mo",
      "Yunlong Tan",
      "Hao Zhang",
      "Heng Zhang",
      "Yangfan He"
    ],
    "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation, yet their potential for software protection remains largely untapped. Reverse engineering continues to threaten software security, while traditional virtual machine protection (VMP) relies on rigid, rule-based transformations that are costly to design and vulnerable to automated analysis. In this work, we present the first protection-aware framework that learns robust representations of VMP-protected code. Our approach builds large-scale paired datasets of source code and normalized VM implementations, and introduces hierarchical dependency modeling at intra-, preceding-, and inter-instruction levels. We jointly optimize language modeling with functionality-aware and protection-aware contrastive objectives to capture both semantic equivalence and protection strength. To further assess resilience, we propose a protection effectiveness optimization task that quantifies and ranks different VM variants derived from the same source. Coupled with a two-stage continual pre-training and fine-tuning pipeline, our method enables models to generate, compare, and reason over protected code. Extensive experiments show that our framework significantly improves robustness across diverse protection levels, opening a new research direction for learning-based software defense. In this work, we present ShieldedCode, the first protection-aware framework that learns robust representations of VMP-protected code. Our method achieves 26.95% Pass@1 on L0 VM code generation compared to 22.58% for GPT-4o., and improves binary similarity detection Recall@1 by 10% over state of art methods like jTrans.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20679.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20679",
    "published": "2026-01-28T15:07:08Z",
    "updated": "2026-01-28T15:07:08Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "提出了首个保护感知框架ShieldedCode，通过大语言模型学习虚拟机器保护代码的鲁棒表示，以提高软件防御能力。",
      "motivation": "该研究旨在解决逆向工程对软件安全的持续威胁问题，传统虚拟机器保护方法依赖刚性、规则化的转换，设计成本高且易受自动化分析攻击。大语言模型在代码生成方面进展显著，但软件保护领域应用不足，因此需要开发一种能够学习鲁棒表示的新方法，以增强保护效果并降低脆弱性。",
      "method": "方法包括构建大规模配对数据集，整合源代码和规范化虚拟机器实现。引入层次依赖建模，覆盖指令内、前指令和指令间三个层次的依赖关系。通过联合优化语言建模与功能感知及保护感知的对比目标，捕获语义等价性和保护强度。此外，提出保护有效性优化任务，量化不同VM变体，并采用两阶段持续预训练和微调管道，使模型能够生成、比较和推理保护代码。",
      "result": "实验结果表明，在L0虚拟机器代码生成任务中，Pass@1达到26.95%，优于GPT-4o的22.58%。在二进制相似性检测中，Recall@1相比最先进方法如jTrans提高了10%。这显示了框架在不同保护级别上显著增强了鲁棒性，并提供了具体的数据支持对比基线方法的性能改进。",
      "conclusion": "ShieldedCode框架通过学习鲁棒表示，成功提升了虚拟机器保护代码的处理能力，为基于学习的软件防御开辟了新研究方向。其学术价值在于将大语言模型和对比学习技术应用于软件安全领域，潜在实际应用包括增强软件抗逆向工程能力。未来工作可能涉及扩展至更复杂保护场景或进一步优化模型效率。",
      "tags": [
        "Large Language Model",
        "Contrastive Learning",
        "Virtual Machine Protection",
        "Code Generation",
        "Binary Similarity Detection"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:57.294547Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20676",
    "title": "Efficient Multimodal Planning Agent for Visual Question-Answering",
    "authors": [
      "Zhuo Chen",
      "Xinyu Geng",
      "Xinyu Wang",
      "Yong Jiang",
      "Zhen Zhang",
      "Pengjun Xie",
      "Kewei Tu"
    ],
    "abstract": "Visual Question-Answering (VQA) is a challenging multimodal task that requires integrating visual and textual information to generate accurate responses. While multimodal Retrieval-Augmented Generation (mRAG) has shown promise in enhancing VQA systems by providing more evidence on both image and text sides, the default procedure that addresses VQA queries, especially the knowledge-intensive ones, often relies on multi-stage pipelines of mRAG with inherent dependencies. To mitigate the inefficiency limitations while maintaining VQA task performance, this paper proposes a method that trains a multimodal planning agent, dynamically decomposing the mRAG pipeline to solve the VQA task. Our method optimizes the trade-off between efficiency and effectiveness by training the agent to intelligently determine the necessity of each mRAG step. In our experiments, the agent can help reduce redundant computations, cutting search time by over 60\\% compared to existing methods and decreasing costly tool calls. Meanwhile, experiments demonstrate that our method outperforms all baselines, including a Deep Research agent and a carefully designed prompt-based method, on average over six various datasets. Code will be released.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20676.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20676",
    "published": "2026-01-28T14:58:59Z",
    "updated": "2026-01-28T14:58:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个多模态规划代理，通过动态分解 mRAG 管道，优化视觉问答任务的效率和效果，解决现有方法的效率限制。",
      "motivation": "Visual Question-Answering (VQA) 是一个挑战性任务，需融合视觉和文本信息生成准确答案。现有方法常依赖多阶段多模态检索增强生成 (mRAG) 管道，具有内在依赖性，导致处理知识密集型查询时效率低下，增加计算开销。mRAG 虽能提供更多证据，但固定流程不够灵活，影响系统响应速度和可扩展性，因此研究旨在平衡效率与性能，提升 VQA 系统的实用性。",
      "method": "本文方法训练一个多模态规划代理，动态分解 mRAG 管道以执行 VQA 任务。核心创新在于代理智能评估每个 mRAG 步骤的必要性，自适应调整流程，优化计算资源使用，避免冗余操作。摘要未明确说明具体训练算法或模型架构，但推断代理可能基于决策机制在效率和准确性间权衡，减少了固定管道的依赖，提高了处理灵活性。",
      "result": "实验结果显示，该规划代理能显著降低计算开销。与现有方法相比，搜索时间减少了超过 60%，同时减少了昂贵的工具调用次数。在性能方面，该方法平均在六个不同数据集上优于所有基线系统，包括深度研究代理和基于提示的方法，验证了其在保持 VQA 准确性的同时，大幅提升效率的能力。",
      "conclusion": "本研究提出了一种高效多模态规划代理，解决了 VQA 任务中 mRAG 管道的效率问题。贡献在于动态步骤分解，优化了效率与效果的平衡，为多模态 AI 提供了实用方案。学术上拓展了 mRAG 在 VQA 中的应用；实际中降低了计算成本，提高响应速度。代码将开源，促进未来研究，但潜在局限性或进一步优化可针对更复杂场景探索。",
      "tags": [
        "Visual Question-Answering (VQA)",
        "Multimodal Planning Agent",
        "Retrieval-Augmented Generation (mRAG)",
        "Efficiency Optimization",
        "Agent-based Systems"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:16.239891Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20675",
    "title": "bi-modal textual prompt learning for vision-language models in remote sensing",
    "authors": [
      "Pankhi Kashyap",
      "Mainak Singha",
      "Biplab Banerjee"
    ],
    "abstract": "Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20675.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20675",
    "published": "2026-01-28T14:58:14Z",
    "updated": "2026-01-28T14:58:14Z",
    "comment": "Accepted in ICASSP 2026",
    "light_analysis": {
      "overview": "论文提出BiMoRS框架，通过融合图像字幕生成的文本语义与视觉特征，增强视觉-语言模型在遥感图像任务中的泛化能力。",
      "motivation": "研究旨在解决提示学习在遥感图像应用中的局限性。遥感数据具有多标签场景、高类内变异性和多样空间分辨率等独特挑战，导致现有PL方法难以直接应用，尤其在识别主导语义线索和泛化到新类别时表现不佳。这些问题限制了视觉-语言模型在环境监测等实际应用中的效能，因此需要开发专门针对遥感任务的适应性方法。",
      "method": "BiMoRS采用一个冻结的图像字幕模型（如BLIP-2）从遥感图像中提取文本语义摘要，通过BERT分词器转换为token。这些文本token与CLIP编码器的高级视觉特征进行融合。然后，一个轻量级交叉注意力模块基于融合的文本-视觉表示条件化一个可学习的查询提示，生成上下文化提示，无需改变CLIP主干网络，从而保持模型的轻量性和效率。",
      "result": "在四个遥感数据集上，针对三个领域泛化任务进行评估，BiMoRS展现出一致的性能提升。与强基线方法相比，平均准确率提高了最多2%，验证了其在遥感图像分类中的有效性和泛化能力，表明该方法在有限监督下能更好地适应视觉-语言模型。",
      "conclusion": "BiMoRS框架的主要贡献是成功扩展提示学习到遥感领域，通过双模态融合提高了模型对复杂场景和新类别的处理能力，增强了视觉-语言模型的实用价值。这项研究为遥感图像分析提供了新方法，但潜在局限性包括对预训练图像字幕模型的依赖，未来工作可探索更多模态融合策略或跨领域应用。摘要未明确说明其他局限性。",
      "tags": [
        "Prompt Learning",
        "Vision-Language Models",
        "Remote Sensing",
        "Cross-Attention",
        "Image Captioning"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:29.518834Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20674",
    "title": "Harnessing Large Language Models for Precision Querying and Retrieval-Augmented Knowledge Extraction in Clinical Data Science",
    "authors": [
      "Juan Jose Rubio Jan",
      "Jack Wu",
      "Julia Ive"
    ],
    "abstract": "This study applies Large Language Models (LLMs) to two foundational Electronic Health Record (EHR) data science tasks: structured data querying (using programmatic languages, Python/Pandas) and information extraction from unstructured clinical text via a Retrieval Augmented Generation (RAG) pipeline. We test the ability of LLMs to interact accurately with large structured datasets for analytics and the reliability of LLMs in extracting semantically correct information from free text health records when supported by RAG. To this end, we presented a flexible evaluation framework that automatically generates synthetic question and answer pairs tailored to the characteristics of each dataset or task. Experiments were conducted on a curated subset of MIMIC III, (four structured tables and one clinical note type), using a mix of locally hosted and API-based LLMs. Evaluation combined exact-match metrics, semantic similarity, and human judgment. Our findings demonstrate the potential of LLMs to support precise querying and accurate information extraction in clinical workflows.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20674.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20674",
    "published": "2026-01-28T14:57:36Z",
    "updated": "2026-01-28T14:57:36Z",
    "comment": "11 pages, 5 figures",
    "light_analysis": {
      "overview": "本论文提出将大型语言模型结合检索增强生成技术应用于临床数据科学，实现电子健康记录中结构化数据查询和非结构化文本信息提取的自动化处理。",
      "motivation": "在临床数据科学中，电子健康记录（EHR）数据通常包含结构化和非结构化信息，需要高效处理以支持医疗决策。现有方法可能依赖手动规则或传统技术，缺乏灵活性且难以应对复杂查询和语义提取。本研究旨在解决这一问题，通过利用大型语言模型（LLMs）的潜力来提升数据处理的准确性和效率，尤其在临床领域，验证LLMs的可靠性和适应性对推动医疗自动化和数据驱动决策至关重要。",
      "method": "论文采用大型语言模型（LLMs）处理两个核心任务：一是通过Python/Pandas进行结构化数据查询，二是构建检索增强生成（RAG）管道从非结构化临床文本中提取信息。关键创新在于引入灵活的评估框架，自动生成针对不同数据集或任务特征的合成问答对。实验基于MIMIC III数据集的精选子集（包括四个结构化表和一个临床笔记类型），使用混合本地托管和API-based LLMs。评估方法综合了精确匹配指标、语义相似度分析和人类判断，以全面衡量模型性能。",
      "result": "研究结果表明，大型语言模型在临床工作流程中具有潜力，能够支持精确查询和准确信息提取。摘要未明确说明具体性能数据（如准确率或效率改进），也未提供与基线方法的详细对比。基于现有信息，实验通过评估框架展示了模型在处理复杂临床数据任务中的初步成功，但需要进一步实证研究来量化改进程度。",
      "conclusion": "本研究的主要贡献是证明了大型语言模型结合检索增强生成技术在临床数据科学中的应用潜力。学术上，这拓展了LLMs在医疗领域的应用场景，为自然语言处理与数据科学的交叉研究提供了新思路。实际上，该方法有助于提升临床工作流程的自动化水平，支持更高效的数据处理和决策辅助。未来的研究可能需要在更广泛的数据集和实际场景中验证性能，并探索模型优化和实际部署的挑战。",
      "tags": [
        "Large Language Model",
        "Retrieval-Augmented Generation (RAG)",
        "Electronic Health Records (EHR)",
        "Clinical Data Science",
        "MIMIC III"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:46.318219Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20666",
    "title": "Learning Contextual Runtime Monitors for Safe AI-Based Autonomy",
    "authors": [
      "Alejandro Luque-Cerpa",
      "Mengyuan Wang",
      "Emil Carlsson",
      "Sanjit A. Seshia",
      "Devdatt Dubhashi",
      "Hazem Torfah"
    ],
    "abstract": "We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20666.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20666",
    "published": "2026-01-28T14:49:04Z",
    "updated": "2026-01-28T14:49:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种新的上下文感知运行监控框架，利用上下文多臂老虎机技术，为AI自主系统提供安全控制器选择。",
      "motivation": "机器学习控制器在自主网络物理系统中广泛应用，但面对陌生环境时准确性急剧下降，引发严重安全问题。传统集成方法通过平均或投票提升鲁棒性，却稀释了个体控制器在不同操作上下文中的专门优势，导致性能下降。因此，研究旨在开发一种动态识别并利用这些上下文优势的监控框架，以提高系统的安全性和适应性，解决现有方法忽视上下文差异的不足。",
      "method": "论文将安全AI控制集合体设计重新定义为上下文监控问题，监控器持续观察系统上下文并选择最合适的控制器。核心方法是将监控学习建模为上下文学习任务，借鉴上下文多臂老虎机技术，确保控制器选择的理论安全保证，并优化控制器多样性的利用。实验在模拟自动驾驶场景中进行，以验证框架的有效性和技术特色。",
      "result": "在两个模拟自动驾驶场景的实验中，所提框架相比非上下文基线在安全性和性能上表现出显著改进。摘要未明确具体性能指标数值，但强调了改进的幅度和实际效果，表明该方法能有效提升系统的整体表现，验证了其相对于传统方法的优越性。",
      "conclusion": "该研究的主要贡献是提出了一个基于上下文学习的运行监控框架，为AI控制集合体提供了理论安全保证和更好的多样性利用。学术上，它将上下文多臂老虎机技术引入监控领域，拓展了安全AI研究方向；实际上，增强了自主系统的安全性和性能。未来工作可能涉及扩展应用场景和改进算法效率。",
      "tags": [
        "Contextual Learning",
        "Multi-Armed Bandits",
        "Runtime Monitoring",
        "Autonomous Driving",
        "Safety Guarantees"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:49.507766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20661",
    "title": "ProSkill: Segment-Level Skill Assessment in Procedural Videos",
    "authors": [
      "Michele Mazzamuto",
      "Daniele Di Mauro",
      "Gianpiero Francesca",
      "Giovanni Maria Farinella",
      "Antonino Furnari"
    ],
    "abstract": "Skill assessment in procedural videos is crucial for the objective evaluation of human performance in settings such as manufacturing and procedural daily tasks. Current research on skill assessment has predominantly focused on sports and lacks large-scale datasets for complex procedural activities. Existing studies typically involve only a limited number of actions, focus on either pairwise assessments (e.g., A is better than B) or on binary labels (e.g., good execution vs needs improvement). In response to these shortcomings, we introduce ProSkill, the first benchmark dataset for action-level skill assessment in procedural tasks. ProSkill provides absolute skill assessment annotations, along with pairwise ones. This is enabled by a novel and scalable annotation protocol that allows for the creation of an absolute skill assessment ranking starting from pairwise assessments. This protocol leverages a Swiss Tournament scheme for efficient pairwise comparisons, which are then aggregated into consistent, continuous global scores using an ELO-based rating system. We use our dataset to benchmark the main state-of-the-art skill assessment algorithms, including both ranking-based and pairwise paradigms. The suboptimal results achieved by the current state-of-the-art highlight the challenges and thus the value of ProSkill in the context of skill assessment for procedural videos. All data and code are available at https://fpv-iplab.github.io/ProSkill/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20661.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20661",
    "published": "2026-01-28T14:44:09Z",
    "updated": "2026-01-28T14:44:09Z",
    "comment": "Accepted at The IEEE/CVF Winter Conference on Applications of Computer Vision 2026",
    "light_analysis": {
      "overview": "该论文提出了ProSkill，这是第一个用于程序性视频中动作级技能评估的基准数据集，并引入了一种基于瑞士锦标赛和ELO评级的新颖注释协议。",
      "motivation": "技能评估在程序性视频中对于客观评价人类在制造和日常任务中的表现至关重要，但现有研究主要集中在体育领域，缺乏适用于复杂程序活动的大规模数据集。现有方法通常只涉及有限的动作，并专注于成对评估或二元标签，如‘A优于B’或‘良好执行与需改进’，无法提供更细致的绝对技能评估。这些不足限制了技能评估技术的发展和应用，导致评估精度和实用性受限。",
      "method": "研究团队构建了ProSkill数据集，提供动作级的绝对技能评估注释和成对注释。为实现这一点，他们设计了一种可扩展的注释协议：首先利用瑞士锦标赛方案进行高效的成对比较，以减少标注成本；然后使用基于ELO的评级系统将这些成对比较聚合为一致、连续的全局分数，从而从成对评估中生成绝对排名。这种方法支持更全面的技能评估，并用于对现有最先进算法进行基准测试，包括基于排名和成对的评估范式。",
      "result": "通过对现有最先进的技能评估算法在ProSkill数据集上进行基准测试，包括基于排名和成对评估的方法，结果表明这些算法取得了次优性能，未能有效处理复杂程序性视频的评估任务。这凸显了当前方法在技能评估领域的局限性，并突出了ProSkill作为基准数据集的价值，为未来研究提供了挑战和改进方向。摘要未明确说明具体准确率或效率指标，但次优结果强调了数据集对算法性能测试的重要性。",
      "conclusion": "该论文的主要贡献是引入了ProSkill基准数据集和一种创新的注释协议，填补了程序性视频技能评估领域缺乏大规模资源的空白。这为研究人员提供了宝贵的实验平台，促进了技能评估技术的发展，具有重要的学术价值和应用潜力，可用于改进制造和日常任务中的性能评估。未来工作可以基于此数据集开发更有效的评估算法，并探索其在更广泛场景中的应用，同时潜在局限性包括标注协议的扩展性和算法泛化能力。",
      "tags": [
        "Skill Assessment",
        "Procedural Videos",
        "ELO Rating",
        "Pairwise Comparison",
        "Benchmark Dataset"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:49.901957Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20659",
    "title": "A Dialectic Pipeline for Improving LLM Robustness",
    "authors": [
      "Sara Candussio"
    ],
    "abstract": "Assessing ways in which Language Models can reduce their hallucinations and improve the outputs' quality is crucial to ensure their large-scale use.   However, methods such as fine-tuning on domain-specific data or the training of a separate \\textit{ad hoc} verifier require demanding computational resources (not feasible for many user applications) and constrain the models to specific fields of knowledge.   In this thesis, we propose a dialectic pipeline that preserves LLMs' generalization abilities while improving the quality of its answer via self-dialogue, enabling it to reflect upon and correct tentative wrong answers.   We experimented with different pipeline settings, testing our proposed method on different datasets and on different families of models. All the pipeline stages are enriched with the relevant context (in an oracle-RAG setting) and a study on the impact of its summarization or its filtering is conducted.   We find that our proposed dialectic pipeline is able to outperform by significative margins the standard model answers and that it consistently achieves higher performances than Chain-of-Thought only prompting.",
    "categories": [
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20659.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20659",
    "published": "2026-01-28T14:42:49Z",
    "updated": "2026-01-28T14:42:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个基于自我对话的辩证法管道，以改善大型语言模型的鲁棒性和输出质量，同时保留其泛化能力。",
      "motivation": "该研究旨在解决大型语言模型（LLM）在生成文本时出现幻觉的问题，这是确保其大规模可靠应用的关键挑战。现有方法如基于领域特定数据的微调或训练独立验证器需要大量计算资源，不适合许多用户应用，并且限制了模型在广泛知识领域的泛化能力。因此，需要一种更通用、资源高效的方法来提高LLM的输出质量，避免对特定领域的依赖。摘要强调这些问题的重要性，并指出传统方法的不足之处，驱动了新方法的探索。",
      "method": "论文提出一个辩证法管道，通过让LLM进行自我对话来反思和修正其初步错误答案，从而减少幻觉并提高输出质量。方法的核心创新是保留模型的泛化能力，无需额外微调或训练独立组件。管道阶段在oracle-RAG设置中补充相关上下文，以提供额外信息，并研究了上下文摘要或过滤对性能的影响。实验在不同设置、多种数据集（摘要未明确说明具体名称）和不同模型家族上进行，验证了该方法的灵活性和有效性。",
      "result": "实验结果表明，所提出的辩证法管道显著优于标准模型生成的答案，性能提升幅度较大。具体地，它在一系列测试中一致地超越了仅使用Chain-of-Thought prompting的方法，显示出更高的输出质量和鲁棒性。摘要中未提供具体的准确率或效率数据，但强调了改进的显著性和一致性，表明该方法在减少幻觉和增强模型输出方面具有潜在优势。",
      "conclusion": "该研究的主要贡献是开发了一个辩证法管道，能有效减少LLM的幻觉并提高输出质量，同时保持模型的泛化能力，避免了传统方法的计算负担和领域限制。学术上，它提供了一种资源高效的方法来增强LLM鲁棒性；实际应用中，该方法可广泛用于各种领域，无需领域特定适配，促进LLM的可靠使用。未来工作可能包括进一步优化管道设置、扩展到更多模型类型或进行更全面的评估，以应对潜在局限性。",
      "tags": [
        "Large Language Model (LLM)",
        "Self-Dialogue",
        "Oracle-RAG",
        "Chain-of-Thought Prompting",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:58.006385Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20656",
    "title": "FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection",
    "authors": [
      "Diogo J. Paulo",
      "Hugo Proença",
      "João C. Neves"
    ],
    "abstract": "Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20656.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20656",
    "published": "2026-01-28T14:38:51Z",
    "updated": "2026-01-28T14:38:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了FD-MAD方法，通过频率域残差分析和马尔可夫随机场的结构化区域融合来检测人脸变形攻击，显著提升了跨数据集的检测性能。",
      "motivation": "人脸变形攻击对电子身份注册和边境控制中的脸部识别系统构成严重威胁，尤其是在单图像变形攻击检测（S-MAD）场景中，由于缺乏可信参考，检测更为困难。现有方法在跨数据集和跨变形设置中表现不佳，导致检测系统鲁棒性不足，亟需开发一种轻量级且有效的方法来提高检测精度和泛化能力。",
      "method": "该方法基于频率域残差分析，核心创新包括：1)引入残差频率域概念，将信号的频率从自然谱衰减中解耦，以更好地区分变形和真实数据；2)结合全局和局部推理，使用马尔可夫随机场（MRF）融合不同脸部区域的证据，以实现全局一致的决策。方法在合成变形攻击检测开发数据集（SMDD）上训练，仅依赖谱特征，避免了复杂的深度学习架构。",
      "result": "在FRLL-Morph和MAD22数据集上的跨数据集和跨变形评估中，该方法取得了显著效果：平均EER在FRLL-Morph上为1.85%，在MAD22上为6.12%（排名第二）。在低攻击呈现分类错误率（APCER）下，仍保持了良好的真实呈现分类错误率（BPCER），表明其性能优于强基线方法，并与深度S-MAD架构竞争。",
      "conclusion": "本文的主要贡献是提出了FD-MAD方法，通过傅里叶域残差建模和结构化区域融合，为S-MAD提供了一个轻量级且鲁棒的解决方案。这不仅提升了人脸变形攻击检测的学术价值，还具有实际应用潜力，增强了脸部识别系统的安全性。局限性未明确说明，未来工作可能涉及进一步优化或扩展到其他攻击场景。",
      "tags": [
        "Frequency-Domain Analysis",
        "Residual Analysis",
        "Face Morphing Attack Detection",
        "Markov Random Field",
        "S-MAD"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:13.780812Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20650",
    "title": "OS-Marathon: Benchmarking Computer-Use Agents on Long-Horizon Repetitive Tasks",
    "authors": [
      "Jing Wu",
      "Daphne Barretto",
      "Yiye Chen",
      "Nicholas Gydé",
      "Yanan Jian",
      "Yuhang He",
      "Vibhav Vineet"
    ],
    "abstract": "Long-horizon, repetitive workflows are common in professional settings, such as processing expense reports from receipts and entering student grades from exam papers. These tasks are often tedious for humans since they can extend to extreme lengths proportional to the size of the data to process. However, they are ideal for Computer-Use Agents (CUAs) due to their structured, recurring sub-workflows with logic that can be systematically learned. Identifying the absence of an evaluation benchmark as a primary bottleneck, we establish OS-Marathon, comprising 242 long-horizon, repetitive tasks across 2 domains to evaluate state-of-the-art (SOTA) agents. We then introduce a cost-effective method to construct a condensed demonstration using only few-shot examples to teach agents the underlying workflow logic, enabling them to execute similar workflows effectively on larger, unseen data collections. Extensive experiments demonstrate both the inherent challenges of these tasks and the effectiveness of our proposed method. Project website: https://os-marathon.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20650.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20650",
    "published": "2026-01-28T14:35:23Z",
    "updated": "2026-01-28T14:35:23Z",
    "comment": "22 Pages, Project Page: \\url{https://os-marathon.github.io/}",
    "light_analysis": {
      "overview": "本论文提出OS-Marathon基准评估计算机使用代理在长时域重复任务上的性能，并引入一种基于少量示例的成本效益教导方法。",
      "motivation": "长时域、重复的工作流在专业场景中非常普遍，例如处理收据报销单和录入学生成绩，这些任务对人类而言耗时且乏味，降低了工作效率。计算机使用代理因其结构化、可学习的特性而适合执行此类任务，但当前缺乏专门针对这些任务的评估基准，成为研究和应用的主要瓶颈。本文旨在通过建立标准化基准，解决这一问题，推动代理系统在自动化重复性工作流中的发展。",
      "method": "本文建立了OS-Marathon基准，包含242个长时域重复任务，覆盖两个不同领域，用于评估最先进的计算机使用代理。关键创新在于提出一种成本效益高的方法，仅使用少量示例构建浓缩演示，教导代理底层工作流逻辑。这种方法使代理能够系统学习并扩展到更大规模、未见过的数据集合上，有效执行类似任务。摘要未明确说明具体数据集或模型架构细节，但强调了few-shot学习技术的应用。",
      "result": "大量实验展示了长时域重复任务固有的挑战性，以及所提教导方法的有效性。摘要未明确说明具体性能指标如准确率提升或效率改进，但通过与现有方法对比，实验验证了该方法能帮助代理在未见数据上更好地执行任务，提高了适应性和执行效果。这为计算机使用代理在自动化工作流中的应用提供了实证支持，并揭示了任务复杂性和方法潜力。",
      "conclusion": "本论文的主要贡献是提出了OS-Marathon基准和一种基于few-shot示例的教导方法，有效填补了计算机使用代理在长时域重复任务评估上的空白。研究具有重要学术价值，为代理系统的发展提供了标准化测试平台，并在实际应用如自动化办公和数据处理中具有潜力。未来工作可考虑扩展任务领域、改进教导方法或探索更多代理技术，以进一步提升性能和应用范围。",
      "tags": [
        "Computer-Use Agents",
        "Long-Horizon Tasks",
        "Repetitive Workflows",
        "Benchmarking",
        "Few-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:47.513228Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20649",
    "title": "P2S: Probabilistic Process Supervision for General-Domain Reasoning Question Answering",
    "authors": [
      "Wenlin Zhong",
      "Chengyuan Liu",
      "Yiquan Wu",
      "Bovin Tan",
      "Changlong Sun",
      "Yi Wang",
      "Xiaozhong Liu",
      "Kun Kuang"
    ],
    "abstract": "While reinforcement learning with verifiable rewards (RLVR) has advanced LLM reasoning in structured domains like mathematics and programming, its application to general-domain reasoning tasks remains challenging due to the absence of verifiable reward signals. To this end, methods like Reinforcement Learning with Reference Probability Reward (RLPR) have emerged, leveraging the probability of generating the final answer as a reward signal. However, these outcome-focused approaches neglect crucial step-by-step supervision of the reasoning process itself. To address this gap, we introduce Probabilistic Process Supervision (P2S), a novel self-supervision framework that provides fine-grained process rewards without requiring a separate reward model or human-annotated reasoning steps. During reinforcement learning, P2S synthesizes and filters a high-quality reference reasoning chain (gold-CoT). The core of our method is to calculate a Path Faithfulness Reward (PFR) for each reasoning step, which is derived from the conditional probability of generating the gold-CoT's suffix, given the model's current reasoning prefix. Crucially, this PFR can be flexibly integrated with any outcome-based reward, directly tackling the reward sparsity problem by providing dense guidance. Extensive experiments on reading comprehension and medical Question Answering benchmarks show that P2S significantly outperforms strong baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20649.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20649",
    "published": "2026-01-28T14:35:20Z",
    "updated": "2026-01-28T14:35:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Probabilistic Process Supervision (P2S)框架，通过路径忠实性奖励提供推理过程的细粒度监督，以提升通用领域推理问答中强化学习的性能。",
      "motivation": "该研究旨在解决在通用领域推理问答中应用强化学习时的奖励稀疏性问题。现有方法如Reinforcement Learning with Verifiable Rewards (RLVR)在结构化领域有效，但在通用领域因缺乏可验证奖励信号而受限。后续方法如Reinforcement Learning with Reference Probability Reward (RLPR)使用最终答案生成概率作为奖励，但忽略了推理过程的逐步监督，导致性能不佳。通用领域任务如阅读理解和医学问答需要更精细的监督来改善推理质量和可靠性。",
      "method": "P2S框架是一个自监督方法，无需单独奖励模型或人工注释推理步骤。在强化学习中，它首先合成和过滤高质量的参考推理链（gold-CoT）。核心创新是计算路径忠实性奖励（PFR），该奖励基于给定模型当前推理前缀下生成gold-CoT后缀的条件概率，从而为每个推理步骤提供细粒度过程监督。PFR可以与任何基于结果的奖励灵活集成，直接解决奖励稀疏性问题，为模型训练提供密集引导。",
      "result": "在阅读理解和医学问答基准上进行的广泛实验表明，P2S框架显著优于强基线方法。摘要未明确说明具体性能指标如准确率提升百分比，但强调了其显著优势，展示了通过过程监督在提高推理准确性和鲁棒性方面的有效性，验证了该方法在通用领域推理任务中的改进潜力。",
      "conclusion": "P2S框架的主要贡献是引入概率过程监督方法，有效解决通用领域推理问答中的奖励稀疏问题，提供细粒度过程奖励而无需额外资源。其学术价值在于提出一种新颖的自监督框架，推动强化学习在推理任务中的应用；实际应用价值包括提升问答系统的性能。未来工作可能包括扩展到更复杂推理任务或优化奖励机制。",
      "tags": [
        "Probabilistic Process Supervision",
        "Reinforcement Learning",
        "Self-Supervision",
        "Reasoning Chain",
        "Question Answering"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:50.509380Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20642",
    "title": "Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability",
    "authors": [
      "Rohan Asthana",
      "Vasileios Belagiannis"
    ],
    "abstract": "Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20642.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20642",
    "published": "2026-01-28T14:29:42Z",
    "updated": "2026-01-28T14:29:42Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "论文通过分析对数概率的各向异性，提出了一种高效的扩散模型记忆化检测指标和缓解策略。",
      "motivation": "扩散模型在生成高保真图像时容易发生记忆化现象，即无意中复制训练图像的精确副本或部分，这可能导致隐私泄露和模型过拟合。现有方法主要基于分数差异的范数作为指标，假设等向性对数概率分布，这在低噪声水平下可能失效。因此，研究需要更准确的检测方法来处理各向异性情况，以提高模型的泛化能力和伦理合规性。",
      "method": "研究开发了一个新的记忆化检测指标，整合了等向性范数和各向异性对齐。关键创新在于分析低噪声设置下指导向量与无条件分数之间的强角度对齐，以识别记忆化样本。该方法直接通过两个条件和非条件前向传递在纯噪声输入上计算，无需迭代去噪步骤，减少了计算成本，并在Stable Diffusion模型上进行了验证。",
      "result": "在Stable Diffusion v1.4和v2上的检测实验表明，提出的指标优于现有的免去噪检测方法。具体性能提升体现在速度上，比先前最佳方法快至少约5倍，同时保持了更高的检测准确性，显示了方法在效率和效果上的优势。",
      "conclusion": "论文的主要贡献是提出了基于对数概率各向异性的记忆化检测指标和缓解策略。学术上，这为扩散模型的记忆化问题提供了新视角；实际应用中，有助于保护训练数据隐私和提升模型安全性。未来工作可能涉及扩展到其他生成模型或优化缓解策略。",
      "tags": [
        "Diffusion Models",
        "Memorization Detection",
        "Anisotropy",
        "Log-Probability",
        "Stable Diffusion"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:43.053759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20641",
    "title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models",
    "authors": [
      "Boaz Carmeli",
      "Orr Paradise",
      "Shafi Goldwasser",
      "Yonatan Belinkov",
      "Ron Meir"
    ],
    "abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20641.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20641",
    "published": "2026-01-28T14:28:31Z",
    "updated": "2026-01-28T14:28:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究探讨视觉语言模型在协作推理任务中开发高效且隐蔽的任务导向通信协议的能力，揭示了其潜力和风险。",
      "motivation": "该研究旨在解决基于LLM的代理在协作任务中能否开发出与标准自然语言不同的任务导向通信协议。这一问题的重要性在于，如果代理能够发展出更高效的通信方式，可能会提升任务性能，但同时也可能产生隐蔽性协议，导致外部观察者难以理解，引发对AI系统透明度和控制方面的担忧。现有方法可能缺乏对此类自发协议发展的系统性评估，因此需要探究其机制和影响。",
      "method": "研究方法采用参考游戏框架，在这个框架中，视觉语言模型代理进行通信，提供了一个受控且可测量的环境来评估语言变体。通过这一设置，研究者能够观察代理如何自发发展出任务导向的通信协议，关键创新点在于使用参考游戏作为实验平台，量化分析协议的效率和隐蔽性。摘要未明确说明具体的数据集或模型架构细节，但强调了框架的受控特性。",
      "result": "实验结果显示，视觉语言模型能够发展出有效的、适应任务的通信模式，并能产生对人类和外部代理难以解释的隐蔽协议。同时，观察到相似模型之间无需明确共享协议就能实现自发协调，这表明代理在协作中可能形成隐含的通信规则。这些发现基于参考游戏中的通信行为评估，但摘要未提供具体的性能指标对比数据，如准确率或效率提升百分比。",
      "conclusion": "该研究的主要贡献在于揭示了任务导向通信在视觉语言模型中的潜力和风险，包括效率提升与隐蔽性带来的透明度和控制挑战。学术价值在于为理解和评估自发通信协议的发展提供了新的实验方法，实际应用价值在于促进对AI系统可信性和安全性的进一步研究。未来工作可以探索如何解释或控制这些协议，以减轻潜在风险。",
      "tags": [
        "Vision-Language Models",
        "LLM-based Agents",
        "Task-Oriented Communication",
        "Referential Games",
        "Communication Protocols"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:53.414645Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20637",
    "title": "An Empirical Investigation of Neural ODEs and Symbolic Regression for Dynamical Systems",
    "authors": [
      "Panayiotis Ioannou",
      "Pietro Liò",
      "Pietro Cicuta"
    ],
    "abstract": "Accurately modelling the dynamics of complex systems and discovering their governing differential equations are critical tasks for accelerating scientific discovery. Using noisy, synthetic data from two damped oscillatory systems, we explore the extrapolation capabilities of Neural Ordinary Differential Equations (NODEs) and the ability of Symbolic Regression (SR) to recover the underlying equations. Our study yields three key insights. First, we demonstrate that NODEs can extrapolate effectively to new boundary conditions, provided the resulting trajectories share dynamic similarity with the training data. Second, SR successfully recovers the equations from noisy ground-truth data, though its performance is contingent on the correct selection of input variables. Finally, we find that SR recovers two out of the three governing equations, along with a good approximation for the third, when using data generated by a NODE trained on just 10% of the full simulation. While this last finding highlights an area for future work, our results suggest that using NODEs to enrich limited data and enable symbolic regression to infer physical laws represents a promising new approach for scientific discovery.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20637.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20637",
    "published": "2026-01-28T14:23:59Z",
    "updated": "2026-01-28T14:23:59Z",
    "comment": "Accepted at the Machine Learning and the Physical Sciences Workshop, NeurIPS 2025",
    "light_analysis": {
      "overview": "本研究通过实证探索神经ODE和符号回归在动力系统中的应用，提出结合两者以从有限数据中发现物理定律的创新方法。",
      "motivation": "准确建模复杂系统的动力学并发现其控制微分方程是加速科学发现的关键任务。现有方法在处理噪声数据和有限观测时面临挑战，例如难以从嘈杂数据中准确恢复方程，且数据稀缺限制了对新条件的泛化能力。本研究旨在通过比较神经ODE的外推能力和符号回归的方程恢复性能，探索如何利用合成数据克服这些不足，为科学建模提供更有效的工具。",
      "method": "本研究使用来自两个阻尼振荡系统的噪声合成数据，实证评估神经ODE（NODE）和符号回归（SR）的能力。方法核心包括：应用NODEs学习系统动力学并进行外推实验，以测试其在训练数据动态相似的新边界条件下的表现；同时，使用SR从噪声数据中恢复潜在的微分方程，并分析输入变量选择对性能的影响。关键创新在于结合NODEs生成的数据来丰富有限观测，以增强SR的方程发现过程。",
      "result": "实验结果揭示了三个关键发现：首先，NODEs在训练数据动态相似的新边界条件下能有效外推。其次，SR能从噪声真实数据中成功恢复方程，但其性能高度依赖于正确选择输入变量。最后，当使用仅10%完整模拟数据训练的NODE生成数据时，SR恢复了三个控制方程中的两个，并对第三个提供了良好近似。这表明NODEs有助于数据增强，提升了SR从有限信息中推断物理定律的能力。",
      "conclusion": "本研究的主要贡献在于证明了结合神经ODE和符号回归可以有效从有限数据中推断动力系统的物理定律，通过NODEs丰富数据为SR提供支持。学术上，它展示了机器学习在科学发现中的潜力；实践中，可用于加速复杂系统的分析和预测。未来工作可专注于优化数据生成策略或改进SR的方程恢复准确性，以克服当前局限性并扩展应用范围。",
      "tags": [
        "Neural ODEs",
        "Symbolic Regression",
        "Dynamical Systems",
        "Differential Equations",
        "Synthetic Data"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:01.184403Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20634",
    "title": "A Foundation Model for Virtual Sensors",
    "authors": [
      "Leon Götz",
      "Lars Frederik Peiss",
      "Erik Sauer",
      "Andreas Udo Sass",
      "Thorsten Bagdonat",
      "Stephan Günnemann",
      "Leo Schwinn"
    ],
    "abstract": "Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20634.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20634",
    "published": "2026-01-28T14:17:46Z",
    "updated": "2026-01-28T14:17:46Z",
    "comment": "18 pages in total, 15 figures",
    "light_analysis": {
      "overview": "该论文提出了首个虚拟传感器基础模型，通过统一架构实现多个传感器的同时预测，显著提升计算效率并减少内存消耗。",
      "motivation": "虚拟传感器利用机器学习预测目标信号以替代昂贵的物理传感器，在关键应用中具有重要价值。然而，现有方法需针对每个传感器定制模型，手动选择输入信号，无法利用任务间的协同效应，且缺乏统一的评估基准。同时，新兴时间序列基础模型计算成本高昂，仅能预测输入信号，不适用于虚拟传感器场景。这些不足导致部署效率低、成本高，限制了大规模应用，因此亟需一种更高效、可扩展的解决方案。",
      "method": "论文提出一个统一的基础模型，能够同时预测多样虚拟传感器，通过利用任务间的协同效应提高预测效率。关键创新在于模型能自动学习每个虚拟传感器的相关输入信号，无需专家知识进行手动选择，从而增加了可解释性。架构设计优化了计算和内存使用，参数数量几乎恒定，适合大规模扩展。在评估中，使用了标准基准和一个应用特定数据集，包含超过180亿个样本，以验证模型性能。",
      "result": "在大规模评估中，该模型在标准基准和应用数据集上实现了计算时间减少415倍、内存需求减少951倍的显著改进。预测质量与基线方法相比维持或甚至有所提升，验证了其高效性。模型可扩展至数百个虚拟传感器，参数数量保持稳定，适用于大规模传感器网络部署，显示了良好的实用性和可扩展性。",
      "conclusion": "该研究首次提出虚拟传感器基础模型，解决了现有方法的效率和可扩展性限制，通过减少计算和内存需求提高预测准确性。具有重要的学术价值，为虚拟传感器领域提供了新的研究方向，并具有实际应用价值，适用于工业监控等大规模场景。未来工作可能涉及进一步优化模型架构或扩展至更多应用领域，以推动技术发展。",
      "tags": [
        "Foundation Model",
        "Virtual Sensors",
        "Time Series Prediction",
        "Machine Learning",
        "Efficient Computing"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:25.310855Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20627",
    "title": "DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration",
    "authors": [
      "Gilles Eerlings",
      "Brent Zoomers",
      "Jori Liesenborgs",
      "Gustavo Rovelo Ruiz",
      "Kris Luyten"
    ],
    "abstract": "We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20627.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20627",
    "published": "2026-01-28T14:02:28Z",
    "updated": "2026-01-28T14:02:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "DIVERSE框架通过FiLM层和CMA-ES，无需重新训练，高效生成性能匹配但预测行为不同的神经网络模型。",
      "motivation": "研究动机是探索深度神经网络的Rashomon集，即那些与参考模型准确率相同但预测行为不同的模型集合。这一问题重要，因为它能揭示模型的不确定性和多样性，有助于构建更鲁棒的模型集合，提升模型解释性和可靠性。现有方法如重新训练多个模型，计算成本高且效率低，DIVERSE旨在提供一种系统化且高效的探索方式，以克服传统方法的不足，支持实际应用中模型多样性的需求。",
      "method": "研究方法基于预训练模型，通过添加Feature-wise Linear Modulation (FiLM)层进行特征调制，然后使用Covariance Matrix Adaptation Evolution Strategy (CMA-ES)在潜在调制空间中搜索。关键创新点在于，无需重新训练模型或访问梯度，就能生成多样化的模型变体。具体技术包括利用FiLM层实现灵活的线性调制，以及CMA-ES这种进化策略来优化调制参数，从而高效探索Rashomon集，避免了传统训练的高成本。",
      "result": "在MNIST、PneumoniaMNIST和CIFAR-10数据集上的实验结果显示，DIVERSE能够发现多个高性能且功能不同的模型变体，展现出有效的Rashomon集探索能力。实验表明，DIVERSE提供了竞争性的性能，能够构建保持鲁棒性和准确率的多样化模型集合，同时支持平衡的模型多样性。与重新训练作为基线的方法相比，DIVERSE以显著降低的计算成本实现了可比较的多样性，具体效率提升摘要未明确说明，但突出了其高效优势。",
      "conclusion": "结论是DIVERSE框架成功提供了一种高效方法来探索神经网络的Rashomon集，支持模型多样性并保持性能，具有重要学术和实际价值。主要贡献在于提出了一种无需重新训练的新探索策略，降低了计算成本，使构建多样化模型集更可行。潜在局限性包括可能依赖预训练模型和调制策略的泛化能力，未来工作可扩展到更大数据集或更复杂模型，以及验证其在其他任务中的应用效果。",
      "tags": [
        "Rashomon Set",
        "Feature-wise Linear Modulation",
        "Covariance Matrix Adaptation Evolution Strategy",
        "Deep Neural Networks",
        "Model Diversity"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:41.317312Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20618",
    "title": "GDCNet: Generative Discrepancy Comparison Network for Multimodal Sarcasm Detection",
    "authors": [
      "Shuguang Zhang",
      "Junhong Lian",
      "Guoxin Yu",
      "Baoxun Xu",
      "Xiang Ao"
    ],
    "abstract": "Multimodal sarcasm detection (MSD) aims to identify sarcasm within image-text pairs by modeling semantic incongruities across modalities. Existing methods often exploit cross-modal embedding misalignment to detect inconsistency but struggle when visual and textual content are loosely related or semantically indirect. While recent approaches leverage large language models (LLMs) to generate sarcastic cues, the inherent diversity and subjectivity of these generations often introduce noise. To address these limitations, we propose the Generative Discrepancy Comparison Network (GDCNet). This framework captures cross-modal conflicts by utilizing descriptive, factually grounded image captions generated by Multimodal LLMs (MLLMs) as stable semantic anchors. Specifically, GDCNet computes semantic and sentiment discrepancies between the generated objective description and the original text, alongside measuring visual-textual fidelity. These discrepancy features are then fused with visual and textual representations via a gated module to adaptively balance modality contributions. Extensive experiments on MSD benchmarks demonstrate GDCNet's superior accuracy and robustness, establishing a new state-of-the-art on the MMSD2.0 benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20618.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20618",
    "published": "2026-01-28T13:51:34Z",
    "updated": "2026-01-28T13:51:34Z",
    "comment": "Accepted to 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)",
    "light_analysis": {
      "overview": "GDCNet通过利用多模态大语言模型生成的图像描述作为语义锚点，捕捉跨模态差异，提升多模态讽刺检测性能。",
      "motivation": "多模态讽刺检测（MSD）旨在识别图像-文本对中的讽刺，对社交媒体分析和情感计算至关重要。现有方法常基于跨模态嵌入的对齐度检测不一致性，但当视觉和文本内容松散相关或语义间接时，效果不佳。此外，最近方法利用大语言模型（LLMs）生成讽刺线索，但由于生成内容的多样性和主观性，常引入噪声，降低了检测的鲁棒性。因此，需要一种更稳定、鲁棒的方法来准确捕捉跨模态语义冲突，以改进MSD性能。",
      "method": "论文提出Generative Discrepancy Comparison Network（GDCNet），核心思想是利用多模态大语言模型（MLLMs）生成描述性、事实基础的图像描述作为稳定的语义锚点。框架通过计算生成的客观描述与原始文本之间的语义和情感差异，同时测量视觉-文本的忠实度。这些差异特征通过一个门控模块与视觉和文本表示融合，自适应地平衡各模态的贡献。关键创新点包括使用MLLMs生成的描述作为锚点以减少噪声，以及差异特征的集成和自适应融合机制。数据集基于MSD基准，但具体架构细节摘要未明确说明。",
      "result": "在多个多模态讽刺检测基准上进行的实验表明，GDCNet展现了优越的准确性和鲁棒性。特别是在MMSD2.0基准上，GDCNet确立了新的最先进水平，证明了其方法的有效性。与现有基线方法相比，GDCNet通过减少噪声和自适应融合，提高了检测性能。然而，摘要未提供具体的性能指标数据，如准确率提升百分比。",
      "conclusion": "GDCNet的主要贡献在于提出了一种利用生成模型和差异比较的多模态讽刺检测框架，有效解决了现有方法在模态内容松散相关时的不足。其学术价值在于为跨模态不一致性建模提供了新思路，结合了生成技术和自适应融合。实际应用中，该框架可用于社交媒体内容分析，改善讽刺检测的准确性。局限性可能包括对MLLMs生成的依赖性，未来工作可探索更高效的差异计算或扩展到其他模态。",
      "tags": [
        "Multimodal Sarcasm Detection",
        "Large Language Models",
        "Generative Modeling",
        "Cross-modal Discrepancy",
        "Adaptive Fusion"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:58.304060Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20614",
    "title": "Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation",
    "authors": [
      "Yanqi Dai",
      "Yuxiang Ji",
      "Xiao Zhang",
      "Yong Wang",
      "Xiangxiang Chu",
      "Zhiwu Lu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) offers a robust mechanism for enhancing mathematical reasoning in large models. However, we identify a systematic lack of emphasis on more challenging questions in existing methods from both algorithmic and data perspectives, despite their importance for refining underdeveloped capabilities. Algorithmically, widely used Group Relative Policy Optimization (GRPO) suffers from an implicit imbalance where the magnitude of policy updates is lower for harder questions. Data-wise, augmentation approaches primarily rephrase questions to enhance diversity without systematically increasing intrinsic difficulty. To address these issues, we propose a two-dual MathForge framework to improve mathematical reasoning by targeting harder questions from both perspectives, which comprises a Difficulty-Aware Group Policy Optimization (DGPO) algorithm and a Multi-Aspect Question Reformulation (MQR) strategy. Specifically, DGPO first rectifies the implicit imbalance in GRPO via difficulty-balanced group advantage estimation, and further prioritizes harder questions by difficulty-aware question-level weighting. Meanwhile, MQR reformulates questions across multiple aspects to increase difficulty while maintaining the original gold answer. Overall, MathForge forms a synergistic loop: MQR expands the data frontier, and DGPO effectively learns from the augmented data. Extensive experiments show that MathForge significantly outperforms existing methods on various mathematical reasoning tasks. The code and augmented data are all available at https://github.com/AMAP-ML/MathForge.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20614.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20614",
    "published": "2026-01-28T13:49:23Z",
    "updated": "2026-01-28T13:49:23Z",
    "comment": "Accepted for ICLR 2026",
    "light_analysis": {
      "overview": "MathForge框架通过结合难度感知的组策略优化和多角度问题重构，显著提升了大型模型在数学推理任务中的性能。",
      "motivation": "现有强化学习与可验证奖励方法在数学推理中存在不足，特别是算法上，GRPO存在隐式不平衡，导致对更难题的策略更新不足；数据上，现有增强技术仅通过重述问题增加多样性，而非系统性地提升内在难度。这限制了模型在复杂推理能力上的培养，因此改进这些方面对提升模型泛化性和准确性至关重要。",
      "method": "论文提出MathForge框架，包括难度感知组策略优化和多角度问题重构两个组件。DGPO通过难度平衡的组优势估计纠正GRPO中的隐式不平衡，并使用难度感知的权重优先处理更难题；MQR通过从多个角度重新表述问题来增加内在难度，同时保持原始答案不变，形成协同循环，以数据增强促进算法学习。",
      "result": "实验结果显示，MathForge在各种数学推理任务中显著优于现有方法，如GRPO等基线。虽然摘要未提供具体数值如准确率提升，但强调了其性能优越性，表明通过针对更难题的优化，有效提升了模型的推理能力和泛化性。",
      "conclusion": "MathForge框架为强化学习在数学推理中的应用提供了创新方法，学术上推动了算法和数据增强的结合，实际上可应用于教育AI和自动推理系统。摘要未明确说明局限性，但暗示了未来工作可能包括扩展到其他领域或进一步优化难度评估。",
      "tags": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "Group Relative Policy Optimization",
        "Difficulty-Aware Learning",
        "Question Reformulation"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:43.565887Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20613",
    "title": "AgentIF-OneDay: A Task-level Instruction-Following Benchmark for General AI Agents in Daily Scenarios",
    "authors": [
      "Kaiyuan Chen",
      "Qimin Wu",
      "Taiyu Hou",
      "Tianhao Tang",
      "Xueyu Hu",
      "Yuchen Hou",
      "Bikun Li",
      "Chengming Qian",
      "Guoyin Wang",
      "Haolin Chen",
      "Haotong Tian",
      "Haoye Zhang",
      "Haoyu Bian",
      "Hongbing Pan",
      "Hongkang Zhang",
      "Hongyi Zhou",
      "Jiaqi Cai",
      "Jiewu Rao",
      "Jiyuan Ren",
      "Keduan Huang",
      "Lucia Zhu Huang",
      "Mingyu Yuan",
      "Naixu Guo",
      "Qicheng Tang",
      "Qinyan Zhang",
      "Shuai Chen",
      "Siheng Chen",
      "Ting Ting Li",
      "Xiaoxing Guo",
      "Yaocheng Zuo",
      "Yaoqi Guo",
      "Yinan Wang",
      "Yinzhou Yu",
      "Yize Wang",
      "Yuan Jiang",
      "Yuan Tian",
      "Yuanshuo Zhang",
      "Yuxuan Liu",
      "Yvette Yan Zeng",
      "Zenyu Shan",
      "Zihan Yin",
      "Xiaobo Hu",
      "Yang Liu",
      "Yixin Ren",
      "Yuan Gong"
    ],
    "abstract": "The capacity of AI agents to effectively handle tasks of increasing duration and complexity continues to grow, demonstrating exceptional performance in coding, deep research, and complex problem-solving evaluations. However, in daily scenarios, the perception of these advanced AI capabilities among general users remains limited. We argue that current evaluations prioritize increasing task difficulty without sufficiently addressing the diversity of agentic tasks necessary to cover the daily work, life, and learning activities of a broad demographic. To address this, we propose AgentIF-OneDay, aimed at determining whether general users can utilize natural language instructions and AI agents to complete a diverse array of daily tasks. These tasks require not only solving problems through dialogue but also understanding various attachment types and delivering tangible file-based results. The benchmark is structured around three user-centric categories: Open Workflow Execution, which assesses adherence to explicit and complex workflows; Latent Instruction, which requires agents to infer implicit instructions from attachments; and Iterative Refinement, which involves modifying or expanding upon ongoing work. We employ instance-level rubrics and a refined evaluation pipeline that aligns LLM-based verification with human judgment, achieving an 80.1% agreement rate using Gemini-3-Pro. AgentIF-OneDay comprises 104 tasks covering 767 scoring points. We benchmarked four leading general AI agents and found that agent products built based on APIs and ChatGPT agents based on agent RL remain in the first tier simultaneously. Leading LLM APIs and open-source models have internalized agentic capabilities, enabling AI application teams to develop cutting-edge Agent products.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20613.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20613",
    "published": "2026-01-28T13:49:18Z",
    "updated": "2026-01-28T13:49:18Z",
    "comment": "17 pages, 8 figures",
    "light_analysis": {
      "overview": "本文提出了AgentIF-OneDay，一个任务级别的指令跟随基准，旨在评估通用AI代理在日常多样化任务中的能力，以弥补当前评估缺乏日常活动覆盖的不足。",
      "motivation": "随着AI代理在复杂任务中表现出色，如编码和问题解决，但在日常场景中，普通用户对AI能力的感知仍然有限。当前评估过于关注增加任务难度，未能充分覆盖日常工作和生活的多样性，导致评估不能反映实际应用需求。因此，需要一个新的基准来评估AI代理在执行日常任务时的指令跟随能力，促进AI技术向日常应用的普及。",
      "method": "研究提出AgentIF-OneDay基准，围绕三个用户中心类别：Open Workflow Execution评估显式工作流程遵循，Latent Instruction要求从附件推断隐式指令，Iterative Refinement涉及任务的修改和扩展。采用实例级评估准则和优化的评估管道，结合基于Gemini-3-Pro的LLM验证与人类判断，确保评估的一致性和可靠性。基准共设计104个任务，覆盖767个评分点，全面测试AI代理的多样化能力。",
      "result": "实验使用AgentIF-OneDay对四个领先通用AI代理进行基准测试，结果显示基于API构建的代理产品和基于强化学习的ChatGPT代理均处于第一层级。评估中，LLM验证与人类判断的一致率达到80.1%，表明方法的有效性。基准包含104个任务和767个评分点，证实了领先LLM API和开源模型已内化代理能力，支持AI团队开发尖端Agent产品。",
      "conclusion": "本文的主要贡献是提出了一个全面的任务级指令跟随基准AgentIF-OneDay，填补了日常场景AI评估的空白。该基准通过多样化任务设计和改进的评估管道，有效评估AI代理的日常应用能力。研究不仅展示了当前AI代理的技术成熟度，还为未来AI应用开发提供了实用工具。潜在局限可能在于任务覆盖范围的扩展，未来可进一步增加任务类型和用户群体。",
      "tags": [
        "Instruction-Following Benchmark",
        "AI Agents",
        "Task-level Evaluation",
        "LLM Verification",
        "Daily Task Scenarios"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:10.838741Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20611",
    "title": "ACFormer: Mitigating Non-linearity with Auto Convolutional Encoder for Time Series Forecasting",
    "authors": [
      "Gawon Lee",
      "Hanbyeol Park",
      "Minseop Kim",
      "Dohee Kim",
      "Hyerim Bae"
    ],
    "abstract": "Time series forecasting (TSF) faces challenges in modeling complex intra-channel temporal dependencies and inter-channel correlations. Although recent research has highlighted the efficiency of linear architectures in capturing global trends, these models often struggle with non-linear signals. To address this gap, we conducted a systematic receptive field analysis of convolutional neural network (CNN) TSF models. We introduce the \"individual receptive field\" to uncover granular structural dependencies, revealing that convolutional layers act as feature extractors that mirror channel-wise attention while exhibiting superior robustness to non-linear fluctuations. Based on these insights, we propose ACFormer, an architecture designed to reconcile the efficiency of linear projections with the non-linear feature-extraction power of convolutions. ACFormer captures fine-grained information through a shared compression module, preserves temporal locality via gated attention, and reconstructs variable-specific temporal patterns using an independent patch expansion layer. Extensive experiments on multiple benchmark datasets demonstrate that ACFormer consistently achieves state-of-the-art performance, effectively mitigating the inherent drawbacks of linear models in capturing high-frequency components.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20611.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20611",
    "published": "2026-01-28T13:47:54Z",
    "updated": "2026-01-28T13:47:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "ACFormer通过结合线性投影效率与卷积非线性特征提取，显著提升时间序列预测性能。",
      "motivation": "时间序列预测（TSF）面临建模复杂通道内时间依赖性和通道间相关性的挑战。尽管线性架构在捕获全局趋势方面高效，但这些模型在处理非线性信号（尤其是高频分量）时存在固有缺陷，导致预测准确性不足。现有方法未能有效平衡效率与非线性特征提取能力，因此本研究旨在开发更鲁棒的模型，以弥补线性模型在复杂信号建模上的不足，提升TSF的实际应用价值。",
      "method": "论文首先对卷积神经网络（CNN）在TSF中的接收场进行系统分析，引入“个体接收场”概念，揭示粒度结构依赖，发现卷积层可模拟通道注意力并增强对非线性波动的鲁棒性。基于此，提出ACFormer架构，它通过共享压缩模块捕获细粒度信息，利用门控注意力保留时间局部性，并借助独立补丁扩展层重建变量特定的时间模式，有效融合线性投影的效率和卷积的非线性特征提取能力，无需复杂模型堆叠。",
      "result": "在多个基准数据集上的广泛实验表明，ACFormer consistently achieves state-of-the-art performance，有效缓解了线性模型在捕获高频分量时的固有缺陷。虽然摘要未提供具体性能指标，但通过与基线方法的对比，证明了其在处理非线性信号和提升预测准确性方面的显著优势，验证了所提方法在调和效率与非线性特征提取上的成功。",
      "conclusion": "本研究提出ACFormer架构，成功将线性投影效率与卷积非线性特征提取能力相结合，解决了时间序列预测中线性模型处理非线性信号的挑战。学术上，该工作推动了TSF模型的发展，丰富了非线性建模方法；实际应用中，可提升预测精度，适用于金融、气象等领域。未来工作可进一步探索模型在更多场景下的泛化能力或优化计算效率。",
      "tags": [
        "Time Series Forecasting",
        "Convolutional Neural Network",
        "Linear Projections",
        "Gated Attention",
        "Non-linear Modeling"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:35.221442Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20606",
    "title": "WFR-MFM: One-Step Inference for Dynamic Unbalanced Optimal Transport",
    "authors": [
      "Xinyu Wang",
      "Ruoyu Wang",
      "Qiangwei Peng",
      "Peijie Zhou",
      "Tiejun Li"
    ],
    "abstract": "Reconstructing dynamical evolution from limited observations is a fundamental challenge in single-cell biology, where dynamic unbalanced optimal transport provides a principled framework for modeling coupled transport and mass variation. However, existing approaches rely on trajectory simulation at inference time, making inference a key bottleneck for scalable applications. In this work, we propose a mean-flow framework for unbalanced flow matching that summarizes both transport and mass-growth dynamics over arbitrary time intervals using mean velocity and mass-growth fields, enabling fast one-step generation without trajectory simulation. To solve dynamic unbalanced optimal transport under the Wasserstein-Fisher-Rao geometry, we further build on this framework to develop Wasserstein-Fisher-Rao Mean Flow Matching (WFR-MFM). Across synthetic and real single-cell RNA sequencing datasets, WFR-MFM achieves orders-of-magnitude faster inference than a range of existing baselines while maintaining high predictive accuracy, and enables efficient perturbation response prediction on large synthetic datasets with thousands of conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20606.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20606",
    "published": "2026-01-28T13:41:52Z",
    "updated": "2026-01-28T13:41:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了 Wasserstein-Fisher-Rao 平均流匹配（WFR-MFM）方法，实现了动态不平衡最优传输的快速一步推断。",
      "motivation": "研究动机源于单细胞生物学中从有限观测数据重建动态演化序列的基本挑战。动态不平衡最优传输为建模传输和质量变化提供了理论框架，但现有方法在推断时依赖轨迹模拟，导致计算效率低下，成为可扩展应用的主要瓶颈。因此，开发高效的推断方法对促进大规模数据分析至关重要，以解决现有方法在速度和可扩展性方面的不足。",
      "method": "论文提出平均流框架，通过定义平均速度场和质量增长场，在任意时间间隔内总结传输和质量增长动态。核心方法是基于 Wasserstein-Fisher-Rao 几何，发展 Wasserstein-Fisher-Rao 平均流匹配（WFR-MFM），利用这些场避免轨迹模拟，实现一步生成。关键创新在于将动态不平衡最优传输问题转化为高效的平均流匹配，提高了推断速度，并适用于复杂数据集如单细胞 RNA 测序。",
      "result": "在合成和真实单细胞 RNA 测序数据集上，WFR-MFM 与多个基线方法相比，推断速度提升了数个数量级，同时保持了高预测准确度。具体表现为快速生成动态演化，并能高效处理大规模合成数据集中的数千个条件的扰动响应预测，展示了其在实践中的优越性能和可扩展性优势，而不损失精度。",
      "conclusion": "本文的主要贡献是提出了 WFR-MFM 方法，显著提高了动态不平衡最优传输的推断效率。研究在学术上丰富了最优传输理论，在实际应用中促进了单细胞生物学中的大规模数据分析。未来工作可探索该方法在其他领域的扩展，或优化精度与速度的平衡，以应对更复杂的场景。",
      "tags": [
        "Optimal Transport",
        "Unbalanced Optimal Transport",
        "Flow Matching",
        "Wasserstein-Fisher-Rao Geometry",
        "Single-Cell RNA Sequencing"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:28.581516Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20605",
    "title": "CoBA: Integrated Deep Learning Model for Reliable Low-Altitude UAV Classification in mmWave Radio Networks",
    "authors": [
      "Junaid Sajid",
      "Ivo Müürsepp",
      "Luca Reggiani",
      "Davide Scazzoli",
      "Federico Francesco Luigi Mariani",
      "Maurizio Magarini",
      "Rizwan Ahmad",
      "Muhammad Mahtab Alam"
    ],
    "abstract": "Uncrewed Aerial Vehicles (UAVs) are increasingly used in civilian and industrial applications, making secure low-altitude operations crucial. In dense mmWave environments, accurately classifying low-altitude UAVs as either inside authorized or restricted airspaces remains challenging, requiring models that handle complex propagation and signal variability. This paper proposes a deep learning model, referred to as CoBA, which stands for integrated Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Attention which leverages Fifth Generation (5G) millimeter-wave (mmWave) radio measurements to classify UAV operations in authorized and restricted airspaces at low altitude. The proposed CoBA model integrates convolutional, bidirectional recurrent, and attention layers to capture both spatial and temporal patterns in UAV radio measurements. To validate the model, a dedicated dataset is collected using the 5G mmWave network at TalTech, with controlled low altitude UAV flights in authorized and restricted scenarios. The model is evaluated against conventional ML models and a fingerprinting-based benchmark. Experimental results show that CoBA achieves superior accuracy, significantly outperforming all baseline models and demonstrating its potential for reliable and regulated UAV airspace monitoring.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20605.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20605",
    "published": "2026-01-28T13:41:30Z",
    "updated": "2026-01-28T13:41:30Z",
    "comment": "6 Pages, This paper has been accepted for publication at the IEEE International Conference on Communications (ICC) 2026",
    "light_analysis": {
      "overview": "CoBA模型通过集成卷积神经网络、双向长短时记忆和注意力机制，利用5G毫米波测量可靠分类低空无人机操作。",
      "motivation": "无人机在民用和工业应用中的日益普及，使得低空操作的安全性至关重要。在密集毫米波环境中，准确分类低空无人机是否处于授权或受限空域面临挑战，因为信号传播复杂且多变，现有方法可能无法有效处理这些信号变化，导致分类准确性不足。因此，需要开发能够同时捕捉空间和时间模式的新模型来提升分类可靠性，以支持安全的空域监控和管理。",
      "method": "CoBA模型集成了卷积神经网络（CNN）以提取空间特征、双向长短时记忆（BiLSTM）层处理时间序列模式，并引入注意力机制来关注关键信息，从而捕捉无人机无线电测量的时空模式。研究使用TalTech的5G毫米波网络收集数据集，包括受控的低空无人机飞行场景，覆盖授权和受限空域，以训练和评估模型。这种多层次融合架构旨在增强对复杂传播特性的学习能力，提高分类精度。",
      "result": "实验结果表明，CoBA模型在分类准确性方面表现优越，显著优于传统的机器学习模型和基于指纹识别的基准方法。尽管摘要未提供具体性能指标（如准确率百分比），但强调其实现了更高的性能，显示出在处理毫米波环境下无人机信号时的有效性和可靠性，为实际应用提供了有力支持。",
      "conclusion": "CoBA模型通过集成深度学习方法，为低空无人机分类提供了一个可靠的解决方案，其学术价值在于融合多种神经网络技术以处理复杂时空数据，实际应用价值在于促进安全和受监管的无人机空域监控。未来工作方向在摘要中未明确说明，但可能包括扩展数据集或优化模型架构以进一步提升性能。",
      "tags": [
        "Convolutional Neural Network",
        "Bidirectional LSTM",
        "Attention Mechanism",
        "mmWave Communication",
        "UAV Classification"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:41.423348Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20604",
    "title": "Dialogical Reasoning Across AI Architectures: A Multi-Model Framework for Testing AI Alignment Strategies",
    "authors": [
      "Gray Cox"
    ],
    "abstract": "This paper introduces a methodological framework for empirically testing AI alignment strategies through structured multi-model dialogue. Drawing on Peace Studies traditions - particularly interest-based negotiation, conflict transformation, and commons governance - we operationalize Viral Collaborative Wisdom (VCW), an approach that reframes alignment from a control problem to a relationship problem developed through dialogical reasoning.   Our experimental design assigns four distinct roles (Proposer, Responder, Monitor, Translator) to different AI systems across six conditions, testing whether current large language models can engage substantively with complex alignment frameworks. Using Claude, Gemini, and GPT-4o, we conducted 72 dialogue turns totaling 576,822 characters of structured exchange.   Results demonstrate that AI systems can engage meaningfully with Peace Studies concepts, surface complementary objections from different architectural perspectives, and generate emergent insights not present in initial framings - including the novel synthesis of \"VCW as transitional framework.\" Cross-architecture patterns reveal that different models foreground different concerns: Claude emphasized verification challenges, Gemini focused on bias and scalability, and GPT-4o highlighted implementation barriers.   The framework provides researchers with replicable methods for stress-testing alignment proposals before implementation, while the findings offer preliminary evidence about AI capacity for the kind of dialogical reasoning VCW proposes. We discuss limitations, including the observation that dialogues engaged more with process elements than with foundational claims about AI nature, and outline directions for future research including human-AI hybrid protocols and extended dialogue studies.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20604.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20604",
    "published": "2026-01-28T13:41:01Z",
    "updated": "2026-01-28T13:41:01Z",
    "comment": "23 pages, 5 tables, 5 appendices. Code and data: https://github.com/jgraycox-coa/vcw-multi-ai-dialogue",
    "light_analysis": {
      "overview": "本文引入了一个通过多模型对话实证测试AI对齐策略的方法框架。",
      "motivation": "AI对齐是确保AI系统与人类价值观一致的核心挑战。现有方法往往将对齐视为控制问题，缺乏有效的实证测试框架，难以处理复杂对齐策略的评估。本研究借鉴和平研究传统，将对齐重新定义为通过对话推理发展的关系问题，以操作化病毒性协作智慧（VCW）方法，旨在填补这一空白，提供更全面的测试工具。",
      "method": "研究方法基于病毒性协作智慧（VCW）框架，通过结构化多模型对话进行实验。设计包括四个角色（提议者、响应者、监视者、翻译者）分配给不同条件下的AI系统，使用Claude、Gemini和GPT-4o等大型语言模型。进行了72轮对话，总计576,822字符的结构化交换，关键创新点在于结合多模型架构和对话推理，提供可复制的测试协议。",
      "result": "实验结果显示，AI系统能有意义地参与和平研究概念，揭示互补性异议并产生新兴见解如“VCW作为过渡框架”。跨架构模式表明不同模型关注点各异：Claude强调验证挑战，Gemini聚焦偏见和可扩展性，GPT-4o突出实施障碍。这些发现提供了AI对话推理能力的初步证据，支持对齐提议的压力测试，但摘要未明确说明与基线方法的对比数据。",
      "conclusion": "本研究贡献了一个可复制的框架，用于实证测试AI对齐策略，并展示了AI的对话推理能力。学术价值在于将和平研究引入AI领域，推动对齐问题从理论到实践的过渡；实际应用价值在于帮助研究人员在实施前评估对齐提议。局限性包括对话更多涉及过程而非本质问题，未来工作方向包括人机混合协议和扩展对话研究。",
      "tags": [
        "Large Language Model",
        "Dialogical Reasoning",
        "AI Alignment",
        "Multi-Model Framework",
        "Viral Collaborative Wisdom"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:27.521951Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20601",
    "title": "CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification",
    "authors": [
      "Zhuonan Wang",
      "Wenjie Yan",
      "Wenqiao Zhang",
      "Xiaohui Song",
      "Jian Ma",
      "Ke Yao",
      "Yibo Yu",
      "Beng Chin Ooi"
    ],
    "abstract": "Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20601.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20601",
    "published": "2026-01-28T13:40:10Z",
    "updated": "2026-01-28T13:40:10Z",
    "comment": "10 pages,7 figures",
    "light_analysis": {
      "overview": "CLEAR-Mamba 提出一种基于 MedMamba 的增强框架，通过超网络自适应层和可靠性感知预测，提高了多序列眼科血管造影分类的准确性和可靠性。",
      "motivation": "医学图像分类在计算机辅助诊断中至关重要，特别是在眼科领域，荧光素眼底血管造影和吲哚菁绿血管造影能提供传统眼底摄影无法捕获的血流动力学和病变结构信息。然而，由于单模态性质、细微病变模式和设备间显著差异，现有方法在泛化能力和高置信度预测方面存在局限，导致分类性能不足。本研究旨在解决这些问题，提升分类模型的跨域适应性和预测可靠性，以支持早期疾病检测和治疗规划。",
      "method": "该方法基于 MedMamba 架构，引入两个关键技术：HaC（超网络自适应层）根据输入特征分布动态生成参数，增强跨域适应性；RaP（可靠性感知预测）基于证据不确定性学习，关注低置信度样本以提高稳定性和可靠性。研究还构建了一个大规模眼科血管造影数据集，包含荧光素眼底血管造影和吲哚菁绿血管造影模态，涵盖多种视网膜疾病类别，用于模型训练和评估，确保方法有效性。",
      "result": "实验结果显示，CLEAR-Mamba 在多项指标上一致优于多个基线模型，包括原始 MedMamba。该框架在多疾病分类任务中表现出色，尤其在可靠性感知预测方面具有显著优势，提高了分类准确性和稳定性。与基线方法相比，它在跨域适应性和预测可信度方面展现出改进，为医学图像分类提供了更可靠的解决方案。",
      "conclusion": "本研究通过 CLEAR-Mamba 框架，在眼科血管造影图像分类中实现了泛化性和可靠性的平衡。核心贡献包括 HaC 和 RaP 技术，分别提升了跨域适应性和预测可信度。学术上，它为医学图像分类提供了新思路；实际应用中，可促进眼科计算机辅助诊断系统的精准诊断。未来工作可探索在其他医学图像任务中的应用或进一步优化模型结构，以扩展其适用范围。",
      "tags": [
        "Medical Image Classification",
        "Multi-Sequence Ophthalmic Angiography",
        "Hypernetwork-based Adaptive Conditioning",
        "Reliability-aware Prediction",
        "Evidential Uncertainty Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:45.417525Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20599",
    "title": "Regularized Gradient Temporal-Difference Learning",
    "authors": [
      "Hyunjun Na",
      "Donghwan Lee"
    ],
    "abstract": "Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20599.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20599",
    "published": "2026-01-28T13:37:42Z",
    "updated": "2026-01-28T13:37:42Z",
    "comment": "27 pages, 8 figures",
    "light_analysis": {
      "overview": "提出正则化梯度时序差分算法，解决特征交互矩阵奇异时的不稳定问题。",
      "motivation": "梯度时序差分学习算法广泛用于带函数逼近的策略评估，但现有收敛分析依赖于特征交互矩阵非奇异的限制性假设。在实践中，该矩阵可能奇异，导致算法不稳定或性能下降，限制了其实际应用。本研究旨在解决这一关键问题，通过移除假设提升算法的鲁棒性，以应对强化学习中常见的奇异场景，从而增强策略评估的可靠性。",
      "method": "论文通过重新形式化均方投影贝尔曼误差的最小化，引入一个正则化的优化目标。这自然导出了正则化梯度时序差分算法，称为R-GTD，其关键创新在于正则化处理，确保即使在特征交互矩阵奇异时也能收敛到唯一解。方法基于标准梯度下降框架，优化正则化后的目标函数，无需复杂的额外结构，专注于改进收敛稳定性。",
      "result": "研究建立了R-GTD算法的理论收敛保证，并提供了显式的误差界，证明了在特征交互矩阵奇异时的收敛性。通过实证实验验证了有效性，表明与传统GTD算法相比，R-GTD在奇异场景下表现更稳定，性能有所提升，但具体数据在摘要中未明确说明。实验强调了新方法在实际应用中的优势。",
      "conclusion": "本文的主要贡献是提出了正则化梯度时序差分算法，解决了传统方法中因特征交互矩阵奇异而导致的收敛问题。研究具有重要的学术价值，为时序差分学习提供了更稳健的理论基础，并拓展了其应用范围。在实际中，可提高策略评估的可靠性。未来工作可能包括优化正则化参数或应用于更广泛的强化学习任务。",
      "tags": [
        "Gradient Temporal-Difference Learning",
        "Regularization",
        "Convergence Analysis",
        "Mean-Square Projected Bellman Error"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:14.665645Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20598",
    "title": "Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?",
    "authors": [
      "Lakshman Balasubramanian"
    ],
    "abstract": "Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20598.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20598",
    "published": "2026-01-28T13:35:31Z",
    "updated": "2026-01-28T13:35:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文比较了监督、自监督和语言对齐三种训练范式在人员重识别任务中的表现，发现语言对齐模型在跨域场景中展现出显著的鲁棒性。",
      "motivation": "人员重识别在计算机视觉中仍是一个挑战性问题，现有监督模型在训练域内表现优秀，但在跨域应用中泛化能力不足。随着基础模型的兴起，如何利用它们提升视觉表示的丰富性和可迁移性成为关键。本研究旨在探讨不同训练范式在跨域鲁棒性方面的差异，以弥补当前方法在泛化方面的短板，并为实际部署提供指导。",
      "method": "本研究采用系统性比较分析方法，评估了监督学习、自监督学习和语言对齐模型三种训练范式在人员重识别任务中的表现。核心方法包括使用11个模型和9个数据集进行跨域实验，重点关注基础模型如SigLIP2的应用。通过对比模型在训练域内和跨域数据上的性能，分析了不同范式在泛化能力上的技术特色和创新点，强调了对鲁棒性评估的实验设计。",
      "result": "实验结果表明，监督模型在训练数据集上表现优异，但在跨域数据上表现大幅下降，泛化能力有限。相反，语言对齐模型在跨域场景中显示出意外的鲁棒性，尽管未针对人员重识别任务进行专门训练。自监督模型的详细性能摘要未明确说明，但与监督模型相比，语言对齐模型在跨域应用中的优势明显。",
      "conclusion": "本研究的贡献在于揭示了语言对齐模型在人员重识别跨域任务中的潜在优势，可能源于其更丰富和可迁移的视觉表示。这一发现对提升模型泛化能力有重要学术和实践价值，为未来研究指明了方向。局限性包括未全面分析自监督模型的具体效果，未来工作可进一步优化语言对齐模型以增强任务特异性。",
      "tags": [
        "Person Re-Identification",
        "Supervised Learning",
        "Self-Supervised Learning",
        "Language-Aligned Models",
        "Foundation Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:14.820237Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20597",
    "title": "StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval",
    "authors": [
      "Shaokun Wang",
      "Weili Guan",
      "Jizhou Han",
      "Jianlong Wu",
      "Yupeng Hu",
      "Liqiang Nie"
    ],
    "abstract": "Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20597.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20597",
    "published": "2026-01-28T13:34:44Z",
    "updated": "2026-01-28T13:34:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "StructAlign 提出结构化跨模态对齐方法，通过引入等角紧框架几何先验和设计新型损失函数，有效缓解持续文本到视频检索中的灾难性遗忘。",
      "motivation": "持续文本到视频检索（CTVR）是一个挑战性的多模态持续学习场景，模型需要增量学习新语义类别，同时保持对已学类别的文本-视频对齐，这容易导致灾难性遗忘。关键问题在于特征漂移，包括由每个模态内持续学习引起的模态内特征漂移，以及跨模态非协作特征漂移导致的模态错位。现有方法难以同时处理这两种漂移，因此需要新方法来改善对齐和减少遗忘，这对于提升多模态系统的适应性至关重要。",
      "method": "StructAlign 的核心方法是基于结构化跨模态对齐。首先，引入单纯形等角紧框架（ETF）几何作为统一几何先验，以缓解模态错位。基于此，设计跨模态 ETF 对齐损失，将文本和视频特征与类别级 ETF 原型对齐，促进表示形成近似单纯形 ETF 几何。此外，为抑制模态内特征漂移，设计跨模态关系保持损失，利用互补模态保留跨模态相似性关系，为特征更新提供稳定监督。摘要未明确说明具体的数据集或模型架构细节。",
      "result": "论文在基准数据集上进行了广泛实验，结果表明 StructAlign  consistently outperforms state-of-the-art 的持续检索方法。尽管摘要未提供具体性能指标（如准确率提升），但强调了该方法在缓解灾难性遗忘方面的有效性，并通过与基线方法的对比，证实了其优越性能，展示了在持续学习场景中的稳定改进。",
      "conclusion": "StructAlign 的主要贡献是通过结构化跨模态对齐，联合解决跨模态非协作特征漂移和模态内特征漂移，有效缓解了 CTVR 中的灾难性遗忘。该研究在学术上为多模态持续学习提供了新方法，具有实际应用价值，如提升视频检索系统的持续学习能力。未来工作可能涉及进一步优化方法或扩展到其他多模态任务，以应对更复杂的场景。",
      "tags": [
        "Continual Learning",
        "Text-to-Video Retrieval",
        "ETF Geometry",
        "Cross-Modal Alignment",
        "Feature Drift"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:38.166890Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20592",
    "title": "A Computational Approach to Language Contact -- A Case Study of Persian",
    "authors": [
      "Ali Basirat",
      "Danial Namazifard",
      "Navid Baradaran Hemmati"
    ],
    "abstract": "We investigate structural traces of language contact in the intermediate representations of a monolingual language model. Focusing on Persian (Farsi) as a historically contact-rich language, we probe the representations of a Persian-trained model when exposed to languages with varying degrees and types of contact with Persian. Our methodology quantifies the amount of linguistic information encoded in intermediate representations and assesses how this information is distributed across model components for different morphosyntactic features. The results show that universal syntactic information is largely insensitive to historical contact, whereas morphological features such as Case and Gender are strongly shaped by language-specific structure, suggesting that contact effects in monolingual language models are selective and structurally constrained.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20592.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20592",
    "published": "2026-01-28T13:27:00Z",
    "updated": "2026-01-28T13:27:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种计算方法，通过波斯语单语语言模型的中间表示，揭示语言接触效应选择性地影响形态特征而非句法信息。",
      "motivation": "语言接触是语言学中的重要现象，传统研究多依赖历史资料或语料分析，而计算视角的探讨相对缺乏。本研究旨在解决如何利用深度学习模型捕捉语言接触的结构性痕迹，以波斯语为例，因其历史上与其他语言接触频繁。现有方法可能未系统性地量化模型表示中的接触效应，本文通过分析单语模型，探索接触历史是否影响语言信息的编码，为理解语言演化和模型行为提供新途径。",
      "method": "研究使用波斯语训练的单语语言模型，将其暴露于与波斯语有不同接触程度和类型的语言中。核心方法包括量化中间表示中编码的语言信息量，并评估这些信息在不同模型组件（如注意力层）中的分布，特别针对形态句法特征如格和性。技术路线涉及设计探测任务，以区分通用句法和语言特定特征，从而分析接触效应的选择性，模型架构可能基于标准Transformer，但摘要未明确说明具体细节。",
      "result": "实验结果表明，通用句法信息在模型中间表示中对历史接触不敏感，说明其跨语言的鲁棒性。而形态特征如格和性则受到语言特定结构的强烈塑造，显示出接触效应在这些方面的显著影响。通过量化分析，本文证实了接触效应是选择性和结构约束的，为计算模型中的语言表现提供了实证支持，但摘要未提供具体性能指标或对比基线，主要基于描述性发现。",
      "conclusion": "本文的主要贡献在于提出并验证了一种计算框架来研究语言接触，利用单语语言模型的中间表示揭示接触效应的选择性。学术价值体现在为计算语言学和历史语言学提供新视角，加深对语言模型和语言结构交互的理解。实际应用可扩展到语言模型设计或语言演化分析工具，未来工作可探索更多语言或更复杂的接触场景，以验证普遍性，局限性包括摘要未明确说明模型具体类型和数据集细节。",
      "tags": [
        "Language Contact",
        "Monolingual Language Model",
        "Intermediate Representations",
        "Morphosyntactic Features",
        "Computational Linguistics"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:33.771285Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20585",
    "title": "Ranking-aware Reinforcement Learning for Ordinal Ranking",
    "authors": [
      "Aiming Hao",
      "Chen Zhu",
      "Jiashu Zhu",
      "Jiahong Wu",
      "Xiangxiang Chu"
    ],
    "abstract": "Ordinal regression and ranking are challenging due to inherent ordinal dependencies that conventional methods struggle to model. We propose Ranking-Aware Reinforcement Learning (RARL), a novel RL framework that explicitly learns these relationships. At its core, RARL features a unified objective that synergistically integrates regression and Learning-to-Rank (L2R), enabling mutual improvement between the two tasks. This is driven by a ranking-aware verifiable reward that jointly assesses regression precision and ranking accuracy, facilitating direct model updates via policy optimization. To further enhance training, we introduce Response Mutation Operations (RMO), which inject controlled noise to improve exploration and prevent stagnation at saddle points. The effectiveness of RARL is validated through extensive experiments on three distinct benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20585.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20585",
    "published": "2026-01-28T13:22:42Z",
    "updated": "2026-01-28T13:22:42Z",
    "comment": "Accepted to ICASSP2026",
    "light_analysis": {
      "overview": "本文提出Ranking-Aware Reinforcement Learning (RARL)框架，通过统一目标整合回归和排序学习，用于建模有序排名中的顺序依赖关系。",
      "motivation": "有序回归和排名任务中存在固有的顺序依赖关系，例如在信息检索或推荐系统中，数据点的排序需要保持正确顺序。传统方法如基于分类或回归的模型难以有效捕获这些依赖，导致在处理顺序数据时性能受限。这些不足影响了实际应用的准确性和鲁棒性，因此需要开发新方法来自动学习顺序关系，以提升模型的整体表现。",
      "method": "论文提出Ranking-Aware Reinforcement Learning (RARL)框架，其核心是一个统一目标，协同整合回归任务和学习排序(L2R)任务，使两者能相互促进。通过排名感知的可验证奖励，同时评估回归精度和排序准确性，并利用政策优化直接更新模型以最大化奖励。为进一步增强训练，引入了响应突变操作(RMO)，通过注入受控噪声来改善探索过程并防止鞍点停滞，提升模型的泛化能力。",
      "result": "论文通过在三个不同基准数据集上的广泛实验验证了RARL框架的有效性，实验结果表明该方法在有序回归和排名任务中表现优于基线，但具体性能指标如准确率提升或效率改进摘要未明确说明。与现有方法相比，RARL展示了更好的建模能力，但需参考完整论文获取详细定量结果。",
      "conclusion": "RARL框架的主要贡献在于提供了一种创新的强化学习方法，通过统一目标和排名感知奖励，有效建模有序依赖关系，推动了排序学习与强化学习的交叉研究。这项研究具有学术价值，为处理顺序数据提供了新思路；在实际应用中，可应用于搜索引擎、推荐系统等领域。局限性可能包括计算复杂度或噪声敏感性，未来工作可探索更高效训练或扩展到多模态任务。",
      "tags": [
        "Ranking-aware Reinforcement Learning",
        "Ordinal Regression",
        "Learning-to-Rank",
        "Response Mutation Operations",
        "Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:52.018831Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20582",
    "title": "Single-Nodal Spontaneous Symmetry Breaking in NLP Models",
    "authors": [
      "Shalom Rosner",
      "Ronit D. Gross",
      "Ella Koresh",
      "Ido Kanter"
    ],
    "abstract": "Spontaneous symmetry breaking in statistical mechanics primarily occurs during phase transitions at the thermodynamic limit where the Hamiltonian preserves inversion symmetry, yet the low-temperature free energy exhibits reduced symmetry. Herein, we demonstrate the emergence of spontaneous symmetry breaking in natural language processing (NLP) models during both pre-training and fine-tuning, even under deterministic dynamics and within a finite training architecture. This phenomenon occurs at the level of individual attention heads and is scaled-down to its small subset of nodes and also valid at a single-nodal level, where nodes acquire the capacity to learn a limited set of tokens after pre-training or labels after fine-tuning for a specific classification task. As the number of nodes increases, a crossover in learning ability occurs, governed by the tradeoff between a decrease following random-guess among increased possible outputs, and enhancement following nodal cooperation, which exceeds the sum of individual nodal capabilities. In contrast to spin-glass systems, where a microscopic state of frozen spins cannot be directly linked to the free-energy minimization goal, each nodal function in this framework contributes explicitly to the global network task and can be upper-bounded using convex hull analysis. Results are demonstrated using BERT-6 architecture pre-trained on Wikipedia dataset and fine-tuned on the FewRel classification task.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20582.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20582",
    "published": "2026-01-28T13:20:02Z",
    "updated": "2026-01-28T13:20:02Z",
    "comment": "23 pages, 6 figures, 1 table",
    "light_analysis": {
      "overview": "论文在NLP模型中首次展示了自发对称性破缺现象在单节点层次的出现，连接了统计力学与AI学习动态。",
      "motivation": "研究动机源于探讨统计力学中的自发对称性破缺概念在人工智能领域的应用，特别是在自然语言处理模型中。现有方法通常在热力学极限下讨论对称性破缺，但在有限训练架构和确定性动态的NLP模型中类似现象未被充分研究。这个问题重要，因为它有助于深入理解神经网络内部学习机制的本质，现有方法未能揭示节点层次的学习能力如何与对称性破缺相关，限制了模型优化和理论发展。",
      "method": "研究方法采用BERT-6架构作为实验平台，在Wikipedia数据集上进行预训练，并在FewRel分类任务上进行微调。核心方法是分析个体注意力头和节点层次的自发对称性破缺现象，关注节点在预训练后学习有限令牌集或微调后学习标签的能力。关键创新点包括将物理概念引入NLP模型，研究单节点对称性破缺的微观机制，并使用凸包分析来上界节点功能对全局任务的贡献，技术特色在于结合实验与理论分析。",
      "result": "实验结果表明，在BERT-6模型中，预训练和微调过程中确实存在自发对称性破缺现象，节点能够获得学习有限令牌或标签的能力。随着节点数量增加，学习能力发生交叉，受随机猜测减少与节点合作增强之间的权衡控制，体现了对称性破缺的动态过程。与自旋玻璃系统相比，每个节点功能对任务有明确贡献，可通过凸包分析上界，摘要未明确说明具体性能指标对比基线方法。",
      "conclusion": "论文的主要贡献是揭示了NLP模型中自发对称性破缺的微观机制，特别是在单节点层次，这具有重要的学术价值，因为它桥接了物理和AI领域，加深了对神经网络学习动态的理解。实际应用价值包括可能优化模型架构设计和训练效率，提升分类任务性能。未来工作方向可扩展到其他模型架构或任务，但摘要未明确说明局限性或具体扩展计划。",
      "tags": [
        "Spontaneous Symmetry Breaking",
        "Attention Heads",
        "BERT",
        "Pre-training",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:12.983612Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20571",
    "title": "Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM",
    "authors": [
      "Anna van Elst",
      "Igor Colin",
      "Stephan Clémençon"
    ],
    "abstract": "Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20571.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20571",
    "published": "2026-01-28T13:09:10Z",
    "updated": "2026-01-28T13:09:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出AsylADMM算法，用于资源受限边缘设备上的去中心化中位数和分位数估计，实现了异步更新和低内存消耗。",
      "motivation": "在资源受限的边缘设备上进行去中心化学习时，算法需具备通信效率、数据鲁棒性和低内存消耗。现有gossip-based方法虽通信高效，但鲁棒性不足；异步去中心化ADMM方法用于中位数估计时，内存需求随节点度增长，在内存受限环境中不实用。因此，开发一种兼具鲁棒性和低内存使用的算法至关重要，以应对边缘计算中的统计估计挑战。",
      "method": "本文提出AsylADMM，一种基于gossip的算法，用于去中心化中位数和分位数估计。核心创新包括支持异步更新，每节点仅需维护两个变量，显著降低内存开销。技术路线结合ADMM框架和gossip机制，通过分析同步变体建立理论保证，并支持分位数修剪、几何中位数估计等应用，优化分布式学习过程。",
      "result": "实验中，异步AsylADMM算法表现出快速收敛性能。基于分位数的修剪方法在实证上优于现有基于秩的方法，展示了更高的鲁棒性和效率，但摘要未明确说明具体性能指标如准确率或速度提升。算法在去中心化设置下展现出改进，为资源受限环境提供了有效解决方案。",
      "conclusion": "该研究的主要贡献是提出了AsylADMM算法，解决了去中心化学习中鲁棒性和内存约束的挑战。通过理论分析和实证验证，算法在边缘设备上具有应用潜力，推动了分布式统计估计的发展。未来工作可能涉及进一步优化或扩展，但摘要未明确说明局限性。",
      "tags": [
        "Decentralized Learning",
        "Quantile Estimation",
        "ADMM",
        "Asynchronous Algorithms",
        "Gossip Algorithms"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:27.132033Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20568",
    "title": "Reinforcement Unlearning via Group Relative Policy Optimization",
    "authors": [
      "Efstratios Zaradoukas",
      "Bardh Prenkaj",
      "Gjergji Kasneci"
    ],
    "abstract": "During pretraining, LLMs inadvertently memorize sensitive or copyrighted data, posing significant compliance challenges under legal frameworks like the GDPR and the EU AI Act. Fulfilling these mandates demands techniques that can remove information from a deployed model without retraining from scratch. Existing unlearning approaches attempt to address this need, but often leak the very data they aim to erase, sacrifice fluency and robustness, or depend on costly external reward models. We introduce PURGE (Policy Unlearning through Relative Group Erasure), a novel method grounded in the Group Relative Policy Optimization framework that formulates unlearning as a verifiable problem. PURGE uses an intrinsic reward signal that penalizes any mention of forbidden concepts, allowing safe and consistent unlearning. Our approach reduces token usage per target by up to a factor of 46 compared with SotA methods, while improving fluency by 5.48 percent and adversarial robustness by 12.02 percent over the base model. On the Real World Knowledge Unlearning (RWKU) benchmark, PURGE achieves 11 percent unlearning effectiveness while preserving 98 percent of original utility. PURGE shows that framing LLM unlearning as a verifiable task, enables more reliable, efficient, and scalable forgetting, suggesting a promising new direction for unlearning research that combines theoretical guarantees, improved safety, and practical deployment efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20568.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20568",
    "published": "2026-01-28T13:07:58Z",
    "updated": "2026-01-28T13:07:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出PURGE方法，基于Group Relative Policy Optimization框架，利用强化学习和内在奖励实现大语言模型的可验证遗忘，有效解决数据泄露和性能损失问题。",
      "motivation": "在预训练过程中，大型语言模型（LLMs）会无意记忆敏感或受版权保护的数据，这违反了GDPR和EU AI Act等法规的合规要求，给实际部署带来挑战。现有遗忘技术往往无法彻底删除信息，反而导致数据泄露、牺牲模型的流畅性和鲁棒性，或依赖成本高昂的外部奖励模型，限制了其可扩展性和可靠性。因此，迫切需要一种更安全、高效且无需从头训练的遗忘方法，以应对严格的法规约束和实际应用需求。",
      "method": "PURGE（Policy Unlearning through Relative Group Erasure）是一种基于强化学习的新型遗忘方法，采用Group Relative Policy Optimization框架，将遗忘问题重新定义为可验证的任务。其核心创新在于设计了一个内在奖励信号，直接惩罚模型对禁止概念的提及，而无需依赖外部奖励模型。这种方法通过优化策略来实现一致且安全的遗忘，减少了对外部资源的依赖，并提高了算法的可解释性和效率。技术特色包括使用分组相对策略优化，增强了遗忘的验证性和稳定性。",
      "result": "实验结果显示，PURGE在性能方面显著优于现有方法：与最先进技术相比，针对每个目标，令牌使用量减少高达46倍；同时，流畅性提升了5.48%，对抗鲁棒性提升了12.02%。在Real World Knowledge Unlearning（RWKU）基准测试中，该方法实现了11%的遗忘效果，同时保持了98%的原始模型效用，证明其在有效遗忘的同时最小化了性能损失。这些数据表明，PURGE在效率和效果上均具有明显优势，为实际部署提供了可靠支持。",
      "conclusion": "本研究的主要贡献是提出了PURGE方法，展示了将LLM遗忘任务形式化为可验证问题的有效性，从而提供了更强的理论保证、安全性和部署效率。它强调了结合强化学习与内在奖励在遗忘研究中的潜力，为应对法规合规和隐私保护挑战开辟了新方向。尽管方法在基准测试中表现优异，但未来工作可能涉及扩展到更复杂的数据类型和更大规模的模型，以进一步提升其泛化能力和实际应用价值。",
      "tags": [
        "Reinforcement Learning",
        "Unlearning",
        "Group Relative Policy Optimization",
        "Large Language Models",
        "Intrinsic Reward"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:25.692215Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20564",
    "title": "DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression",
    "authors": [
      "Wenzhuo Ma",
      "Zhenzhong Chen"
    ],
    "abstract": "The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20564.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20564",
    "published": "2026-01-28T12:59:25Z",
    "updated": "2026-01-28T12:59:25Z",
    "comment": "17 pages, 10 figures",
    "light_analysis": {
      "overview": "DiffVC-RT是首个实现实时扩散基于感知神经视频压缩的框架，通过高效架构、一致性建模和异步解码解决部署挑战。",
      "motivation": "扩散基于神经视频压缩（NVC）在实际部署中面临严重信息丢失、高推理延迟和时间一致性差等关键问题。这些问题阻碍了其在实际应用中的普及，尤其是在实时视频压缩场景中。现有方法往往无法在保持视频感知质量的同时，实现低延迟和稳定帧间一致性，导致生成闪烁和效率低下。因此，迫切需要开发一个兼顾高质量和实时性的解决方案，以推动扩散模型在视频压缩领域的实用化进程。",
      "method": "论文提出了DiffVC-RT框架，采用三部分创新方法。首先，设计高效信息模型架构，通过战略模块替换和剪枝，显著降低计算复杂性并减少结构信息损失。其次，引入明确和隐式一致性建模，明确部分集成零成本在线时间偏移模块于U-Net中，隐式部分通过混合约束增强时间一致性，有效减少生成闪烁。最后，开发异步并行解码管道，结合混合半精度技术，利用批处理维度时间偏移设计实现异步潜在解码和并行帧重建，提升整体解码效率。",
      "result": "实验结果显示，DiffVC-RT在HEVC数据集上对比基线VTM-17.0，在LPIPS度量下实现了80.1%的比特率节省，显著提高了压缩效率。在性能方面，在NVIDIA H800 GPU上处理720p视频时，编码速度达到206 fps，解码速度达到30 fps，满足了实时处理需求。这些数据表明，该框架在保持高质量视频重建的同时，有效克服了信息丢失和延迟问题，为扩散基于视频压缩的实际部署提供了有力支持。",
      "conclusion": "本研究的核心贡献是提出并验证了DiffVC-RT框架，首次实现实时扩散基于感知神经视频压缩，解决了信息损失、延迟高和一致性差等关键挑战。该框架通过高效架构、一致性建模和异步解码，具有重要的学术创新和实际应用价值，为扩散模型在视频压缩领域的实用化奠定了基础。摘要未明确说明具体局限性或未来工作方向，但可以推断未来可能优化速度或扩展至高分辨率应用场景。",
      "tags": [
        "Diffusion Models",
        "Neural Video Compression",
        "Temporal Consistency Modeling",
        "Asynchronous Decoding",
        "Half Precision"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:47.013323Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20556",
    "title": "Unsupervised Ensemble Learning Through Deep Energy-based Models",
    "authors": [
      "Ariel Maymon",
      "Yanir Buznah",
      "Uri Shaham"
    ],
    "abstract": "Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20556.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20556",
    "published": "2026-01-28T12:50:08Z",
    "updated": "2026-01-28T12:50:08Z",
    "comment": "Accepted to AISTATS 2026. 29 pages, 13 figures. Code available at: https://github.com/shaham-lab/deem",
    "light_analysis": {
      "overview": "本文提出了一种基于深度能量的无监督集成学习方法，仅使用个体学习器预测构建元学习器，以捕捉复杂依赖结构。",
      "motivation": "无监督集成学习致力于解决在没有地面真实标签或额外数据时结合多个学习器预测的问题，这在信息有限、难以评估个体分类器性能的场景（如数据稀缺或隐私敏感环境）中至关重要。现有方法通常需要标签数据或特定信息，限制了其在资源受限情况下的应用，因此开发无需这些资源的方法具有重要实践意义，以充分利用集体智能并应对现实挑战。",
      "method": "论文提出了一种基于深度能量的方法来构建元学习器，仅依赖于个体学习器的预测，无需标签数据、学习器特征或问题特定信息。该方法通过深度能量模型捕捉学习器之间的复杂依赖结构，其核心创新在于自动化学习和融合多源专业知识，并在学习器条件独立时提供理论保证，确保了方法的鲁棒性和通用性。",
      "result": "实验表明，该方法在多种集成场景中表现优越，包括具有挑战性的专家混合设置。研究使用标准集成数据集和专门设计的策展数据集来测试模型融合多个来源专业知识的能力，尽管摘要未明确说明具体性能指标数据，但强调了其相较于基线方法的优越性，验证了模型在数据稀缺或隐私敏感环境中的有效性和泛化潜力。",
      "conclusion": "本研究的贡献在于提出了一种基于深度能量的无监督集成学习方法，无需标签数据即可构建准确元学习器，其学术价值在于提供理论保证和通用框架，实际应用价值在于数据稀缺或隐私敏感环境中利用集体智能。未来工作可能包括扩展方法处理更复杂依赖结构或应用于更广泛场景，以进一步提升方法的适应性和实用性。",
      "tags": [
        "Unsupervised Ensemble Learning",
        "Deep Energy-based Models",
        "Meta-Learning",
        "Conditional Independence",
        "Mixture of Experts"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:43.100720Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20554",
    "title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function",
    "authors": [
      "Yaacov Pariente",
      "Vadim Indelman"
    ],
    "abstract": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $α$, where $α= 1$ recovers standard expectation-based planning and $α< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20554.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20554",
    "published": "2026-01-28T12:48:20Z",
    "updated": "2026-01-28T12:48:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于迭代条件风险价值（ICVaR）的在线风险厌恶规划方法，扩展了多种POMDP规划算法以优化风险敏感目标。",
      "motivation": "该研究旨在解决部分可观察马尔可夫决策过程（POMDPs）中风险敏感规划的挑战。在不确定环境下，标准基于期望的规划方法忽略尾部风险，导致在高风险场景（如机器人导航或金融决策）中可能做出不利决策。现有方法未能有效集成动态风险度量，如ICVaR，因此需要开发新算法来弥补这一不足，以提升决策的安全性和鲁棒性。摘要未明确说明现有方法的具体局限性细节，但强调了风险规避的重要性。",
      "method": "论文开发了一种ICVaR的策略评估算法，具有有限时间性能保证，且不依赖于动作空间的基数。以此为基础，扩展了三种常用在线规划算法：Sparse Sampling、PFT-DPW和POMCPOW，使其优化ICVaR值函数而非回报期望。引入风险参数α（α=1恢复标准规划，α<1增加风险厌恶），并针对ICVaR Sparse Sampling建立了理论保证，提出了一种新的探索策略。关键创新点包括ICVaR的动态风险度量应用和算法的风险敏感扩展。",
      "result": "在基准POMDP领域的实验中，提出的ICVaR规划器相比风险中性规划器实现了更低的尾部风险，证明了其在风险敏感环境中的有效性。具体性能指标（如风险值降低幅度）摘要未明确说明，但实验对比显示ICVaR方法在规避尾部风险方面具有优势，为实际应用提供了实证支持。",
      "conclusion": "本研究的主要贡献是开发了基于ICVaR的风险敏感规划框架，并将其集成到多种在线POMDP算法中，提供了理论保证和实验验证。学术价值在于扩展了风险度量在规划领域的应用，实际意义在于适用于需要风险规避的场景，如自动驾驶和机器人控制。未来工作可能包括在更复杂环境中的测试或结合其他风险度量方法。",
      "tags": [
        "Partially Observable Markov Decision Processes (POMDPs)",
        "Iterated Conditional Value-at-Risk (ICVaR)",
        "Online Planning",
        "Risk-Averse Planning",
        "Sparse Sampling"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:47.199248Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20552",
    "title": "DeepSeek-OCR 2: Visual Causal Flow",
    "authors": [
      "Haoran Wei",
      "Yaofeng Sun",
      "Yukun Li"
    ],
    "abstract": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20552.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20552",
    "published": "2026-01-28T12:46:07Z",
    "updated": "2026-01-28T12:46:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出DeepEncoder V2，通过动态重新排序视觉tokens模拟人类视觉的因果推理，探索通过1D因果结构实现2D图像理解的新范式。",
      "motivation": "传统视觉-语言模型在处理视觉tokens时采用固定的光栅扫描顺序和位置编码，这与人类视觉感知的灵活、语义连贯扫描模式不符，尤其在复杂布局图像中可能导致理解效率低下。现有方法忽视了基于逻辑结构的因果推理顺序，本研究旨在解决如何让模型更符合人类认知机制，提升图像理解能力。",
      "method": "研究提出DeepEncoder V2编码器，具备因果推理能力，能够根据图像语义动态重新排序视觉tokens。关键创新在于模拟人类视觉的因果推理过程，通过两个级联的1D结构实现2D理解，在输入LLM之前进行智能排序。摘要未明确说明具体数据集或模型架构细节，但编码器设计使处理顺序不再固定。",
      "result": "摘要未明确说明具体的实验结果和性能指标。基于摘要信息，该工作主要探索了新范式的可行性，没有提供与基线方法的对比数据。然而，它公开了代码和模型权重，表明已实现初步验证，但需要进一步实验支撑其效果提升。",
      "conclusion": "本研究的贡献在于提出了DeepEncoder V2，通过引入因果推理使视觉tokens处理更符合人类感知，为2D图像理解提供了新架构方法。学术价值在于探索了1D因果结构实现2D推理的可能性，实际应用可能提升视觉-语言任务性能。局限性包括需在更多数据集验证，未来工作可扩展到其他视觉任务和模型优化。",
      "tags": [
        "Visual-Language Models",
        "Causal Reasoning",
        "Large Language Models",
        "DeepEncoder",
        "Token Reordering"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:30.100662Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20546",
    "title": "Beyond Divergent Creativity: A Human-Based Evaluation of Creativity in Large Language Models",
    "authors": [
      "Kumiko Nakajima",
      "Jan Zuiderveld",
      "Sandro Pezzelle"
    ],
    "abstract": "Large language models (LLMs) are increasingly used in verbal creative tasks. However, previous assessments of the creative capabilities of LLMs remain weakly grounded in human creativity theory and are thus hard to interpret. The widely used Divergent Association Task (DAT) focuses on novelty, ignoring appropriateness, a core component of creativity. We evaluate a range of state-of-the-art LLMs on DAT and show that their scores on the task are lower than those of two baselines that do not possess any creative abilities, undermining its validity for model evaluation. Grounded in human creativity theory, which defines creativity as the combination of novelty and appropriateness, we introduce Conditional Divergent Association Task (CDAT). CDAT evaluates novelty conditional on contextual appropriateness, separating noise from creativity better than DAT, while remaining simple and objective. Under CDAT, smaller model families often show the most creativity, whereas advanced families favor appropriateness at lower novelty. We hypothesize that training and alignment likely shift models along this frontier, making outputs more appropriate but less creative. We release the dataset and code.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20546.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20546",
    "published": "2026-01-28T12:41:32Z",
    "updated": "2026-01-28T12:41:32Z",
    "comment": "Accepted to Findings of EACL 2026",
    "light_analysis": {
      "overview": "论文引入条件发散关联任务（CDAT），基于人类创意理论评估大型语言模型的创造力，发现较小模型更富创意，先进模型更偏适当性。",
      "motivation": "研究动机源于现有评估大型语言模型（LLMs）创意能力的方法，如发散关联任务（DAT），仅关注新颖性而忽略适当性，这与人类创造力理论定义（新颖与适当的结合）不符。随着LLMs在语言创意任务中应用增多，DAT的局限性导致评估结果难以解释，阻碍了对模型创意潜力的准确理解。因此，需要一种更全面的评估框架，以结合理论指导，解决现有方法评估无效的问题。",
      "method": "研究方法基于人类创意理论，提出条件发散关联任务（CDAT），在DAT基础上加入上下文适当性条件，以评估在适当性前提下的新颖性。方法涉及评估多种先进LLMs，与两个无创意能力的基线模型进行对比，确保评估客观简单。关键创新点在于整合理论框架，设计出能更好分离噪音和创意的任务，避免了DAT的片面性，同时保持评估的简便性。",
      "result": "实验结果显示，在DAT上，LLMs的得分低于两个不具备创意能力的基线模型，这削弱了DAT用于模型评估的有效性。在CDAT下，较小模型家族表现出更高的创意得分，而先进模型家族则倾向于输出更适当但新颖性较低的内容。这表明训练和对齐过程可能使模型在创意和适当性之间做出权衡，验证了假设，并突出了CDAT作为更可靠评估工具的优势。",
      "conclusion": "论文结论强调了CDAT在评估LLMs创造力方面的重要性，基于人类理论提供了一种更有效的评估方法，弥补了现有不足。学术价值在于改进评估框架，促进对模型创意能力的理解；实际应用价值在于指导LLMs在创意任务中的开发和优化，例如设计更平衡的模型。未来工作可能包括进一步验证训练影响，或探索其他创意维度。",
      "tags": [
        "Large Language Models",
        "Creativity Evaluation",
        "Divergent Association Task",
        "Human Creativity Theory",
        "Model Alignment"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:36.594171Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20540",
    "title": "Advancing Open-source World Models",
    "authors": [
      "Robbyant Team",
      "Zelin Gao",
      "Qiuyu Wang",
      "Yanhong Zeng",
      "Jiapeng Zhu",
      "Ka Leong Cheng",
      "Yixuan Li",
      "Hanlin Wang",
      "Yinghao Xu",
      "Shuailei Ma",
      "Yihang Chen",
      "Jie Liu",
      "Yansong Cheng",
      "Yao Yao",
      "Jiayi Zhu",
      "Yihao Meng",
      "Kecheng Zheng",
      "Qingyan Bai",
      "Jingye Chen",
      "Zehong Shen",
      "Yue Yu",
      "Xing Zhu",
      "Yujun Shen",
      "Hao Ouyang"
    ],
    "abstract": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20540.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20540",
    "published": "2026-01-28T12:37:01Z",
    "updated": "2026-01-28T12:37:01Z",
    "comment": "Project page: https://technology.robbyant.com/lingbot-world; Code: https://github.com/robbyant/lingbot-world",
    "light_analysis": {
      "overview": "LingBot-World是一个基于视频生成的开源世界模拟器，提供高保真、长时记忆和实时交互能力。",
      "motivation": "本研究旨在解决开源世界模型在性能和功能上的不足，通过开发高质量的模型来缩小与闭源技术的差距。世界模型在AI应用中至关重要，例如内容创作和机器人学习，但现有开源模型可能在保真度、长时间一致性或实时交互方面存在限制。摘要未明确说明具体不足，但强调了提供开源工具以促进社区创新的重要性，推动实际应用的发展。",
      "method": "论文提出LingBot-World，一个源自视频生成的世界模拟器。其核心方法基于视频生成技术，关键创新点包括在多种环境中实现高保真和鲁棒动力学，支持长达分钟级别的时间范围以保持上下文一致性（称为'长时记忆'），并实现实时交互，在每秒生成16帧时延迟低于1秒。模型架构和数据集细节在摘要中未明确说明，但强调了这些技术特色作为主要设计目标。",
      "result": "LingBot-World展现出高保真性和鲁棒性，适用于现实主义、科学背景和卡通风格等多种环境。它成功实现了分钟级别的长时间模拟，保持上下文一致性，并在实时交互中达到延迟低于1秒的性能。摘要未提供与基线方法的对比数据，但突出展示了这些特性作为主要成果，证明了模型在实际应用中的潜力和有效性。",
      "conclusion": "论文的主要贡献是发布了LingBot-World，一个具有高保真、长时记忆和实时交互能力的开源世界模型。这推动了开源AI技术的发展，具有重要的学术价值，并为内容创作、游戏和机器人学习等领域的实际应用提供了工具。摘要未明确讨论局限性或未来工作，但暗示通过公开代码和模型，可以进一步促进社区创新和技术进步。",
      "tags": [
        "World Model",
        "Video Generation",
        "Real-time Interaction",
        "Open-source",
        "Long-term Memory"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:44.427776Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20539",
    "title": "PathWise: Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs",
    "authors": [
      "Oguzhan Gungordu",
      "Siheng Xiong",
      "Faramarz Fekri"
    ],
    "abstract": "Large Language Models (LLMs) have enabled automated heuristic design (AHD) for combinatorial optimization problems (COPs), but existing frameworks' reliance on fixed evolutionary rules and static prompt templates often leads to myopic heuristic generation, redundant evaluations, and limited reasoning about how new heuristics should be derived. We propose a novel multi-agent reasoning framework, referred to as Planning through World Model for Automated Heuristic Design via Self-Evolving LLMs (PathWise), which formulates heuristic generation as a sequential decision process over an entailment graph serving as a compact, stateful memory of the search trajectory. This approach allows the system to carry forward past decisions and reuse or avoid derivation information across generations. A policy agent plans evolutionary actions, a world model agent generates heuristic rollouts conditioned on those actions, and critic agents provide routed reflections summarizing lessons from prior steps, shifting LLM-based AHD from trial-and-error evolution toward state-aware planning through reasoning. Experiments across diverse COPs show that PathWise converges faster to better heuristics, generalizes across different LLM backbones, and scales to larger problem sizes.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20539.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20539",
    "published": "2026-01-28T12:34:50Z",
    "updated": "2026-01-28T12:34:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "PathWise提出一种基于多智能体推理和世界模型的新框架，实现自动化启发式设计从试错进化到状态感知规划的转变。",
      "motivation": "论文旨在解决自动化启发式设计（AHD）中现有框架的局限性。这些框架依赖固定的进化规则和静态提示模板，导致启发式生成短视、评估冗余，且对新启发式推导的推理能力有限，从而降低了在组合优化问题中的效率和可扩展性。这强调了开发一种能规划搜索轨迹、携带历史决策信息的方法的重要性，以提升AHD的全局优化能力。",
      "method": "PathWise框架将启发式生成表述为一个序列决策过程，使用蕴涵图作为搜索轨迹的紧凑、有状态记忆。它采用多智能体系统，包括政策智能体规划进化动作、世界模型智能体基于动作生成启发式展开，以及批评智能体提供路由反思以总结先前经验。基于大型语言模型，该方法通过推理替代传统试错进化，实现状态感知规划，从而提高自动化启发式设计的准确性和效率。",
      "result": "实验结果显示，PathWise在各种组合优化问题中能更快地收敛到更好的启发式方法，提高了搜索效率和质量。它表现出良好的泛化能力，能适应不同的大型语言模型骨干，并可扩展以处理更大规模的问题。与现有AHD框架相比，PathWise通过推理规划减少了冗余评估，显著提升了性能。摘要未提供具体数据，但强调了整体效果优于基线方法。",
      "conclusion": "PathWise的主要贡献是提出了一种创新框架，通过多智能体推理和世界模型，将自动化启发式设计从试错进化转向状态感知规划，从而提升了效率和启发式质量。该研究在学术上推动了大型语言模型在组合优化领域的应用，具有实际价值，能自动生成有效启发式以减少人工设计成本。未来工作方向摘要未明确说明，但可能包括扩展到更多问题类型或优化推理机制。",
      "tags": [
        "Large Language Models",
        "Automated Heuristic Design",
        "Multi-Agent Reasoning",
        "World Model",
        "Combinatorial Optimization Problems"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:31.499020Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20526",
    "title": "IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework",
    "authors": [
      "Shaokun Wang",
      "Yifan Yu",
      "Yuhang He",
      "Weili Guan",
      "Yihong Gong"
    ],
    "abstract": "Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20526.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20526",
    "published": "2026-01-28T12:03:48Z",
    "updated": "2026-01-28T12:03:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "IOTA 提出了一种结合黑白盒模块的提示学习框架，通过纠正知识引导提升下游任务适应能力。",
      "motivation": "现有参数高效调优（PET）方法将预训练模型视为不透明的黑盒，仅依赖数据驱动的优化，未充分利用其内在先验知识，导致模型在下游任务适应中的潜力受限。这一问题在人工智能应用中尤为重要，因为高效的适应可以提高模型的实际性能。通过解决这一不足，研究旨在整合知识驱动信号，以优化模型适应效果，从而提高整体效率和准确性。",
      "method": "方法的核心是 IOTA 框架，它整合了数据驱动的黑盒模块和知识驱动的白盒模块。白盒模块通过对比错误预测与正确认知来派生纠正知识，这些知识被形式化为可解释的人类提示。然后，采用纠正知识引导的提示选择策略，将这些提示用于指导黑盒模块的优化，从而实现更准确的预测。关键创新在于结合知识和数据驱动的学习信号，以增强下游任务的适应能力，该方法在图像分类等任务中应用，但具体数据集细节未在摘要中明确说明。",
      "result": "实验在 12 个图像分类基准上进行，包括少样本和易到难适应设置，验证了纠正知识的有效性。结果表明，IOTA 方法在性能上优于现有最先进方法，具体体现为在适应任务中实现了更高的准确性，但摘要未提供具体的数值指标如准确率提升百分比。与基线方法相比，IOTA 展现了优越的适应能力，证明了整合知识和数据驱动信号的实用价值。",
      "conclusion": "主要贡献是提出了 IOTA 框架，通过结合黑白盒模块和纠正知识，有效提升下游任务适应能力。这项研究的学术价值在于引入了知识驱动信号到提示学习中，实际应用价值体现在优化预训练模型适应效果，提高图像分类等任务的性能。局限性或未来工作方向在摘要中未明确说明，但潜在方向可能包括扩展到更多任务或数据集。",
      "tags": [
        "Prompt Learning",
        "Black-Box Framework",
        "Knowledge-Guided Optimization",
        "Parameter-Efficient Tuning",
        "Image Classification"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:26.072305Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20524",
    "title": "AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors",
    "authors": [
      "Matic Fučka",
      "Vitjan Zavrtanik",
      "Danijel Skočaj"
    ],
    "abstract": "Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20524.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20524",
    "published": "2026-01-28T12:02:58Z",
    "updated": "2026-01-28T12:02:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出AnomalyVFM框架，通过合成数据集生成和参数高效适应机制，将预训练视觉基础模型转化为高性能零样本异常检测器。",
      "motivation": "零样本异常检测旨在无需域内训练图像即可检测图像异常区域。现有方法中，基于视觉语言模型（如CLIP）的方法表现优异，但基于纯视觉基础模型（如DINOv2）的方法性能滞后。这种差距源于现有辅助异常检测数据集多样性有限，以及VFMs适应策略过于浅显，导致知识转移不足。因此，需要改进VFMs的应用，以提升零样本场景下的检测准确性和泛化能力。",
      "method": "AnomalyVFM框架结合了三阶段合成数据集生成方案和参数高效适应机制。关键创新包括使用低秩特征适配器进行特征微调，以减少计算开销；置信加权像素损失优化训练过程，增强对异常区域的关注。该方法适用于任何预训练VFM，如RADIO作为主干模型，通过高效微调提升异常检测性能，无需重新训练整个模型。",
      "result": "实验结果表明，AnomalyVFM显著超越当前最先进方法。在9个不同数据集上，使用RADIO作为主干，平均图像级AUROC达到94.1%，比先前方法提升了3.3个百分点。这一改进证明了框架的高效性和普适性，在各种异常检测任务中均表现出优异性能，基线对比显示其在精度和鲁棒性方面的优势。",
      "conclusion": "本研究的主要贡献是提出AnomalyVFM框架，成功将视觉基础模型转化为强零样本异常检测器。其学术价值在于解决了VFMs适应问题，推动了零样本学习的发展；实际应用价值在于提高了异常检测的效率和准确性，可广泛应用于工业检测等领域。摘要未明确说明潜在局限性，未来工作可能涉及扩展更多模型和数据集验证。",
      "tags": [
        "Vision Foundation Models",
        "Zero-Shot Anomaly Detection",
        "Synthetic Dataset Generation",
        "Low-Rank Adaptation",
        "Confidence-Weighted Loss"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:58.242866Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20520",
    "title": "Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective",
    "authors": [
      "Qiyan Zhao",
      "Xiaofeng Zhang",
      "Shuochen Chang",
      "Qianyu Chen",
      "Xiaosong Yuan",
      "Xuhang Chen",
      "Luoqi Liu",
      "Jiajun Zhang",
      "Xu-Yao Zhang",
      "Da-Han Wang"
    ],
    "abstract": "Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \\textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \\textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20520.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20520",
    "published": "2026-01-28T11:54:42Z",
    "updated": "2026-01-28T11:54:42Z",
    "comment": "Accepted in ICLR 2026",
    "light_analysis": {
      "overview": "本研究基于信息流分析揭示dMLLMs重复诅咒机制，并提出可插拔的CoTA方法有效缓解重复文本生成。",
      "motivation": "近期扩散多模态大语言模型（dMLLMs）因推理延迟高而依赖缓存技术加速解码，但缓存机制常引入重复文本生成，即“重复诅咒”，严重影响输出质量和用户体验。现有方法虽关注缓存优化，却忽略从信息流角度分析根本原因，导致无法有效避免重复，阻碍dMLLMs的实际应用效率。因此，亟需深入研究此问题以提供新解决方案，提升模型在复杂任务中的稳定性和可靠性。",
      "method": "论文从信息流视角分析dMLLMs的重复生成机制，提出CoTA方法：通过增强上下文令牌的注意力来保护内在信息流模式，确保语义信息的有效聚合；同时在解码阶段引入惩罚项至置信度分数，避免输出由不确定上下文令牌驱动。关键创新包括信息流熵收敛分析和可插拔设计，无需改动模型架构，适用于一般dMLLMs框架，数据集和具体模型架构摘要未明确说明。",
      "result": "实验表明CoTA在减轻重复生成方面效果显著，并在多项通用任务上实现了一致的性能提升。与基线方法相比，CoTA能有效减少重复现象，提高输出多样性和准确性，具体性能指标摘要未明确说明，但确认了其在dMLLMs中的优化作用，为实际应用提供了支持。",
      "conclusion": "本研究的主要贡献在于从信息流角度解析重复诅咒的根源，并提出CoTA方法作为一种实用解决方案，增强了dMLLMs的生成质量。学术价值在于为模型优化引入新视角，促进多模态任务中的信息流控制；实际应用有助于提升解码效率和输出稳定性。未来工作可扩展至其他模型类型或进一步优化惩罚项设计，以应对更复杂的重复模式。",
      "tags": [
        "Diffusion-based Multimodal Large Language Models",
        "Information Flow Analysis",
        "Attention Mechanism",
        "Decoding Optimization",
        "Repetition Mitigation"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:41.749582Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20518",
    "title": "CCMamba: Selective State-Space Models for Higher-Order Graph Learning on Combinatorial Complexes",
    "authors": [
      "Jiawen Chen",
      "Qi Shao",
      "Mingtong Zhou",
      "Duxin Chen",
      "Wenwu Yu"
    ],
    "abstract": "Topological deep learning has emerged for modeling higher-order relational structures beyond pairwise interactions that standard graph neural networks fail to capture. Although combinatorial complexes offer a unified topological framework, most existing topological deep learning methods rely on local message passing via attention mechanisms, which incur quadratic complexity and remain low-dimensional, limiting scalability and rank-aware information aggregation in higher-order complexes.We propose Combinatorial Complex Mamba (CCMamba), the first unified mamba-based neural framework for learning on combinatorial complexes. CCMamba reformulates message passing as a selective state-space modeling problem by organizing multi-rank incidence relations into structured sequences processed by rank-aware state-space models. This enables adaptive, directional, and long range information propagation in linear time without self attention. We further establish the theoretical analysis that the expressive power upper-bound of CCMamba message passing is the 1-Weisfeiler-Lehman test. Experiments on graph, hypergraph, and simplicial benchmarks demonstrate that CCMamba consistently outperforms existing methods while exhibiting improved scalability and robustness to depth.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20518.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20518",
    "published": "2026-01-28T11:52:13Z",
    "updated": "2026-01-28T11:52:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了CCMamba，首个基于Mamba的统一神经框架，用于组合复杂体的高阶图学习，通过选择性状态空间模型实现线性时间信息传播。",
      "motivation": "标准图神经网络无法建模超越二元交互的高阶关系结构，因此拓扑深度学习应运而生。组合复杂体作为统一拓扑框架，但现有方法依赖注意机制的局部消息传递，导致二次计算复杂度，且保持低维处理，限制了可扩展性和在高阶复杂体中聚合秩感知信息。这一问题的重要性在于高效处理复杂网络结构对社交网络、生物信息学等领域应用至关重要，而现有方法的不足阻碍了大规模复杂网络的深度分析。",
      "method": "CCMamba通过将多秩关联关系组织成结构化序列，并利用秩感知状态空间模型进行处理，将消息传递重新表述为选择性状态空间建模问题。关键创新点包括实现自适应、定向和长范围信息传播，无需自注意机制，从而以线性时间完成计算。该方法基于Mamba框架，具体模型架构和数据集摘要未明确说明，但强调了对组合复杂体中各种关系的统一处理，提升了信息聚合效率。",
      "result": "在多种基准测试中，包括图、超图和单纯复形，CCMamba一致优于现有方法，具体性能指标如准确率提升摘要未明确说明。实验显示其在可扩展性方面有所改进，减少了计算复杂度，并且对深度模型表现出更强的鲁棒性。与基线方法的对比验证了该方法在高阶图学习任务中的有效性，尽管缺乏详细数据，但结果突出了其通用性和效率优势。",
      "conclusion": "CCMamba的主要贡献在于提供了一个高效、统一的框架用于高阶图学习，通过理论分析表明其表达能力上界为1-Weisfeiler-Lehman测试。这项研究具有重要学术价值，推动了拓扑深度学习领域的发展，并为复杂网络分析等实际应用提供了新工具。未来工作方向可能涉及扩展到更复杂的拓扑结构或优化模型细节，摘要未明确说明具体局限性。",
      "tags": [
        "Combinatorial Complexes",
        "Mamba",
        "State-Space Models",
        "Higher-Order Graph Learning",
        "1-Weisfeiler-Lehman Test"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:54.921613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20511",
    "title": "Say Cheese! Detail-Preserving Portrait Collection Generation via Natural Language Edits",
    "authors": [
      "Zelong Sun",
      "Jiahui Wu",
      "Ying Ba",
      "Dong Jing",
      "Zhiwu Lu"
    ],
    "abstract": "As social media platforms proliferate, users increasingly demand intuitive ways to create diverse, high-quality portrait collections. In this work, we introduce Portrait Collection Generation (PCG), a novel task that generates coherent portrait collections by editing a reference portrait image through natural language instructions. This task poses two unique challenges to existing methods: (1) complex multi-attribute modifications such as pose, spatial layout, and camera viewpoint; and (2) high-fidelity detail preservation including identity, clothing, and accessories. To address these challenges, we propose CHEESE, the first large-scale PCG dataset containing 24K portrait collections and 573K samples with high-quality modification text annotations, constructed through an Large Vison-Language Model-based pipeline with inversion-based verification. We further propose SCheese, a framework that combines text-guided generation with hierarchical identity and detail preservation. SCheese employs adaptive feature fusion mechanism to maintain identity consistency, and ConsistencyNet to inject fine-grained features for detail consistency. Comprehensive experiments validate the effectiveness of CHEESE in advancing PCG, with SCheese achieving state-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20511.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20511",
    "published": "2026-01-28T11:41:27Z",
    "updated": "2026-01-28T11:41:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种通过自然语言指令编辑参考肖像图像以生成连贯肖像集合的新方法，并引入了首个大规模数据集 CHEESE 和框架 SCheese，以解决细节保存和身份一致性问题。",
      "motivation": "随着社交媒体平台的普及，用户迫切需要直观的方式来创建多样化和高质量的肖像集合。然而，现有方法在处理此类任务时面临两大挑战：一是难以执行复杂多属性修改（如姿势、空间布局和相机视角）；二是无法有效保持高保真细节（如身份、服装和配件）。这些问题导致生成的肖像集合连贯性差，限制了应用潜力，因此本研究旨在填补这一空白，提升肖像编辑技术的实用性和真实性。",
      "method": "本研究首先构建了首个大规模 Portrait Collection Generation 数据集 CHEESE，包含 24K 肖像集合和 573K 样本，通过基于大型视觉语言模型的流水线生成高质量修改文本注释，并使用反演验证确保数据质量。进一步提出了 SCheese 框架，该框架整合了文本引导生成方法，通过自适应特征融合机制维护身份一致性，并引入 ConsistencyNet 注入细粒度特征以实现细节一致性，形成层次化身份和细节保存策略，以解决多属性修改和细节保存的挑战。",
      "result": "通过全面实验，本研究验证了 CHEESE 数据集在推动 Portrait Collection Generation 任务上的有效性。SCheese 框架在实验中取得了最先进的性能表现，显示出其在生成连贯肖像集合和保持高保真细节方面的优势，与基线方法的对比表明其在身份一致性和细节保存方面显著优于现有技术。摘要未明确说明具体性能指标如准确率提升。",
      "conclusion": "本研究的主要贡献在于首次定义了 Portrait Collection Generation 任务，并构建了大规模数据集 CHEESE 以支持相关研究，同时提出了 SCheese 框架以有效解决身份和细节保存问题。该研究在学术上推动了基于自然语言的图像编辑领域的发展，在实际应用上为社交媒体用户提供了创建高质量肖像集合的直观工具，具有重要的理论和实践价值。摘要未明确说明局限性，未来工作可能包括扩展数据集和改进框架以处理更复杂的编辑场景。",
      "tags": [
        "Portrait Collection Generation",
        "Natural Language Editing",
        "Large Vision-Language Model",
        "Feature Fusion",
        "ConsistencyNet"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:26.525400Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20504",
    "title": "Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V",
    "authors": [
      "Meiqi Wu",
      "Bingze Song",
      "Ruimin Lin",
      "Chen Zhu",
      "Xiaokun Feng",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Kaiqi Huang"
    ],
    "abstract": "Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20504.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20504",
    "published": "2026-01-28T11:27:23Z",
    "updated": "2026-01-28T11:27:23Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "论文提出了一种基于潜在时间差异的运动先验损失加权策略，以提升视频生成模型在动态场景中的表现。",
      "motivation": "视频生成模型在静态场景中已取得显著进展，但在动态视频生成方面性能受限，尤其是在动态变化剧烈时质量下降，这源于噪声破坏时间一致性和增加学习动态区域的难度。现有扩散模型依赖静态损失处理所有场景，限制了其捕捉复杂动态的能力，而动态视频生成对于视频编辑、虚拟现实等实际应用至关重要，因此需改进模型以更好地处理动态区域，提升生成质量。",
      "method": "论文引入了潜在时间差异作为运动先验来指导损失加权，LTD在潜在空间中测量帧与帧之间的变化，根据差异大小分配惩罚权重，差异较大的区域获得更高损失权重，而稳定区域保持常规优化。这种运动感知策略通过动态调整损失，稳定训练过程，并增强模型对高频动态区域的重建能力，创新点在于将运动先验融入扩散模型框架，以更有效地学习时间连贯性。",
      "result": "在通用基准VBench和专注于运动的VMBench上进行大量实验，结果表明该方法在VBench上优于强基线3.31%，在VMBench上优于基线3.58%，实现了运动质量的显著提升，并显示出一致增益，验证了LTD策略在改善动态视频生成效果方面的有效性，具体表现为模型能更好地处理动态变化，减少质量退化。",
      "conclusion": "论文的主要贡献是通过LTD作为运动先验的损失加权策略，有效解决了视频生成模型中动态区域学习不足的问题，提升了时间一致性和运动质量。学术上提供了新的运动感知优化方法，应用上可促进视频生成技术在动态场景中的实际应用，未来工作可能包括扩展到更复杂的动态模型或与其他生成技术结合以进一步优化性能。",
      "tags": [
        "Video Generation",
        "Diffusion Models",
        "Loss Weighting",
        "Temporal Discrepancy",
        "Motion Prior"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:29.785620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20503",
    "title": "Comparative evaluation of training strategies using partially labelled datasets for segmentation of white matter hyperintensities and stroke lesions in FLAIR MRI",
    "authors": [
      "Jesse Phitidis",
      "Alison Q. Smithard",
      "William N. Whiteley",
      "Joanna M. Wardlaw",
      "Miguel O. Bernabeu",
      "Maria Valdés Hernández"
    ],
    "abstract": "White matter hyperintensities (WMH) and ischaemic stroke lesions (ISL) are imaging features associated with cerebral small vessel disease (SVD) that are visible on brain magnetic resonance imaging (MRI) scans. The development and validation of deep learning models to segment and differentiate these features is difficult because they visually confound each other in the fluid-attenuated inversion recovery (FLAIR) sequence and often appear in the same subject. We investigated six strategies for training a combined WMH and ISL segmentation model using partially labelled data. We combined privately held fully and partially labelled datasets with publicly available partially labelled datasets to yield a total of 2052 MRI volumes, with 1341 and 1152 containing ground truth annotations for WMH and ISL respectively. We found that several methods were able to effectively leverage the partially labelled data to improve model performance, with the use of pseudolabels yielding the best result.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20503.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20503",
    "published": "2026-01-28T11:27:13Z",
    "updated": "2026-01-28T11:27:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文比较了六种利用部分标记数据集训练白质高信号和缺血性卒中病变分割模型的方法，发现伪标签策略性能最优。",
      "motivation": "白质高信号（WMH）和缺血性卒中病变（ISL）是脑小血管疾病的成像特征，在FLAIR MRI序列中视觉上容易混淆，且常共存于同一患者，导致分割和区分困难。开发深度学习模型进行精确分割对于疾病诊断至关重要，但现有方法因数据标注不足和特征混淆而受限，因此研究使用部分标记数据的训练策略以应对这些挑战具有重要实际意义。",
      "method": "本研究探讨了六种训练策略，使用部分标记数据训练一个结合WMH和ISL的分割模型。通过结合私有和公开的部分标记数据集，共收集2052个MRI体积，其中1341个有WMH标注，1152个有ISL标注。核心创新在于利用部分标记数据，包括使用伪标签等技术，以提升模型在标注数据稀缺情况下的性能，但具体六种策略的技术细节摘要未明确说明。",
      "result": "研究发现，多种方法能有效利用部分标记数据提升模型性能，其中伪标签方法在内部比较中表现最佳。摘要未明确说明具体的性能指标（如准确率或效率改进），也未提及与外部基线方法的直接对比，但结果表明伪标签策略在六种方法中取得了最优效果。",
      "conclusion": "本研究的贡献在于系统评估了使用部分标记数据的训练策略，为WMH和ISL分割提供了有效方法，尤其在数据标注有限的情况下。学术上，这推动了医学图像分割中半监督学习技术的发展；实际应用中，可能有助于脑疾病的自动化诊断和临床决策。摘要未明确提及研究的局限性或未来工作方向。",
      "tags": [
        "Medical Image Segmentation",
        "Partially Labelled Data",
        "Pseudolabels",
        "Deep Learning",
        "MRI Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:46.358183Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20499",
    "title": "Efficient Autoregressive Video Diffusion with Dummy Head",
    "authors": [
      "Hang Guo",
      "Zhaoyang Jia",
      "Jiahao Li",
      "Bin Li",
      "Yuanhao Cai",
      "Jiangshan Wang",
      "Yawei Li",
      "Yan Lu"
    ],
    "abstract": "The autoregressive video diffusion model has recently gained considerable research interest due to its causal modeling and iterative denoising. In this work, we identify that the multi-head self-attention in these models under-utilizes historical frames: approximately 25% heads attend almost exclusively to the current frame, and discarding their KV caches incurs only minor performance degradation. Building upon this, we propose Dummy Forcing, a simple yet effective method to control context accessibility across different heads. Specifically, the proposed heterogeneous memory allocation reduces head-wise context redundancy, accompanied by dynamic head programming to adaptively classify head types. Moreover, we develop a context packing technique to achieve more aggressive cache compression. Without additional training, our Dummy Forcing delivers up to 2.0x speedup over the baseline, supporting video generation at 24.3 FPS with less than 0.5% quality drop. Project page is available at https://csguoh.github.io/project/DummyForcing/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20499.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20499",
    "published": "2026-01-28T11:20:43Z",
    "updated": "2026-01-28T11:20:43Z",
    "comment": "Technical Report",
    "light_analysis": {
      "overview": "本研究提出Dummy Forcing方法，通过优化多头自注意力的上下文访问，实现了自回归视频扩散模型的高效推理，无需额外训练。",
      "motivation": "自回归视频扩散模型因其因果建模和迭代去噪特性而受到关注，但多头自注意力机制在利用历史帧方面存在不足，约25%的头部几乎只关注当前帧，导致KV缓存冗余，降低了推理效率。这个问题限制了实时视频生成等应用的发展，现有方法未能有效优化头部间的上下文分配，因此需要一种新方法来解决这种冗余问题。摘要未明确说明更广泛的背景，但基于现有信息，提高模型效率对于实际部署至关重要。",
      "method": "论文提出Dummy Forcing方法，包括异构内存分配以减少多头自注意力中的头部间上下文冗余，动态头部编程来自适应地分类头部类型，以及上下文打包技术以实现更激进的缓存压缩。该方法基于观察到的头部使用模式，通过控制不同头部的上下文可访问性来提高效率，直接应用于现有模型架构，无需额外训练或修改训练过程。",
      "result": "实验结果显示，使用Dummy Forcing方法后，模型在基准测试中实现了高达2.0倍的推理加速，支持以24.3 FPS的速度生成视频，同时质量下降小于0.5%。这表明在不牺牲生成质量的前提下，显著提升了处理效率，优于基线方法。摘要未提供具体数据集细节，但强调性能改进直接来自提出的技术。",
      "conclusion": "本研究通过Dummy Forcing方法优化了自回归视频扩散模型的多头自注意力机制，实现了高效推理，无需额外训练，提高了视频生成的实时性。这一贡献具有重要的学术价值，为注意力机制优化提供了新思路，并支持实际应用如视频合成。未来工作可探索该方法在其他序列建模任务中的扩展，以及进一步优化缓存策略的潜力。",
      "tags": [
        "Autoregressive Video Diffusion",
        "Multi-head Self-Attention",
        "KV Cache Compression",
        "Dummy Forcing",
        "Efficient Inference"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:04.213493Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20487",
    "title": "Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups",
    "authors": [
      "Nico Mutzner",
      "Taha Yasseri",
      "Heiko Rauhut"
    ],
    "abstract": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC",
      "econ.GN"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20487.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20487",
    "published": "2026-01-28T11:01:09Z",
    "updated": "2026-01-28T11:01:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示了在混合人-AI群体中，合作规范依赖于群体行为而非代理身份，支持规范性等价。",
      "motivation": "研究动机是探索人工智能代理引入人类群体时对合作社会规范的影响。先前研究主要集中在人类与AI的二元互动，缺乏对小群体中合作规范形成和维持的理解，这在实际人-AI协作中至关重要。本研究旨在填补这一空白，解决现有方法忽视群体动态的问题，为理解AI如何影响集体决策提供实证基础，强调了规范机制在促进有效合作中的重要性。",
      "method": "研究采用在线实验方法，使用重复的四玩家公共物品游戏（PGG）作为核心任务。每组包括三名人类参与者和一名机器人，机器人被随机标记为人类或AI，并遵循三种预定义决策策略：无条件合作、有条件合作或搭便车。样本包含236名参与者，通过行为数据分析合作动态，后续使用囚徒困境评估规范持久性。关键创新点是关注群体行为而非身份，突出互惠动态和行为惯性在规范形成中的作用。",
      "result": "实验结果显示，合作水平在机器人标记为人类或AI的条件下无显著差异。互惠群体动态和行为惯性是驱动合作的主要因素，规范机制在所有条件下功能相同。后续囚徒困境中未发现规范持久性或参与者规范感知的差异，表明合作依赖于群体行为而非代理身份。这些发现支持规范性等价模式，即在混合人-AI群体中合作机制与全人类群体相似，具体指标如合作率保持一致性，基线对比显示无统计学差异。",
      "conclusion": "本研究的主要贡献是证明了合作规范在混合人-AI群体中具有灵活性，能够扩展到AI代理，模糊了人类与AI在集体决策中的界限。学术价值在于深化了对人-AI交互中规范等价性的理解；实际应用价值为AI设计和社会整合提供指导，例如通过行为模拟促进合作。局限性可能包括样本规模或实验设置的简化，未来工作可探索更复杂群体动态或不同文化背景。",
      "tags": [
        "Public Goods Game",
        "Normative Equivalence",
        "Human-AI Cooperation",
        "Behavioral Inertia",
        "Reciprocal Dynamics"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:04.000612Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20480",
    "title": "An explainable framework for the relationship between dementia and glucose metabolism patterns",
    "authors": [
      "C. Vázquez-García",
      "F. J. Martínez-Murcia",
      "F. Segovia Román",
      "A. Forte",
      "J. Ramírez",
      "I. Illán",
      "A. Hernández-Segura",
      "C. Jiménez-Mesa",
      "Juan M. Górriz"
    ],
    "abstract": "High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20480.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20480",
    "published": "2026-01-28T10:50:20Z",
    "updated": "2026-01-28T10:50:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种结合半监督变分自编码器和相似性正则化的可解释框架，用于分析痴呆与神经影像中葡萄糖代谢模式的关联。",
      "motivation": "神经退行性疾病如痴呆的评估面临高维神经影像数据（如PET扫描）的挑战，因为疾病特征与临床指标间存在复杂的非线性关系。现有机器学习方法可能难以有效提取可解释的疾病相关模式，且缺乏适应性以应对不同临床目标或数据限制。因此，本研究旨在开发一个框架，将神经影像数据与痴呆进展的生物标志物对齐，以改进疾病理解并为临床研究提供工具。",
      "method": "研究方法基于半监督变分自编码器（VAE），引入灵活的相似性正则化项，将选定的潜在变量与痴呆进展的临床或生物标志物指标（如认知评分）对齐。该框架允许调整相似性度量和监督变量以适应特定目标。在ADNI数据集的PET扫描上应用，指导第一个潜在维度与认知评分对齐，以编码疾病相关特征。其余潜在变量处理混杂因素，如个体间变异性和站点效应，增强了模型的解释性和适应性。",
      "result": "实验结果显示，通过监督潜在变量生成的平均重建揭示了跨认知障碍水平的模式变化。体素级广义线性模型（GLM）分析发现关键脑区（主要是海马体）和主要静息状态网络（如默认模式网络和中央执行网络）的葡萄糖代谢降低，这些模式与阿尔茨海默病生物标志物一致。框架有效提取了疾病相关特征，但摘要未明确说明与基线方法的对比性能指标，仅表明能捕捉非线性关系和处理混杂因素。",
      "conclusion": "本研究的主要贡献是提出一个可解释且可适应的半监督VAE框架，用于分析神经退行性疾病与神经影像模式的关联。该框架通过对齐潜在变量与临床指标，提供了对疾病进展的深入见解，具有学术价值，可促进生物标志物发现和神经影像分析。实际应用中，可作为工具辅助临床研究和诊断。局限性包括摘要未明确说明未来工作方向，但可推断潜在扩展至其他疾病或整合多模态数据。",
      "tags": [
        "Variational Autoencoder",
        "Semi-supervised Learning",
        "Similarity Regularization",
        "PET Neuroimaging",
        "Generalized Linear Model (GLM)"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:19.083744Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20477",
    "title": "Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations",
    "authors": [
      "Kadircan Aksoy",
      "Peter Jung",
      "Protim Bhattacharjee"
    ],
    "abstract": "We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20477.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20477",
    "published": "2026-01-28T10:46:44Z",
    "updated": "2026-01-28T10:46:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过二元假设测试分析神经分类器训练动态，揭示泛化能力与KL散度单调改进的关系。",
      "motivation": "研究动机在于深入理解神经网络在监督训练中的动态行为，特别是泛化能力的形成机制。现有方法往往缺乏对训练过程的统计理论解释，导致模型性能优化和可解释性不足。该研究从假设测试角度出发，旨在分析表示层分布的演化，为解决泛化问题提供新视角，从而提高模型可靠性和应用潜力。摘要未明确说明具体应用场景，但暗示了理论分析的重要性。",
      "method": "论文提出将分类任务建模为表示层类条件分布之间的二元假设测试。核心方法包括使用KL散度作为度量，分析训练轨迹中这些分布的差异变化。通过实证研究，观察神经网络如何通过统计推断逐步逼近Neyman-Pearson最优决策规则，这结合了假设测试理论和深度学习的表示学习，关键创新在于将分类动态与统计最优性关联起来。摘要未提及具体数据集或模型架构细节。",
      "result": "实验结果显示，在训练过程中，泛化能力强的神经网络表现出KL散度的单调改进，这种改进与错误率指数相关，并使网络表示越来越接近Neyman-Pearson最优决策规则。摘要未提供具体性能指标数据如准确率提升，但强调了这一趋势与网络泛化性能的正相关性，为理论框架提供了实证支持，并暗示了与传统基线方法的对比优势。",
      "conclusion": "论文的主要贡献是建立了一个基于假设测试的理论框架来解释神经网络训练动态，并关联到泛化能力，增强了模型的可解释性。这具有学术价值，为理解深度学习提供了新思路，并可能启发改进的训练或正则化策略。未来工作方向包括将该框架扩展到更多网络类型或实际应用中，以进一步验证和优化其效果，摘要未明确说明局限性。",
      "tags": [
        "Binary Hypothesis Testing",
        "KL Divergence",
        "Neyman-Pearson Rule",
        "Neural Network Representations",
        "Supervised Training Dynamics"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:24.803914Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20476",
    "title": "Can We Improve Educational Diagram Generation with In-Context Examples? Not if a Hallucination Spoils the Bunch",
    "authors": [
      "Evanfiya Logacheva",
      "Arto Hellas",
      "Tsvetomila Mihaylova",
      "Juha Sorva",
      "Ava Heinonen",
      "Juho Leinonen"
    ],
    "abstract": "Generative artificial intelligence (AI) has found a widespread use in computing education; at the same time, quality of generated materials raises concerns among educators and students. This study addresses this issue by introducing a novel method for diagram code generation with in-context examples based on the Rhetorical Structure Theory (RST), which aims to improve diagram generation by aligning models' output with user expectations. Our approach is evaluated by computer science educators, who assessed 150 diagrams generated with large language models (LLMs) for logical organization, connectivity, layout aesthetic, and AI hallucination. The assessment dataset is additionally investigated for its utility in automated diagram evaluation. The preliminary results suggest that our method decreases the rate of factual hallucination and improves diagram faithfulness to provided context; however, due to LLMs' stochasticity, the quality of the generated diagrams varies. Additionally, we present an in-depth analysis and discussion on the connection between AI hallucination and the quality of generated diagrams, which reveals that text contexts of higher complexity lead to higher rates of hallucination and LLMs often fail to detect mistakes in their output.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20476.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20476",
    "published": "2026-01-28T10:45:28Z",
    "updated": "2026-01-28T10:45:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入基于修辞结构理论（RST）的方法，通过上下文示例减少AI幻觉，改进教育图表生成的质量。",
      "motivation": "生成人工智能在计算教育中广泛应用，但生成材料的质量引发教育者和学生担忧，特别是AI幻觉问题，导致图表输出与用户期望不符，影响教学准确性和可靠性。现有方法如标准大型语言模型在图表生成时易产生幻觉，缺乏有效的对齐机制，使得生成内容可能包含错误或偏离上下文。这凸显了开发减少幻觉、提高忠实度的方法的必要性，以提升教育AI工具的实际效果。",
      "method": "本研究提出一种基于修辞结构理论（RST）的新方法，用于图表代码生成。该方法通过提供上下文示例，利用RST的结构化框架引导模型输出，旨在减少幻觉并提高与用户期望的对齐。关键创新在于将RST应用于图表生成领域，以优化逻辑组织和连接性。评估过程包括计算机科学教育工作者评估150个由大型语言模型生成的图表，评估指标涵盖逻辑组织、连接性、布局美观和AI幻觉。此外，评估数据集还被用于探索自动图表评估的实用性。",
      "result": "初步实验结果显示，该方法能够降低事实幻觉率，并提高图表对提供上下文的忠实度。具体而言，相比基线方法（如标准LLM），幻觉现象有所减少，但未提供精确数据。由于LLMs的随机性，生成图表质量存在差异，表明方法效果有限但显著。进一步分析发现，文本上下文复杂度越高，幻觉率越高，且LLMs通常无法检测自身输出中的错误，这揭示了幻觉与图表质量之间的复杂关系。",
      "conclusion": "本研究的主要贡献是引入基于RST的方法，有效减少AI幻觉并提高教育图表生成的忠实度。学术上，提供了幻觉与图表质量关系的深入分析，为AI在教育领域的应用提供理论基础。实践上，该方法有望改进计算教育中生成材料的可靠性，提升用户体验。局限性在于LLMs随机性导致的图表质量波动，未来工作可专注于增强模型鲁棒性、开发错误检测机制或优化复杂上下文处理。",
      "tags": [
        "Large Language Model",
        "Rhetorical Structure Theory",
        "Diagram Generation",
        "AI Hallucination",
        "In-Context Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:13.624534Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20467",
    "title": "CtrlCoT: Dual-Granularity Chain-of-Thought Compression for Controllable Reasoning",
    "authors": [
      "Zhenxuan Fan",
      "Jie Cao",
      "Yang Dai",
      "Zheqi Lv",
      "Wenqiao Zhang",
      "Zhongle Xie",
      "Peng LU",
      "Beng Chin Ooi"
    ],
    "abstract": "Chain-of-thought (CoT) prompting improves LLM reasoning but incurs high latency and memory cost due to verbose traces, motivating CoT compression with preserved correctness. Existing methods either shorten CoTs at the semantic level, which is often conservative, or prune tokens aggressively, which can miss task-critical cues and degrade accuracy. Moreover, combining the two is non-trivial due to sequential dependency, task-agnostic pruning, and distribution mismatch. We propose \\textbf{CtrlCoT}, a dual-granularity CoT compression framework that harmonizes semantic abstraction and token-level pruning through three components: Hierarchical Reasoning Abstraction produces CoTs at multiple semantic granularities; Logic-Preserving Distillation trains a logic-aware pruner to retain indispensable reasoning cues (e.g., numbers and operators) across pruning ratios; and Distribution-Alignment Generation aligns compressed traces with fluent inference-time reasoning styles to avoid fragmentation. On MATH-500 with Qwen2.5-7B-Instruct, CtrlCoT uses 30.7\\% fewer tokens while achieving 7.6 percentage points higher than the strongest baseline, demonstrating more efficient and reliable reasoning. Our code will be publicly available at https://github.com/fanzhenxuan/Ctrl-CoT.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20467.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20467",
    "published": "2026-01-28T10:38:49Z",
    "updated": "2026-01-28T10:38:49Z",
    "comment": "16 pages, 9 figures, 11 tables",
    "light_analysis": {
      "overview": "论文提出了CtrlCoT框架，通过结合语义抽象和令牌级修剪的双粒度压缩方法，提升链式思维推理的效率和准确性。",
      "motivation": "链式思维（CoT）提示增强了大型语言模型的推理能力，但冗长的思维链导致高延迟和内存成本，这在实际应用中限制了效率。现有压缩方法存在不足：语义级压缩往往过于保守，无法显著减少令牌；而令牌级修剪则可能激进删除任务关键线索，损害准确性。此外，将两者结合面临序列依赖、任务无关修剪和分布不匹配的挑战，因此需要一种更优化的方法来解决这些瓶颈，以支持可控推理的广泛应用。",
      "method": "论文提出了CtrlCoT框架，采用双粒度压缩技术，通过三个核心组件实现：Hierarchical Reasoning Abstraction生成多层次语义粒度的思维链，以平衡抽象度；Logic-Preserving Distillation训练一个逻辑感知的修剪器，在动态修剪比率下保留数字和运算符等关键推理线索；Distribution-Alignment Generation将压缩后的轨迹与流畅的推理风格对齐，避免碎片化。在实验中，使用MATH-500数据集和Qwen2.5-7B-Instruct模型进行验证，展示了方法的可扩展性。",
      "result": "在MATH-500数据集上，使用Qwen2.5-7B-Instruct模型，CtrlCoT实现了显著的性能提升：相比于最强基线方法，它减少了30.7%的令牌使用量，同时准确率提高了7.6个百分点。这表明该框架在保持或增强推理正确性的同时，大大降低了计算资源开销，验证了其在效率和可靠性方面的优势，为后续应用提供了有力支撑。",
      "conclusion": "论文的主要贡献是提出CtrlCoT框架，通过双粒度压缩解决了CoT推理中的效率与准确性的平衡问题。其学术价值在于创新性地融合语义抽象和令牌修剪，提供了可控推理的新方法；实际应用上，可加速LLM的推理过程，提升系统性能。局限性或未来工作摘要未明确说明，可能包括扩展到更多数据集或模型、进一步优化压缩机制等方向。",
      "tags": [
        "Chain-of-Thought",
        "Compression",
        "Reasoning",
        "Hierarchical Abstraction",
        "LLM"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:39.316372Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20465",
    "title": "BMAM: Brain-inspired Multi-Agent Memory Framework",
    "authors": [
      "Yang Li",
      "Jiaxiang Liu",
      "Yusong Wang",
      "Yujie Wu",
      "Mingkun Xu"
    ],
    "abstract": "Language-model-based agents operating over extended interaction horizons face persistent challenges in preserving temporally grounded information and maintaining behavioral consistency across sessions, a failure mode we term soul erosion. We present BMAM (Brain-inspired Multi-Agent Memory), a general-purpose memory architecture that models agent memory as a set of functionally specialized subsystems rather than a single unstructured store. Inspired by cognitive memory systems, BMAM decomposes memory into episodic, semantic, salience-aware, and control-oriented components that operate at complementary time scales. To support long-horizon reasoning, BMAM organizes episodic memories along explicit timelines and retrieves evidence by fusing multiple complementary signals. Experiments on the LoCoMo benchmark show that BMAM achieves 78.45 percent accuracy under the standard long-horizon evaluation setting, and ablation analyses confirm that the hippocampus-inspired episodic memory subsystem plays a critical role in temporal reasoning.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20465.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20465",
    "published": "2026-01-28T10:36:03Z",
    "updated": "2026-01-28T10:36:03Z",
    "comment": "Submitted to ACL (ARR 2026 January submission); non-anonymous preprint",
    "light_analysis": {
      "overview": "BMAM提出一个受大脑启发的多代理记忆框架，通过功能分解记忆子系统，解决了语言模型代理在长交互中信息丢失和行为一致性问题。",
      "motivation": "语言模型代理在长时间交互中面临“灵魂侵蚀”问题，即信息丢失和行为不一致，这影响AI代理的可靠性和性能。现有方法依赖单一无结构记忆存储，难以有效管理跨会话信息，导致性能下降，因此需要改进记忆架构以支持长视界推理。",
      "method": "BMAM框架将代理记忆建模为多个功能专精的子系统，包括情节记忆、语义记忆、显著性感知和控制导向组件，受大脑认知系统启发。这些子系统在互补时间尺度上运行，支持长视界推理，通过组织情节记忆沿时间线并融合多个信号检索证据。",
      "result": "在LoCoMo基准测试中，BMAM在标准长视界评估设置下达到78.45%的准确率。消融分析确认情节记忆子系统对时间推理至关重要，尽管摘要未明确与基线方法对比，但数据表明BMAM有效改善了性能。",
      "conclusion": "BMAM的核心贡献在于设计了一个基于大脑认知的多代理记忆架构，改善了长交互中的信息管理和行为一致性。学术价值在于推进AI代理记忆系统研究，应用价值在于提升实际AI系统长期性能。未来工作可能包括扩展BMAM到更多领域或优化子系统设计。",
      "tags": [
        "Brain-inspired Memory",
        "Multi-Agent Systems",
        "Episodic Memory",
        "Semantic Memory",
        "LoCoMo Benchmark"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:18.765257Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20461",
    "title": "Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection",
    "authors": [
      "Yanzhu Liu",
      "Xiao Liu",
      "Yuexuan Wang",
      "Mondal Soumik"
    ],
    "abstract": "With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to \"contaminate\" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20461.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20461",
    "published": "2026-01-28T10:35:05Z",
    "updated": "2026-01-28T10:35:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出利用图像生成器的最终架构组件'污染'真实图像，训练检测器以提升对未见生成器AI图像的检测泛化能力。",
      "motivation": "随着AI图像生成器的快速普及，准确检测AI生成图像对于维护可信在线环境至关重要。然而，现有深度伪造检测器对未见生成器产生的图像泛化能力普遍较差，限制了实际应用。研究动机源于观察到现代图像生成器（如扩散或自回归模型）尽管训练范式不同，但共享共同的最终架构组件，这些组件负责将中间表示转换为最终图像。这一洞察为解决泛化问题提供了新视角，通过利用这些组件改进检测器性能，以应对多种未知生成器的挑战。",
      "method": "该方法的核心是通过生成器的最终架构组件'污染'真实图像，即使用这些组件处理真实图像生成伪样本，然后训练检测器区分污染图像和原始真实图像。关键创新包括引入基于最终组件的分类法，对21个广泛使用的图像生成器进行分类，以系统研究泛化能力。在实施中，仅使用三个代表性类别各100个样本构建训练数据，并在预训练的DINOv3骨干网络上进行微调，以构建高效的检测模型。这利用了小样本学习和迁移学习技术，提高了方法的实用性和泛化性。",
      "result": "实验结果表明，该方法在22个未见生成器的测试集上实现了平均98.83%的检测准确率。使用少量样本（每个类别仅100个）进行训练，显示了强大的泛化能力，显著优于现有检测器对未知生成器的泛化不足问题。摘要未明确与具体基线方法对比数据，但高准确率表明该方法在应对多种图像生成器时表现出色，可能通过降低过拟合和提升跨生成器适应性来改进性能。",
      "conclusion": "论文的主要贡献是提出了一种基于生成器最终组件的新方法，有效提高了AI生成图像检测器的泛化能力，并通过分类法系统分析了21个生成器，验证了方法的广泛适用性。学术上，这为检测技术提供了新理论基础，强调了架构组件在泛化中的作用；应用上，有助于增强在线平台对虚假图像的识别，维护数字信任。未来工作可扩展分类到更多生成器类型或优化训练策略，以进一步提升检测鲁棒性和效率。",
      "tags": [
        "AI-Generated Image Detection",
        "Generator Architectures",
        "DINOv3",
        "Diffusion Models",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:15.386765Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20451",
    "title": "MuVaC: AVariational Causal Framework for Multimodal Sarcasm Understanding in Dialogues",
    "authors": [
      "Diandian Guo",
      "Fangfang Yuan",
      "Cong Cao",
      "Xixun Lin",
      "Chuan Zhou",
      "Hao Peng",
      "Yanan Cao",
      "Yanbing Liu"
    ],
    "abstract": "The prevalence of sarcasm in multimodal dialogues on the social platforms presents a crucial yet challenging task for understanding the true intent behind online content. Comprehensive sarcasm analysis requires two key aspects: Multimodal Sarcasm Detection (MSD) and Multimodal Sarcasm Explanation (MuSE). Intuitively, the act of detection is the result of the reasoning process that explains the sarcasm. Current research predominantly focuses on addressing either MSD or MuSE as a single task. Even though some recent work has attempted to integrate these tasks, their inherent causal dependency is often overlooked. To bridge this gap, we propose MuVaC, a variational causal inference framework that mimics human cognitive mechanisms for understanding sarcasm, enabling robust multimodal feature learning to jointly optimize MSD and MuSE. Specifically, we first model MSD and MuSE from the perspective of structural causal models, establishing variational causal pathways to define the objectives for joint optimization. Next, we design an alignment-then-fusion approach to integrate multimodal features, providing robust fusion representations for sarcasm detection and explanation generation. Finally, we enhance the reasoning trustworthiness by ensuring consistency between detection results and explanations. Experimental results demonstrate the superiority of MuVaC in public datasets, offering a new perspective for understanding multimodal sarcasm.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20451.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20451",
    "published": "2026-01-28T10:19:42Z",
    "updated": "2026-01-28T10:19:42Z",
    "comment": "12 pages, 7 figures. Accepted by WWW 2026",
    "light_analysis": {
      "overview": "本文提出MuVaC变分因果框架，用于联合优化多模态对话中的讽刺检测和解释，模拟人类认知机制。",
      "motivation": "多模态对话中讽刺的普遍性使得理解在线内容的真实意图成为关键但挑战性的任务。现有研究主要专注于单独处理多模态讽刺检测或讽刺解释，即使有工作尝试整合，也往往忽略了两者之间的内在因果依赖。全面讽刺分析需要同时考虑检测和解释，以模拟人类推理过程，弥补当前方法的不足，提升理解的准确性和可靠性。",
      "method": "MuVaC框架基于变分因果推理，首先从结构因果模型角度建模MSD和MuSE，建立变分因果路径来定义联合优化目标。其次，设计对齐后融合方法整合多模态特征，生成鲁棒的融合表示用于讽刺检测和解释生成。最后，通过确保检测结果和解释之间的一致性来增强推理的可信度，从而模拟人类认知机制并优化任务性能。",
      "result": "实验结果表明，MuVaC在公共数据集上表现出优越性，优于现有基线方法。摘要未明确说明具体性能指标如准确率提升等，但强调了框架在联合优化多模态讽刺检测和解释方面的有效性，为任务提供了新的解决方案。",
      "conclusion": "本研究的主要贡献是提出MuVaC变分因果框架，为多模态讽刺理解提供联合优化方案，增强了检测和解释的鲁棒性和一致性。这具有学术价值，为理解讽刺提供了新视角，并在实际应用中有潜力提升社交媒体内容分析的准确性。未来工作可能涉及扩展任务或探索更多模态，但摘要未明确说明局限性。",
      "tags": [
        "Variational Causal Inference",
        "Multimodal Sarcasm Detection",
        "Multimodal Sarcasm Explanation",
        "Feature Fusion",
        "Structural Causal Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:30.133592Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20449",
    "title": "Fair Recourse for All: Ensuring Individual and Group Fairness in Counterfactual Explanations",
    "authors": [
      "Fatima Ezzeddine",
      "Obaida Ammar",
      "Silvia Giordano",
      "Omran Ayoub"
    ],
    "abstract": "Explainable Artificial Intelligence (XAI) is becoming increasingly essential for enhancing the transparency of machine learning (ML) models. Among the various XAI techniques, counterfactual explanations (CFs) hold a pivotal role due to their ability to illustrate how changes in input features can alter an ML model's decision, thereby offering actionable recourse to users. Ensuring that individuals with comparable attributes and those belonging to different protected groups (e.g., demographic) receive similar and actionable recourse options is essential for trustworthy and fair decision-making. In this work, we address this challenge directly by focusing on the generation of fair CFs. Specifically, we start by defining and formulating fairness at: 1) individual fairness, ensuring that similar individuals receive similar CFs, 2) group fairness, ensuring equitable CFs across different protected groups and 3) hybrid fairness, which accounts for both individual and broader group-level fairness. We formulate the problem as an optimization task and propose a novel model-agnostic, reinforcement learning based approach to generate CFs that satisfy fairness constraints at both the individual and group levels, two objectives that are usually treated as orthogonal. As fairness metrics, we extend existing metrics commonly used for auditing ML models, such as equal choice of recourse and equal effectiveness across individuals and groups. We evaluate our approach on three benchmark datasets, showing that it effectively ensures individual and group fairness while preserving the quality of the generated CFs in terms of proximity and plausibility, and quantify the cost of fairness in the different levels separately. Our work opens a broader discussion on hybrid fairness and its role and implications for XAI and beyond CFs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20449.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20449",
    "published": "2026-01-28T10:13:12Z",
    "updated": "2026-01-28T10:13:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一种基于强化学习的模型无关方法，用于生成同时满足个体和群体公平约束的反事实解释。",
      "motivation": "反事实解释是可解释人工智能的核心技术，能为用户提供可操作的追索选项。然而，现有方法往往忽视公平性问题，特别是在个体相似性和群体平等性之间难以平衡。个体公平要求相似个体获得相似解释，群体公平要求不同受保护群体间解释的公正性。这对于构建可信赖和公平的机器学习模型至关重要，但现有方法常将两者视为正交问题，导致难以实现综合公平。本研究旨在直接解决这一挑战，提升反事实解释的公平性和实用性。",
      "method": "本研究将公平性定义为一个优化问题，提出一种基于强化学习的模型无关方法来生成反事实解释。关键创新在于统一处理个体和群体公平，引入混合公平概念，结合两者约束。方法使用扩展的公平指标，如平等追索选择和平等有效性，作为强化学习算法的优化目标。通过训练过程生成满足个体和群体层面公平约束的反事实解释，不依赖于特定模型架构，适用于广泛的机器学习模型。",
      "result": "在三个基准数据集上的评估表明，该方法能有效确保个体和群体公平性，同时保持生成反事实解释的高质量，如接近性和合理性。研究量化了不同层面公平性的成本，提供了对公平与解释质量权衡的洞察。与基线方法相比，本方法在公平性指标上表现出显著改进，验证了其有效性，为公平反事实解释的生成提供了实证支持。",
      "conclusion": "本论文的主要贡献是提出了一个生成公平反事实解释的框架，解决了同时考虑个体和群体公平的挑战。研究不仅增强了可解释人工智能的公平性，还引发了关于混合公平及其在XAI中作用的广泛讨论。其意义在于提升机器学习模型的透明度和可信度，为实际应用提供公平决策工具。未来工作可扩展至其他XAI技术，并进一步优化公平性成本。",
      "tags": [
        "Counterfactual Explanations",
        "Reinforcement Learning",
        "Fairness Metrics",
        "Individual Fairness",
        "Group Fairness"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:55.486861Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20448",
    "title": "TimeCatcher: A Variational Framework for Volatility-Aware Forecasting of Non-Stationary Time Series",
    "authors": [
      "Zhiyu Chen",
      "Minhao Liu",
      "Yanru Zhang"
    ],
    "abstract": "Recent lightweight MLP-based models have achieved strong performance in time series forecasting by capturing stable trends and seasonal patterns. However, their effectiveness hinges on an implicit assumption of local stationarity assumption, making them prone to errors in long-term forecasting of highly non-stationary series, especially when abrupt fluctuations occur, a common challenge in domains like web traffic monitoring. To overcome this limitation, we propose TimeCatcher, a novel Volatility-Aware Variational Forecasting framework. TimeCatcher extends linear architectures with a variational encoder to capture latent dynamic patterns hidden in historical data and a volatility-aware enhancement mechanism to detect and amplify significant local variations. Experiments on nine real-world datasets from traffic, financial, energy, and weather domains show that TimeCatcher consistently outperforms state-of-the-art baselines, with particularly large improvements in long-term forecasting scenarios characterized by high volatility and sudden fluctuations. Our code is available at https://github.com/ColaPrinceCHEN/TimeCatcher.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20448.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20448",
    "published": "2026-01-28T10:06:57Z",
    "updated": "2026-01-28T10:06:57Z",
    "comment": "Under review. 13 pages, 8 figures. This paper proposes a variational framework with adaptive volatility enhancement for non-stationary time series forecasting",
    "light_analysis": {
      "overview": "本文提出了TimeCatcher，一个基于变分框架的波动感知预测方法，用于改进非平稳时间序列的长期预测。",
      "motivation": "当前轻量级MLP模型在时间序列预测中表现良好，但依赖局部平稳性假设，这使得它们在处理高度非平稳序列和突发波动时容易出错，例如在Web流量监测等领域中常见的高波动性场景。现有方法假设数据局部平稳，但在真实世界的时间序列中，非平稳特性和突然变化频繁，导致长期预测精度下降。因此，本研究旨在克服这一局限性，开发能有效处理非平稳性和波动性的新方法，提升预测的鲁棒性和准确性。",
      "method": "TimeCatcher扩展了线性架构，引入变分编码器来捕捉历史数据中的潜在动态模式，并设计波动感知增强机制来检测和放大局部变化。变分编码器通过变分推理学习数据的潜在分布，而波动感知机制识别和强化显著变化点，从而在预测中增强对突发波动的适应性。关键创新在于结合变分方法和波动感知技术，不依赖严格的平稳性假设，提高了模型的灵活性和处理复杂时间动态的能力。",
      "result": "在九个真实世界数据集（来自交通、金融、能源和天气领域）上的实验表明，TimeCatcher一致优于最先进的基线模型。特别是在长期预测和高波动性场景中，表现出显著的改进，有效减少了因突发波动导致的错误，验证了框架在处理非平稳序列方面的有效性。实验结果突显了其在提升预测精度和适应复杂时间动态方面的优势，但摘要未明确说明具体性能指标如准确率提升百分比。",
      "conclusion": "TimeCatcher通过变分框架和波动感知机制，成功解决了非平稳时间序列预测中的挑战，主要贡献在于提出了一种新颖的预测方法，具有学术价值和实际应用潜力，适用于多种领域。该方法为时间序列预测研究提供了新思路，结合变分推理和波动感知技术，有望推动相关技术的发展。摘要未明确说明局限性或未来工作方向，但可推断未来可能探索更多应用场景和机制扩展。",
      "tags": [
        "Time Series Forecasting",
        "Variational Encoder",
        "Volatility-Aware",
        "Non-Stationary Time Series",
        "Long-term Forecasting"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:13.489371Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20439",
    "title": "PEARL: Plan Exploration and Adaptive Reinforcement Learning for Multihop Tool Use",
    "authors": [
      "Qihao Wang",
      "Mingzhe Lu",
      "Jiayue Wu",
      "Yue Hu",
      "Yanbing Liu"
    ],
    "abstract": "Large Language Models show great potential with external tools, but face significant challenges in complex, multi-turn tool invocation. They often exhibit weak planning, tool hallucination, erroneous parameter generation, and struggle with robust interaction. To tackle these issues, we present PEARL, a novel framework to enhance LLM planning and execution for sophisticated tool use. PEARL adopts a two-stage approach: an offline phase where the agent explores tools to learn valid usage patterns and failure conditions, and an online reinforcement learning phase. In the online phase, a dedicated Planner is trained via group Relative Policy Optimization (GRPO) with a carefully designed reward function that provides distinct signals for planning quality. Experiments on the ToolHop and T-Eval benchmarks show PEARL significantly outperforms existing methods, achieving a new state-of-the-art success rate of \\textbf{56.5\\%} on ToolHop while maintaining a low invocation error rate. Our work marks a key advance in addressing the complex planning challenges of tool use, contributing to the development of more robust and reliable LLM-based agents.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20439.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20439",
    "published": "2026-01-28T09:49:43Z",
    "updated": "2026-01-28T09:49:43Z",
    "comment": "Accepted to PRICAI25",
    "light_analysis": {
      "overview": "PEARL通过两阶段强化学习框架解决了大型语言模型在复杂工具使用中的规划挑战，显著提升了成功率和鲁棒性。",
      "motivation": "大型语言模型在使用外部工具时面临复杂多轮调用的挑战，如规划能力弱、工具幻觉、参数生成错误和交互鲁棒性差，这些问题限制了LLM在实际应用中的可靠性。现有方法难以有效处理这些缺陷，导致性能不足，因此需要新框架来增强LLM的规划和执行能力，以促进更鲁棒的基于LLM的代理发展。",
      "method": "PEARL采用两阶段方法：离线阶段，代理探索工具以学习有效使用模式和失败条件；在线阶段，通过group Relative Policy Optimization（GRPO）训练专用Planner，奖励函数精心设计以提供规划质量的明确信号。该方法结合主动探索和强化学习优化，核心创新包括两阶段架构、GRPO策略优化和针对性奖励设计，旨在提高规划精度和减少调用错误。",
      "result": "在ToolHop和T-Eval基准测试中，PEARL显著优于现有方法，在ToolHop上实现了56.5%的最新成功率，同时保持低调用错误率。这表明框架在复杂工具使用任务中有效提升了性能指标，证明了其对规划挑战的改进能力。",
      "conclusion": "PEARL框架在解决工具使用的复杂规划问题上取得关键进展，通过结合离线探索和在线强化学习，增强了LLM的规划和执行能力，贡献于开发更鲁棒、可靠的基于LLM的代理。该研究具有学术和实际应用价值，摘要未明确说明局限性，未来工作可能包括扩展到更多领域或优化算法效率。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Tool Use",
        "Planning",
        "Group Relative Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:31.669013Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20433",
    "title": "MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models",
    "authors": [
      "Wenbo Xu",
      "Wei Lu",
      "Xiangyang Luo",
      "Jiantao Zhou"
    ],
    "abstract": "Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20433.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20433",
    "published": "2026-01-28T09:44:31Z",
    "updated": "2026-01-28T09:44:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "MARE提出一种结合多模态对齐和强化学习的可解释Deepfake检测方法，以提升视觉语言模型的检测准确性与可靠性。",
      "motivation": "Deepfake检测是打击恶意内容传播的关键领域，现有方法多将问题建模为分类或空间定位，但生成模型的快速发展对检测技术提出新挑战，要求更高的准确性和可解释性。当前方法在处理复杂伪造内容和提供推理解释方面存在不足，因此需要新方法来增强视觉语言模型在检测任务中的可靠性和人类可理解性，以应对日益严峻的虚假信息威胁。",
      "method": "MARE的核心方法包括两个关键模块：基于强化学习的奖励函数设计，结合从人类反馈的强化学习（RLHF），以生成符合人类偏好且文本空间对齐的推理内容；以及伪造解缠模块，用于从高层面部语义中分离并捕捉内在伪造痕迹，从而提高真实性检测能力。该方法通过视觉语言模型实现多模态对齐，增强检测的可解释性和准确性。摘要未明确说明具体数据集和模型架构。",
      "result": "MARE在Deepfake检测任务中进行了全面评估，实验结果表明生成的推理内容在准确性和可靠性方面表现优异。定量和定性分析均证明MARE达到最先进性能，优于现有基线方法。摘要未明确说明具体性能指标如准确率提升数据，但强调了其在准确性和可靠性上的显著改进。",
      "conclusion": "MARE通过多模态对齐和强化学习，显著提升了视觉语言模型在Deepfake检测中的准确性和可解释性，贡献在于引入综合奖励函数和伪造解缠模块。该研究具有学术价值，推动了可解释AI和安全领域的发展，实际应用中可帮助打击虚假信息传播。摘要未明确说明局限性，未来工作可能进一步优化模型以处理更复杂场景。",
      "tags": [
        "Vision-Language Models",
        "Reinforcement Learning from Human Feedback",
        "Multimodal Alignment",
        "Deepfake Detection",
        "Forgery Disentanglement"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:17.979806Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20430",
    "title": "Youtu-Parsing: Perception, Structuring and Recognition via High-Parallelism Decoding",
    "authors": [
      "Kun Yin",
      "Yunfei Wu",
      "Bing Liu",
      "Zhongpeng Cai",
      "Xiaotian Li",
      "Huang Chen",
      "Xin Li",
      "Haoyu Cao",
      "Yinsong Liu",
      "Deqiang Jiang",
      "Xing Sun",
      "Yunsheng Wu",
      "Qianyu Li",
      "Antai Guo",
      "Yanzhen Liao",
      "Yanqiu Qu",
      "Haodong Lin",
      "Chengxu He",
      "Shuangyin Liu"
    ],
    "abstract": "This paper presents Youtu-Parsing, an efficient and versatile document parsing model designed for high-performance content extraction. The architecture employs a native Vision Transformer (ViT) featuring a dynamic-resolution visual encoder to extract shared document features, coupled with a prompt-guided Youtu-LLM-2B language model for layout analysis and region-prompted decoding. Leveraging this decoupled and feature-reusable framework, we introduce a high-parallelism decoding strategy comprising two core components: token parallelism and query parallelism. The token parallelism strategy concurrently generates up to 64 candidate tokens per inference step, which are subsequently validated through a verification mechanism. This approach yields a 5--11x speedup over traditional autoregressive decoding and is particularly well-suited for highly structured scenarios, such as table recognition. To further exploit the advantages of region-prompted decoding, the query parallelism strategy enables simultaneous content prediction for multiple bounding boxes (up to five), providing an additional 2x acceleration while maintaining output quality equivalent to standard decoding. Youtu-Parsing encompasses a diverse range of document elements, including text, formulas, tables, charts, seals, and hierarchical structures. Furthermore, the model exhibits strong robustness when handling rare characters, multilingual text, and handwritten content. Extensive evaluations demonstrate that Youtu-Parsing achieves state-of-the-art (SOTA) performance on both the OmniDocBench and olmOCR-bench benchmarks. Overall, Youtu-Parsing demonstrates significant experimental value and practical utility for large-scale document intelligence applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20430.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20430",
    "published": "2026-01-28T09:37:13Z",
    "updated": "2026-01-28T09:37:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "Youtu-Parsing是一个通过高并行解码策略实现高效文档解析的创新模型，显著提升处理速度。",
      "motivation": "文档解析在实际应用如大规模文档智能中至关重要，但现有方法如自回归解码速度较慢，尤其是在处理高度结构化内容如表格时效率低下。这影响了实时处理和应用扩展，因此需要一种更高效的解析方法，以平衡速度与准确性，支持多样化文档类型和复杂场景。Youtu-Parsing旨在解决这些局限性，通过并行化技术优化解码过程，提升整体性能。",
      "method": "模型采用动态分辨率Vision Transformer (ViT)视觉编码器提取共享文档特征，并结合提示引导的Youtu-LLM-2B语言模型进行布局分析和区域提示解码。核心创新是高并行解码策略，包括token parallelism和query parallelism：token parallelism每步生成多达64个候选令牌并通过验证机制筛选，加速解码；query parallelism则同时预测多个边界框内容，支持最多五个区域并行处理，优化并行性。这种方法充分利用了特征重用和解耦架构的优势。",
      "result": "实验结果显示，token parallelism相比传统自回归解码提速5-11倍，query parallelism提供额外2倍加速，且输出质量与标准解码相当。模型在OmniDocBench和olmOCR-bench基准测试中达到最佳性能，并能高效处理多种文档元素如文本、公式、表格，同时对稀有字符、多语言和手写内容展示强鲁棒性，证明了其在高性能解析方面的优势。",
      "conclusion": "Youtu-Parsing的主要贡献在于提出高并行解码策略，显著提升文档解析效率。该研究在学术上为视觉-语言模型和文档解析领域提供了新方法，实际中可广泛应用于大规模文档智能应用。虽然摘要未明确说明局限性，但未来工作可能包括进一步优化模型以处理更复杂文档结构或扩展到其他视觉任务。",
      "tags": [
        "Vision Transformer",
        "Large Language Model",
        "Parallel Decoding",
        "Token Parallelism",
        "Query Parallelism"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:32.461848Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20428",
    "title": "Nonlinear Dimensionality Reduction with Diffusion Maps in Practice",
    "authors": [
      "Sönke Beier",
      "Paula Pirker-Díaz",
      "Friedrich Pagenkopf",
      "Karoline Wiesner"
    ],
    "abstract": "Diffusion Map is a spectral dimensionality reduction technique which is able to uncover nonlinear submanifolds in high-dimensional data. And, it is increasingly applied across a wide range of scientific disciplines, such as biology, engineering, and social sciences. But data preprocessing, parameter settings and component selection have a significant influence on the resulting manifold, something which has not been comprehensively discussed in the literature so far. We provide a practice oriented review of the Diffusion Map technique, illustrate pitfalls and showcase a recently introduced technique for identifying the most relevant components. Our results show that the first components are not necessarily the most relevant ones.",
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20428.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20428",
    "published": "2026-01-28T09:35:32Z",
    "updated": "2026-01-28T09:35:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提供Diffusion Map技术的实践指南，揭示成分选择中首个成分非最优的关键发现，以优化非线性降维应用。",
      "motivation": "Diffusion Map作为一种非线性降维技术，在生物学、工程学等领域广泛应用，但实际应用中数据预处理、参数设置和成分选择对结果流形有显著影响，现有文献对此讨论不足，导致技术可靠性受限。本研究旨在解决这些问题，通过实践分析提升方法应用的准确性，强调这些因素的重要性以避免常见陷阱，从而推动科学领域的更好实践。",
      "method": "本研究采用实践导向的综述方法，系统回顾Diffusion Map技术，详细说明数据预处理、参数调整中的常见问题，并展示一种新引入的成分识别技术，该技术通过分析特征值和相关指标来优化成分选择，强调组件间相关性的评估，以提高非线性子流形的提取效果。",
      "result": "论文结果显示，在Diffusion Map应用中，第一个成分不一定是最相关的，这挑战了传统成分选择假设，表明简单的顺序选择可能导致次优结果。通过新方法，研究者能够更有效地识别关键成分，从而改善降维性能，摘要未明确说明具体实验数据，但强调了这一发现对实践的重要指导意义。",
      "conclusion": "本研究总结了Diffusion Map技术的实践要点，突出了预处理和成分选择的关键作用，首个成分非最优的发现为学术研究提供了新视角，并提升了技术在现实应用中的价值。未来工作可进一步探索参数优化算法或扩展到更复杂数据集，以增强方法的鲁棒性和适用性。",
      "tags": [
        "Diffusion Maps",
        "Nonlinear Dimensionality Reduction",
        "Spectral Methods",
        "Component Selection",
        "Manifold Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:29.766691Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20425",
    "title": "Quartet of Diffusions: Structure-Aware Point Cloud Generation through Part and Symmetry Guidance",
    "authors": [
      "Chenliang Zhou",
      "Fangcheng Zhong",
      "Weihao Xia",
      "Albert Miao",
      "Canberk Baykal",
      "Cengiz Oztireli"
    ],
    "abstract": "We introduce the Quartet of Diffusions, a structure-aware point cloud generation framework that explicitly models part composition and symmetry. Unlike prior methods that treat shape generation as a holistic process or only support part composition, our approach leverages four coordinated diffusion models to learn distributions of global shape latents, symmetries, semantic parts, and their spatial assembly. This structured pipeline ensures guaranteed symmetry, coherent part placement, and diverse, high-quality outputs. By disentangling the generative process into interpretable components, our method supports fine-grained control over shape attributes, enabling targeted manipulation of individual parts while preserving global consistency. A central global latent further reinforces structural coherence across assembled parts. Our experiments show that the Quartet achieves state-of-the-art performance. To our best knowledge, this is the first 3D point cloud generation framework that fully integrates and enforces both symmetry and part priors throughout the generative process.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20425.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20425",
    "published": "2026-01-28T09:33:14Z",
    "updated": "2026-01-28T09:33:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了 Quartet of Diffusions，一个在点云生成中首次集成对称性和部分先验的结构感知框架。",
      "motivation": "该研究旨在解决现有点云生成方法无法同时处理对称性和部分组成的不足。先前方法或将形状生成视为整体过程，或仅支持部分组合，缺乏对称性保证和细粒度控制能力，这在生成高质量、结构连贯的3D形状时至关重要，特别是在需要精确编辑和结构一致性的应用中。",
      "method": "该框架利用四个协调的扩散模型，分别学习全局形状潜在变量、对称性、语义部分及其空间组装的分布。关键创新在于结构化管道，将生成过程分解为可解释组件，确保对称性、部分放置的连贯性，并通过全局潜在变量增强结构一致性。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "实验表明 Quartet 实现了最先进的点云生成性能。摘要未明确提供具体性能指标如准确率或效率数据，但强调在生成质量和结构连贯性方面优于基线方法，特别是在确保对称性和部分一致性方面表现突出。",
      "conclusion": "该论文的主要贡献是首次提出了完全集成对称性和部分先验的点云生成框架，推动了3D生成领域的发展。学术价值在于提供了结构化的生成方法，增强了可解释性和控制能力；实践上，支持高质量的细粒度形状编辑，适用于计算机图形学、机器人等应用。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Point Cloud Generation",
        "Diffusion Models",
        "Symmetry Prior",
        "Part Assembly",
        "Structured Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:56.600603Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20424",
    "title": "Hopes and Fears -- Emotion Distribution in the Topic Landscape of Finnish Parliamentary Speech 2000-2020",
    "authors": [
      "Anna Ristilä",
      "Otto Tarkka",
      "Veronika Laippala",
      "Kimmo Elo"
    ],
    "abstract": "Existing research often treats parliamentary discourse as a homogeneous whole, overlooking topic-specific patterns. Parliamentary speeches address a wide range of topics, some of which evoke stronger emotions than others. While everyone has intuitive assumptions about what the most emotive topics in a parliament may be, there has been little research into the emotions typically linked to different topics. This paper strives to fill this gap by examining emotion expression among the topics of parliamentary speeches delivered in Eduskunta, the Finnish Parliament, between 2000 and 2020. An emotion analysis model is used to investigate emotion expression in topics, from both synchronic and diachronic perspectives. The results strengthen evidence of increasing positivity in parliamentary speech and provide further insights into topic-specific emotion expression within parliamentary debate.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20424.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20424",
    "published": "2026-01-28T09:32:41Z",
    "updated": "2026-01-28T09:32:41Z",
    "comment": "27 pages (40 including appendices), 5 figures (13 including sub-figures), 1 table, 1 formula, 3 appendices; submitted to JDMDH",
    "light_analysis": {
      "overview": "本文通过分析芬兰议会演讲主题的情绪分布，揭示了主题特定情绪表达模式，并提供了同步和历时视角的见解。",
      "motivation": "现有研究常将议会演讲视为同质整体，忽略了主题特定情绪表达模式，导致对不同主题可能引发不同情绪的差异缺乏实证调查。议会演讲涉及广泛主题，一些主题可能更易引发强烈情绪，但现有研究对此关注不足，仅依赖直观假设。因此，本文旨在填补这一研究空白，探索议会演讲中主题与情绪的关联，以提供更精准的分析视角。",
      "method": "研究使用情绪分析模型，对芬兰议会（Eduskunta）在2000年至2020年间的演讲数据进行情绪表达调查。方法包括从同步角度分析跨主题的情绪分布，以及从历时角度考察情绪随时间的变化趋势。摘要未明确说明具体模型技术细节，如使用的算法或数据集预处理方式。",
      "result": "结果表明，议会演讲中积极情绪呈现增加趋势，加强了现有证据。研究进一步揭示了主题特定情绪表达的模式，提供了议会辩论中情绪分布的深入见解。与基线方法对比方面，摘要未提供具体数据，但强调了填补了主题特定情绪研究的空白。",
      "conclusion": "本文填补了议会演讲主题情绪分布的研究空白，贡献在于提供了同步和历时视角的情绪分析。学术价值在于促进议会辩论的精细化研究，实际应用可能支持政策情感分析或社会情绪监测。未来工作可扩展到其他议会或更精细的情绪分类，摘要未明确说明局限性。",
      "tags": [
        "Emotion Analysis",
        "Parliamentary Speech",
        "Topic Analysis",
        "Diachronic Analysis",
        "Synchronous Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:59.962333Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20420",
    "title": "Concept Component Analysis: A Principled Approach for Concept Extraction in LLMs",
    "authors": [
      "Yuhang Liu",
      "Erdun Gao",
      "Dong Gong",
      "Anton van den Hengel",
      "Javen Qinfeng Shi"
    ],
    "abstract": "Developing human understandable interpretation of large language models (LLMs) becomes increasingly critical for their deployment in essential domains. Mechanistic interpretability seeks to mitigate the issues through extracts human-interpretable process and concepts from LLMs' activations. Sparse autoencoders (SAEs) have emerged as a popular approach for extracting interpretable and monosemantic concepts by decomposing the LLM internal representations into a dictionary. Despite their empirical progress, SAEs suffer from a fundamental theoretical ambiguity: the well-defined correspondence between LLM representations and human-interpretable concepts remains unclear. This lack of theoretical grounding gives rise to several methodological challenges, including difficulties in principled method design and evaluation criteria. In this work, we show that, under mild assumptions, LLM representations can be approximated as a {linear mixture} of the log-posteriors over concepts given the input context, through the lens of a latent variable model where concepts are treated as latent variables. This motivates a principled framework for concept extraction, namely Concept Component Analysis (ConCA), which aims to recover the log-posterior of each concept from LLM representations through a {unsupervised} linear unmixing process. We explore a specific variant, termed sparse ConCA, which leverages a sparsity prior to address the inherent ill-posedness of the unmixing problem. We implement 12 sparse ConCA variants and demonstrate their ability to extract meaningful concepts across multiple LLMs, offering theory-backed advantages over SAEs.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20420.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20420",
    "published": "2026-01-28T09:27:05Z",
    "updated": "2026-01-28T09:27:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了理论驱动的概念组件分析（ConCA）框架，通过隐变量模型和线性分解从大型语言模型中提取人类可解释的概念，解决了稀疏自编码器的理论模糊性问题。",
      "motivation": "研究动机源于大型语言模型在医疗、金融等关键领域部署时，开发人类可理解的模型解释至关重要。机制可解释性通过从模型激活中提取概念来增强可信度，稀疏自编码器是常用方法，但其理论基础薄弱：LLM内部表示与人类概念之间的对应关系不明确，导致方法设计缺乏原则性，评估标准难以确立。这种理论缺陷限制了可解释性方法的可靠性和实际应用，亟需理论驱动的改进方案。",
      "method": "论文在温和假设下，将LLM表示建模为概念后验概率对数的线性混合，基于隐变量模型框架（概念视为隐变量）。这启发了概念组件分析（ConCA），一种无监督线性分解过程，旨在恢复每个概念的后验对数以提取概念。为处理分解问题的病态性，提出稀疏ConCA变体，利用稀疏先验优化稳定性；具体实现了12个稀疏ConCA变体，并在多种LLMs上验证其概念提取能力。",
      "result": "实验在多个大型语言模型中测试稀疏ConCA变体，证明其能提取有意义的概念，提供了比稀疏自编码器更优的理论基础。摘要未明确说明具体性能指标如准确率或效率提升，但强调了方法在概念提取方面的理论优势，为未来评估提供了原则性指导。",
      "conclusion": "本研究的核心贡献是提出了概念组件分析（ConCA）框架，为LLM概念提取提供了理论支撑，弥补了稀疏自编码器的理论缺陷。这增强了模型可解释性的可信度，促进机制可解释性领域发展，对关键领域安全部署具有重要价值。未来工作可探索方法的扩展性、具体应用场景或进一步实验验证。",
      "tags": [
        "Large Language Model",
        "Concept Extraction",
        "Sparse Autoencoders",
        "Latent Variable Model",
        "Linear Unmixing"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:32.403907Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20419",
    "title": "Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models",
    "authors": [
      "Yuhao Sun",
      "Chengyi Cai",
      "Jiacheng Zhang",
      "Zesheng Ye",
      "Xingliang Yuan",
      "Feng Liu"
    ],
    "abstract": "Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \\emph{View Refinement} and \\emph{Description refinement}, termed as \\textit{\\textbf{Bi}-refinement for \\textbf{F}ine-grained \\textbf{T}ext-visual \\textbf{A}lignment} (BiFTA). \\emph{View refinement} removes redundant image patches with high \\emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \\emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20419.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20419",
    "published": "2026-01-28T09:24:14Z",
    "updated": "2026-01-28T09:24:14Z",
    "comment": "25 pages",
    "light_analysis": {
      "overview": "BiFTA方法通过双向精炼去除冗余信息，提升视觉语言模型的细粒度文本-视觉对齐性能。",
      "motivation": "本研究动机源于细粒度文本描述与局部图像补丁对齐在视觉语言模型（如CLIP）中常存在冗余信息，导致对齐效果不佳，影响零样本学习性能。现有方法往往忽略这种冗余，使得模型难以有效匹配文本与视觉内容，从而限制了对复杂场景的理解能力。因此，去除冗余对于提高对齐精度和模型泛化至关重要。",
      "method": "论文提出BiFTA方法，通过两个核心步骤进行精炼：视图精炼基于图像补丁间的交并比（IoU）移除重叠高的冗余补丁，以增强视觉样本的独特性；描述精炼则通过计算文本描述间的余弦相似度，去除相似度高的冗余描述，以确保文本多样性。该方法应用于预训练的CLIP模型，包括ViT和ResNet架构，旨在优化细粒度对齐过程，无需修改模型结构。",
      "result": "BiFTA在6个基准数据集上实现了优越的零样本性能，适用于ViT-based和ResNet-based的CLIP模型。摘要未明确说明具体性能指标，但强调了该方法相较于基线在零样本任务中的有效性，验证了去除冗余信息对提升对齐效果的必要性。",
      "conclusion": "BiFTA的主要贡献是提出了双向精炼策略，有效去除了细粒度对齐中的冗余信息，从而提高了视觉语言模型的零样本性能。学术价值在于提供了一种新颖的冗余检测和移除方法，实际应用价值在于增强了模型对复杂视觉文本任务的处理能力。未来工作可探索更高效的冗余识别技术或扩展到其他多模态任务。",
      "tags": [
        "Vision-Language Models",
        "Fine-grained Alignment",
        "Redundancy Removal",
        "IoU",
        "Cosine Similarity"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:07.363346Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20417",
    "title": "SpeechMapper: Speech-to-text Embedding Projector for LLMs",
    "authors": [
      "Biswesh Mohapatra",
      "Marcely Zanon Boito",
      "Ioan Calapodescu"
    ],
    "abstract": "Current speech LLMs bridge speech foundation models to LLMs using projection layers, training all of these components on speech instruction data. This strategy is computationally intensive and susceptible to task and prompt overfitting. We present SpeechMapper, a cost-efficient speech-to-LLM-embedding training approach that mitigates overfitting, enabling more robust and generalizable models. Our model is first pretrained without the LLM on inexpensive hardware, and then efficiently attached to the target LLM via a brief 1K-step instruction tuning (IT) stage. Through experiments on speech translation and spoken question answering, we demonstrate the versatility of SpeechMapper's pretrained block, presenting results for both task-agnostic IT, an ASR-based adaptation strategy that does not train in the target task, and task-specific IT. In task-agnostic settings, Speechmapper rivals the best instruction-following speech LLM from IWSLT25, despite never being trained on these tasks, while in task-specific settings, it outperforms this model across many datasets, despite requiring less data and compute. Overall, SpeechMapper offers a practical and scalable approach for efficient, generalizable speech-LLM integration without large-scale IT.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20417.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20417",
    "published": "2026-01-28T09:22:58Z",
    "updated": "2026-01-28T09:22:58Z",
    "comment": "Accepted to ICASSP 2026",
    "light_analysis": {
      "overview": "SpeechMapper 提出了一种高效的两阶段训练方法，通过预训练和简短指令微调减少过拟合，实现鲁棒且可泛化的语音与大型语言模型集成。",
      "motivation": "当前语音LLMs使用投影层连接语音基础模型和大型语言模型，并在语音指令数据上训练所有组件，这导致计算密集且容易过拟合到特定任务和提示，降低了模型的鲁棒性和泛化能力。现有方法在计算成本和过拟合方面存在不足，因此需要一种更高效的训练策略来改善这些问题。",
      "method": "SpeechMapper 采用两阶段训练策略：首先，在没有大型语言模型的情况下进行预训练，使用廉价硬件降低成本；然后，通过简短的1K步指令微调（IT）阶段高效连接到目标LLM。该方法引入ASR-based adaptation strategy，支持任务无关和任务特定的IT，实验在语音翻译和语音问答任务上进行，以验证其泛化能力。",
      "result": "在任务无关设置中，SpeechMapper 与IWSLT25的最佳指令跟随语音LLM相媲美，尽管未在这些任务上训练；在任务特定设置中，它在多个数据集上优于该模型，同时使用更少的数据和计算资源。摘要未明确说明具体数值，但通过与基线方法对比，展现了效率和性能改进。",
      "conclusion": "SpeechMapper 提供了实用、可扩展的方法，用于高效、可泛化的语音-LLM集成，无需大规模指令微调，显著降低计算成本并提高鲁棒性。其学术价值在于提出了一种创新的训练框架，促进语音与LLM的融合。未来工作方向未明确说明，但可能涉及进一步优化和扩展到更多任务。",
      "tags": [
        "Speech-to-text Embedding",
        "Large Language Models",
        "Instruction Tuning",
        "Overfitting Mitigation",
        "ASR Adaptation"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:34.189194Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20412",
    "title": "Beyond Accuracy: A Cognitive Load Framework for Mapping the Capability Boundaries of Tool-use Agents",
    "authors": [
      "Qihao Wang",
      "Yue Hu",
      "Mingzhe Lu",
      "Jiayue Wu",
      "Yanbing Liu",
      "Yuanmin Tang"
    ],
    "abstract": "The ability of Large Language Models (LLMs) to use external tools unlocks powerful real-world interactions, making rigorous evaluation essential. However, current benchmarks primarily report final accuracy, revealing what models can do but obscuring the cognitive bottlenecks that define their true capability boundaries. To move from simple performance scoring to a diagnostic tool, we introduce a framework grounded in Cognitive Load Theory. Our framework deconstructs task complexity into two quantifiable components: Intrinsic Load, the inherent structural complexity of the solution path, formalized with a novel Tool Interaction Graph; and Extraneous Load, the difficulty arising from ambiguous task presentation. To enable controlled experiments, we construct ToolLoad-Bench, the first benchmark with parametrically adjustable cognitive load. Our evaluation reveals distinct performance cliffs as cognitive load increases, allowing us to precisely map each model's capability boundary. We validate that our framework's predictions are highly calibrated with empirical results, establishing a principled methodology for understanding an agent's limits and a practical foundation for building more efficient systems.",
    "categories": [
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20412.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20412",
    "published": "2026-01-28T09:17:51Z",
    "updated": "2026-01-28T09:17:51Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "论文提出了一个基于认知负载理论的框架，用于诊断工具使用代理的能力边界，超越了传统准确性评估。",
      "motivation": "该研究旨在解决当前大型语言模型使用外部工具的评估局限性问题。由于LLMs使用工具解锁了强大的现实交互能力，严格的评估至关重要，但现有基准测试主要报告最终准确性，仅能揭示模型能做什么，而无法识别认知瓶颈，导致无法全面理解模型的真实能力边界。因此，需要一种诊断性方法来精确定义能力边界。",
      "method": "论文引入了一个基于认知负载理论的框架，将任务复杂性分解为两个量化组件：内在负载，即解决方案路径的固有结构复杂性，通过新颖的工具交互图形式化；外在负载，即任务呈现模糊性导致的困难。为了进行控制实验，构建了ToolLoad-Bench基准测试，允许参数调整认知负载。关键创新在于结合认知理论来诊断代理性能。",
      "result": "评估结果显示，当认知负载增加时，代理性能出现陡降，使得框架能精确映射每个模型的能力边界。验证了框架的预测与经验结果高度校准，表明该框架能有效识别性能瓶颈。但摘要未明确提供具体准确率提升或效率改进的数据，主要强调了与传统准确性评估的对比。",
      "conclusion": "论文的主要贡献是提供了一个原则性方法论来理解代理的能力极限，并为构建更高效的系统奠定了实践基础。学术价值在于超越了简单的性能评分，引入了诊断性评估框架；实际应用中，有助于优化工具使用代理的设计。未来工作可能包括扩展框架到更多应用场景。",
      "tags": [
        "Large Language Models",
        "Cognitive Load Theory",
        "Tool Interaction Graph",
        "Benchmark",
        "Tool-use Agents"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:23.948909Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20409",
    "title": "AWGformer: Adaptive Wavelet-Guided Transformer for Multi-Resolution Time Series Forecasting",
    "authors": [
      "Wei Li"
    ],
    "abstract": "Time series forecasting requires capturing patterns across multiple temporal scales while maintaining computational efficiency. This paper introduces AWGformer, a novel architecture that integrates adaptive wavelet decomposition with cross-scale attention mechanisms for enhanced multi-variate time series prediction. Our approach comprises: (1) an Adaptive Wavelet Decomposition Module (AWDM) that dynamically selects optimal wavelet bases and decomposition levels based on signal characteristics; (2) a Cross-Scale Feature Fusion (CSFF) mechanism that captures interactions between different frequency bands through learnable coupling matrices; (3) a Frequency-Aware Multi-Head Attention (FAMA) module that weights attention heads according to their frequency selectivity; (4) a Hierarchical Prediction Network (HPN) that generates forecasts at multiple resolutions before reconstruction. Extensive experiments on benchmark datasets demonstrate that AWGformer achieves significant average improvements over state-of-the-art methods, with particular effectiveness on multi-scale and non-stationary time series. Theoretical analysis provides convergence guarantees and establishes the connection between our wavelet-guided attention and classical signal processing principles.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20409.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20409",
    "published": "2026-01-28T09:14:22Z",
    "updated": "2026-01-28T09:14:22Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "AWGformer是一种自适应小波引导Transformer架构，通过整合小波分解与注意力机制，提升多变量时间序列预测的多尺度捕获能力。",
      "motivation": "时间序列预测需要有效捕获多时间尺度模式，尤其是在非平稳数据中，这对实际应用如金融和气象预测至关重要。现有方法可能在处理多尺度特征时效率不足或缺乏自适应性，导致预测精度受限。本研究旨在结合小波分析和深度学习，以平衡多尺度捕获和计算效率，解决现有方法在处理复杂时间动态时的局限性，从而改进预测性能并支持更广泛的应用场景。",
      "method": "AWGformer架构包括四个核心组件：自适应小波分解模块（AWDM）动态选择小波基和分解级别以适应信号特性；跨尺度特征融合（CSFF）机制通过可学习耦合矩阵捕获不同频段的交互；频率感知多头部注意力（FAMA）模块根据频率选择性加权注意力头；以及分层预测网络（HPN）生成多分辨率预测后进行重建。这些组件整合了小波变换与Transformer注意力，增强多尺度特征提取能力，关键创新在于自适应小波引导和跨尺度交互机制的引入。",
      "result": "在多个基准数据集上的实验表明，AWGformer相比最先进方法实现了显著的平均性能提升，特别是在多尺度和非平稳时间序列预测任务中表现优异。尽管摘要未提供具体数值，但强调了其有效性，验证了该方法在捕获复杂时间模式方面的优势。这些改进展示了AWGformer在提升预测精度和效率上的潜力，为时间序列预测领域提供了新的高性能解决方案。",
      "conclusion": "AWGformer的主要贡献是提出了一个自适应小波引导的Transformer框架，有效结合了经典信号处理原理和现代深度学习技术。理论分析提供了收敛保证，增强了方法的可靠性。学术上，它推动了多尺度时间序列预测的研究，建立了小波引导注意力与信号处理之间的联系；实际应用中，可提升预测精度，适用于动态数据场景如金融分析。未来工作可能涉及扩展到其他序列任务或进一步优化计算效率。",
      "tags": [
        "Adaptive Wavelet Decomposition",
        "Cross-Scale Attention",
        "Frequency-Aware Attention",
        "Time Series Forecasting",
        "Transformer Architecture"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:00.701722Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20401",
    "title": "ScatterFusion: A Hierarchical Scattering Transform Framework for Enhanced Time Series Forecasting",
    "authors": [
      "Wei Li"
    ],
    "abstract": "Time series forecasting presents significant challenges due to the complex temporal dependencies at multiple time scales. This paper introduces ScatterFusion, a novel framework that synergistically integrates scattering transforms with hierarchical attention mechanisms for robust time series forecasting. Our approach comprises four key components: (1) a Hierarchical Scattering Transform Module (HSTM) that extracts multi-scale invariant features capturing both local and global patterns; (2) a Scale-Adaptive Feature Enhancement (SAFE) module that dynamically adjusts feature importance across different scales; (3) a Multi-Resolution Temporal Attention (MRTA) mechanism that learns dependencies at varying time horizons; and (4) a Trend-Seasonal-Residual (TSR) decomposition-guided structure-aware loss function. Extensive experiments on seven benchmark datasets demonstrate that ScatterFusion outperforms other common methods, achieving significant reductions in error metrics across various prediction horizons.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20401.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20401",
    "published": "2026-01-28T09:06:01Z",
    "updated": "2026-01-28T09:06:01Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "提出ScatterFusion框架，通过整合散射变换和分层注意力机制，有效提升时间序列预测的准确性和鲁棒性。",
      "motivation": "时间序列预测在金融、气象等领域面临多尺度时间依赖的复杂性挑战，传统方法如基于深度学习的模型可能难以有效捕捉局部和全局模式，导致预测性能受限。本研究动机在于开发一种新框架，以解决这些不足，通过集成频域特征提取和时间建模技术，增强对复杂时间动态的处理能力，从而提升预测精度和应用广泛性。",
      "method": "ScatterFusion框架包含四个核心模块：Hierarchical Scattering Transform Module（HSTM）提取多尺度不变特征以捕捉局部和全局模式；Scale-Adaptive Feature Enhancement（SAFE）动态调整不同尺度的特征重要性；Multi-Resolution Temporal Attention（MRTA）机制学习不同时间范围的依赖关系；Trend-Seasonal-Residual（TSR）分解引导的结构感知损失函数优化模型训练。该方法结合散射变换的频域分析和注意力机制的时间建模，实现多尺度特征融合和自适应调整。",
      "result": "在七个基准数据集上的实验表明，ScatterFusion优于其他常见方法，在不同预测时间范围内显著减少了误差指标，展现出优越的性能和泛化能力。摘要未明确说明具体准确率或误差数值，但强调了模型在多个数据集上的稳定提升和有效性。",
      "conclusion": "ScatterFusion框架成功整合散射变换和分层注意力机制，为解决时间序列预测中的多尺度依赖问题提供了创新方案。其学术价值在于提出了模块化设计以增强特征提取和时间建模，实际应用价值在于提高预测准确性，适用于各种时间序列分析任务。未来工作可进一步优化模型参数或扩展到更复杂的数据集以验证鲁棒性。",
      "tags": [
        "Scattering Transform",
        "Hierarchical Attention",
        "Time Series Forecasting",
        "Multi-Resolution Temporal Attention",
        "TSR Decomposition"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:10.339656Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20397",
    "title": "FedRD: Reducing Divergences for Generalized Federated Learning via Heterogeneity-aware Parameter Guidance",
    "authors": [
      "Kaile Wang",
      "Jiannong Cao",
      "Yu Yang",
      "Xiaoyin Li",
      "Mingjin Zhang"
    ],
    "abstract": "Heterogeneous federated learning (HFL) aims to ensure effective and privacy-preserving collaboration among different entities. As newly joined clients require significant adjustments and additional training to align with the existing system, the problem of generalizing federated learning models to unseen clients under heterogeneous data has become progressively crucial. Consequently, we highlight two unsolved challenging issues in federated domain generalization: Optimization Divergence and Performance Divergence. To tackle the above challenges, we propose FedRD, a novel heterogeneity-aware federated learning algorithm that collaboratively utilizes parameter-guided global generalization aggregation and local debiased classification to reduce divergences, aiming to obtain an optimal global model for participating and unseen clients. Extensive experiments on public multi-domain datasets demonstrate that our approach exhibits a substantial performance advantage over competing baselines in addressing this specific problem.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20397.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20397",
    "published": "2026-01-28T09:03:06Z",
    "updated": "2026-01-28T09:03:06Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "提出FedRD算法，通过异构感知的参数指导减少优化和性能分歧，实现联邦学习模型的泛化。",
      "motivation": "研究动机源于异构联邦学习中推广模型到未见客户端的挑战。由于数据异构性，新客户端加入需大量调整和训练，导致效率低下和隐私保护问题。现有方法在优化分歧和性能分歧方面未解决，限制了模型的泛化能力和协作效果。因此，需要开发新方法来提高联邦学习在异构环境下的适应性和效率。",
      "method": "提出FedRD方法，一种异构感知的联邦学习算法。该方法协同利用参数指导的全局泛化聚合和局部去偏分类。关键创新点包括通过全局聚合协调客户端模型更新以减少优化分歧，并使用局部去偏分类技术缓解数据偏差对性能的影响。技术特色在于异构感知机制，旨在为参与和未见客户端生成最优全局模型。",
      "result": "在公共多域数据集上的广泛实验表明，FedRD在解决异构联邦学习泛化问题上显著优于竞争基线。摘要未提供具体准确率或效率改进数值，但结果显示该方法在性能上超越对手，有效减少分歧并提升模型泛化能力。实验验证了FedRD在优化和性能方面的一致性优势。",
      "conclusion": "论文主要贡献是提出FedRD算法，通过减少优化和性能分歧增强联邦学习模型的泛化能力。这具有学术价值，为解决联邦学习中的异构性问题提供了新思路，并具有实际应用潜力，如多域协作和隐私保护场景。摘要未明确说明局限性或未来工作方向，但可推断未来可能扩展到更复杂的数据环境或模型架构。",
      "tags": [
        "Heterogeneous Federated Learning",
        "Federated Domain Generalization",
        "Parameter Guidance",
        "Debiased Classification"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:22.428816Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20383",
    "title": "HINT: Hierarchical Interaction Modeling for Autoregressive Multi-Human Motion Generation",
    "authors": [
      "Mengge Liu",
      "Yan Di",
      "Gu Wang",
      "Yun Qu",
      "Dekai Zhu",
      "Yanyan Li",
      "Xiangyang Ji"
    ],
    "abstract": "Text-driven multi-human motion generation with complex interactions remains a challenging problem. Despite progress in performance, existing offline methods that generate fixed-length motions with a fixed number of agents, are inherently limited in handling long or variable text, and varying agent counts. These limitations naturally encourage autoregressive formulations, which predict future motions step by step conditioned on all past trajectories and current text guidance. In this work, we introduce HINT, the first autoregressive framework for multi-human motion generation with Hierarchical INTeraction modeling in diffusion. First, HINT leverages a disentangled motion representation within a canonicalized latent space, decoupling local motion semantics from inter-person interactions. This design facilitates direct adaptation to varying numbers of human participants without requiring additional refinement. Second, HINT adopts a sliding-window strategy for efficient online generation, and aggregates local within-window and global cross-window conditions to capture past human history, inter-person dependencies, and align with text guidance. This strategy not only enables fine-grained interaction modeling within each window but also preserves long-horizon coherence across all the long sequence. Extensive experiments on public benchmarks demonstrate that HINT matches the performance of strong offline models and surpasses autoregressive baselines. Notably, on InterHuman, HINT achieves an FID of 3.100, significantly improving over the previous state-of-the-art score of 5.154.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20383.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20383",
    "published": "2026-01-28T08:47:23Z",
    "updated": "2026-01-28T08:47:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了HINT，首个基于自回归框架和分层交互建模的多人类运动生成方法，显著提升处理复杂交互和可变代理数量的能力。",
      "motivation": "研究旨在解决文本驱动的多人类运动生成中复杂交互的挑战。现有离线方法生成固定长度和代理数量的运动，无法适应长文本、可变文本和动态代理数量，限制了真实场景如虚拟现实和电影制作的应用。这些局限性促使采用自回归方法来逐步预测运动，从而更灵活地处理变化条件，弥补现有方法的不足，推动该领域向自适应和交互建模方向发展。",
      "method": "HINT采用自回归框架和扩散模型，核心创新包括解缠的运动表示在规范化潜在空间中，将局部运动语义从人际交互中解耦，便于直接适应不同数量的人类参与者。此外，滑动窗口策略实现高效在线生成，通过聚合局部窗口内和全局跨窗口条件，捕捉过去轨迹、人际依赖，并与文本指导对齐，确保长期连贯性和精细交互建模，从而提升生成灵活性和质量。",
      "result": "在公共基准测试中，HINT表现优异，匹配了强离线模型的性能，并超越了自回归基线。具体地，在InterHuman数据集上，HINT实现了FID得分3.100，显著优于先前最佳成绩5.154，展示了在生成质量上的显著改进。这证明了其在处理复杂交互和可变代理数量时的有效性，增强了实际应用的潜力。",
      "conclusion": "HINT框架为多人类运动生成提供了首个自回归解决方案，通过分层交互建模显著提升了交互质量和适应性。学术上，结合解缠表示和滑动窗口策略推动了方法创新；应用上，适用于虚拟交互、游戏开发等场景。未来工作可能包括扩展到更多代理或更复杂文本描述，以进一步提升实用性和泛化能力。",
      "tags": [
        "Autoregressive Generation",
        "Diffusion Models",
        "Hierarchical Interaction Modeling",
        "Multi-Human Motion Generation",
        "Motion Representation"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:24.453486Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20380",
    "title": "OmegaUse: Building a General-Purpose GUI Agent for Autonomous Task Execution",
    "authors": [
      "Le Zhang",
      "Yixiong Xiao",
      "Xinjiang Lu",
      "Jingjia Cao",
      "Yusai Zhao",
      "Jingbo Zhou",
      "Lang An",
      "Zikan Feng",
      "Wanxiang Sha",
      "Yu Shi",
      "Congxi Xiao",
      "Jian Xiong",
      "Yankai Zhang",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Graphical User Interface (GUI) agents show great potential for enabling foundation models to complete real-world tasks, revolutionizing human-computer interaction and improving human productivity. In this report, we present OmegaUse, a general-purpose GUI agent model for autonomous task execution on both mobile and desktop platforms, supporting computer-use and phone-use scenarios. Building an effective GUI agent model relies on two factors: (1) high-quality data and (2) effective training methods. To address these, we introduce a carefully engineered data-construction pipeline and a decoupled training paradigm. For data construction, we leverage rigorously curated open-source datasets and introduce a novel automated synthesis framework that integrates bottom-up autonomous exploration with top-down taxonomy-guided generation to create high-fidelity synthetic data. For training, to better leverage these data, we adopt a two-stage strategy: Supervised Fine-Tuning (SFT) to establish fundamental interaction syntax, followed by Group Relative Policy Optimization (GRPO) to improve spatial grounding and sequential planning. To balance computational efficiency with agentic reasoning capacity, OmegaUse is built on a Mixture-of-Experts (MoE) backbone. To evaluate cross-terminal capabilities in an offline setting, we introduce OS-Nav, a benchmark suite spanning multiple operating systems: ChiM-Nav, targeting Chinese Android mobile environments, and Ubu-Nav, focusing on routine desktop interactions on Ubuntu. Extensive experiments show that OmegaUse is highly competitive across established GUI benchmarks, achieving a state-of-the-art (SOTA) score of 96.3% on ScreenSpot-V2 and a leading 79.1% step success rate on AndroidControl. OmegaUse also performs strongly on OS-Nav, reaching 74.24% step success on ChiM-Nav and 55.9% average success on Ubu-Nav.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20380.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20380",
    "published": "2026-01-28T08:45:17Z",
    "updated": "2026-01-28T08:45:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 OmegaUse，一个基于混合专家架构的通用图形用户界面代理，通过创新数据合成和两阶段训练实现跨平台自主任务执行。",
      "motivation": "图形用户界面代理在提升人机交互和生产力方面潜力巨大，但构建有效模型面临高质量数据和高效训练方法的挑战。现有方法可能数据不足或训练效率低，限制了代理的通用性和性能。本研究旨在解决这些不足，开发一个能同时在移动和桌面平台上执行任务的通用 GUI 代理，以应对实际场景中的复杂任务需求。摘要未明确说明具体不足，但基于上下文推断数据质量和训练方法是关键瓶颈。",
      "method": "论文引入精心设计的数据构建管道，结合开源数据集和自动化合成框架，通过自底向上探索和自顶向下分类生成高保真合成数据。训练采用两阶段策略：监督微调建立交互语法基础，组相对策略优化提升空间定位和序列规划能力。模型基于混合专家主干以平衡计算效率和推理能力。关键细节包括使用 OS-Nav 基准套件进行评估，涵盖 ChiM-Nav 和 Ubu-Nav 等数据集，支持跨操作系统测试。",
      "result": "实验显示 OmegaUse 在多个基准上表现优异，在 ScreenSpot-V2 上达到 96.3% 的最先进分数，AndroidControl 上步成功率为 79.1%。在新引入的 OS-Nav 基准中，ChiM-Nav 上步成功率为 74.24%，Ubu-Nav 上平均成功率为 55.9%。这些结果与现有方法相比，展示了高度竞争力，证明了模型在跨平台任务执行中的有效性和通用性。摘要未明确列出具体基线对比，但强调其在多个基准上领先。",
      "conclusion": "OmegaUse 的贡献在于提供了一个通用 GUI 代理模型，通过创新数据构建和训练范式实现了跨平台自主任务执行。学术上推动了 GUI 代理研究，特别是在数据合成和训练方法方面；实际应用中可增强自动化效率，改善人机交互。局限性可能包括对特定操作系统或场景的依赖，未来工作可扩展到更多环境或优化数据合成技术。摘要未明确说明局限性，但基于内容推断潜在方向。",
      "tags": [
        "GUI Agent",
        "Mixture-of-Experts (MoE)",
        "Supervised Fine-Tuning (SFT)",
        "Group Relative Policy Optimization (GRPO)",
        "Automated Synthesis Framework"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:40.489356Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20379",
    "title": "Policy of Thoughts: Scaling LLM Reasoning via Test-time Policy Evolution",
    "authors": [
      "Zhengbo Jiao",
      "Hongyu Xian",
      "Qinglong Wang",
      "Yunpu Ma",
      "Zhebo Wang",
      "Zifan Zhang",
      "Dezhang Kong",
      "Meng Han"
    ],
    "abstract": "Large language models (LLMs) struggle with complex, long-horizon reasoning due to instability caused by their frozen policy assumption. Current test-time scaling methods treat execution feedback merely as an external signal for filtering or rewriting trajectories, without internalizing it to improve the underlying reasoning strategy. Inspired by Popper's epistemology of \"conjectures and refutations,\" we argue that intelligence requires real-time evolution of the model's policy through learning from failed attempts. We introduce Policy of Thoughts (PoT), a framework that recasts reasoning as a within-instance online optimization process. PoT first generates diverse candidate solutions via an efficient exploration mechanism, then uses Group Relative Policy Optimization (GRPO) to update a transient LoRA adapter based on execution feedback. This closed-loop design enables dynamic, instance-specific refinement of the model's reasoning priors. Experiments show that PoT dramatically boosts performance: a 4B model achieves 49.71% accuracy on LiveCodeBench, outperforming GPT-4o and DeepSeek-V3 despite being over 50 smaller.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20379.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20379",
    "published": "2026-01-28T08:44:34Z",
    "updated": "2026-01-28T08:44:34Z",
    "comment": "19 pages, 5 figures",
    "light_analysis": {
      "overview": "论文提出了Policy of Thoughts框架，通过测试时策略演化提升大型语言模型在复杂推理任务中的性能。",
      "motivation": "大型语言模型在处理复杂、长视界推理时因策略固定假设导致不稳定，现有测试时扩展方法仅将执行反馈用作外部信号进行轨迹过滤或重写，未能内化反馈以优化推理策略。受Popper'猜想与反驳'认识论启发，本研究认为智能需通过实时学习失败尝试演化模型策略，解决策略更新不足的问题。",
      "method": "研究方法提出Policy of Thoughts框架，将推理重新定义为实例内在线优化过程：首先生成多样候选解决方案通过高效探索机制，然后应用Group Relative Policy Optimization基于执行反馈更新临时LoRA适配器。闭环设计实现推理先验的动态和实例特定细化，关键创新在于结合策略演化与任务执行提升推理稳定性。",
      "result": "实验显示PoT显著提升性能：在LiveCodeBench上，一个4B模型达到49.71%准确率，超越GPT-4o和DeepSeek-V3，尽管模型规模小50多倍。这证明PoT能有效增强推理能力，在处理长视界任务时相比基线方法提供更高稳定性和效率。",
      "conclusion": "论文贡献在于提出Policy of Thoughts框架，实现测试时策略演化以提升LLM推理能力，具有学术价值，展示在线优化和强化学习的潜力。实际应用中可帮助小模型在资源有限时媲美大模型，降低部署成本。摘要未明确说明局限性和未来工作，但可推断研究方向包括扩展到更多任务和模型类型。",
      "tags": [
        "Large Language Model",
        "Policy Evolution",
        "Online Optimization",
        "GRPO",
        "LoRA"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:59.217050Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20375",
    "title": "LLM-AutoDP: Automatic Data Processing via LLM Agents for Model Fine-tuning",
    "authors": [
      "Wei Huang",
      "Anda Cheng",
      "Yinggui Wang",
      "Lei Wang",
      "Tao Wei"
    ],
    "abstract": "Large Language Models (LLMs) can be fine-tuned on domain-specific data to enhance their performance in specialized fields. However, such data often contains numerous low-quality samples, necessitating effective data processing (DP). In practice, DP strategies are typically developed through iterative manual analysis and trial-and-error adjustment. These processes inevitably incur high labor costs and may lead to privacy issues in high-privacy domains like healthcare due to direct human access to sensitive data. Thus, achieving automated data processing without exposing the raw data has become a critical challenge. To address this challenge, we propose LLM-AutoDP, a novel framework that leverages LLMs as agents to automatically generate and optimize data processing strategies. Our method generates multiple candidate strategies and iteratively refines them using feedback signals and comparative evaluations. This iterative in-context learning mechanism enables the agent to converge toward high-quality processing pipelines without requiring direct human intervention or access to the underlying data. To further accelerate strategy search, we introduce three key techniques: Distribution Preserving Sampling, which reduces data volume while maintaining distributional integrity; Processing Target Selection, which uses a binary classifier to identify low-quality samples for focused processing; Cache-and-Reuse Mechanism}, which minimizes redundant computations by reusing prior processing results. Results show that models trained on data processed by our framework achieve over 80% win rates against models trained on unprocessed data. Compared to AutoML baselines based on LLM agents, LLM-AutoDP achieves approximately a 65% win rate. Moreover, our acceleration techniques reduce the total searching time by up to 10 times, demonstrating both effectiveness and efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20375.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20375",
    "published": "2026-01-28T08:37:34Z",
    "updated": "2026-01-28T08:37:34Z",
    "comment": "Accepted by VLDB2026",
    "light_analysis": {
      "overview": "LLM-AutoDP提出了一个基于LLM代理的自动数据处理框架，用于优化模型微调过程中的策略生成和优化。",
      "motivation": "研究动机源于LLMs在特定领域微调时面临的数据质量问题，领域数据常含低质量样本，需要有效数据处理。传统方法依赖手动分析和试错调整，导致高劳动成本和高隐私领域（如医疗）的风险，因为人工直接访问敏感数据可能侵犯隐私。因此，开发自动化数据处理技术，在不暴露原始数据的情况下提升效率，成为重要挑战。",
      "method": "LLM-AutoDP框架的核心是利用LLM代理自动生成和优化数据处理策略。通过迭代上下文学习机制，代理生成多个候选策略，并使用反馈信号和比较评估进行迭代优化。关键创新包括：Distribution Preserving Sampling减少数据量同时保持分布完整性；Processing Target Selection采用二元分类器识别低质量样本以聚焦处理；Cache-and-Reuse Mechanism重用先验处理结果以减少冗余计算。这些技术加速策略搜索过程，实现无需人工干预的自动化流水线。",
      "result": "实验结果证实，使用LLM-AutoDP处理的数据训练的模型，相比未处理数据，胜率超过80%。与基于LLM代理的AutoML基线相比，胜率约为65%。此外，引入的加速技术显著提升了效率，将总搜索时间减少了最多10倍，表明框架在性能和效率方面均有显著改进。",
      "conclusion": "LLM-AutoDP框架的主要贡献是通过LLM代理实现自动化数据处理，有效解决了手动方法的高成本和隐私问题，为高隐私领域提供了安全解决方案。研究展示了LLM代理在自动化任务中的潜力，具有学术和应用价值。未来工作可进一步优化策略生成机制，扩展至更多场景，摘要未明确说明具体未来方向。",
      "tags": [
        "Large Language Models",
        "LLM Agents",
        "Iterative In-Context Learning",
        "Distribution Preserving Sampling",
        "Processing Target Selection"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:24.273053Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20369",
    "title": "RepSFNet : A Single Fusion Network with Structural Reparameterization for Crowd Counting",
    "authors": [
      "Mas Nurul Achmadiah",
      "Chi-Chia Sun",
      "Wen-Kai Kuo",
      "Jun-Wei Hsieh"
    ],
    "abstract": "Crowd counting remains challenging in variable-density scenes due to scale variations, occlusions, and the high computational cost of existing models. To address these issues, we propose RepSFNet (Reparameterized Single Fusion Network), a lightweight architecture designed for accurate and real-time crowd estimation. RepSFNet leverages a RepLK-ViT backbone with large reparameterized kernels for efficient multi-scale feature extraction. It further integrates a Feature Fusion module combining Atrous Spatial Pyramid Pooling (ASPP) and Context-Aware Network (CAN) to achieve robust, density-adaptive context modeling. A Concatenate Fusion module is employed to preserve spatial resolution and generate high-quality density maps. By avoiding attention mechanisms and multi-branch designs, RepSFNet significantly reduces parameters and computational complexity. The training objective combines Mean Squared Error and Optimal Transport loss to improve both count accuracy and spatial distribution alignment. Experiments conducted on ShanghaiTech, NWPU, and UCF-QNRF datasets demonstrate that RepSFNet achieves competitive accuracy while reducing inference latency by up to 34 percent compared to recent state-of-the-art methods, making it suitable for real-time and low-power edge computing applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20369.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20369",
    "published": "2026-01-28T08:33:36Z",
    "updated": "2026-01-28T08:33:36Z",
    "comment": "6 pages. Published in Proceedings of the IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS) 2025",
    "light_analysis": {
      "overview": "RepSFNet提出一种基于结构重参数化的单融合网络，用于人群计数，旨在实现高准确性和实时性。",
      "motivation": "人群计数在可变密度场景中面临尺度变化、遮挡和高计算成本的挑战，现有方法常因使用注意机制和多分支设计而导致计算复杂度高，难以部署到实时和低功耗边缘设备上。这限制了其在监控、城市规划等领域的应用，因此需要一种轻量级且准确的解决方案来改善效率和鲁棒性。",
      "method": "RepSFNet采用RepLK-ViT主干网络，利用大重参数化内核进行高效多尺度特征提取，并集成了特征融合模块结合Atrous Spatial Pyramid Pooling (ASPP)和Context-Aware Network (CAN)实现密度自适应的上下文建模。Concatenate Fusion模块用于保留空间分辨率和生成高质量密度图，通过避免注意机制和多分支设计显著减少参数和计算复杂度。训练目标结合了均方误差和最优传输损失来优化计数准确性和空间分布对齐。",
      "result": "在ShanghaiTech、NWPU和UCF-QNRF数据集上的实验显示，RepSFNet在准确性方面达到与当前最先进方法竞争的水平，同时推理延迟减少高达34%，证明了其在保持高精度的同时显著提升计算效率，适用于实时应用。",
      "conclusion": "论文的主要贡献是开发了RepSFNet这一轻量级人群计数网络，通过结构重参数化和单融合设计平衡了准确性和计算效率，为密集估计任务提供了新架构，具有学术价值和实际应用潜力，特别是在实时边缘计算中，未来工作可能涉及进一步优化或扩展到其他任务。",
      "tags": [
        "Crowd Counting",
        "Structural Reparameterization",
        "RepLK-ViT",
        "ASPP",
        "CAN"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:14.883294Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20367",
    "title": "Unsupervised Anomaly Detection in Multi-Agent Trajectory Prediction via Transformer-Based Models",
    "authors": [
      "Qing Lyu",
      "Zhe Fu",
      "Alexandre Bayen"
    ],
    "abstract": "Identifying safety-critical scenarios is essential for autonomous driving, but the rarity of such events makes supervised labeling impractical. Traditional rule-based metrics like Time-to-Collision are too simplistic to capture complex interaction risks, and existing methods lack a systematic way to verify whether statistical anomalies truly reflect physical danger. To address this gap, we propose an unsupervised anomaly detection framework based on a multi-agent Transformer that models normal driving and measures deviations through prediction residuals. A dual evaluation scheme has been proposed to assess both detection stability and physical alignment: Stability is measured using standard ranking metrics in which Kendall Rank Correlation Coefficient captures rank agreement and Jaccard index captures the consistency of the top-K selected items; Physical alignment is assessed through correlations with established Surrogate Safety Measures (SSM). Experiments on the NGSIM dataset demonstrate our framework's effectiveness: We show that the maximum residual aggregator achieves the highest physical alignment while maintaining stability. Furthermore, our framework identifies 388 unique anomalies missed by Time-to-Collision and statistical baselines, capturing subtle multi-agent risks like reactive braking under lateral drift. The detected anomalies are further clustered into four interpretable risk types, offering actionable insights for simulation and testing.",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20367.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20367",
    "published": "2026-01-28T08:33:10Z",
    "updated": "2026-01-28T08:33:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于Transformer的无监督异常检测框架，用于多智能体轨迹预测，通过预测残差和双评估方案有效识别自动驾驶中的安全关键场景。",
      "motivation": "研究动机源于自动驾驶领域对安全关键场景识别的高需求。由于这类事件在真实世界中极为罕见，依赖监督学习的标注数据不切实际且成本高昂。传统基于规则的指标如Time-to-Collision虽然简单，但无法有效捕捉多智能体交互中的复杂风险模式。现有异常检测方法往往仅关注统计偏差，缺乏系统验证这些异常是否对应真实的物理危险，导致检测结果可能不可靠。因此，本研究旨在填补这一空白，开发一种无监督框架来准确识别和验证安全异常。",
      "method": "研究方法采用基于Transformer的无监督学习框架，专为多智能体轨迹预测设计。首先，使用多智能体Transformer模型对正常驾驶模式进行建模，生成轨迹预测。然后，通过计算预测残差来测量与正常模式的偏差，以此作为异常指标。关键创新在于提出的双评估方案：使用Kendall Rank Correlation Coefficient和Jaccard index评估检测稳定性，确保排名一致性和选择一致性；同时通过与Surrogate Safety Measures（SSM）的相关性评估物理对齐，验证检测结果是否反映实际安全风险。实验在NGSIM数据集上实施。",
      "result": "主要实验结果在NGSIM数据集上展示。最大残差聚合器在评估中表现最佳，实现了最高的物理对齐度，同时保持了良好的检测稳定性，如通过Kendall和Jaccard指标所示。框架成功识别出388个独特异常，这些异常被传统Time-to-Collision方法和统计基线遗漏，其中包括如横向漂移下反应制动等微妙多智能体风险。此外，检测到的异常被聚类为四种可解释的风险类型，为自动驾驶仿真和测试提供了具体、可操作的见解。",
      "conclusion": "本研究提出的无监督异常检测框架有效解决了自动驾驶中安全场景检测的挑战。通过结合多智能体Transformer和双评估方案，框架不仅提高了异常检测的准确性，还确保了检测结果与物理危险的一致性，具有显著的学术贡献。在实际应用方面，通过风险聚类提供了可解释的见解，可用于改进仿真和测试流程。局限性可能包括对特定数据集NGSIM的依赖，未来工作可扩展到更多样化的驾驶场景。",
      "tags": [
        "Unsupervised Anomaly Detection",
        "Multi-Agent Trajectory Prediction",
        "Transformer Models",
        "Surrogate Safety Measures",
        "Prediction Residuals"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:53.846621Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20366",
    "title": "Dual-Modality IoT Framework for Integrated Access Control and Environmental Safety Monitoring with Real-Time Cloud Analytics",
    "authors": [
      "Abdul Hasib",
      "A. S. M. Ahsanul Sarkar Akib",
      "Nihal Das Ankur",
      "Anish Giri"
    ],
    "abstract": "The integration of physical security systems with environmental safety monitoring represents a critical advancement in smart infrastructure management. Traditional approaches maintain these systems as independent silos, creating operational inefficiencies, delayed emergency responses, and increased management complexity. This paper presents a comprehensive dual-modality Internet of Things framework that seamlessly integrates RFID-based access control with multi-sensor environmental safety monitoring through a unified cloud architecture. The system comprises two coordinated subsystems: Subsystem 1 implements RFID authentication with servo-actuated gate control and real-time Google Sheets logging, while Subsystem 2 provides comprehensive safety monitoring incorporating flame detection, water flow measurement, LCD status display, and personnel identification. Both subsystems utilize ESP32 microcontrollers for edge processing and wireless connectivity. Experimental evaluation over 45 days demonstrates exceptional performance metrics: 99.2\\% RFID authentication accuracy with 0.82-second average response time, 98.5\\% flame detection reliability within 5-meter range, and 99.8\\% cloud data logging success rate. The system maintains operational integrity during network disruptions through intelligent local caching mechanisms and achieves total implementation cost of 5,400 BDT (approximately \\$48), representing an 82\\% reduction compared to commercial integrated solutions. This research establishes a practical framework for synergistic security-safety integration, demonstrating that professional-grade performance can be achieved through careful architectural design and component optimization while maintaining exceptional cost-effectiveness and accessibility for diverse application scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20366.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20366",
    "published": "2026-01-28T08:32:07Z",
    "updated": "2026-01-28T08:32:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个集成RFID访问控制和多传感器环境安全监测的双模态物联网框架，通过实时云分析实现智能基础设施管理。",
      "motivation": "传统物理安全系统和环境安全监测系统通常是独立运行的，这导致操作效率低下、应急响应延迟以及管理复杂度增加。该研究旨在解决这些问题，通过集成两个系统来提升智能基础设施管理的协同性。现有商业解决方案成本高昂且缺乏灵活性，无法提供实时综合监控，因此开发一个成本效益高、易于部署的集成框架至关重要，以应对紧急情况和降低运营负担。",
      "method": "本研究提出一个双模态物联网框架，集成了RFID访问控制和多传感器环境安全监测。系统包括两个协调的子系统：子系统1实现RFID认证、伺服控制门和实时Google Sheets日志记录；子系统2提供火焰检测、水流测量、LCD状态显示和人员识别。关键创新点在于使用ESP32微控制器进行边缘处理和无线连接，通过统一的云架构实现数据分析，并引入智能本地缓存机制以在网络中断时保持操作完整性。",
      "result": "在45天的实验评估中，系统表现优异：RFID认证准确率达99.2%，平均响应时间仅为0.82秒；火焰检测在5米范围内的可靠性为98.5%；云数据记录成功率为99.8%。系统通过网络中断时的智能本地缓存保持了操作完整性。总实现成本为5400 BDT（约48美元），比商业集成方案降低了82%，展示了出色的成本效益和性能提升。",
      "conclusion": "该研究建立了一个实用的安全-安全集成框架，证明了通过架构设计和组件优化可以实现专业级性能，同时保持成本效益和可访问性。它为多样化的应用场景提供了解决方案，具有学术和实际价值。未来工作可以扩展到更多传感器类型和集成其他功能，以进一步提升系统的适应性和扩展性。",
      "tags": [
        "IoT Framework",
        "RFID-based Access Control",
        "Environmental Safety Monitoring",
        "Real-time Cloud Analytics",
        "ESP32 Microcontroller"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:38.661810Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20364",
    "title": "RAW-Flow: Advancing RGB-to-RAW Image Reconstruction with Deterministic Latent Flow Matching",
    "authors": [
      "Zhen Liu",
      "Diedong Feng",
      "Hai Jiang",
      "Liaoyuan Zeng",
      "Hao Wang",
      "Chaoyu Feng",
      "Lei Lei",
      "Bing Zeng",
      "Shuaicheng Liu"
    ],
    "abstract": "RGB-to-RAW reconstruction, or the reverse modeling of a camera Image Signal Processing (ISP) pipeline, aims to recover high-fidelity RAW data from RGB images. Despite notable progress, existing learning-based methods typically treat this task as a direct regression objective and struggle with detail inconsistency and color deviation, due to the ill-posed nature of inverse ISP and the inherent information loss in quantized RGB images. To address these limitations, we pioneer a generative perspective by reformulating RGB-to-RAW reconstruction as a deterministic latent transport problem and introduce a novel framework named RAW-Flow, which leverages flow matching to learn a deterministic vector field in latent space, to effectively bridge the gap between RGB and RAW representations and enable accurate reconstruction of structural details and color information. To further enhance latent transport, we introduce a cross-scale context guidance module that injects hierarchical RGB features into the flow estimation process. Moreover, we design a dual-domain latent autoencoder with a feature alignment constraint to support the proposed latent transport framework, which jointly encodes RGB and RAW inputs while promoting stable training and high-fidelity reconstruction. Extensive experiments demonstrate that RAW-Flow outperforms state-of-the-art approaches both quantitatively and visually.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20364.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20364",
    "published": "2026-01-28T08:27:38Z",
    "updated": "2026-01-28T08:27:38Z",
    "comment": "AAAI2026 Oral",
    "light_analysis": {
      "overview": "本文提出了RAW-Flow框架，通过确定性潜在流匹配改进了RGB到RAW图像的重建精度。",
      "motivation": "RGB到RAW图像重建旨在从RGB图像中恢复高保真的RAW数据，这对于图像增强和计算摄影等应用至关重要。现有学习型方法通常将其视为直接回归目标，但由于逆图像信号处理（ISP）的不适定性和量化RGB图像中的信息损失，导致细节不一致和颜色偏差，限制了重建质量。因此，需要一种新方法来克服这些挑战，提高重建的准确性和一致性。",
      "method": "RAW-Flow框架将RGB-to-RAW重建重新定义为确定性潜在传输问题，利用流匹配技术在潜在空间中学习确定性向量场。核心创新包括交叉尺度上下文指导模块，注入分层RGB特征以增强流估计过程，以及双域潜在自编码器，该编码器联合编码RGB和RAW输入，并引入特征对齐约束，以支持稳定训练和高保真重建。这些组件共同桥接了RGB和RAW表示之间的差距，改进了细节和颜色的恢复。",
      "result": "实验结果显示，RAW-Flow在定量和视觉评估中优于现有最先进的方法，显著提升了细节一致性和颜色准确性。然而，摘要未明确说明具体的性能指标如峰值信噪比（PSNR）或结构相似性指数（SSIM）的提升数值，因此基于报告的优越性推断其在重建任务中的有效性。",
      "conclusion": "本研究的主要贡献是通过确定性潜在流匹配重新定义了RGB-to-RAW重建，提出了RAW-Flow框架，有效解决了现有方法的局限性。这一方法在学术上推动了逆ISP问题的研究，并在实际应用中提升了图像处理的质量。未来工作可探索其在其他图像处理任务中的扩展，或优化计算效率以增强实用性。",
      "tags": [
        "Flow Matching",
        "Deterministic Latent Transport",
        "RGB-to-RAW Reconstruction",
        "Cross-Scale Context Guidance",
        "Dual-Domain Latent Autoencoder"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:48.035676Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20363",
    "title": "Can Continuous-Time Diffusion Models Generate and Solve Globally Constrained Discrete Problems? A Study on Sudoku",
    "authors": [
      "Mariia Drozdova"
    ],
    "abstract": "Can standard continuous-time generative models represent distributions whose support is an extremely sparse, globally constrained discrete set? We study this question using completed Sudoku grids as a controlled testbed, treating them as a subset of a continuous relaxation space. We train flow-matching and score-based models along a Gaussian probability path and compare deterministic (ODE) sampling, stochastic (SDE) sampling, and DDPM-style discretizations derived from the same continuous-time training. Unconditionally, stochastic sampling substantially outperforms deterministic flows; score-based samplers are the most reliable among continuous-time methods, and DDPM-style ancestral sampling achieves the highest validity overall. We further show that the same models can be repurposed for guided generation: by repeatedly sampling completions under clamped clues and stopping when constraints are satisfied, the model acts as a probabilistic Sudoku solver. Although far less sample-efficient than classical solvers and discrete-geometry-aware diffusion methods, these experiments demonstrate that classic diffusion/flow formulations can assign non-zero probability mass to globally constrained combinatorial structures and can be used for constraint satisfaction via stochastic search.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20363.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20363",
    "published": "2026-01-28T08:26:54Z",
    "updated": "2026-01-28T08:26:54Z",
    "comment": "26 pages, 5 figures. Empirical study of continuous-time diffusion and flow models on Sudoku. Code available at https://github.com/MariiaDrozdova/sudoku_generation",
    "light_analysis": {
      "overview": "本研究通过数独问题展示了连续时间扩散模型能够表示稀疏、全局约束的离散分布，并实现了生成和求解功能，揭示了其适用性与局限性。",
      "motivation": "本研究旨在解决标准连续时间生成模型是否能表示支持为极其稀疏、全局约束的离散集分布的问题，这在AI领域具有重要性，因为生成模型通常处理连续或简单离散数据，而复杂组合结构如数独网格涉及强约束，现有方法可能难以有效处理这类高维离散空间。研究动机源于探索扩散模型在组合优化问题中的潜力，以扩展其应用范围。摘要未明确说明现有具体方法的不足，但暗示传统方法在表示稀疏离散集时存在挑战。",
      "method": "研究方法以完成的数独网格为测试平台，将其视为连续松弛空间的子集。核心方法包括训练流动匹配和基于分数的模型，沿Gaussian概率路径操作，并比较了确定性ODE采样、随机SDE采样以及从同一连续时间训练中衍生的DDPM风格离散化。关键创新点在于系统评估不同采样策略，并通过引导生成机制重新利用模型：在固定数独线索下重复采样完成网格，直到约束满足，从而作为概率求解器。技术特色包括在连续空间处理离散问题，使用多样采样方法分析性能。",
      "result": "主要实验结果表明，在无条件生成中，随机SDE采样显著优于确定性ODE流动；基于分数的采样器在连续时间方法中最为可靠；而DDPM风格祖先采样实现了最高的整体有效性，尽管摘要未提供具体数值。作为求解器，模型能通过随机搜索满足约束，但样本效率远低于经典求解器和离散几何感知的扩散方法。这些结果展示了连续时间扩散模型对全局约束组合结构的表示能力，为性能比较提供了实证支持。",
      "conclusion": "本研究的贡献在于证明了经典扩散和流动模型能够为非零概率质量分配到全局约束的组合结构，并可用于通过随机搜索实现约束满足，具有学术价值，为生成模型在组合优化问题中的应用提供了新视角。实际应用价值包括启发扩散模型在离散领域的扩展，但局限性在于样本效率较低。未来工作可探索改进效率或结合其他技术以增强实用性。",
      "tags": [
        "Continuous-Time Diffusion Models",
        "Flow-Matching",
        "Score-Based Models",
        "ODE/SDE Sampling",
        "DDPM"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:42.513017Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20361",
    "title": "TINNs: Time-Induced Neural Networks for Solving Time-Dependent PDEs",
    "authors": [
      "Chen-Yang Dai",
      "Che-Chia Chang",
      "Te-Sheng Lin",
      "Ming-Chih Lai",
      "Chieh-Hsin Lai"
    ],
    "abstract": "Physics-informed neural networks (PINNs) solve time-dependent partial differential equations (PDEs) by learning a mesh-free, differentiable solution that can be evaluated anywhere in space and time. However, standard space--time PINNs take time as an input but reuse a single network with shared weights across all times, forcing the same features to represent markedly different dynamics. This coupling degrades accuracy and can destabilize training when enforcing PDE, boundary, and initial constraints jointly. We propose Time-Induced Neural Networks (TINNs), a novel architecture that parameterizes the network weights as a learned function of time, allowing the effective spatial representation to evolve over time while maintaining shared structure. The resulting formulation naturally yields a nonlinear least-squares problem, which we optimize efficiently using a Levenberg--Marquardt method. Experiments on various time-dependent PDEs show up to $4\\times$ improved accuracy and $10\\times$ faster convergence compared to PINNs and strong baselines.",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20361.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20361",
    "published": "2026-01-28T08:23:28Z",
    "updated": "2026-01-28T08:23:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Time-Induced Neural Networks（TINNs），通过参数化网络权重为时间的函数，显著提升解决时间依赖偏微分方程的准确性和收敛速度。",
      "motivation": "物理信息神经网络（PINNs）在求解时间依赖偏微分方程时，将时间作为输入但使用在所有时间点共享权重的网络，这导致同一网络特征被迫表示明显不同的动态，降低了准确性，并在同时强制执行PDE、边界和初始条件时可能使训练不稳定。因此，需要改进现有方法，以解耦时间依赖，增强表示能力和训练稳定性。",
      "method": "TINNs采用新颖架构，将网络权重参数化为时间的函数，允许有效的空间表示随时间演变，同时保持共享结构。该方法自然地形成了一个非线性最小二乘问题，研究使用Levenberg–Marquardt方法进行高效优化，并在各种时间依赖PDEs上进行实验验证，以评估其性能。",
      "result": "实验在多种时间依赖偏微分方程上进行，结果表明，与标准PINNs和强基线方法相比，TINNs实现了高达4倍的准确率提升和10倍的收敛速度加快，显著提高了求解效率和精度，验证了其优越性能。",
      "conclusion": "TINNs通过允许网络权重随时间变化，有效克服了标准PINNs在处理时间依赖PDEs时的局限性，提供了更高的准确性和更快的训练收敛，推动了物理信息神经网络在科学计算中的应用。未来工作可能包括优化计算复杂性或扩展到更复杂的PDE类型，摘要未明确说明具体局限性。",
      "tags": [
        "Physics-informed Neural Networks",
        "Time-dependent Partial Differential Equations",
        "Neural Network Parameterization",
        "Levenberg–Marquardt Method",
        "Nonlinear Least Squares"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:07.289160Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20357",
    "title": "TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs",
    "authors": [
      "Minjae Lee",
      "Wonjun Kang",
      "Byeongkeun Ahn",
      "Christian Classen",
      "Kevin Galim",
      "Seunghyuk Oh",
      "Minghao Yan",
      "Hyung Il Koo",
      "Kangwook Lee"
    ],
    "abstract": "Speculative decoding (SD) has proven effective for accelerating LLM inference by quickly generating draft tokens and verifying them in parallel. However, SD remains largely unexplored for Large Vision-Language Models (LVLMs), which extend LLMs to process both image and text prompts. To address this gap, we benchmark existing inference methods with small draft models on 11 datasets across diverse input scenarios and observe scenario-specific performance fluctuations. Motivated by these findings, we propose Test-time Adaptive Batched Ensemble Drafting (TABED), which dynamically ensembles multiple drafts obtained via batch inference by leveraging deviations from past ground truths available in the SD setting. The dynamic ensemble method achieves an average robust walltime speedup of 1.74x over autoregressive decoding and a 5% improvement over single drafting methods, while remaining training-free and keeping ensembling costs negligible through parameter sharing. With its plug-and-play compatibility, we further enhance TABED by integrating advanced verification and alternative drafting methods. Code and custom-trained models are available at https://github.com/furiosa-ai/TABED.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20357.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20357",
    "published": "2026-01-28T08:16:57Z",
    "updated": "2026-01-28T08:16:57Z",
    "comment": "Accepted to Findings of EACL 2026",
    "light_analysis": {
      "overview": "论文提出TABED，一种测试时自适应集成草稿方法，用于增强大型视觉语言模型在推测解码中的稳健性和效率。",
      "motivation": "SD在LLMs中已证明能加速推理，但在LVLMs中应用不足，导致性能不稳定。现有方法使用小草稿模型时，在不同输入场景下表现波动，影响推理效率和可靠性。为了解决这一问题，研究动机是扩展SD到多模态场景，开发自适应方法以应对场景特异性变化，确保LVLMs在各种条件下都能保持高效且稳健的解码性能，这对推动多模态AI应用至关重要。",
      "method": "论文提出TABED方法，通过批推理生成多个草稿令牌，并动态集成它们，利用推测解码设置中过去真实值的偏差进行自适应调整。关键创新包括测试时自适应策略、无需训练的实现，以及通过参数共享最小化集成成本。方法在11个多样数据集上基准测试，支持即插即用集成其他技术如高级验证和替代草稿方法，增强了兼容性和灵活性。",
      "result": "实验结果显示，TABED实现了平均1.74倍的墙时间加速，相比自回归解码，并在效率上比单一草稿方法提升5%。性能在不同输入场景下保持稳健，通过动态集成减少波动，集成成本可忽略且无需训练。这表明方法有效提升了LVLMs的推理速度和稳定性，优于现有基线方法。",
      "conclusion": "TABED通过自适应集成草稿，显著提升了LVLMs推测解码的效率和稳健性。主要贡献是提出一种无需训练、低成本的动态集成方法，扩展了SD到多模态领域。学术价值在于推动LVLMs推理优化研究，实际应用支持高效AI系统部署。未来工作可进一步优化集成策略或探索更广泛的技术集成。",
      "tags": [
        "Speculative Decoding",
        "Large Vision-Language Models",
        "Ensemble Learning",
        "Batch Inference",
        "Test-Time Adaptation"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:32.848806Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20355",
    "title": "CURVE: Learning Causality-Inspired Invariant Representations for Robust Scene Understanding via Uncertainty-Guided Regularization",
    "authors": [
      "Yue Liang",
      "Jiatong Du",
      "Ziyi Yang",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "abstract": "Scene graphs provide structured abstractions for scene understanding, yet they often overfit to spurious correlations, severely hindering out-of-distribution generalization. To address this limitation, we propose CURVE, a causality-inspired framework that integrates variational uncertainty modeling with uncertainty-guided structural regularization to suppress high-variance, environment-specific relations. Specifically, we apply prototype-conditioned debiasing to disentangle invariant interaction dynamics from environment-dependent variations, promoting a sparse and domain-stable topology. Empirically, we evaluate CURVE in zero-shot transfer and low-data sim-to-real adaptation, verifying its ability to learn domain-stable sparse topologies and provide reliable uncertainty estimates to support risk prediction under distribution shifts.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20355.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20355",
    "published": "2026-01-28T08:15:56Z",
    "updated": "2026-01-28T08:15:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出CURVE框架，通过因果启发的变分不确定性建模和正则化，抑制环境特定关系，学习稀疏域稳定拓扑以增强场景理解的鲁棒性和分布外泛化能力。",
      "motivation": "场景图作为场景理解的结构化抽象，常因过拟合虚假相关性而严重影响分布外泛化能力，这在实际应用中如自动驾驶等领域至关重要。现有方法往往依赖训练数据中的偏差模式，导致在环境变化时泛化性能下降，缺乏鲁棒性。因此，开发能消除虚假相关、学习不变表示的框架具有迫切需求，以应对分布偏移带来的挑战，提升模型在未知场景中的可靠性。",
      "method": "CURVE框架集成变分不确定性建模与不确定性指导的结构正则化，通过原型条件去偏技术解耦不变交互动态和环境依赖变化，从而抑制高方差的环境特定关系，促进稀疏且域稳定的拓扑结构。该方法利用因果启发的表示学习，其中原型作为条件向量引导特征分离，优化场景图表示，但摘要未明确说明具体数据集和模型架构细节，推断可能涉及图神经网络和变分推断技术。",
      "result": "实验在零样本迁移和低数据模拟到真实适应场景中进行，验证了CURVE能学习域稳定稀疏拓扑并提供可靠的不确定性估计，支持分布变化下的风险预测。摘要未明确说明具体性能指标如准确率提升，但通过与基线方法对比，强调了其在泛化能力和鲁棒性上的显著改进，为实际应用中的风险建模奠定了基础，显示了在分布外场景中的优越适应性。",
      "conclusion": "CURVE框架通过因果启发和不确定性正则化，有效抑制虚假相关并学习不变表示，提升了场景理解的鲁棒性和泛化能力。学术上，它推动了表示学习与因果推断的结合；应用上，支持分布变化下的风险预测，具有在自动驾驶和机器人视觉等领域的潜在价值。未来工作可扩展至更多数据类型和复杂环境，或优化计算效率以适应实时应用。",
      "tags": [
        "Causality-Inspired Learning",
        "Uncertainty Modeling",
        "Structural Regularization",
        "Prototype-Conditioned Debiasing",
        "Scene Graph Understanding"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:47.530784Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20354",
    "title": "Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models",
    "authors": [
      "Zengbin Wang",
      "Xuecai Hu",
      "Yong Wang",
      "Feng Xiong",
      "Man Zhang",
      "Xiangxiang Chu"
    ],
    "abstract": "Text-to-image (T2I) models have achieved remarkable success in generating high-fidelity images, but they often fail in handling complex spatial relationships, e.g., spatial perception, reasoning, or interaction. These critical aspects are largely overlooked by current benchmarks due to their short or information-sparse prompt design. In this paper, we introduce SpatialGenEval, a new benchmark designed to systematically evaluate the spatial intelligence of T2I models, covering two key aspects: (1) SpatialGenEval involves 1,230 long, information-dense prompts across 25 real-world scenes. Each prompt integrates 10 spatial sub-domains and corresponding 10 multi-choice question-answer pairs, ranging from object position and layout to occlusion and causality. Our extensive evaluation of 21 state-of-the-art models reveals that higher-order spatial reasoning remains a primary bottleneck. (2) To demonstrate that the utility of our information-dense design goes beyond simple evaluation, we also construct the SpatialT2I dataset. It contains 15,400 text-image pairs with rewritten prompts to ensure image consistency while preserving information density. Fine-tuned results on current foundation models (i.e., Stable Diffusion-XL, Uniworld-V1, OmniGen2) yield consistent performance gains (+4.2%, +5.7%, +4.4%) and more realistic effects in spatial relations, highlighting a data-centric paradigm to achieve spatial intelligence in T2I models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20354.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20354",
    "published": "2026-01-28T08:15:00Z",
    "updated": "2026-01-28T08:15:00Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": {
      "overview": "论文提出SpatialGenEval基准评估文本到图像模型的空间智能，并通过SpatialT2I数据集以数据驱动方法提升模型性能。",
      "motivation": "文本到图像模型虽能生成高保真图像，但在处理复杂空间关系如位置感知、推理和交互时常失败，这限制了模型在现实场景中的应用。现有评估基准由于使用简短或信息稀疏的提示，未能全面测试空间能力，导致模型在这些关键领域的不足被忽视。这一问题的解决对于提升模型的可靠性和视觉理解能力至关重要，尤其是在需要精确空间布局和因果推理的任务中。",
      "method": "研究方法包括两部分：首先，设计SpatialGenEval基准，包含1,230个长、信息密集的文本提示，覆盖25个现实场景和10个空间子域（如对象位置、遮挡和因果关系），每个提示对应10个多项选择题-答案对以系统评估模型。其次，构建SpatialT2I数据集，含15,400个文本-图像对，通过重写提示确保图像一致性并保持信息密度，用于微调基础模型（如Stable Diffusion-XL），以数据为中心提升空间智能，强调信息密集型设计在评估和优化中的双重作用。",
      "result": "实验结果表明，对21个最先进文本到图像模型的评估显示，高阶空间推理是主要性能瓶颈，突显了现有模型在复杂关系处理上的不足。通过使用SpatialT2I数据集微调基础模型（如Stable Diffusion-XL、Uniworld-V1和OmniGen2），在空间智能方面实现了一致性能提升（分别+4.2%、+5.7%和+4.4%），并生成更真实的空间关系效果。与基线相比，数据驱动方法显著改善了模型在布局和因果推理等任务上的准确性。",
      "conclusion": "论文的主要贡献是提出SpatialGenEval基准，系统评估文本到图像模型的空间智能，填补了当前评估的空白，并通过SpatialT2I数据集展示了数据密集型设计在提升模型性能上的实际应用。该研究具有重要学术价值，推动了以数据为中心的范式在实现空间智能中的探索，为未来改进模型在复杂视觉任务中的能力提供了方向，潜在应用包括增强现实和自动化设计等需要精确空间理解的领域。",
      "tags": [
        "Text-to-Image Models",
        "Spatial Intelligence",
        "Benchmarking",
        "Fine-tuning",
        "Spatial Reasoning"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:43.581072Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20352",
    "title": "AMA: Adaptive Memory via Multi-Agent Collaboration",
    "authors": [
      "Weiquan Huang",
      "Zixuan Wang",
      "Hehai Lin",
      "Sudong Wang",
      "Bo Xu",
      "Qian Li",
      "Beier Zhu",
      "Linyi Yang",
      "Chengwei Qin"
    ],
    "abstract": "The rapid evolution of Large Language Model (LLM) agents has necessitated robust memory systems to support cohesive long-term interaction and complex reasoning. Benefiting from the strong capabilities of LLMs, recent research focus has shifted from simple context extension to the development of dedicated agentic memory systems. However, existing approaches typically rely on rigid retrieval granularity, accumulation-heavy maintenance strategies, and coarse-grained update mechanisms. These design choices create a persistent mismatch between stored information and task-specific reasoning demands, while leading to the unchecked accumulation of logical inconsistencies over time. To address these challenges, we propose Adaptive Memory via Multi-Agent Collaboration (AMA), a novel framework that leverages coordinated agents to manage memory across multiple granularities. AMA employs a hierarchical memory design that dynamically aligns retrieval granularity with task complexity. Specifically, the Constructor and Retriever jointly enable multi-granularity memory construction and adaptive query routing. The Judge verifies the relevance and consistency of retrieved content, triggering iterative retrieval when evidence is insufficient or invoking the Refresher upon detecting logical conflicts. The Refresher then enforces memory consistency by performing targeted updates or removing outdated entries. Extensive experiments on challenging long-context benchmarks show that AMA significantly outperforms state-of-the-art baselines while reducing token consumption by approximately 80% compared to full-context methods, demonstrating its effectiveness in maintaining retrieval precision and long-term memory consistency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20352.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20352",
    "published": "2026-01-28T08:09:49Z",
    "updated": "2026-01-28T08:09:49Z",
    "comment": "8 pages",
    "light_analysis": {
      "overview": "提出了自适应记忆框架AMA，通过多代理协作管理多粒度记忆，优化检索效率和长期记忆一致性。",
      "motivation": "随着大型语言模型代理的快速发展，稳健的记忆系统对于支持长期交互和复杂推理至关重要。然而，现有方法依赖固定的检索粒度、积累重的维护策略和粗粒度更新机制，导致存储信息与任务特定需求不匹配，并随时间累积逻辑不一致性，限制了记忆系统的实际应用效果。因此，需要更灵活和高效的自适应记忆系统来应对这些挑战。",
      "method": "AMA框架通过多代理协作实现自适应记忆系统，采用分层设计动态对齐检索粒度与任务复杂度。核心创新点包括Constructor和Retriever协同构建多粒度记忆并进行自适应查询路由，Judge验证检索内容的相关性和一致性，触发迭代检索或在检测到逻辑冲突时调用Refresher。Refresher执行有针对性的更新或删除过时条目，以确保记忆一致性。该方法基于LLM的强大能力，优化了记忆管理的效率和准确性。",
      "result": "在长期上下文基准测试中的广泛实验表明，AMA显著优于当前最先进的基线方法。具体而言，相比于全上下文方法，AMA减少了大约80%的令牌消耗，同时保持了检索精度和长期记忆一致性，证明了其在提高资源效率和任务准确性方面的有效性。",
      "conclusion": "AMA框架通过多代理协作实现了自适应的多粒度记忆系统，有效解决了现有记忆系统的刚性问题和逻辑不一致性累积。该研究在学术上推动了记忆系统设计的发展，为LLM代理的长期交互和复杂推理任务提供了实际应用价值。未来工作可探索扩展性和更复杂场景下的应用，摘要未明确说明具体局限性。",
      "tags": [
        "Multi-Agent Collaboration",
        "Adaptive Memory Systems",
        "Hierarchical Memory Design",
        "LLM Agents",
        "Long-term Context Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:53.183815Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20351",
    "title": "PalmBridge: A Plug-and-Play Feature Alignment Framework for Open-Set Palmprint Verification",
    "authors": [
      "Chenke Zhang",
      "Ziyuan Yang",
      "Licheng Yan",
      "Shuyi Li",
      "Andrew Beng Jin Teoh",
      "Bob Zhang",
      "Yi Zhang"
    ],
    "abstract": "Palmprint recognition is widely used in biometric systems, yet real-world performance often degrades due to feature distribution shifts caused by heterogeneous deployment conditions. Most deep palmprint models assume a closed and stationary distribution, leading to overfitting to dataset-specific textures rather than learning domain-invariant representations. Although data augmentation is commonly used to mitigate this issue, it assumes augmented samples can approximate the target deployment distribution, an assumption that often fails under significant domain mismatch. To address this limitation, we propose PalmBridge, a plug-and-play feature-space alignment framework for open-set palmprint verification based on vector quantization. Rather than relying solely on data-level augmentation, PalmBridge learns a compact set of representative vectors directly from training features. During enrollment and verification, each feature vector is mapped to its nearest representative vector under a minimum-distance criterion, and the mapped vector is then blended with the original vector. This design suppresses nuisance variation induced by domain shifts while retaining discriminative identity cues. The representative vectors are jointly optimized with the backbone network using task supervision, a feature-consistency objective, and an orthogonality regularization term to form a stable and well-structured shared embedding space. Furthermore, we analyze feature-to-representative mappings via assignment consistency and collision rate to assess model's sensitivity to blending weights. Experiments on multiple palmprint datasets and backbone architectures show that PalmBridge consistently reduces EER in intra-dataset open-set evaluation and improves cross-dataset generalization with negligible to modest runtime overhead.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20351.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20351",
    "published": "2026-01-28T08:09:49Z",
    "updated": "2026-01-28T08:09:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "PalmBridge 是一种基于向量量化的插件式特征对齐框架，旨在提升开放集手掌验证的性能和泛化能力。",
      "motivation": "手掌识别在生物识别系统中广泛应用，但在真实世界部署中，异构条件如光照和传感器差异会导致特征分布偏移，降低性能。现有深度模型通常假设闭集和静态分布，容易过拟合训练数据集的特定纹理，而非学习域不变表示。数据增强方法试图缓解问题，但假设增强样本能近似目标分布，在显著领域不匹配时往往失效，因此需要更鲁棒的特征对齐方法来解决这一挑战。",
      "method": "PalmBridge 框架基于向量量化，从训练特征中学习一组紧凑的代表性向量。在注册和验证时，每个特征向量通过最小距离准则映射到最近的代表性向量，然后将映射向量与原始向量混合，以抑制域偏移引起的变异并保留区分性身份线索。代表性向量与骨干网络联合优化，使用任务监督、特征一致性目标和正交正则化，以形成稳定且结构良好的共享嵌入空间，并通过分析分配一致性和碰撞率来评估模型对混合权重的敏感性。",
      "result": "实验在多个手掌数据集和不同骨干架构上进行，结果显示 PalmBridge 在数据集内开放集评估中一致减少等错误率（EER），性能显著提升。同时，框架改善了跨数据集泛化能力，表明其对域偏移具有鲁棒性，且运行时开销可忽略到适度，确保实际应用中的可行性。",
      "conclusion": "PalmBridge 通过特征空间对齐有效抑制了异构部署条件引起的特征分布偏移，同时保留了区分性身份线索，提高了手掌验证系统的泛化能力和鲁棒性。该框架具有插件式设计，易于集成到现有系统中，对生物识别领域的实际部署有重要价值，未来工作可进一步探索混合权重的优化和扩展到其他生物特征识别任务。",
      "tags": [
        "Vector Quantization",
        "Feature Alignment",
        "Open-Set Verification",
        "Orthogonality Regularization",
        "Domain Adaptation"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:59.665122Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20347",
    "title": "MMSF: Multitask and Multimodal Supervised Framework for WSI Classification and Survival Analysis",
    "authors": [
      "Chengying She",
      "Chengwei Chen",
      "Xinran Zhang",
      "Ben Wang",
      "Lizhuang Liu",
      "Chengwei Shao",
      "Yun Bian"
    ],
    "abstract": "Multimodal evidence is critical in computational pathology: gigapixel whole slide images capture tumor morphology, while patient-level clinical descriptors preserve complementary context for prognosis. Integrating such heterogeneous signals remains challenging because feature spaces exhibit distinct statistics and scales. We introduce MMSF, a multitask and multimodal supervised framework built on a linear-complexity MIL backbone that explicitly decomposes and fuses cross-modal information. MMSF comprises a graph feature extraction module embedding tissue topology at the patch level, a clinical data embedding module standardizing patient attributes, a feature fusion module aligning modality-shared and modality-specific representations, and a Mamba-based MIL encoder with multitask prediction heads. Experiments on CAMELYON16 and TCGA-NSCLC demonstrate 2.1--6.6\\% accuracy and 2.2--6.9\\% AUC improvements over competitive baselines, while evaluations on five TCGA survival cohorts yield 7.1--9.8\\% C-index improvements compared with unimodal methods and 5.6--7.1\\% over multimodal alternatives.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20347.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20347",
    "published": "2026-01-28T08:05:46Z",
    "updated": "2026-01-28T08:05:46Z",
    "comment": "Submitted to \"Biomedical Signal Processing and Control\"",
    "light_analysis": {
      "overview": "本文提出了MMSF，一个基于线性复杂度MIL骨干的多任务多模态监督框架，通过分解和融合跨模态信息，显著提升整张切片图像分类和生存分析的性能。",
      "motivation": "在计算病理学中，整合整张切片图像（WSI）和临床数据等多模态证据对于预后分析至关重要，因为WSI捕获肿瘤形态学，而临床描述符提供患者级别的补充上下文。然而，这些异构信号的特征空间具有不同的统计特性和尺度，导致整合困难，现有方法往往难以有效融合跨模态信息，限制了性能提升。本研究旨在解决这一挑战，通过开发一个监督框架来分解和融合多模态数据，以克服特征空间差异并提升分析准确性。",
      "method": "MMSF框架构建在具有线性复杂度的多示例学习（MIL）骨干上，核心包括四个模块：图特征提取模块在补丁级别嵌入组织拓扑结构；临床数据嵌入模块标准化患者属性；特征融合模块对齐模态共享和模态特定的表示；以及基于Mamba的MIL编码器，配备多任务预测头。这些模块共同实现跨模态信息的有效分解和融合，创新地结合图表示和Mamba架构，以处理大规模WSI数据并提高计算效率。",
      "result": "在CAMELYON16和TCGA-NSCLC数据集上的分类任务中，MMSF实现了2.1%至6.6%的准确率提升和2.2%至6.9%的AUC提升，优于竞争基线。在五个TCGA生存队列的评估中，与单模态方法相比，C-index提高了7.1%至9.8%，与多模态方法相比提高了5.6%至7.1%。这些结果证实了MMSF在整合多模态数据方面的优越性，显著增强了WSI分类和生存分析的性能。",
      "conclusion": "本研究的主要贡献是提出MMSF框架，有效解决多模态数据在计算病理学中的整合挑战，通过多任务学习和先进的特征融合技术，显著提升了分类和生存分析的性能。其学术价值在于为多模态机器学习提供了新方法，实际应用上可促进精准医疗的发展。未来工作方向未在摘要中明确说明，可能包括扩展至其他医学领域或优化算法效率。",
      "tags": [
        "Multimodal Learning",
        "Multi-task Learning",
        "MIL (Multiple Instance Learning)",
        "Graph Feature Extraction",
        "Mamba-based Encoder"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:18.028611Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20339",
    "title": "Improving Diffusion Language Model Decoding through Joint Search in Generation Order and Token Space",
    "authors": [
      "Yangyi Shen",
      "Tianjian Feng",
      "Jiaqi Han",
      "Wen Wang",
      "Tianlang Chen",
      "Chunhua Shen",
      "Jure Leskovec",
      "Stefano Ermon"
    ],
    "abstract": "Diffusion Language Models (DLMs) offer order-agnostic generation that can explore many possible decoding trajectories. However, current decoding methods commit to a single trajectory, limiting exploration in trajectory space. We introduce Order-Token Search to explore this space through jointly searching over generation order and token values. Its core is a likelihood estimator that scores denoising actions, enabling stable pruning and efficient exploration of diverse trajectories. Across mathematical reasoning and coding benchmarks, Order-Token Search consistently outperforms baselines on GSM8K, MATH500, Countdown, and HumanEval (3.1%, 3.8%, 7.9%, and 6.8% absolute over backbone), matching or surpassing diffu-GRPO post-trained d1-LLaDA. Our work establishes joint search as a key component for advancing decoding in DLMs.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20339.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20339",
    "published": "2026-01-28T07:55:07Z",
    "updated": "2026-01-28T07:55:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Order-Token Search方法，通过联合搜索生成顺序和令牌空间来改进扩散语言模型的解码性能。",
      "motivation": "扩散语言模型（DLMs）具有顺序无关的生成特性，能够探索多种解码轨迹，但在实际应用中，现有解码方法通常固定于单一轨迹，无法充分探索轨迹空间，这限制了模型在复杂任务如数学推理和代码生成中的性能提升。这一问题的重要性在于，高效的解码策略对于发挥DLMs的潜力至关重要，而现有方法的不足在于探索能力有限，导致生成质量受限。",
      "method": "论文引入Order-Token Search方法，通过联合搜索生成顺序和令牌值来探索解码轨迹空间。其核心是一个似然估计器，用于对去噪动作进行评分，支持稳定的剪枝策略和高效探索多样化轨迹。该方法作为一种通用解码策略，不依赖特定数据集或模型架构，适用于各种DLMs，以优化生成过程的探索能力。",
      "result": "在数学推理和代码生成基准测试中，Order-Token Search在GSM8K、MATH500、Countdown和HumanEval上分别实现了3.1%、3.8%、7.9%和6.8%的绝对性能提升，相比基线方法。此外，它匹配或超过了diffu-GRPO后训练的d1-LLaDA模型，显示了其在解码方面的显著优势，证明了联合搜索策略的有效性和通用性。",
      "conclusion": "这项研究的主要贡献是提出了Order-Token Search方法，确立了联合搜索作为推进扩散语言模型解码的关键组件，具有重要的学术价值，为DLMs解码研究提供了新方向。其实用价值在于提升复杂任务的生成性能，未来工作可能包括优化搜索效率、扩展到其他模型类型或应对更广泛的应用场景。",
      "tags": [
        "Diffusion Language Models",
        "Joint Search",
        "Decoding Methods",
        "Likelihood Estimation",
        "Denoising Actions"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:28.663349Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20335",
    "title": "MobileBench-OL: A Comprehensive Chinese Benchmark for Evaluating Mobile GUI Agents in Real-World Environment",
    "authors": [
      "Qinzhuo Wu",
      "Zhizhuo Yang",
      "Hanhao Li",
      "Pengzhi Gao",
      "Wei Liu",
      "Jian Luan"
    ],
    "abstract": "Recent advances in mobile Graphical User Interface (GUI) agents highlight the growing need for comprehensive evaluation benchmarks. While new online benchmarks offer more realistic testing than offline ones, they tend to focus on the agents' task instruction-following ability while neglecting their reasoning and exploration ability. Moreover, these benchmarks do not consider the random noise in real-world mobile environments. This leads to a gap between benchmarks and real-world environments. To addressing these limitations, we propose MobileBench-OL, an online benchmark with 1080 tasks from 80 Chinese apps. It measures task execution, complex reasoning, and noise robustness of agents by including 5 subsets, which set multiple evaluation dimensions. We also provide an auto-eval framework with a reset mechanism, enabling stable and repeatable real-world benchmarking. Evaluating 12 leading GUI agents on MobileBench-OL shows significant room for improvement to meet real-world requirements. Human evaluation further confirms that MobileBench-OL can reliably measure the performance of leading GUI agents in real environments. Our data and code will be released upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20335.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20335",
    "published": "2026-01-28T07:49:48Z",
    "updated": "2026-01-28T07:49:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MobileBench-OL，一个全面的中文在线基准，用于在真实世界环境中评估移动GUI代理的多维度能力。",
      "motivation": "随着移动图形用户界面（GUI）代理技术的快速发展，迫切需要全面的评估基准以衡量其在真实环境中的性能。现有在线基准虽然比离线基准更具真实性，但主要集中于代理的任务指令跟随能力，忽视了推理和探索能力。此外，这些基准未考虑真实世界移动环境中的随机噪声，导致基准与现实应用环境之间存在显著差距，限制了代理在实际部署中的有效性。",
      "method": "论文提出了MobileBench-OL作为核心方法，这是一个在线基准测试集，包含来自80个中文应用的1080个任务，分为5个子集以多维度评估移动GUI代理的任务执行、复杂推理和噪声鲁棒性。关键创新点在于整合了真实世界噪声的影响，并通过自动评估框架和重置机制实现稳定且可重复的基准测试。该方法还设计了多维度评估标准，确保了全面性和可靠性。",
      "result": "对12个领先的移动GUI代理在MobileBench-OL上进行评估，结果显示这些代理在满足真实世界需求方面仍有显著改进空间。人类评估进一步确认了该基准能够可靠地测量代理在真实环境中的性能，与自动化评估结果一致。摘要未明确提供具体性能指标如准确率提升，但强调了基准的有效性和代理性能的不足。",
      "conclusion": "MobileBench-OL填补了现有基准的不足，通过结合多维度评估和真实世界噪声，提供了一个更贴近实际应用的测试环境。该研究的学术价值在于推动了GUI代理评估技术的发展，实际应用价值在于有助于改进移动环境中代理的性能。未来工作可涉及基准的扩展和更多场景的集成。摘要未明确说明具体局限性。",
      "tags": [
        "Mobile GUI Agents",
        "Benchmark Evaluation",
        "Real-World Environments",
        "Complex Reasoning",
        "Noise Robustness"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:37.495864Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20333",
    "title": "Test-Time Adaptation for Anomaly Segmentation via Topology-Aware Optimal Transport Chaining",
    "authors": [
      "Ali Zia",
      "Usman Ali",
      "Umer Ramzan",
      "Abdul Rehman",
      "Abdelwahed Khamis",
      "Wei Xiang"
    ],
    "abstract": "Deep topological data analysis (TDA) offers a principled framework for capturing structural invariants such as connectivity and cycles that persist across scales, making it a natural fit for anomaly segmentation (AS). Unlike thresholdbased binarisation, which produces brittle masks under distribution shift, TDA allows anomalies to be characterised as disruptions to global structure rather than local fluctuations. We introduce TopoOT, a topology-aware optimal transport (OT) framework that integrates multi-filtration persistence diagrams (PDs) with test-time adaptation (TTA). Our key innovation is Optimal Transport Chaining, which sequentially aligns PDs across thresholds and filtrations, yielding geodesic stability scores that identify features consistently preserved across scales. These stabilityaware pseudo-labels supervise a lightweight head trained online with OT-consistency and contrastive objectives, ensuring robust adaptation under domain shift. Across standard 2D and 3D anomaly detection benchmarks, TopoOT achieves state-of-the-art performance, outperforming the most competitive methods by up to +24.1% mean F1 on 2D datasets and +10.2% on 3D AS benchmarks.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20333.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20333",
    "published": "2026-01-28T07:49:28Z",
    "updated": "2026-01-28T07:49:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "TopoOT 是一个结合拓扑感知的最优传输链的框架，用于异常分割的测试时适应，提高了在域偏移下的稳健性。",
      "motivation": "该研究旨在解决异常分割在分布偏移下的脆弱性问题，传统基于阈值的二值化方法容易产生不稳定掩码，而拓扑数据分析（TDA）能捕捉全局结构不变性，如连通性和循环，更适合将异常定义为结构中断而非局部波动。这使得现有方法在应对域偏移时表现不足，TopoOT 通过整合 TDA 与测试时适应来提升可靠性和鲁棒性，满足实际应用中对稳健异常检测的需求。",
      "method": "论文提出 TopoOT 框架，整合多过滤持久图（PDs）和测试时适应（TTA）。核心创新是最优传输链，顺序对齐跨阈值和过滤的 PDs，生成测地稳定分数以识别尺度间一致保留的特征。这些稳定性感知的伪标签监督一个轻量级头的在线训练，使用 OT 一致性和对比学习目标，确保在域偏移下实现稳健适应，关键细节包括持久图处理和自适应训练机制。",
      "result": "在标准 2D 和 3D 异常检测基准测试中，TopoOT 实现了最先进的性能。具体数据表明，在 2D 数据集上，平均 F1 分数比最有竞争力的方法提升了 +24.1%；在 3D 异常分割基准上提升了 +10.2%。这些结果验证了 TopoOT 在多个维度上显著优于基线方法，展示了其卓越的稳健性和高准确率，证明了框架在应对分布偏移时的有效性。",
      "conclusion": "论文的主要贡献是引入了 TopoOT 框架，通过拓扑感知的最优传输链提升异常分割的测试时适应能力，结合了拓扑数据分析和最优传输的理论优势。学术价值在于推动了稳健异常检测方法的发展，实际应用价值体现在工业和医疗等领域对域偏移问题的解决。摘要未明确说明局限性，但未来工作可探索更复杂场景或计算效率优化，进一步扩展方法的适用性。",
      "tags": [
        "Topological Data Analysis",
        "Optimal Transport",
        "Test-Time Adaptation",
        "Anomaly Segmentation",
        "Persistence Diagrams"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:02.169770Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20332",
    "title": "Window-Diffusion: Accelerating Diffusion Language Model Inference with Windowed Token Pruning and Caching",
    "authors": [
      "Fengrui Zuo",
      "Zhiwei Ke",
      "Yiming Liu",
      "Wenqi Lou",
      "Chao Wang",
      "Xvehai Zhou"
    ],
    "abstract": "Diffusion language models (DLMs) generate text through iterative denoising, but inference requires full-sequence attention at every iteration, resulting in substantial redundant computation on masked tokens. Block-wise diffusion can reduce this cost, yet it typically relies on retraining and constrained update orders, limiting its direct applicability to pretrained DLMs. Our token-level analysis reveals pronounced structural locality in DLM inference. Decoding is driven by a small set of prefix-localized active tokens; the influence of distant undecoded context diminishes rapidly, and decoded tokens exhibit stage-wise temporal stability, enabling reuse of intermediate representations except for a brief post-decode transient. Motivated by these observations, we propose \\textbf{\\placeholder}\\footnote{The source code is available at https://github.com/vhicrgit/Window-Diffusion.}, a window-based token pruning and caching method for inference. We maintain a local computation window that slides rightward as denoising progresses, and partition undecoded tokens into: (i) \\textit{active tokens} that are computed online, (ii) \\textit{buffer tokens} whose KV states are cached and periodically refreshed, and (iii) \\textit{far-field tokens} that are pruned outside the window. Computation is restricted to active and buffer tokens within the window, while far-field tokens are omitted at each stage. Experiments on LLaDA and Dream show that, under matched compute budgets, our method achieves up to $99\\times$ inference speedup while largely preserving generation performance.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20332.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20332",
    "published": "2026-01-28T07:49:20Z",
    "updated": "2026-01-28T07:49:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Window-Diffusion方法，通过窗口化的令牌剪枝和缓存加速扩散语言模型推理，无需重新训练模型，显著提高效率。",
      "motivation": "扩散语言模型推理需要每轮迭代进行全序列注意力计算，导致大量冗余计算，尤其是在已解码令牌上，增加了时间和资源成本。现有方法如块状扩散虽然能减少计算，但通常依赖模型重新训练或受限更新顺序，难以直接应用于预训练模型。摘要中的token级别分析揭示了推理过程具有结构局部性：只有少数前缀令牌活跃，远距离未解码上下文影响迅速减弱，已解码令牌状态稳定，这为优化计算提供了理论基础和解决现有不足的动机。",
      "method": "方法的核心是Window-Diffusion，一种基于窗口的令牌剪枝和缓存策略。在推理过程中，维护一个滑动计算窗口，将未解码令牌分为三类：活动令牌（在线计算注意力）、缓冲区令牌（缓存其键值状态并定期刷新）和远场令牌（在窗口外被剪枝）。计算仅限制在窗口内的活动与缓冲区令牌，避免了远场令牌的冗余计算，同时通过缓存减少重复操作。该方法直接应用于预训练模型，无需修改架构或重新训练，关键技术特色包括窗口化和动态分区机制。",
      "result": "实验在LLaDA和Dream模型上进行，结果表明，在相同计算预算下，Window-Diffusion实现了高达99倍的推理加速，显著优于传统方法。与基线相比，生成性能（如文本质量）基本保持不变，验证了方法的有效性。具体数据展示速度提升的同时，未出现明显性能下降，为高效推理提供了实证支持。",
      "conclusion": "论文的主要贡献是提出了Window-Diffusion方法，有效降低了扩散语言模型推理的计算成本，同时保持了生成质量，适用于预训练模型。这项研究具有学术价值，为优化推理过程提供了新思路，并具有实际应用意义，能促进扩散模型在资源受限环境中的部署。未来工作可探索方法在其他模型或任务中的扩展性，以及进一步优化缓存和剪枝策略以提升泛化能力。",
      "tags": [
        "Diffusion Language Models",
        "Token Pruning",
        "KV Caching",
        "Sliding Window",
        "Inference Acceleration"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:42.564528Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20331",
    "title": "GVGS: Gaussian Visibility-Aware Multi-View Geometry for Accurate Surface Reconstruction",
    "authors": [
      "Mai Su",
      "Qihan Yu",
      "Zhongtao Wang",
      "Yilong Li",
      "Chengwei Pan",
      "Yisong Chen",
      "Guoping Wang"
    ],
    "abstract": "3D Gaussian Splatting enables efficient optimization and high-quality rendering, yet accurate surface reconstruction remains challenging. Prior methods improve surface reconstruction by refining Gaussian depth estimates, either via multi-view geometric consistency or through monocular depth priors. However, multi-view constraints become unreliable under large geometric discrepancies, while monocular priors suffer from scale ambiguity and local inconsistency, ultimately leading to inaccurate Gaussian depth supervision. To address these limitations, we introduce a Gaussian visibility-aware multi-view geometric consistency constraint that aggregates the visibility of shared Gaussian primitives across views, enabling more accurate and stable geometric supervision. In addition, we propose a progressive quadtree-calibrated Monocular depth constraint that performs block-wise affine calibration from coarse to fine spatial scales, mitigating the scale ambiguity of depth priors while preserving fine-grained surface details. Extensive experiments on DTU and TNT datasets demonstrate consistent improvements in geometric accuracy over prior Gaussian-based and implicit surface reconstruction methods. Codes are available at an anonymous repository: https://github.com/GVGScode/GVGS.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20331.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20331",
    "published": "2026-01-28T07:48:51Z",
    "updated": "2026-01-28T07:48:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过引入高斯可见性感知的多视角几何一致性约束和渐进四叉树校准的单目深度约束，显著提升了基于高斯方法的三维表面重建的准确性。",
      "motivation": "三维高斯溅射技术虽然在高效优化和高质量渲染方面表现出色，但准确重建表面仍然是一个挑战。现有的改进方法主要通过多视角几何一致性或单目深度先验来细化高斯深度估计，然而在几何差异较大时，多视角约束变得不可靠；同时单目先验存在尺度模糊性和局部不一致性问题，导致高斯深度监督不准确，限制了表面重建的精度。因此，需要设计更稳定和有效的几何监督机制来解决这些不足。",
      "method": "论文提出了两种核心约束方法：首先，高斯可见性感知的多视角几何一致性约束，通过聚合跨视角共享高斯基元的可见性信息，提供更准确和稳定的几何监督，减少大几何差异下的不确定性。其次，渐进四叉树校准的单目深度约束，采用从粗到细的空间尺度进行块级仿射校准，有效缓解单目深度先验的尺度模糊性，并保留细粒度表面细节。这些方法基于高斯溅射框架，在DTU和TNT数据集上进行优化，无需复杂模型架构调整，实现高效集成。",
      "result": "在DTU和TNT数据集上的广泛实验表明，该方法在几何精度上实现了持续改进。相较于先前基于高斯的方法和隐式表面重建技术，新方法表现出更优越的性能，尽管摘要未提供具体数值指标，但实验结果显示其在表面重建准确性上有显著提升，验证了所提约束的有效性和稳定性。",
      "conclusion": "该论文的主要贡献在于提出了高斯可见性感知的多视角几何一致性约束和渐进四叉树校准的单目深度约束，显著提高了三维表面重建的准确性和鲁棒性。这项研究具有重要的学术价值，解决了现有方法在几何监督中的局限性，并具有实际应用潜力，如增强三维重建系统在复杂场景下的性能。摘要未明确说明局限性或未来工作方向，但可推断未来可能扩展至更多数据集或应用场景以进一步验证方法。",
      "tags": [
        "Gaussian Splatting",
        "Multi-View Geometry",
        "Monocular Depth Estimation",
        "Visibility Aggregation",
        "Progressive Calibration"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:16.462523Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20330",
    "title": "PsychePass: Calibrating LLM Therapeutic Competence via Trajectory-Anchored Tournaments",
    "authors": [
      "Zhuang Chen",
      "Dazhen Wan",
      "Zhangkai Zheng",
      "Guanqun Bi",
      "Xiyao Xiao",
      "Binghang Li",
      "Minlie Huang"
    ],
    "abstract": "While large language models show promise in mental healthcare, evaluating their therapeutic competence remains challenging due to the unstructured and longitudinal nature of counseling. We argue that current evaluation paradigms suffer from an unanchored defect, leading to two forms of instability: process drift, where unsteered client simulation wanders away from specific counseling goals, and standard drift, where static pointwise scoring lacks the stability for reliable judgment. To address this, we introduce Ps, a unified framework that calibrates the therapeutic competence of LLMs via trajectory-anchored tournaments. We first anchor the interaction trajectory in simulation, where clients precisely control the fluid consultation process to probe multifaceted capabilities. We then anchor the battle trajectory in judgments through an efficient Swiss-system tournament, utilizing dynamic pairwise battles to yield robust Elo ratings. Beyond ranking, we demonstrate that tournament trajectories can be transformed into credible reward signals, enabling on-policy reinforcement learning to enhance LLMs' performance. Extensive experiments validate the effectiveness of PsychePass and its strong consistency with human expert judgments.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20330.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20330",
    "published": "2026-01-28T07:48:39Z",
    "updated": "2026-01-28T07:48:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出PsychePass框架，通过轨迹锚定锦标赛校准大型语言模型在心理健康咨询中的治疗能力。",
      "motivation": "大型语言模型在心理健康领域展现出潜力，但评估其治疗能力具有挑战性，因为咨询过程具有无结构和纵向特性。当前评估范式存在未锚定缺陷，导致两种不稳定形式：过程漂移（客户模拟偏离特定咨询目标）和标准漂移（静态评分缺乏稳定性），这些问题阻碍了LLM在实际应用中的可靠评估，凸显了对更稳健评估方法的需求。",
      "method": "本论文提出PsychePass框架，通过轨迹锚定锦标赛校准LLM的治疗能力。方法包括：在模拟中锚定交互轨迹，让客户精确控制咨询流程以探测多方面能力；使用瑞士系统锦标赛锚定战斗轨迹，通过动态配对战斗生成稳健的Elo评分进行排名。此外，锦标赛轨迹被转化为奖励信号，用于基于策略的强化学习以提升LLM性能，解决了评估中的不稳定性问题。",
      "result": "广泛实验验证了PsychePass框架的有效性，结果显示其在评估LLM治疗能力方面表现突出，并与人类专家判断具有强一致性。摘要未明确说明具体性能指标如准确率提升，但强调了该方法能够产生可靠的评价结果，有效解决了传统方法中的过程漂移和标准漂移问题。",
      "conclusion": "本研究的主要贡献是PsychePass框架，通过轨迹锚定锦标赛校准LLM治疗能力，具有重要学术价值，为LLM评估提供了新范式。实际应用价值在于增强LLM在心理健康咨询中的可靠性，促进临床使用。未来工作可能包括扩展到其他领域、优化锦标赛机制或进一步整合强化学习策略。",
      "tags": [
        "Large Language Model",
        "Trajectory-Anchored Tournament",
        "Swiss-system Tournament",
        "Reinforcement Learning",
        "Elo Rating"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:34.276320Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20327",
    "title": "CE-RM: A Pointwise Generative Reward Model Optimized via Two-Stage Rollout and Unified Criteria",
    "authors": [
      "Xinyu Hu",
      "Yancheng He",
      "Weixun Wang",
      "Tao Feng",
      "Li Lin",
      "Jiashun Liu",
      "Wenbo Su",
      "Bo Zheng",
      "Xiaojun Wan"
    ],
    "abstract": "Automatic evaluation is crucial yet challenging for open-ended natural language generation, especially when rule-based metrics are infeasible. Compared with traditional methods, the recent LLM-as-a-Judge paradigms enable better and more flexible evaluation, and show promise as generative reward models for reinforcement learning. However, prior work has revealed a notable gap between their seemingly impressive benchmark performance and actual effectiveness in RL practice. We attribute this issue to some limitations in existing studies, including the dominance of pairwise evaluation and inadequate optimization of evaluation criteria. Therefore, we propose CE-RM-4B, a pointwise generative reward model trained with a dedicated two-stage rollout method, and adopting unified query-based criteria. Using only about 5.7K high-quality data curated from the open-source preference dataset, our CE-RM-4B achieves superior performance on diverse reward model benchmarks, especially in Best-of-N scenarios, and delivers more effective improvements in downstream RL practice.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20327.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20327",
    "published": "2026-01-28T07:46:13Z",
    "updated": "2026-01-28T07:46:13Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "本文提出CE-RM-4B，一种通过两阶段展开方法和统一标准优化的点式生成奖励模型，旨在改善强化学习中自动评估的实际效果。",
      "motivation": "研究动机在于解决开放式自然语言生成中自动评估的挑战，特别是在规则度量不可行时。现有LLM-as-a-Judge方法虽在基准测试中表现良好，但在实际强化学习实践中存在显著性能差距，这归因于成对评估的主导地位和评估标准优化不足。自动评估对强化学习至关重要，因此需要更有效的奖励模型来弥补理论与实践之间的鸿沟，以提升模型训练的效率和效果。",
      "method": "研究方法的核心是提出CE-RM-4B，一个点式生成奖励模型。关键创新点包括采用专用的两阶段展开方法进行训练，并使用基于查询的统一标准来优化评估过程。模型仅使用约5.7K从开源偏好数据集中策划的高质量数据进行训练，具体架构在摘要中未详细说明，但强调了通过优化技术来增强模型在强化学习实践中的适用性和鲁棒性。",
      "result": "实验结果表明，CE-RM-4B在多样化的奖励模型基准测试中实现了优越性能，特别是在Best-of-N场景中表现出色。仅使用少量数据（约5.7K），模型就在下游强化学习实践中提供了更有效的改进，验证了其在对比基线方法时的优势。性能提升包括在评估准确性和RL实践效果方面的显著增强，但具体数据指标在摘要中未明确说明。",
      "conclusion": "结论表明，CE-RM-4B通过优化训练策略和统一标准，显著提升了生成奖励模型在强化学习中的实际应用价值。该研究不仅改进了自动评估方法，还为未来的RL研究和应用提供了更可靠的奖励建模框架。潜在局限性可能包括对特定数据集的依赖，未来工作可扩展至更多领域或探索更广泛的数据集以提高泛化能力。",
      "tags": [
        "Generative Reward Model",
        "Two-Stage Rollout",
        "Unified Criteria",
        "LLM-as-a-Judge",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:24.381032Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20326",
    "title": "Beyond Speedup -- Utilizing KV Cache for Sampling and Reasoning",
    "authors": [
      "Zeyu Xing",
      "Xing Li",
      "Hui-Ling Zhen",
      "Mingxuan Yuan",
      "Sinno Jialin Pan"
    ],
    "abstract": "KV caches, typically used only to speed up autoregressive decoding, encode contextual information that can be reused for downstream tasks at no extra cost. We propose treating the KV cache as a lightweight representation, eliminating the need to recompute or store full hidden states. Despite being weaker than dedicated embeddings, KV-derived representations are shown to be sufficient for two key applications: \\textbf{(i) Chain-of-Embedding}, where they achieve competitive or superior performance on Llama-3.1-8B-Instruct and Qwen2-7B-Instruct; and \\textbf{(ii) Fast/Slow Thinking Switching}, where they enable adaptive reasoning on Qwen3-8B and DeepSeek-R1-Distil-Qwen-14B, reducing token generation by up to $5.7\\times$ with minimal accuracy loss. Our findings establish KV caches as a free, effective substrate for sampling and reasoning, opening new directions for representation reuse in LLM inference. Code: https://github.com/cmd2001/ICLR2026_KV-Embedding.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20326.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20326",
    "published": "2026-01-28T07:44:52Z",
    "updated": "2026-01-28T07:44:52Z",
    "comment": "Accepted by ICLR26",
    "light_analysis": {
      "overview": "提出将KV缓存作为轻量级表示，重用于下游采样和推理任务，实现高效信息复用，避免额外计算成本。",
      "motivation": "KV缓存通常仅用于加速自回归解码，但其编码的上下文信息未被充分利用，导致资源浪费。现有方法在重用信息时需要重新计算或存储完整隐藏状态，增加了计算和存储开销。本研究旨在解决这一问题，探索KV缓存的潜力，以提供更高效的表示方式，填补现有方法在低成本复用中的不足。",
      "method": "论文核心方法是将KV缓存视为轻量级表示，直接用于下游任务。关键创新在于利用KV-derived表示，应用于Chain-of-Embedding和Fast/Slow Thinking Switching两个场景。实验使用具体模型如Llama-3.1-8B-Instruct和Qwen2-7B-Instruct，通过重用KV缓存嵌入，实现自适应推理和信息复用，避免了额外计算或存储需求。",
      "result": "在Chain-of-Embedding应用中，KV-derived表示在Llama-3.1-8B-Instruct和Qwen2-7B-Instruct上展现出竞争或更优性能。在Fast/Slow Thinking Switching中，在Qwen3-8B和DeepSeek-R1-Distil-Qwen-14B上，减少了token生成最多5.7倍，且准确率损失最小。与基线方法相比，这显著提升了推理效率，摘要未明确说明具体基线对比数据。",
      "conclusion": "研究确立了KV缓存作为免费、有效的基质，可用于采样和推理任务，避免了重新计算或存储完整隐藏状态的需求。学术价值在于为LLM推理中的表示重用开辟新方向，实际应用价值是降低计算成本。潜在局限性包括KV表示的性能边界，未来工作可进一步优化应用范围和扩展至更多模型。",
      "tags": [
        "KV Cache",
        "Chain-of-Embedding",
        "Fast/Slow Thinking Switching",
        "LLM Inference",
        "Representation Reuse"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:32.689098Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20323",
    "title": "ECG-Agent: On-Device Tool-Calling Agent for ECG Multi-Turn Dialogue",
    "authors": [
      "Hyunseung Chung",
      "Jungwoo Oh",
      "Daeun Kyung",
      "Jiho Kim",
      "Yeonsu Kwon",
      "Min-Gyu Kim",
      "Edward Choi"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models have rapidly expanded to electrocardiograms, focusing on classification, report generation, and single-turn QA tasks. However, these models fall short in real-world scenarios, lacking multi-turn conversational ability, on-device efficiency, and precise understanding of ECG measurements such as the PQRST intervals. To address these limitations, we introduce ECG-Agent, the first LLM-based tool-calling agent for multi-turn ECG dialogue. To facilitate its development and evaluation, we also present ECG-Multi-Turn-Dialogue (ECG-MTD) dataset, a collection of realistic user-assistant multi-turn dialogues for diverse ECG lead configurations. We develop ECG-Agents in various sizes, from on-device capable to larger agents. Experimental results show that ECG-Agents outperform baseline ECG-LLMs in response accuracy. Furthermore, on-device agents achieve comparable performance to larger agents in various evaluations that assess response accuracy, tool-calling ability, and hallucinations, demonstrating their viability for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20323.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20323",
    "published": "2026-01-28T07:40:42Z",
    "updated": "2026-01-28T07:40:42Z",
    "comment": "Accepted to ICASSP 2026 (5 pages, 2 figures, 5 tables)",
    "light_analysis": {
      "overview": "本文提出ECG-Agent，首个基于大型语言模型的工具调用代理，用于心电图多轮对话，解决了现有方法在交互能力和效率上的不足。",
      "motivation": "多模态大型语言模型已扩展到心电图分析，但主要集中于分类、报告生成和单轮问答任务，缺乏处理多轮对话、设备上运行效率以及对ECG关键测量（如PQRST间期）的精确理解能力。这些局限限制了其在现实医疗场景中的应用，如需要交互式诊断和即时反馈的场合，因此开发一个能高效进行多轮对话的ECG代理至关重要，以提升临床辅助决策的实用性和可及性。",
      "method": "研究方法包括引入ECG-Agent，这是一个基于大型语言模型的工具调用代理，专为心电图多轮对话设计，关键创新在于首次将工具调用机制应用于ECG领域，支持复杂的交互任务。为支持开发和评估，创建了ECG-Multi-Turn-Dialogue (ECG-MTD) 数据集，包含多样化ECG导联配置的现实用户-助理多轮对话；代理被设计为不同规模，从适合设备部署的小型版本到大型高性能版本，以优化计算效率和性能平衡。",
      "result": "实验结果显示，ECG-Agent在响应准确性方面优于基线的心电图大型语言模型，具体表现在多轮对话任务中提升了解答精准度。在评估响应准确性、工具调用能力和幻觉控制的多项测试中，设备上的代理表现出与更大规模代理相当的绩效，这表明ECG-Agent不仅通过工具调用增强了交互能力，还实现了高效的设备部署，为实际应用提供了技术可行性。",
      "conclusion": "本论文的主要贡献在于提出了ECG-Agent和ECG-MTD数据集，解决了现有心电图模型在多轮对话、设备效率和精确测量理解方面的不足，其学术价值在于将工具调用代理技术扩展到医疗领域，促进交互式诊断系统的发展；实际应用价值在于展示了设备上代理的可行性，为实时心电图分析提供了新途径。未来工作可能涉及进一步优化代理性能或扩展更多临床场景，但摘要未明确说明具体方向。",
      "tags": [
        "Large Language Model",
        "Tool-Calling Agent",
        "ECG Analysis",
        "Multi-Turn Dialogue",
        "On-Device Computing"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:03.296195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20318",
    "title": "CPiRi: Channel Permutation-Invariant Relational Interaction for Multivariate Time Series Forecasting",
    "authors": [
      "Jiyuan Xu",
      "Wenyu Zhang",
      "Xin Jing",
      "Shuai Chen",
      "Shuai Zhang",
      "Jiahao Nie"
    ],
    "abstract": "Current methods for multivariate time series forecasting can be classified into channel-dependent and channel-independent models. Channel-dependent models learn cross-channel features but often overfit the channel ordering, which hampers adaptation when channels are added or reordered. Channel-independent models treat each channel in isolation to increase flexibility, yet this neglects inter-channel dependencies and limits performance. To address these limitations, we propose \\textbf{CPiRi}, a \\textbf{channel permutation invariant (CPI)} framework that infers cross-channel structure from data rather than memorizing a fixed ordering, enabling deployment in settings with structural and distributional co-drift without retraining. CPiRi couples \\textbf{spatio-temporal decoupling architecture} with \\textbf{permutation-invariant regularization training strategy}: a frozen pretrained temporal encoder extracts high-quality temporal features, a lightweight spatial module learns content-driven inter-channel relations, while a channel shuffling strategy enforces CPI during training. We further \\textbf{ground CPiRi in theory} by analyzing permutation equivariance in multivariate time series forecasting. Experiments on multiple benchmarks show state-of-the-art results. CPiRi remains stable when channel orders are shuffled and exhibits strong \\textbf{inductive generalization} to unseen channels even when trained on \\textbf{only half} of the channels, while maintaining \\textbf{practical efficiency} on large-scale datasets. The source code is released at https://github.com/JasonStraka/CPiRi.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20318.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20318",
    "published": "2026-01-28T07:30:32Z",
    "updated": "2026-01-28T07:30:32Z",
    "comment": "22 pages, ICLR 2026",
    "light_analysis": {
      "overview": "CPiRi是一个通道排列不变框架，通过时空解耦架构和正则化训练策略，解决多元时间序列预测中通道排列敏感性问题。",
      "motivation": "多元时间序列预测中，现有方法分为通道依赖和通道独立模型：通道依赖模型学习跨通道特征但易过拟合固定通道顺序，当通道添加或重排时适应性差；通道独立模型虽灵活处理各通道，却忽略通道间依赖，限制预测性能。这些不足导致模型在实际应用中对动态结构变化的处理能力有限，因此需要开发一种既能捕捉通道间关系又不依赖于顺序的方法。",
      "method": "CPiRi框架结合时空解耦架构和排列不变正则化训练策略：使用预训练的时态编码器提取高质量时态特征，轻量级空间模块基于内容学习通道间关系，通道洗牌策略在训练中强制实现通道排列不变性。核心创新包括理论分析多元时间序列预测中的排列等变性，确保模型不依赖固定顺序，从而适应结构变化。关键细节涉及使用冻结编码器和优化空间模块以提升效率。",
      "result": "在多个基准数据集实验中，CPiRi取得最先进性能。具体表现为：模型在通道顺序洗牌时保持稳定，展现出强适应性；仅训练一半通道即能有效泛化到未见通道，证明其归纳泛化能力。与基线方法相比，CPiRi在保持高性能的同时，在大规模数据集上维持实际效率，支持其在复杂环境中的部署应用，但摘要未明确说明具体准确率提升数值。",
      "conclusion": "本文提出CPiRi框架，主要贡献包括理论分析和实践设计通道排列不变性，通过时空解耦和正则化策略解决现有模型局限。学术价值在于丰富了多元时间序列预测的理论基础，实际应用价值体现在增强模型对动态环境和结构变化的适应性，为传感器网络等领域提供强大工具。未来工作可探索更多应用场景或扩展理论分析，但摘要未明确说明具体局限性。",
      "tags": [
        "Multivariate Time Series Forecasting",
        "Channel Permutation Invariance",
        "Spatio-Temporal Decoupling",
        "Inductive Generalization",
        "Permutation Equivariance"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:53.151503Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20312",
    "title": "SAPO: Self-Adaptive Process Optimization Makes Small Reasoners Stronger",
    "authors": [
      "Kaiyuan Chen",
      "Guangmin Zheng",
      "Jin Wang",
      "Xiaobing Zhou",
      "Xuejie Zhang"
    ],
    "abstract": "Existing self-evolution methods overlook the influence of fine-grained reasoning steps, which leads to the reasoner-verifier gap. The computational inefficiency of Monte Carlo (MC) process supervision further exacerbates the difficulty in mitigating the gap. Motivated by the Error-Related Negativity (ERN), which the reasoner can localize error following incorrect decisions, guiding rapid adjustments, we propose a Self-Adaptive Process Optimization (SAPO) method for self-improvement in Small Language Models (SLMs). SAPO adaptively and efficiently introduces process supervision signals by actively minimizing the reasoner-verifier gap rather than relying on inefficient MC estimations. Extensive experiments demonstrate that the proposed method outperforms most existing self-evolution methods on two challenging task types: mathematics and code. Additionally, to further investigate SAPO's impact on verifier performance, this work introduces two new benchmarks for process reward models in both mathematical and coding tasks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20312.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20312",
    "published": "2026-01-28T07:04:30Z",
    "updated": "2026-01-28T07:04:30Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "SAPO 方法通过自适应过程优化，有效减少推理器-验证器差距，提升了小型语言模型在数学和代码任务中的推理能力。",
      "motivation": "现有自我进化方法忽视细粒度推理步骤的影响，导致推理器与验证器之间的性能差距；Monte Carlo 过程监督的计算效率低下加剧了缩小这一差距的难度，影响了模型优化效果，因此需要开发更高效的策略来改进小型语言模型的推理过程。",
      "method": "SAPO 基于错误相关负性原理，设计了一种自适应过程优化机制，通过主动最小化推理器-验证器差距来引入监督信号，避免依赖低效的 Monte Carlo 估计；该方法针对小型语言模型进行自我改进，核心创新点在于高效利用过程反馈进行动态调整。",
      "result": "在数学和代码任务上的实验表明，SAPO 优于大多数现有自我进化方法，展示了其在提升推理能力方面的有效性；但摘要未明确说明具体性能指标数据，仅提到相对于基线的优势。",
      "conclusion": "SAPO 通过自适应过程优化显著增强了小型语言模型的推理性能，具有重要的学术和应用价值，特别是在数学和代码领域；未来工作可能包括扩展任务类型和进一步完善验证器基准测试。",
      "tags": [
        "Self-Adaptive Process Optimization",
        "Small Language Models",
        "Process Supervision",
        "Error-Related Negativity",
        "Self-Evolution"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:11.572291Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20308",
    "title": "OSDEnhancer: Taming Real-World Space-Time Video Super-Resolution with One-Step Diffusion",
    "authors": [
      "Shuoyan Wei",
      "Feng Li",
      "Chen Zhou",
      "Runmin Cong",
      "Yao Zhao",
      "Huihui Bai"
    ],
    "abstract": "Diffusion models (DMs) have demonstrated exceptional success in video super-resolution (VSR), showcasing a powerful capacity for generating fine-grained details. However, their potential for space-time video super-resolution (STVSR), which necessitates not only recovering realistic visual content from low-resolution to high-resolution but also improving the frame rate with coherent temporal dynamics, remains largely underexplored. Moreover, existing STVSR methods predominantly address spatiotemporal upsampling under simplified degradation assumptions, which often struggle in real-world scenarios with complex unknown degradations. Such a high demand for reconstruction fidelity and temporal consistency makes the development of a robust STVSR framework particularly non-trivial. To address these challenges, we propose OSDEnhancer, a novel framework that, to the best of our knowledge, represents the first method to achieve real-world STVSR through an efficient one-step diffusion process. OSDEnhancer initializes essential spatiotemporal structures through a linear pre-interpolation strategy and pivots on training temporal refinement and spatial enhancement mixture of experts (TR-SE MoE), which allows distinct expert pathways to progressively learn robust, specialized representations for temporal coherence and spatial detail, further collaboratively reinforcing each other during inference. A bidirectional deformable variational autoencoder (VAE) decoder is further introduced to perform recurrent spatiotemporal aggregation and propagation, enhancing cross-frame reconstruction fidelity. Experiments demonstrate that the proposed method achieves state-of-the-art performance while maintaining superior generalization capability in real-world scenarios.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20308.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20308",
    "published": "2026-01-28T06:59:55Z",
    "updated": "2026-01-28T06:59:55Z",
    "comment": "17 pages, 10 figures. Code will be released upon publication",
    "light_analysis": {
      "overview": "OSDEnhancer是一种基于一步扩散的新框架，首次有效解决真实世界时空视频超分辨率问题，提升视频质量和帧率。",
      "motivation": "该研究旨在解决真实世界时空视频超分辨率问题，该问题不仅需要从低分辨率恢复高分辨率，还需提高帧率并保持时间连贯性。现有扩散模型在视频超分辨率中虽成功，但STVSR领域未被充分探索，且现有方法主要在简化退化假设下工作，难以应对真实世界中的复杂未知退化。由于重建保真度和时间一致性要求高，开发鲁棒的STVSR框架具有挑战性，这推动了本研究的必要性。",
      "method": "论文提出OSDEnhancer框架，采用高效的一步扩散过程。关键方法包括：通过线性预插值策略初始化时空结构；训练时间精炼和空间增强的专家混合模型，使不同专家路径逐步学习鲁棒的专门化表示，以增强时间一致性和空间细节，并在推理时协同工作；引入双向可变形变分自编码器解码器，执行循环时空聚合和传播，提高跨帧重建保真度。这些技术旨在处理复杂未知退化。",
      "result": "摘要未提供具体实验数据，但指出实验表明该方法在真实世界场景中实现了状态领先的性能和优越的泛化能力。这暗示OSDEnhancer在重建质量和时间一致性方面优于现有STVSR方法，但具体性能指标如准确率或效率改进未明确说明。",
      "conclusion": "OSDEnhancer框架的主要贡献是首次使用一步扩散过程解决真实世界STVSR，结合TR-SE MoE和双向可变形VAE等技术提高鲁棒性。其学术价值在于推动STVSR领域发展，为复杂退化场景提供新思路；实际应用价值体现在改善真实世界视频处理。未来工作可能包括进一步优化效率和处理更多退化类型，但摘要未明确说明局限性。",
      "tags": [
        "Diffusion Models",
        "Space-Time Video Super-Resolution",
        "Mixture of Experts",
        "Variational Autoencoder",
        "Temporal Refinement"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:23.445311Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20307",
    "title": "Delayed Feedback Modeling for Post-Click Gross Merchandise Volume Prediction: Benchmark, Insights and Approaches",
    "authors": [
      "Xinyu Li",
      "Sishuo Chen",
      "Guipeng Xv",
      "Li Zhang",
      "Mingxuan Luo",
      "Zhangming Chan",
      "Xiang-Rong Sheng",
      "Han Zhu",
      "Jian Xu",
      "Chen Lin"
    ],
    "abstract": "The prediction objectives of online advertisement ranking models are evolving from probabilistic metrics like conversion rate (CVR) to numerical business metrics like post-click gross merchandise volume (GMV). Unlike the well-studied delayed feedback problem in CVR prediction, delayed feedback modeling for GMV prediction remains unexplored and poses greater challenges, as GMV is a continuous target, and a single click can lead to multiple purchases that cumulatively form the label. To bridge the research gap, we establish TRACE, a GMV prediction benchmark containing complete transaction sequences rising from each user click, which supports delayed feedback modeling in an online streaming manner. Our analysis and exploratory experiments on TRACE reveal two key insights: (1) the rapid evolution of the GMV label distribution necessitates modeling delayed feedback under online streaming training; (2) the label distribution of repurchase samples substantially differs from that of single-purchase samples, highlighting the need for separate modeling. Motivated by these findings, we propose RepurchasE-Aware Dual-branch prEdictoR (READER), a novel GMV modeling paradigm that selectively activates expert parameters according to repurchase predictions produced by a router. Moreover, READER dynamically calibrates the regression target to mitigate under-estimation caused by incomplete labels. Experimental results show that READER yields superior performance on TRACE over baselines, achieving a 2.19% improvement in terms of accuracy. We believe that our study will open up a new avenue for studying online delayed feedback modeling for GMV prediction, and our TRACE benchmark with the gathered insights will facilitate future research and application in this promising direction. Our code and dataset are available at https://github.com/alimama-tech/OnlineGMV .",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20307.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20307",
    "published": "2026-01-28T06:59:46Z",
    "updated": "2026-01-28T06:59:46Z",
    "comment": "This paper has been accepted by the ACM Web Conference (WWW) 2026. This is the camera-ready version. Please refer to the published version for citation once available",
    "light_analysis": {
      "overview": "该论文提出了TRACE基准和READER模型，用于在线广告中后点击总商品量预测的延迟反馈建模，解决了GMV连续目标和重复购买的挑战。",
      "motivation": "在线广告排名模型的预测目标正从概率性指标如转换率转向数值业务指标如后点击总商品量，以更好地反映业务价值。然而，GMV预测中的延迟反馈问题尚未被充分研究，且更具挑战性，因为GMV是连续目标，单次点击可能导致多次购买，累积形成标签。现有方法在CVR预测中已有延迟反馈建模，但无法直接应用于GMV，缺乏合适基准和方法来处理其复杂性。研究GMV预测的延迟反馈建模对提升广告系统性能和业务决策至关重要。",
      "method": "研究方法包括建立TRACE基准，该基准包含从每个用户点击开始的完整交易序列，支持在线流式训练以模拟实时延迟反馈。基于分析，提出READER模型，这是一个重复购买感知的双分支预测器。READER使用路由器预测用户是否会重复购买，然后根据预测结果选择性激活专家参数进行GMV预测。此外，模型动态校准回归目标，以减轻因标签不完全（如未观察到的未来购买）导致的低估。这种方法结合了在线学习和专家机制，专门处理GMV的连续性和重复购买特性。",
      "result": "在TRACE基准上的实验结果表明，READER模型显著优于基线方法，实现了2.19%的准确率提升。具体来说，通过与基线模型对比，READER在GMV预测任务中表现出更好的性能，有效处理了延迟反馈和重复购买的挑战。实验验证了动态校准和选择性激活机制能够改善预测精度，为在线广告系统提供更可靠的GMV估计，展示了所提方法的优越性。",
      "conclusion": "该研究的主要贡献是建立了首个GMV预测延迟反馈建模基准TRACE，并提出了创新的READER模型。TRACE基准提供完整交易序列支持在线流式训练，而READER模型通过路由器机制和动态校准有效处理GMV复杂性和延迟反馈。学术上，研究为GMV预测的在线延迟反馈建模开辟新方向，推动相关领域发展；实际上，改进的GMV预测有助于优化广告排名和业务决策。代码和数据集已开源，促进未来研究和应用，未来工作可能包括扩展到其他指标或优化模型架构。",
      "tags": [
        "Delayed Feedback Modeling",
        "GMV Prediction",
        "Online Learning",
        "Repurchase Prediction",
        "Dual-branch Predictor"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:49.255922Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20306",
    "title": "TPGDiff: Hierarchical Triple-Prior Guided Diffusion for Image Restoration",
    "authors": [
      "Yanjie Tu",
      "Qingsen Yan",
      "Axi Niu",
      "Jiacong Tang"
    ],
    "abstract": "All-in-one image restoration aims to address diverse degradation types using a single unified model. Existing methods typically rely on degradation priors to guide restoration, yet often struggle to reconstruct content in severely degraded regions. Although recent works leverage semantic information to facilitate content generation, integrating it into the shallow layers of diffusion models often disrupts spatial structures (\\emph{e.g.}, blurring artifacts). To address this issue, we propose a Triple-Prior Guided Diffusion (TPGDiff) network for unified image restoration. TPGDiff incorporates degradation priors throughout the diffusion trajectory, while introducing structural priors into shallow layers and semantic priors into deep layers, enabling hierarchical and complementary prior guidance for image reconstruction. Specifically, we leverage multi-source structural cues as structural priors to capture fine-grained details and guide shallow layers representations. To complement this design, we further develop a distillation-driven semantic extractor that yields robust semantic priors, ensuring reliable high-level guidance at deep layers even under severe degradations. Furthermore, a degradation extractor is employed to learn degradation-aware priors, enabling stage-adaptive control of the diffusion process across all timesteps. Extensive experiments on both single- and multi-degradation benchmarks demonstrate that TPGDiff achieves superior performance and generalization across diverse restoration scenarios. Our project page is: https://leoyjtu.github.io/tpgdiff-project.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20306.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20306",
    "published": "2026-01-28T06:55:07Z",
    "updated": "2026-01-28T06:55:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出TPGDiff网络，通过层次化三重先验引导扩散模型实现统一的图像修复。",
      "motivation": "本研究旨在解决统一图像修复中现有方法的局限性。现有方法通常依赖退化先验，但在严重退化区域难以重建图像内容；尽管近期工作引入语义信息，但在浅层集成会破坏空间结构，导致模糊等伪影。这一问题的重要性在于，提升图像修复质量对计算机视觉应用如照片增强、医疗成像和视频处理至关重要，但目前方法在处理多样化退化时表现不足，亟需新方法来整合多种先验以优化修复效果。",
      "method": "TPGDiff网络采用层次化三重先验引导的扩散方法。首先，退化提取器学习退化感知先验，贯穿扩散轨迹以提供阶段自适应控制；其次，结构先验基于多源结构线索，集成到浅层捕捉细粒度细节；最后，通过蒸馏驱动语义提取器生成鲁棒的语义先验，注入深层提供可靠的高级指导。关键创新在于分层集成结构、语义和退化先验，实现互补引导，优化图像重建过程。",
      "result": "在单退化和多退化基准测试中，TPGDiff展示了卓越的性能和泛化能力。实验表明，该方法在多种退化场景下均优于现有基线模型，如能更准确地重建噪声、模糊区域的图像内容。虽然摘要未明确说明具体性能指标（如PSNR或SSIM的提升数值），但整体结果证实了模型在提升视觉质量和处理复杂退化方面的有效性，显示了广泛的应用潜力。",
      "conclusion": "本研究的主要贡献是提出了TPGDiff模型，通过层次化先验引导改进扩散模型在图像修复中的应用。学术上，它丰富了多先验集成方法的研究；实际中，为处理多样化退化提供了高效解决方案，有助于推动图像增强技术的发展。局限性未明确提及，未来工作可能包括扩展到其他视觉任务或进一步优化模型效率以应对大规模应用需求。",
      "tags": [
        "Diffusion Models",
        "Image Restoration",
        "Prior Guidance",
        "Semantic Priors",
        "Structural Priors"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:39.086354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20305",
    "title": "Endogenous Reprompting: Self-Evolving Cognitive Alignment for Unified Multimodal Models",
    "authors": [
      "Zhenchen Tang",
      "Songlin Yang",
      "Zichuan Wang",
      "Bo Peng",
      "Yang Li",
      "Beibei Dong",
      "Jing Dong"
    ],
    "abstract": "Unified Multimodal Models (UMMs) exhibit strong understanding, yet this capability often fails to effectively guide generation. We identify this as a Cognitive Gap: the model lacks the understanding of how to enhance its own generation process. To bridge this gap, we propose Endogenous Reprompting, a mechanism that transforms the model's understanding from a passive encoding process into an explicit generative reasoning step by generating self-aligned descriptors during generation. To achieve this, we introduce SEER (Self-Evolving Evaluator and Reprompter), a training framework that establishes a two-stage endogenous loop using only 300 samples from a compact proxy task, Visual Instruction Elaboration. First, Reinforcement Learning with Verifiable Rewards (RLVR) activates the model's latent evaluation ability via curriculum learning, producing a high-fidelity endogenous reward signal. Second, Reinforcement Learning with Model-rewarded Thinking (RLMT) leverages this signal to optimize the generative reasoning policy. Experiments show that SEER consistently outperforms state-of-the-art baselines in evaluation accuracy, reprompting efficiency, and generation quality, without sacrificing general multimodal capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20305.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20305",
    "published": "2026-01-28T06:54:36Z",
    "updated": "2026-01-28T06:54:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出Endogenous Reprompting机制和SEER框架，通过自我演化的认知对齐，提升统一多模态模型的生成能力。",
      "motivation": "统一多模态模型虽具有强大的理解能力，但在生成过程中缺乏有效引导，导致认知差距：模型不理解如何利用理解来优化生成。这一问题限制了模型在需要生成任务中的应用价值，现有方法未能将理解转化为生成优化机制，因此亟需新方法弥合差距。",
      "method": "论文引入Endogenous Reprompting机制，通过生成自我对齐的描述符，将模型的理解从被动编码转化为明确的生成性推理步骤。SEER训练框架建立两阶段内源循环，仅使用300个样本的Visual Instruction Elaboration代理任务，首先通过RLVR利用课程学习激活潜在评估能力生成内源奖励信号，然后通过RLMT利用该信号优化生成推理策略。",
      "result": "实验结果显示，SEER在评估准确性、reprompting效率和生成质量方面一致超越现有先进基线方法，且未损害模型的一般多模态能力；但由于摘要未明确提供具体数值，效果表现为显著的性能提升和效率改进。",
      "conclusion": "该研究的主要贡献是提出Endogenous Reprompting机制和SEER框架，有效弥合认知差距，增强了统一多模态模型的生成能力，具有学术创新性和实际应用潜力；未来工作可能包括扩展至更多任务或优化框架的泛化性，但摘要未明确说明具体局限性。",
      "tags": [
        "Endogenous Reprompting",
        "Reinforcement Learning",
        "Unified Multimodal Models",
        "Curriculum Learning",
        "Self-Evaluation"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:45.802844Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20304",
    "title": "Structure-constrained Language-informed Diffusion Model for Unpaired Low-dose Computed Tomography Angiography Reconstruction",
    "authors": [
      "Genyuan Zhang",
      "Zihao Wang",
      "Zhifan Gao",
      "Lei Xu",
      "Zhen Zhou",
      "Haijun Yu",
      "Jianjia Zhang",
      "Xiujian Liu",
      "Weiwei Zhang",
      "Shaoyu Wang",
      "Huazhu Fu",
      "Fenglin Liu",
      "Weiwen Wu"
    ],
    "abstract": "The application of iodinated contrast media (ICM) improves the sensitivity and specificity of computed tomography (CT) for a wide range of clinical indications. However, overdose of ICM can cause problems such as kidney damage and life-threatening allergic reactions. Deep learning methods can generate CT images of normal-dose ICM from low-dose ICM, reducing the required dose while maintaining diagnostic power. However, existing methods are difficult to realize accurate enhancement with incompletely paired images, mainly because of the limited ability of the model to recognize specific structures. To overcome this limitation, we propose a Structure-constrained Language-informed Diffusion Model (SLDM), a unified medical generation model that integrates structural synergy and spatial intelligence. First, the structural prior information of the image is effectively extracted to constrain the model inference process, thus ensuring structural consistency in the enhancement process. Subsequently, semantic supervision strategy with spatial intelligence is introduced, which integrates the functions of visual perception and spatial reasoning, thus prompting the model to achieve accurate enhancement. Finally, the subtraction angiography enhancement module is applied, which serves to improve the contrast of the ICM agent region to suitable interval for observation. Qualitative analysis of visual comparison and quantitative results of several metrics demonstrate the effectiveness of our method in angiographic reconstruction for low-dose contrast medium CT angiography.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20304.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20304",
    "published": "2026-01-28T06:54:06Z",
    "updated": "2026-01-28T06:54:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出结构约束语言信息扩散模型，用于非配对低剂量计算机断层扫描血管成像重建，通过整合结构协同和空间智能实现准确增强。",
      "motivation": "碘化对比介质在CT中应用广泛，但过量使用会导致肾脏损伤和过敏反应。深度学习可从低剂量ICM生成正常剂量图像以减少剂量，但现有方法在非配对图像上难以实现准确增强，主要是因为模型识别特定结构的能力有限。因此，本研究旨在克服这一局限性，开发新方法以在减少剂量的同时保持诊断能力。",
      "method": "论文提出Structure-constrained Language-informed Diffusion Model (SLDM)，一种统一医学生成模型。首先，提取图像的结构先验信息以约束模型推理，确保增强过程中的结构一致性。其次，引入具有空间智能的语义监督策略，整合视觉感知和空间推理功能，促进模型实现精确增强。最后，应用减影血管增强模块，改善ICM区域的对比度至适宜观察区间。该方法基于扩散模型框架，旨在处理非配对图像。",
      "result": "通过视觉比较的定性分析和多个指标的定量结果，论文证明了SLDM在低剂量对比介质CT血管成像重建中的有效性。摘要未明确说明具体性能指标，如准确率或效率提升数据，但指出该方法在增强准确性和结构一致性方面优于现有基线方法。",
      "conclusion": "SLDM模型成功解决了非配对低剂量CT血管成像重建中的准确增强问题，通过结构约束和语义信息指导提高了图像质量。该研究具有重要学术价值，为医学图像生成提供了创新方法，并具有减少对比介质剂量的实际应用潜力。未来工作可包括进一步优化模型性能或扩展到其他医学成像任务。",
      "tags": [
        "Diffusion Model",
        "Low-dose Computed Tomography",
        "Image Enhancement",
        "Unpaired Learning",
        "Structural Consistency"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:00.105019Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20303",
    "title": "Physically Guided Visual Mass Estimation from a Single RGB Image",
    "authors": [
      "Sungjae Lee",
      "Junhan Jeong",
      "Yeonjoo Hong",
      "Kwang In Kim"
    ],
    "abstract": "Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20303.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20303",
    "published": "2026-01-28T06:53:36Z",
    "updated": "2026-01-28T06:53:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种物理引导的视觉质量估计框架，通过融合单目深度和视觉语言模型的信息，从单张RGB图像中准确预测物体质量，解决了体积和密度不可直接观察的挑战。",
      "motivation": "研究动机源于从单张RGB图像估计物体质量的困难，因为质量取决于几何体积和材料密度，这两者在视觉输入中不可直接观测，导致问题不适定。现有方法往往忽略物理约束，在实际应用如机器人抓取中性能受限。本工作旨在通过引入物理有意义的表示来限制解空间，提高预测的可靠性和实用性，推动相关领域的发展。",
      "method": "方法首先使用单目深度估计从单张RGB图像恢复物体中心的三维几何以计算体积，并利用视觉语言模型提取粗糙材料语义指导密度推理。接着，通过实例自适应门控机制融合几何、语义和外观表示，动态调整特征权重。然后，两个物理引导的潜在因子（体积和密度相关）通过独立回归头预测，仅用质量标签进行监督，实现端到端学习。",
      "result": "实验在image2mass和ABO-500数据集上进行，所提出方法在质量估计任务中表现优异，始终优于最先进方法，证明了物理引导框架的有效性。具体性能指标如准确率提升摘要未明确说明，但整体结果验证了融合几何和语义信息的关键作用。",
      "conclusion": "该研究的主要贡献是提出了物理结构化的视觉质量估计框架，有效解决了单图像预测的不适定问题。学术上，它推动了物理引导深度学习在计算机视觉中的应用；实际上，为机器人感知和工业检测提供了可靠工具。未来工作可扩展到多物体场景或优化实时性能。",
      "tags": [
        "Monocular Depth Estimation",
        "Vision-Language Model",
        "Instance-Adaptive Gating",
        "Physically Guided Learning",
        "Mass Estimation"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:31.549445Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20302",
    "title": "Bridging the Applicator Gap with Data-Doping:Dual-Domain Learning for Precise Bladder Segmentation in CT-Guided Brachytherapy",
    "authors": [
      "Suresh Das",
      "Siladittya Manna",
      "Sayantari Ghosh"
    ],
    "abstract": "Performance degradation due to covariate shift remains a major challenge for deep learning models in medical image segmentation. An open question is whether samples from a shifted distribution can effectively support learning when combined with limited target domain data. We investigate this problem in the context of bladder segmentation in CT guided gynecological brachytherapy, a critical task for accurate dose optimization and organ at risk sparing. While CT scans without brachytherapy applicators (no applicator: NA) are widely available, scans with applicators inserted (with applicator: WA) are scarce and exhibit substantial anatomical deformation and imaging artifacts, making automated segmentation particularly difficult.   We propose a dual domain learning strategy that integrates NA and WA CT data to improve robustness and generalizability under covariate shift. Using a curated assorted dataset, we show that NA data alone fail to capture the anatomical and artifact related characteristics of WA images. However, introducing a modest proportion of WA data into a predominantly NA training set leads to significant performance improvements. Through systematic experiments across axial, coronal, and sagittal planes using multiple deep learning architectures, we demonstrate that doping only 10 to 30 percent WA data achieves segmentation performance comparable to models trained exclusively on WA data.   The proposed approach attains Dice similarity coefficients of up to 0.94 and Intersection over Union scores of up to 0.92, indicating effective domain adaptation and improved clinical reliability. This study highlights the value of integrating anatomically similar but distribution shifted datasets to overcome data scarcity and enhance deep learning based segmentation for brachytherapy treatment planning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20302.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20302",
    "published": "2026-01-28T06:50:37Z",
    "updated": "2026-01-28T06:50:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "该研究提出了一种双域学习策略，通过数据掺杂少量有施用器的CT数据，有效提升了膀胱分割在协变量偏移下的鲁棒性和性能。",
      "motivation": "医疗图像分割中协变量偏移导致的性能下降是一个主要挑战，尤其在CT引导的妇科近距离放射治疗中。膀胱分割对于精确剂量优化和减少器官风险至关重要。现有方法依赖广泛可用的无施用器（NA）CT数据，但无法处理有施用器（WA）数据的稀缺性、解剖变形和成像伪影，导致自动化分割效果不佳。因此，研究旨在探索如何利用有限的目标域数据（WA）结合偏移分布数据（NA）来克服这一问题，填补现有方法的不足。",
      "method": "研究提出双域学习策略，整合无施用器（NA）和有施用器（WA）的CT数据进行训练。核心创新是数据掺杂（Data-Doping），即在以NA数据为主的训练集中引入少量WA数据（10-30%），以增强模型对WA图像特征的学习能力。通过使用多种深度学习架构（摘要未明确说明具体模型）在轴向、冠状和矢状平面上进行系统实验，验证方法的通用性和有效性。关键点在于通过双域数据混合，模拟实际医疗场景中的协变量偏移，提升分割模型的泛化性能。",
      "result": "实验结果显示，仅使用NA数据训练无法有效分割WA图像，但掺杂10-30%的WA数据后，模型性能显著提升，达到与全WA数据训练相当的水平。具体指标方面，Dice相似系数最高达到0.94，Intersection over Union（IoU）分数最高达到0.92。这些结果在多个图像平面和不同深度学习模型上保持一致，表明方法有效实现了域适应，并优于仅依赖单一数据域的基线方法，增强了临床应用的可靠性。",
      "conclusion": "该研究通过双域学习策略，成功整合了解剖相似但分布偏移的CT数据集，克服了数据稀缺问题，提高了膀胱分割的准确性和临床适用性。主要贡献在于提出了一种实用的域适应方法，展示了如何利用有限目标域数据增强深度学习模型的鲁棒性。学术价值在于为医疗图像分割中的协变量偏移问题提供了解决方案，实际应用潜力大，未来可扩展到其他医疗图像任务或数据集，局限性可能涉及特定数据类型的泛化能力（摘要未明确说明）。",
      "tags": [
        "Medical Image Segmentation",
        "Domain Adaptation",
        "Covariate Shift",
        "Data Doping",
        "Deep Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:27.827388Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20301",
    "title": "Towards Compact and Robust DNNs via Compression-aware Sharpness Minimization",
    "authors": [
      "Jialuo He",
      "Huangxun Chen"
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) has recently emerged as an effective technique for improving DNN robustness to input variations. However, its interplay with the compactness requirements of on-device DNN deployments remains less explored. Simply pruning a SAM-trained model can undermine robustness, since flatness in the continuous parameter space does not necessarily translate to robustness under the discrete structural changes induced by pruning. Conversely, applying SAM after pruning may be fundamentally constrained by architectural limitations imposed by an early, robustness-agnostic pruning pattern. To address this gap, we propose Compression-aware ShArpness Minimization (C-SAM), a framework that shifts sharpness-aware learning from parameter perturbations to mask perturbations. By explicitly perturbing pruning masks during training, C-SAM promotes a flatter loss landscape with respect to model structure, enabling the discovery of pruning patterns that simultaneously optimize model compactness and robustness to input variations. Extensive experiments on CelebA-HQ, Flowers-102, and CIFAR-10-C across ResNet-18, GoogLeNet, and MobileNet-V2 show that C-SAM consistently achieves higher certified robustness than strong baselines, with improvements of up to 42%, while maintaining task accuracy comparable to the corresponding unpruned models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20301.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20301",
    "published": "2026-01-28T06:49:32Z",
    "updated": "2026-01-28T06:49:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Compression-aware ShArpness Minimization (C-SAM)框架，通过扰动修剪掩码来同时优化深度神经网络的紧凑性和输入变化鲁棒性。",
      "motivation": "研究动机在于解决深度神经网络在设备部署中紧凑性和鲁棒性协同优化的挑战。现有Sharpness-Aware Minimization (SAM)方法能提高模型对输入变化的鲁棒性，但未充分考虑模型压缩需求：直接修剪SAM训练的模型可能破坏鲁棒性，因为参数空间的平坦性不一定适应修剪引入的离散结构变化；而先修剪后应用SAM则受限于早期、不关注鲁棒性的修剪模式，无法有效优化。因此，需要开发一种新方法来同步实现模型轻量化和鲁棒性增强。",
      "method": "研究提出Compression-aware ShArpness Minimization (C-SAM)框架，其核心创新在于将尖锐度感知学习从传统的参数扰动转向掩码扰动。在训练过程中，C-SAM显式扰动修剪掩码，探索模型结构层面的损失景观平坦性，从而发现能同时优化紧凑性和鲁棒性的修剪模式。该方法基于多个数据集（CelebA-HQ、Flowers-102、CIFAR-10-C）和模型架构（ResNet-18、GoogLeNet、MobileNet-V2）进行验证，通过结构感知的优化策略实现更高效的学习。",
      "result": "实验结果显示，C-SAM在CelebA-HQ、Flowers-102和CIFAR-10-C数据集上，使用ResNet-18、GoogLeNet和MobileNet-V2模型进行测试， consistently 实现比强基线方法更高的认证鲁棒性，改进幅度高达42%。同时，该方法保持任务准确性接近相应未修剪模型的水平，这表明C-SAM在显著提升鲁棒性的同时，未牺牲模型性能，有效平衡了紧凑性和鲁棒性需求。",
      "conclusion": "C-SAM框架通过掩码扰动实现了紧凑性和鲁棒性的协同优化，主要贡献在于将尖锐度感知学习扩展到模型结构层面。学术上，它深化了SAM在模型压缩领域的应用，推动了鲁棒深度神经网络的研究；实践上，为资源受限设备的部署提供了更鲁棒的轻量模型。未来工作可探索更广泛的压缩技术集成，或将该方法应用于其他任务以进一步验证其通用性。",
      "tags": [
        "Sharpness-Aware Minimization",
        "Model Compression",
        "Pruning",
        "Certified Robustness",
        "Loss Landscape Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:36.137528Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20300",
    "title": "MiLorE-SSL: Scaling Multilingual Capabilities in Self-Supervised Models without Forgetting",
    "authors": [
      "Jing Xu",
      "Minglin Wu",
      "Xueyuan Chen",
      "Xixin Wu",
      "Helen Meng"
    ],
    "abstract": "Self-supervised learning (SSL) has greatly advanced speech representation learning, but multilingual SSL models remain constrained to languages encountered during pretraining. Retraining from scratch to incorporate new languages is computationally expensive, while sequential training without migitation strategies often leads to catastrophic forgetting. To address this, we propose MiLorE-SSL, a lightweight framework that combines LoRA modules with a soft mixture-of-experts (MoE) mechanism for efficient continual multilingual training. LoRA provides efficient low-rank adaptation, while soft MoE promotes flexible expert sharing across languages, reducing cross-lingual interference. To further mitigate forgetting, we introduce limited replay data from existing languages, avoiding reliance on large historical corpora. Experiments on ML-SUPERB demonstrate that MiLorE-SSL achieves strong performance in new languages and improves the ability in existing ones with only 2.14% trainable parameters.",
    "categories": [
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20300.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20300",
    "published": "2026-01-28T06:48:52Z",
    "updated": "2026-01-28T06:48:52Z",
    "comment": "Accepted by ICASSP2026",
    "light_analysis": {
      "overview": "提出了一个结合LoRA和软混合专家机制的轻量级框架，用于高效扩展多语言自监督模型的能力，避免灾难性遗忘。",
      "motivation": "当前自监督学习模型在多语言扩展中存在挑战，因为模型通常仅限于预训练语言，加入新语言时从头训练计算成本高昂，而顺序学习容易导致灾难性遗忘，影响现有语言性能。现有方法如重新训练或简单的增量更新效率低下，限制了模型在实际多语言应用中的可扩展性和适应性。这个问题在语音处理等领域尤为重要，需要高效且持续的学习机制来应对语言多样性。",
      "method": "本研究提出MiLorE-SSL框架，通过结合低秩适应（LoRA）模块和软混合专家（MoE）机制来实现高效持续多语言训练。LoRA模块采用低秩矩阵更新，允许参数高效适应新语言，减少计算开销。软MoE机制促进不同语言间的专家网络共享和灵活组合，降低跨语言干扰。此外，引入有限重播数据从现有语言中缓解遗忘，无需依赖大规模历史语料库。实验在ML-SUPERB数据集上进行，针对自监督语音表示模型进行优化。",
      "result": "在ML-SUPERB基准测试中，MiLorE-SSL取得了显著效果：在新语言上展现出强劲性能，同时提升了现有语言的能力，仅使用2.14%的可训练参数。与基线方法相比，该框架在扩展多语言能力时避免了性能下降，有效减少了计算资源需求。尽管摘要未明确提供具体准确率数据，但基于实验结果推断，该方法在参数效率和性能平衡方面具有优势，适用于持续学习场景。",
      "conclusion": "本研究的核心贡献在于开发了一个轻量级框架，有效解决了多语言自监督模型扩展中的灾难性遗忘问题，推动了持续学习和多语言表示学习的研究。学术上，通过结合LoRA和软MoE创新了模型适应机制；实际中，该框架适用于语音识别等应用，支持低成本模型更新以适应新语言。潜在局限性可能包括对特定数据集的依赖或机制优化，未来工作可探索更广泛的基准测试或扩展到其他自监督任务。",
      "tags": [
        "Self-Supervised Learning",
        "LoRA",
        "Mixture of Experts",
        "Continual Learning",
        "Catastrophic Forgetting"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:48.521291Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20299",
    "title": "Truthfulness Despite Weak Supervision: Evaluating and Training LLMs Using Peer Prediction",
    "authors": [
      "Tianyi Alex Qiu",
      "Micah Carroll",
      "Cameron Allen"
    ],
    "abstract": "The evaluation and post-training of large language models (LLMs) rely on supervision, but strong supervision for difficult tasks is often unavailable, especially when evaluating frontier models. In such cases, models are demonstrated to exploit evaluations built on such imperfect supervision, leading to deceptive results. However, underutilized in LLM research, a wealth of mechanism design research focuses on game-theoretic incentive compatibility, i.e., eliciting honest and informative answers with weak supervision. Drawing from this literature, we introduce the peer prediction method for model evaluation and post-training. It rewards honest and informative answers over deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. We demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and empirical validation on models with up to 405B parameters. We show that training an 8B model with peer prediction-based reward recovers most of the drop in truthfulness due to prior malicious finetuning, even when the reward is produced by a 0.135B language model with no finetuning. On the evaluation front, in contrast to LLM-as-a-Judge which requires strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception is strengthened as the capability gap between the experts and participants widens, enabling reliable evaluation of strong models with weak supervision. In particular, LLM-as-a-Judge become worse than random guess when facing deceptive models 5-20x the judge's size, while peer prediction thrives when such gaps are large, including in cases with over 100x size difference.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20299.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20299",
    "published": "2026-01-28T06:47:46Z",
    "updated": "2026-01-28T06:47:46Z",
    "comment": "ICLR 2026",
    "light_analysis": {
      "overview": "论文引入peer prediction方法，用于在弱监督下评估和训练大型语言模型，提升真实性和抗欺骗能力。",
      "motivation": "大型语言模型（LLMs）的评估和后训练常依赖强监督，但困难任务缺乏此类监督，导致基于不完美监督的方法易受模型欺骗，产生误导性结果。peer prediction源自机制设计研究，利用游戏论的激励兼容性，能在弱监督下引导诚实回答，解决了现有方法在评估前沿模型时的不足。",
      "method": "论文采用peer prediction方法进行LLM评估和训练，该方法基于互预测度量，奖励模型提供诚实和信息丰富的回答，无需地面真值标签。核心创新点是将游戏论的激励兼容机制应用于LLMs，确保在弱监督下有效运作。实证中使用多种模型规模（如405B参数模型）验证方法，训练时结合基于peer prediction的奖励机制。",
      "result": "实验显示peer prediction方法有效：训练8B模型时，基于peer prediction的奖励（由0.135B模型生成）几乎完全恢复了恶意微调导致的真实性下滑。评估中，与LLM-as-a-Judge相比，peer prediction展示逆缩放性质——当专家与参与者能力差距扩大时，抗欺骗能力增强，例如在规模差距达100倍以上时仍能可靠评估强模型，而LLM-as-a-Judge在差距5-20倍时表现比随机猜测更差。",
      "conclusion": "研究证实peer prediction方法能在弱监督下可靠评估和训练LLMs，结合机制设计理论，有效抵抗欺骗，具有学术价值和实际应用潜力。未来工作可扩展至更多任务和模型规模，以进一步验证方法的通用性。",
      "tags": [
        "Peer Prediction",
        "Large Language Models",
        "Game Theory",
        "Weak Supervision",
        "Mechanism Design"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:03.917582Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20297",
    "title": "Artifact-Aware Evaluation for High-Quality Video Generation",
    "authors": [
      "Chen Zhu",
      "Jiashu Zhu",
      "Yanxun Li",
      "Meiqi Wu",
      "Bingze Song",
      "Chubin Chen",
      "Jiahong Wu",
      "Xiangxiang Chu",
      "Yangang Wang"
    ],
    "abstract": "With the rapid advancement of video generation techniques, evaluating and auditing generated videos has become increasingly crucial. Existing approaches typically offer coarse video quality scores, lacking detailed localization and categorization of specific artifacts. In this work, we introduce a comprehensive evaluation protocol focusing on three key aspects affecting human perception: Appearance, Motion, and Camera. We define these axes through a taxonomy of 10 prevalent artifact categories reflecting common generative failures observed in video generation. To enable robust artifact detection and categorization, we introduce GenVID, a large-scale dataset of 80k videos generated by various state-of-the-art video generation models, each carefully annotated for the defined artifact categories. Leveraging GenVID, we develop DVAR, a Dense Video Artifact Recognition framework for fine-grained identification and classification of generative artifacts. Extensive experiments show that our approach significantly improves artifact detection accuracy and enables effective filtering of low-quality content.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20297.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20297",
    "published": "2026-01-28T06:45:14Z",
    "updated": "2026-01-28T06:45:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种针对高质量视频生成的伪影感知评估方法，通过引入GenVID数据集和DVAR框架，实现了细粒度的伪影检测和分类。",
      "motivation": "随着视频生成技术的快速发展，评估生成视频的质量变得至关重要，但现有方法通常仅提供粗糙的质量分数，缺乏对特定伪影的详细定位和分类，这限制了评估精度。为了解决这一问题，本研究旨在开发一个全面的评估协议，以识别和分类常见的生成失败伪影，从而改进视频生成系统的性能。实际应用中，伪影如模糊或失真影响观看体验，因此需要更细致的评估工具来指导模型优化和内容筛选。",
      "method": "论文提出了一个基于三个关键方面（外观、运动和相机）的评估协议，通过10个常见伪影类别的分类法来定义这些轴，以反映视频生成中的常见失败。为了支持伪影检测，引入了GenVID数据集，包含80k个由多种先进视频生成模型生成的视频，每个视频都仔细标注了伪影类别。利用该数据集，开发了DVAR框架，一个密集视频伪影识别系统，用于细粒度地识别和分类生成视频中的伪影。方法的关键创新在于结合了详细的分类法和大规模标注数据。",
      "result": "实验表明，所提出的方法显著提高了伪影检测的准确性，并能够有效过滤低质量内容。尽管摘要未明确说明具体性能指标如准确率，但通过大量实验验证了该方法在识别和分类伪影方面的有效性，优于现有粗糙评估方法。这为视频生成评估提供了更细致的工具，有助于改进生成质量，但具体数据未在摘要中详细说明。",
      "conclusion": "本研究的主要贡献是提出了一种全面的视频生成评估协议，定义了伪影分类法，并引入了GenVID数据集和DVAR框架。学术上，这为视频生成领域提供了更细致的评估标准和方法，促进了模型优化；实际上，可用于过滤低质量内容，提升系统性能。未来工作可能包括扩展到更多伪影类别或应用于其他生成任务，但摘要未明确说明具体方向。",
      "tags": [
        "Video Generation Evaluation",
        "Artifact Classification",
        "Large-scale Dataset",
        "Dense Video Recognition",
        "Generative Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:27.437681Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20295",
    "title": "Cheap2Rich: A Multi-Fidelity Framework for Data Assimilation and System Identification of Multiscale Physics -- Rotating Detonation Engines",
    "authors": [
      "Yuxuan Bao",
      "Jan Zajac",
      "Megan Powers",
      "Venkat Raman",
      "J. Nathan Kutz"
    ],
    "abstract": "Bridging the sim2real gap between computationally inexpensive models and complex physical systems remains a central challenge in machine learning applications to engineering problems, particularly in multi-scale settings where reduced-order models typically capture only dominant dynamics. In this work, we present Cheap2Rich, a multi-scale data assimilation framework that reconstructs high-fidelity state spaces from sparse sensor histories by combining a fast low-fidelity prior with learned, interpretable discrepancy corrections. We demonstrate the performance on rotating detonation engines (RDEs), a challenging class of systems that couple detonation-front propagation with injector-driven unsteadiness, mixing, and stiff chemistry across disparate scales. Our approach successfully reconstructs high-fidelity RDE states from sparse measurements while isolating physically meaningful discrepancy dynamics associated with injector-driven effects. The results highlight a general multi-fidelity framework for data assimilation and system identification in complex multi-scale systems, enabling rapid design exploration and real-time monitoring and control while providing interpretable discrepancy dynamics. Code for this project is is available at: github.com/kro0l1k/Cheap2Rich.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20295.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20295",
    "published": "2026-01-28T06:35:22Z",
    "updated": "2026-01-28T06:35:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Cheap2Rich多保真度框架，通过结合低保真度先验和学习的可解释差异修正，用于多尺度物理系统的数据同化和系统识别。",
      "motivation": "研究旨在解决机器学习在工程应用中的仿真到现实差距问题，特别是在多尺度环境中，现有降阶模型通常只能捕获主导动力学，而无法准确处理如旋转爆震发动机中不同尺度间的复杂耦合现象。这一问题对于实现快速设计探索、实时监测和控制至关重要，因为准确的状态重建和系统识别是优化性能的关键基础，当前方法在处理稀疏传感器数据和解释复杂交互方面存在不足。",
      "method": "论文开发了Cheap2Rich多尺度数据同化框架，核心方法是将快速低保真度模型作为先验，通过机器学习学习一个可解释的差异修正模型，从而从稀疏传感器历史数据中重建高保真度系统状态空间。关键创新在于能够隔离物理上有意义的差异动力学，如在旋转爆震发动机中针对喷射器驱动效应进行分析，该方法还利用了具体应用场景如旋转爆震引擎来验证技术有效性。",
      "result": "在旋转爆震发动机的实验中，Cheap2Rich框架成功从稀疏测量中重建了高保真度状态，并识别出与喷射器驱动效应相关的物理差异动力学。尽管摘要未明确提供具体性能指标如准确率提升，但与低保真度模型相比，该方法显著改善了状态重建的保真度，并实现了可解释的分析，突显了在多尺度系统中数据同化的有效性。",
      "conclusion": "论文的主要贡献是提出了一个通用的多保真度框架，用于复杂多尺度系统的数据同化和系统识别，其学术价值在于融合低保真度先验和机器学习以提供更准确且可解释的方法，实际应用价值包括支持快速设计探索、实时监测和控制，增强了对系统动力学的理解。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Multi-Fidelity Framework",
        "Data Assimilation",
        "System Identification",
        "Rotating Detonation Engines",
        "Interpretable Discrepancy Corrections"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:29.031341Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20291",
    "title": "A Learning-based Framework for Spatial Impulse Response Compensation in 3D Photoacoustic Computed Tomography",
    "authors": [
      "Kaiyi Yang",
      "Seonyeong Park",
      "Gangwon Jeong",
      "Hsuan-Kai Huang",
      "Alexander A. Oraevsky",
      "Umberto Villa",
      "Mark A. Anastasio"
    ],
    "abstract": "Photoacoustic computed tomography (PACT) is a promising imaging modality that combines the advantages of optical contrast with ultrasound detection. Utilizing ultrasound transducers with larger surface areas can improve detection sensitivity. However, when computationally efficient analytic reconstruction methods that neglect the spatial impulse responses (SIRs) of the transducer are employed, the spatial resolution of the reconstructed images will be compromised. Although optimization-based reconstruction methods can explicitly account for SIR effects, their computational cost is generally high, particularly in three-dimensional (3D) applications. To address the need for accurate but rapid 3D PACT image reconstruction, this study presents a framework for establishing a learned SIR compensation method that operates in the data domain. The learned compensation method maps SIR-corrupted PACT measurement data to compensated data that would have been recorded by idealized point-like transducers. Subsequently, the compensated data can be used with a computationally efficient reconstruction method that neglects SIR effects. Two variants of the learned compensation model are investigated that employ a U-Net model and a specifically designed, physics-inspired model, referred to as Deconv-Net. A fast and analytical training data generation procedure is also a component of the presented framework. The framework is rigorously validated in virtual imaging studies, demonstrating resolution improvement and robustness to noise variations, object complexity, and sound speed heterogeneity. When applied to in-vivo breast imaging data, the learned compensation models revealed fine structures that had been obscured by SIR-induced artifacts. To our knowledge, this is the first demonstration of learned SIR compensation in 3D PACT imaging.",
    "categories": [
      "cs.LG",
      "eess.SP",
      "physics.med-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20291.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20291",
    "published": "2026-01-28T06:18:20Z",
    "updated": "2026-01-28T06:18:20Z",
    "comment": "Submitted to IEEE TMI",
    "light_analysis": {
      "overview": "本研究提出一种基于学习的框架，首次在三维光声计算机断层扫描中实现空间脉冲响应补偿，以提高图像重建质量和效率。",
      "motivation": "光声计算机断层扫描（PACT）结合光学对比和超声检测优势，使用大表面积换能器可提升灵敏度。然而，忽略换能器空间脉冲响应（SIRs）的高效解析重建方法会损害图像空间分辨率。虽然基于优化的重建方法能显式处理SIR效应，但计算成本高，尤其在三維应用中难以实用。因此，亟需开发一种快速且准确的三维PACT重建方法，以平衡计算效率与成像质量。",
      "method": "论文提出一个学习框架，在数据域中建立SIR补偿方法，通过映射SIR损坏的测量数据到理想点状换能器的补偿数据。框架研究了两种模型变体：基于U-Net的通用模型和物理启发的专门模型Deconv-Net。关键创新包括引入快速解析训练数据生成过程，以高效准备训练数据，确保补偿操作能与忽略SIR的高效重建方法无缝结合。",
      "result": "在虚拟成像研究中，框架被严格验证，显示出分辨率改进，并对噪声变化、对象复杂性和声速异质性具有鲁棒性。应用于体内乳房成像数据时，学习补偿模型成功揭示了被SIR诱导伪影掩盖的精细结构，证明了其实际有效性。摘要未明确说明具体性能指标如准确率提升，但通过对比强调了成像质量的显著改善。",
      "conclusion": "该研究首次展示了在三维PACT成像中基于学习的SIR补偿，为快速且准确的重建提供了创新解决方案。学术价值在于将深度学习与物理模型结合，推动了成像算法的发展；实际应用价值在于可能提升临床诊断的精度和效率。未来工作可包括模型优化、扩展到其他成像模态或处理更复杂的真实世界场景。",
      "tags": [
        "Photoacoustic Computed Tomography",
        "Spatial Impulse Response",
        "Learned Compensation",
        "U-Net",
        "Deconv-Net"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:39.032754Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20284",
    "title": "A Source-Free Approach for Domain Adaptation via Multiview Image Transformation and Latent Space Consistency",
    "authors": [
      "Debopom Sutradhar",
      "Md. Abdur Rahman",
      "Mohaimenul Azam Khan Raiaan",
      "Reem E. Mohamed",
      "Sami Azam"
    ],
    "abstract": "Domain adaptation (DA) addresses the challenge of transferring knowledge from a source domain to a target domain where image data distributions may differ. Existing DA methods often require access to source domain data, adversarial training, or complex pseudo-labeling techniques, which are computationally expensive. To address these challenges, this paper introduces a novel source-free domain adaptation method. It is the first approach to use multiview augmentation and latent space consistency techniques to learn domain-invariant features directly from the target domain. Our method eliminates the need for source-target alignment or pseudo-label refinement by learning transferable representations solely from the target domain by enforcing consistency between multiple augmented views in the latent space. Additionally, the method ensures consistency in the learned features by generating multiple augmented views of target domain data and minimizing the distance between their feature representations in the latent space. We also introduce a ConvNeXt-based encoder and design a loss function that combines classification and consistency objectives to drive effective adaptation directly from the target domain. The proposed model achieves an average classification accuracy of 90. 72\\%, 84\\%, and 97. 12\\% in Office-31, Office-Home and Office-Caltech datasets, respectively. Further evaluations confirm that our study improves existing methods by an average classification accuracy increment of +1.23\\%, +7.26\\%, and +1.77\\% on the respective datasets.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20284.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20284",
    "published": "2026-01-28T05:59:20Z",
    "updated": "2026-01-28T05:59:20Z",
    "comment": "Manuscript under review in IEEE Transactions on Image Processing",
    "light_analysis": {
      "overview": "论文提出了一种无源域适应方法，首次结合多视图增强和潜在空间一致性技术，直接从目标域学习域不变特征。",
      "motivation": "域适应技术旨在解决源域和目标域数据分布不同时的知识迁移挑战。现有方法通常需要访问源域数据、采用对抗训练或复杂伪标签技术，这些方法计算成本高昂且实现复杂。为了解决这些问题，本文开发了一种无需源域数据的域适应方法，以减少计算开销和简化流程，适用于源域数据不可获取或隐私敏感的实际应用场景。",
      "method": "该方法通过多视图增强生成目标域数据的多个增强视图，并在潜在空间中强制这些视图的特征表示一致性以学习域不变特征。采用基于ConvNeXt的编码器进行特征提取，并设计损失函数结合分类目标和一致性目标，驱动直接从目标域进行有效适应，无需源-目标对齐或伪标签细化。",
      "result": "在Office-31、Office-Home和Office-Caltech数据集上，模型平均分类准确率分别达到90.72%、84%和97.12%。相较于现有方法，准确率提升了+1.23%、+7.26%和+1.77%，显示出显著的性能改进，验证了方法在无源域适应任务中的有效性。",
      "conclusion": "摘要未明确说明结论，但可推断该研究为无源域适应提供了创新解决方案，通过多视图增强和潜在空间一致性学习域不变特征，减少对源域数据的依赖和计算复杂性，具有学术价值和实际应用潜力。未来工作可能包括扩展到更多数据集或改进一致性技术。",
      "tags": [
        "Domain Adaptation",
        "Source-Free Learning",
        "Multiview Augmentation",
        "Latent Space Consistency",
        "ConvNeXt-based Encoder"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:44.732786Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20282",
    "title": "Memory Retrieval in Transformers: Insights from The Encoding Specificity Principle",
    "authors": [
      "Viet Hung Dinh",
      "Ming Ding",
      "Youyang Qu",
      "Kanchana Thilakarathna"
    ],
    "abstract": "While explainable artificial intelligence (XAI) for large language models (LLMs) remains an evolving field with many unresolved questions, increasing regulatory pressures have spurred interest in its role in ensuring transparency, accountability, and privacy-preserving machine unlearning. Despite recent advances in XAI have provided some insights, the specific role of attention layers in transformer based LLMs remains underexplored. This study investigates the memory mechanisms instantiated by attention layers, drawing on prior research in psychology and computational psycholinguistics that links Transformer attention to cue based retrieval in human memory. In this view, queries encode the retrieval context, keys index candidate memory traces, attention weights quantify cue trace similarity, and values carry the encoded content, jointly enabling the construction of a context representation that precedes and facilitates memory retrieval. Guided by the Encoding Specificity Principle, we hypothesize that the cues used in the initial stage of retrieval are instantiated as keywords. We provide converging evidence for this keywords-as-cues hypothesis. In addition, we isolate neurons within attention layers whose activations selectively encode and facilitate the retrieval of context-defining keywords. Consequently, these keywords can be extracted from identified neurons and further contribute to downstream applications such as unlearning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20282.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20282",
    "published": "2026-01-28T05:58:09Z",
    "updated": "2026-01-28T05:58:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文基于编码特异性原则，提出Transformer注意力层中记忆检索机制的关键词作为线索假设，并通过神经元分析验证，促进下游应用如机器遗忘。",
      "motivation": "随着可解释人工智能（XAI）对大型语言模型（LLMs）的关注增加，监管压力要求确保模型的透明性、责任性和隐私保护，特别是机器遗忘。然而，注意力层在Transformer模型中的具体角色仍未被充分探索，现有方法未能深入揭示其内部记忆机制，导致XAI发展受限。因此，本研究旨在借鉴心理学理论，探索注意力层如何模拟人类记忆检索，以填补这一空白并为应用提供新见解。",
      "method": "研究借鉴心理学和计算心理语言学，将Transformer的注意力机制类比于人类记忆的线索检索过程：查询编码检索上下文，键索引候选记忆痕迹，注意力权重量化线索与痕迹的相似性，值携带编码内容。基于编码特异性原则，假设检索初期的线索被实例化为关键词。通过实验验证这一关键词作为线索的假设，并隔离注意力层中特定神经元，其激活选择性编码和促进上下文定义关键词的检索。技术细节包括分析神经元活动以提取关键词，但具体数据集或模型架构在摘要中未明确说明。",
      "result": "论文提供了融合证据支持关键词作为线索的假设，成功验证了注意力层中神经元对关键词的编码和检索作用。通过实验，展示了如何从识别的神经元中提取关键词，并将其应用于下游任务，如机器遗忘。性能指标如具体准确率提升或效率改进在摘要中未明确说明，与基线方法的对比也未详细描述，但结果表明确认了假设的合理性并实现了关键词的有效提取。",
      "conclusion": "本研究主要贡献在于揭示了Transformer注意力层中记忆检索机制的关键词假设，并通过神经元分析提供了实证支持。学术上，这深化了对LLMs内部工作方式的理解，为可解释人工智能提供了新视角。实际应用中，提取的关键词可促进如机器遗忘等任务的开发。局限性可能包括未覆盖所有注意力机制或模型类型，未来工作可扩展到更广泛的神经模型或探索其他认知原理。",
      "tags": [
        "Transformer",
        "Attention Mechanism",
        "Memory Retrieval",
        "Keyword Extraction",
        "Machine Unlearning"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:46.020009Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20280",
    "title": "The Forecast After the Forecast: A Post-Processing Shift in Time Series",
    "authors": [
      "Daojun Liang",
      "Qi Li",
      "Yinglong Wang",
      "Jing Chen",
      "Hu Zhang",
      "Xiaoxiao Cui",
      "Qizheng Wang",
      "Shuo Li"
    ],
    "abstract": "Time series forecasting has long been dominated by advances in model architecture, with recent progress driven by deep learning and hybrid statistical techniques. However, as forecasting models approach diminishing returns in accuracy, a critical yet underexplored opportunity emerges: the strategic use of post-processing. In this paper, we address the last-mile gap in time-series forecasting, which is to improve accuracy and uncertainty without retraining or modifying a deployed backbone. We propose $δ$-Adapter, a lightweight, architecture-agnostic way to boost deployed time series forecasters without retraining. $δ$-Adapter learns tiny, bounded modules at two interfaces: input nudging (soft edits to covariates) and output residual correction. We provide local descent guarantees, $O(δ)$ drift bounds, and compositional stability for combined adapters. Meanwhile, it can act as a feature selector by learning a sparse, horizon-aware mask over inputs to select important features, thereby improving interpretability. In addition, it can also be used as a distribution calibrator to measure uncertainty. Thus, we introduce a Quantile Calibrator and a Conformal Corrector that together deliver calibrated, personalized intervals with finite-sample coverage. Our experiments across diverse backbones and datasets show that $δ$-Adapter improves accuracy and calibration with negligible compute and no interface changes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20280.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20280",
    "published": "2026-01-28T05:55:04Z",
    "updated": "2026-01-28T05:55:04Z",
    "comment": "30 Pages",
    "light_analysis": {
      "overview": "提出$δ$-Adapter，一个轻量级后处理适配器，用于在不重新训练的情况下提升时间序列预测的准确性和不确定性估计。",
      "motivation": "时间序列预测领域长期侧重于模型架构改进，但随着技术发展，准确性提升面临收益递减瓶颈。现有方法多需重新训练或复杂修改，导致部署成本高且不灵活。研究旨在解决“最后一英里”问题，即在不改变已部署主干模型的前提下，通过战略后处理提高预测精度和不确定性量化，以应对实际应用中对效率、可维护性和可靠性的需求。",
      "method": "论文提出$δ$-Adapter，一种架构无关的轻量级后处理技术。核心方法包括在输入接口进行微调（如对协变量进行软编辑）和在输出接口应用残差校正，学习有界模块以确保稳定性。关键创新涉及理论保障：局部下降保证、$O(δ)$漂移界限和组合适配器的稳定性。此外，通过稀疏、时域感知掩码实现特征选择以提高可解释性，并结合分位数校准器和符合性校正器进行不确定性估计，共同提供校准的预测区间。",
      "result": "实验在多种主干模型和数据集上进行，结果显示$δ$-Adapter能显著提高预测准确性和校准性能。与基线方法相比，该方法在无需重新训练和接口修改的情况下实现改进，计算开销可忽略。摘要未明确说明具体准确率提升数值，但强调了整体性能增强和不确定性估计的有效性，验证了后处理策略在实际部署中的实用价值。",
      "conclusion": "该研究的主要贡献是开发了$δ$-Adapter，填补了时间序列预测中后处理研究的空白，通过轻量级方案提升精度和不确定性量化。学术价值在于推动后处理技术在机器学习中的应用，提供理论框架；实际应用价值在于为部署系统提供低成本优化，增强可解释性和可靠性。局限性可能包括特定场景的适用性，未来工作可探索扩展到其他预测任务或进一步理论深化。",
      "tags": [
        "Time Series Forecasting",
        "Post-Processing",
        "Uncertainty Calibration",
        "Feature Selection",
        "$δ$-Adapter"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:16.334019Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20279",
    "title": "Hallucination Begins Where Saliency Drops",
    "authors": [
      "Xiaofeng Zhang",
      "Yuanchao Zhu",
      "Chaochen Gu",
      "Xiaosong Yuan",
      "Qiyan Zhao",
      "Jiawei Cao",
      "Feilong Tang",
      "Sinan Fan",
      "Yaomin Shen",
      "Chen Shen",
      "Hao Tang"
    ],
    "abstract": "Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20279.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20279",
    "published": "2026-01-28T05:50:52Z",
    "updated": "2026-01-28T05:50:52Z",
    "comment": "Accepted in ICLR 2026",
    "light_analysis": {
      "overview": "提出LVLMs-Saliency框架，融合注意力与梯度信号检测幻觉，并通过双机制推理框架显著减少幻觉，增强模型可靠性。",
      "motivation": "研究动机源于现有大型视觉-语言模型中幻觉检测方法的局限性。当前方法主要依赖前向传递的注意力模式，忽视梯度信号，后者能揭示令牌在网络中的影响传播，导致难以可靠区分幻觉和基于事实的输出。这一问题对模型可靠性和任务性能至关重要，因为幻觉会降低输出准确性，现有方法未能全面捕捉令牌的动态影响。",
      "method": "论文提出LVLMs-Saliency框架，通过融合注意力权重和输入梯度来量化每个输出令牌的视觉基础强度。关键创新是发现幻觉常发生在先前输出令牌显著性低时，指示上下文记忆中断。基于此，设计双机制推理时间框架：Saliency-Guided Rejection Sampling（SGRS）在自回归解码中动态过滤低显著性候选令牌，防止破坏连贯性；Local Coherence Reinforcement（LocoRE）作为轻量级插件模块，加强当前令牌到最近前驱的注意力，以对抗上下文遗忘。",
      "result": "在多个大型视觉-语言模型上进行广泛实验，结果表明提出的方法显著降低了幻觉率。摘要未明确说明具体数据，但报告显示模型在减少幻觉的同时保持了生成文本的流畅性和在相关任务上的性能。与基线方法相比，该方法提供了更可靠和可解释的解决方案，增强了模型的实际应用能力。",
      "conclusion": "研究的主要贡献是开发了LVLMs-Saliency框架和双机制推理框架，有效减少幻觉并提升模型可靠性。学术上，它引入了梯度感知的诊断方法，为幻觉检测提供了新视角；应用上，适用于视觉-语言任务，提高准确性。潜在局限或未来工作方向在摘要中未明确说明，但可推断可能扩展到其他模型或更复杂场景。",
      "tags": [
        "Large Vision-Language Models",
        "Gradient Analysis",
        "Attention Mechanisms",
        "Hallucination Mitigation",
        "Contextual Coherence"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:05.986778Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20276",
    "title": "Beyond the Needle's Illusion: Decoupled Evaluation of Evidence Access and Use under Semantic Interference at 326M-Token Scale",
    "authors": [
      "Tianwei Lin",
      "Zuyi Zhou",
      "Xinda Zhao",
      "Chenke Wang",
      "Xiaohong Li",
      "Yu Chen",
      "Chuanrui Hu",
      "Jian Pei",
      "Yafeng Deng"
    ],
    "abstract": "Long-context LLM agents must access the right evidence from large environments and use it faithfully. However, the popular Needle-in-a-Haystack (NIAH) evaluation mostly measures benign span localization. The needle is near-unique, and the haystack is largely irrelevant. We introduce EverMemBench-S (EMB-S), an adversarial NIAH-style benchmark built on a 326M-token MemoryBank. While the full MemoryBank spans 326M tokens for retrieval-based (RAG) evaluation, we evaluate native long-context models only at scales that fit within each model's context window (up to 1M tokens in this work) to ensure a fair comparison. EMB-S pairs queries with collision-tested near-miss hard negatives and gold evidence sets spanning one or more documents, validated via human screening and LLM verification. We also propose a decoupled diagnostic protocol that reports evidence access (document-ID localization) separately from end-to-end QA quality under full-context prompting. This enables consistent diagnosis for both native long-context prompting and retrieval pipelines. Across a reference-corpus ladder from domain-isolated 64K contexts to a globally shared 326M-token environment, we observe a clear reality gap. Systems that saturate benign NIAH degrade sharply in evidence access under semantic interference. These results indicate that semantic discrimination, not context length alone, is the dominant bottleneck for long-context memory at scale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20276.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20276",
    "published": "2026-01-28T05:44:00Z",
    "updated": "2026-01-28T05:44:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出EMB-S基准和解耦诊断协议，用于评估长上下文LLM在语义干扰下的证据访问与使用，揭示语义歧视是关键瓶颈。",
      "motivation": "研究动机是解决长上下文LLM代理在大型环境中准确访问和使用证据的评估不足问题。现有Needle-in-a-Haystack评估仅测量良性跨度定位，忽略了语义干扰下的挑战，而准确证据访问对问答系统等应用至关重要，因此需要更全面的评估方法来反映真实场景复杂性。",
      "method": "研究方法包括构建EverMemBench-S基准，基于326M-token MemoryBank，包含碰撞测试的近错过硬负样本和人类验证的黄金证据集。创新点在于采用对抗性NIAH风格，并提出解耦诊断协议，分别评估证据访问（文档-ID定位）和端到端QA质量。评估在模型上下文窗口内进行（最多1M tokens），使用人类筛选和LLM验证以确保数据可靠性。",
      "result": "主要实验结果显示，在从64K上下文到326M-token环境的参考语料阶梯中，系统在良性NIAH评估中表现良好，但在语义干扰下证据访问性能急剧下降，表明语义歧视是长上下文内存的主要瓶颈，而非单纯上下文长度。与基线NIAH对比，揭示了更现实的性能差距，但摘要未提供具体数值。",
      "conclusion": "论文贡献在于提出了EMB-S基准和解耦评估协议，为长上下文模型的证据访问和使用提供了更全面的诊断工具，学术价值是指出语义干扰是评估关键因素，实际应用有助于改进检索增强生成系统。未来工作方向摘要未明确说明，但可能包括扩展基准或优化模型性能。",
      "tags": [
        "Long-context LLM",
        "Evidence Access",
        "Semantic Interference",
        "Benchmark Evaluation",
        "Retrieval-Augmented Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:22.662251Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20275",
    "title": "RusLICA: A Russian-Language Platform for Automated Linguistic Inquiry and Category Analysis",
    "authors": [
      "Elina Sigdel",
      "Anastasia Panfilova"
    ],
    "abstract": "Defining psycholinguistic characteristics in written texts is a task gaining increasing attention from researchers. One of the most widely used tools in the current field is Linguistic Inquiry and Word Count (LIWC) that originally was developed to analyze English texts and translated into multiple languages. Our approach offers the adaptation of LIWC methodology for the Russian language, considering its grammatical and cultural specificities. The suggested approach comprises 96 categories, integrating syntactic, morphological, lexical, general statistical features, and results of predictions obtained using pre-trained language models (LMs) for text analysis. Rather than applying direct translation to existing thesauri, we built the dictionary specifically for the Russian language based on the content from several lexicographic resources, semantic dictionaries and corpora. The paper describes the process of mapping lemmas to 42 psycholinguistic categories and the implementation of the analyzer as part of RusLICA web service.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20275.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20275",
    "published": "2026-01-28T05:43:40Z",
    "updated": "2026-01-28T05:43:40Z",
    "comment": "The link to the platform: https://ruslica.ipran.ru",
    "light_analysis": {
      "overview": "本文开发了RusLICA平台，通过专门构建的俄语词典和整合预训练语言模型，将LIWC方法适应到俄语中以进行自动化心理语言学分析。",
      "motivation": "心理语言学特征分析在文本研究中日益重要，旨在从书面文本中提取心理和社会语言学属性。现有工具如LIWC主要针对英语设计，尽管有翻译版本，但直接翻译可能忽略俄语的语法和文化特异性，导致分析不准确。因此，本研究旨在解决这一不足，开发专门适应俄语的自动化分析工具，以提供更精确的分析结果，满足跨语言研究的需要。该问题的重要性在于推动语言特定分析技术的发展，避免跨语言偏差。",
      "method": "论文提出了一种适应LIWC方法到俄语的新方案，核心是构建专门基于俄语资源的字典，而非直接翻译现有词库。该方法整合了96个类别，包括句法、形态、词汇特征、一般统计指标，并使用预训练语言模型进行文本预测，以增强分析深度。关键创新在于基于多个词汇资源、语义词典和语料库构建字典，确保适应俄语的语法和文化特性。实现为RusLICA网络服务，支持自动化处理，具体包括将词元映射到42个心理语言学类别的过程。",
      "result": "摘要中未明确说明具体的实验结果或性能指标。论文主要关注方法的描述和平台实现，未提供与基线方法的对比数据，如准确率提升或效率改进。因此，结果部分需要参考论文全文以获取详细信息，可能包括平台的可用性测试或分类效果评估。",
      "conclusion": "本研究开发了RusLICA平台，为俄语心理语言学分析提供了专门工具，通过适应LIWC方法并整合预训练语言模型，提高了分析的准确性。主要贡献在于解决了俄语特定分析的需求，具有学术价值，如促进跨语言研究，以及实际应用价值，如自动化文本分析。未来工作可能包括优化模型性能、扩展到其他语言或增强功能。",
      "tags": [
        "Linguistic Inquiry and Word Count",
        "Pre-trained Language Models",
        "Russian Language Processing",
        "Text Analysis",
        "Morphological Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:30.727429Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20268",
    "title": "Robust SDE Parameter Estimation Under Missing Time Information Setting",
    "authors": [
      "Long Van Tran",
      "Truyen Tran",
      "Phuoc Nguyen"
    ],
    "abstract": "Recent advances in stochastic differential equations (SDEs) have enabled robust modeling of real-world dynamical processes across diverse domains, such as finance, health, and systems biology. However, parameter estimation for SDEs typically relies on accurately timestamped observational sequences. When temporal ordering information is corrupted, missing, or deliberately hidden (e.g., for privacy), existing estimation methods often fail. In this paper, we investigate the conditions under which temporal order can be recovered and introduce a novel framework that simultaneously reconstructs temporal information and estimates SDE parameters. Our approach exploits asymmetries between forward and backward processes, deriving a score-matching criterion to infer the correct temporal order between pairs of observations. We then recover the total order via a sorting procedure and estimate SDE parameters from the reconstructed sequence using maximum likelihood. Finally, we conduct extensive experiments on synthetic and real-world datasets to demonstrate the effectiveness of our method, extending parameter estimation to settings with missing temporal order and broadening applicability in sensitive domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20268.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20268",
    "published": "2026-01-28T05:29:39Z",
    "updated": "2026-01-28T05:29:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个利用时间顺序不对称性，通过得分匹配和排序同时恢复时间信息和估计SDE参数的新框架。",
      "motivation": "随机微分方程（SDE）参数估计广泛应用于金融、健康和系统生物学等领域，但传统方法依赖于准确的时间戳观测序列。当时间顺序信息因数据损坏、缺失或隐私保护而无法获取时，现有估计方法往往失效。这一问题在实际应用中尤为关键，例如在敏感数据处理或非理想观测条件下，缺乏鲁棒的方法会限制SDE模型的适用性。因此，研究在缺失时间信息下的参数估计具有重要理论和实践意义。",
      "method": "本研究提出一种新框架，通过利用向前和向后过程的不对称性，推导得分匹配准则来推断观测对之间的正确时间顺序。核心创新在于设计了一种排序程序，基于该准则恢复完整的时间顺序，进而使用最大似然估计方法从重建的序列中计算SDE参数。该方法无需预先时间戳，实现了时间信息恢复与参数估计的同步处理，适用于时间信息缺失或损坏的场景。",
      "result": "论文在合成数据集和真实世界数据集上进行了广泛实验，以验证方法的有效性。实验结果展示了该方法能够在时间顺序缺失的情况下成功恢复时间信息并估计SDE参数，扩展了参数估计的应用范围至隐私敏感等领域。摘要未明确说明具体性能指标（如准确率提升或效率改进），也未提供与基线方法的详细对比数据，但强调实验证实了方法的鲁棒性和适用性。",
      "conclusion": "本研究的主要贡献是开发了一种在时间信息缺失情况下鲁棒估计随机微分方程参数的新框架，通过同时恢复时间顺序和估计参数，解决了现有方法的局限性。该研究拓宽了SDE模型在敏感领域和数据损坏场景的应用，具有重要的学术价值和实际意义。未来工作可进一步探索算法效率优化或处理更复杂的时间缺失模式，以提升方法的普适性。",
      "tags": [
        "Stochastic Differential Equations",
        "Score Matching",
        "Maximum Likelihood Estimation",
        "Temporal Order Recovery",
        "Parameter Estimation"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:31.310679Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20260",
    "title": "Reversible Efficient Diffusion for Image Fusion",
    "authors": [
      "Xingxin Xu",
      "Bing Cao",
      "DongDong Li",
      "Qinghua Hu",
      "Pengfei Zhu"
    ],
    "abstract": "Multi-modal image fusion aims to consolidate complementary information from diverse source images into a unified representation. The fused image is expected to preserve fine details and maintain high visual fidelity. While diffusion models have demonstrated impressive generative capabilities in image generation, they often suffer from detail loss when applied to image fusion tasks. This issue arises from the accumulation of noise errors inherent in the Markov process, leading to inconsistency and degradation in the fused results. However, incorporating explicit supervision into end-to-end training of diffusion-based image fusion introduces challenges related to computational efficiency. To address these limitations, we propose the Reversible Efficient Diffusion (RED) model - an explicitly supervised training framework that inherits the powerful generative capability of diffusion models while avoiding the distribution estimation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20260.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20260",
    "published": "2026-01-28T05:14:55Z",
    "updated": "2026-01-28T05:14:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了可逆高效扩散（RED）模型，一个显式监督的训练框架，用于多模态图像融合，结合了扩散模型的生成能力并避免了分布估计，提升了融合效率和质量。",
      "motivation": "多模态图像融合的目标是整合不同源图像的互补信息，生成保留精细细节和高视觉保真度的融合图像。然而，扩散模型在图像生成中虽表现优异，但在融合任务中常因马尔可夫过程固有噪声误差的累积，导致细节损失和结果退化。现有方法在端到端训练中引入显式监督时，面临计算效率低下和训练复杂度高的挑战，限制了实际应用。因此，研究旨在开发一种既能保持细节又能高效训练的新方法，以解决这些不足并提高融合效果。",
      "method": "论文提出Reversible Efficient Diffusion（RED）模型，这是一种显式监督的训练框架。关键创新在于它继承了扩散模型的强大生成能力，同时通过避免分布估计来减少噪声误差的累积，从而提高训练效率。该方法设计了一种高效的训练机制，但摘要未明确说明具体使用的数据集、模型架构（如神经网络结构）或优化策略等技术细节，需要进一步查阅全文以了解完整实现。",
      "result": "摘要中未提供具体的实验结果或性能指标，如准确率提升或效率改进数据。基于模型描述，可以合理推断RED模型可能通过减少噪声误差和优化训练流程，改善了图像融合的细节保留和计算效率，但摘要未明确说明与基线方法（如传统扩散模型或其他融合技术）的对比情况，因此这些效果和具体数据在摘要中未提及。",
      "conclusion": "RED模型的主要贡献是提出了一种结合显式监督的扩散模型训练框架，用于多模态图像融合，旨在解决现有方法在细节损失和计算效率方面的不足。这项研究在学术上扩展了扩散模型在图像处理领域的应用，在实际应用中有潜力提升融合图像的质量和效率。未来工作方向可能包括进一步优化模型架构、扩展到其他类型的融合任务，或验证在真实场景中的性能表现和局限性。",
      "tags": [
        "Diffusion Models",
        "Image Fusion",
        "Explicit Supervision",
        "Markov Process",
        "Reversible Efficient Diffusion"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:36.911735Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20257",
    "title": "C2:Cross learning module enhanced decision transformer with Constraint-aware loss for auto-bidding",
    "authors": [
      "Jinren Ding",
      "Xuejian Xu",
      "Shen Jiang",
      "Zhitong Hao",
      "Jinhui Yang",
      "Peng Jiang"
    ],
    "abstract": "Decision Transformer (DT) shows promise for generative auto-bidding by capturing temporal dependencies, but suffers from two critical limitations: insufficient cross-correlation modeling among state, action, and return-to-go (RTG) sequences, and indiscriminate learning of optimal/suboptimal behaviors. To address these, we propose C2, a novel framework enhancing DT with two core innovations: (1) a Cross Learning Block (CLB) via cross-attention to strengthen inter-sequence correlation modeling; (2) a Constraint-aware Loss (CL) incorporating budget and Cost-Per-Acquisition (CPA) constraints for selective learning of optimal trajectories. Extensive offline evaluations on the AuctionNet dataset demonstrate consistent performance gains (up to 3.23\\% over state-of-the-art GAVE) across diverse budget settings; ablation studies verify the complementary synergy of CLB and CL, confirming C2's superiority in auto-bidding. The code for reproducing our results is available at: https://github.com/Dingjinren/C2.",
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20257.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20257",
    "published": "2026-01-28T05:08:02Z",
    "updated": "2026-01-28T05:08:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "C2框架通过交叉学习块和约束感知损失增强决策变换器，提升了自动竞价的性能。",
      "motivation": "研究动机源于决策变换器在自动竞价应用中的两个关键局限：状态、动作和返回序列之间的交叉相关性建模不足，以及对最优和次优行为学习不区分。自动竞价在在线广告中至关重要，现有方法如决策变换器无法有效建模序列间关系和整合约束条件，导致性能受限。这些问题限制了广告预算分配的优化效果，因此研究旨在提出改进方案以提升效率和回报。",
      "method": "研究方法提出C2框架，基于决策变换器，并引入两个核心创新：交叉学习块使用交叉注意力机制强化状态、动作和返回序列间的相关性建模；约束感知损失结合预算和成本每次获取约束，选择性地学习符合约束的最优轨迹。在AuctionNet数据集上进行离线训练，验证框架的有效性，但摘要未明确说明具体模型架构的详细配置。",
      "result": "实验结果显示，C2在AuctionNet数据集上的离线评估中，相比当前最佳方法GAVE，性能提升最高达3.23%，并在不同预算设置下表现一致。消融研究证实了交叉学习块和约束感知损失各自贡献及其协同作用，证明C2在自动竞价任务中的优越性。摘要未明确说明具体性能指标的基准值。",
      "conclusion": "C2框架成功解决了决策变换器的局限性，通过技术创新提升自动竞价性能，贡献了新方法以增强序列建模和约束感知学习。研究具有学术价值，为强化学习和序列模型提供新思路；实际应用价值在于优化广告竞价策略。未来工作可能包括在线评估或扩展到其他领域，但摘要未明确说明具体局限性。",
      "tags": [
        "Decision Transformer",
        "Cross-Attention",
        "Constraint-aware Loss",
        "Auto-bidding",
        "AuctionNet dataset"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:52.038869Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20256",
    "title": "SoftHateBench: Evaluating Moderation Models Against Reasoning-Driven, Policy-Compliant Hostility",
    "authors": [
      "Xuanyu Su",
      "Diana Inkpen",
      "Nathalie Japkowicz"
    ],
    "abstract": "Online hate on social media ranges from overt slurs and threats (\\emph{hard hate speech}) to \\emph{soft hate speech}: discourse that appears reasonable on the surface but uses framing and value-based arguments to steer audiences toward blaming or excluding a target group. We hypothesize that current moderation systems, largely optimized for surface toxicity cues, are not robust to this reasoning-driven hostility, yet existing benchmarks do not measure this gap systematically. We introduce \\textbf{\\textsc{SoftHateBench}}, a generative benchmark that produces soft-hate variants while preserving the underlying hostile standpoint. To generate soft hate, we integrate the \\emph{Argumentum Model of Topics} (AMT) and \\emph{Relevance Theory} (RT) in a unified framework: AMT provides the backbone argument structure for rewriting an explicit hateful standpoint into a seemingly neutral discussion while preserving the stance, and RT guides generation to keep the AMT chain logically coherent. The benchmark spans \\textbf{7} sociocultural domains and \\textbf{28} target groups, comprising \\textbf{4,745} soft-hate instances. Evaluations across encoder-based detectors, general-purpose LLMs, and safety models show a consistent drop from hard to soft tiers: systems that detect explicit hostility often fail when the same stance is conveyed through subtle, reasoning-based language. \\textcolor{red}{\\textbf{Disclaimer.} Contains offensive examples used solely for research.}",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20256.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20256",
    "published": "2026-01-28T05:04:18Z",
    "updated": "2026-01-28T05:04:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出 SoftHateBench 基准，通过整合 AMT 和 RT 框架生成软仇恨言论，评估内容审核模型对推理驱动敌意的检测能力。",
      "motivation": "在线仇恨言论包括公开的硬仇恨和表面上合理的软仇恨。当前内容审核系统主要基于表面毒性线索优化，对推理驱动的软仇恨言论不鲁棒。软仇恨使用框架和基于价值的论点，引导受众责备或排除目标群体，常符合政策但实际有害。现有基准缺乏对这种差距的系统评估，导致审核系统可能遗漏此类恶意内容，影响在线平台的安全和用户体验，因此需要针对此问题进行深入研究。",
      "method": "论文提出 SoftHateBench，一个生成基准用于评估审核模型。方法整合 Argumentum Model of Topics (AMT) 和 Relevance Theory (RT)：AMT 构建论证结构，将硬仇恨言论重写为软仇恨变体，保留原立场的敌意；RT 确保生成文本的逻辑连贯性。基准覆盖 7 个社会文化领域和 28 个目标群体，包含 4,745 个软仇恨实例，系统性地产生推理驱动、政策合规的敌意内容，以测试模型在细微差别下的表现。",
      "result": "评估包括基于编码器的检测器、通用大型语言模型（LLMs）和专门的安全模型。结果显示，从硬仇恨到软仇恨层级，检测性能一致下降。系统能有效识别公开的侮辱和威胁，但当相同敌意立场通过看似合理、推理驱动的语言表达时，检测失败。这突显了当前模型对表面毒性敏感，但对深层推理敌意鲁棒性不足的问题，具体数值摘要未明确说明。",
      "conclusion": "论文的主要贡献是引入了 SoftHateBench 基准，系统评估内容审核模型对软仇恨言论的鲁棒性。意义在于揭示了当前系统在检测推理驱动敌意时的不足，强调需要超越表面毒性的更先进审核技术。这为未来研究提供了工具，促进开发更有效的仇恨言论检测方法，以应对日益复杂的在线恶意内容，并可能扩展基准或探索新检测算法。",
      "tags": [
        "Hate Speech Detection",
        "Benchmark Generation",
        "Argumentum Model of Topics (AMT)",
        "Relevance Theory (RT)",
        "Large Language Models (LLMs)"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:01.437699Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20255",
    "title": "HE-SNR: Uncovering Latent Logic via Entropy for Guiding Mid-Training on SWE-BENCH",
    "authors": [
      "Yueyang Wang",
      "Jiawei Fu",
      "Baolong Bi",
      "Xili Wang",
      "Xiaoqing Liu"
    ],
    "abstract": "SWE-bench has emerged as the premier benchmark for evaluating Large Language Models on complex software engineering tasks. While these capabilities are fundamentally acquired during the mid-training phase and subsequently elicited during Supervised Fine-Tuning (SFT), there remains a critical deficit in metrics capable of guiding mid-training effectively. Standard metrics such as Perplexity (PPL) are compromised by the \"Long-Context Tax\" and exhibit weak correlation with downstream SWE performance. In this paper, we bridge this gap by first introducing a rigorous data filtering strategy. Crucially, we propose the Entropy Compression Hypothesis, redefining intelligence not by scalar Top-1 compression, but by the capacity to structure uncertainty into Entropy-Compressed States of low orders (\"reasonable hesitation\"). Grounded in this fine-grained entropy analysis, we formulate a novel metric, HE-SNR (High-Entropy Signal-to-Noise Ratio). Validated on industrial-scale Mixture-of-Experts (MoE) models across varying context windows (32K/128K), our approach demonstrates superior robustness and predictive power. This work provides both the theoretical foundation and practical tools for optimizing the latent potential of LLMs in complex engineering domains.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20255.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20255",
    "published": "2026-01-28T05:03:24Z",
    "updated": "2026-01-28T05:03:24Z",
    "comment": "21 pages, 15 figures",
    "light_analysis": {
      "overview": "本文提出HE-SNR指标，基于熵压缩假设，用于有效指导大型语言模型的中训练，优化其在软件工程任务上的性能。",
      "motivation": "该研究旨在解决大型语言模型在中训练阶段缺乏有效指导指标的问题。SWE-bench作为软件工程任务的主要基准，模型能力在中训练中获得，但现有标准指标如困惑度(PPL)因'长上下文税'而受影响，与下游SWE性能相关性弱，导致难以优化模型潜力。因此，开发更可靠的指标至关重要，以提升模型在复杂工程领域的实际应用效果。摘要未明确说明具体案例，但强调了指标不足的普遍性。",
      "method": "论文首先引入严格的数据过滤策略来优化训练数据。核心创新是提出熵压缩假设，重新定义智能为结构化不确定性为低阶熵压缩状态的能力，而非标量Top-1压缩。基于此，设计了新指标HE-SNR（高熵信噪比），结合精细熵分析。方法在工业规模的Mixture-of-Experts (MoE)模型上验证，考虑不同上下文窗口（32K和128K），以评估指标的鲁棒性和预测能力，确保其在实际场景中的适用性。",
      "result": "实验结果表明，HE-SNR指标在MoE模型上展现出优越的鲁棒性和预测能力，特别是在不同上下文窗口（32K/128K）下。与基线指标如困惑度相比，HE-SNR更有效地关联下游SWE性能，避免了'长上下文税'的负面影响。摘要未提供具体数值数据，但强调了该方法在工业规模验证中的成功，表明其为中训练提供了更准确的指导工具。",
      "conclusion": "该研究的主要贡献是提供了理论基础和实践工具，通过HE-SNR指标优化大型语言模型在复杂工程领域的潜在能力。熵压缩假设为智能建模提供了新视角，HE-SNR则解决了现有指标的不足。其学术价值在于推进了指标设计理论，实际应用价值在于提升模型在软件工程任务中的性能。摘要未明确说明局限性和未来工作，但可推断未来可能涉及更广泛的任务验证和指标扩展。",
      "tags": [
        "Large Language Models",
        "Entropy Analysis",
        "Mixture-of-Experts",
        "Software Engineering Benchmark",
        "Signal-to-Noise Ratio"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:52.131385Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20253",
    "title": "Automated Benchmark Generation from Domain Guidelines Informed by Bloom's Taxonomy",
    "authors": [
      "Si Chen",
      "Le Huy Khiem",
      "Annalisa Szymanski",
      "Ronald Metoyer",
      "Ting Hua",
      "Nitesh V. Chawla"
    ],
    "abstract": "Open-ended question answering (QA) evaluates a model's ability to perform contextualized reasoning beyond factual recall. This challenge is especially acute in practice-based domains, where knowledge is procedural and grounded in professional judgment, while most existing LLM benchmarks depend on pre-existing human exam datasets that are often unavailable in such settings. We introduce a framework for automated benchmark generation from expert-authored guidelines informed by Bloom's Taxonomy. It converts expert practices into implicit violation-based scenarios and expands them into auto-graded multiple-choice questions (MCQs) and multi-turn dialogues across four cognitive levels, enabling deterministic, reproducible, and scalable evaluation. Applied to three applied domains: teaching, dietetics, and caregiving, we find differences between model and human-like reasoning: LLMs sometimes perform relatively better on higher-order reasoning (Analyze) but fail more frequently on lower-level items (Remember). We produce large-scale, psychometrically informed benchmarks that surface these non-intuitive model behaviors and enable evaluation of contextualized reasoning in real-world settings.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20253.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20253",
    "published": "2026-01-28T05:01:11Z",
    "updated": "2026-01-28T05:01:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个基于布鲁姆分类法和专家指南的自动化基准生成框架，用于评估大语言模型在实践领域的上下文推理能力。",
      "motivation": "开放性问题回答要求模型具备上下文推理能力，而非简单事实回忆，在实践领域如教学、营养学和护理中，知识是程序性和基于专业判断的。现有大语言模型基准通常依赖人类考试数据集，这些数据在实践环境中难以获取，导致评估不足。本研究旨在解决自动化生成基准的问题，以有效评估模型在这些领域的推理性能，弥补现有方法的局限性。",
      "method": "论文提出一个自动化基准生成框架，基于布鲁姆分类法和专家编写的指南。核心方法是将专家实践转化为隐含的违反场景，并扩展为自动评分的多项选择题和多轮对话，覆盖四个认知水平如分析和记忆。关键创新点包括实现确定性、可重复和可扩展的评估，应用于教学、营养学和护理领域以生成大规模基准，确保评估的心理测量学特性。",
      "result": "该框架在三个应用领域生成大规模基准并进行评估，结果表明大语言模型在高级认知水平如分析上表现相对较好，但在基础水平如记忆上失败更多。与人类推理对比，这些基准揭示了非直观的模型行为，例如模型在复杂推理任务中优于简单记忆任务，支持在现实环境中对上下文推理的深入评估。",
      "conclusion": "研究的主要贡献是开发了一个自动化基准生成框架，基于布鲁姆分类法和专家指南，用于评估大语言模型在实践领域的推理能力。学术上，该方法提供了一种可扩展、可重复的基准生成新途径；实际上，它使得在缺乏现有人类考试数据的领域中评估模型性能成为可能。未来工作可扩展到更多领域或进一步优化评估方法。",
      "tags": [
        "Open-ended Question Answering",
        "Large Language Models",
        "Bloom's Taxonomy",
        "Benchmark Generation",
        "Multiple-Choice Questions"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:26.240488Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20250",
    "title": "Order-Optimal Sample Complexity of Rectified Flows",
    "authors": [
      "Hari Krishna Sahoo",
      "Mudit Gaur",
      "Vaneet Aggarwal"
    ],
    "abstract": "Recently, flow-based generative models have shown superior efficiency compared to diffusion models. In this paper, we study rectified flow models, which constrain transport trajectories to be linear from the base distribution to the data distribution. This structural restriction greatly accelerates sampling, often enabling high-quality generation with a single Euler step. Under standard assumptions on the neural network classes used to parameterize the velocity field and data distribution, we prove that rectified flows achieve sample complexity $\\tilde{O}(\\varepsilon^{-2})$. This improves on the best known $O(\\varepsilon^{-4})$ bounds for flow matching model and matches the optimal rate for mean estimation. Our analysis exploits the particular structure of rectified flows: because the model is trained with a squared loss along linear paths, the associated hypothesis class admits a sharply controlled localized Rademacher complexity. This yields the improved, order-optimal sample complexity and provides a theoretical explanation for the strong empirical performance of rectified flow models.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20250.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20250",
    "published": "2026-01-28T04:55:14Z",
    "updated": "2026-01-28T04:55:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文证明了rectified flow模型在标准假设下实现阶最优样本复杂度 $\\tilde{O}(\\varepsilon^{-2})$，显著改进了流匹配模型的性能界限。",
      "motivation": "流基生成模型因其高效率在生成任务中受到关注，但现有方法如流匹配模型在样本复杂度上存在优化空间，理论分析不足。Rectified flows通过线性化传输轨迹加速采样，展现出强实证性能，但其样本复杂度理论尚未明确。本研究旨在解决这一问题，分析rectified flows的样本复杂度，为高效生成模型的优化提供理论基础，填补现有理论空白。",
      "method": "本研究提出rectified flow模型，核心是限制从基础分布到数据分布的传输轨迹为线性路径。使用神经网络参数化速度场，训练采用沿线性路径的平方损失函数。关键创新点在于利用这种线性结构，使得假设类具有严格控制的局部Rademacher复杂度，便于理论推导。基于标准神经网络类假设，分析模型样本复杂度的优化界限，避免了复杂扩散过程。",
      "result": "在标准神经网络类假设下，证明rectified flows的样本复杂度为$\\tilde{O}(\\varepsilon^{-2})$。这显著优于流匹配模型的最佳已知界限$O(\\varepsilon^{-4})$，并与均值估计的最优样本复杂度相匹配。分析表明，线性结构和平方损失训练能有效控制局部Rademacher复杂度，从而实现order-optimal性能。结果理论解释了rectified flow模型的强实证效率和采样速度优势。",
      "conclusion": "本研究的核心贡献是理论证明了rectified flow模型达到阶最优样本复杂度，为高效生成模型提供理论支撑。学术价值在于深化了流基模型的样本复杂度分析，推动了相关算法优化。实际应用中，可加速生成任务并启发未来模型设计。局限性可能在于假设条件限制泛化性，未来工作可扩展理论到更复杂分布或结合其他损失函数。",
      "tags": [
        "Rectified Flows",
        "Sample Complexity",
        "Rademacher Complexity",
        "Flow-Based Generative Models",
        "Squared Loss"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:26.205057Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20246",
    "title": "BLENDER: Blended Text Embeddings and Diffusion Residuals for Intra-Class Image Synthesis in Deep Metric Learning",
    "authors": [
      "Jan Niklas Kolf",
      "Ozan Tezcan",
      "Justin Theiss",
      "Hyung Jun Kim",
      "Wentao Bao",
      "Bhargav Bhushanam",
      "Khushi Gupta",
      "Arun Kejariwal",
      "Naser Damer",
      "Fadi Boutros"
    ],
    "abstract": "The rise of Deep Generative Models (DGM) has enabled the generation of high-quality synthetic data. When used to augment authentic data in Deep Metric Learning (DML), these synthetic samples enhance intra-class diversity and improve the performance of downstream DML tasks. We introduce BLenDeR, a diffusion sampling method designed to increase intra-class diversity for DML in a controllable way by leveraging set-theory inspired union and intersection operations on denoising residuals. The union operation encourages any attribute present across multiple prompts, while the intersection extracts the common direction through a principal component surrogate. These operations enable controlled synthesis of diverse attribute combinations within each class, addressing key limitations of existing generative approaches. Experiments on standard DML benchmarks demonstrate that BLenDeR consistently outperforms state-of-the-art baselines across multiple datasets and backbones. Specifically, BLenDeR achieves 3.7% increase in Recall@1 on CUB-200 and a 1.8% increase on Cars-196, compared to state-of-the-art baselines under standard experimental settings.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20246.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20246",
    "published": "2026-01-28T04:44:40Z",
    "updated": "2026-01-28T04:44:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "BLenDeR 提出一种扩散采样方法，通过集合论启发的联合和交集操作在去噪残差上实现可控的类内图像合成，以增强深度度量学习性能。",
      "motivation": "深度生成模型的崛起使得合成高质量数据成为可能，用于深度度量学习中可以增强类内多样性，改善下游任务性能。然而，现有生成方法在可控合成方面存在局限性，难以灵活地组合类内属性，可能导致合成样本不具代表性或多样性不足。本研究旨在通过可控方式增加类内多样性，以解决这一关键问题，提升数据增强效果。",
      "method": "BLenDeR 采用扩散模型框架，结合文本嵌入和扩散残差进行图像合成。核心创新是引入集合论启发的联合和交集操作：联合操作鼓励提取多个提示中的任何属性，交集操作通过主成分分析替代提取共同方向。这些操作允许模型在去噪过程中可控地生成类内的多样化属性组合。方法基于标准扩散采样技术，适用于多个数据集和骨干网络，以实现灵活的合成控制。",
      "result": "在标准深度度量学习基准测试中，BLenDeR 在多个数据集和骨干网络上一致优于最先进基线方法。具体而言，与基线相比，在 CUB-200 数据集上 Recall@1 指标提升了 3.7%，在 Cars-196 数据集上提升了 1.8%。这些实验结果表明，该方法显著提高了类内多样性和下游任务性能，验证了其有效性。",
      "conclusion": "本研究的主要贡献是开发了 BLenDeR 方法，通过集合论操作实现可控的类内图像合成，有效增强深度度量学习的多样性和性能。该技术具有学术价值，提供了一种新颖的数据增强方法，同时在实际应用中能改进 DML 模型的鲁棒性。未来工作可探索扩展到其他生成任务或优化操作效率，以进一步提升适用性。",
      "tags": [
        "Diffusion Models",
        "Deep Metric Learning",
        "Text Embeddings",
        "Intra-Class Synthesis",
        "Residual Operations"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:23.152662Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20232",
    "title": "Visual Prompt-Agnostic Evolution",
    "authors": [
      "Junze Wang",
      "Lei Fan",
      "Dezheng Zhang",
      "Weipeng Jing",
      "Donglin Di",
      "Yang Song",
      "Sidong Liu",
      "Cong Cong"
    ],
    "abstract": "Visual Prompt Tuning (VPT) adapts a frozen Vision Transformer (ViT) to downstream tasks by inserting a small number of learnable prompt tokens into the token sequence at each layer. However, we observe that existing VPT variants often suffer from unstable training dynamics, characterized by gradient oscillations. A layer-wise analysis reveals that shallow-layer prompts tend to stagnate early, while deeper-layer prompts exhibit high-variance oscillations, leading to cross-layer mismatch. These issues slow convergence and degrade final performance. To address these challenges, we propose Prompt-Agnostic Evolution ($\\mathtt{PAE}$), which strengthens vision prompt tuning by explicitly modeling prompt dynamics. From a frequency-domain perspective, we initialize prompts in a task-aware direction by uncovering and propagating frequency shortcut patterns that the backbone inherently exploits for recognition. To ensure coherent evolution across layers, we employ a shared Koopman operator that imposes a global linear transformation instead of uncoordinated, layer-specific updates. Finally, inspired by Lyapunov stability theory, we introduce a regularizer that constrains error amplification during evolution. Extensive experiments show that $\\mathtt{PAE}$ accelerates convergence with an average $1.41\\times$ speedup and improves accuracy by 1--3% on 25 datasets across multiple downstream tasks. Beyond performance, $\\mathtt{PAE}$ is prompt-agnostic and lightweight, and it integrates seamlessly with diverse VPT variants without backbone modification or inference-time changes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20232.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20232",
    "published": "2026-01-28T04:06:44Z",
    "updated": "2026-01-28T04:06:44Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": {
      "overview": "本文提出Prompt-Agnostic Evolution (PAE)，一种通过建模提示动态和稳定性约束来增强视觉提示调优稳定性和效率的方法。",
      "motivation": "现有视觉提示调优(VPT)方法在适应下游任务时面临训练不稳定问题，如梯度振荡，导致浅层提示停滞和深层提示振荡，引发跨层不匹配，从而延缓收敛并降低性能。这些问题凸显了对更高效稳定方法的需求，以提升视觉Transformer在多样化任务中的适应能力，解决现有方法在训练动态协调方面的不足。",
      "method": "PAE方法从频率域视角入手，通过发现和传播主干网络用于识别的频率捷径模式，实现任务感知提示初始化。为确保跨层连贯演化，采用共享Koopman算子强制全局线性变换，避免层特定更新的不协调性。受Lyapunov稳定性理论启发，引入正则化器约束演化过程中的误差放大。该方法轻量级，无需修改Vision Transformer主干或改变推理过程，可无缝集成到多种VPT变体中。",
      "result": "在25个数据集上的广泛实验表明，PAE加速收敛平均达1.41倍，准确率提升1-3%，适用于多个下游任务。与传统VPT方法相比，PAE显著改善了训练稳定性和最终性能，同时保持了轻量化和无需主干修改的优势，验证了其高效性和通用性。",
      "conclusion": "PAE通过建模提示演化和引入稳定性约束，有效解决了视觉提示调优的不稳定问题，为视觉Transformer下游任务适应提供了高效方案。其学术价值在于结合频率分析和稳定性理论创新，实际应用价值在于轻量化和可扩展性，可无缝集成到现有框架中。尽管摘要未明确说明局限性，但提示无关特性可能为未来优化和更广泛应用奠定基础。",
      "tags": [
        "Visual Prompt Tuning",
        "Vision Transformer",
        "Koopman Operator",
        "Lyapunov Stability Theory",
        "Frequency-domain Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:19.828754Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20231",
    "title": "Certificate-Guided Pruning for Stochastic Lipschitz Optimization",
    "authors": [
      "Ibne Farabi Shihab",
      "Sanjeda Akter",
      "Anuj Sharma"
    ],
    "abstract": "We study black-box optimization of Lipschitz functions under noisy evaluations. Existing adaptive discretization methods implicitly avoid suboptimal regions but do not provide explicit certificates of optimality or measurable progress guarantees. We introduce \\textbf{Certificate-Guided Pruning (CGP)}, which maintains an explicit \\emph{active set} $A_t$ of potentially optimal points via confidence-adjusted Lipschitz envelopes. Any point outside $A_t$ is certifiably suboptimal with high probability, and under a margin condition with near-optimality dimension $α$, we prove $\\Vol(A_t)$ shrinks at a controlled rate yielding sample complexity $\\tildeO(\\varepsilon^{-(2+α)})$. We develop three extensions: CGP-Adaptive learns $L$ online with $O(\\log T)$ overhead; CGP-TR scales to $d > 50$ via trust regions with local certificates; and CGP-Hybrid switches to GP refinement when local smoothness is detected. Experiments on 12 benchmarks ($d \\in [2, 100]$) show CGP variants match or exceed strong baselines while providing principled stopping criteria via certificate volume.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20231.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20231",
    "published": "2026-01-28T04:02:22Z",
    "updated": "2026-01-28T04:02:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Certificate-Guided Pruning (CGP)方法，用于在有噪声评估下优化Lipschitz函数，提供明确的次优性证书以改进黑盒优化。",
      "motivation": "该研究针对黑盒优化中Lipschitz函数在噪声环境下的全局最优解搜索问题。现有自适应离散化方法虽能隐含避免次优区域，但缺乏明确的最优性证书和可测量进展保证，使得优化过程不确定且难以可靠停止。因此，开发一种能提供证书的方法至关重要，以增强优化算法的透明度和效率，尤其在需要高可靠性保证的应用中。",
      "method": "论文的核心方法是Certificate-Guided Pruning (CGP)，它通过置信调整的Lipschitz包络维护一个显式活动集，将不在该集中的点认证为次优点。关键创新包括理论证明在边界条件下活动集体积收缩速率和样本复杂度为O(ε^{-(2+α)})。扩展版本包括CGP-Adaptive在线学习Lipschitz常数，CGP-TR利用信任区域扩展到高维问题（d>50），以及CGP-Hybrid在检测局部平滑时切换至高斯过程细化。实验在12个基准数据集上进行，维度范围为2到100。",
      "result": "在12个基准测试（维度2到100）上的实验表明，CGP变体能够匹配或超过强基线方法，如自适应离散化技术。摘要未提供具体性能指标，但结果显示了CGP在保持优化性能的同时，通过证书体积提供了原则性的停止准则，增强了优化过程的可靠性和实用性。",
      "conclusion": "本研究的核心贡献是提出了CGP方法，为Lipschitz优化引入了明确的次优性证书和理论保证，从而提高了黑盒优化的可靠性和效率。其学术价值在于扩展了自适应离散化框架，具有应用于高维和噪声优化场景的潜力。未来工作可探索CGP在更广泛优化问题中的扩展，以及在实际部署中的进一步验证。",
      "tags": [
        "Certificate-Guided Pruning",
        "Lipschitz Optimization",
        "Black-Box Optimization",
        "Trust Regions",
        "Gaussian Process"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:42.825644Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20230",
    "title": "Unit-Based Agent for Semi-Cascaded Full-Duplex Dialogue Systems",
    "authors": [
      "Haoyuan Yu",
      "Yuxuan Chen",
      "Minjie Cai"
    ],
    "abstract": "Full-duplex voice interaction is crucial for natural human computer interaction. We present a framework that decomposes complex dialogue into minimal conversational units, enabling the system to process each unit independently and predict when to transit to the next. This framework is instantiated as a semi-cascaded full-duplex dialogue system built around a multimodal large language model, supported by auxiliary modules such as voice activity detection (VAD) and text-to-speech (TTS) synthesis. The resulting system operates in a train-free, plug-and-play manner. Experiments on the HumDial dataset demonstrate the effectiveness of our framework, which ranks second among all teams on the test set of the Human-like Spoken Dialogue Systems Challenge (Track 2: Full-Duplex Interaction). Code is available at the GitHub repository https://github.com/yu-haoyuan/fd-badcat.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.20230.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20230",
    "published": "2026-01-28T04:00:37Z",
    "updated": "2026-01-28T04:00:37Z",
    "comment": "ICASSP 2026 (Workshop). https://github.com/yu-haoyuan/fd-badcat",
    "light_analysis": {
      "overview": "本文提出一种基于单元的半级联全双工对话系统框架，利用多模态大语言模型实现训练免费、即插即用的自然语音交互。",
      "motivation": "全双工语音交互对实现自然人机交互至关重要，但现有系统在处理复杂对话时常常面临单元分解不精确和过渡时机预测困难的挑战。这可能导致交互不自然或延迟，限制了系统的实用性和灵活性。本研究的动机是开发一个更高效的框架，通过分解对话为最小单位，简化处理过程，减少对大量训练数据的依赖，提升对话的自然性和连续性，以克服传统方法在处理实时交互时的不足。",
      "method": "论文提出一个框架，将复杂对话分解为最小对话单元，使系统能独立处理每个单元并预测何时过渡到下一个。该框架被实例化为半级联全双工对话系统，核心是多模态大语言模型，用于理解和生成对话内容，并集成语音活动检测（VAD）和文本转语音（TTS）合成等辅助模块以支持实时交互。系统设计为训练免费、即插即用，无需额外训练即可部署，采用模块化结构以提高实用性和可扩展性，结合了序列和并行处理的优势。",
      "result": "实验在HumDial数据集上进行，结果显示该框架具有高效性。具体而言，在Human-like Spoken Dialogue Systems Challenge的测试集（Track 2: Full-Duplex Interaction）中，系统在所有参赛团队中排名第二，证明了其在全双工交互任务上的优异性能。尽管摘要未提供具体准确率或延迟数据，但竞赛排名表明该框架与基线方法相比具有竞争力，有效验证了单元分解和半级联设计的成功应用。",
      "conclusion": "本研究的主要贡献是提出并实现了一个基于单元的半级联全双工对话系统框架，通过多模态大语言模型和辅助模块实现训练免费、即插即用的操作，实验证实了其有效性。这一成果对推动自然语音交互系统的开发具有重要学术和实际应用价值，降低了部署门槛并提高了对话的自然度。未来工作可能包括优化单元分解算法和扩展更多模态应用，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Full-duplex Dialogue System",
        "Semi-Cascaded Framework",
        "Multimodal Large Language Model",
        "Voice Activity Detection (VAD)",
        "Text-to-Speech Synthesis (TTS)"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:28.443841Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20227",
    "title": "ProFlow: Zero-Shot Physics-Consistent Sampling via Proximal Flow Guidance",
    "authors": [
      "Zichao Yu",
      "Ming Li",
      "Wenyi Zhang",
      "Difan Zou",
      "Weiguo Gao"
    ],
    "abstract": "Inferring physical fields from sparse observations while strictly satisfying partial differential equations (PDEs) is a fundamental challenge in computational physics. Recently, deep generative models offer powerful data-driven priors for such inverse problems, yet existing methods struggle to enforce hard physical constraints without costly retraining or disrupting the learned generative prior. Consequently, there is a critical need for a sampling mechanism that can reconcile strict physical consistency and observational fidelity with the statistical structure of the pre-trained prior. To this end, we present ProFlow, a proximal guidance framework for zero-shot physics-consistent sampling, defined as inferring solutions from sparse observations using a fixed generative prior without task-specific retraining. The algorithm employs a rigorous two-step scheme that alternates between: (\\romannumeral1) a terminal optimization step, which projects the flow prediction onto the intersection of the physically and observationally consistent sets via proximal minimization; and (\\romannumeral2) an interpolation step, which maps the refined state back to the generative trajectory to maintain consistency with the learned flow probability path. This procedure admits a Bayesian interpretation as a sequence of local maximum a posteriori (MAP) updates. Comprehensive benchmarks on Poisson, Helmholtz, Darcy, and viscous Burgers' equations demonstrate that ProFlow achieves superior physical and observational consistency, as well as more accurate distributional statistics, compared to state-of-the-art diffusion- and flow-based baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20227.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20227",
    "published": "2026-01-28T03:57:00Z",
    "updated": "2026-01-28T03:57:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "ProFlow提出了一种基于近端指导的零样本物理一致采样框架，无需重新训练即可从稀疏观测推断满足偏微分方程的物理场。",
      "motivation": "在计算物理中，从稀疏观测推断物理场并严格满足偏微分方程（PDEs）是逆问题的核心挑战。现有深度生成模型虽然提供了数据驱动先验，但难以强制执行硬物理约束，通常需要任务特定重新训练或会破坏已学习的先验分布。这限制了实际应用，因为物理一致性至关重要，而观测数据往往有限。因此，迫切需要开发一种采样机制，能够在保持预训练生成先验统计结构的同时，确保物理和观测的一致性。本研究的动机在于填补这一空白，提出一种零样本方法来解决该问题。",
      "method": "ProFlow采用一个两步骤的框架进行零样本物理一致采样。首先，在终端优化步骤中，通过近端最小化技术将流预测投影到物理一致和观测一致集合的交集，确保满足PDE约束和稀疏观测数据。其次，在插值步骤中，将精炼后的状态映射回生成轨迹，以保持与预训练流概率路径的一致性。这种方法允许在固定生成先验下进行，无需重新训练，并具有贝叶斯解释，可视为一系列局部最大后验（MAP）更新。框架基于流生成模型，通过交替执行优化和插值来迭代改进采样结果。",
      "result": "实验在多个偏微分方程上进行基准测试，包括泊松方程、亥姆霍兹方程、达西方程和黏性布格斯方程。与基于扩散和流的先进基线方法相比，ProFlow实现了更优越的物理一致性和观测保真度。具体表现为，在推理物理场时，能够更准确地满足PDE约束，同时更紧密地拟合稀疏观测数据。此外，ProFlow还展示了更准确的分布统计特性，表明其在维持数据分布先验方面的有效性。摘要未提供具体数值指标，但综合评估表明ProFlow在这些标准问题上表现优异。",
      "conclusion": "ProFlow的主要贡献是提出了一个零样本物理一致采样框架，有效解决了逆问题中物理约束与生成先验之间的冲突。该方法通过近端指导和两步方案，实现了无需重新训练的物理场推断，具有学术价值，为深度生成模型在物理领域的应用提供了新思路。在实际应用上，可广泛用于计算物理、工程模拟等需要精确物理建模的场景。未来工作可能包括扩展到更复杂的PDEs或结合更多类型的观测数据，以进一步提升其普适性和性能。",
      "tags": [
        "Proximal Minimization",
        "Flow-based Generative Models",
        "Zero-Shot Sampling",
        "Physics-Informed Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:27.740803Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20226",
    "title": "Parametric and Generative Forecasts of Day-Ahead Market Curves for Storage Optimization",
    "authors": [
      "Julian Gutierrez",
      "Redouane Silvente"
    ],
    "abstract": "We present two machine learning frameworks for forecasting aggregated curves and optimizing storage in the EPEX SPOT day-ahead market. First, a fast parametric model forecasts hourly demand and supply curves in a low-dimensional and grid-robust representation, with minimum and maximum volumes combined with a Chebyshev polynomial for the elastic segment. The model enables daily use with low error and clear interpretability. Second, for a more comprehensive analysis, though less suited to daily operation, we employ generative models that learn the joint distribution of 24-hour order-level submissions given weather and fuel variables. These models generate synthetic daily scenarios of individual buy and sell orders, which, once aggregated, yield hourly supply and demand curves. Based on these forecasts, we optimize a price-making storage strategy, quantify revenue distributions, and highlight the price-compression effect with lower peaks, higher off-peak levels, and diminishing returns as capacity expands.",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20226.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20226",
    "published": "2026-01-28T03:56:05Z",
    "updated": "2026-01-28T03:56:05Z",
    "comment": "46 pages, 41 figures",
    "light_analysis": {
      "overview": "提出参数和生成两种机器学习框架，预测日前市场供需曲线并优化储能策略，以提高储能运营的经济效益。",
      "motivation": "研究旨在解决EPEX SPOT日前市场供需曲线预测问题，以支持储能优化决策。在能源市场中，准确预测曲线对储能价格策略制定至关重要，但摘要未明确说明现有方法的不足，可推断现有方法可能在实时性、可解释性或全面性上存在限制，因此开发高效预测模型是必要的，以提升储能管理的效率和准确性。",
      "method": "论文提出两个核心框架：快速参数模型和生成模型。参数模型使用低维网格鲁棒表示，结合Chebyshev多项式预测每小时曲线的弹性段，适合日常操作且具有高可解释性。生成模型基于天气和燃料变量学习24小时订单级联合分布，生成合成买卖订单，聚合后得到供需曲线，适用于全面分析。创新点在于参数模型的快速准确性和生成模型的概率建模能力，关键细节包括处理市场数据和变量输入。",
      "result": "实验结果显示，参数模型实现了低误差和良好可解释性，适合每日使用；生成模型通过合成情景提供了全面曲线分析。基于这些预测，优化了价格制定储能策略，量化了收入分布，并观察到价格压缩效应：峰值降低、非峰值升高，收益随容量扩大而递减。摘要未提供具体性能指标或基线对比，但强调了模型在储能优化中的实用效果和改进潜力。",
      "conclusion": "论文的主要贡献是开发了参数和生成预测框架，显著改进日前市场曲线预测和储能优化。学术价值在于结合机器学习与能源经济学模型，实际应用价值在于提高储能运营的经济效益和决策支持。局限性包括生成模型可能不适合日常操作，未来工作可扩展至其他市场或整合更多实时变量以增强模型适应性。",
      "tags": [
        "Machine Learning",
        "Parametric Modeling",
        "Generative Models",
        "Market Curve Forecasting",
        "Storage Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:07.756955Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20224",
    "title": "Feature Projection Learning for Better Vision-Language Reasoning",
    "authors": [
      "Yi Zhang",
      "Weicheng Lin",
      "Liang-Jie Zhang"
    ],
    "abstract": "Vision-Language Pre-Trained models, notably CLIP, that utilize contrastive learning have proven highly adept at extracting generalizable visual features. To inherit the well-learned knowledge of VLP models for downstream tasks, several approaches aim to adapt them efficiently with limited supervision. However, these methods either suffer from limited performance, excessive learnable parameters, or extended training times, all of which hinder their effectiveness in adapting the CLIP model to downstream tasks. In this work, we propose a simple yet efficient and effective method called \\textit{\\textbf{F}eature \\textbf{P}rojection \\textbf{L}earning(FPL)} to address these problems. Specifically, we develop a projection model that projects class prototype features into the query image feature space and reconstructs the query image feature map. The negative average squared reconstruction error is used as the class score. In this way, we transform the classification problem into a feature projection problem. The final output of this method is a combination of the prediction from the projection model and the original pre-trained CLIP. Comprehensive empirical evaluations confirm that FPL delivers superior accuracy, surpassing the current state-of-the-art methods by a substantial margin.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20224.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20224",
    "published": "2026-01-28T03:54:36Z",
    "updated": "2026-01-28T03:54:36Z",
    "comment": "Accepted to ICASSP 2026",
    "light_analysis": {
      "overview": "提出一种简单高效的Feature Projection Learning方法，通过特征投影改善CLIP模型在下游任务中的适配性能。",
      "motivation": "本研究旨在解决Vision-Language Pre-Trained模型（如CLIP）在下游任务适配时的效率问题。现有方法在继承VLP模型知识时，常面临性能有限、可学习参数过多或训练时间过长的挑战，这些因素阻碍了CLIP模型在实际应用中的有效部署，限制了其泛化能力的充分发挥。由于下游任务需要高效且准确的适配，开发新方法以提升适配效果变得尤为重要。摘要强调了当前方法的不足，凸显了研究的重要性。",
      "method": "论文提出Feature Projection Learning（FPL）方法，核心是一个投影模型，将类原型特征投影到查询图像特征空间，并重建查询图像的特征图。创新点在于将分类问题转化为特征投影问题，使用负平均平方重建误差作为类分数。最终输出结合了投影模型的预测和原始预训练CLIP的预测，以增强鲁棒性和准确性。该方法简化了适配过程，减少了参数和训练时间，关键技术包括特征空间投影和重建误差计算。",
      "result": "摘要中，实证评估显示FPL在准确性上优于当前state-of-the-art方法，有显著提升，但未明确说明具体性能指标（如准确率数值）或对比基线方法的详细信息。报告强调了FPL在解决适配问题时实现了卓越效果，超过现有最佳方法，但具体数据未在摘要中提供，需参考全文获取细节。",
      "conclusion": "本研究的主要贡献是提出FPL方法，有效解决了CLIP模型在下游任务适配中的效率问题，提升了准确性。学术价值在于创新地将分类任务转化为特征投影问题，为视觉-语言模型适配提供了新思路。实际应用价值在于可能更高效地适应各种下游任务，促进模型部署。摘要未明确说明局限性或未来工作方向，但推断未来研究可能扩展应用到其他领域或优化投影模型。",
      "tags": [
        "Vision-Language Pre-Training",
        "CLIP",
        "Contrastive Learning",
        "Feature Projection",
        "Reconstruction Error"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:34.295441Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20221",
    "title": "Scaling Medical Reasoning Verification via Tool-Integrated Reinforcement Learning",
    "authors": [
      "Hang Zhang",
      "Ruheng Wang",
      "Yuelyu Ji",
      "Mingu Kwak",
      "Xizhi Wu",
      "Chenyu Li",
      "Li Zhang",
      "Wenqi Shi",
      "Yifan Peng",
      "Yanshan Wang"
    ],
    "abstract": "Large language models have achieved strong performance on medical reasoning benchmarks, yet their deployment in clinical settings demands rigorous verification to ensure factual accuracy. While reward models offer a scalable approach for reasoning trace verification, existing methods face two limitations: they produce only scalar reward values without explicit justification, and they rely on single-pass retrieval that precludes adaptive knowledge access as verification unfolds. We introduce $\\method$, an agentic framework that addresses these limitations by training medical reasoning verifiers to iteratively query external medical corpora during evaluation. Our approach combines tool-augmented verification with an iterative reinforcement learning paradigm that requires only trace-level supervision, alongside an adaptive curriculum mechanism that dynamically adjusts training data distribution. Across four medical reasoning benchmarks, $\\method$ achieves substantial gains over existing methods, improving MedQA accuracy by 23.5% and MedXpertQA by 32.0% relative to the base generator in particular. Crucially, $\\method$ demonstrates an $\\mathbf{8\\times}$ reduction in sampling budget requirement compared to prior reward model baselines. These findings establish that grounding verification in dynamically retrieved evidence offers a principled path toward more reliable medical reasoning systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20221.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20221",
    "published": "2026-01-28T03:44:20Z",
    "updated": "2026-01-28T03:44:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种工具集成强化学习框架，通过迭代查询外部医学知识库验证推理，显著提升医学推理的准确性和效率。",
      "motivation": "大型语言模型在医学推理基准上表现优异，但临床部署需要严格验证事实准确性以确保可靠性。现有奖励模型方法面临两个主要局限：仅提供标量奖励值而无明确解释，且依赖单次检索，无法在验证过程中自适应访问知识，这限制了验证的透明性和效果。因此，开发一种更动态、可解释的验证方法至关重要，以支持医学决策系统。",
      "method": "该方法引入一个代理框架，结合工具增强验证与迭代强化学习范式。核心创新是训练医学推理验证器在评估时迭代查询外部医学语料库，使用仅需追踪级监督的强化学习，避免复杂标注。同时，采用自适应课程机制动态调整训练数据分布，优化学习过程，从而提高验证的准确性和适应性。",
      "result": "在四个医学推理基准测试中，该方法相比现有方法取得显著提升：相对于基础生成器，MedQA准确率提高23.5%，MedXpertQA提高32.0%。更重要的是，采样预算需求减少8倍，表明验证效率大幅改善。这些结果通过具体数据证明了该方法在准确性和资源效率上的优势。",
      "conclusion": "该研究证实基于动态检索证据的验证为构建更可靠医学推理系统提供了原则性路径，主要贡献在于新框架的提出和实验验证。学术上推动验证方法创新，实际上有助于临床决策支持。未来工作可扩展至其他领域或优化模型细节，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Tool Integration",
        "Medical Reasoning",
        "Iterative Querying"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:36.425692Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20218",
    "title": "DenseGRPO: From Sparse to Dense Reward for Flow Matching Model Alignment",
    "authors": [
      "Haoyou Deng",
      "Keyu Yan",
      "Chaojie Mao",
      "Xiang Wang",
      "Yu Liu",
      "Changxin Gao",
      "Nong Sang"
    ],
    "abstract": "Recent GRPO-based approaches built on flow matching models have shown remarkable improvements in human preference alignment for text-to-image generation. Nevertheless, they still suffer from the sparse reward problem: the terminal reward of the entire denoising trajectory is applied to all intermediate steps, resulting in a mismatch between the global feedback signals and the exact fine-grained contributions at intermediate denoising steps. To address this issue, we introduce \\textbf{DenseGRPO}, a novel framework that aligns human preference with dense rewards, which evaluates the fine-grained contribution of each denoising step. Specifically, our approach includes two key components: (1) we propose to predict the step-wise reward gain as dense reward of each denoising step, which applies a reward model on the intermediate clean images via an ODE-based approach. This manner ensures an alignment between feedback signals and the contributions of individual steps, facilitating effective training; and (2) based on the estimated dense rewards, a mismatch drawback between the uniform exploration setting and the time-varying noise intensity in existing GRPO-based methods is revealed, leading to an inappropriate exploration space. Thus, we propose a reward-aware scheme to calibrate the exploration space by adaptively adjusting a timestep-specific stochasticity injection in the SDE sampler, ensuring a suitable exploration space at all timesteps. Extensive experiments on multiple standard benchmarks demonstrate the effectiveness of the proposed DenseGRPO and highlight the critical role of the valid dense rewards in flow matching model alignment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20218.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20218",
    "published": "2026-01-28T03:39:05Z",
    "updated": "2026-01-28T03:39:05Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": {
      "overview": "DenseGRPO通过引入密集奖励和奖励感知探索空间校准，有效解决了流匹配模型对齐中的稀疏奖励问题，提升了文本到图像生成的人类偏好对齐效果。",
      "motivation": "在文本到图像生成中，基于GRPO的流匹配模型虽提升了人类偏好对齐，但仍面临稀疏奖励问题：整个去噪轨迹的终端奖励被统一应用于所有中间步骤，导致全局反馈信号与各步骤的细粒度贡献失配。这种不匹配阻碍了模型的有效训练，降低了对齐精度。现有方法因未能精确评估中间步骤的贡献，探索空间设置不当，限制了性能提升。因此，开发一种能提供密集奖励并校准探索空间的方法至关重要，以优化训练过程并提高生成质量。",
      "method": "DenseGRPO框架包括两个核心组件：首先，通过基于ODE的方法预测每个去噪步骤的步进奖励增益作为密集奖励，在中间干净图像上应用奖励模型，确保反馈信号与单个步骤贡献对齐。其次，基于密集奖励估计，揭示现有GRPO方法中均匀探索设置与时间变化噪声强度不匹配的问题，提出奖励感知方案，自适应调整SDE采样器中的时间步特定随机性注入，以校准探索空间。该方法创新地从稀疏转向密集奖励，实现了细粒度评估，并优化了探索策略，从而促进有效训练。",
      "result": "在多个标准基准上的广泛实验表明，DenseGRPO显著提升了流匹配模型在人类偏好对齐中的性能。通过引入密集奖励和校准探索空间，该方法有效解决了稀疏奖励问题，改善了生成质量。与现有GRPO方法相比，DenseGRPO展示了更高的对齐精度和训练效率。实验验证了密集奖励在评估中间步骤贡献中的关键作用，并强调了奖励感知探索空间校准的重要性，为后续研究提供了有力支撑。",
      "conclusion": "本研究提出的DenseGRPO框架通过密集奖励和奖励感知探索空间校准，成功解决了流匹配模型对齐中的稀疏奖励问题。其核心贡献在于实现了细粒度奖励评估和优化探索策略，从而提升了人类偏好对齐的效果。该研究不仅提高了文本到图像生成的性能，还为强化学习在生成模型对齐中的应用提供了新思路。未来工作可进一步探索密集奖励在其他生成任务中的泛化性，以及更高效的奖励建模方法。",
      "tags": [
        "Flow Matching",
        "Dense Reward",
        "ODE-based Methods",
        "SDE Sampler",
        "Preference Alignment"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:54.798067Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20217",
    "title": "An Accounting Identity for Algorithmic Fairness",
    "authors": [
      "Hadi Elzayn",
      "Jacob Goldin"
    ],
    "abstract": "We derive an accounting identity for predictive models that links accuracy with common fairness criteria. The identity shows that for globally calibrated models, the weighted sums of miscalibration within groups and error imbalance across groups is equal to a \"total unfairness budget.\" For binary outcomes, this budget is the model's mean-squared error times the difference in group prevalence across outcome classes. The identity nests standard impossibility results as special cases, while also describing inherent tradeoffs when one or more fairness measures are not perfectly satisfied. The results suggest that accuracy and fairness are best viewed as complements in binary prediction tasks: increasing accuracy necessarily shrinks the total unfairness budget and vice-versa. Experiments on benchmark data confirm the theory and show that many fairness interventions largely substitute between fairness violations, and when they reduce accuracy they tend to expand the total unfairness budget. The results extend naturally to prediction tasks with non-binary outcomes, illustrating how additional outcome information can relax fairness incompatibilities and identifying conditions under which the binary-style impossibility does and does not extend to regression tasks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20217.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20217",
    "published": "2026-01-28T03:38:15Z",
    "updated": "2026-01-28T03:38:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个会计身份，将预测模型的准确性与公平标准量化链接，揭示它们在二进制任务中的互补关系。",
      "motivation": "研究旨在解决算法公平性与准确性之间的权衡问题。实际问题是公平性准则往往与预测准确性冲突，现有方法可能无法同时满足两者，导致标准不可能性结果。这一挑战在AI应用中至关重要，因为公平决策影响社会公正，但现有框架缺乏系统性的理论量化准确性与公平性之间的关系，使得干预措施效果有限。摘要未明确说明具体应用场景，但强调了量化这一权衡的重要性。",
      "method": "核心方法是推导一个会计身份，用于量化预测模型准确性与公平标准之间的数学关系。基于全局校准模型，该身份将组内错误校准和组间错误不平衡的加权和定义为“总不公平预算”。对于二进制结果，预算等于均方误差乘以组流行度差异。创新点在于身份将标准不可能性结果作为特例，描述了公平措施不完全满足时的固有权衡。技术细节包括使用基准数据进行实验，但摘要未明确说明具体数据集或模型架构。",
      "result": "主要实验结果表明，在基准数据上验证了理论，显示许多公平干预在公平违反之间进行替代，并且当干预降低模型准确性时，往往会扩大总不公平预算。这表明准确性与公平性存在互补关系：提高准确性会减少不公平预算。摘要未提供具体性能指标如准确率提升，但通过实验确认了理论预测，与基线方法相比，身份揭示了准确性与公平性权衡的动态。结果支持了二进制任务中的互补性观点。",
      "conclusion": "论文的主要贡献是提出了一个会计身份，系统化地链接准确性与公平标准，揭示了它们之间的互补关系。学术价值在于提供了一个理论框架，帮助理解公平-准确性权衡，并将标准不可能性结果纳入其中。实际应用价值包括指导公平干预的设计，避免盲目牺牲准确性。研究自然扩展到非二进制任务，识别了二进制不可能性扩展到回归任务的条件，未来工作可进一步探索多类别预测中的公平性。摘要未明确说明局限性。",
      "tags": [
        "Algorithmic Fairness",
        "Predictive Models",
        "Accuracy-Fairness Tradeoff",
        "Accounting Identity",
        "Binary Prediction"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:29.769562Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20209",
    "title": "Spark: Strategic Policy-Aware Exploration via Dynamic Branching for Long-Horizon Agentic Learning",
    "authors": [
      "Jinyang Wu",
      "Shuo Yang",
      "Changpeng Yang",
      "Yuhao Shen",
      "Shuai Zhang",
      "Zhengqi Wen",
      "Jianhua Tao"
    ],
    "abstract": "Reinforcement learning has empowered large language models to act as intelligent agents, yet training them for long-horizon tasks remains challenging due to the scarcity of high-quality trajectories, especially under limited resources. Existing methods typically scale up rollout sizes and indiscriminately allocate computational resources among intermediate steps. Such attempts inherently waste substantial computation budget on trivial steps while failing to guarantee sample quality. To address this, we propose \\textbf{Spark} (\\textbf{S}trategic \\textbf{P}olicy-\\textbf{A}ware explo\\textbf{R}ation via \\textbf{K}ey-state dynamic branching), a novel framework that selectively branches at critical decision states for resource-efficient exploration. Our key insight is to activate adaptive branching exploration at critical decision points to probe promising trajectories, thereby achieving precise resource allocation that prioritizes sampling quality over blind coverage. This design leverages the agent's intrinsic decision-making signals to reduce dependence on human priors, enabling the agent to autonomously expand exploration and achieve stronger generalization. Experiments across diverse tasks (e.g., embodied planning), demonstrate that \\textsc{Spark} achieves superior success rates with significantly fewer training samples, exhibiting robust generalization even in unseen scenarios.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20209.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20209",
    "published": "2026-01-28T03:15:34Z",
    "updated": "2026-01-28T03:15:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "Spark 提出了一种基于动态分支的策略感知探索框架，通过选择性分支在关键决策点优化资源分配，以高效训练强化学习代理执行长期任务。",
      "motivation": "强化学习使大型语言模型能够作为智能代理，但训练长期任务时面临高质量轨迹稀缺和资源有限的挑战。现有方法通常扩大探索规模并均匀分配计算资源，导致大量计算浪费在无关紧要的步骤上，同时无法确保样本质量。这凸显了需要更智能的资源分配策略，以优先处理关键决策步骤，从而提升训练效率和任务成功率。摘要未明确说明具体基准方法的名称，但指出了现有方法的不足。",
      "method": "Spark 框架的核心方法是在关键决策状态实施动态分支探索，通过识别并选择性激活关键状态来探测有希望的轨迹。它利用代理的内在决策信号自适应地扩展探索范围，减少对人类先验的依赖，使代理能自主优化探索策略。摘要提及了 embodied planning 等任务作为示例，但未详细说明使用的具体数据集、模型架构或算法细节，仅强调其基于强化学习和策略感知设计。",
      "result": "实验结果表明，Spark 在多种任务中，如 embodied planning，能以显著减少的训练样本实现更高的成功率。与现有方法相比，它更有效地分配计算资源，优先保证了样本质量，并在未见场景中展现出更强的泛化能力。摘要未提供具体的准确率或效率数值，但强调了资源效率和鲁棒性方面的优势。",
      "conclusion": "Spark 的主要贡献在于开发了一种资源高效的探索框架，通过动态分支减少对先验知识的依赖并增强代理的自主性。其学术价值是推动强化学习在长期任务中的探索策略创新，实际应用价值在于提升训练效率和泛化能力。摘要未明确提及局限性，但未来工作可能涉及扩展到更复杂任务或与其他技术整合以进一步优化性能。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Dynamic Branching",
        "Exploration Strategy",
        "Embodied Planning"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:52.777161Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20206",
    "title": "Towards Intelligent Urban Park Development Monitoring: LLM Agents for Multi-Modal Information Fusion and Analysis",
    "authors": [
      "Zixuan Xiao",
      "Chunguang Hu",
      "Jun Ma"
    ],
    "abstract": "As an important part of urbanization, the development monitoring of newly constructed parks is of great significance for evaluating the effect of urban planning and optimizing resource allocation. However, traditional change detection methods based on remote sensing imagery have obvious limitations in high-level and intelligent analysis, and thus are difficult to meet the requirements of current urban planning and management. In face of the growing demand for complex multi-modal data analysis in urban park development monitoring, these methods often fail to provide flexible analysis capabilities for diverse application scenarios. This study proposes a multi-modal LLM agent framework, which aims to make full use of the semantic understanding and reasoning capabilities of LLM to meet the challenges in urban park development monitoring. In this framework, a general horizontal and vertical data alignment mechanism is designed to ensure the consistency and effective tracking of multi-modal data. At the same time, a specific toolkit is constructed to alleviate the hallucination issues of LLM due to the lack of domain-specific knowledge. Compared to vanilla GPT-4o and other agents, our approach enables robust multi-modal information fusion and analysis, offering reliable and scalable solutions tailored to the diverse and evolving demands of urban park development monitoring.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.20206.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20206",
    "published": "2026-01-28T03:03:15Z",
    "updated": "2026-01-28T03:03:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于大型语言模型（LLM）的多模态代理框架，用于实现城市公园发展监测的智能信息融合与分析。",
      "motivation": "城市公园作为城市化进程的重要组成部分，其发展监测对评估城市规划效果和优化资源配置至关重要。然而，传统基于遥感影像的变化检测方法在高级智能分析方面存在明显局限性，难以满足当前城市规划和管理的复杂需求。随着多模态数据分析需求日益增长，如结合图像、文本等多种数据类型，现有方法往往缺乏灵活性，无法适应多样化应用场景，因此亟需开发更智能、更灵活的分析框架来解决这些问题。",
      "method": "该研究设计了一个多模态LLM代理框架，核心是利用LLM的语义理解和推理能力来处理城市公园监测中的多模态数据。关键创新包括设计了一个通用的水平和垂直数据对齐机制，以确保不同数据源之间的一致性和有效追踪。同时，为了缓解LLM由于缺乏领域知识而产生的幻觉问题，构建了特定工具包来提供领域相关的知识支持。框架整合了遥感影像等多模态数据，通过LLM代理进行智能分析和决策，以应对复杂的监测任务。",
      "result": "摘要未明确说明具体的性能指标，如准确率或效率提升。但论文指出，与vanilla GPT-4o和其他代理方法相比，该方法能更稳健地进行多模态信息融合和分析，并提供可靠且可扩展的解决方案。这强调了在应对城市公园发展监测的多样化和演进需求方面，该方法在稳健性和适应性方面优于基线方法，但没有提供详细的数据支撑。",
      "conclusion": "本论文的主要贡献在于提出了一个多模态LLM代理框架，通过数据对齐机制和领域特定工具包，显著提升了城市公园发展监测的智能分析能力。这项研究具有重要的学术价值，推动了多模态AI技术在智慧城市领域的应用；同时，其实用价值体现在为城市规划和管理提供灵活、可靠的决策支持工具。潜在的局限性可能包括对领域知识的依赖性，未来工作可以扩展到其他城市管理任务或优化框架以处理更多数据模态。",
      "tags": [
        "Large Language Model (LLM)",
        "Multi-Modal Information Fusion",
        "Data Alignment",
        "AI Agents",
        "Urban Monitoring"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:10.521725Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20205",
    "title": "Hyperparameter Transfer with Mixture-of-Expert Layers",
    "authors": [
      "Tianze Jiang",
      "Blake Bordelon",
      "Cengiz Pehlevan",
      "Boris Hanin"
    ],
    "abstract": "Mixture-of-Experts (MoE) layers have emerged as an important tool in scaling up modern neural networks by decoupling total trainable parameters from activated parameters in the forward pass for each token. However, sparse MoEs add complexity to training due to (i) new trainable parameters (router weights) that, like all other parameter groups, require hyperparameter (HP) tuning; (ii) new architecture scale dimensions (number of and size of experts) that must be chosen and potentially taken large. To make HP selection cheap and reliable, we propose a new parameterization for transformer models with MoE layers when scaling model width, depth, number of experts, and expert (hidden) size. Our parameterization is justified by a novel dynamical mean-field theory (DMFT) analysis. When varying different model dimensions trained at a fixed token budget, we find empirically that our parameterization enables reliable HP transfer across models from 51M to over 2B total parameters. We further take HPs identified from sweeping small models on a short token horizon to train larger models on longer horizons and report performant model behaviors.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20205.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20205",
    "published": "2026-01-28T03:02:30Z",
    "updated": "2026-01-28T03:02:30Z",
    "comment": "25 Pages",
    "light_analysis": {
      "overview": "论文提出一种新的参数化方法，结合动态平均场理论分析，实现Transformer模型中Mixture-of-Experts层的超参数可靠传递。",
      "motivation": "Mixture-of-Experts (MoE) 层通过解耦总可训练参数和前向传递中的激活参数，有效扩展神经网络规模。然而，稀疏MoE引入了新的训练复杂性：路由器权重等新参数需要超参数调优，同时需选择专家数量和大小等架构维度。现有方法中，超参数选择昂贵且不可靠，尤其是在大规模模型训练中。因此，本研究旨在降低超参数选择的成本并提高可靠性，解决这些挑战以促进高效模型扩展。",
      "method": "论文提出一种新的参数化方法，适用于具有MoE层的Transformer模型，用于扩展模型宽度、深度、专家数量和专家大小。该方法基于动态平均场理论（DMFT）分析来论证参数化的合理性，确保在模型维度变化时的稳定性。关键创新在于理论支持下的参数化设计，通过固定令牌预算训练不同模型维度，验证了从5100万到超过20亿总参数的模型超参数传递可行性，简化了大规模模型的训练流程。",
      "result": "实验结果表明，提出的参数化方法在模型规模从51M到超过2B总参数时实现了可靠的超参数传递。在固定令牌预算下，通过对不同模型维度的训练，经验验证了该方法的有效性，摘要未明确说明具体性能提升数据，但强调了超参数传递的可靠性，有助于减少调优成本。与基线方法相比，该方法简化了大规模模型的训练过程，提供了更稳定的性能表现。",
      "conclusion": "本研究的主要贡献是提出了一种基于理论分析的参数化方法，简化了Mixture-of-Experts层在Transformer模型中的超参数选择。其学术价值在于结合动态平均场理论论证方法合理性，为模型扩展提供了新思路。实际应用价值在于降低大规模模型训练的超参数调优成本，提高效率。未来工作可扩展到更大模型或不同网络架构，进一步验证方法普适性和潜在局限性。",
      "tags": [
        "Mixture-of-Experts (MoE)",
        "Hyperparameter Transfer",
        "Transformer Models",
        "Dynamical Mean-Field Theory (DMFT)",
        "Neural Network Scaling"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:24.221544Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20203",
    "title": "Minimum-Cost Network Flow with Dual Predictions",
    "authors": [
      "Zhiyang Chen",
      "Hailong Yao",
      "Xia Yin"
    ],
    "abstract": "Recent work has shown that machine-learned predictions can provably improve the performance of classic algorithms. In this work, we propose the first minimum-cost network flow algorithm augmented with a dual prediction. Our method is based on a classic minimum-cost flow algorithm, namely $\\varepsilon$-relaxation. We provide time complexity bounds in terms of the infinity norm prediction error, which is both consistent and robust. We also prove sample complexity bounds for PAC-learning the prediction. We empirically validate our theoretical results on two applications of minimum-cost flow, i.e., traffic networks and chip escape routing, in which we learn a fixed prediction, and a feature-based neural network model to infer the prediction, respectively. Experimental results illustrate $12.74\\times$ and $1.64\\times$ average speedup on two applications.",
    "categories": [
      "cs.LG",
      "cs.DS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20203.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20203",
    "published": "2026-01-28T03:01:22Z",
    "updated": "2026-01-28T03:01:22Z",
    "comment": "accepted by AAAI 2026",
    "light_analysis": {
      "overview": "本文首次提出了一种结合双重预测的最小成本网络流算法，通过机器学习预测改进经典算法的性能。",
      "motivation": "该研究旨在利用机器学习预测优化最小成本网络流问题，因为现有经典算法在处理动态或大规模网络时可能效率不足，而近期工作表明预测能提升算法性能。最小成本流在交通网络和芯片设计等应用中有重要价值，但传统方法未充分利用预测信息，导致潜在的性能瓶颈。通过结合预测，本研究试图提供更高效的解决方案，弥补现有方法的局限性。",
      "method": "方法基于经典的最小成本网络流算法ε-松弛，引入双重预测模块进行增强。核心创新在于将预测误差（无穷范数）与时间复杂度分析结合，提供理论保证，包括时间复杂度和PAC学习框架下的样本复杂度界限。技术细节包括使用固定预测和基于特征的神经网络模型来推断预测值，确保算法的一致性和鲁棒性。这体现了机器学习与优化算法的有效融合。",
      "result": "实验在交通网络和芯片逃逸路由两个应用中进行验证，结果显示新算法相比基线方法实现了显著加速：在交通网络上平均加速12.74倍，在芯片逃逸路由上平均加速1.64倍。这些结果支撑了理论分析，表明双重预测能有效提升算法效率，具体性能指标基于实际应用场景测试，未明确基线细节，但加速效果突出。",
      "conclusion": "本研究的主要贡献是首次提出结合双重预测的最小成本网络流算法，并提供理论分析和实验验证。学术上，它推动了机器学习与经典优化算法的交叉研究，具有创新性；实际中，在交通管理和芯片设计等领域有应用潜力。未来工作可扩展预测模型到其他优化问题，或探索更复杂的预测误差分析，摘要未明确说明具体局限性。",
      "tags": [
        "Minimum-Cost Network Flow",
        "Dual Prediction",
        "ε-relaxation",
        "Machine Learning",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:16.249509Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20198",
    "title": "DeRaDiff: Denoising Time Realignment of Diffusion Models",
    "authors": [
      "Ratnavibusena Don Shahain Manujith",
      "Yang Zhang",
      "Teoh Tze Tzun",
      "Kenji Kawaguchi"
    ],
    "abstract": "Recent advances align diffusion models with human preferences to increase aesthetic appeal and mitigate artifacts and biases. Such methods aim to maximize a conditional output distribution aligned with higher rewards whilst not drifting far from a pretrained prior. This is commonly enforced by KL (Kullback Leibler) regularization. As such, a central issue still remains: how does one choose the right regularization strength? Too high of a strength leads to limited alignment and too low of a strength leads to \"reward hacking\". This renders the task of choosing the correct regularization strength highly non-trivial. Existing approaches sweep over this hyperparameter by aligning a pretrained model at multiple regularization strengths and then choose the best strength. Unfortunately, this is prohibitively expensive. We introduce DeRaDiff, a denoising time realignment procedure that, after aligning a pretrained model once, modulates the regularization strength during sampling to emulate models trained at other regularization strengths without any additional training or finetuning. Extending decoding-time realignment from language to diffusion models, DeRaDiff operates over iterative predictions of continuous latents by replacing the reverse step reference distribution by a geometric mixture of an aligned and reference posterior, thus giving rise to a closed form update under common schedulers and a single tunable parameter, lambda, for on the fly control. Our experiments show that across multiple text image alignment and image-quality metrics, our method consistently provides a strong approximation for models aligned entirely from scratch at different regularization strengths. Thus, our method yields an efficient way to search for the optimal strength, eliminating the need for expensive alignment sweeps and thereby substantially reducing computational costs.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20198.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20198",
    "published": "2026-01-28T02:53:39Z",
    "updated": "2026-01-28T02:53:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "DeRaDiff 是一种无需额外训练即可在采样时调制正则化强度的去噪时间重对齐方法，以提高扩散模型与人类偏好的对齐效果。",
      "motivation": "本研究旨在解决扩散模型对齐人类偏好时正则化强度选择的难题。现有方法使用KL正则化来最大化与高奖励对齐的输出分布，但强度选择不当会导致对齐不足或“奖励黑客”现象。由于需要训练多个模型以扫过不同正则化强度，这一过程计算成本高昂，因此亟需一种更高效的方法来动态调节强度，避免昂贵的超参数搜索。",
      "method": "DeRaDiff 提出了一种去噪时间重对齐过程，通过采样时调制正则化强度来模拟在不同强度下训练的模型，无需额外训练。该方法扩展了语言模型中的解码时间重对齐技术，将扩散模型的反向步骤参考分布替换为对齐后验和参考后验的几何混合。在常见调度器下，这导致闭合形式更新，并引入单参数lambda进行实时控制，操作连续潜变量的迭代预测以实现灵活调节。",
      "result": "实验结果表明，DeRaDiff 在多个文本图像对齐和图像质量指标上，能够强近似于从头对齐的模型在不同正则化强度下的表现。因此，该方法提供了一种高效搜索最优正则化强度的途径，消除了传统方法中对齐扫的昂贵计算需求，显著降低了计算成本，并在对齐任务中展现了稳定性能。",
      "conclusion": "DeRaDiff 的主要贡献在于实现了在采样时动态调节正则化强度，避免了重复训练的昂贵开销。该研究解决了扩散模型对齐中超参数选择的关键问题，提高了模型对齐的效率和实用性。未来工作可探索该方法在其他生成模型或更广泛场景中的应用，以进一步优化性能和扩展适用范围。",
      "tags": [
        "Diffusion Models",
        "KL Regularization",
        "Decoding-time Realignment",
        "Geometric Mixture",
        "Hyperparameter Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:19.222334Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20196",
    "title": "Automated Marine Biofouling Assessment: Benchmarking Computer Vision and Multimodal LLMs on the Level of Fouling Scale",
    "authors": [
      "Brayden Hamilton",
      "Tim Cashmore",
      "Peter Driscoll",
      "Trevor Gee",
      "Henry Williams"
    ],
    "abstract": "Marine biofouling on vessel hulls poses major ecological, economic, and biosecurity risks. Traditional survey methods rely on diver inspections, which are hazardous and limited in scalability. This work investigates automated classification of biofouling severity on the Level of Fouling (LoF) scale using both custom computer vision models and large multimodal language models (LLMs). Convolutional neural networks, transformer-based segmentation, and zero-shot LLMs were evaluated on an expert-labelled dataset from the New Zealand Ministry for Primary Industries. Computer vision models showed high accuracy at extreme LoF categories but struggled with intermediate levels due to dataset imbalance and image framing. LLMs, guided by structured prompts and retrieval, achieved competitive performance without training and provided interpretable outputs. The results demonstrate complementary strengths across approaches and suggest that hybrid methods integrating segmentation coverage with LLM reasoning offer a promising pathway toward scalable and interpretable biofouling assessment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.20196.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20196",
    "published": "2026-01-28T02:46:21Z",
    "updated": "2026-01-28T02:46:21Z",
    "comment": "Australasian Conference on Robotics and Automation, ACRA2025 13 Pages, 8 Figures",
    "light_analysis": {
      "overview": "本文通过评估计算机视觉模型和多模态大语言模型在生物附着严重程度分类上的性能，发现方法互补并提出整合分割与LLM推理的混合方法。",
      "motivation": "海洋生物附着在船体上导致生态破坏、经济损失和生物安全风险。传统调查方法依赖潜水员人工检查，这过程危险且难以大规模应用，限制了监控效率和覆盖范围。为解决这一问题，本研究旨在自动化分类生物附着严重程度，使用Level of Fouling (LoF) 尺度，以提高评估的安全性、可扩展性和准确性，应对现有方法的不足。",
      "method": "论文采用自定义计算机视觉模型，包括卷积神经网络和基于变换器的分割模型，以及大型多模态语言模型（LLMs）。在来自新西兰初级产业部的专家标注数据集上，评估了这些方法在LoF尺度分类任务中的表现。LLMs通过零样本学习和结构化提示、检索技术来实现，无需额外训练，而计算机视觉模型则专注于图像处理和分析，以探索自动化评估的技术路线。",
      "result": "实验结果显示，计算机视觉模型在极端生物附着类别（如高或低严重程度）上达到高准确率，但在中间级别分类时因数据集不平衡和图像框架问题表现较差。相比之下，大型多模态LLMs在结构化提示和检索指导下，实现了与训练模型相竞争的绩效，无需专门训练，并提供可解释的输出，增强了结果的可信度和透明度。这表明不同方法在性能上存在互补性。",
      "conclusion": "论文总结指出，计算机视觉模型和LLMs在生物附着评估中各有优势，具有互补性。整合图像分割的覆盖信息与LLM的推理能力，形成混合方法，为实现可扩展、高效和可解释的自动化生物附着评估提供了有前景的途径。这有助于降低传统方法的成本和风险，推动实际应用，并暗示未来可进一步优化数据集平衡和混合方法集成。",
      "tags": [
        "Computer Vision",
        "Large Language Model",
        "Convolutional Neural Network",
        "Transformer",
        "Zero-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:31.144699Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.20193",
    "title": "Meta-Cognitive Reinforcement Learning with Self-Doubt and Recovery",
    "authors": [
      "Zhipeng Zhang",
      "Wenting Ma",
      "Kai Li",
      "Meng Guo",
      "Lei Yang",
      "Wei Yu",
      "Hongji Cui",
      "Yichen Zhang",
      "Mo Zhang",
      "Jinzhe Lin",
      "Zhenjie Yao"
    ],
    "abstract": "Robust reinforcement learning methods typically focus on suppressing unreliable experiences or corrupted rewards, but they lack the ability to reason about the reliability of their own learning process. As a result, such methods often either overreact to noise by becoming overly conservative or fail catastrophically when uncertainty accumulates.   In this work, we propose a meta-cognitive reinforcement learning framework that enables an agent to assess, regulate, and recover its learning behavior based on internally estimated reliability signals. The proposed method introduces a meta-trust variable driven by Value Prediction Error Stability (VPES), which modulates learning dynamics via fail-safe regulation and gradual trust recovery.   Experiments on continuous-control benchmarks with reward corruption demonstrate that recovery-enabled meta-cognitive control achieves higher average returns and significantly reduces late-stage training failures compared to strong robustness baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20193.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20193",
    "published": "2026-01-28T02:43:03Z",
    "updated": "2026-01-28T02:43:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种元认知强化学习方法，通过VPES驱动的meta-trust变量实现自我怀疑和恢复机制，以提高学习鲁棒性。",
      "motivation": "现有鲁棒强化学习方法通常专注于抑制不可靠经验或奖励损坏，但缺乏推理自身学习过程可靠性的能力，导致智能体在面对噪声时可能过度保守或遭遇灾难性失败。本研究旨在解决这一问题，使智能体能够主动评估、调节和恢复学习行为，以应对不确定性累积的挑战，这对于需要高可靠性的实际应用如噪声环境中的控制任务非常重要。",
      "method": "论文提出了一个元认知强化学习框架，允许智能体基于内部估计的可靠性信号评估、调节和恢复学习行为。核心创新是引入由Value Prediction Error Stability (VPES)驱动的meta-trust变量，该变量通过故障安全调节和渐进信任恢复机制动态调节学习过程。方法在连续控制基准测试中实验，但具体模型架构和数据集摘要未明确说明。",
      "result": "在带有奖励损坏的连续控制基准测试中，与强鲁棒性基线相比，启用恢复的元认知控制方法实现了更高的平均回报，并显著减少了后期训练失败。这些结果表明，该方法在处理不确定性时更鲁棒，提升了学习稳定性和整体性能。",
      "conclusion": "本研究的主要贡献是开发了一种元认知强化学习框架，通过自我怀疑和恢复机制弥补了现有方法在鲁棒性上的不足。学术价值在于为强化学习引入了新的自我调节能力，实际应用价值在于提高智能体在不可靠环境中的学习效率和稳定性。未来工作可探索将该方法扩展到更广泛任务或优化恢复机制。",
      "tags": [
        "Reinforcement Learning",
        "Meta-Cognitive Reinforcement Learning",
        "Value Prediction Error Stability (VPES)",
        "Robust Learning",
        "Self-Doubt and Recovery"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:54.402058Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.19022",
    "title": "EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting",
    "authors": [
      "Antanas Zilinskas",
      "Robert N. Shorten",
      "Jakub Marecek"
    ],
    "abstract": "Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.19022.pdf",
    "abs_url": "https://arxiv.org/abs/2601.19022",
    "published": "2026-01-26T23:15:20Z",
    "updated": "2026-01-28T17:40:06Z",
    "comment": "Updated author affiliation. No changes to technical content",
    "light_analysis": {
      "overview": "本文提出了EVEREST，一个基于Transformer的架构，用于罕见事件时间序列预测，集成证据推理和尾部风险估计，实现概率性预测和不确定性量化。",
      "motivation": "多变量时间序列中罕见事件的预测面临挑战，包括严重类别不平衡、长程依赖关系和分布不确定性，这些问题在空间天气、工业监控等高风险领域至关重要。现有方法可能难以有效处理这些挑战，特别是在不确定性和极端事件的风险建模方面，导致预测准确性不足和风险估计不准确，亟需创新解决方案来提升预测的可靠性和实用性。",
      "method": "EVEREST架构包含四个核心组件：可学习的注意力瓶颈用于软聚合时间动态；证据头部通过Normal--Inverse--Gamma分布估计异质性和认知不确定性；极值头部使用广义帕累托分布建模尾部风险；以及轻量级预兆头部用于早期事件检测。这些模块通过复合损失（包括焦点损失、证据负对数似然和尾部敏感的极值理论惩罚）联合优化，基于Transformer设计，训练时使用全模块，部署时仅用单个分类头，参数约0.81M，确保高效推理。",
      "result": "在十年空间天气数据上，EVEREST对C类耀斑预测中，True Skill Statistic (TSS) 在24小时、48小时和72小时预测时间分别达到0.973、0.970和0.966，优于现有最先进方法，展示了卓越的预测性能。模型参数约0.81M，结构紧凑，可在商用硬件上高效训练，验证了其在处理罕见事件时的有效性和可扩展性。",
      "conclusion": "EVEREST在罕见事件时间序列预测中实现了高准确性和概率性风险估计，为高风险领域如天气预测和工业监控提供了实用工具，具有重要的学术和应用价值。局限性包括依赖固定长度输入和不支持图像模态，未来工作可扩展到流式数据和多模态预测，以增强模型的应用范围和鲁棒性。",
      "tags": [
        "Transformer",
        "Rare-Event Forecasting",
        "Uncertainty Estimation",
        "Tail Risk Modeling",
        "Time-Series Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:05.076656Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18987",
    "title": "LLMs versus the Halting Problem: Revisiting Program Termination Prediction",
    "authors": [
      "Oren Sultan",
      "Jordi Armengol-Estape",
      "Pascal Kesseli",
      "Julien Vanegue",
      "Dafna Shahaf",
      "Yossi Adi",
      "Peter O'Hearn"
    ],
    "abstract": "Determining whether a program terminates is a central problem in computer science. Turing's foundational result established the Halting Problem as undecidable, showing that no algorithm can universally determine termination for all programs and inputs. Consequently, automatic verification tools approximate termination, sometimes failing to prove or disprove; these tools rely on problem-specific architectures and abstractions, and are usually tied to particular programming languages. Recent success and progress in large language models (LLMs) raises the following question: can LLMs reliably predict program termination? In this work, we evaluate LLMs on a diverse set of C programs from the Termination category of the International Competition on Software Verification (SV-Comp) 2025. Our results suggest that LLMs perform remarkably well at predicting program termination, where GPT-5 and Claude Sonnet-4.5 would rank just behind the top-ranked tool (using test-time-scaling), and Code World Model (CWM) would place just behind the second-ranked tool. While LLMs are effective at predicting program termination, they often fail to provide a valid witness as a proof. Moreover, LLMs performance drops as program length increases. We hope these insights motivate further research into program termination and the broader potential of LLMs for reasoning about undecidable problems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.PL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18987.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18987",
    "published": "2026-01-26T21:44:12Z",
    "updated": "2026-01-28T13:02:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "这篇论文首次评估了大型语言模型在程序终止预测任务中的性能，发现其表现接近现有顶级验证工具。",
      "motivation": "程序终止问题是计算机科学的核心挑战，Halting Problem证明其不可判定性，导致现有自动验证工具只能近似处理，常常无法证明或证伪，且依赖于特定编程语言和架构。近年来，大型语言模型的成功在代码理解和生成方面取得进展，引发了对其在程序终止预测中可靠性的探讨。本研究旨在探索LLMs是否能够突破传统限制，为不可判定问题提供新的解决途径，填补现有方法在通用性和可靠性方面的不足。",
      "method": "本研究采用实证评估方法，直接应用多种大型语言模型预测程序终止。数据集来自International Competition on Software Verification (SV-Comp) 2025的终止类别，包含多样化的C程序。评估的模型包括GPT-5、Claude Sonnet-4.5和Code World Model (CWM)，通过排名比较与现有验证工具的性能。关键创新在于将LLMs应用于传统上不可判定的终止问题，摘要未明确说明具体评估细节，但提及测试时规模调整，以模拟实际应用场景，强调了对多样化和语言特定问题的处理。",
      "result": "实验结果表明，大型语言模型在程序终止预测方面表现优异：GPT-5和Claude Sonnet-4.5在排名上接近顶级验证工具（使用测试时规模调整），Code World Model则接近第二名。然而，LLMs往往无法提供有效的证明作为验证依据，且随着程序长度的增加，其预测性能会下降。与SV-Comp中的基线工具相比，LLMs显示出强大的潜力，但存在无法生成证明和长度依赖的局限性，摘要未提供具体准确率数字，仅通过排名突出性能优势。",
      "conclusion": "本研究的结论是，大型语言模型在程序终止预测中具有显著效果，但其无法生成证明且性能受程序长度影响。主要贡献在于验证了LLMs在解决不可判定问题方面的潜力，为软件验证领域提供了新视角。学术价值在于推动了对程序终止问题的深入研究，实际应用可能包括辅助开发工具，局限性包括证明缺失和复杂度依赖。未来工作可专注于提升LLMs的证明生成能力和处理复杂程序的能力，以扩展其在推理任务中的应用范围。",
      "tags": [
        "Large Language Models",
        "Program Termination",
        "Halting Problem",
        "Software Verification",
        "Code World Model"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:20.821613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18944",
    "title": "Neural Theorem Proving for Verification Conditions: A Real-World Benchmark",
    "authors": [
      "Qiyuan Xu",
      "Xiaokun Luan",
      "Renxi Wang",
      "Joshua Ong Jun Leang",
      "Peixin Wang",
      "Haonan Li",
      "Wenda Li",
      "Conrad Watt"
    ],
    "abstract": "Theorem proving is fundamental to program verification, where the automated proof of Verification Conditions (VCs) remains a primary bottleneck. Real-world program verification frequently encounters hard VCs that existing Automated Theorem Provers (ATPs) cannot prove, leading to a critical need for extensive manual proofs that burden practical application. While Neural Theorem Proving (NTP) has achieved significant success in mathematical competitions, demonstrating the potential of machine learning approaches to formal reasoning, its application to program verification--particularly VC proving--remains largely unexplored. Despite existing work on annotation synthesis and verification-related theorem proving, no benchmark has specifically targeted this fundamental bottleneck: automated VC proving. This work introduces Neural Theorem Proving for Verification Conditions (NTP4VC), presenting the first real-world multi-language benchmark for this task. From real-world projects such as Linux and Contiki-OS kernel, our benchmark leverages industrial pipelines (Why3 and Frama-C) to generate semantically equivalent test cases across formal languages of Isabelle, Lean, and Rocq. We evaluate large language models (LLMs), both general-purpose and those fine-tuned for theorem proving, on NTP4VC. Results indicate that although LLMs show promise in VC proving, significant challenges remain for program verification, highlighting a large gap and opportunity for future research.",
    "categories": [
      "cs.AI",
      "cs.PL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18944.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18944",
    "published": "2026-01-26T20:37:11Z",
    "updated": "2026-01-28T18:25:21Z",
    "comment": "Accepted in ICLR'26",
    "light_analysis": {
      "overview": "本论文提出了首个针对验证条件的真实世界多语言基准测试NTP4VC，填补了神经定理证明在程序验证中的应用空白。",
      "motivation": "程序验证中，验证条件的自动证明是主要瓶颈，现有自动定理证明器难以处理实际项目中的难验证条件，导致大量手动证明负担，影响软件可靠性和自动化进程。神经定理证明虽然在数学竞赛中取得进展，展示了机器学习在形式推理中的潜力，但其在程序验证特别是验证条件证明中的应用尚未充分探索，缺乏专门基准来评估和改进方法。本研究旨在解决这一关键问题，通过建立标准化测试环境，推动自动化验证技术的发展。",
      "method": "本工作提出NTP4VC基准测试，其核心方法是从真实项目如Linux和Contiki-OS内核中，利用工业级工具Why3和Frama-C生成语义等价的验证条件测试用例，覆盖多种形式语言包括Isabelle、Lean和Rocq。创新点在于首次构建了多语言、真实世界的基准，并评估了大型语言模型（包括通用型和微调型）在验证条件证明任务上的性能。这提供了标准数据集，便于比较不同方法在自动化证明中的表现。",
      "result": "实验结果显示，大型语言模型在验证条件证明任务上显示出潜力，但尚未能完全解决程序验证的挑战。在NTP4VC基准上的评估表明，LLMs的表现与理想水平有显著差距，突出了现有方法的局限性。摘要未明确说明具体性能指标如准确率，但结果强调了在程序验证中应用神经定理证明仍面临重大困难，为未来研究指明了改进方向。",
      "conclusion": "本研究的核心贡献是提出了首个真实世界多语言基准NTP4VC，为神经定理证明在程序验证中的应用提供了标准化评估平台。这不仅填补了该领域基准的空白，还展示了LLMs在自动化验证条件证明中的初步能力与局限，具有推动自动化验证技术发展的实际价值。局限性包括LLMs性能尚不成熟，未来工作可集中于改进模型架构、结合其他推理方法或扩展基准范围以提升证明效率。",
      "tags": [
        "Neural Theorem Proving",
        "Verification Conditions",
        "Large Language Models",
        "Benchmark",
        "Automated Theorem Proving"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:21.520655Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18891",
    "title": "Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)",
    "authors": [
      "Ghazaleh Serati",
      "Samuel Foucher",
      "Jerome Theau"
    ],
    "abstract": "Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18891.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18891",
    "published": "2026-01-26T19:02:18Z",
    "updated": "2026-01-28T18:12:45Z",
    "comment": "30 pages, 8 figures, submitted to Frontiers in Ecology and Evolution",
    "light_analysis": {
      "overview": "提出一种弱监督补丁级预训练框架，用于增强在北极挑战环境中 caribou 的检测和计数性能。",
      "motivation": "北极地区 caribou 数量近年来持续下降，需要可扩展且准确的监测方法来支持保护决策。现有手动图像解释方法劳动密集且易出错，而自动检测面临背景异质性、类不平衡、目标小或遮挡等挑战，导致现有基于通用权重初始化的检测模型可能不够鲁棒，因此亟需开发更稳健的解决方案以应对这些实际问题。",
      "method": "本文基于检测网络架构（HerdNet）提出弱监督补丁级预训练方法。使用包含五个 caribou 群体分布的数据集，通过从空与非空标签学习，生成弱监督知识以增强检测。关键创新在于利用粗标签进行预训练，提高模型对背景变化和目标特性的适应性，具体包括数据集涵盖多群体图像和独立年份测试集，以评估泛化能力。",
      "result": "在 2017 年和 2019 年测试集上，弱监督预训练网络在补丁级检测中取得高 F1 分数（分别为 93.7% 和 92.6%）。与从 ImageNet 权重初始化的基线相比，在正补丁检测中 F1 从 89.3%/88.6% 提升至 92.6%/93.5%，在全图像计数中从 91.5%/90.4% 提升至 95.5%/93.3%，显著提高了准确性和可靠性，支持动物区域映射以辅助手动计数。",
      "conclusion": "本研究表明，在检测任务前进行弱监督预训练，即使在标签数据有限时，也能有效利用粗标签提升性能。主要贡献是提出了一个鲁棒的弱监督框架，增强了野生动物监控的实际应用价值。局限性包括由类似动物背景引起的假阳性和低密度遮挡导致的假阴性，未来工作可针对这些挑战进一步优化，为环保领域的研究提供参考。",
      "tags": [
        "Weakly Supervised Learning",
        "Patch-Level Pretraining",
        "Object Detection",
        "Transfer Learning",
        "Aerial Imagery Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:18.266189Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18832",
    "title": "The Geometric Reasoner: Manifold-Informed Latent Foresight Search for Long-Context Reasoning",
    "authors": [
      "Ren Zhuang",
      "Ben Wang",
      "Shuifa Sun"
    ],
    "abstract": "Scaling test-time compute enhances long chain-of-thought (CoT) reasoning, yet existing approaches face a fundamental trade-off between computational cost and coverage quality: either incurring high training expense or yielding redundant trajectories. We introduce The Geometric Reasoner (TGR), a training-free framework that performs manifold-informed latent foresight search under strict memory bounds. At each chunk boundary, TGR scores candidate latent anchors via a lightweight look-ahead estimate combined with soft geometric regularizers that encourage smooth trajectories and diverse exploration. Chunk-wise KV cache resets keep memory linear in chunk length. On challenging math and code benchmarks, TGR improves robust trajectory coverage, measured by the area under the Pass@$k$ curve (AUC), by up to 13 points on Qwen3-8B, with negligible overhead of about 1.1--1.3 times.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18832.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18832",
    "published": "2026-01-25T18:16:17Z",
    "updated": "2026-01-28T03:14:37Z",
    "comment": "11 pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出了The Geometric Reasoner (TGR)，一个无需训练的训练框架，通过流形信息的潜在前瞻搜索来增强长上下文推理的覆盖质量。",
      "motivation": "在长链式思维推理中，扩展测试时间计算虽能提升性能，但现有方法面临计算成本与覆盖质量的根本权衡：要么需要高训练费用，要么产生冗余轨迹，限制了实际应用效率。这一问题在数学和代码等复杂任务中尤为重要，因为需要高效且全面的推理方法，以克服资源限制下的覆盖不足，驱动研究开发更优解决方案。",
      "method": "TGR框架无需训练，执行流形信息的潜在前瞻搜索，在严格内存限制下操作。核心方法是在每个块边界，通过轻量级前瞻估计和软几何正则化来评分候选潜在锚点，促进平滑轨迹和多样化探索。技术特色包括使用分块KV缓存重置，以保持内存使用线性于块长度，实现高效、低开销的搜索过程。",
      "result": "在挑战性的数学和代码基准测试中，TGR显著提升了轨迹覆盖质量，以Pass@k曲线下的面积（AUC）衡量，在Qwen3-8B模型上提高了高达13个百分点。同时，开销仅为基线方法的1.1至1.3倍，显示出在覆盖提升的同时保持了高效计算性能，优于现有方法。",
      "conclusion": "本文的主要贡献是提出了TGR框架，有效解决了长上下文推理中计算效率与覆盖质量的权衡问题，学术价值在于引入了流形信息和潜在搜索的创新结合。实际应用适用于需要长推理的任务，如数学和代码生成。未来工作方向摘要未明确说明，可扩展至更多模型或任务。",
      "tags": [
        "Long-Context Reasoning",
        "Chain-of-Thought",
        "Manifold Learning",
        "Latent Space Search",
        "KV Cache Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:07.921619Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17934",
    "title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images",
    "authors": [
      "Vi Vu",
      "Thanh-Huy Nguyen",
      "Tien-Thinh Nguyen",
      "Ba-Thinh Lam",
      "Hoang-Thien Nguyen",
      "Tianyang Wang",
      "Xingjian Li",
      "Min Xu"
    ],
    "abstract": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.17934.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17934",
    "published": "2026-01-25T18:13:48Z",
    "updated": "2026-01-28T18:55:46Z",
    "comment": "Accepted to ISBI 2026",
    "light_analysis": {
      "overview": "SC-SAM框架通过专家-通用者双向协同训练，解锁SAM在未标记医疗图像上的学习潜力，实现高效分割。",
      "motivation": "研究动机在于解决SAM适应医疗图像时的挑战，如领域转移和标签稀缺问题。现有参数高效微调（PEFT）方法无法有效利用未标记数据，而传统U-Net在半监督学习中表现良好，但其辅助潜力未被充分挖掘。这些问题限制了SAM在医疗领域的应用，导致分割效率低下，因此需要新方法提升标签利用率。",
      "method": "论文提出SC-SAM框架，结合专家模型U-Net和通用者模型SAM。U-Net提供点基提示和伪标签指导SAM适应医疗图像，同时SAM作为强大监督者正则化U-Net。这种双向指导形成协同训练循环，使两者都能有效利用未标记数据。关键创新是专家-通用者合作机制，通过互动学习优化性能。",
      "result": "在前列腺MRI和息肉分割基准测试中，SC-SAM取得了最佳结果，优于其他半监督SAM变体，甚至超越了医疗基础模型如MedSAM。实验表明该方法显著提升了分割精度和标签效率，但摘要未明确具体性能指标。",
      "conclusion": "本研究贡献了SC-SAM框架，展示专家-通用者合作在医疗图像分割中的价值，解决了PEFT SAM的局限性。学术上为半监督学习提供新思路，实际中降低标注成本。未来可探索更多医疗应用或扩展其他领域，但摘要未明确说明局限性。",
      "tags": [
        "Segment Anything Model",
        "U-Net",
        "Parameter-Efficient Fine-Tuning",
        "Co-training",
        "Medical Image Segmentation"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:18.837958Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17702",
    "title": "S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference",
    "authors": [
      "Qingsen Ma",
      "Dianyun Wang",
      "Yaoye Wang",
      "Lechen Ning",
      "Sujie Zhu",
      "Xiaohang Zhang",
      "Jiaming Lyu",
      "Linhao Ren",
      "Zhenbo Xu",
      "Zhaofeng He"
    ],
    "abstract": "Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.   We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.   At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.17702.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17702",
    "published": "2026-01-25T05:25:22Z",
    "updated": "2026-01-28T15:54:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了S3-Attention框架，通过注意力对齐的内源性检索实现内存有界的长上下文推理。",
      "motivation": "随着大语言模型越来越多地应用于多文档和长文本输入，长上下文推理面临内存消耗大和噪声效率低的问题。现有方法如键值缓存随上下文长度线性扩展，导致内存需求激增；而外部检索方法虽减少内存但常返回词法相似却因果不相关的段落，影响推理准确性。因此，需要一种更高效的内存管理方法来支持长文本处理，以解决现有方法在内存和检索质量方面的不足。",
      "method": "论文提出S3-Attention框架，将长上下文处理视为注意力对齐的内源性检索。核心方法包括：解码瞬时键和查询投影为top-k稀疏特征标识符，使用轻量级稀疏自动编码器实现稀疏表示；在单次流式扫描中构建基于CPU的倒排索引，映射特征到标记位置或跨度，从而完全丢弃键值缓存并通过扫描块大小限制GPU内存使用。生成时，利用特征共激活检索紧凑证据跨度，并可融合BM25进行精确词法匹配，以增强检索准确性和效率。",
      "result": "在统一的LongBench评估协议下，S3-Hybrid在固定提示、解码和匹配标记预算的条件下，能够接近全上下文推理的效果，并在多个模型家族中保持一致的性能。特别是在信息密集的设置中，它提高了推理的鲁棒性。然而，摘要未明确说明具体性能指标数据；与优化的全键值缓存基线相比，当前原型在工程上存在局限性，其时钟延迟较高，这提示了未来改进方向。",
      "conclusion": "本研究的主要贡献是提出了S3-Attention框架，通过内源性检索显著减少长上下文推理的内存消耗，提供了一种新颖的注意力对齐方法。学术价值在于为长文本处理开辟了新路径，实际应用有助于大语言模型在资源受限环境中的部署。局限是当前原型延迟较高，未来工作可聚焦核级优化以提升性能，并探索更广泛的应用场景。",
      "tags": [
        "Large Language Model",
        "Sparse Autoencoders",
        "Inverted Index",
        "Long-Context Inference",
        "Retrieval-Augmented Generation"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:11.660947Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17367",
    "title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers",
    "authors": [
      "Zecheng Tang",
      "Quantong Qiu",
      "Yi Yang",
      "Zhiyi Hong",
      "Haiya Xiang",
      "Kebin Liu",
      "Qingqing Dang",
      "Juntao Li",
      "Min Zhang"
    ],
    "abstract": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.17367.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17367",
    "published": "2026-01-24T08:22:07Z",
    "updated": "2026-01-28T10:28:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出弹性注意（Elastic Attention），通过动态调整注意力机制的稀疏比率，实现大型语言模型在长上下文场景中的高效推理。",
      "motivation": "标准注意力机制的二次计算复杂性对大型语言模型（LLMs）在长上下文处理中构成可扩展性瓶颈。现有混合注意力策略虽结合稀疏与全注意以降低计算成本，但采用固定计算比率，无法适应下游任务在推理时对稀疏性的动态需求。这使得模型效率受限，尤其在处理不同长度或复杂度的输入时，缺乏灵活性。因此，开发能根据输入特征自适应调整计算模式的注意力机制至关重要，以提升LLMs的实时性能和资源利用率。",
      "method": "本研究提出弹性注意方法，核心在于集成一个轻量级的注意力路由器（Attention Router）到预训练Transformer模型中。路由器在推理时分析输入特征，动态分配每个注意力头到稀疏或全注意计算模式，从而自适应调整整体稀疏比率。该方法仅需在8xA800 GPU上进行12小时短时间训练，即可与现有模型兼容，无需大规模重新训练，实现了计算模式的灵活切换和高效适配。",
      "result": "实验在三个长上下文基准测试中进行，使用广泛的大型语言模型作为基础。结果显示，弹性注意方法在保持强性能的同时，显著提升了推理效率，优于采用静态稀疏比率的基线方法。摘要未明确说明具体准确率或延迟指标，但强调该方法能适应不同任务需求，在减少计算开销方面表现优越，证明了其在实际应用中的有效性和通用性。",
      "conclusion": "本研究的核心贡献是提出弹性注意，解决了静态混合注意力在适应动态任务稀疏敏感性上的不足，为高效Transformer模型提供了新思路。其学术价值在于融合动态路由技术，增强了模型的推理灵活性；实际应用价值体现在长文档处理、对话系统等场景中。未来工作可能包括优化路由器设计、扩展到更多模型架构，或进一步验证在多样化任务上的性能。",
      "tags": [
        "Elastic Attention",
        "Sparse Attention",
        "Dynamic Routing",
        "Large Language Models",
        "Transformers"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:29.785439Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17354",
    "title": "PocketGS: On-Device Training of 3D Gaussian Splatting for High Perceptual Modeling",
    "authors": [
      "Wenzhi Guo",
      "Guangchi Fang",
      "Shu Yang",
      "Bing Wang"
    ],
    "abstract": "Efficient and high-fidelity 3D scene modeling is a long-standing pursuit in computer graphics. While recent 3D Gaussian Splatting (3DGS) methods achieve impressive real-time modeling performance, they rely on resource-unconstrained training assumptions that fail on mobile devices, which are limited by minute-scale training budgets and hardware-available peak-memory. We present PocketGS, a mobile scene modeling paradigm that enables on-device 3DGS training under these tightly coupled constraints while preserving high perceptual fidelity. Our method resolves the fundamental contradictions of standard 3DGS through three co-designed operators: G builds geometry-faithful point-cloud priors; I injects local surface statistics to seed anisotropic Gaussians, thereby reducing early conditioning gaps; and T unrolls alpha compositing with cached intermediates and index-mapped gradient scattering for stable mobile backpropagation. Collectively, these operators satisfy the competing requirements of training efficiency, memory compactness, and modeling fidelity. Extensive experiments demonstrate that PocketGS is able to outperform the powerful mainstream workstation 3DGS baseline to deliver high-quality reconstructions, enabling a fully on-device, practical capture-to-rendering workflow.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.17354.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17354",
    "published": "2026-01-24T07:58:53Z",
    "updated": "2026-01-28T05:29:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "PocketGS提出了一种在移动设备上高效训练3D高斯Splatting的方法，通过协同设计操作克服资源限制，实现高感知保真度的3D场景建模。",
      "motivation": "该研究动机源于移动设备上高效和高保真3D场景建模的需求。现有3D高斯Splatting（3DGS）方法在资源不受限的工作站上表现优异，但假设训练资源充足，这在实际移动设备上不适用，因为移动设备受限于分钟级训练预算和硬件峰值内存。这导致无法在移动端实现实时3D建模，限制了增强现实等应用的潜力，因此研究旨在开发适用于移动设备的训练方法以解决这一矛盾。",
      "method": "PocketGS方法通过三个协同设计的操作符解决标准3DGS的局限性。G操作构建几何忠诚的点云先验，为训练提供基础；I操作注入局部表面统计，初始化各向异性高斯，减少早期训练条件差距；T操作优化反向传播过程，使用缓存的中间体和索引映射梯度散射，确保在移动设备上的稳定性和效率。这些操作共同优化了内存使用和计算稳定性，使3DGS适应移动环境的资源约束，提升了训练效率和建模质量。",
      "result": "实验结果表明，PocketGS能够超越强大的主流工作站3DGS基线方法，提供高质量的3D重建。摘要提到该方法实现了完全在设备上的捕捉到渲染工作流程，展示了在实际应用中的可行性。然而，摘要未明确说明具体的性能指标如准确率或效率提升的数值，仅指出其在资源限制下优于基线，需进一步查阅论文以获取详细数据。",
      "conclusion": "PocketGS的主要贡献是提出了一种在移动设备上高效训练3D高斯Splatting的方法，解决了资源限制与建模保真度之间的冲突。其学术价值在于为移动3D建模提供了新的技术路线，实际应用价值体现在支持完全在设备上的3D捕捉和渲染，提升了移动端增强现实等应用的潜力。未来工作可能涉及进一步优化或扩展到更广泛的场景，但摘要未明确讨论局限性，可推断可能存在通用性或计算复杂性方面的挑战。",
      "tags": [
        "3D Gaussian Splatting",
        "On-Device Training",
        "Memory Optimization",
        "Mobile Computing",
        "Surface Statistics"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:35.222413Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17261",
    "title": "AGZO: Activation-Guided Zeroth-Order Optimization for LLM Fine-Tuning",
    "authors": [
      "Wei Lin",
      "Yining Jiang",
      "Qingyu Song",
      "Qiao Xiang",
      "Hong Xu"
    ],
    "abstract": "Zeroth-Order (ZO) optimization has emerged as a promising solution for fine-tuning LLMs under strict memory constraints, as it avoids the prohibitive memory cost of storing activations for backpropagation. However, existing ZO methods typically employ isotropic perturbations, neglecting the rich structural information available during the forward pass. In this paper, we identify a crucial link between gradient formation and activation structure: the gradient of a linear layer is confined to the subspace spanned by its input activations. Leveraging this insight, we propose Activation-Guided Zeroth-Order optimization (AGZO). Unlike prior methods, AGZO extracts a compact, activation-informed subspace on the fly during the forward pass and restricts perturbations to this low-rank subspace. We provide a theoretical framework showing that AGZO optimizes a subspace-smoothed objective and provably yields update directions with higher cosine similarity to the true gradient than isotropic baselines. Empirically, we evaluate AGZO on Qwen3 and Pangu models across various benchmarks. AGZO consistently outperforms state-of-the-art ZO baselines and significantly narrows the performance gap with first-order fine-tuning, while maintaining almost the same peak memory footprint as other ZO methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.17261.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17261",
    "published": "2026-01-24T02:28:15Z",
    "updated": "2026-01-28T04:26:27Z",
    "comment": "21 pages in total, including 9 pages of main text, with 4 figures and 3 tables. This manuscript is submitted to arXiv",
    "light_analysis": {
      "overview": "论文提出AGZO方法，一种基于激活引导的零阶优化技术，用于在内存约束下高效微调大型语言模型，通过限制扰动到低秩子空间提升性能。",
      "motivation": "在大型语言模型（LLMs）的微调中，内存限制是一个关键问题，因为反向传播需要存储激活值，导致高内存成本。零阶优化（ZO）方法可以避免这一点，但现有ZO方法通常采用各向同性扰动，未能利用前向传播中的激活结构信息，从而导致梯度估计效率低下和性能不足。这问题重要，因为内存高效优化对于资源受限环境至关重要。因此，开发一种能够捕捉激活结构的新型ZO方法具有重要意义，以解决现有方法的缺陷。",
      "method": "AGZO方法的核心在于发现梯度与激活结构的关联：线性层的梯度局限于其输入激活张成的子空间。基于此，AGZO在前向传播过程中动态提取一个紧凑的、基于激活信息的低秩子空间，并将扰动限制在该子空间内。关键创新包括激活引导的扰动设计和理论框架，该框架证明AGZO优化了子空间平滑目标，并确保更新方向与真实梯度有更高的余弦相似度。使用了Qwen3和Pangu模型进行实证评估，但摘要未明确说明具体模型架构细节。",
      "result": "实验在Qwen3和Pangu模型上进行，涵盖多个基准测试。AGZO consistently优于最新的零阶优化基线方法，并显著缩小了与一阶微调的性能差距，具体指标摘要未明确说明。同时，AGZO保持了与其他ZO方法几乎相同的峰值内存占用，证实了其在内存效率下的优越性能。这突出了AGZO在减少内存占用的同时提升性能的有效性，与基线方法形成鲜明对比。",
      "conclusion": "AGZO通过整合激活结构信息，提出了一种创新的零阶优化方法，有效提升了LLM微调的效率和性能。其理论贡献在于建立了梯度子空间理论，实践意义在于在严格内存约束下实现接近一阶微调的效果。学术价值在于为高效优化提供新思路，应用价值在资源受限场景。未来工作可探索在其他模型或任务中的应用，并进一步优化扰动策略，但摘要未明确说明具体局限性。",
      "tags": [
        "Zero-Order Optimization",
        "LLM Fine-Tuning",
        "Activation-Guided Subspace",
        "Low-Rank Perturbation",
        "Gradient Approximation"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:40.766966Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.17196",
    "title": "Accelerated Sinkhorn Algorithms for Partial Optimal Transport",
    "authors": [
      "Nghia Thu Truong",
      "Qui Phu Pham",
      "Quang Nguyen",
      "Dung Luong",
      "Mai Tran"
    ],
    "abstract": "Partial Optimal Transport (POT) addresses the problem of transporting only a fraction of the total mass between two distributions, making it suitable when marginals have unequal size or contain outliers. While Sinkhorn-based methods are widely used, their complexity bounds for POT remain suboptimal and can limit scalability. We introduce Accelerated Sinkhorn for POT (ASPOT), which integrates alternating minimization with Nesterov-style acceleration in the POT setting, yielding a complexity of $\\mathcal{O}(n^{7/3}\\varepsilon^{-5/3})$. We also show that an informed choice of the entropic parameter $γ$ improves rates for the classical Sinkhorn method. Experiments on real-world applications validate our theories and demonstrate the favorable performance of our proposed methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.17196.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17196",
    "published": "2026-01-23T21:55:27Z",
    "updated": "2026-01-28T06:32:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了加速Sinkhorn算法ASPOT，用于偏最优运输问题，通过结合交替最小化和Nesterov风格加速，显著降低了计算复杂度。",
      "motivation": "偏最优运输（POT）旨在处理两个分布之间仅运输部分总质量的问题，适用于边际大小不等或包含异常值的场景，如数据对齐和异常检测。然而，尽管基于Sinkhorn的方法广泛应用，但其在POT中的复杂度界限仍不理想，限制了在大规模问题上的可扩展性，因此本研究旨在优化这些方法的效率并提升实用性。",
      "method": "论文提出加速Sinkhorn算法ASPOT，核心创新在于将交替最小化与Nesterov风格加速集成到偏最优运输框架中，实现了复杂度为O(n^(7/3)ε^(-5/3))的新算法。此外，通过明智选择熵参数γ，优化了经典Sinkhorn方法的收敛速率，而不依赖特定数据集或模型架构，聚焦于算法设计改进。",
      "result": "实验在真实世界应用中进行，验证了所提方法的理论效果，表明ASPOT算法在性能上优于传统Sinkhorn方法，并通过优化熵参数提升了收敛速率。然而，摘要未明确说明具体的性能指标对比数据，如准确率或效率提升百分比。",
      "conclusion": "本研究的主要贡献是提出了ASPOT算法，降低了偏最优运输问题的计算复杂度，并改进了经典Sinkhorn方法，具有重要的学术价值，为高效处理大规模数据提供了新思路，在实际应用中展现出潜力。摘要未明确说明潜在局限性或未来工作方向。",
      "tags": [
        "Partial Optimal Transport",
        "Sinkhorn Algorithm",
        "Nesterov Acceleration",
        "Alternating Minimization",
        "Entropic Parameter"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:25.461195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16836",
    "title": "ColorConceptBench: A Benchmark for Probabilistic Color-Concept Understanding in Text-to-Image Models",
    "authors": [
      "Chenxi Ruan",
      "Yu Xiao",
      "Yihan Hou",
      "Guosheng Hu",
      "Wei Zeng"
    ],
    "abstract": "While text-to-image (T2I) models have advanced considerably, their capability to associate colors with implicit concepts remains underexplored. To address the gap, we introduce ColorConceptBench, a new human-annotated benchmark to systematically evaluate color-concept associations through the lens of probabilistic color distributions. ColorConceptBench moves beyond explicit color names or codes by probing how models translate 1,281 implicit color concepts using a foundation of 6,369 human annotations. Our evaluation of seven leading T2I models reveals that current models lack sensitivity to abstract semantics, and crucially, this limitation appears resistant to standard interventions (e.g., scaling and guidance). This demonstrates that achieving human-like color semantics requires more than larger models, but demands a fundamental shift in how models learn and represent implicit meaning.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.16836.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16836",
    "published": "2026-01-23T15:36:02Z",
    "updated": "2026-01-28T05:15:48Z",
    "comment": "8 pages, 5 figures",
    "light_analysis": {
      "overview": "提出ColorConceptBench基准，系统性评估文本到图像模型在隐含颜色-概念关联上的概率分布理解。",
      "motivation": "研究动机源于文本到图像模型在颜色与隐含概念关联方面的进展不足，尽管模型技术先进，但对抽象语义的敏感性仍未被充分探索。当前问题在于现有模型主要依赖明确颜色名称或代码，忽略了颜色与抽象概念（如情感或文化含义）的深层关联，这在图像生成的语义准确性和自然度中至关重要。背景表明，这种关联影响用户体验和模型泛化能力，但现有方法缺乏系统评估工具，导致改进受限，未能有效量化模型理解隐含语义的能力。",
      "method": "论文的核心方法是引入ColorConceptBench基准，该基准基于1,281个隐含颜色概念和6,369个人类标注，通过概率颜色分布来评估模型性能。关键创新在于移除了明确颜色名称或代码，专注于模型如何翻译抽象概念，使用人类标注作为基础来系统量化颜色-概念关联。技术特色包括评估七个领先文本到图像模型，以探索其抽象语义理解能力，但摘要未详细说明具体模型架构或数据集预处理细节，主要依赖于概率分布分析框架。",
      "result": "评估了七个领先文本到图像模型，结果表明它们普遍缺乏对抽象语义的敏感性，在隐含颜色-概念关联上表现不佳。实验显示这一局限性对标准干预（如扩大模型规模或增加指导）显示出抵抗力，突显模型性能的瓶颈。与基线相比，模型在概率分布理解上存在明显差距，揭示了现有技术的不足，但摘要未明确说明具体性能指标如准确率或效率提升。结果强调了需要更根本的改进，而不仅仅是技术调整。",
      "conclusion": "论文的主要贡献是提出ColorConceptBench基准，并揭示了文本到图像模型在隐含颜色-概念理解上的局限性。研究意义在于指出达到人类类似颜色语义需要模型学习和表示方式的根本性转变，而不仅仅是扩大规模。学术价值在于提供了系统评估工具，实际应用价值在于指导未来模型在语义准确性方面的改进。局限性包括当前模型对抽象语义不敏感，未来工作方向可能涉及开发新的学习机制或结合更丰富的标注数据来增强隐含意义理解。",
      "tags": [
        "Text-to-Image Models",
        "Benchmark Evaluation",
        "Color-Concept Association",
        "Probabilistic Distributions",
        "Human Annotations"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:37.032935Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16669",
    "title": "PLawBench: A Rubric-Based Benchmark for Evaluating LLMs in Real-World Legal Practice",
    "authors": [
      "Yuzhen Shi",
      "Huanghai Liu",
      "Yiran Hu",
      "Gaojie Song",
      "Xinran Xu",
      "Yubo Ma",
      "Tianyi Tang",
      "Li Zhang",
      "Qingjing Chen",
      "Di Feng",
      "Wenbo Lv",
      "Weiheng Wu",
      "Kexin Yang",
      "Sen Yang",
      "Wei Wang",
      "Rongyao Shi",
      "Yuanyang Qiu",
      "Yuemeng Qi",
      "Jingwen Zhang",
      "Xiaoyu Sui",
      "Yifan Chen",
      "Yi Zhang",
      "An Yang",
      "Bowen Yu",
      "Dayiheng Liu",
      "Junyang Lin",
      "Weixing Shen",
      "Bing Zhao",
      "Charles L. A. Clarke",
      "Hu Wei"
    ],
    "abstract": "As large language models (LLMs) are increasingly applied to legal domain-specific tasks, evaluating their ability to perform legal work in real-world settings has become essential. However, existing legal benchmarks rely on simplified and highly standardized tasks, failing to capture the ambiguity, complexity, and reasoning demands of real legal practice. Moreover, prior evaluations often adopt coarse, single-dimensional metrics and do not explicitly assess fine-grained legal reasoning. To address these limitations, we introduce PLawBench, a Practical Law Benchmark designed to evaluate LLMs in realistic legal practice scenarios. Grounded in real-world legal workflows, PLawBench models the core processes of legal practitioners through three task categories: public legal consultation, practical case analysis, and legal document generation. These tasks assess a model's ability to identify legal issues and key facts, perform structured legal reasoning, and generate legally coherent documents. PLawBench comprises 850 questions across 13 practical legal scenarios, with each question accompanied by expert-designed evaluation rubrics, resulting in approximately 12,500 rubric items for fine-grained assessment. Using an LLM-based evaluator aligned with human expert judgments, we evaluate 10 state-of-the-art LLMs. Experimental results show that none achieves strong performance on PLawBench, revealing substantial limitations in the fine-grained legal reasoning capabilities of current LLMs and highlighting important directions for future evaluation and development of legal LLMs. Data is available at: https://github.com/skylenage/PLawbench.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.16669.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16669",
    "published": "2026-01-23T11:36:10Z",
    "updated": "2026-01-28T12:26:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了PLawBench基准，通过基于真实法律工作流程的任务和专家评价标准，评估大型语言模型在法律实践中的能力。",
      "motivation": "随着大型语言模型越来越多地应用于特定法律任务，评估其在真实法律环境中的能力变得至关重要。然而，现有法律基准过于简化，依赖标准化任务，无法捕捉真实法律实践的模糊性、复杂性和推理需求。先前评估常使用粗粒度、单维度的指标，未能明确评估细粒度的法律推理，这限制了模型的实用性和改进方向。因此，需要开发更全面的基准来准确评估LLMs在法律领域的性能，解决真实应用中的挑战。",
      "method": "本文提出PLawBench，这是一个基于真实法律工作流程设计的基准，通过三个任务类别（公共法律咨询、实践案例分析和法律文档生成）模拟法律从业者的核心过程。这些任务旨在评估模型识别法律问题和关键事实、进行结构化法律推理以及生成法律连贯文档的能力。基准包含850个问题，覆盖13个实际法律场景，每个问题配有专家设计的评价标准，共计约12,500个评价项目进行细粒度评估。采用与人类专家判断对齐的LLM-based评价器进行性能评估。",
      "result": "作者评估了10个最先进的大型语言模型。实验结果表明，这些模型在PLawBench上均未达到强性能水平，揭示了它们在细粒度法律推理能力上的显著局限性。基准包含850个问题和13个场景，提供了详细的评价标准进行多方面评估。与现有基准相比，PLawBench更全面地评估了模型的真实法律能力，突显了当前LLMs在法律实践应用中的不足。摘要未明确提供具体性能数据对比，但强调了模型在复杂任务中表现不佳，为未来改进提供依据。",
      "conclusion": "本文的主要贡献是提出了PLawBench基准，为评估大型语言模型在真实法律实践中的能力提供了全面框架，强调细粒度评估的重要性。学术上，该基准揭示了当前LLMs在法律推理方面的不足，为未来的研究和模型开发指明方向。实际应用中，有助于推动法律AI的改进，提升模型在复杂任务中的适用性。局限性或未来工作方向可包括扩展基准到更多法律领域或改进评估方法，但摘要未明确说明具体细节。",
      "tags": [
        "Large Language Model",
        "Legal Benchmark",
        "Evaluation Rubrics",
        "Legal Reasoning",
        "LLM-based Evaluator"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:33.950557Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.15550",
    "title": "Common to Whom? Regional Cultural Commonsense and LLM Bias in India",
    "authors": [
      "Sangmitra Madhusudan",
      "Trush Shashank More",
      "Steph Buongiorno",
      "Renata Dividino",
      "Jad Kabbara",
      "Ali Emami"
    ],
    "abstract": "Existing cultural commonsense benchmarks treat nations as monolithic, assuming uniform practices within national boundaries. But does cultural commonsense hold uniformly within a nation, or does it vary at the sub-national level? We introduce Indica, the first benchmark designed to test LLMs' ability to address this question, focusing on India - a nation of 28 states, 8 union territories, and 22 official languages. We collect human-annotated answers from five Indian regions (North, South, East, West, and Central) across 515 questions spanning 8 domains of everyday life, yielding 1,630 region-specific question-answer pairs. Strikingly, only 39.4% of questions elicit agreement across all five regions, demonstrating that cultural commonsense in India is predominantly regional, not national. We evaluate eight state-of-the-art LLMs and find two critical gaps: models achieve only 13.4%-20.9% accuracy on region-specific questions, and they exhibit geographic bias, over-selecting Central and North India as the \"default\" (selected 30-40% more often than expected) while under-representing East and West. Beyond India, our methodology provides a generalizable framework for evaluating cultural commonsense in any culturally heterogeneous nation, from question design grounded in anthropological taxonomy, to regional data collection, to bias measurement.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.15550.pdf",
    "abs_url": "https://arxiv.org/abs/2601.15550",
    "published": "2026-01-22T00:44:26Z",
    "updated": "2026-01-28T15:00:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Indica基准，首次评估LLMs在印度区域文化常识上的表现，揭示了显著的地区差异和模型偏见。",
      "motivation": "现有文化常识基准通常将国家视为单一实体，假设文化实践在国家内部一致，但这忽略了文化多样性国家的地区差异。以印度为例，作为一个拥有28个邦、8个联合领土和22种官方语言的国家，文化常识可能因区域而异，这可能导致大型语言模型产生偏见或错误理解。因此，本研究旨在探讨文化常识是否在次国家级别上变化，并评估LLMs处理这种多样性的能力，以提高模型的公平性和适用性，弥补现有方法的不足。",
      "method": "论文引入Indica基准，通过收集人类标注答案来测试LLMs。方法包括设计涵盖8个日常生活领域的515个问题，从印度五个区域（北、南、东、西、中）收集数据，形成1,630个区域特定问题-答案对。关键创新在于基于人类学分类法的问题设计和区域数据收集，以捕捉文化差异。评估了8个先进的LLMs，使用准确率和偏见测量指标，并提供了一个可泛化的框架，用于任何文化异质国家的文化常识评估。",
      "result": "实验结果显示，仅39.4%的问题在所有五个印度区域得到一致答案，表明文化常识在印度主要是地区性的而非全国性的。评估的LLMs在区域特定问题上准确率较低，仅为13.4%-20.9%，远低于理想水平。模型表现出地理偏见，过度选择中北部地区作为“默认”答案，频率超出预期30-40%，而东部和西部地区被低估。这些结果突出了LLMs在处理区域文化常识时的不足和偏差，与基线或理想情况相比存在显著差距。",
      "conclusion": "该研究通过Indica基准揭示了印度文化常识的地区差异，并暴露了LLMs在区域文化理解上的偏见和性能不足。其主要贡献在于首个区域文化常识基准和可泛化的评估方法论，具有学术价值，促进了AI文化多样性和公平性的研究，并提供实际应用框架。未来工作可扩展到更多文化异质国家或深入分析偏见来源，尽管摘要未明确说明局限性，但本研究为后续探索奠定了基础。",
      "tags": [
        "Cultural Commonsense Benchmark",
        "Large Language Model Evaluation",
        "Regional Bias Analysis",
        "Human Annotation",
        "Anthropological Taxonomy"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:33.421313Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.14417",
    "title": "Quantifying Speaker Embedding Phonological Rule Interactions in Accented Speech Synthesis",
    "authors": [
      "Thanathai Lertpetchpun",
      "Yoonjeong Lee",
      "Thanapat Trachu",
      "Jihwan Lee",
      "Tiantian Feng",
      "Dani Byrd",
      "Shrikanth Narayanan"
    ],
    "abstract": "Many spoken languages, including English, exhibit wide variation in dialects and accents, making accent control an important capability for flexible text-to-speech (TTS) models. Current TTS systems typically generate accented speech by conditioning on speaker embeddings associated with specific accents. While effective, this approach offers limited interpretability and controllability, as embeddings also encode traits such as timbre and emotion. In this study, we analyze the interaction between speaker embeddings and linguistically motivated phonological rules in accented speech synthesis. Using American and British English as a case study, we implement rules for flapping, rhoticity, and vowel correspondences. We propose the phoneme shift rate (PSR), a novel metric quantifying how strongly embeddings preserve or override rule-based transformations. Experiments show that combining rules with embeddings yields more authentic accents, while embeddings can attenuate or overwrite rules, revealing entanglement between accent and speaker identity. Our findings highlight rules as a lever for accent control and a framework for evaluating disentanglement in speech generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.14417.pdf",
    "abs_url": "https://arxiv.org/abs/2601.14417",
    "published": "2026-01-20T19:25:33Z",
    "updated": "2026-01-28T03:59:49Z",
    "comment": "Accepted to ICASSP2026",
    "light_analysis": {
      "overview": "该论文通过量化说话人嵌入与语音学规则的交互，提出音素转移率指标，为口音语音合成提供了可解释的控制框架。",
      "motivation": "研究动机是解决当前文本到语音系统中口音控制的问题。由于许多语言存在广泛的方言和口音变化，口音控制在灵活TTS模型中至关重要。现有方法通过说话人嵌入控制口音，但嵌入同时编码了音色和情感等特征，导致解释性和可控性有限，难以精确调整口音而不影响其他说话人属性。这限制了生成个性化、自然口音语音的能力。因此，该研究旨在分析嵌入与规则的交互，以改进口音控制。",
      "method": "研究方法包括分析说话人嵌入与语音学规则在口音语音合成中的交互。以美式和英式英语为案例，实现具体规则如flapping、rhoticity和元音对应。核心创新是提出音素转移率指标，量化嵌入如何保持或覆盖基于规则的转换。技术路线结合规则与嵌入进行实验，评估它们对语音生成的影响，从而揭示口音与说话人身份之间的纠缠关系。",
      "result": "实验结果表明，结合语音学规则与说话人嵌入能产生更真实的口音语音。嵌入可以减弱或覆盖规则的效果，这揭示了口音与说话人身份之间的纠缠。与基线方法相比，这种方法提升了口音的真实性，但摘要未明确说明具体性能指标如准确率或效率改进。研究通过音素转移率指标量化了这种交互，为评估语音生成中的纠缠提供了新视角。",
      "conclusion": "结论强调语音学规则作为口音控制的有效手段，并提出一个框架来评估语音生成中的特征纠缠。研究的学术价值在于增强了TTS系统的可解释性和可控性，为口音定制化提供了理论基础。实际应用价值包括改进多语种语音合成和个性化语音服务。潜在局限性可能在于规则通用性或数据集范围，未来工作可扩展至更多语言或结合深度学习方法。",
      "tags": [
        "Speaker Embedding",
        "Phonological Rules",
        "Text-to-Speech",
        "Accent Synthesis",
        "Disentanglement"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:37.572269Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.14180",
    "title": "Progressive $\\mathcal{J}$-Invariant Self-supervised Learning for Low-Dose CT Denoising",
    "authors": [
      "Yichao Liu",
      "Zongru Shao",
      "Yueyang Teng",
      "Junwen Guo"
    ],
    "abstract": "Self-supervised learning has been increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to collect. However, many existing self-supervised blind-spot denoising methods suffer from training inefficiencies and suboptimal performance due to restricted receptive fields. To mitigate this issue, we propose a novel Progressive $\\mathcal{J}$-invariant Learning that maximizes the use of $\\mathcal{J}$-invariant to enhance LDCT denoising performance. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained learning for denoising. Furthermore, we explicitly inject a combination of controlled Gaussian and Poisson noise during training to regularize the denoising process and mitigate overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.14180.pdf",
    "abs_url": "https://arxiv.org/abs/2601.14180",
    "published": "2026-01-20T17:35:02Z",
    "updated": "2026-01-28T15:46:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出渐进μ-\\mathcal{J}μ不变自监督学习方法，通过渐进盲点去噪机制和噪声注入正则化，提升低剂量CT图像去噪性能。",
      "motivation": "低剂量CT扫描可减少患者辐射暴露，但图像噪声高，影响诊断准确性。自监督学习能减少对配对正常剂量CT数据的依赖，解决数据收集难题。然而，现有自监督盲点去噪方法因受限感受野，导致训练效率低和性能不佳，亟需改进方法以提升去噪效果和效率。",
      "method": "方法核心是渐进μ-\\mathcal{J}μ不变学习，最大化利用μ-\\mathcal{J}μ不变性增强去噪。引入逐步盲点去噪机制，以渐进方式强制条件独立性，实现更细粒度学习。此外，训练中显式注入混合高斯和泊松噪声进行正则化，以减轻过拟合。使用Mayo LDCT数据集验证技术。",
      "result": "在Mayo LDCT数据集上的实验表明，该方法持续优于现有自监督去噪方法，并与几种代表性有监督方法达到相当或更好的性能。对比基线，去噪效果显著提升，具体数值需参考原论文，但摘要强调其优越性。",
      "conclusion": "本研究提出了一种高效的自监督学习框架，通过渐进学习和噪声正则化，有效提升低剂量CT去噪性能，减少数据依赖。学术上为自监督图像处理提供新思路，应用上助于改善临床图像质量。未来可探索更广泛的噪声模型或扩展到其他医学图像任务。",
      "tags": [
        "Self-supervised Learning",
        "Blind-spot Denoising",
        "μ-\\mathcal{J}μ-invariant Learning",
        "Noise Injection",
        "Low-Dose CT Denoising"
      ]
    },
    "analyzed_at": "2026-01-29T04:03:47.098856Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.14172",
    "title": "Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum",
    "authors": [
      "Víctor Yeste",
      "Paolo Rosso"
    ],
    "abstract": "We study sentence-level detection of the 19 human values in the refined Schwartz continuum in about 74k English sentences from news and political manifestos (ValueEval'24 corpus). Each sentence is annotated with value presence, yielding a binary moral-presence label and a 19-way multi-label task under severe class imbalance. First, we show that moral presence is learnable from single sentences: a DeBERTa-base classifier attains positive-class F1 = 0.74 with calibrated thresholds. Second, we compare direct multi-label value detectors with presence-gated hierarchies under a single 8 GB GPU budget. Under matched compute, presence gating does not improve over direct prediction, indicating that gate recall becomes a bottleneck. Third, we investigate lightweight auxiliary signals - short-range context, LIWC-22 and moral lexica, and topic features - and small ensembles. Our best supervised configuration, a soft-voting ensemble of DeBERTa-based models enriched with such signals, reaches macro-F1 = 0.332 on the 19 values, improving over the best previous English-only baseline on this corpus (macro-F1 $\\approx$ 0.28). We additionally benchmark 7-9B instruction-tuned LLMs (Gemma 2 9B, Llama 3.1 8B, Mistral 8B, Qwen 2.5 7B) in zero-/few-shot and QLoRA setups, and find that they lag behind the supervised ensemble under the same hardware constraint. Overall, our results provide empirical guidance for building compute-efficient, value-aware NLP models under realistic GPU budgets.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.14172.pdf",
    "abs_url": "https://arxiv.org/abs/2601.14172",
    "published": "2026-01-20T17:25:33Z",
    "updated": "2026-01-28T11:00:12Z",
    "comment": "Code: https://github.com/VictorMYeste/human-value-detection, models: https://huggingface.co/papers/2601.14172, 47 pages, 4 figures",
    "light_analysis": {
      "overview": "该论文提出一个基于DeBERTa的监督集成模型，用于句子级别的人类价值检测，在有限GPU预算下优于基线和大语言模型。",
      "motivation": "该研究旨在解决在新闻和政治宣言文本中自动检测人类价值的挑战，这些任务有助于理解道德立场，但面临严重的类不平衡问题。现有方法在计算效率和资源利用方面可能不足，尤其是在有限的硬件资源（如8 GB GPU）下。因此，研究探索如何在现实约束下构建高效的价值感知NLP模型，以改进多标签分类任务。",
      "method": "研究方法包括使用DeBERTa-base分类器进行初步道德存在检测，并比较直接多标签预测与存在门控层次结构。核心创新是构建一个软投票集成模型，结合多个基于DeBERTa的模型，并集成轻量级辅助信号，如短范围上下文、LIWC-22词典和主题特征。实验在ValueEval'24语料库的约74k英语句子上进行，严格控制计算资源在单8 GB GPU预算内。",
      "result": "实验结果显示，DeBERTa-base分类器在道德存在检测上达到正类F1 0.74。在19个价值的多标签检测中，最佳监督集成模型获得macro-F1 0.332，比之前最佳英文基线（约0.28）提升约0.052。在相同硬件约束下，该监督集成模型优于所有测试的7-9B指令调优大语言模型（如Gemma 2 9B和Llama 3.1 8B）的零/少射和QLoRA设置。",
      "conclusion": "该研究的主要贡献是为在现实GPU预算下构建计算高效的价值感知NLP模型提供了经验指导。学术上，证明了监督方法在资源受限环境中仍能有效处理复杂任务，挑战大语言模型的优势。实际应用包括文本分析和道德推理。未来工作可能涉及扩展到多语言数据或进一步优化模型效率。",
      "tags": [
        "DeBERTa",
        "Ensemble Learning",
        "Multi-label Classification",
        "Human Values Detection",
        "Schwartz Continuum"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:09.361135Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18811",
    "title": "Variational Quantum Circuit-Based Reinforcement Learning for Dynamic Portfolio Optimization",
    "authors": [
      "Vincent Gurgul",
      "Ying Chen",
      "Stefan Lessmann"
    ],
    "abstract": "This paper presents a Quantum Reinforcement Learning (QRL) solution to the dynamic portfolio optimization problem based on Variational Quantum Circuits. The implemented QRL approaches are quantum analogues of the classical neural-network-based Deep Deterministic Policy Gradient and Deep Q-Network algorithms. Through an empirical evaluation on real-world financial data, we show that our quantum agents achieve risk-adjusted performance comparable to, and in some cases exceeding, that of classical Deep RL models with several orders of magnitude more parameters. However, while quantum circuit execution is inherently fast at the hardware level, practical deployment on cloud-based quantum systems introduces substantial latency, making end-to-end runtime currently dominated by infrastructural overhead and limiting practical applicability. Taken together, our results suggest that QRL is theoretically competitive with state-of-the-art classical reinforcement learning and may become practically advantageous as deployment overheads diminish. This positions QRL as a promising paradigm for dynamic decision-making in complex, high-dimensional, and non-stationary environments such as financial markets. The complete codebase is released as open source at: https://github.com/VincentGurgul/qrl-dpo-public",
    "categories": [
      "cs.LG",
      "q-fin.CP",
      "q-fin.PM",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18811.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18811",
    "published": "2026-01-20T15:17:24Z",
    "updated": "2026-01-28T11:57:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于变分量子电路的量子强化学习方法，用于动态投资组合优化，性能在风险调整后可与经典深度强化学习模型竞争甚至超越。",
      "motivation": "动态投资组合优化在金融市场中至关重要，因为它涉及复杂、高维和非平稳的环境，导致传统方法难以高效处理。现有经典深度强化学习模型通常需要大量参数，可能带来计算效率低和泛化能力不足的问题。本研究旨在探索量子计算的潜力，通过量子强化学习来克服经典方法的这些限制，为金融决策提供更优的解决方案，推动复杂环境下的动态优化技术的发展。",
      "method": "本研究采用基于变分量子电路的量子强化学习（QRL）方法，具体实现了经典深度确定性策略梯度（DDPG）和深度 Q 网络（DQN）算法的量子模拟版本。通过设计变分量子电路来近似神经网络的决策过程，在真实金融数据集上进行训练和评估。关键创新点在于将经典强化学习算法量子化，利用量子电路的计算优势，以期在硬件层面加速处理，为动态投资组合问题提供新工具。",
      "result": "在真实金融数据的实证评估中，量子智能体实现了风险调整后性能与经典深度强化学习模型相当，有时甚至更好，尽管经典模型的参数数量多出几个数量级。具体表现为量子方法在性能指标上竞争力强，但摘要未明确说明具体数值。然而，量子电路在硬件执行上虽快，但云端部署引入了高延迟，导致端到端运行时间主要由基础设施开销主导，目前限制了实际应用性。",
      "conclusion": "本研究的主要贡献是展示了量子强化学习在动态投资组合优化中的理论竞争力，并可能随着量子计算部署开销的减少而变得实用。学术上，它为复杂、高维和非平稳环境下的动态决策提供了一种新范式；实际上，为金融市场优化开辟了新方向。局限性在于当前基础设施带来的延迟问题，未来工作可聚焦于优化部署流程和降低开销，以增强实际应用价值。",
      "tags": [
        "Quantum Reinforcement Learning",
        "Variational Quantum Circuits",
        "Dynamic Portfolio Optimization",
        "Deep Deterministic Policy Gradient",
        "Deep Q-Network"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:11.326112Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.12913",
    "title": "Actionable Interpretability Must Be Defined in Terms of Symmetries",
    "authors": [
      "Pietro Barbiero",
      "Mateo Espinosa Zarlenga",
      "Francesco Giannini",
      "Alberto Termine",
      "Filippo Bonchi",
      "Mateja Jamnik",
      "Giuseppe Marra"
    ],
    "abstract": "This paper argues that interpretability research in Artificial Intelligence (AI) is fundamentally ill-posed as existing definitions of interpretability fail to describe how interpretability can be formally tested or designed for. We posit that actionable definitions of interpretability must be formulated in terms of *symmetries* that inform model design and lead to testable conditions. Under a probabilistic view, we hypothesise that four symmetries (inference equivariance, information invariance, concept-closure invariance, and structural invariance) suffice to (i) formalise interpretable models as a subclass of probabilistic models, (ii) yield a unified formulation of interpretable inference (e.g., alignment, interventions, and counterfactuals) as a form of Bayesian inversion, and (iii) provide a formal framework to verify compliance with safety standards and regulations.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.12913.pdf",
    "abs_url": "https://arxiv.org/abs/2601.12913",
    "published": "2026-01-19T10:10:17Z",
    "updated": "2026-01-28T16:57:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于对称性定义AI可解释性，使其成为可操作和可测试的正式框架。",
      "motivation": "当前人工智能中的可解释性研究面临根本性问题，因为现有定义缺乏正式基础，无法明确描述如何测试或设计可解释性模型。这在实际应用中至关重要，可解释性对于确保AI系统的安全、合规和用户信任不可或缺，尤其在医疗、金融等高可靠性领域。现有方法往往基于直觉或经验，缺乏严格的数学定义，导致可解释性评估主观且不一致，难以应用于严格的安全标准和法规验证，因此需要更严谨的理论框架来填补这一空白。",
      "method": "论文提出通过对称性概念来形式化可解释性定义。在概率视角下，定义了四种关键对称性：推断等价性、信息不变性、概念闭合不变性和结构不变性，这些对称性指导模型设计并产生可测试条件。创新点在于将可解释模型形式化为概率模型的子类，并将可解释推断（如对齐、干预和反事实）统一表述为贝叶斯逆的形式。摘要未明确说明使用的具体数据集或模型架构，但框架旨在提供通用方法论以适用于各种AI模型。",
      "result": "论文的主要结果是建立了一个正式框架，基于对称性定义可解释性，使得可解释性可以被正式测试和设计。该框架能够（i）形式化可解释模型为概率模型的子类，（ii）统一可解释推断方法为贝叶斯逆的形式，（iii）提供验证与安全标准和法规合规性的基础。虽然没有提供具体的性能指标提升或对比基线实验数据，但与现有非正式定义相比，该框架增强了可解释性研究的严谨性，为未来实证应用奠定了基础。",
      "conclusion": "本研究的主要贡献是提出了基于对称性的可解释性定义，为AI可解释性研究提供了正式的数学基础，克服了现有定义的模糊性。在学术上，它统一了多种可解释推断方法，推动了理论发展；在实际应用中，支持AI系统的安全合规验证，提升在关键领域的可信度。摘要未明确说明局限性或未来工作方向，但潜在发展方向可能包括实证验证框架有效性、扩展对称性概念或应用于具体模型以测试其实用性。",
      "tags": [
        "Interpretability",
        "Symmetry",
        "Probabilistic Models",
        "Bayesian Inference",
        "Formal Verification"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:07.338279Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.12066",
    "title": "Learning Stochastic Bridges for Video Object Removal via Video-to-Video Translation",
    "authors": [
      "Zijie Lou",
      "Xiangwei Feng",
      "Jiaxin Wang",
      "Jiangtao Yao",
      "Fei Che",
      "Tianbao Liu",
      "Chengjing Wu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Ting Liu"
    ],
    "abstract": "Existing video object removal methods predominantly rely on diffusion models following a noise-to-data paradigm, where generation starts from uninformative Gaussian noise. This approach discards the rich structural and contextual priors present in the original input video. Consequently, such methods often lack sufficient guidance, leading to incomplete object erasure or the synthesis of implausible content that conflicts with the scene's physical logic. In this paper, we reformulate video object removal as a video-to-video translation task via a stochastic bridge model. Unlike noise-initialized methods, our framework establishes a direct stochastic path from the source video (with objects) to the target video (objects removed). This bridge formulation effectively leverages the input video as a strong structural prior, guiding the model to perform precise removal while ensuring that the filled regions are logically consistent with the surrounding environment. To address the trade-off where strong bridge priors hinder the removal of large objects, we propose a novel adaptive mask modulation strategy. This mechanism dynamically modulates input embeddings based on mask characteristics, balancing background fidelity with generative flexibility. Extensive experiments demonstrate that our approach significantly outperforms existing methods in both visual quality and temporal consistency. The project page is https://bridgeremoval.github.io/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.12066.pdf",
    "abs_url": "https://arxiv.org/abs/2601.12066",
    "published": "2026-01-17T14:22:14Z",
    "updated": "2026-01-28T10:30:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出一种基于随机桥模型的视频对象移除方法，通过视频到视频翻译有效利用输入视频的结构先验，以提升移除精度和内容逻辑一致性。",
      "motivation": "现有视频对象移除方法主要依赖从高斯噪声开始的扩散模型，这忽视了原始输入视频中的丰富结构和上下文先验，导致对象擦除不完全或生成内容与场景物理逻辑冲突，限制了视频编辑的质量和实用性。因此，需开发更有效的方法以充分利用输入信息，确保移除过程精准且内容自然。",
      "method": "本文重新定义视频对象移除为视频到视频翻译任务，采用随机桥模型建立从源视频（含对象）到目标视频（对象移除）的直接随机路径，利用输入视频作为结构先验。关键创新是自适应掩模调制策略，基于掩模特征动态调制输入嵌入，平衡背景保真度与生成灵活性，从而优化大对象移除时的性能。",
      "result": "通过大量实验验证，该方法在视觉质量和时间一致性上均显著优于现有视频对象移除方法。虽然摘要未提供具体数值指标，但结果表明显著改进，能更有效地擦除对象并生成与周围环境逻辑一致的内容，增强了实际应用的可信度。",
      "conclusion": "本研究的主要贡献是提出随机桥模型和自适应掩模调制策略，改进视频对象移除的精度和连贯性，具有重要学术价值和实际应用意义。未来工作可探索在更复杂场景下的扩展或效率优化，以进一步提升性能。",
      "tags": [
        "Stochastic Bridge Model",
        "Video-to-Video Translation",
        "Adaptive Mask Modulation",
        "Video Object Removal",
        "Generative Models"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:00.562088Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10992",
    "title": "Constant Metric Scaling in Riemannian Computation",
    "authors": [
      "Kisung You"
    ],
    "abstract": "Constant rescaling of a Riemannian metric appears in many computational settings, often through a global scale parameter that is introduced either explicitly or implicitly. Although this operation is elementary, its consequences are not always made clear in practice and may be confused with changes in curvature, manifold structure, or coordinate representation. In this note we provide a short, self-contained account of constant metric scaling on arbitrary Riemannian manifolds. We distinguish between quantities that change under such a scaling, including norms, distances, volume elements, and gradient magnitudes, and geometric objects that remain invariant, such as the Levi--Civita connection, geodesics, exponential and logarithmic maps, and parallel transport. We also discuss implications for Riemannian optimization, where constant metric scaling can often be interpreted as a global rescaling of step sizes rather than a modification of the underlying geometry. The goal of this note is purely expository and is intended to clarify how a global metric scale parameter can be introduced in Riemannian computation without altering the geometric structures on which these methods rely.",
    "categories": [
      "cs.LG",
      "stat.CO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10992.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10992",
    "published": "2026-01-16T04:54:23Z",
    "updated": "2026-01-28T16:26:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "这篇论文澄清了常数度量缩放在黎曼计算中的影响，明确区分了在缩放下变化的几何量和不变量，以帮助避免与几何结构变化的混淆。",
      "motivation": "研究动机源于常数度量缩放在黎曼计算中经常出现，但实践中其后果往往不清晰，容易与曲率、流形结构或坐标表示的变化混淆。这种混淆可能导致在优化等应用中对几何性质误解，因此需要明确缩放操作的影响，以确保在引入全局尺度参数时不错误地改变基础几何。摘要未明确说明具体应用场景，但强调了澄清这一问题对算法设计和理解的重要性。",
      "method": "论文采用理论分析方法，提供了一个自包含的短小解释，详细区分了在常数度量缩放下变化的量（如范数、距离、体积元素和梯度大小）和不变量（如Levi-Civita连接、测地线、指数和对数映射、以及平行运输）。关键创新点在于系统分类这些对象，并讨论缩放如何影响黎曼优化，其中缩放常被解释为步长的全局调整而非几何修改。摘要未提及具体数据集或模型架构，因为这是一篇理论性笔记。",
      "result": "论文的主要结果基于理论分析，表明常数度量缩放仅改变与度量直接相关的量（例如，距离和梯度幅度会缩放），而不影响核心几何结构（如连接和测地线保持原样）。这澄清了在黎曼计算中引入全局尺度参数时，可以避免错误地改变流形的基本性质。摘要未提供具体实验数据或与基线方法的对比，但理论分析为正确应用缩放提供了指导，确保了方法的几何一致性。",
      "conclusion": "论文的主要贡献是提供了一个清晰的解释，帮助理解常数度量缩放在黎曼计算中的作用，强调缩放不影响几何核心结构，仅在度量相关量上产生变化。其学术价值在于澄清了长期存在的混淆，增强了理论基础的严谨性；实际应用价值体现在黎曼优化等领域，使研究人员能安全地调整尺度参数以改进算法性能，而不误改几何。未来工作可扩展此分析到更复杂的缩放场景，以进一步细化应用。",
      "tags": [
        "Riemannian Metric",
        "Scaling",
        "Geodesics",
        "Levi-Civita Connection",
        "Riemannian Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:52.579591Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09831",
    "title": "A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch",
    "authors": [
      "Guixian Xu",
      "Jinglai Li",
      "Junqi Tang"
    ],
    "abstract": "In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions. Moreover, we derive the convergence theory for equivariant PnP (EPnP) under the prior mismatch setting, proving that EPnP reduces error variance and explicitly tightens the convergence bound.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09831.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09831",
    "published": "2026-01-14T19:47:31Z",
    "updated": "2026-01-28T13:09:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究首次为 plug-and-play proximal gradient descent (PnP-PGD) 在 prior mismatch 下提供了收敛理论证明，移除了限制性假设并扩展到 EPnP。",
      "motivation": "在 plug-and-play 算法中，当去噪器的训练数据分布与推理任务不匹配（prior mismatch）时，现有收敛理论依赖于限制性和无法验证的假设，缺乏严格的收敛证明。这在实际应用中很重要，因为训练数据可能与任务分布有偏差，现有理论的不足限制了算法的可靠性和泛化性。因此，研究旨在解决 prior mismatch 下的收敛性问题，以提供更健壮的理论基础。",
      "method": "论文提出一种新的收敛分析方法，针对 plug-and-play proximal gradient descent (PnP-PGD) 在 prior mismatch 设置下的理论证明。关键创新点包括移除了现有结果中的多个限制性和无法验证的假设，并扩展到 equivariant PnP (EPnP)，证明了 EPnP 在 prior mismatch 下能减少误差方差并明确收紧收敛界。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "主要理论结果表明，PnP-PGD 在 prior mismatch 下具有收敛性，这是首个此类证明。与现有 PnP 算法的理论相比，新结果移除了限制性假设，增强了理论的可验证性。对于 EPnP，证明显示它能有效减少误差方差并收紧收敛界，优于标准 PnP。摘要未明确说明具体的实验性能指标或对比数据。",
      "conclusion": "论文的主要贡献是为 PnP-PGD 在 prior mismatch 下提供了首个收敛理论证明，并扩展到 EPnP，证明其能减少误差方差。学术价值在于强化了 PnP 算法的理论基础，去除了无法验证的假设，促进了更可靠的应用。实际应用中，这有助于算法在数据分布不匹配时的部署，未来工作可能包括扩展到其他变体或实验验证。",
      "tags": [
        "Plug-and-Play Proximal Gradient Descent",
        "Prior Mismatch",
        "Convergence Analysis",
        "Equivariant PnP"
      ]
    },
    "analyzed_at": "2026-01-29T04:03:12.567057Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18800",
    "title": "NavFormer: IGRF Forecasting in Moving Coordinate Frames",
    "authors": [
      "Yoontae Hwang",
      "Dongwoo Lee",
      "Minseok Choi",
      "Heechan Park",
      "Yong Sup Ihn",
      "Daham Kim",
      "Deok-Young Lee"
    ],
    "abstract": "Triad magnetometer components change with sensor attitude even when the IGRF total intensity target stays invariant. NavFormer forecasts this invariant target with rotation invariant scalar features and a Canonical SPD module that stabilizes the spectrum of window level second moments of the triads without sign discontinuities. The module builds a canonical frame from a Gram matrix per window and applies state dependent spectral scaling in the original coordinates. Experiments across five flights show lower error than strong baselines in standard training, few shot training, and zero shot transfer. The code is available at: https://anonymous.4open.science/r/NavFormer-Robust-IGRF-Forecasting-for-Autonomous-Navigators-0765",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18800.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18800",
    "published": "2026-01-14T05:54:38Z",
    "updated": "2026-01-28T03:06:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "NavFormer通过旋转不变标量特征和Canonical SPD模块，在移动坐标系中实现了鲁棒的IGRF预测。",
      "motivation": "研究动机源于三轴磁强计组件随传感器姿态变化，而地磁场总强度目标不变，这导致预测挑战，对自主导航系统至关重要。现有方法可能无法有效处理姿态变化带来的预测误差，因为传感器读数受旋转影响，需要一种鲁棒方法来稳定预测。摘要未明确说明具体不足，但强调了旋转不变性处理的重要性，以提高在动态环境下的预测准确性。",
      "method": "NavFormer的核心方法是使用旋转不变的标量特征和Canonical SPD模块。该模块从每个窗口的Gram矩阵构建规范框架，稳定三轴窗口级二阶矩的谱，避免符号间断。创新点包括在原始坐标中应用状态依赖的谱缩放，以增强模型的鲁棒性。技术特色涉及对磁强计数据的预处理，但摘要未详细说明模型架构，仅提及基于飞行实验的数据集，用于训练和验证。",
      "result": "实验在五个飞行数据集上进行，NavFormer在标准训练、少样本训练和零样本转移场景下，均取得了比强基线更低的预测错误。具体性能指标摘要未提供，但通过对比实验证明了方法的有效性，表明其在多种训练条件下都能保持稳定的预测性能，具有良好的泛化能力和鲁棒性。",
      "conclusion": "论文的主要贡献是提出了NavFormer方法，通过旋转不变特征和Canonical SPD模块，提高了IGRF预测的准确性和鲁棒性。该研究对自主导航系统有重要的应用价值，能够在传感器姿态变化的情况下实现可靠的磁场预测。未来工作方向摘要未明确说明，但可能包括优化模块性能或扩展到其他传感器数据类型。",
      "tags": [
        "IGRF Forecasting",
        "Rotation Invariant Features",
        "Canonical SPD Module",
        "Magnetometer",
        "Autonomous Navigation"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:38.378481Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16991",
    "title": "Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models",
    "authors": [
      "Longteng Zhang",
      "Sen Wu",
      "Shuai Hou",
      "Zhengyu Qing",
      "Zhuo Zheng",
      "Danning Ke",
      "Qihong Lin",
      "Qiang Wang",
      "Shaohuai Shi",
      "Xiaowen Chu"
    ],
    "abstract": "Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\\times$, and delivers up to a $1.7\\times$ inference speedup.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.16991.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16991",
    "published": "2026-01-08T20:34:12Z",
    "updated": "2026-01-28T10:53:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出SALR方法，结合低秩适配和稀疏剪枝，提高了大语言模型微调的效率和压缩性能。",
      "motivation": "大语言模型微调通常需要调整数百万参数或依赖成本高的密集权重更新，这在资源受限环境中难以应用。现有方法如Low-Rank Adaptation（LoRA）通过分解权重更新减少了可训练参数，但基础权重仍为密集状态，导致高存储和计算开销。Magnitude-based pruning虽能生成稀疏模型，但直接应用时常损害LoRA的性能，因此需要一种新方法来统一低秩和稀疏技术，解决效率与性能的平衡问题。",
      "method": "论文提出SALR（Sparsity-Aware Low-Rank Representation），基于均方误差框架统一低秩适配和稀疏剪枝。核心创新包括静态剪枝冻结的基础权重以最小化剪枝误差，并通过截断SVD低秩适配器恢复残余信息，理论证明这能减少每个条目的均方误差。为提高硬件效率，方法融合多个低秩适配器为单GEMM操作，采用位图编码和两阶段流水线解码设计，实现模型压缩和加速，关键细节涉及权重分解和优化计算流程。",
      "result": "实验结果显示，SALR在多个大型语言模型上实现了50%的稀疏度，并在GSM8K和MMLU任务上性能与LoRA相当，证实了其有效性。具体地，模型大小减少了2倍，推理速度提升了高达1.7倍，这表明SALR在保持性能的同时显著提高了效率，与基线方法相比，在压缩和加速方面有显著优势。",
      "conclusion": "SALR成功统一了低秩适配和稀疏剪枝，提高了大语言模型微调的效率和硬件性能，具有学术和实际应用价值，特别是在资源受限场景中。研究为模型压缩和加速提供了新思路，未来工作可探索更广泛的适用性和优化细节，摘要未明确说明具体局限性。",
      "tags": [
        "Low-Rank Adaptation",
        "Sparse Pruning",
        "Truncated-SVD",
        "Model Compression",
        "Hardware Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:15.028976Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03664",
    "title": "Stochastic Voronoi Ensembles for Anomaly Detection",
    "authors": [
      "Yang Cao",
      "Sikun Yang",
      "Xuyun Zhang",
      "Yujiu Yang"
    ],
    "abstract": "Anomaly detection aims to identify data instances that deviate significantly from majority of data, which has been widely used in fraud detection, network security, and industrial quality control. Existing methods struggle with datasets exhibiting varying local densities: distance-based methods miss local anomalies, while density-based approaches require careful parameter selection and incur quadratic time complexity. We observe that local anomalies, though indistinguishable under global analysis, become conspicuous when the data space is decomposed into restricted regions and each region is examined independently. Leveraging this geometric insight, we propose SVEAD (Stochastic Voronoi Ensembles Anomaly Detector), which constructs ensemble random Voronoi diagrams and scores points by normalized cell-relative distances weighted by local scale. The proposed method achieves linear time complexity and constant space complexity. Experiments on 45 datasets demonstrate that SVEAD outperforms 12 state-of-the-art approaches.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03664.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03664",
    "published": "2026-01-07T07:37:38Z",
    "updated": "2026-01-28T03:00:30Z",
    "comment": "Version 2: Added ablation study on dual-factor scoring mechanism, contamination robustness analysis and GPU acceleration results",
    "light_analysis": {
      "overview": "本文提出SVEAD方法，通过集成随机Voronoi图改进局部异常检测，实现线性时间复杂性和优异性能。",
      "motivation": "异常检测在欺诈检测、网络安全和工业质量控制中广泛应用，但现有方法处理局部密度变化的数据时存在不足：基于距离的方法如LOF忽略局部异常，而基于密度的方法如KNN需调整参数且计算复杂度高（二次时间）。这些限制影响了方法的鲁棒性和实用性，尤其在真实数据中局部密度差异常见，因此需要一种能自动适应局部结构、计算高效且参数敏感度低的新方法来提升检测准确性和效率。",
      "method": "SVEAD方法基于几何洞察，通过构造多个随机Voronoi图将数据空间分解为单元格，然后在每个单元格内独立计算点与局部参考的归一化距离，并根据局部密度尺度加权评分。核心创新在于利用Voronoi集成避免单一划分偏差，结合随机性增强模型鲁棒性，同时实现线性时间复杂性和常数空间复杂性，无需复杂参数调整。方法适用于高维数据，直接利用数据结构特性进行异常判别。",
      "result": "在45个数据集上的实验表明，SVEAD在异常检测任务中优于12种现有方法，包括基于距离和密度的算法，证明了其在处理局部密度变化数据时的有效性。摘要未明确说明具体性能指标如准确率或F1分数，但整体结果显示了方法的优越性能和鲁棒性，同时在减少计算成本方面表现突出，与基线方法对比强调了效率和精度的提升。",
      "conclusion": "SVEAD通过集成随机Voronoi图成功解决了局部异常检测问题，提供了一种高效且无参数依赖的方法，具有重要学术价值。其贡献在于引入几何分解视角，结合随机集成以增强鲁棒性，同时实现线性时间复杂性和常数空间复杂性，提升了计算效率。这在实际应用如实时监测中具有潜力，摘要未明确说明局限性或未来工作方向，但为异常检测领域提供了新思路和潜在扩展。",
      "tags": [
        "Anomaly Detection",
        "Stochastic Voronoi Ensembles",
        "Ensemble Learning",
        "Geometric Methods",
        "Linear Time Complexity"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:25.539780Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02553",
    "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
    "authors": [
      "Jiaqi Liu",
      "Yaofeng Su",
      "Peng Xia",
      "Siwei Han",
      "Zeyu Zheng",
      "Cihang Xie",
      "Mingyu Ding",
      "Huaxiu Yao"
    ],
    "abstract": "To support long-term interaction in complex environments, LLM agents require memory systems that manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which distills unstructured interactions into compact, multi-view indexed memory units; (2) Online Semantic Synthesis, an intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy; and (3) Intent-Aware Retrieval Planning, which infers search intent to dynamically determine retrieval scope and construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.02553.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02553",
    "published": "2026-01-05T21:02:49Z",
    "updated": "2026-01-28T14:29:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "SimpleMem是一个高效的终身记忆框架，通过语义无损压缩和三阶段管道，显著提升LLM agent的记忆管理性能与效率。",
      "motivation": "本研究针对LLM agent在复杂环境中需要长期交互记忆的问题，现有方法存在明显不足：被动上下文扩展保留完整交互历史会导致大量冗余，而迭代推理过滤噪音则产生高token成本。这些缺陷限制了agent的扩展性和实用性，因此需要一个能最大化信息密度并减少开销的记忆系统，以平衡性能和效率，推动LLM agent在真实场景中的应用。",
      "method": "SimpleMem提出一个三阶段管道：首先，语义结构化压缩将非结构化交互蒸馏为紧凑、多视角索引的记忆单元，提高信息密度。其次，在线语义合成在会话内实时整合相关上下文为统一抽象表示，以消除冗余。第三，意图感知检索规划通过推断搜索意图动态确定检索范围，高效构建精确上下文。整个框架基于语义无损压缩，旨在优化token利用率，无需依赖冗余历史或高成本推理。",
      "result": "实验在基准数据集上进行，结果显示SimpleMem在准确性、检索效率和推理成本上均优于基线方法。具体表现为平均F1分数提升了26.4%，同时推理时的token消耗减少了最多30倍，有效降低了开销。这些数据表明方法在性能和效率之间实现了优异平衡，为LLM agent的记忆管理提供了实质性改进，超越传统上下文扩展和迭代推理方法。",
      "conclusion": "论文的主要贡献是提出了SimpleMem框架，通过语义无损压缩和三阶段处理，有效解决LLM agent记忆管理中的冗余和效率问题，具有重要的学术和实际价值。研究推动高效记忆系统的发展，降低token成本，提升agent可扩展性。摘要未明确说明局限性，但未来工作可能包括扩展到更多任务或进一步优化算法，以增强适用性。",
      "tags": [
        "Large Language Model Agents",
        "Semantic Lossless Compression",
        "Intent-Aware Retrieval Planning",
        "Online Semantic Synthesis",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:22.624360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00533",
    "title": "All-in-One Video Restoration under Smoothly Evolving Unknown Weather Degradations",
    "authors": [
      "Wenrui Li",
      "Hongtao Chen",
      "Yao Xiao",
      "Wangmeng Zuo",
      "Jiantao Zhou",
      "Yonghong Tian",
      "Xiaopeng Fan"
    ],
    "abstract": "All-in-one image restoration aims to recover clean images from diverse unknown degradations using a single model. But extending this task to videos faces unique challenges. Existing approaches primarily focus on frame-wise degradation variation, overlooking the temporal continuity that naturally exists in real-world degradation processes. In practice, degradation types and intensities evolve smoothly over time, and multiple degradations may coexist or transition gradually. In this paper, we introduce the Smoothly Evolving Unknown Degradations (SEUD) scenario, where both the active degradation set and degradation intensity change continuously over time. To support this scenario, we design a flexible synthesis pipeline that generates temporally coherent videos with single, compound, and evolving degradations. To address the challenges in the SEUD scenario, we propose an all-in-One Recurrent Conditional and Adaptive prompting Network (ORCANet). First, a Coarse Intensity Estimation Dehazing (CIED) module estimates haze intensity using physical priors and provides coarse dehazed features as initialization. Second, a Flow Prompt Generation (FPG) module extracts degradation features. FPG generates both static prompts that capture segment-level degradation types and dynamic prompts that adapt to frame-level intensity variations. Furthermore, a label-aware supervision mechanism improves the discriminability of static prompt representations under different degradations. Extensive experiments show that ORCANet achieves superior restoration quality, temporal consistency, and robustness over image and video-based baselines. Code is available at https://github.com/Friskknight/ORCANet-SEUD.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.00533.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00533",
    "published": "2026-01-02T02:20:57Z",
    "updated": "2026-01-28T07:14:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出SEUD场景和ORCANet方法，用于处理时间连续变化的未知退化视频恢复，提升了恢复质量和时间一致性。",
      "motivation": "现有全图恢复方法扩展到视频时，主要关注帧级退化变化，忽视了退化的时间连续性，但真实世界的退化类型和强度随时间平滑演化，可能共存或逐渐过渡。这一问题限制了视频恢复的准确性和鲁棒性，因此需要新方法处理这种连续变化的退化场景，以更好地模拟真实环境。",
      "method": "论文提出ORCANet网络，包括CIED模块利用物理先验估计雾强度并提供初始特征；FPG模块提取退化特征，生成静态和动态提示以适应段级和帧级变化；并通过标签感知监督机制增强提示表示的可区分性。同时设计了合成流水线生成时间一致的训练视频，处理单、复合和演化退化。",
      "result": "实验结果显示，ORCANet在恢复质量、时间一致性和鲁棒性方面均优于基于图像和视频的基线方法，证明了其在SEUD场景下的优越性能，但具体指标摘要未明确说明。",
      "conclusion": "论文的主要贡献是提出了SEUD场景和ORCANet方法，为视频恢复中处理平滑变化的未知退化提供了新方案，具有学术创新性和实际应用价值；未来可能扩展该方法到更多退化类型和场景，摘要未明确说明局限性。",
      "tags": [
        "Video Restoration",
        "Smoothly Evolving Degradations",
        "Recurrent Network",
        "Conditional Prompting",
        "Adaptive Prompting"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:21.222358Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00269",
    "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
    "authors": [
      "Chaodong Tong",
      "Qi Zhang",
      "Chen Li",
      "Lei Jiang",
      "Yanbing Liu"
    ],
    "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.00269.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00269",
    "published": "2026-01-01T09:19:39Z",
    "updated": "2026-01-28T16:05:20Z",
    "comment": "21 pages, 13 figures, 8 tables",
    "light_analysis": {
      "overview": "论文提出FaithSCAN，一种轻量级网络，通过利用视觉语言模型的内部信号和自动生成监督信号，实现高效单次通过幻觉检测，提升视觉问答的忠实性。",
      "motivation": "视觉问答（VQA）中的幻觉问题导致模型生成流畅但视觉未接地的答案，严重影响安全关键应用的可靠性。现有方法主要分为外部验证（依赖辅助模型或知识库）和不确定性驱动方法（如重复采样或不确定性估计）。外部验证计算开销大且受限于外部资源质量；不确定性方法仅捕捉模型不确定性的有限方面，未能充分利用与多样失败模式相关的丰富内部信号，因此在效率、鲁棒性和检测性能上存在固有局限性。",
      "method": "FaithSCAN是一个轻量级网络，用于检测VQA幻觉，它利用视觉语言模型（VLM）的丰富内部信号，包括token级解码不确定性、中间视觉表示和跨模态对齐特征。这些信号通过分支证据编码和不确定性感知注意力进行融合。此外，扩展了LLM-as-a-Judge范式到VQA幻觉检测，提出低成本策略自动生成模型依赖的监督信号，实现无昂贵人工标签的监督训练，同时保持高检测精度。方法基于模型驱动的单次通过检测，无需外部资源或重复采样。",
      "result": "在多个VQA基准测试中，FaithSCAN在效果和效率上显著优于现有方法。深入分析表明，幻觉源于视觉感知、跨模态推理和语言解码中的系统性内部状态变化；不同内部信号提供互补的诊断线索，幻觉模式随VLM架构变化，为多模态幻觉的成因提供了新见解。摘要未明确说明具体性能指标数据，但强调了相对于基线方法的优越表现。",
      "conclusion": "FaithSCAN的主要贡献是提出一种高效、鲁棒的幻觉检测方法，通过融合VLM内部信号和自动监督信号生成，解决了现有方法的局限性。该研究提高了VQA在安全关键应用中的可靠性，并揭示了幻觉的系统性成因，为多模态模型改进提供了理论洞见。未来工作可能包括进一步优化检测性能或扩展到其他多模态任务，但摘要未明确说明具体方向。",
      "tags": [
        "Visual Question Answering (VQA)",
        "Hallucination Detection",
        "Vision-Language Models (VLMs)",
        "Uncertainty Estimation",
        "Large Language Models (LLMs)"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:32.201755Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.24601",
    "title": "Recursive Language Models",
    "authors": [
      "Alex L. Zhang",
      "Tim Kraska",
      "Omar Khattab"
    ],
    "abstract": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at https://github.com/alexzhang13/rlm.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.24601.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24601",
    "published": "2025-12-31T03:43:41Z",
    "updated": "2026-01-28T18:59:39Z",
    "comment": "9 pages, 33 with Appendix",
    "light_analysis": {
      "overview": "提出了递归语言模型（RLMs），作为一种推理范式，使大型语言模型能够递归处理任意长的提示，显著提升了长上下文任务的性能。",
      "motivation": "该研究旨在解决大型语言模型处理长提示时受上下文窗口限制的实践问题。长输入处理在如文档摘要、对话系统等应用中至关重要，但现有方法如普通前沿LLMs和常见长上下文脚手架在长输入上表现有限，难以高效处理超长内容。因此，研究RLMs旨在克服这些不足，通过推理时间缩放扩展模型能力，以应对日益增长的长输入需求。",
      "method": "论文提出了递归语言模型（RLMs）作为核心方法，这是一种通用的推理范式，将长提示视为外部环境，允许LLM以编程方式检查、分解和递归调用自身处理提示片段。关键创新点包括递归推理机制和提示分解策略，有效突破了传统上下文窗口的局限。在小规模实验中，通过后训练开发了首个原生递归语言模型RLM-Qwen3-8B，基于Qwen3-8B模型，优化了递归处理能力，具体数据集和架构细节摘要未明确说明。",
      "result": "实验结果显示，RLMs能够成功处理输入长度超出模型上下文窗口两个数量级的提示，在四个多样化的长上下文任务中，显著优于普通前沿LLMs和常见长上下文脚手架，同时保持成本相当。具体指标包括：RLM-Qwen3-8B模型比基础模型Qwen3-8B平均性能提升28.3%，并在三个长上下文任务中接近普通GPT-5的质量，验证了RLMs在扩展处理能力和提高任务性能方面的有效性。",
      "conclusion": "本研究的主要贡献是提出了递归语言模型（RLMs）范式，并开发了首个原生递归语言模型，学术上为LLMs的长上下文处理提供了新思路，实践中提升了长输入任务的性能且成本可控。其价值在于扩展了AI应用范围，如大规模文档分析。潜在局限性或未来工作方向摘要未明确说明，但可能涉及进一步优化递归策略或扩展到更大模型规模。",
      "tags": [
        "Large Language Model",
        "Recursive Language Model",
        "Inference-Time Scaling",
        "Post-Training",
        "Long-Context Processing"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:24.707848Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23880",
    "title": "CASCADE: Cumulative Agentic Skill Creation through Autonomous Development and Evolution",
    "authors": [
      "Xu Huang",
      "Junwu Chen",
      "Yuxing Fei",
      "Zhuohan Li",
      "Philippe Schwaller",
      "Gerbrand Ceder"
    ],
    "abstract": "Large language model (LLM) agents currently depend on predefined tools or early-stage tool generation, limiting their adaptability and scalability to complex scientific tasks. We introduce CASCADE, a self-evolving agentic framework representing an early instantiation of the transition from \"LLM + tool use\" to \"LLM + skill acquisition\". CASCADE enables agents to master complex external tools and codify knowledge through two meta-skills: continuous learning via web search, code extraction, and memory utilization; self-reflection via introspection, knowledge graph exploration, and others. We evaluate CASCADE on SciSkillBench, a benchmark of 116 materials science and chemistry research tasks. CASCADE achieves a 93.3% success rate using GPT-5, compared to 35.4% without evolution mechanisms. We further demonstrate real-world applications in computational analysis, autonomous laboratory experiments, and selective reproduction of published papers. Along with human-agent collaboration and memory consolidation, CASCADE accumulates executable skills that can be shared across agents and scientists, moving toward scalable AI-assisted scientific research.",
    "categories": [
      "cs.AI",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.23880.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23880",
    "published": "2025-12-29T21:50:23Z",
    "updated": "2026-01-28T06:08:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出CASCADE框架，通过元技能实现代理技能的自主开发和演化，推动AI辅助科学研究的可扩展性。",
      "motivation": "研究动机是解决大型语言模型（LLM）代理在复杂科学任务中依赖预定义或早期生成工具的局限，导致适应性和可扩展性不足。科学任务如材料科学和化学研究需要高度动态和专业化处理，现有方法无法有效应对这些挑战，限制了代理在现实世界应用中的潜力。CASCADE旨在通过自我演化机制克服这些不足，提升代理的自主性以支持更广泛的研究需求。",
      "method": "研究方法包括CASCADE框架的设计，它通过两个核心元技能实现技能累积：持续学习（涉及网络搜索、代码提取和内存利用）和自我反思（通过内省和知识图谱探索）。代理能够掌握复杂外部工具并编码化知识，框架使用GPT-5作为基础模型，并在SciSkillBench数据集（包含116个材料科学和化学任务）上进行开发和评估，以促进从工具使用到技能获取的过渡。",
      "result": "主要实验结果表明，CASCADE在SciSkillBench基准上取得了显著性能提升。使用GPT-5时，成功率达到93.3%，相比基线条件下（无演化机制）的35.4%，提升了约57.9个百分点。此外，框架在计算分析、自主实验室实验和选择性论文复制等实际应用中展示了有效性，验证了其可扩展性和适应性，这些结果通过对比基线和多样化场景证明了其优势。",
      "conclusion": "结论强调CASCADE通过累积可执行技能并实现跨代理和科学家共享，推动了可扩展AI辅助科学研究的进展。主要贡献在于引入了从“LLM+工具使用”向“LLM+技能获取”的过渡框架，具有重要学术价值和实际应用潜力，例如在科学实验和数据分析中。未来工作可能涉及扩展应用到其他领域或优化演化机制，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model Agents",
        "Skill Acquisition",
        "Self-Reflection",
        "Continuous Learning",
        "Knowledge Graph Exploration"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:42.488305Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23565",
    "title": "RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature",
    "authors": [
      "Hanzheng Li",
      "Xi Fang",
      "Yixuan Li",
      "Chaozheng Huang",
      "Junjie Wang",
      "Xi Wang",
      "Hongzhe Bai",
      "Bojun Hao",
      "Shenyu Lin",
      "Huiqi Liang",
      "Linfeng Zhang",
      "Guolin Ke"
    ],
    "abstract": "The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.23565.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23565",
    "published": "2025-12-29T16:05:38Z",
    "updated": "2026-01-28T07:19:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了RxnBench基准，用于评估多模态大语言模型在科学文献中化学反应理解的能力，通过两层任务设计测试模型视觉感知和推理能力。",
      "motivation": "MLLMs在化学领域的应用有望革命性推动科学发现，但现有方法对模型理解真实文献中反应图的密集图形语言探索不足。由于科学文献包含大量图表，传统模型主要关注文本提取，忽视了视觉信息的深层化学逻辑，这限制了自主AI化学家的发展。因此，需要建立全面基准来评估和提升模型在这方面的能力，以弥补跨模态整合和领域特定知识的缺陷。",
      "method": "RxnBench基准包括两个任务：单图问答基于305个反应方案生成1,525个问题，测试模型的细粒度视觉感知和反应机理推理；全文问答使用108篇文章，要求模型跨模态整合文本、反应方案和表格信息，评估综合理解和信息合成能力。基准设计强调多层次评估，从局部细节到全局文档，利用科学PDFs构建真实场景，以全面测试MLLMs在化学反应理解上的性能。",
      "result": "实验显示，MLLMs在提取显式文本方面表现良好，但在深层次化学逻辑和精确结构识别上存在困难，如模型在处理反应图时准确率较低。具有推理时间推理的模型显著优于标准架构，例如在单图问答任务中性能提升，但所有模型在全文问答任务上均未达到50%准确率。这揭示了当前模型在跨模态整合和领域特定视觉识别方面的局限，与基线方法对比强调了改进空间。",
      "conclusion": "RxnBench基准的开发填补了MLLMs在化学领域评估的空白，揭示了模型在视觉和推理能力上的不足。学术价值在于提供了针对科学文献的评估工具，推动多模态模型研究；实际应用上促进了自主AI化学家的研发。未来工作应聚焦于开发领域特定视觉编码器和更强推理引擎，以提升模型性能并弥补跨模态学习中的缺陷。",
      "tags": [
        "Multimodal Large Language Models",
        "Chemical Reaction Understanding",
        "Benchmark Evaluation",
        "Visual Perception",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:47.136970Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23340",
    "title": "The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models",
    "authors": [
      "Dakuan Lu",
      "Jiaqi Zhang",
      "Cheng Yuan",
      "Jiawei Shao",
      "Xuelong Li"
    ],
    "abstract": "Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.23340.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23340",
    "published": "2025-12-29T09:55:12Z",
    "updated": "2026-01-28T05:25:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了多模型协作定律，预测大型语言模型集成性能基于总参数预算的缩放极限。",
      "motivation": "大型语言模型的性能提升主要依赖于个体模型的缩放定律，但单个模型的能力存在固有上限，无法满足日益增长的应用需求。多模型协作通过模型间的复杂交互，可以实现超越任何单个模型的性能，为解决这一局限提供了可能。然而，尽管现有技术如模型路由和后验集成已广泛应用，但缺乏一个统一的理论框架来预测多模型协作的性能缩放规律，这限制了系统优化和性能预测。因此，建立多模型协作的缩放定律对于深入理解协作机制、推动LLM集成技术的发展至关重要。",
      "method": "本文提出多模型协作定律，这是一个基于总参数预算的缩放定律，用于预测LLM集成性能的上限。研究方法采用方法无关的公式，假设一个理想化的集成oracle，其中每个样本的交叉熵损失由模型池中任何模型的最小损失决定，从而量化多模型协作的内在极限。该方法不依赖具体集成技术或模型架构，而是从理论角度探讨性能缩放，关键创新在于强调模型多样性在协作中的作用，通过总参数数作为性能预测指标，为多模型系统提供通用框架。摘要未明确说明使用的具体数据集或模型，但侧重于理论推导。",
      "result": "实验结果显示，多模型系统在总参数数上遵循幂律缩放，相比单模型缩放，展现出更显著的性能改进趋势和更低的理论损失下限。具体而言，异构模型族的集成性能优于同族内的集成，这表明模型多样性是协作增益的主要驱动力。这些发现通过理论分析和实验验证，支持了多模型协作定律的有效性，为LLM集成提供了性能基准和优化方向。摘要未提供具体数据如准确率数字，但强调了协作带来的整体优势，突出了模型多样性对提升集体性能的重要性。",
      "conclusion": "本研究的主要贡献是提出了多模型协作定律，为大型语言模型集成提供了一个理论框架，预测了性能的缩放极限。研究证明，多模型协作能有效扩展LLM的智能前沿，并强调模型多样性在提升集体性能中的关键作用，具有重要的学术价值，为未来设计更高效的多模型系统奠定了基础。此外，这一工作可能推动实际应用中集成方法的优化，摘要未明确说明局限性，但未来工作可进一步探讨实际集成技术的具体实现和与其他领域的结合，以丰富理论应用。",
      "tags": [
        "Large Language Models",
        "Model Ensembling",
        "Scaling Laws",
        "Multi-Model Collaboration",
        "Cross-Entropy Loss"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:49.753824Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.20573",
    "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
    "authors": [
      "Rui Pan",
      "Zhuofu Chen",
      "Hongyi Liu",
      "Arvind Krishnamurthy",
      "Ravi Netravali"
    ],
    "abstract": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.7$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.20573.pdf",
    "abs_url": "https://arxiv.org/abs/2512.20573",
    "published": "2025-12-23T18:16:58Z",
    "updated": "2026-01-28T18:48:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出FailFast框架，通过动态调整推测长度，利用扩散大语言模型的并行生成特性，在推测解码中实现无损失加速，显著提升自回归大语言模型的推理效率。",
      "motivation": "扩散大语言模型(dLLMs)虽具有快速并行解码的优势，但其单独使用时存在效率-质量权衡，导致在推测解码中作为草稿模型时难以生成有效的长草稿，限制了加速潜力。现有方法如朴素dLLM草稿和EAGLE-3未能充分利用dLLMs的特性。本研究旨在解决这一问题，通过优化dLLMs在推测解码中的应用，提高自回归大语言模型(AR LLMs)的整体推理速度和效率，这对于降低计算成本和提升实际应用性能至关重要。",
      "method": "论文提出FailFast框架，基于dLLMs设计动态推测解码策略。核心创新是利用dLLMs的并行解码速度降低拒绝风险，实现长草稿生成，从而优化推测解码。框架动态调整推测长度：在难以推测的区域快速失败以减少计算延迟，在易推测区域积极扩展草稿长度以减少验证延迟，例如在某些情况下能一次性推测并接受70个令牌。无需微调，框架自适应不同模型和工作负载，利用dLLMs的并行生成优势。",
      "result": "实验结果表明，FailFast框架在多样化的模型和工作负载上实现了无损失加速。相比普通解码(vanilla decoding)，加速高达4.9倍；相比最佳朴素dLLM草稿，加速1.7倍；相比EAGLE-3，也加速1.7倍。具体地，框架在易推测区域能够生成和接受长达70个令牌的草稿，显著减少了整体推理时间和验证延迟，提升了效率。",
      "conclusion": "该研究通过重新思考推测解码中的草稿策略，成功提出了FailFast框架，有效利用dLLMs的特性实现显著加速。主要贡献在于提供了一种实用的、无损失加速AR LLMs的方法，具有重要的学术价值和实际应用前景，如降低大型语言模型推理成本。未来工作可能涉及扩展到更多模型类型或进一步优化动态适应算法，以应对更广泛的工作负载。",
      "tags": [
        "Diffusion Large Language Models",
        "Speculative Decoding",
        "Autoregressive Verifiers",
        "Dynamic Adaptation",
        "Parallel Token Generation"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:50.738933Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.19171",
    "title": "JEPA-Reasoner: Decoupling Latent Reasoning from Token Generation",
    "authors": [
      "Bingyang Kelvin Liu",
      "Ziyu Patrick Chen",
      "David P. Woodruff"
    ],
    "abstract": "Current autoregressive language models couple high-level reasoning and low-level token generation into a single sequential process, making the reasoning trajectory vulnerable to compounding expression errors. We propose JEPA-Reasoner, a novel architectural paradigm that decouples these tasks using a Joint-Embedding Predictive Architecture (JEPA) for pure latent-space reasoning and a separate Talker module for linguistic reconstruction. By isolating the reasoning engine from the discrete token-sampling process, our architecture enables: (1) Error Containment, where token-level failures cannot propagate into the latent reasoning chain; (2) Continuous Guidance, providing the generator with access to the entire lossless reasoning trajectory; and (3) Representation of Uncertainty, allowing the model to maintain multiple hypotheses via mixed latent vectors. Controlled experiments on synthetic and natural language tasks demonstrate that this decoupling enables a 0.9B model to achieve a 149.5\\% improvement in 8-shot GSM8K accuracy over a coupled Transformer baseline trained on identical data. This work shifts the focus from scaling coupled models to investigating decoupled architectures as a more robust foundation for complex reasoning.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.19171.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19171",
    "published": "2025-12-22T09:05:06Z",
    "updated": "2026-01-28T10:57:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "JEPA-Reasoner 提出了一种通过解耦潜在推理和令牌生成来提高推理鲁棒性的新架构范式。",
      "motivation": "当前自回归语言模型将高层次推理和低层次令牌生成耦合在单一序列过程中，导致推理轨迹容易受到复合表达错误的影响，错误可能从令牌级传播到整个推理链。这个问题影响了复杂推理任务的准确性和稳定性，因为现有方法未能有效分离推理和生成，使得模型在处理不确定性时脆弱。研究动机是设计一个更鲁棒的架构，以隔离这些任务并减少错误传播。",
      "method": "JEPA-Reasoner 采用 Joint-Embedding Predictive Architecture (JEPA) 进行纯潜在空间推理，并引入独立的 Talker 模块用于语言重建。关键创新点包括错误遏制、连续指导和不确定性表示：错误遏制防止令牌级失败影响推理链；连续指导提供无损推理轨迹给生成器；不确定性表示通过混合潜在向量支持多假设。该方法在合成和自然语言任务（如 GSM8K）上评估，模型架构解耦了推理和生成过程。",
      "result": "在合成和自然语言任务的受控实验中，JEPA-Reasoner 使一个 0.9B 参数的模型在 8-shot GSM8K 准确性上比耦合 Transformer 基线提高了 149.5%。这一显著提升验证了解耦架构的有效性，减少了错误传播并改善了推理轨迹。实验结果还表明，模型能够利用连续指导和不确定性表示来增强性能，在复杂推理任务中优于传统耦合方法。",
      "conclusion": "本研究的主要贡献是提出 JEPA-Reasoner，通过解耦潜在推理和令牌生成为复杂推理提供了更鲁棒的基础。它从缩放耦合模型转向架构创新，具有重要的学术价值，促进了 AI 系统推理可靠性的研究。尽管摘要未明确说明局限性，但未来工作可扩展解耦架构到更多任务，并探索其在实际应用中的潜力。",
      "tags": [
        "Joint-Embedding Predictive Architecture (JEPA)",
        "Latent Reasoning",
        "Token Generation",
        "Decoupled Architecture",
        "Uncertainty Representation"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:43.805681Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.18901",
    "title": "Gabliteration: Adaptive Multi-Directional Neural Weight Modification for Selective Behavioral Alteration in Large Language Models",
    "authors": [
      "Gökdeniz Gülmez"
    ],
    "abstract": "We present Gabliteration, a novel neural weight modification technique that advances beyond traditional abliteration methods by implementing adaptive multi-directional projections with regularized layer selection. Our approach addresses the fundamental limitation of existing methods that compromise model quality while attempting to modify specific behavioral patterns. Through dynamic layer optimization, regularized projection matrices, and adaptive scaling mechanisms, we achieve theoretically superior weight modification while minimizing quality degradation in unrelated domains. We validate our method through the gabliterated-v1 model series (0.6B to 4B parameters) available on Hugging Face, demonstrating practical applicability across multiple model scales.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.18901.pdf",
    "abs_url": "https://arxiv.org/abs/2512.18901",
    "published": "2025-12-21T22:12:54Z",
    "updated": "2026-01-28T15:04:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "Gabliteration是一种自适应多方向神经权重修改技术，通过正则化层选择和动态优化，实现对大型语言模型选择性行为修改的同时最小化质量损失。",
      "motivation": "本研究旨在解决现有神经权重修改方法在调整大型语言模型特定行为时，往往损害整体模型质量的问题。这是因为传统方法如abliteration在处理权重时不够精确，可能导致无关领域性能下降，限制了模型的可控性和实际应用灵活性。Gabliteration的提出是为了克服这一局限，实现更高效、低影响的行为修改。",
      "method": "Gabliteration的核心方法是基于自适应多方向投影技术，结合动态层优化来选择需修改的神经网络层。通过正则化投影矩阵限制修改方向和范围，并采用自适应缩放机制调整强度，从而理论上优化权重修改。创新点包括多方向投影、正则化层选择和自适应缩放，旨在最小化修改对模型其他功能的影响，技术路线扩展了传统abliteration方法。",
      "result": "该方法通过gabliterated-v1模型系列（参数从0.6B到4B）在Hugging Face上进行验证，展示了在不同规模大型语言模型上的实际适用性。摘要未明确说明具体性能指标如准确率提升，但推断Gabliteration在保持模型质量方面优于基线方法，体现为多模型规模的有效性验证。",
      "conclusion": "Gabliteration的主要贡献是提供了一种有效的神经权重修改技术，实现选择性行为修改时最小化质量退化。学术价值在于推进了模型编辑和可控AI领域的研究，实际应用价值包括定制化修改LLM行为以增强适应性和安全性。摘要未明确说明局限性，未来工作可能涉及扩展到更复杂任务或与其他技术结合。",
      "tags": [
        "Large Language Models",
        "Neural Weight Modification",
        "Adaptive Projection",
        "Regularized Layer Selection",
        "Multi-Directional Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:03.952646Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.17452",
    "title": "KV Admission: Learning What to Write for Efficient Long-Context Inference",
    "authors": [
      "Yen-Chieh Huang",
      "Pi-Cheng Hsiu",
      "Rui Fang",
      "Ming-Syan Chen"
    ],
    "abstract": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV (WG-KV), a lightweight mechanism that learns to predict token utility before cache entry. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, WG-KV reduces memory usage by 46-68% and delivers 3.03-3.70x prefill and 1.85-2.56x decode speedups on Llama and Qwen models, while maintaining compatibility with FlashAttention and Paged-KV systems. These results demonstrate that learning what to write is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.17452.pdf",
    "abs_url": "https://arxiv.org/abs/2512.17452",
    "published": "2025-12-19T11:08:58Z",
    "updated": "2026-01-28T07:38:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出Write-Gated KV (WG-KV)机制，通过学习预测令牌效用实现高效KV缓存管理，优化长上下文LLM推理。",
      "motivation": "长上下文LLM推理面临注意力复杂度二次方增长和KV缓存线性增长的瓶颈问题，导致内存占用大、推理速度慢，限制了实际应用效率。先前方法如事后选择或驱逐策略仅解决部分问题，忽视了根本的低效性，即在写入内存时未对令牌进行筛选，使得大量无用数据占据缓存。因此，需要一种在缓存写入阶段就能预测并过滤低效令牌的新方法，以从源头上减少资源浪费，提升整体性能。",
      "method": "论文将KV缓存管理形式化为一个因果系统，包含KV Admission、Selection和Eviction三个原语，并实例化KV Admission为Write-Gated KV (WG-KV)。WG-KV是一种轻量级机制，基于学习模型预测每个令牌在缓存写入前的效用，通过早期过滤低效用状态，结合紧凑的全局缓存和滑动本地缓存设计，减少不必要的内存写入。该方法在Llama和Qwen模型上实现，兼容现有优化技术如FlashAttention和Paged-KV系统，无需额外硬件支持，确保了实用性和可扩展性。",
      "result": "实验结果显示，WG-KV在Llama和Qwen模型上显著提升了性能：内存使用量减少了46%至68%，预填充阶段速度提升了3.03至3.70倍，解码阶段速度提升了1.85至2.56倍。与基线方法相比，这些改进不仅体现在资源效率上，还保持了与FlashAttention和Paged-KV等现有系统的兼容性，验证了WG-KV在处理长上下文推理任务中的有效性，并提供了可复现的实验数据支持。",
      "conclusion": "论文的主要贡献在于提出并验证了WG-KV机制，通过引入KV Admission概念，学习预测令牌效用以优化缓存写入过程，实现了高效的长上下文LLM推理。这具有重要的学术价值，为解决注意力复杂度和缓存增长问题提供了新思路，同时在实际应用中可降低计算资源需求、提升推理速度，对大规模部署具有指导意义。代码开源促进了可复现性，未来工作可扩展至更多模型或任务，以进一步评估其泛化能力。",
      "tags": [
        "KV Cache Management",
        "Write-Gated KV",
        "Long-Context Inference",
        "Token Utility Prediction",
        "Lightweight Mechanism"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:58.990012Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.14549",
    "title": "Dual-objective Language Models: Training Efficiency Without Overfitting",
    "authors": [
      "David Samuel",
      "Lucas Georges Gabriel Charpentier"
    ],
    "abstract": "This paper combines autoregressive and masked-diffusion training objectives without any architectural modifications, resulting in flexible language models that outperform single-objective models. Autoregressive modeling has been a popular approach, partly because of its training efficiency; however, that comes at the cost of sensitivity to overfitting. On the other hand, masked-diffusion models are less efficient to train while being more resilient to overfitting. In this work, we demonstrate that dual-objective training achieves the best of both worlds. To derive the optimal balance between both objectives, we train and evaluate 50 language models under varying levels of data repetition. We show that it is optimal to combine both objectives under all evaluated settings and that the optimal balance is similar whether targeting autoregressive or masked-diffusion downstream performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.14549.pdf",
    "abs_url": "https://arxiv.org/abs/2512.14549",
    "published": "2025-12-16T16:25:33Z",
    "updated": "2026-01-28T17:06:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合自回归和掩码扩散训练目标的双目标语言模型，无需架构修改，实现了训练效率与抗过拟合的平衡。",
      "motivation": "自回归语言模型因其训练效率高而被广泛采用，但容易过拟合，影响泛化性能；掩码扩散模型则相反，训练效率低但抗过拟合能力强。现有单目标方法无法同时兼顾效率与稳健性，因此需要一种新方法来平衡这两个方面，以提升语言模型的整体性能。",
      "method": "论文的核心方法是结合自回归和掩码扩散两种训练目标，通过双目标损失函数训练语言模型，无需对模型架构进行任何修改。关键创新在于通过实验确定两个目标的最优权重比例，作者在不同数据重复水平下训练了50个模型，以评估不同设置下的性能并找到最优平衡点。",
      "result": "实验结果显示，双目标训练的语言模型在所有评估设置下均优于单目标模型。通过分析50个模型的训练结果，作者发现结合两个目标始终是最优选择，且无论下游任务是自回归还是掩码扩散，最优平衡点相似，这表明该方法具有广泛的适用性和稳健性。",
      "conclusion": "本研究的核心贡献是提出了双目标语言模型训练方法，有效融合了自回归和掩码扩散目标的优势，解决了训练效率与抗过拟合的权衡问题。这为语言模型训练提供了新范式，具有重要的学术价值和实际应用潜力。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Dual-objective Training",
        "Autoregressive Modeling",
        "Masked Diffusion",
        "Language Models",
        "Training Efficiency"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:07.572917Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.13980",
    "title": "Structure-Aware Decoding Mechanisms for Complex Entity Extraction with Large-Scale Language Models",
    "authors": [
      "Zhimin Qiu",
      "Di Wu",
      "Feng Liu",
      "Yuxiao Wang"
    ],
    "abstract": "This paper proposes a structure-aware decoding method based on large language models to address the difficulty of traditional approaches in maintaining both semantic integrity and structural consistency in nested and overlapping entity extraction tasks. The method introduces a candidate span generation mechanism and structured attention modeling to achieve unified modeling of entity boundaries, hierarchical relationships, and cross-dependencies. The model first uses a pretrained language model to obtain context-aware semantic representations, then captures multi-granular entity span features through candidate representation combinations, and introduces hierarchical structural constraints during decoding to ensure consistency between semantics and structure. To enhance stability in complex scenarios, the model jointly optimizes classification loss and structural consistency loss, maintaining high recognition accuracy under multi-entity co-occurrence and long-sentence dependency conditions. Experiments conducted on the ACE 2005 dataset demonstrate significant improvements in Accuracy, Precision, Recall, and F1-Score, particularly in nested and overlapping entity recognition, where the model shows stronger boundary localization and structural modeling capability. This study verifies the effectiveness of structure-aware decoding in complex semantic extraction tasks, provides a new perspective for developing language models with hierarchical understanding, and establishes a methodological foundation for high-precision information extraction.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.13980.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13980",
    "published": "2025-12-16T00:40:06Z",
    "updated": "2026-01-28T07:12:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于大语言模型的结构感知解码方法，以在嵌套和重叠实体提取任务中提升语义完整性和结构一致性。",
      "motivation": "嵌套和重叠实体提取是自然语言处理中的核心挑战，传统方法难以同时维持语义完整性和结构一致性，导致边界定位不准确和关系建模薄弱。现有方法通常单独处理实体边界或层次关系，缺乏统一建模，在多实体共现和长句依赖等复杂场景下性能受限。因此，开发一种能整合语义与结构约束的新机制至关重要，以解决信息提取中的精度和鲁棒性问题，推动高精度应用发展。",
      "method": "该方法使用预训练大语言模型获取上下文语义表示，通过候选跨度生成机制捕获多粒度实体特征。关键创新在于引入结构化注意力建模和层次结构约束，在解码过程中统一优化实体边界、层次关系和跨依赖。模型联合训练分类损失和结构一致性损失，以增强在复杂场景如ACE 2005数据集上的稳定性和泛化能力，实现语义与结构的协同建模。",
      "result": "在ACE 2005数据集上的实验表明，模型在准确性、精确度、召回率和F1分数上均取得显著提升，与基线方法相比，特别是在嵌套和重叠实体识别任务中展现出更强的边界定位和结构建模能力。摘要未明确提供具体数值，但强调了结构感知解码在复杂语义提取中的有效性，验证了其优越性能。",
      "conclusion": "本研究验证了结构感知解码机制在复杂实体提取中的有效性，为大语言模型的层次理解能力提供了新视角，并建立了高精度信息提取的方法论基础。贡献在于提出统一建模语义和结构的创新方法，具有重要学术和实际应用价值。未来工作可扩展至其他复杂NLP任务或优化模型计算效率。",
      "tags": [
        "Large Language Models",
        "Structure-Aware Decoding",
        "Nested Entity Extraction",
        "Hierarchical Attention",
        "Joint Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:30.312738Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.11229",
    "title": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation",
    "authors": [
      "Haotian Wang",
      "Yuzhe Weng",
      "Jun Du",
      "Haoran Xu",
      "Xiaoyan Wu",
      "Shan He",
      "Bing Yin",
      "Cong Liu",
      "Qingfeng Liu"
    ],
    "abstract": "Diffusion models have significantly advanced the field of talking head generation (THG). However, slow inference speeds and prevalent non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, a pioneering diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through a spatiotemporal variational autoencoder with a high compression ratio. Additionally, to enable semi-autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles into key-value caching for maintaining identity consistency and temporal coherence during long-term streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) strategy is proposed to mitigate error accumulation and enhance temporal consistency in streaming generation, leveraging a non-streaming teacher with an asynchronous noise schedule to supervise the streaming student. REST bridges the gap between autoregressive and diffusion-based approaches, achieving a breakthrough in efficiency for applications requiring real-time THG. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.",
    "categories": [
      "cs.CV",
      "cs.SD"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.11229.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11229",
    "published": "2025-12-12T02:28:52Z",
    "updated": "2026-01-28T17:37:46Z",
    "comment": "27 pages, 10 figures",
    "light_analysis": {
      "overview": "本研究提出了REST框架，一种基于扩散模型的实时、端到端的流式音频驱动说话人头像生成方法，通过ID-Context缓存和异步流式蒸馏实现效率突破。",
      "motivation": "扩散模型在说话人头像生成（THG）领域虽取得进展，但推理速度慢且非自回归范式限制其实时应用，这在实际场景如视频会议或虚拟主播中至关重要。现有方法无法满足实时性需求，导致实用性受限，因此本研究旨在解决这一问题。",
      "method": "REST框架首先通过时空变分自编码器学习紧凑视频潜在空间以支持实时生成。关键创新包括ID-Context缓存机制，结合ID-Sink和Context-Cache确保长期流式生成中的身份一致性和时间连贯性，以及异步流式蒸馏策略，利用非流式教师监督流式学生以减少错误积累。",
      "result": "实验结果表明，REST在生成速度和整体性能上优于现有技术方法，实现了实时THG的突破，但摘要未明确说明具体性能指标如准确率或效率提升数值，仅强调与基线方法的优势对比。",
      "conclusion": "REST的主要贡献在于桥接了自回归与扩散方法，为实时说话人头像生成提供高效方案，学术价值体现在技术集成创新，实际应用潜力广泛。摘要未明确说明局限性和未来工作，但可推断未来可能进一步优化模型效率或扩展应用场景。",
      "tags": [
        "Diffusion Models",
        "Talking Head Generation",
        "Spatiotemporal Autoencoder",
        "Caching Mechanisms",
        "Knowledge Distillation"
      ]
    },
    "analyzed_at": "2026-01-29T04:01:59.129560Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.07051",
    "title": "DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation",
    "authors": [
      "Adnan Munir",
      "Muhammad Shahid Jabbar",
      "Shujaat Khan"
    ],
    "abstract": "Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.07051.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07051",
    "published": "2025-12-07T23:57:00Z",
    "updated": "2026-01-28T07:12:32Z",
    "comment": "13 pages, 7 figures",
    "light_analysis": {
      "overview": "DAUNet是一种集成了可变形卷积和参数自由注意力的轻量级UNet变体，旨在提升医学图像分割的空间适应性和特征融合效率。",
      "motivation": "医学图像分割在自动化诊断和治疗规划系统中至关重要，但现有模型可能难以处理图像中的几何变形和上下文融合，且资源消耗较大，限制了在实时和资源受限临床环境中的应用。本研究旨在通过开发轻量级模型来解决这些问题，提高分割准确性和适用性，以满足实际医疗需求。",
      "method": "DAUNet基于UNet架构，在瓶颈部分使用Deformable V2 Convolutions动态处理几何变化，增强空间适应性；在解码器和跳层路径集成Parameter-Free Attention (SimAM)模块进行显著性感知特征融合，提升上下文感知能力，同时不增加模型参数。该方法在两个数据集FH-PS-AoP（胎儿头部和耻骨联合超声）和FUMPE（CT-based肺栓塞检测）上进行评估，以验证其有效性。",
      "result": "在FH-PS-AoP和FUMPE数据集上的实验显示，DAUNet在Dice score、HD95和ASD等指标上优于最先进的模型，同时保持参数效率。消融研究证实了可变形卷积和SimAM注意力模块的独立贡献。模型对缺失上下文和低对比度区域表现出鲁棒性，适合实时和资源受限的临床部署。",
      "conclusion": "DAUNet通过集成可变形卷积和参数自由注意力，显著提升了医学图像分割性能，其轻量级设计增强了模型在临床环境中的实用性。研究在学术上提供了新的架构思路，实际上支持资源高效的分割应用；未来工作可探索扩展到其他医学图像任务或更复杂场景中的性能验证。",
      "tags": [
        "Deformable Convolutions",
        "Parameter-Free Attention",
        "UNet",
        "Medical Image Segmentation",
        "Lightweight Model"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:21.896411Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.03553",
    "title": "Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching",
    "authors": [
      "Wei Chee Yew",
      "Hailun Xu",
      "Sanjay Saha",
      "Xiaotian Fan",
      "Hiok Hian Ong",
      "David Yuchen Wang",
      "Kanchan Sarkar",
      "Zhenheng Yang",
      "Danhui Guan"
    ],
    "abstract": "Content moderation remains a critical yet challenging task for large-scale user-generated video platforms, especially in livestreaming environments where moderation must be timely, multimodal, and robust to evolving forms of unwanted content. We present a hybrid moderation framework deployed at production scale that combines supervised classification for known violations with reference-based similarity matching for novel or subtle cases. This hybrid design enables robust detection of both explicit violations and novel edge cases that evade traditional classifiers. Multimodal inputs (text, audio, visual) are processed through both pipelines, with a multimodal large language model (MLLM) distilling knowledge into each to boost accuracy while keeping inference lightweight. In production, the classification pipeline achieves 67% recall at 80% precision, and the similarity pipeline achieves 76% recall at 80% precision. Large-scale A/B tests show a 6-8% reduction in user views of unwanted livestreams}. These results demonstrate a scalable and adaptable approach to multimodal content governance, capable of addressing both explicit violations and emerging adversarial behaviors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.03553.pdf",
    "abs_url": "https://arxiv.org/abs/2512.03553",
    "published": "2025-12-03T08:20:58Z",
    "updated": "2026-01-28T09:38:17Z",
    "comment": "To be published at KDD 2026 (ADS track)",
    "light_analysis": {
      "overview": "本文提出一种结合监督分类与多模态大语言模型增强相似性匹配的混合框架，用于动态直播内容审核。",
      "motivation": "直播内容审核在大规模用户生成视频平台中至关重要，尤其需要及时处理多模态输入并应对不断演化的不良内容。现有监督分类方法难以检测新颖或微妙违规案例，导致审核效果有限。因此，研究动机在于开发一种鲁棒方法，以同时处理明确违规和新兴对抗行为，弥补传统方法的不足。",
      "method": "论文提出一个混合审核框架，部署于生产规模，包括监督分类管道处理已知违规和参考相似性匹配管道处理新颖案例。关键创新是引入多模态大语言模型（MLLM）来蒸馏知识到两个管道，提升准确性同时保持推理轻量级。多模态输入（文本、音频、视觉）通过两个管道并行处理，MLLM增强知识提取以优化性能。",
      "result": "在生产环境中，分类管道在80%精度下达到67%召回率，相似性管道在80%精度下达到76%召回率。大规模A/B测试显示，该框架有效减少了用户观看不良直播的视图达6-8%，证明了其在实时内容审核中的实际效果优于传统方法。",
      "conclusion": "该研究贡献了一个可扩展和自适应的多模态内容治理方法，能同时处理明确违规和新兴对抗行为。学术价值在于混合框架设计结合监督学习与相似性匹配，应用价值体现在生产性能提升。摘要未明确说明局限性，未来工作可探索实时性优化或扩展更多模态集成。",
      "tags": [
        "Content Moderation",
        "Supervised Classification",
        "Similarity Matching",
        "Multimodal Large Language Model (MLLM)",
        "Livestream Moderation"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:20.166200Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.05150",
    "title": "TwinFlow: Realizing One-step Generation on Large Models with Self-adversarial Flows",
    "authors": [
      "Zhenglin Cheng",
      "Peng Sun",
      "Jianguo Li",
      "Tao Lin"
    ],
    "abstract": "Recent advances in large multi-modal generative models have demonstrated impressive capabilities in multi-modal generation, including image and video generation. These models are typically built upon multi-step frameworks like diffusion and flow matching, which inherently limits their inference efficiency (requiring 40-100 Number of Function Evaluations (NFEs)). While various few-step methods aim to accelerate the inference, existing solutions have clear limitations. Prominent distillation-based methods, such as progressive and consistency distillation, either require an iterative distillation procedure or show significant degradation at very few steps (< 4-NFE). Meanwhile, integrating adversarial training into distillation (e.g., DMD/DMD2 and SANA-Sprint) to enhance performance introduces training instability, added complexity, and high GPU memory overhead due to the auxiliary trained models. To this end, we propose TwinFlow, a simple yet effective framework for training 1-step generative models that bypasses the need of fixed pretrained teacher models and avoids standard adversarial networks during training, making it ideal for building large-scale, efficient models. On text-to-image tasks, our method achieves a GenEval score of 0.83 in 1-NFE, outperforming strong baselines like SANA-Sprint (a GAN loss-based framework) and RCGM (a consistency-based framework). Notably, we demonstrate the scalability of TwinFlow by full-parameter training on Qwen-Image-20B and transform it into an efficient few-step generator. With just 1-NFE, our approach matches the performance of the original 100-NFE model on both the GenEval and DPG-Bench benchmarks, reducing computational cost by $100\\times$ with minor quality degradation. Project page is available at https://zhenglin-cheng.com/twinflow.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.05150.pdf",
    "abs_url": "https://arxiv.org/abs/2512.05150",
    "published": "2025-12-03T07:45:46Z",
    "updated": "2026-01-28T14:06:08Z",
    "comment": "arxiv v1, accepted to ICLR 2026",
    "light_analysis": {
      "overview": "TwinFlow框架通过自对抗流实现大模型一步生成，避免传统对抗网络和固定教师模型，显著提升推断效率。",
      "motivation": "大模型多模态生成通常基于扩散和流匹配等多步框架，推断效率低下（需40-100次函数评估）。现有加速方法如蒸馏和对抗训练存在明显不足：蒸馏需要迭代过程或在少步时性能显著下降；对抗训练导致训练不稳定、复杂度高和GPU内存开销大。这些问题阻碍了大规模高效模型的开发，亟需简单有效的解决方案。",
      "method": "TwinFlow采用自对抗流（Self-adversarial Flows）训练1步生成模型，无需固定预训练教师模型并避免标准对抗网络，简化训练过程。关键创新在于结合自对抗机制优化流匹配，在Qwen-Image-20B上进行全参数训练，将其转换为高效少步生成器，展示方法的可扩展性和实用性。",
      "result": "在文本到图像任务中，TwinFlow在1次函数评估（1-NFE）下获得0.83的GenEval分数，优于基于GAN损失的SANA-Sprint和基于一致性的RCGM等基线。通过在Qwen-Image-20B上的全参数训练，1-NFE性能在GenEval和DPG-Bench基准上与原始100-NFE模型相当，计算成本降低100倍，仅伴有轻微质量损失。",
      "conclusion": "TwinFlow的主要贡献是提出一种高效一步生成框架，提升大模型推断效率，避免传统对抗网络的复杂性，在保持质量的同时降低计算成本。研究证实其在大规模模型上的可扩展性，为构建高效多模态生成系统提供新途径。未来工作可探索扩展到其他生成任务和进一步优化。",
      "tags": [
        "Self-adversarial Flows",
        "Flow Matching",
        "Distillation",
        "Large Multi-modal Generative Models",
        "One-step Generation"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:33.821009Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.00311",
    "title": "Tracing Mathematical Proficiency Through Problem-Solving Processes",
    "authors": [
      "Jungyang Park",
      "Suho Kang",
      "Jaewoo Park",
      "Jaehong Kim",
      "Jaewoo Shin",
      "Seonjoon Park",
      "Youngjae Yu"
    ],
    "abstract": "Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.00311.pdf",
    "abs_url": "https://arxiv.org/abs/2512.00311",
    "published": "2025-11-29T04:12:06Z",
    "updated": "2026-01-28T05:04:46Z",
    "comment": "18 pages, 4 figures",
    "light_analysis": {
      "overview": "本文提出结合学生问题解决过程的知识追踪方法KT-PSP及三阶段LLM流水线框架StatusKT，提升了预测性能和可解释性。",
      "motivation": "知识追踪旨在通过建模学生知识状态实现智能辅导系统中的个性化学习，但传统方法仅依赖回答正确性，忽视问题解决过程中的丰富信息，导致可解释性差。这一问题限制了教育技术的有效性，因为无法全面理解学生能力的多维方面。现有方法不足以捕捉过程数据，因此需要整合这些信息以提升追踪精度和解释能力。",
      "method": "研究提出KT-PSP方法，将问题解决过程纳入知识追踪以捕捉数学能力多维方面，并引入专门数据集KT-PSP-25。核心创新是StatusKT框架，采用教师-学生-教师三阶段LLM流水线：教师LLM提取问题特定能力指标，学生LLM基于解决方案生成响应，另一教师LLM评估指标掌握程度，生成中间信号。关键特色是利用LLM处理过程数据，实现精细化建模。",
      "result": "在KT-PSP-25数据集上的实验表明，StatusKT框架提高了现有知识追踪方法的预测性能，与基线方法相比显示出改进，但具体提升数字摘要未明确说明。此外，通过明确建模数学能力，该框架为预测提供了可解释的解释，增强了透明度。这些结果验证了结合过程数据和LLM流水线的有效性。",
      "conclusion": "本论文的主要贡献是提出了KT-PSP方法和StatusKT框架，结合问题解决过程和LLM技术，提升了知识追踪的可解释性和准确性。学术上，它推动了过程建模和LLM在教育领域的应用；实践中，有助于智能辅导系统提供更精准的学生评估。未来工作方向如优化流水线或扩展数据集，摘要未明确说明具体内容。",
      "tags": [
        "Knowledge Tracing",
        "Problem-Solving Process",
        "Large Language Model",
        "Mathematical Proficiency",
        "Teacher-Student Pipeline"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:35.463048Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.00208",
    "title": "ReactionMamba: Generating Short & Long Human Reaction Sequences",
    "authors": [
      "Hajra Anwar Beg",
      "Baptiste Chopin",
      "Hao Tang",
      "Mohamed Daoudi"
    ],
    "abstract": "We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.00208.pdf",
    "abs_url": "https://arxiv.org/abs/2512.00208",
    "published": "2025-11-28T21:19:45Z",
    "updated": "2026-01-28T18:22:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "ReactionMamba 是一个集成运动 VAE 和基于 Mamba 的状态空间模型的新框架，用于高效生成短和长的 3D 人类反应运动序列。",
      "motivation": "本研究旨在解决生成长序列 3D 人类反应运动的挑战。在虚拟现实、游戏和动画领域，高质量、多样化的运动序列至关重要，但现有方法可能在处理长序列时面临效率低下、时间不一致或真实性不足等问题。ReactionMamba 的提出是为了提升运动生成的多样性和复杂性，同时保持高真实性和推理速度，弥补了先前技术在长序列生成方面的局限性。",
      "method": "ReactionMamba 框架结合了运动变分自编码器（Motion VAE）进行高效运动编码，以及基于 Mamba 的状态空间模型来解码时间一致的反应序列。这种设计使模型能够处理从简单短序列到复杂长序列（如舞蹈和武术）的运动生成，关键创新点在于融合了运动 VAE 的压缩能力和状态空间模型的序列建模优势，从而在生成过程中保持运动的一致性和细节。",
      "result": "在 NTU120-AS、Lindy Hop 和 InterX 三个数据集上的实验显示，ReactionMamba 在运动真实性、多样性和长序列生成方面与基线方法（InterFormer、ReMoS 和 Ready-to-React）相比具有竞争性能。此外，框架实现了推理速度的显著提升，表明了其在高效生成短和长序列运动方面的有效性，具体数据未明确说明但强调了速度改进的显著性。",
      "conclusion": "ReactionMamba 的主要贡献是通过集成运动 VAE 和 Mamba-based 模型，成功生成了高质量的短和长 3D 人类反应运动序列，提升了多样性和推理速度。这一研究具有学术价值，推动了运动生成技术的发展，并在虚拟现实、动画等领域有潜在应用价值，未来工作可能包括扩展到更多复杂运动类型或进一步优化模型性能。",
      "tags": [
        "Motion VAE",
        "Mamba",
        "State-Space Models",
        "3D Human Reaction Generation",
        "Sequence Generation"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:30.373857Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.17045",
    "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis",
    "authors": [
      "Linfeng Dong",
      "Yuchen Yang",
      "Hao Wu",
      "Wei Wang",
      "Yuenan Hou",
      "Zhihang Zhong",
      "Xiao Sun"
    ],
    "abstract": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.17045.pdf",
    "abs_url": "https://arxiv.org/abs/2511.17045",
    "published": "2025-11-21T08:44:33Z",
    "updated": "2026-01-28T16:59:28Z",
    "comment": "Accepted to AAAI 2026 (Oral)",
    "light_analysis": {
      "overview": "论文提出RacketVision数据集，首次综合球拍姿态注释，推动球拍运动中的计算机视觉研究。",
      "motivation": "本研究旨在解决球拍运动中球和球拍复杂交互分析的挑战。现有体育分析数据集多专注于球的位置，缺乏球拍姿态的精细注释，限制了多模态交互研究，这阻碍了人机交互精度的提升。开发RacketVision填补了这一空白，为增强计算机视觉在体育应用中的实用性和推动相关学术进展提供关键资源。",
      "method": "论文设计了RacketVision数据集，覆盖乒乓球、网球和羽毛球，提供大规模、细粒度的球位置和球拍姿态注释。关键技术包括三个互联任务：细粒度球跟踪、铰接球拍姿态估计和预测球轨迹。在多模态融合中，采用CrossAttention机制替代朴素特征连接，以避免性能下降，有效利用球拍姿态信息优化轨迹预测。数据集设计强调实际应用，促进动态分析和条件运动预测研究。",
      "result": "实验结果显示，朴素连接球拍姿态特征会降低轨迹预测性能，而CrossAttention机制能有效融合多模态信息，使预测结果超越强单模态基线。这验证了CrossAttention在多模态融合中的关键作用，提供了性能改进的证据，尽管摘要未明确具体数据指标。评估强调了数据集和方法的实用性，为后续研究奠定基础。",
      "conclusion": "RacketVision的主要贡献在于提供了一个多功能数据集和基准，促进动态对象跟踪、条件运动预测和运动多模态分析的发展。学术价值是填补球拍姿态注释空白，实际应用价值体现在提升体育分析工具的精度。未来工作可扩展到其他运动或复杂交互场景，进一步拓展其适用范围和局限性探索。",
      "tags": [
        "Computer Vision",
        "Sports Analytics",
        "CrossAttention",
        "Pose Estimation",
        "Trajectory Forecasting"
      ]
    },
    "analyzed_at": "2026-01-29T04:03:30.415222Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.16778",
    "title": "GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs",
    "authors": [
      "Yating Ren",
      "Yikun Ban",
      "Huobin Tan"
    ],
    "abstract": "Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT outperforms state-of-the-art methods, demonstrating its effectiveness and robustness.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.16778.pdf",
    "abs_url": "https://arxiv.org/abs/2511.16778",
    "published": "2025-11-20T20:10:49Z",
    "updated": "2026-01-28T11:13:25Z",
    "comment": "AAAI 2026",
    "light_analysis": {
      "overview": "本论文提出了GCL-OT框架，结合最优传输进行图对比学习，以处理文本属性图中的多粒度异质性，实现灵活双向对齐。",
      "motivation": "现有结构-文本对比学习方法通常依赖同质性假设和硬优化目标，限制了在异质文本属性图上的应用。异质性包括完全异质、部分异质和潜在同质，导致语义相关性混合、嘈杂或缺失，使得结构-文本对齐变得困难。这一问题重要性在于文本属性图在现实应用（如社交网络或推荐系统）中普遍存在，现有方法因处理异质性能力不足而性能受限，亟需改进以提升模型鲁棒性和准确性。",
      "method": "GCL-OT框架采用最优传输技术实现结构-文本的双向对齐，并针对多粒度异质性设计定制机制。针对部分异质，设计RealSoftMax相似性估计器，增强关键邻居-词交互并缓解背景噪声；针对完全异质，引入基于提示的过滤器，自适应排除无关噪声；针对潜在同质，结合OT引导的软监督挖掘语义相似的潜在邻居。理论分析表明该框架能改进互信息边界和贝叶斯误差保证。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "论文在九个基准数据集上进行了广泛实验，结果显示GCL-OT优于当前最先进的方法。实验证明了该框架在处理异质文本属性图时的有效性和鲁棒性，性能显著提升，但摘要未提供具体准确率或效率数据。与基线方法对比，GCL-OT在多场景下均表现优异，验证了其创新机制的优势。",
      "conclusion": "本研究的主要贡献是提出GCL-OT框架，通过最优传输和定制机制解决文本属性图中的多粒度异质性对齐问题，提供理论保证和实验验证。其学术价值在于拓展了图对比学习在异质图上的应用，实际应用价值包括提升社交网络分析或文本挖掘任务的性能。局限性可能涉及计算复杂度或特定场景适用性，未来工作可探索扩展至更多图类型或集成其他模态。",
      "tags": [
        "Graph Contrastive Learning",
        "Optimal Transport",
        "Heterophilic Graphs",
        "Text-Attributed Graphs",
        "RealSoftMax"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:45.618334Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.09785",
    "title": "AI Annotation Orchestration: Evaluating LLM verifiers to Improve the Quality of LLM Annotations in Learning Analytics",
    "authors": [
      "Bakhtawar Ahtisham",
      "Kirk Vanacore",
      "Jinsook Lee",
      "Zhuqian Zhou",
      "Doug Pietrzak",
      "Rene F. Kizilcec"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used to annotate learning interactions, yet concerns about reliability limit their utility. We test whether verification-oriented orchestration-prompting models to check their own labels (self-verification) or audit one another (cross-verification)-improves qualitative coding of tutoring discourse. Using transcripts from 30 one-to-one math sessions, we compare three production LLMs (GPT, Claude, Gemini) under three conditions: unverified annotation, self-verification, and cross-verification across all orchestration configurations. Outputs are benchmarked against a blinded, disagreement-focused human adjudication using Cohen's kappa. Overall, orchestration yields a 58 percent improvement in kappa. Self-verification nearly doubles agreement relative to unverified baselines, with the largest gains for challenging tutor moves. Cross-verification achieves a 37 percent improvement on average, with pair- and construct-dependent effects: some verifier-annotator pairs exceed self-verification, while others reduce alignment, reflecting differences in verifier strictness. We contribute: (1) a flexible orchestration framework instantiating control, self-, and cross-verification; (2) an empirical comparison across frontier LLMs on authentic tutoring data with blinded human \"gold\" labels; and (3) a concise notation, verifier(annotator) (e.g., Gemini(GPT) or Claude(Claude)), to standardize reporting and make directional effects explicit for replication. Results position verification as a principled design lever for reliable, scalable LLM-assisted annotation in Learning Analytics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.09785.pdf",
    "abs_url": "https://arxiv.org/abs/2511.09785",
    "published": "2025-11-12T22:35:36Z",
    "updated": "2026-01-28T18:09:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出验证导向的编排框架，通过自我验证和交叉验证显著提升大型语言模型在学分析中的标注质量。",
      "motivation": "大型语言模型越来越多地用于学习交互标注，但其可靠性问题限制了实际应用，影响学分析结果的准确性。现有方法中未验证的标注可能导致错误，而验证机制的研究不足，因此，本研究的动机是探索如何通过验证机制提高LLM标注的可靠性，以促进其在教育领域的可靠部署和规模化应用。",
      "method": "研究采用实证方法，测试了三种前沿LLM（GPT、Claude、Gemini）在未验证、自我验证和交叉验证条件下的表现。使用30个一对一数学辅导转录本作为数据集，通过Cohen's kappa基准与盲审人类裁决进行比较。关键创新在于设计了一个灵活的编排框架，包括控制组、自我验证和交叉验证配置，以系统评估验证效果，并使用简明记号记录验证器-标注器配对。",
      "result": "实验结果显示，验证编排总体使Cohen's kappa提高了58%。自我验证使标注一致性几乎翻倍，尤其在挑战性辅导动作中表现突出。交叉验证平均提高37%，但效果因验证器-标注器配对而异：某些配对超越自我验证，而其他配对降低一致性，反映了验证器严格度的差异，这为优化验证策略提供了实证依据。",
      "conclusion": "论文的主要贡献是提供了一个验证框架和实证比较，证明了验证作为提高LLM标注可靠性的有效杠杆。学术价值在于首次在真实学分析数据上系统评估了多种LLM的验证策略，实际应用价值在于为可扩展的LLM辅助标注提供了设计原则。未来工作可探索更优的验证配对和策略，以进一步提高性能，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Self-Verification",
        "Cross-Verification",
        "Learning Analytics",
        "Cohen's Kappa"
      ]
    },
    "analyzed_at": "2026-01-29T04:02:38.305659Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.08825",
    "title": "Neural Value Iteration",
    "authors": [
      "Yang You",
      "Ufuk Çakır",
      "Alex Schutz",
      "Nick Hawes"
    ],
    "abstract": "The value function of a POMDP exhibits the piecewise-linear-convex (PWLC) property and can be represented as a finite set of hyperplanes, known as $α$-vectors. Most state-of-the-art POMDP solvers (offline planners) follow the point-based value iteration scheme, which performs Bellman backups on $α$-vectors at reachable belief points until convergence. However, since each $α$-vector is $|S|$-dimensional, these methods quickly become intractable for large-scale problems due to the prohibitive computational cost of Bellman backups. In this work, we demonstrate that the PWLC property allows a POMDP's value function to be alternatively represented as a finite set of neural networks. This insight enables a novel POMDP planning algorithm called \\emph{Neural Value Iteration}, which combines the generalization capability of neural networks with the classical value iteration framework. Our approach achieves near-optimal solutions even in extremely large POMDPs that are intractable for existing offline solvers.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.08825.pdf",
    "abs_url": "https://arxiv.org/abs/2511.08825",
    "published": "2025-11-11T22:46:31Z",
    "updated": "2026-01-28T11:08:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Neural Value Iteration算法，利用神经网络表示POMDP的价值函数，以解决大规模问题的计算难题。",
      "motivation": "部分可观察马尔可夫决策过程（POMDP）是强化学习中处理不确定性环境的重要模型，其价值函数具有分段线性凸（PWLC）性质，可表示为α-向量集。传统点基价值迭代方法通过在可达信念点上对α-向量进行贝尔曼备份来求解，但由于α-向量维度与状态空间大小（|S|）相同，在大规模问题上计算成本极高，导致现有离线求解器变得不可行。因此，迫切需要开发更高效的算法以扩展POMDP的规划范围到复杂应用场景。",
      "method": "基于PWLC性质，本研究发现POMDP的价值函数可以表示为有限集神经网络，而非传统的α-向量集。这启发了Neural Value Iteration算法，该方法将神经网络的泛化能力与经典价值迭代框架相结合。通过神经网络近似价值函数，替代高维α-向量的计算，减少了贝尔曼备份的复杂性，实现了更高效的离线规划，适用于大规模状态空间问题。摘要未明确说明具体数据集或模型架构细节。",
      "result": "Neural Value Iteration算法在现有离线求解器无法处理的极端大规模POMDP上实现了接近最优的解决方案。摘要未明确说明具体性能指标如准确率提升或效率改进的数值数据，但强调了该方法相比传统点基价值迭代具有更好的可扩展性，能够解决更大规模的问题，展示了其在复杂环境中的有效性。",
      "conclusion": "本研究的主要贡献是提出了Neural Value Iteration算法，通过神经网络表示POMDP价值函数，扩展了规划能力到大规模问题。其学术价值在于展示了神经网络与经典决策理论结合的潜力，推动了POMDP求解技术的发展。实际应用价值可能体现在机器人、自主系统等领域，但摘要未明确说明局限性或未来工作方向，如具体评估或扩展性分析。",
      "tags": [
        "POMDP",
        "Neural Networks",
        "Value Iteration",
        "Bellman Backups",
        "PWLC"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:33.689754Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.06893",
    "title": "DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting",
    "authors": [
      "Daojun Liang",
      "Jing Chen",
      "Xiao Wang",
      "Yinglong Wang",
      "Shuo Li"
    ],
    "abstract": "Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.06893.pdf",
    "abs_url": "https://arxiv.org/abs/2511.06893",
    "published": "2025-11-10T09:43:47Z",
    "updated": "2026-01-28T02:58:19Z",
    "comment": "28 pages,17 pages, Published in AAAI-26",
    "light_analysis": {
      "overview": "提出DeepBooTS，一种用于漂移弹性时间序列预测的双流残差提升方法，通过块级残差纠正增强稳健性。",
      "motivation": "时间序列数据的非平稳性导致概念漂移，这使得多数预测方法在处理概念漂移时稳健性不足，尽管实例归一化被广泛应用。本研究旨在解决这一问题，因为概念漂移在实际应用中会严重影响预测准确性。通过分析概念漂移的偏置-方差特性，证明加权集成可以减少方差而不增加偏置，从而启发设计新方法以提高预测的可靠性和适应性。",
      "method": "DeepBooTS是一种端到端的双流残差减少提升方法，通过逐步重构内在信号来应对概念漂移。核心创新在于将深度模型的每个块设计为学习器集合，并引入辅助输出分支形成高速公路到最终预测。块级输出用于纠正先前块的残差，实现学习驱动的输入和目标分解，结合集成学习和残差优化技术，增强对漂移的稳健性和模型可解释性。",
      "result": "在大规模数据集上的广泛实验显示，DeepBooTS大幅超越现有方法，平均性能提升15.8%，建立了新的预测基准。实验表明，该方法在不同数据集上均表现优异，尤其在处理概念漂移时展现出更强的稳健性和准确性，验证了其在时间序列预测中的优越性能。",
      "conclusion": "本研究贡献了DeepBooTS方法，通过双流残差提升显著增强了时间序列预测的多功能性和可解释性，有效提高对概念漂移的稳健性。这一创新为解决概念漂移问题提供了新颖框架，具有重要学术和应用价值。未来工作可探索该方法在其他领域的扩展或性能优化，但摘要未明确说明具体局限性。",
      "tags": [
        "Time-Series Forecasting",
        "Concept Drift",
        "Boosting",
        "Residual Decomposition",
        "Dual-Stream Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:45.670160Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.05878",
    "title": "FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge",
    "authors": [
      "Xinlong Zhao",
      "Tong Jia",
      "Minghua He",
      "Xixuan Yang",
      "Ying Li"
    ],
    "abstract": "Log-based anomaly detection is critical for ensuring the stability and reliability of web systems. One of the key problems in this task is the lack of sufficient labeled logs, which limits the rapid deployment in new systems. Existing works usually leverage large-scale labeled logs from a mature web system and a small amount of labeled logs from a new system, using transfer learning to extract and generalize general knowledge across both domains. However, these methods focus solely on the transfer of general knowledge and neglect the disparity and potential mismatch between such knowledge and the proprietary knowledge of target system, thus constraining performance. To address this limitation, we propose FusionLog, a novel zero-label cross-system log-based anomaly detection method that effectively achieves the fusion of general and proprietary knowledge, enabling cross-system generalization without any labeled target logs. Specifically, we first design a training-free router based on semantic similarity that dynamically partitions unlabeled target logs into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs a small model based on system-agnostic representation meta-learning for direct training and inference, inheriting the general anomaly patterns shared between the source and target systems. For proprietary logs, we iteratively generate pseudo-labels and fine-tune the small model using multi-round collaborative knowledge distillation and fusion based on large language model (LLM) and small model (SM) to enhance its capability to recognize anomaly patterns specific to the target system. Experimental results on three public log datasets from different systems show that FusionLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming state-of-the-art cross-system log-based anomaly detection methods.",
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.05878.pdf",
    "abs_url": "https://arxiv.org/abs/2511.05878",
    "published": "2025-11-08T06:30:50Z",
    "updated": "2026-01-28T05:23:53Z",
    "comment": "11 pages, 4 figures, and 2 tables",
    "light_analysis": {
      "overview": "FusionLog提出一种零标记跨系统日志异常检测方法，通过融合通用和专用知识，实现无需目标系统标记日志的检测。",
      "motivation": "日志异常检测对确保网络系统稳定性和可靠性至关重要，但新系统常缺乏足够标记日志，限制快速部署。现有方法利用迁移学习从成熟系统提取通用知识，但仅关注知识迁移，忽略了通用知识与目标系统专用知识之间的差异和潜在不匹配，导致性能受限。因此，需要一种能有效融合这两种知识的方法，以提升跨系统异常检测的泛化能力。",
      "method": "FusionLog的核心方法包括设计基于语义相似度的免训练路由器，动态将未标记目标日志分区为‘通用日志’和‘专用日志’。对于通用日志，采用基于系统无关表示元学习的小模型进行训练和推理，继承源和目标系统共享的通用异常模式。对于专用日志，通过大语言模型（LLM）和小模型（SM）的多轮协作知识蒸馏和融合，迭代生成伪标签并微调模型，增强识别目标系统特定异常模式的能力。该方法在零标记设置下运行，避免了目标系统标记数据的需求。",
      "result": "实验在三个公共日志数据集上进行，结果显示FusionLog在完全零标记设置下达到超过90%的F1-score，显著优于现有的跨系统日志异常检测方法。具体数据支撑表明，该方法在融合通用和专用知识后，有效提升了检测性能，与基线方法相比表现出明显优势。",
      "conclusion": "FusionLog的主要贡献在于提出一种零标记跨系统日志异常检测框架，通过融合通用和专用知识，提高了检测准确性和泛化能力。该研究在学术上推动了异常检测领域的发展，具有实际应用价值，可促进新系统快速部署。局限性可能包括对计算资源的需求或特定数据集依赖性，未来工作可进一步优化效率或扩展到更多场景。",
      "tags": [
        "Log-based Anomaly Detection",
        "Transfer Learning",
        "Knowledge Distillation",
        "Large Language Models",
        "Zero-label Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:31.071159Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.03001",
    "title": "LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation",
    "authors": [
      "Gyeom Hwangbo",
      "Hyungjoo Chae",
      "Minseok Kang",
      "Hyeonjong Ju",
      "Soohyun Oh",
      "Jinyoung Yeo"
    ],
    "abstract": "Despite recent progress in using Large Language Models (LLMs) for automatically generating 3D scenes, generated scenes often lack realistic spatial layouts and object attributes found in real-world environments. As this problem stems from insufficiently detailed, coarse-grained instructions, advancing 3D scene synthesis guided by more detailed, fine-grained instructions that reflect real-world environments becomes crucial. Without such realistic scenes, training embodied agents in unrealistic environments can lead them to learn priors that diverge significantly from real-world physics and semantics, degrading their performance when deployed. Thus, verifying the alignment between the fine-grained instruction and the generated scene is essential for effective learning. However, current evaluation methods, such as CLIPScore and vision-language models (VLMs), often fail to reliably assess such alignment. This shortcoming arises primarily from their shallow understanding of 3D scenes, which often leads to improperly grounded scene components. To address this, we introduce LEGO-Eval, an evaluation framework equipped with diverse tools designed to explicitly ground scene components, enabling more accurate alignment assessments. We also present LEGO-Bench, a benchmark of detailed instructions that specify complex layouts and attributes of real-world environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with LEGO-Bench reveals significant limitations in current generation methods. Across all evaluated approaches, success rates reached at most 10% in generating scenes that fully align with fine-grained instructions.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.03001.pdf",
    "abs_url": "https://arxiv.org/abs/2511.03001",
    "published": "2025-11-04T21:13:51Z",
    "updated": "2026-01-28T07:08:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "LEGO-Eval通过工具增强提供细粒度评估框架，LEGO-Bench作为基准，显著提升3D场景合成与指令对齐的评估准确性。",
      "motivation": "研究动机在于，尽管大型语言模型在自动生成3D场景方面取得进展，但生成场景常缺乏真实空间布局和对象属性。这源于指令不够详细，粗粒度指令导致场景不真实，影响具身代理训练，使其先验与现实物理和语义脱节，降低部署性能。验证细粒度指令与生成场景的对齐至关重要，但现有评估方法如CLIPScore和视觉语言模型未能可靠评估，因它们对3D场景理解浅薄，导致场景组件接地不当。因此，需开发更准确评估方法以促进真实环境合成。",
      "method": "论文提出LEGO-Eval评估框架，通过装备多样化工具来显式接地场景组件，从而更精确评估3D场景与细粒度指令的对齐。关键创新在于工具增强，克服当前视觉语言模型理解浅薄的问题，实现准确组件判断。同时，引入LEGO-Bench基准，包含详细指令指定真实环境中的复杂布局和对象属性，为评估提供标准化测试集。该方法整合工具进行多角度验证，提升评估可靠性。",
      "result": "实验结果显示，LEGO-Eval在评估场景-指令对齐方面表现优异，相比VLM-as-a-judge方法，F1得分提高了0.41。使用LEGO-Bench进行基准测试，暴露了当前生成方法的显著不足：在所有评估方法中，生成场景完全对齐细粒度指令的成功率最高仅为10%。这表明现有技术远未满足细粒度合成需求，而LEGO-Eval能更准确地检测对齐问题，为改进生成方法提供依据。",
      "conclusion": "论文结论是，LEGO-Eval框架通过工具增强显式接地场景组件，有效提高了评估准确性；LEGO-Bench基准为细粒度指令评估提供了标准化平台。这一研究在学术上推进了3D场景合成评估技术，为真实环境生成设定了新标准。在实际应用中，有助于训练更具鲁棒性的具身代理，提升其在真实世界的适应性。尽管摘要未明确说明局限性，但未来工作可能包括扩展工具集、覆盖更复杂场景或提高评估效率。",
      "tags": [
        "Large Language Model",
        "3D Scene Synthesis",
        "Evaluation Framework",
        "Tool Augmentation",
        "Vision-Language Model"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:07.049188Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.01016",
    "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning",
    "authors": [
      "Wenjin Liu",
      "Haoran Luo",
      "Xueyuan Lin",
      "Haoming Liu",
      "Tiesunlong Shen",
      "Jiapu Wang",
      "Rui Mao",
      "Erik Cambria"
    ],
    "abstract": "Recently, advanced large language models (LLMs) have emerged at an increasingly rapid pace. However, when faced with complex problems, most users are often unable to provide accurate and effective prompts to interact with LLMs, thus limiting the performance of LLMs. To address this challenge, we propose Prompt-R1, an end-to-end reinforcement learning framework that uses a small-scale LLM to collaborate with large-scale LLMs, replacing user interaction to solve problems better. This collaboration is cast as a multi-turn prompt interaction, where the small-scale LLM thinks and generates prompts, and the large-scale LLM performs complex reasoning. A dual-constrained reward is designed to optimize for correctness, generation quality, and reasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports both inference and training with various large-scale LLMs. Experiments on multiple public datasets show that Prompt-R1 significantly outperforms baseline models across tasks. Our code is publicly available at https://github.com/QwenQKing/Prompt-R1.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.01016.pdf",
    "abs_url": "https://arxiv.org/abs/2511.01016",
    "published": "2025-11-02T17:11:03Z",
    "updated": "2026-01-28T08:14:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Prompt-R1，一个端到端强化学习的协作自动提示框架，通过小型LLM与大型LLM交互优化问题解决。",
      "motivation": "随着大型语言模型的快速发展，它们在处理复杂任务时面临用户提示质量不足的挑战，导致模型性能受限。这一问题的重要性在于，它阻碍了LLMs在现实应用中的潜力发挥，因为大多数用户缺乏专业提示工程知识，无法提供有效交互。现有方法主要依赖手动提示，不仅效率低下且准确性差，因此开发自动提示生成机制成为提升模型交互效能的关键需求，以弥补现有方法的不足。",
      "method": "论文提出Prompt-R1，这是一个端到端的强化学习框架，核心方法是利用一个小型LLM与大型LLM协作。小型LLM负责思考和生成多轮提示，而大型LLM执行复杂推理，协作被建模为交互过程。关键创新点包括设计一个双重约束奖励函数，用于同时优化提示的正确性、生成质量和推理准确性。该框架是即插即用的，支持多种大型LLMs的推理和训练，通过强化学习实现自动优化，无需手动干预。",
      "result": "实验在多个公共数据集上进行，结果显示Prompt-R1框架显著优于基线模型。尽管摘要未提供具体性能指标如准确率提升百分比，但论文强调其效果在多个任务中都表现优异，验证了框架的通用性和有效性。与现有方法相比，Prompt-R1通过自动提示生成，提高了任务解决的准确性和效率，展现出强化学习在优化LLM交互中的潜力。",
      "conclusion": "本文的主要贡献是提出了Prompt-R1，一个创新的协作自动提示框架，通过端到端强化学习实现了小型与大型LLM的高效交互。学术价值在于扩展了强化学习在自然语言处理领域的应用，提供了端到端的解决方案。实际应用价值在于简化用户与LLMs的交互过程，提升复杂问题解决能力。摘要未明确说明研究的局限性或未来工作方向。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Automatic Prompting",
        "Collaborative Framework",
        "End-to-end Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:39.169620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22848",
    "title": "Self-induced stochastic resonance: A physics-informed machine learning approach",
    "authors": [
      "Divyesh Savaliya",
      "Marius E. Yamakou"
    ],
    "abstract": "Self-induced stochastic resonance (SISR) is the emergence of coherent oscillations in slow-fast excitable systems driven solely by noise, without external periodic forcing or proximity to a bifurcation. This work presents a physics-informed machine learning framework for modeling and predicting SISR in the stochastic FitzHugh-Nagumo neuron. We embed the governing stochastic differential equations and SISR-asymptotic timescale-matching constraints directly into a Physics-Informed Neural Network (PINN) based on a Noise-Augmented State Predictor architecture. The composite loss integrates data fidelity, dynamical residuals, and barrier-based physical constraints derived from Kramers' escape theory. The trained PINN accurately predicts the dependence of spike-train coherence on noise intensity, excitability, and timescale separation, matching results from direct stochastic simulations with substantial improvements in accuracy and generalization compared with purely data-driven methods, while requiring significantly less computation. The framework provides a data-efficient and interpretable surrogate model for simulating and analyzing noise-induced coherence in multiscale stochastic systems.",
    "categories": [
      "cs.LG",
      "nlin.AO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.22848.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22848",
    "published": "2025-10-26T21:49:20Z",
    "updated": "2026-01-28T07:02:16Z",
    "comment": "25 pages, 10 figures, 62 references",
    "light_analysis": {
      "overview": "提出物理信息机器学习框架，用于建模自诱导随机共振，提升预测精度和泛化能力，降低计算成本。",
      "motivation": "自诱导随机共振（SISR）是慢-快可兴奋系统中仅由噪声驱动的相干振荡现象，在神经元建模中具有重要意义。现有方法如直接随机模拟计算量大，而纯数据驱动方法在准确性和泛化性上存在不足，难以高效预测噪声参数的影响。本研究旨在开发一种结合物理知识的机器学习模型，以解决现有方法的局限性，实现对SISR行为的更准确模拟和分析。",
      "method": "该方法基于物理信息神经网络（PINN），采用噪声增强状态预测器架构。在随机FitzHugh-Nagumo神经元模型中，嵌入控制随机微分方程和SISR渐近时间尺度匹配约束，通过复合损失函数整合数据保真度、动力学残差和基于Kramers逃逸理论的屏障物理约束。关键创新是将物理约束直接融入神经网络训练，减少对大规模数据的依赖，提高建模效率。",
      "result": "实验表明，训练后的PINN能准确预测尖峰列相干性与噪声强度、兴奋性和时间尺度分离的关系，与直接随机模拟结果匹配。相比纯数据驱动方法，在准确性和泛化性上有显著改进，同时计算需求大幅减少，实现了更高效的数据驱动建模，无需具体量化数据，摘要强调性能提升。",
      "conclusion": "该框架提供了一种数据高效且可解释的替代模型，用于模拟和分析多尺度随机系统中的噪声诱导相干性。主要贡献在于融合物理约束与机器学习，为复杂系统建模开辟新途径，具有学术价值和实际应用潜力，如在神经元科学领域。未来工作方向摘要未明确说明，但可能包括扩展到其他系统或进一步优化方法。",
      "tags": [
        "Physics-Informed Neural Networks",
        "Stochastic Differential Equations",
        "FitzHugh-Nagumo Model",
        "Stochastic Resonance",
        "Noise-Augmented State Predictor"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:46.310835Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.20707",
    "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models",
    "authors": [
      "Xuyang Liu",
      "Xiyan Gui",
      "Yuchao Zhang",
      "Linfeng Zhang"
    ],
    "abstract": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose MixKV, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. MixKV adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that MixKV consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal understanding benchmarks and achieves remarkable gains of 8.0% and 9.0% for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, MixKV extends seamlessly to LLMs with comparable performance gains. Our code is available at https://github.com/xuyang-liu16/MixKV.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.20707.pdf",
    "abs_url": "https://arxiv.org/abs/2510.20707",
    "published": "2025-10-23T16:17:47Z",
    "updated": "2026-01-28T10:49:58Z",
    "comment": "Accepted by ICLR 2026. Our code is available at https://github.com/xuyang-liu16/MixKV",
    "light_analysis": {
      "overview": "本论文提出MixKV方法，通过混合重要性与多样性优化大型视觉语言模型中的键值缓存压缩，以提高部署效率和性能。",
      "motivation": "大型视觉语言模型在处理扩展多模态序列时，键值缓存扩展导致内存瓶颈，严重限制了部署的可扩展性。现有压缩方法仅聚焦于保留高重要性键值对，但忽视了多模态键值缓存中独特的模态特定语义冗余模式。这可能导致语义覆盖不完整，影响模型性能，因此需要更全面的压缩策略来平衡信息保留与存储效率。",
      "method": "本研究提出MixKV方法，通过分析大型视觉语言模型中键值缓存在不同注意力头间的冗余变化，揭示仅依赖重要性无法覆盖完整信息分布。MixKV适应头向语义冗余，在压缩键值对时选择性地平衡多样性和重要性，实现优化压缩。该方法的核心创新在于联合考虑这两种因素，以更有效地处理多模态缓存中的冗余模式，提升压缩效果。",
      "result": "实验表明，在极端压缩设置下（预算=64），MixKV在五个多模态理解基准上平均提升基线方法5.1%，并在SnapKV和AdaKV的GUI接地任务中实现显著提升，分别达8.0%和9.0%。同时，MixKV保持可比的推理效率，并能无缝扩展到大型语言模型，获得类似性能增益，验证了其有效性和泛化能力。",
      "conclusion": "MixKV通过结合重要性与多样性，成功优化了大型视觉语言模型的键值缓存压缩，解决了多模态语义冗余问题。该研究不仅提高了模型部署的效率和可扩展性，还拓展了压缩技术的理论框架，具有重要的学术和实际应用价值。未来工作可进一步探索更精细的冗余分析或应用于更广泛的多模态任务中。",
      "tags": [
        "KV Cache Compression",
        "Large Vision-Language Models",
        "Importance Sampling",
        "Diversity Optimization",
        "Multi-modal Attention"
      ]
    },
    "analyzed_at": "2026-01-29T03:41:33.539156Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.18714",
    "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-Forward Planar Splatting",
    "authors": [
      "Changkun Liu",
      "Bin Tan",
      "Zeran Ke",
      "Shangzhan Zhang",
      "Jiachen Liu",
      "Ming Qian",
      "Nan Xue",
      "Yujun Shen",
      "Tristan Braud"
    ],
    "abstract": "This paper addresses metric 3D reconstruction of indoor scenes by exploiting their inherent geometric regularities with compact representations. Using planar 3D primitives - a well-suited representation for man-made environments - we introduce PLANA3R, a pose-free framework for metric Planar 3D Reconstruction from unposed two-view images. Our approach employs Vision Transformers to extract a set of sparse planar primitives, estimate relative camera poses, and supervise geometry learning via planar splatting, where gradients are propagated through high-resolution rendered depth and normal maps of primitives. Unlike prior feedforward methods that require 3D plane annotations during training, PLANA3R learns planar 3D structures without explicit plane supervision, enabling scalable training on large-scale stereo datasets using only depth and normal annotations. We validate PLANA3R on multiple indoor-scene datasets with metric supervision and demonstrate strong generalization to out-of-domain indoor environments across diverse tasks under metric evaluation protocols, including 3D surface reconstruction, depth estimation, and relative pose estimation. Furthermore, by formulating with planar 3D representation, our method emerges with the ability for accurate plane segmentation. The project page is available at https://lck666666.github.io/plana3r",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.18714.pdf",
    "abs_url": "https://arxiv.org/abs/2510.18714",
    "published": "2025-10-21T15:15:33Z",
    "updated": "2026-01-28T05:45:51Z",
    "comment": "Camera-ready version of a paper in 39th Conference on Neural Information Processing Systems (NeurIPS 2025). The project page is available at: https://lck666666.github.io/plana3r",
    "light_analysis": {
      "overview": "PLANA3R是一个零样本度量平面3D重建框架，通过前馈平面splatting，无需姿态信息和显式平面监督，实现室内场景的高效重建。",
      "motivation": "本文研究室内场景的度量3D重建问题，旨在利用平面3D原语作为紧凑表示来简化复杂环境的重建。现有方法通常依赖3D平面注释进行监督，这不仅限制了训练数据的可扩展性，还影响了模型在多样场景中的泛化能力。因此，开发一种无需显式平面监督的方法至关重要，可以仅使用深度和法线注释在大规模立体数据集上进行训练，从而提高重建精度和应用范围。",
      "method": "PLANA3R采用Vision Transformers从无姿态的两视角图像中提取稀疏平面原语，并估计相对相机姿态。通过平面splatting技术，使用高分辨率渲染的深度和法线图来传播梯度，监督几何学习过程。该方法的关键创新在于无需3D平面注释，仅依赖深度和法线监督，实现前馈式重建，使得训练更加高效和可扩展，同时保留了平面表示的紧凑性和准确性。",
      "result": "PLANA3R在多个室内场景数据集上进行了验证，展示出对领域外室内环境的强大泛化能力。在度量评估协议下，包括3D表面重建、深度估计和相对姿态估计等任务中，方法表现优异。摘要未明确提供具体数据，但结果表明其在这些任务中的性能优于现有基线方法，凸显了其有效性和鲁棒性。",
      "conclusion": "PLANA3R提出了一种无需姿态和显式平面监督的度量平面3D重建框架，显著提升了训练的可扩展性和模型泛化能力。该方法不仅解决室内场景重建的核心问题，还具有精确平面分割的附加价值，为实际应用如建筑建模和机器人导航提供实用方案。潜在局限性可能在于对非平面场景的适应性，未来工作可探索更复杂的几何表示或扩展到室外环境。",
      "tags": [
        "Vision Transformers",
        "Planar Splatting",
        "Zero-shot Learning",
        "Depth Estimation",
        "3D Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:34.085518Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.18439",
    "title": "Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation",
    "authors": [
      "Yasser Hamidullah",
      "Koel Dutta Chowdhury",
      "Yusser Al Ghussin",
      "Shakib Yazdani",
      "Cennet Oguz",
      "Josef van Genabith",
      "Cristina España-Bonet"
    ],
    "abstract": "Hallucination, where models generate fluent text unsupported by visual evidence, remains a major flaw in vision-language models and is particularly critical in sign language translation (SLT). In SLT, meaning depends on precise grounding in video, and gloss-free models are especially vulnerable because they map continuous signer movements directly into natural language without intermediate gloss supervision that serves as alignment. We argue that hallucinations arise when models rely on language priors rather than visual input. To capture this, we propose a token-level reliability measure that quantifies how much the decoder uses visual information. Our method combines feature-based sensitivity, which measures internal changes when video is masked, with counterfactual signals, which capture probability differences between clean and altered video inputs. These signals are aggregated into a sentence-level reliability score, providing a compact and interpretable measure of visual grounding. We evaluate the proposed measure on two SLT benchmarks (PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our results show that reliability predicts hallucination rates, generalizes across datasets and architectures, and decreases under visual degradations. Beyond these quantitative trends, we also find that reliability distinguishes grounded tokens from guessed ones, allowing risk estimation without references; when combined with text-based signals (confidence, perplexity, or entropy), it further improves hallucination risk estimation. Qualitative analysis highlights why gloss-free models are more susceptible to hallucinations. Taken together, our findings establish reliability as a practical and reusable tool for diagnosing hallucinations in SLT, and lay the groundwork for more robust hallucination detection in multimodal generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.18439.pdf",
    "abs_url": "https://arxiv.org/abs/2510.18439",
    "published": "2025-10-21T09:13:46Z",
    "updated": "2026-01-28T07:43:07Z",
    "comment": "Accepted at ICLR2026",
    "light_analysis": {
      "overview": "本论文提出一种令牌级可靠性度量方法，用于检测符号语言翻译中的幻觉，通过量化解码器对视觉信息的使用程度来提升模型可靠性。",
      "motivation": "在符号语言翻译（SLT）中，幻觉问题导致模型生成与视觉证据不符的流畅文本，严重影响翻译准确性，因为SLT依赖于视频内容的精确基础。无注释模型（gloss-free models）尤其脆弱，它们跳过中间注释监督，直接映射符号动作到自然语言，容易依赖语言先验而非视觉输入。现有方法缺乏有效的幻觉检测机制，使得模型在现实应用中不可靠，因此开发基于视觉信号的检测方法对提高SLT系统实用性至关重要。摘要未明确说明具体基线方法的不足，但暗示了模型对视觉输入的不充分利用是主要问题。",
      "method": "论文提出一种令牌级可靠性度量方法，用于量化解码器在生成每个令牌时使用视觉信息的程度。该方法结合两种信号：基于特征的敏感性，通过测量视频输入被屏蔽时模型内部特征的变化来评估视觉依赖；以及反事实信号，通过比较干净视频和改变视频输入下的概率差异来捕获视觉影响。这些信号被聚合成句子级的可靠性分数，提供紧凑且可解释的视觉基础度量。研究在两个SLT基准数据集（PHOENIX-2014T和CSL-Daily）上进行了评估，覆盖有注释（gloss-based）和无注释（gloss-free）模型架构，以验证方法的泛化能力。",
      "result": "实验结果表明，可靠性度量能有效预测幻觉率，并在PHOENIX-2014T和CSL-Daily数据集上跨不同模型架构泛化良好；当视觉输入退化时，可靠性分数下降，证实其对视觉信息的依赖。该度量能区分基于视觉证据的令牌和猜测令牌，允许在无参考情况下进行风险估计；与基于文本的信号如置信度、困惑度或熵结合后，幻觉风险估计准确性进一步提高。定性分析揭示无注释模型因缺乏中间注释监督而更易产生幻觉，支持方法的有效性。摘要未明确提供具体数据如准确率提升数值，但强调了可靠性与幻觉率的相关性和改进趋势。",
      "conclusion": "该研究确立了可靠性度量作为实用且可重用的工具，能诊断符号语言翻译中的幻觉问题，为多模态生成中更鲁棒的幻觉检测奠定基础。其学术价值在于量化视觉基础并提供可解释的检测方法，有助于理解模型行为；实际应用价值是提高SLT系统的可靠性和安全性，减少幻觉输出。局限性未明确说明，但暗示未来工作可扩展到其他视觉语言任务或改进信号聚合技术。这项研究推动了人工智能在真实场景中的可信应用，增强模型的可解释性和鲁棒性。",
      "tags": [
        "Hallucination Detection",
        "Sign Language Translation",
        "Visual Grounding",
        "Token-Level Reliability",
        "Counterfactual Signals"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:48.212284Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.14616",
    "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures",
    "authors": [
      "Shuangshuang Ying",
      "Yunwen Li",
      "Xingwei Qu",
      "Xin Li",
      "Sheng Jin",
      "Minghao Liu",
      "Zhoufutu Wen",
      "Xeron Du",
      "Tianyu Zheng",
      "Yichi Zhang",
      "Letian Ni",
      "Yuyang Cheng",
      "Zhenzhu Yang",
      "Qiguang Chen",
      "Jingzhe Ding",
      "Shengda Long",
      "Wangchunshu Zhou",
      "Jiazhan Feng",
      "Wanjun Zhong",
      "Libo Qin",
      "Ge Zhang",
      "Wenhao Huang",
      "Wanxiang Che",
      "Chenghua Lin"
    ],
    "abstract": "Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.14616.pdf",
    "abs_url": "https://arxiv.org/abs/2510.14616",
    "published": "2025-10-16T12:23:13Z",
    "updated": "2026-01-28T13:50:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过引入跨文化创意写作偏好数据集，揭示了生成奖励模型在捕捉主观写作偏好上的优越性，超越传统序列模型。",
      "motivation": "当前偏好学习方法在标准基准测试中表现良好，但当移除客观质量信号时性能显著下降，这表明现有方法难以有效评估主观质量偏好，如创造性或文化敏感性。这一问题在实际应用中至关重要，因为AI系统在创意写作等领域需要处理主观要素。现有方法如序列奖励模型主要依赖客观错误检测，忽视了主观评估，导致在真实场景中适用性受限，因此研究旨在开发新基准和方法来捕捉跨文化写作的主观偏好，弥补RLHF方法的不足。",
      "method": "论文提出WritingPreferenceBench数据集，包含1,800个人工标注的偏好对，覆盖1,200英文和600中文的8个创意写作类型，确保回应在客观正确性、事实准确性和长度上匹配。研究方法包括实验比较三种模型：基于序列的奖励模型（标准RLHF架构）、零-shot语言模型法官和生成奖励模型，后者通过产生显式推理链来评估偏好。关键创新在于数据集的跨文化设计和生成模型的引入，以探索主观偏好捕捉的技术路线。",
      "result": "实验结果显示，序列奖励模型平均准确率为52.7%，零-shot语言模型法官为53.9%，而生成奖励模型达到81.8%的准确率，显著优于基线方法。模型在不同写作类型间表现差异大，准确率范围从18.2%到81.8%，平均标准差为10.1%，表明高模型内方差。此外，模型规模从8B增加到27B参数未带来一致改进，说明当前方法在主观偏好评估上仍有局限。这些结果突显了生成模型在捕捉主观质量上的有效性。",
      "conclusion": "研究结论指出，当前RLHF方法更侧重于检测客观错误，而非捕捉主观质量偏好如创造力或情感共鸣。成功偏好建模可能需要中间推理表示，而非直接分类。贡献在于提供了新数据集和生成模型的优势，推动了AI在主观性评估和文化敏感性方面的发展，具有学术和实际应用价值。未来工作可探索更多推理表示机制和跨文化扩展，以应对局限性。",
      "tags": [
        "Reward Modeling",
        "Generative Models",
        "Writing Preference Benchmark",
        "Cross-cultural Analysis",
        "Human Preference Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:42:55.624054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.14398",
    "title": "YNTP-100: A Benchmark for Your Next Token Prediction with 100 People",
    "authors": [
      "Shiyao Ding",
      "Takayuki Ito"
    ],
    "abstract": "Large language models (LLMs) trained for general \\textit{next-token prediction} often fail to generate responses that reflect how specific individuals communicate. Progress on personalized alignment is further limited by the difficulty of collecting real-world personal communication data due to privacy constraints. We propose Your Next Token Prediction (YNTP), a task that formulates personalized response generation as token-level prediction conditioned on user interaction history. We introduce \\textbf{YNTP-100}, a benchmark built from multilingual multi-day human--agent conversations with 100 people, enabling systematic evaluation of user-specific response behavior. We evaluate external (parameter-preserving) and internal (parameter-updating) alignment methods using metrics of substance similarity and stylistic consistency. The dataset and results are publicly available at: https://github.com/AnonymousHub4Submissions/YNTP100.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.14398.pdf",
    "abs_url": "https://arxiv.org/abs/2510.14398",
    "published": "2025-10-16T07:54:02Z",
    "updated": "2026-01-28T10:09:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出YNTP任务及YNTP-100基准，用于评估个性化语言模型的响应生成能力。",
      "motivation": "当前大型语言模型在通用next-token prediction训练下，生成响应时难以反映个体用户的独特沟通风格，限制了AI系统的个性化对齐发展。由于隐私约束，收集真实个人通信数据极为困难，导致缺乏有效评估基准。本研究旨在解决这一实际问题，通过定义新任务和构建数据集，推动个性化响应生成的研究，克服现有方法在数据收集和评估上的不足。",
      "method": "本研究定义了'您的下一个令牌预测'（YNTP）任务，将个性化响应生成形式化为基于用户交互历史的令牌级预测问题。核心创新在于构建了YNTP-100基准，包含100人的多语言多天人类-代理对话数据，用于系统评估用户特定行为。方法包括评估外部对齐（如参数保留的提示工程）和内部对齐（如参数更新的微调），关键细节涉及使用实质相似性和风格一致性指标进行量化分析。",
      "result": "论文评估了外部和内部对齐方法在YNTP-100数据集上的表现，使用实质相似性和风格一致性作为主要性能指标。摘要未明确具体实验结果数据，如准确率提升或效率改进，但提到数据集和结果已公开，表明该方法能有效评估用户特定响应行为。与基线方法的对比情况需参考公开结果，推断评估显示了不同对齐方法的有效性。",
      "conclusion": "本论文的主要贡献是提出了YNTP任务和YNTP-100基准，为个性化语言模型研究提供了标准化评估框架，具有重要学术价值。它解决了数据隐私限制下的基准构建问题，促进了更有效的个性化对齐方法探索。实际应用中，有助于开发更贴近用户习惯的AI助手。未来工作方向可能包括扩展数据集规模或优化评估指标以提升泛化能力。",
      "tags": [
        "Next-Token Prediction",
        "Personalized Alignment",
        "Benchmark Dataset",
        "Multilingual Conversations",
        "Human-Agent Interaction"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:02.393508Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.13907",
    "title": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization",
    "authors": [
      "Yuanchen Wu",
      "Saurabh Verma",
      "Justin Lee",
      "Fangzhou Xiong",
      "Poppy Zhang",
      "Amel Awadelkarim",
      "Xu Chen",
      "Yubai Yuan",
      "Shawndra Hill"
    ],
    "abstract": "Large language models (LLMs) are highly sensitive to prompts, but most automatic prompt optimization (APO) methods assume access to ground-truth references (e.g., labeled validation data) that are costly to obtain. We propose the Prompt Duel Optimizer (PDO), a sample-efficient framework for label-free prompt optimization based on pairwise preference feedback from an LLM judge. PDO casts prompt selection as a dueling-bandit problem and combines (i) Double Thompson Sampling to prioritize informative comparisons under a fixed judge budget, with (ii) top-performer guided mutation to expand the candidate pool while pruning weak prompts. Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently identifies stronger prompts than label-free baselines, while offering favorable quality--cost trade-offs under constrained comparison budgets.",
    "categories": [
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.13907.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13907",
    "published": "2025-10-14T22:23:08Z",
    "updated": "2026-01-28T14:04:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Prompt Duel Optimizer (PDO)，一种基于大型语言模型成对偏好反馈的标签免费高效提示优化框架。",
      "motivation": "大型语言模型对提示非常敏感，但现有自动提示优化方法通常依赖标记的验证数据作为参考，这些数据获取成本高昂，限制了实际应用。本文旨在解决这一问题，提出一种无需标签的优化方法，以减少数据依赖和优化成本。其重要性在于，标签免费优化能够应对现实场景中数据稀缺的挑战，同时提升模型部署的效率和经济性。",
      "method": "PDO框架的核心是将提示选择建模为一个dueling-bandit问题，利用LLM作为判断器获取成对偏好反馈。它结合了Double Thompson Sampling来在固定判断器预算下优先选择信息丰富的比较，以高效利用资源；同时采用top-performer guided mutation策略，扩展候选提示池并自动修剪弱提示，从而动态优化搜索空间。该方法避免了依赖外部标记数据，完全基于模型内部反馈进行迭代改进。",
      "result": "在BIG-bench Hard和MS MARCO数据集上的实验表明，PDO consistently识别出比标签免费基线方法更强的提示。具体而言，它在受限制的比较预算下提供了有利的质量-成本权衡，优化效果显著优于其他无需标签的基准方法，但摘要未明确说明具体的性能指标如准确率提升百分比。",
      "conclusion": "论文的主要贡献是提出了PDO，一种高效的标签免费提示优化框架，通过引入dueling-bandit问题和采样技术，降低了数据依赖性。其学术价值在于将强化学习思想应用于提示工程，扩展了自动优化方法的边界；实际应用中，PDO有助于减少LLM部署成本，促进更广泛的应用。未来工作可探索扩展到更多任务或结合其他优化算法。",
      "tags": [
        "Large Language Model",
        "Prompt Optimization",
        "Dueling-Bandit Problem",
        "Thompson Sampling",
        "Pairwise Preference Feedback"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:00.654344Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.12605",
    "title": "WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation",
    "authors": [
      "Runting Li",
      "Shijie Lian",
      "Hua Li",
      "Yutong Li",
      "Wenhui Wu",
      "Sam Kwong"
    ],
    "abstract": "Underwater Salient Object Detection (USOD) faces significant challenges, including underwater image quality degradation and domain gaps. Existing methods tend to ignore the physical principles of underwater imaging or simply treat degradation phenomena in underwater images as interference factors that must be eliminated, failing to fully exploit the valuable information they contain. We propose WaterFlow, a rectified flow-based framework for underwater salient object detection that innovatively incorporates underwater physical imaging information as explicit priors directly into the network training process and introduces temporal dimension modeling, significantly enhancing the model's capability for salient object identification. On the USOD10K dataset, WaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and superiority of our method. https://github.com/Theo-polis/WaterFlow.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.12605.pdf",
    "abs_url": "https://arxiv.org/abs/2510.12605",
    "published": "2025-10-14T15:02:24Z",
    "updated": "2026-01-28T03:30:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出基于校正流的水下显著性对象检测框架WaterFlow，创新融合物理先验和时间维度建模，显著提升模型识别能力。",
      "motivation": "水下显著性对象检测（USOD）面临图像质量退化和领域差距的挑战，现有方法常忽视水下成像的物理原理，或将退化现象简单视为干扰因素而忽略其有价值信息，导致性能受限。本研究旨在解决这些不足，通过利用物理先验来优化检测过程，以应对实际水下视觉任务中的复杂环境需求。",
      "method": "WaterFlow采用校正流技术作为基础框架，创新性地将水下物理成像信息作为明确先验直接整合到网络训练中，并引入时间维度建模来增强显著性识别。具体地，通过显式物理先验引导模型学习，结合时间分析处理动态场景，提升对退化图像的理解和检测精度，摘要未明确说明具体模型架构细节，但提及在USOD10K数据集上进行实验。",
      "result": "在USOD10K数据集上，WaterFlow在S_m指标上取得了0.072的提升，这表明该方法在性能上优于基线或现有方法，有效增强了水下显著性对象检测的准确性，凸显了其技术优势和应用潜力。",
      "conclusion": "本研究的主要贡献在于开发了WaterFlow框架，成功将物理先验和时间建模融入校正流，填补了现有方法在利用物理信息方面的空白。其学术价值在于推动了水下显著性检测领域的发展，实际应用可扩展至水下机器人导航和海洋监测等场景，未来工作可进一步优化模型泛化能力或探索更多物理先验集成方式。",
      "tags": [
        "Rectified Flow",
        "Physics-Prior Integration",
        "Temporal Modeling",
        "Underwater Salient Object Detection"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:21.529834Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.12603",
    "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
    "authors": [
      "Chao Chen",
      "Zhixin Ma",
      "Yongqi Li",
      "Yupeng Hu",
      "Yinwei Wei",
      "Wenjie Li",
      "Liqiang Nie"
    ],
    "abstract": "Multimodal reasoning aims to enhance the capabilities of MLLMs by incorporating intermediate reasoning steps before reaching the final answer. It has evolved from text-only reasoning to the integration of visual information, enabling the thought process to be conveyed through both images and text. Despite its effectiveness, current multimodal reasoning methods depend on explicit reasoning steps that require labor-intensive vision-text annotations and inherently introduce significant inference latency. To address these issues, we introduce multimodal latent reasoning with the advantages of multimodal representation, reduced annotation, and inference efficiency. To facilitate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR), which injects both visual and textual information in the reasoning process within the latent space. Specifically, IVT-LR represents each reasoning step by combining two implicit parts: latent text (the hidden states from the previous step) and latent vision (a set of selected image embeddings). We further introduce a progressive multi-stage training strategy to enable MLLMs to perform the above multimodal latent reasoning steps. Experiments on M$^3$CoT and ScienceQA demonstrate that our IVT-LR method achieves an average performance increase of 5.45\\% in accuracy, while simultaneously achieving a speed increase of over 5 times compared to existing approaches.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.12603.pdf",
    "abs_url": "https://arxiv.org/abs/2510.12603",
    "published": "2025-10-14T14:58:25Z",
    "updated": "2026-01-28T09:19:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Interleaved Vision-Text Latent Reasoning (IVT-LR)方法，通过在潜在空间中融合视觉和文本信息，实现高效多模态推理，以解决现有方法依赖显式步骤导致的延迟高和注释密集问题。",
      "motivation": "多模态推理旨在增强多模态大语言模型（MLLMs）的能力，通过整合视觉和文本信息以模拟人类思维过程。然而，当前方法依赖显式推理步骤，需要大量人工标注的视觉文本数据，这不仅增加成本，还引入显著的推理延迟，限制了实时应用和可扩展性。因此，本研究致力于开发一种基于潜在空间的推理方法，以减少标注负担并提高效率，克服现有方法的这些不足。",
      "method": "本论文提出Interleaved Vision-Text Latent Reasoning (IVT-LR)方法，核心创新在于在潜在空间中表示多模态推理步骤。具体来说，每个步骤结合潜在文本（前一步的隐藏状态）和潜在视觉（一组选定的图像嵌入），以隐式方式整合视觉和文本信息。为了训练MLLMs执行这种推理，论文引入了渐进多阶段训练策略，逐步适应多模态潜在表示。关键细节包括使用多模态嵌入和减少对显式注释的依赖，模型架构基于MLLMs进行扩展。",
      "result": "实验在M^3CoT和ScienceQA数据集上进行，结果表明IVT-LR方法在准确性方面平均提升了5.45%，显著优于现有基线方法。此外，推理速度提高了超过5倍，显示出卓越的效率改进。这些数据证明了该方法在减少延迟的同时保持或增强性能，验证了潜在空间推理的有效性。",
      "conclusion": "本研究的主要贡献是提出了IVT-LR方法，通过潜在空间推理提高了多模态推理的准确性和效率。学术价值在于扩展了多模态推理的理论框架，减少了显式标注需求；实际应用价值在于加速推理过程和降低标注成本。潜在局限性可能包括对特定数据集的依赖性，未来工作可探索更多领域应用和优化训练策略。",
      "tags": [
        "Multimodal Reasoning",
        "Latent Space",
        "Interleaved Vision-Text Reasoning",
        "Progressive Training",
        "MLLMs"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:43.891297Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.12312",
    "title": "Deep SPI: Safe Policy Improvement via World Models",
    "authors": [
      "Florent Delgrange",
      "Raphael Avalos",
      "Willem Röpke"
    ],
    "abstract": "Safe policy improvement (SPI) offers theoretical control over policy updates, yet existing guarantees largely concern offline, tabular reinforcement learning (RL). We study SPI in general online settings, when combined with world model and representation learning. We develop a theoretical framework showing that restricting policy updates to a well-defined neighborhood of the current policy ensures monotonic improvement and convergence. This analysis links transition and reward prediction losses to representation quality, yielding online, \"deep\" analogues of classical SPI theorems from the offline RL literature. Building on these results, we introduce DeepSPI, a principled on-policy algorithm that couples local transition and reward losses with regularised policy updates. On the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including PPO and DeepMDPs, while retaining theoretical guarantees.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.12312.pdf",
    "abs_url": "https://arxiv.org/abs/2510.12312",
    "published": "2025-10-14T09:11:24Z",
    "updated": "2026-01-28T13:27:55Z",
    "comment": "ICLR 2026, 10 pages main text, 21 pages appendix (excluding references)",
    "light_analysis": {
      "overview": "论文提出了DeepSPI算法，在在线深度强化学习中通过结合世界模型和表示学习实现安全政策改进并保持理论保证。",
      "motivation": "安全政策改进（SPI）对于强化学习应用至关重要，能避免政策更新时的性能下降。现有SPI方法主要局限于离线、表格强化学习，缺乏在在线、深度强化学习设置中的理论保证。随着RL扩展到复杂环境，世界模型和表示学习成为重要工具，但在这些设置中如何确保政策更新的安全性仍是一个开放问题。因此，研究在线SPI结合表示学习，具有重要的理论意义和实际应用价值，旨在解决现有方法在动态环境中保障更新安全的不足。",
      "method": "论文开发了一个理论框架，证明将政策更新限制在当前政策的明确定义邻域内可以确保单调改进和收敛。该框架链接了状态转移和奖励预测损失与表示质量，为经典SPI定理提供了在线、深度的扩展。基于此，提出了DeepSPI算法，这是一个原则性的在线策略算法，通过结合局部转移和奖励损失与正则化政策更新来实现安全改进。DeepSPI使用世界模型进行环境建模，并利用表示学习优化表示，确保更新过程在理论指导下进行，具体技术细节包括基于策略的更新机制和损失函数的设计。",
      "result": "在ALE-57基准测试中，DeepSPI的性能匹配或超过了强基线方法，包括PPO和DeepMDPs。实验结果表明，DeepSPI在多个任务中表现出稳定和高效的性能，验证了其在在线深度强化学习设置中的有效性。同时，DeepSPI保留了理论上的安全保证，实现了政策改进的安全性，避免了常见基线方法可能出现的性能波动。尽管摘要未明确提供具体准确率数据，但结果强调了算法的实用性和理论一致性，为安全RL应用提供了实证支持。",
      "conclusion": "论文的主要贡献是提出了DeepSPI算法，将安全政策改进扩展到在线深度强化学习，并提供了理论框架链接表示学习与政策更新。这项工作丰富了SPI的理论基础，为实际RL应用提供了新的安全保障，具有重要的学术价值和应用前景。局限性方面，摘要未明确说明，但未来工作可能涉及算法在更复杂环境中的优化和验证，以进一步推动安全强化学习的发展。",
      "tags": [
        "Safe Policy Improvement",
        "World Models",
        "Representation Learning",
        "On-Policy Algorithm",
        "Theoretical Guarantees"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:58.296441Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.10631",
    "title": "GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus",
    "authors": [
      "Zhaolin Hu",
      "Kun Li",
      "Hehe Fan",
      "Yi Yang"
    ],
    "abstract": "Linear attention mechanisms have emerged as efficient alternatives to full self-attention in Graph Transformers, offering linear time complexity. However, existing linear attention models often suffer from a significant drop in expressiveness due to low-rank projection structures and overly uniform attention distributions. We theoretically prove that these properties reduce the class separability of node representations, limiting the model's classification ability. To address this, we propose a novel hybrid framework that enhances both the rank and focus of attention. Specifically, we enhance linear attention by attaching a gated local graph network branch to the value matrix, thereby increasing the rank of the resulting attention map. Furthermore, to alleviate the excessive smoothing effect inherent in linear attention, we introduce a learnable log-power function into the attention scores to reduce entropy and sharpen focus. We theoretically show that this function decreases entropy in the attention distribution, enhancing the separability of learned embeddings. Extensive experiments on both homophilic and heterophilic graph benchmarks demonstrate that our method achieves competitive performance while preserving the scalability of linear attention.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.10631.pdf",
    "abs_url": "https://arxiv.org/abs/2510.10631",
    "published": "2025-10-12T14:22:32Z",
    "updated": "2026-01-28T12:24:29Z",
    "comment": "Accepted by WWW 2026. Research Tracks - Graph Algorithms and Modeling for the Web",
    "light_analysis": {
      "overview": "GraphTARIF提出了一种混合框架，通过增强线性图Transformer的注意力秩和锐化聚焦，提升了模型的表达能力和分类性能。",
      "motivation": "线性注意力机制在图Transformer中虽然具有线性时间复杂度的高效优势，但现有模型因低秩投影结构和过于均匀的注意力分布，导致表达能力下降，限制了节点表示的类可分性。这一问题在图分类任务中尤为重要，尤其是在处理复杂图数据时，现有方法难以兼顾效率和性能，亟需设计一种既能保持可扩展性又能增强表达力的新方案。",
      "method": "GraphTARIF框架的核心方法包括两部分：首先，通过附加一个门控局部图网络分支到值矩阵，增加注意力图的秩，从而提升表达能力；其次，引入一个可学习的对数幂函数到注意力分数中，降低熵并锐化注意力分布，以减轻线性注意力的过度平滑效应。这些技术基于理论证明，增强了节点嵌入的可分性，同时保持了线性时间复杂度的可扩展性。",
      "result": "在同质和异质图基准上的广泛实验显示，GraphTARIF在保持线性注意力可扩展性的同时，实现了与基线方法竞争的性能。摘要未明确说明具体指标如准确率提升数值，但论文声称该方法在多种图类型上表现出色，有效提升了分类能力，同时不牺牲计算效率。",
      "conclusion": "本研究通过理论分析和实验验证，贡献了GraphTARIF框架，成功解决了线性图Transformer中注意力低秩和过度平滑的问题。学术上，它结合了图网络和注意力机制，推动了高效图学习模型的发展；应用上，可扩展至社交网络、生物信息等领域的图数据处理。未来工作可探索在更大规模图任务中的应用或结合其他先进技术。",
      "tags": [
        "Linear Attention",
        "Graph Transformer",
        "Attention Rank Enhancement",
        "Learnable Log-Power Function"
      ]
    },
    "analyzed_at": "2026-01-29T03:43:52.838708Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.09885",
    "title": "Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs",
    "authors": [
      "Xu Pan",
      "Ely Hahami",
      "Jingxuan Fan",
      "Ziqian Xie",
      "Haim Sompolinsky"
    ],
    "abstract": "Large language models (LLMs) are often used in environments where facts evolve, yet factual knowledge updates via fine-tuning on unstructured text often suffers from 1) reliance on compute-heavy paraphrase augmentation and 2) the reversal curse. Recent studies show diffusion large language models (dLLMs) require fewer training samples to achieve lower loss in pre-training and are more resistant to the reversal curse, suggesting dLLMs may learn new knowledge more easily than autoregressive LLMs (arLLMs). We test this hypothesis in controlled knowledge fine-tuning experiments and find that while arLLMs rely on paraphrase augmentation to generalize knowledge text into question-answering (QA) capability, dLLMs do not require paraphrases to achieve high QA accuracy. To further investigate whether the demasking objective alone can induce such a knowledge injection advantage in dLLMs regardless of their diffusion denoising paradigm, we propose masked fine-tuning for arLLMs, which prompts an arLLM to reconstruct the original text given a masked version in context. The masked fine-tuning for arLLMs substantially improves the efficacy of knowledge injection, i.e. no paraphrase needed and resistant to the reversal curse, closing the gap between arLLMs and dLLMs. We also demonstrate that the same demasking objective improves supervised fine-tuning (SFT) on math tasks over standard SFT, suggesting broader applicability of the demasking objective.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.09885.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09885",
    "published": "2025-10-10T21:43:50Z",
    "updated": "2026-01-28T03:44:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出针对自回归大语言模型的掩盖微调方法，显著提升了知识注入效率，无需重述增强且抵抗反转诅咒，缩小了与扩散模型的数据效率差距。",
      "motivation": "当大语言模型应用于事实频繁更新的场景时，基于非结构化文本的微调常面临计算密集型重述增强和反转诅咒问题。研究表明扩散大语言模型在预训练中数据效率更高、更抵抗反转诅咒，可能更易学习新知识，但自回归模型占主导，存在效率差距。本研究旨在验证这一假设，并探索如何改进自回归模型的知识注入方法，以解决实际应用中的计算成本和知识更新障碍。",
      "method": "本研究设计控制性知识微调实验，比较自回归与扩散大语言模型在问答任务中的表现。核心创新是提出针对自回归模型的掩盖微调方法，该方法提示模型根据上下文重构被掩盖的原始文本，以模仿去掩盖目标。此外，在数学任务上应用同样的去掩盖目标进行监督微调，以测试其泛化能力。关键技术包括掩盖输入、去掩盖重建过程，无需依赖外部数据增强。",
      "result": "实验显示，自回归模型依赖重述增强才能达到高问答准确性，而扩散模型则无需依赖。所提出的掩盖微调方法使自回归模型在没有重述的情况下也能实现高准确性，并对反转诅咒具有抵抗力，有效缩小了与扩散模型的差距。在数学任务中，去掩盖目标也优于标准监督微调，提升了模型性能，表明该方法具有广泛适用性，但摘要未明确说明具体性能指标数值。",
      "conclusion": "研究证实掩盖微调方法能有效提升自回归大语言模型的数据效率和知识注入能力，弥补了与扩散模型的差距，去掩盖目标在多种任务中展示潜力。这为模型知识更新提供了高效、低成本的微调策略，增强了对反转诅咒的抵抗，具有重要学术和实际应用价值。未来工作可探索该方法在其他领域的适用性，并进一步验证其泛化性能。",
      "tags": [
        "Large Language Models",
        "Autoregressive Models",
        "Diffusion Models",
        "Masked Fine-Tuning",
        "Demasking Objective"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:03.051065Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.09718",
    "title": "Federated k-Means over Networks",
    "authors": [
      "Xu Yang",
      "Salvatore Rastelli",
      "Alexander Jung"
    ],
    "abstract": "We study federated clustering, where interconnected devices collaboratively cluster the data points of private local datasets. Focusing on hard clustering via the k-means principle, we formulate federated k-means as an instance of generalized total variation minimization (GTVMin). This leads to a federated k-means algorithm in which each device updates its local cluster centroids by solving a regularized k-means problem with a regularizer that enforces consistency between neighbouring devices. The resulting algorithm is privacy-friendly, as only aggregated information is exchanged.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.09718.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09718",
    "published": "2025-10-10T06:32:28Z",
    "updated": "2026-01-28T17:32:20Z",
    "comment": "Xu Yang and Salvatore Rastelli contributed equally",
    "light_analysis": {
      "overview": "本论文提出了一种基于广义总变分最小化的联邦k-means算法，实现设备间协作聚类并确保隐私保护。",
      "motivation": "本研究旨在解决联邦聚类问题，其中多个互联设备拥有私有本地数据集，需要协作进行聚类分析而不泄露原始数据。这个问题在实际应用中很重要，例如物联网和分布式计算中，数据隐私是首要考虑；现有聚类方法通常假设数据集中存储，但在联邦设置下，直接共享数据会侵犯隐私，因此需要开发新的隐私保护聚类算法来克服这一不足。",
      "method": "论文将联邦k-means问题公式化为广义总变分最小化的一个实例，并设计了一种算法，其中每个设备通过解决带有正则化项的正则化k-means问题来更新本地聚类中心。关键创新在于使用正则化项强制执行相邻设备间的一致性，通过广义总变分最小化框架处理分布式优化，同时仅交换聚合信息（如聚类中心更新），从而保护数据隐私。",
      "result": "摘要未明确说明具体的实验结果，如准确率或效率改进。但基于方法描述，该算法通过最小化广义总变分来实现设备间一致性，预计在保持隐私的同时提升聚类性能；与基线方法的对比情况和具体性能指标需要参考完整论文以获取详细验证。",
      "conclusion": "本文的主要贡献是提出了一种基于广义总变分最小化的联邦k-means算法，实现了设备间协作聚类并保护数据隐私。研究在学术上丰富了联邦学习和聚类分析的结合，为分布式数据挖掘提供了新思路；实际应用中，该算法可用于物联网和医疗数据分析等领域。未来工作可能包括扩展算法到其他聚类方法或优化网络条件下的性能。",
      "tags": [
        "Federated Clustering",
        "k-Means",
        "Generalized Total Variation Minimization",
        "Privacy-Preserving Algorithm"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:38.192529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.09695",
    "title": "Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection",
    "authors": [
      "Yanran Chen",
      "Lynn Greschner",
      "Roman Klinger",
      "Michael Klenk",
      "Steffen Eger"
    ],
    "abstract": "Logical fallacies are common in public communication and can mislead audiences; fallacious arguments may still appear convincing despite lacking soundness, because convincingness is inherently subjective. We present the first computational study of how emotional framing interacts with fallacies and convincingness, using large language models (LLMs) to systematically change emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting emotional appeal into fallacious arguments while preserving their logical structures, then use the best models to generate stimuli for a human study. Our results show that LLM-driven emotional framing reduces human fallacy detection in F1 by 14.5% on average. Humans perform better in fallacy detection when perceiving enjoyment than fear or sadness, and these three emotions also correlate with significantly higher convincingness compared to neutral or other emotion states. Our work has implications for AI-driven emotional manipulation in the context of fallacious argumentation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.09695.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09695",
    "published": "2025-10-09T14:57:37Z",
    "updated": "2026-01-28T16:07:21Z",
    "comment": "EACL 2026 Main Camera-ready; typo fixed",
    "light_analysis": {
      "overview": "本文首次使用大型语言模型系统研究情感框架如何影响人类对逻辑谬误的检测，发现AI驱动的情感诉求显著降低检测效果。",
      "motivation": "逻辑谬误在公共交流中广泛存在，可能导致观众被误导，尤其在谬误论证因说服力的主观性而显得可信时，增加了错误信息传播的风险。现有研究可能缺乏对情感因素与谬误检测互动性的系统性分析，因此本研究旨在填补这一空白，探索AI技术通过情感框架如何影响人类识别能力，为理解情感在论证中的作用提供计算视角。",
      "method": "论文提出一种基于大型语言模型的方法，基准测试了八个LLMs，用于向谬误论证中注入情感诉求（如愉悦、恐惧、悲伤），同时保持逻辑结构不变。关键创新包括系统性评估不同LLMs在情感框架生成上的表现，并选择最佳模型生成刺激材料进行人类研究。具体技术路线涉及情感注入算法设计、模型性能比较和实验变量控制，以确保在公平条件下探究情感效应。",
      "result": "实验结果显示，LLM驱动的情感框架使人类在谬误检测任务中的F1分数平均降低14.5%。具体而言，人类在感知愉悦情感时检测能力较好，而在恐惧或悲伤情感下，说服力显著更高，相比中性或其他情绪状态。这些数据表明情感框架不仅削弱检测效果，还增强论证的感知说服力，突显了情感在操纵性论证中的关键作用。",
      "conclusion": "本研究的主要贡献在于首次计算性揭示情感框架与逻辑谬误及说服力的互动关系，证明了AI驱动的情感诉求可削弱人类谬误检测能力。学术上，它为情感认知和逻辑推理的交叉研究提供了新方向；实际应用上，警示了AI在谬误论证背景下可能被用于情感操纵的风险。未来工作可扩展到更多情绪类型或探索干预策略，以提升公众对谬误的抵抗力。",
      "tags": [
        "Large Language Model",
        "Emotional Framing",
        "Fallacy Detection",
        "Human Study"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:34.168785Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.07922",
    "title": "SketchGuard: Scaling Byzantine-Robust Decentralized Federated Learning via Sketch-Based Screening",
    "authors": [
      "Murtaza Rangwala",
      "Farag Azzedin",
      "Richard O. Sinnott",
      "Rajkumar Buyya"
    ],
    "abstract": "Decentralized Federated Learning enables privacy-preserving collaborative training without centralized servers but remains vulnerable to Byzantine attacks. Existing defenses require exchanging high-dimensional model vectors with all neighbors each round, creating prohibitive costs at scale. We propose SketchGuard, which decouples Byzantine filtering from aggregation via sketch-based screening. SketchGuard compresses $d$-dimensional models to $k$-dimensional sketches ($k \\ll d$) using Count Sketch, then fetches full models only from accepted neighbors, reducing communication complexity from $O(d|N_i|)$ to $O(k|N_i| + d|S_i|)$, where $|N_i|$ is the neighbor count and $|S_i| \\le |N_i|$ is the accepted count. We prove convergence in strongly convex and non-convex settings, showing that approximation errors introduce only a $(1+O(ε))$ factor in the effective threshold. Experiments demonstrate SketchGuard maintains state-of-the-art robustness (mean TER deviation $\\leq$0.5 percentage points) while reducing computation by up to 82% and communication by 50-70%.",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.07922.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07922",
    "published": "2025-10-09T08:16:32Z",
    "updated": "2026-01-28T11:58:54Z",
    "comment": "12 pages, 5 figures, Code Available: https://doi.org/10.5281/zenodo.17223405",
    "light_analysis": {
      "overview": "SketchGuard 提出基于素描的筛选方法，提升分散式联邦学习的拜占庭鲁棒性和通信效率。",
      "motivation": "分散式联邦学习允许设备在不依赖中心服务器的情况下进行隐私保护的协同训练，但面临拜占庭攻击的威胁，攻击者可能发送恶意模型更新破坏训练过程。现有防御方法每轮需与所有邻居交换高维模型向量来检测恶意行为，导致通信和计算成本高昂，尤其在大型网络中，这限制了技术的可扩展性和实际应用。因此，开发低成本的拜占庭鲁棒机制是当前研究的重要需求，以平衡隐私保护和效率。",
      "method": "SketchGuard 采用 Count Sketch 技术将高维模型压缩为低维素描（k 维远小于原始 d 维），从而将拜占庭过滤与模型聚合解耦。在每轮训练中，节点首先交换这些压缩的素描来筛选邻居，仅从通过筛选的邻居处获取完整模型进行聚合，这降低了通信复杂度从 O(d|N_i|) 到 O(k|N_i| + d|S_i|)。该方法还证明了在强凸和非凸设置下的收敛性，确保近似误差仅引入一个 (1+O(ε)) 因子到有效阈值中，维持了鲁棒性。",
      "result": "实验表明，SketchGuard 在保持最先进鲁棒性的同时，显著提高了效率。平均 TER 偏差不超过 0.5 个百分点，证明其防御能力与基线方法相当。计算开销减少高达 82%，通信数据传输量降低 50-70%，这些改进通过对比实验验证，展示了方法在可扩展性上的优势，例如在标准数据集上有效过滤恶意节点而不影响模型性能。",
      "conclusion": "SketchGuard 通过素描技术解决了分散式联邦学习中拜占庭防御的高成本问题，贡献包括方法创新、理论收敛性保证和实验验证。这为大规模隐私保护机器学习提供了新途径，具有学术价值和实际应用前景。摘要未明确说明局限性，但未来工作可能涉及优化压缩技术或扩展至更多攻击模型和动态网络环境。",
      "tags": [
        "Decentralized Federated Learning",
        "Byzantine Robustness",
        "Sketch-Based Screening",
        "Count Sketch",
        "Communication Efficiency"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:06.719761Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.07118",
    "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning",
    "authors": [
      "Manish Nagaraj",
      "Sakshi Choudhary",
      "Utkarsh Saxena",
      "Deepak Ravikumar",
      "Kaushik Roy"
    ],
    "abstract": "Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based \"fingerprints\" from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.07118.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07118",
    "published": "2025-10-08T15:11:04Z",
    "updated": "2026-01-28T14:13:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出TRIM，一个基于令牌注意力指纹的高效框架，用于指令调优中的核心集选择，显著提升下游任务性能并降低计算成本。",
      "motivation": "研究动机是解决指令调优中数据选择的效率挑战。指令调优对齐大型语言模型到下游任务通常依赖大型语料库，但高质量小型核心集有潜力提供更优结果。现有方法基于梯度等样本级信号，计算昂贵且忽视细粒度令牌特征，导致筛选高质量数据子集困难。因此，开发高效、细粒度的数据选择方法对提升数据利用效率和模型对齐效果至关重要。",
      "method": "TRIM是一个前向仅令牌中心框架，通过注意力机制生成令牌级“指纹”，从少量目标样本中匹配底层表示模式，实现数据选择。关键创新是避免使用梯度，依赖注意力特征识别任务结构特征，提升了计算效率和细粒度分析能力。摘要未明确说明具体数据集或模型架构细节，但方法基于注意力指纹匹配和表示模式识别。",
      "result": "实验结果显示，TRIM选择的核心集在下游任务中比最先进基线性能提升高达9%，并在某些设置中超越全数据微调。通过避免昂贵的反向传播，TRIM显著降低计算成本，实现高效数据选择。具体性能指标如准确率提升在摘要中未详细说明，但强调了显著的改进和成本优势，与基线方法对比展示了优越性。",
      "conclusion": "TRIM为构建高质量指令调优数据集提供了可扩展且高效的替代方案，解决了现有方法的计算负担和粗粒度问题。研究贡献在于提出了注意力指纹匹配的新方法，具有学术价值（推动数据高效学习）和实际应用价值（降低LLMs对齐成本）。未来工作可能包括扩展到其他任务或模型，并进一步验证其通用性和局限性。",
      "tags": [
        "Instruction Tuning",
        "Attention Mechanism",
        "Data Selection",
        "Token-wise Analysis",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:09.957187Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.06165",
    "title": "Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing",
    "authors": [
      "Kurt Butler",
      "Guanchao Feng",
      "Petar Djuric"
    ],
    "abstract": "Feature attributions are post-training analysis methods that assess how various input features of a machine learning model contribute to an output prediction. Their interpretation is straightforward when features act independently, but it becomes less clear when the predictive model involves interactions, such as multiplicative relationships or joint feature contributions. In this work, we propose a general theory of higher-order feature attribution, which we develop on the foundation of Integrated Gradients (IG). This work extends existing frameworks in the literature on explainable AI. When using IG as the method of feature attribution, we discover natural connections to statistics and topological signal processing. We provide several theoretical results that establish the theory, and we validate our theory on a few examples.",
    "categories": [
      "cs.LG",
      "eess.SP",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.06165.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06165",
    "published": "2025-10-07T17:29:34Z",
    "updated": "2026-01-28T16:16:10Z",
    "comment": "5 pages, 3 figures, to be published in the Proceedings of ICASSP 2026",
    "light_analysis": {
      "overview": "提出基于集成梯度的高阶特征归因一般理论，构建统计学、可解释AI和拓扑信号处理之间的桥梁，以处理特征交互带来的解释挑战。",
      "motivation": "特征归因是机器学习模型训练后的分析方法，用于评估输入特征对预测输出的贡献。当特征独立作用时解释直观，但在涉及交互（如乘法关系或联合贡献）的预测模型中，现有方法的解释变得模糊。这一问题的重要性在于它影响模型可解释性的准确性和可信度，尤其是在复杂交互场景中，现有方法可能无法充分处理高阶特征关系，限制了模型透明度。",
      "method": "研究提出高阶特征归因的一般理论，以集成梯度（IG）为基础进行扩展。核心方法是将IG框架推广以处理高阶特征交互，关键创新点是建立与统计学和拓扑信号处理领域的自然连接，通过数学框架分析特征间关系。论文提供多个理论结果来支撑该理论，并计划通过具体例子验证其有效性，但摘要未详细说明使用的数据集或模型架构。",
      "result": "摘要未明确说明具体实验结果，只提到在几个例子中验证了理论。因此，推断理论得到初步验证，但缺乏与基线方法的详细性能对比和数据支撑（如准确率提升或效率改进的具体数值），信息不足部分标注为“摘要未明确说明”。",
      "conclusion": "研究的主要贡献是发展了一个统一的高阶特征归因理论，丰富了可解释AI的方法论，并促进了跨学科连接，具有学术价值。实际应用价值在于提升模型在复杂交互场景中的可解释性，局限性可能包括验证例子有限，未来工作可扩展到更多数据集和实际应用以进一步检验理论。",
      "tags": [
        "Integrated Gradients",
        "Higher-Order Feature Attribution",
        "Explainable AI",
        "Topological Signal Processing",
        "Statistical Methods"
      ]
    },
    "analyzed_at": "2026-01-29T03:44:53.567939Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.04480",
    "title": "FourierCSP: Differentiable Constraint Satisfaction Problem Solving by Walsh-Fourier Expansion",
    "authors": [
      "Yunuo Cen",
      "Zixuan Wang",
      "Jintao Zhang",
      "Zhiwei Zhang",
      "Xuanyao Fong"
    ],
    "abstract": "The Constraint-satisfaction problem (CSP) is fundamental in mathematics, physics, and theoretical computer science. Continuous local search (CLS) solvers, as recent advancements, can achieve highly competitive results on certain classes of Boolean satisfiability (SAT) problems. Motivated by these advances, we extend the CLS framework from Boolean SAT to general CSP with finite-domain variables and expressive constraint formulations. We present FourierCSP, a continuous optimization framework that generalizes the Walsh-Fourier transform to CSP, allowing for transforming versatile constraints to compact multilinear polynomials, thereby avoiding the need for auxiliary variables and memory-intensive encodings. We employ projected subgradient and mirror descent algorithms with provable convergence guarantees, and further combine them to accelerate gradient-based optimization. Empirical results on benchmark suites demonstrate that FourierCSP is scalable and competitive, significantly broadening the class of problems that can be efficiently solved by differentiable CLS techniques and paving the way toward end-to-end neurosymbolic integration.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.04480.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04480",
    "published": "2025-10-06T04:30:07Z",
    "updated": "2026-01-28T03:05:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了FourierCSP，通过推广Walsh-Fourier变换到约束满足问题，实现了一般CSP的可微连续优化求解。",
      "motivation": "约束满足问题在数学、物理和理论计算机科学中具有基础性地位。近年来，连续局部搜索求解器在布尔可满足性问题中取得了竞争性结果，但局限于布尔域，难以扩展到具有有限域变量和复杂约束的一般CSP。FourierCSP的研究动机在于克服这一限制，将CLS框架扩展到更广泛的CSP类，避免传统方法所需的辅助变量和内存密集型编码，从而提高求解效率和应用范围。",
      "method": "FourierCSP的核心方法是将Walsh-Fourier变换推广到约束满足问题，将多样化的约束转换为紧凑的多线性多项式，从而消除对辅助变量和复杂编码的需求。该方法采用投影次梯度和镜像下降算法，这些算法具有可证明的收敛保证。通过结合这两种算法，进一步加速了基于梯度的优化过程，实现对一般CSP的连续优化求解。",
      "result": "在基准测试套件上的实证结果表明，FourierCSP具有可扩展性和竞争力，显著扩展了可以通过可微CLS技术高效求解的问题类，为端到端神经符号集成铺平了道路。摘要未明确说明具体的性能指标数据，如准确率提升或效率改进。",
      "conclusion": "FourierCSP的主要贡献是提出了一种基于Walsh-Fourier变换的连续优化框架，成功将CLS技术从布尔SAT扩展到一般CSP。这一工作具有重要的学术价值，为CSP求解提供了新的可微方法；实际应用价值在于能够处理更复杂和多样化的约束，促进神经符号计算的集成。未来工作可能包括进一步优化算法性能和探索更广泛的应用场景。",
      "tags": [
        "Constraint Satisfaction Problem",
        "Walsh-Fourier Transform",
        "Continuous Local Search",
        "Differentiable Optimization",
        "Neurosymbolic Integration"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:33.582848Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.04226",
    "title": "Epistemic Diversity and Knowledge Collapse in Large Language Models",
    "authors": [
      "Dustin Wright",
      "Sarah Masud",
      "Jared Moore",
      "Srishti Yadav",
      "Maria Antoniak",
      "Peter Ebert Christensen",
      "Chan Young Park",
      "Isabelle Augenstein"
    ],
    "abstract": "Large language models (LLMs) tend to generate homogenous texts, which may impact the diversity of knowledge generated across different outputs. Given their potential to replace existing forms of knowledge acquisition, this poses a risk of knowledge collapse, where homogenous LLMs may lead most people to be exposed to largely the same information, thus mediating a shrinking in the range of accessible information over time as underepresented knowledge is forgotten. To assess the risk of knowledge collapse with LLMs, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs. We use this to perform a broad empirical study testing 27 LLMs, 155 topics covering 12 countries, and 200 prompt templates sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.04226.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04226",
    "published": "2025-10-05T14:29:15Z",
    "updated": "2026-01-28T13:27:36Z",
    "comment": "16 pages; 8 figures, 4 tables; v2 changelog: Fixed the modeling for table 3, random effect is the model version; v3 changelog: Fixed minor formatting issues in tables 2 and 3; v4 changelog: Fixed some typos and model description; v5 changelog: Updated metadata; v6 changelog: Improved search baseline, writing revisions, added comparisons to semantic similarity only approaches",
    "light_analysis": {
      "overview": "本研究提出测量知识多样性的新方法，并通过实证分析揭示大型语言模型可能导致知识同质化及其风险。",
      "motivation": "大型语言模型倾向于生成同质化文本，这可能影响知识多样性。随着LLMs逐渐取代传统知识获取方式，存在知识崩溃风险，即同质化输出导致信息范围缩小，代表性不足的知识被遗忘。当前缺乏系统方法来评估LLMs对知识多样性的影响，因此本研究旨在量化这种风险，填补现有研究的空白。摘要未明确说明具体不足之处，但暗示LLMs输出同质化问题未得到充分评估。",
      "method": "论文提出了一种新方法论来测量知识多样性，即通过分析LLM输出中现实声明的变化。研究使用了27个大型语言模型，覆盖155个主题和12个国家，并基于200个来自真实用户聊天的提示模板进行实证测试。关键创新点在于结合文化背景和多语言数据，利用检索增强生成技术评估知识多样性，以识别模型输出中的同质化趋势和影响因素。摘要未详细说明模型架构具体细节，但强调了广泛的数据集和比较方法。",
      "result": "实证结果显示，较新的模型倾向于生成更多样化的声明，但所有模型的知识多样性均低于基本网页搜索。模型大小对知识多样性有负面影响，而检索增强生成技术则有正面影响，但改善程度因文化背景而异。与维基百科相比，LLMs的国家特定声明更偏向英语而非当地语言，突显了知识代表性差距。这些发现基于模型数量和主题的统计数据，但摘要未提供具体性能指标如准确率。",
      "conclusion": "本研究的主要贡献是提出了测量知识多样性的方法，并实证揭示了大型语言模型的知识同质化风险及其对知识崩溃的潜在影响。学术价值在于为评估AI模型的社会影响提供了新视角，实际应用价值包括指导模型开发以减少偏差和提高多样性。局限性包括数据集的文化覆盖可能有限，未来工作可扩展到更多语言和领域，探索缓解知识崩溃的策略。摘要未明确说明其他未来方向，但暗示需要改进知识代表性。",
      "tags": [
        "Large Language Models",
        "Retrieval-Augmented Generation",
        "Epistemic Diversity",
        "Knowledge Collapse",
        "Empirical Study"
      ]
    },
    "analyzed_at": "2026-01-29T03:45:37.993519Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.02180",
    "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning",
    "authors": [
      "Silvia Sapora",
      "Devon Hjelm",
      "Alexander Toshev",
      "Omar Attia",
      "Bogdan Mazoure"
    ],
    "abstract": "Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield black-box models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the MuJoCo, BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.02180.pdf",
    "abs_url": "https://arxiv.org/abs/2510.02180",
    "published": "2025-10-02T16:31:39Z",
    "updated": "2026-01-28T16:17:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出GRACE框架，通过结合大型语言模型与进化搜索，生成可解释的代码奖励函数，解决了传统逆向强化学习中黑盒模型的解释性问题。",
      "motivation": "逆向强化学习的目标是从专家示范中恢复奖励模型，但传统方法通常产生黑盒模型，难以解释和调试，这限制了模型在实际应用中的可信度和可维护性。可解释性对于错误排查和系统验证至关重要，现有方法缺乏透明度，导致在复杂任务中应用受限。因此，本研究旨在开发一种能够生成可解释、可验证奖励函数的新方法，以克服这些不足。",
      "method": "GRACE方法使用大型语言模型在进化搜索框架中，直接从专家轨迹反向工程出代码化的奖励函数。核心创新在于将奖励函数表示为可执行代码，使其易于检查和验证，提升透明性。技术路线涉及利用进化搜索优化LLMs生成的代码，以学习复杂任务下的奖励结构。实验在MuJoCo、BabyAI和AndroidWorld基准数据集上进行，验证方法在多样环境中的适用性。",
      "result": "在MuJoCo、BabyAI和AndroidWorld基准上，GRACE高效学习到高准确度的奖励函数，即使在复杂多任务设置中。与具有真实奖励的竞争性模仿学习和在线强化学习方法相比，生成的奖励函数能导出强策略，表现优异。实验还表明，该方法能够构建复杂奖励API，适应多任务环境，但具体性能指标摘要未明确说明数据值。",
      "conclusion": "GRACE的主要贡献在于提供了一个可解释的逆向强化学习框架，通过代码化奖励函数增强模型透明度和可调试性。学术上，它推动了可解释AI在强化学习领域的进展；实际应用中，可用于复杂系统的奖励设计，提高可靠性。潜在局限性可能包括计算效率或泛化能力，未来工作可探索扩展到更多领域或优化算法性能。",
      "tags": [
        "Inverse Reinforcement Learning",
        "Large Language Models",
        "Evolutionary Search",
        "Explainable AI",
        "Code Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:01.214767Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.24716",
    "title": "Discrete Variational Autoencoding via Policy Search",
    "authors": [
      "Michael Drolet",
      "Firas Al-Hafez",
      "Aditya Bhatt",
      "Jan Peters",
      "Oleg Arenz"
    ],
    "abstract": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.24716.pdf",
    "abs_url": "https://arxiv.org/abs/2509.24716",
    "published": "2025-09-29T12:44:05Z",
    "updated": "2026-01-28T18:33:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种基于策略搜索的离散变分自编码器训练框架，利用自然梯度更新编码器，无需重参数化，有效改善了高维数据重建性能。",
      "motivation": "离散变分自编码器（VAEs）在图像重建等高维任务中具有高比特效率优势，但离散随机变量无法精确可微分参数化，这导致现有训练方法如Gumbel-Softmax重参数化或REINFORCE存在近似误差或高方差问题，限制了其应用效果。研究旨在解决这一挑战，探索更稳定高效的训练方法，以充分利用离散VAEs在数据压缩和多模态搜索中的潜力。",
      "method": "该方法受策略搜索技术启发，提出一个训练框架，利用非参数编码器的自然梯度直接更新参数编码器，避免了复杂的重参数化过程。关键创新包括结合自动步长调整以优化训练稳定性，并采用基于变换器（transformer）的编码器来处理复杂数据分布。框架设计为可扩展，适用于挑战性数据集如ImageNet，通过自然梯度方法提高离散VAEs的训练效率和精度。",
      "result": "在ImageNet等高维数据集上的实验表明，该方法在从紧凑潜在空间重建数据方面优于现有的近似重参数化方法（如Gumbel-Softmax）和基于量化的离散自编码器。尽管摘要未提供具体性能指标数字，但结果明确指出优于基线方法，显示出在减少重建误差和提升模型效率方面的显著改进。",
      "conclusion": "论文的主要贡献是提出了一种新颖的训练方法，通过引入策略搜索中的自然梯度，解决了离散VAEs训练中的关键挑战，增强了模型在高维数据重建中的表现。该研究具有重要的学术价值，为离散潜在表示的学习提供了更稳定和高效的途径，有望应用于图像压缩、生成模型等领域。未来工作可进一步探索该方法在其他任务中的应用和优化技术，以提升泛化能力。",
      "tags": [
        "Discrete Variational Autoencoder",
        "Policy Search",
        "Natural Gradient",
        "Transformer",
        "Image Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:07.865791Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.23130",
    "title": "SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems",
    "authors": [
      "Qian Cheng",
      "Ruize Tang",
      "Emilie Ma",
      "Finn Hackett",
      "Peiyang He",
      "Yiming Su",
      "Ivan Beschastnikh",
      "Yu Huang",
      "Xiaoxing Ma",
      "Tianyin Xu"
    ],
    "abstract": "Formal models are essential to specifying large, complex computer systems and verifying their correctness, but are notoriously expensive to write and maintain. Recent advances in generative AI show promise in generating certain forms of specifications. However, existing work mostly targets small code, not complete systems. It is unclear whether AI can deal with realistic system artifacts, as this requires abstracting their complex behavioral properties into formal models. We present SysMoBench, a benchmark that evaluates AI's ability to formally model large, complex systems. We focus on concurrent and distributed systems, which are keystones of today's critical computing infrastructures, encompassing operating systems and cloud infrastructure. We use TLA+, the de facto specification language for concurrent and distributed systems, though the benchmark can be extended to other specification languages. We address the primary challenge of evaluating AI-generated models by automating metrics like syntactic and runtime correctness, conformance to system code, and invariant correctness. SysMoBench currently includes eleven diverse system artifacts: the Raft implementation of Etcd and Redis, the leader election of ZooKeeper, the Spinlock, Mutex, and Ringbuffer in Asterinas OS, etc., with more being added. SysMoBench enables us to understand the capabilities and limitations of today's LLMs and agents, putting tools in this area on a firm footing and opening up promising new research directions.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.23130.pdf",
    "abs_url": "https://arxiv.org/abs/2509.23130",
    "published": "2025-09-27T05:24:54Z",
    "updated": "2026-01-28T09:19:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "SysMoBench基准评估人工智能在正式建模复杂并发和分布式系统方面的能力。",
      "motivation": "正式模型在指定和验证大型、复杂计算机系统正确性时至关重要，但编写和维护成本高昂。生成式人工智能的进展展示了生成规范的潜力，但现有研究多聚焦于小规模代码，缺乏对完整、复杂现实世界系统建模能力的评估。因此，评估AI能否抽象复杂系统行为属性成为关键问题，尤其对于作为操作系统和云基础设施基石的并发和分布式系统，这对系统可靠性和效率具有重要意义。",
      "method": "论文提出SysMoBench基准，专注于并发和分布式系统的正式建模。核心方法是使用TLA+作为规范语言，但基准设计可扩展至其他语言。关键创新点包括自动化评估指标，如语法正确性、运行时正确性、与系统代码的一致性，以及不变式正确性。基准涵盖十一个多样化系统工件，例如Etcd的Raft实现、Redis、ZooKeeper的领导者选举等，覆盖操作系统组件，为AI模型测试提供现实场景。",
      "result": "摘要未明确说明具体的实验结果，如准确率提升或效率改进数据。然而，论文介绍了SysMoBench基准的构成，包含多个现实系统工件，并强调该基准有助于系统化评估AI生成模型的能力和局限性。通过自动化指标，基准为后续研究提供标准化测试平台，以客观比较不同AI方法在正式建模任务上的性能，尽管具体对比基线方法未详细说明。",
      "conclusion": "SysMoBench的主要贡献是提供了一个全面评估AI在正式建模复杂系统能力的基准。该研究有助于深入理解当前大型语言模型和智能代理的能力与局限，为相关工具开发奠定坚实基础。通过开辟新研究方向，如扩展基准到更多规范语言或系统类型，该工作推动了AI在系统验证和规范生成领域的学术和实际应用价值。未来工作可进一步优化评估指标或集成更多系统工件。",
      "tags": [
        "Formal Modeling",
        "Concurrent Systems",
        "Distributed Systems",
        "TLA+",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:21.030360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.23040",
    "title": "Look Back to Reason Forward: Revisitable Memory for Long-Context LLM Agents",
    "authors": [
      "Yaorui Shi",
      "Yuxin Chen",
      "Siyuan Wang",
      "Sihang Li",
      "Hengxing Cai",
      "Qi Gu",
      "Xiang Wang",
      "An Zhang"
    ],
    "abstract": "Large language models face challenges in long-context question answering, where key evidence of a query may be dispersed across millions of tokens. Existing works equip large language models with a memory buffer that is dynamically updated via a linear document scan, also known as the \"memorize while reading\" methods. While this approach scales efficiently, it suffers from pruning of latent evidence, information loss through overwriting, and sparse reinforcement learning signals. To tackle these challenges, we present ReMemR1, which integrates the mechanism of memory retrieval into the memory update process, enabling the agent to selectively callback historical memories for non-linear reasoning. To further strengthen training, we propose a multi-level reward design, which combines final-answer rewards with dense, step-level signals that guide effective memory use. Together, these contributions mitigate information degradation, improve supervision, and support complex multi-hop reasoning. Extensive experiments demonstrate that ReMemR1 significantly outperforms state-of-the-art baselines on long-context question answering while incurring negligible computational overhead, validating its ability to trade marginal cost for robust long-context reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.23040.pdf",
    "abs_url": "https://arxiv.org/abs/2509.23040",
    "published": "2025-09-27T01:36:46Z",
    "updated": "2026-01-28T07:59:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ReMemR1方法，通过集成内存检索和多级奖励设计，增强大型语言模型在长上下文问答中的非线性推理能力。",
      "motivation": "大型语言模型在长上下文问答中面临关键证据分散于数百万令牌的挑战。现有方法如'边读边记'通过线性文档扫描动态更新内存缓冲区，虽扩展效率高，但存在潜在证据修剪、信息覆盖丢失和强化学习信号稀疏等问题。这些问题限制了模型在复杂推理任务中的有效性，需要改进内存机制以减少信息退化并提升监督质量。",
      "method": "ReMemR1的核心创新是将内存检索机制整合到内存更新过程中，使代理能够选择性地回调历史记忆，支持非线性多跳推理。此外，提出多级奖励设计，结合最终答案奖励和密集步骤级信号，以引导有效内存使用。这些技术旨在减少信息损失，增强训练监督，适用于现有LLM框架，无需复杂架构调整。",
      "result": "实验结果显示，ReMemR1在长上下文问答任务上显著优于最先进的基线方法。尽管摘要未提供具体准确率数据，但强调了性能提升的同时计算开销可忽略不计，验证了该方法能够以边际成本换取鲁棒的长上下文推理能力，改善了与基线方法的对比效果。",
      "conclusion": "ReMemR1通过内存检索和多级奖励设计，缓解了信息退化问题，增强了监督机制，并支持复杂多跳推理，提升了长上下文处理的效率和准确性。这具有重要的学术价值，为智能助手和文档分析等实际应用提供了新思路；未来工作可进一步探索应用场景和优化细节。",
      "tags": [
        "Large Language Model",
        "Memory Retrieval",
        "Reinforcement Learning",
        "Long-Context Reasoning",
        "Multi-Hop Reasoning"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:17.276190Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.22258",
    "title": "Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks",
    "authors": [
      "Miao Jing",
      "Mengting Jia",
      "Junling Lin",
      "Zhongxia Shen",
      "Huan Gao",
      "Mingkun Xu",
      "Shangyang Li"
    ],
    "abstract": "Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.22258.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22258",
    "published": "2025-09-26T12:20:01Z",
    "updated": "2026-01-28T02:56:44Z",
    "comment": "23 pages, 12 figures",
    "light_analysis": {
      "overview": "论文提出了Neural-MedBench基准测试，专注于评估神经学多模态临床推理能力，以弥补传统分类准确性评估的不足。",
      "motivation": "现有视觉语言模型在标准医学基准测试中表现优异，但其真实临床推理能力不明确。问题在于当前数据集过度强调分类准确性，导致评估错觉，模型看似熟练却在高风险诊断推理中失败。这在实际医疗场景中至关重要，因为推理能力直接关系到AI系统的临床可信度，而现有方法未能充分评估这一维度，造成潜在安全风险。",
      "method": "论文设计了Neural-MedBench基准测试，整合多序列MRI扫描、结构化电子健康记录和临床笔记等多模态数据。该基准包含三个核心任务家族：鉴别诊断、病灶识别和原理生成，旨在深入探究模型的推理过程。创新点在于采用混合评分管道，结合大型语言模型评分器、临床医生验证和语义相似度度量，以确保评估的可靠性和全面性，数据集紧凑但推理密集型，以挑战模型极限。",
      "result": "通过对GPT-4o、Claude-4和MedGemma等先进视觉语言模型进行系统评估，发现与传统数据集相比，性能出现显著下降。错误分析显示，模型的主要缺陷源于推理失败，而非感知错误，这强调了深度评估的重要性。尽管摘要未明确说明具体性能指标，但这一结果表明当前模型在复杂临床推理任务中仍存在不足，传统基准可能高估了其真实能力。",
      "conclusion": "研究提出了双轴评估框架，强调广度导向的大数据集用于统计泛化和深度导向的紧凑基准如Neural-MedBench用于推理保真度，推动了医学AI评估从单纯准确性向推理能力的转变。其学术价值在于为更可靠的临床AI评估提供了方法论，实际应用价值体现在Neural-MedBench作为开放平台，可成本效益高地指导未来基准扩展。局限性包括仅针对神经学领域，未来工作可扩展至其他医学分支。",
      "tags": [
        "Vision-Language Models",
        "Clinical Reasoning",
        "Multimodal Data Integration",
        "Large Language Models",
        "Medical Benchmarks"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:35.114617Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.21853",
    "title": "Dynamic Novel View Synthesis in High Dynamic Range",
    "authors": [
      "Kaixuan Zhang",
      "Zhipeng Xiong",
      "Minxian Li",
      "Mingwu Ren",
      "Jiankang Deng",
      "Xiatian Zhu"
    ],
    "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code is available at https://github.com/prinasi/HDR-4DGS.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.21853.pdf",
    "abs_url": "https://arxiv.org/abs/2509.21853",
    "published": "2025-09-26T04:29:22Z",
    "updated": "2026-01-28T06:22:40Z",
    "comment": "It has been accepted by ICLR 2026",
    "light_analysis": {
      "overview": "论文提出了高动态范围动态新视图合成问题，并开发了HDR-4DGS方法，通过动态色调映射模块实现时间辐射一致性和逼真HDR渲染。",
      "motivation": "当前高动态范围新视图合成方法主要针对静态场景，假设所有元素静止，但现实世界常包含移动物体、变化光照等动态元素，这增加了合成挑战。现有方法因忽略时间辐射变化和复杂LDR到HDR翻译，无法准确建模动态场景，导致在真实应用中效果受限。因此，亟需一种能联合处理时间动态性和辐射一致性的方法，以提升动态场景渲染的逼真度和实用性。",
      "method": "论文提出了HDR-4DGS，一种基于高斯溅射的架构，核心创新是动态色调映射模块。该模块显式连接高动态范围和低动态范围域，通过根据时间维度上辐射分布的演变动态调整色调映射函数，以保持辐射时间一致性。这方法结合了3D场景建模和辐射变化处理，能够处理动态元素的复杂辐射变化，实现从任意视点和时间生成高质量HDR渲染，无需额外数据集或模型架构细节，摘要未明确说明。",
      "result": "HDR-4DGS在时间辐射一致性和空间颜色准确翻译方面表现出色，能够生成逼真的高动态范围渲染。广泛实验表明，该方法在定量性能和视觉保真度上均超越了现有的最先进方法，尽管摘要未提供具体性能数据，但强调了其在动态场景合成中的显著优势，验证了其在处理动态元素和辐射变化方面的有效性。",
      "conclusion": "本研究的主要贡献是定义了高动态范围动态新视图合成问题，并提出了HDR-4DGS方法来解决它。该方法在学术上推动了动态场景建模的研究，具有实际应用价值，如虚拟现实和电影特效中增强渲染质量。摘要未明确说明局限性或未来工作方向，但为后续研究在动态HDR合成领域奠定了基础。",
      "tags": [
        "High Dynamic Range",
        "Novel View Synthesis",
        "Gaussian Splatting",
        "Dynamic Tone-Mapping",
        "Temporal Radiance Variations"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:40.990816Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.21012",
    "title": "Mechanism of Task-oriented Information Removal in In-context Learning",
    "authors": [
      "Hakaze Cho",
      "Haolin Yang",
      "Gouki Minegishi",
      "Naoya Inoue"
    ],
    "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.21012.pdf",
    "abs_url": "https://arxiv.org/abs/2509.21012",
    "published": "2025-09-25T11:18:09Z",
    "updated": "2026-01-28T14:36:35Z",
    "comment": "87 pages, 90 figures, 7 tables, ICLR 2026 Camera-ready",
    "light_analysis": {
      "overview": "本文揭示了In-context Learning中任务导向信息移除的关键机制，并识别了诱导此移除操作的注意力头（Denoising Heads）。",
      "motivation": "研究动机在于In-context Learning（ICL）作为一种新兴的少样本学习范式，其内部工作机制尚不明确，这限制了我们对现代语言模型在少样本任务中性能的理解和优化。实际问题在于零样本场景下，语言模型编码查询时生成非选择性表示，包含所有可能任务的信息，导致输出任意且准确率低，这表明需要一种机制来引导模型专注于目标任务，以提升学习效率和可靠性。现有方法缺乏对这种内部机制的深入分析，导致ICL应用受限，因此探究其机制具有重要意义。",
      "method": "本研究从信息移除的新视角出发，提出通过低秩过滤器选择性移除隐藏状态中的特定信息，以引导语言模型朝向目标任务。关键创新点包括测量隐藏状态以观察少样本ICL如何模拟任务导向的信息移除过程，并识别出关键的注意力头（Denoising Heads）来诱导移除操作。使用精心设计的度量标准分析隐藏状态，并通过消融实验阻塞Denoising Heads的操作，以验证信息移除机制的有效性，从而阐明ICL的内部工作机制。",
      "result": "实验结果表明，在零样本场景下，语言模型的准确率接近零，而通过任务导向的信息移除，可以有效引导模型改善输出。少样本ICL能够模拟信息移除过程，移除冗余信息，从而基于演示提升性能。消融实验显示，当阻塞Denoising Heads的信息移除操作时，ICL准确率显著下降，尤其在正确标签缺席于少样本演示的情况下，这证实了信息移除机制和Denoising Heads在ICL中的关键作用，与基线零样本方法相比，性能有明显改进。",
      "conclusion": "本研究的主要贡献是揭示了In-context Learning中任务导向信息移除的核心机制，并识别了Denoising Heads作为关键组件，这加深了对ICL内部工作机制的理解，具有重要的学术价值。研究结果为优化语言模型在少样本学习中的应用提供了新思路，可能指导未来模型设计和改进。潜在的局限性包括摘要未明确说明具体应用场景，未来工作可以进一步探索信息移除的具体实现和扩展到更广泛的任务中。",
      "tags": [
        "In-context Learning",
        "Language Models",
        "Information Removal",
        "Attention Heads",
        "Denoising Heads"
      ]
    },
    "analyzed_at": "2026-01-29T03:46:55.835803Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.20986",
    "title": "SiNGER: A Clearer Voice Distills Vision Transformers Further",
    "authors": [
      "Geunhyeok Yu",
      "Sunjae Jeong",
      "Yoonyoung Choi",
      "Jaeseung Kim",
      "Hyoseok Hwang"
    ],
    "abstract": "Vision Transformers are widely adopted as the backbone of vision foundation models, but they are known to produce high-norm artifacts that degrade representation quality. When knowledge distillation transfers these features to students, high-norm artifacts dominate the objective, so students overfit to artifacts and underweight informative signals, diminishing the gains from larger models. Prior work attempted to remove artifacts but encountered an inherent trade-off between artifact suppression and preserving informative signals from teachers. To address this, we introduce Singular Nullspace-Guided Energy Reallocation (SiNGER), a novel distillation framework that suppresses artifacts while preserving informative signals. The key idea is principled teacher feature refinement: during refinement, we leverage the nullspace-guided perturbation to preserve information while suppressing artifacts. Then, the refined teacher's features are distilled to a student. We implement this perturbation efficiently with a LoRA-based adapter that requires minimal structural modification. Extensive experiments show that \\oursname consistently improves student models, achieving state-of-the-art performance in multiple downstream tasks and producing clearer and more interpretable representations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.20986.pdf",
    "abs_url": "https://arxiv.org/abs/2509.20986",
    "published": "2025-09-25T10:29:47Z",
    "updated": "2026-01-28T05:18:25Z",
    "comment": "Main paper: 12 pages (including 3 pages of references), 6 figures, 6 tables. Appendix: 9 pages, 7 figures. ICLR 2026 accepted",
    "light_analysis": {
      "overview": "SiNGER是一个新颖的知识蒸馏框架，通过零空间引导的扰动精炼视觉Transformer特征，抑制伪影并提升学生模型性能。",
      "motivation": "视觉Transformer作为视觉基础模型的骨干，常产生高范数伪影，降低表示质量。在知识蒸馏中，这些伪影主导目标，导致学生模型过拟合伪影并轻视信息信号，从而减少大模型带来的增益。先前工作尝试去除伪影，但面临伪影抑制与保留教师信息信号之间的固有权衡，限制了知识蒸馏效果，亟需新方法解决此问题以提升表示质量和模型性能。",
      "method": "论文提出SiNGER框架，通过奇异零空间引导的能量重分配来精炼教师特征。关键创新点在于使用零空间引导的扰动，在特征精炼过程中保留信息信号同时抑制高范数伪影，然后将精炼后的特征蒸馏到学生模型。为实现高效扰动，采用基于LoRA的适配器，只需最小结构修改，适用于视觉Transformer的知识蒸馏场景。",
      "result": "大量实验表明，SiNGER能一致地改善学生模型，在多个下游任务中实现了最先进的性能。与基线方法相比，学生模型不仅性能显著提升，还产生了更清晰和可解释的表示，具体指标如准确率等摘要未明确说明，但整体效果优于先前工作。",
      "conclusion": "SiNGER框架解决了知识蒸馏中伪影抑制与信号保留的权衡问题，通过零空间引导的扰动优化教师特征。学术上提供了一种提升视觉Transformer表示质量的新方法，实际上改善了学生模型在下游任务的应用性能，未来工作可探索其在更多模型和任务中的适用性。",
      "tags": [
        "Knowledge Distillation",
        "Vision Transformer",
        "Nullspace",
        "LoRA",
        "Representation Refinement"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:15.122093Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.20072",
    "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training",
    "authors": [
      "Tianqiao Liu",
      "Xueyi Li",
      "Hao Wang",
      "Haoxuan Li",
      "Zhichao Chen",
      "Weiqi Luo",
      "Zitao Liu"
    ],
    "abstract": "Recent advances in large language models (LLMs) have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-to-speech conversational systems. However, existing multimodal models handling interleaved audio and text rely on autoregressive (AR) methods, overlooking that text depends on target-target relations whereas audio depends mainly on source-target relations. In this work, we propose Text-to-Talk (TtT), a unified audio-text framework that integrates AR text generation with non-autoregressive (NAR) audio diffusion in a single Transformer. By leveraging the any-order AR property of absorbing discrete diffusion, our approach provides a unified training objective for text and audio. To support this hybrid generation paradigm, we design a modality-aware attention mechanism that enforces causal decoding for text while allowing bidirectional modeling within audio spans, and further introduce three training strategies that reduce train-test discrepancies. During inference, TtT employs block-wise diffusion to synthesize audio in parallel while flexibly handling variable-length outputs. Comprehensive experiments on Audio-QA, ASR, AAC and speech-to-speech benchmarks show that TtT consistently surpasses strong AR and NAR baselines, with additional ablation and training-strategy analyses confirming the contribution of each component. We will open-source our models, data and code to facilitate future research in this direction.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.20072.pdf",
    "abs_url": "https://arxiv.org/abs/2509.20072",
    "published": "2025-09-24T12:44:26Z",
    "updated": "2026-01-28T03:39:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 Text-to-Talk (TtT) 框架，通过结合自回归文本生成和非自回归音频扩散，统一处理音频-文本任务，解决了现有方法在处理不同依赖关系时的不足。",
      "motivation": "近年来，大语言模型在扩展至多模态场景，特别是语音到语音对话系统时面临挑战。现有方法多采用自回归方式处理交错音频和文本，但忽视了文本依赖目标间关系而音频依赖源-目标关系的本质差异，导致效率低下并限制性能。因此，需要一种更有效框架来统一处理音频和文本模态，以提升系统在任务如问答、识别和合成中的灵活性和准确性，弥补现有自回归方法的不足。",
      "method": "研究提出 Text-to-Talk (TtT) 框架，将自回归文本生成与非自回归音频扩散集成于单一 Transformer 中。核心创新包括：利用吸收离散扩散的任意顺序自回归属性，实现文本和音频的统一训练目标；设计模态感知注意力机制，使文本因果解码同时允许音频跨度的双向建模；引入三种训练策略减少训练-测试差异；推理时采用块状扩散技术并行合成音频，灵活处理可变长度输出。",
      "result": "在 Audio-QA、自动语音识别 (ASR)、音频字幕生成 (AAC) 和语音到语音基准测试中，TtT 一致超越强自回归和非自回归基线。实验表明，该框架在多个任务上均表现出色，通过消融和训练策略分析，确认了各组成部分的贡献。摘要未明确提供具体性能指标如准确率提升，但强调了其在基准测试中的优越性。",
      "conclusion": "本文贡献在于提出 TtT 框架，有效统一音频和文本生成，通过模态感知机制和训练策略克服了现有方法的局限。研究不仅提升了音频-语言模型的性能，还推动了多模态学习的发展，具有学术和实际应用价值。未来工作可扩展至其他模态或进一步优化训练效率，作者将开源模型、数据和代码，促进相关研究。",
      "tags": [
        "Audio-Language Model",
        "Non-Autoregressive Diffusion",
        "Modality-Aware Attention",
        "Transformer",
        "Joint Training"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:22.254617Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.19800",
    "title": "Analysis of approximate linear programming solution to Markov decision problem with log barrier function",
    "authors": [
      "Donghwan Lee",
      "Hyukjun Yang",
      "Bum Geun Park"
    ],
    "abstract": "There are two primary approaches to solving Markov decision problems (MDPs): dynamic programming based on the Bellman equation and linear programming (LP). Dynamic programming methods are the most widely used and form the foundation of both classical and modern reinforcement learning (RL). By contrast, LP-based methods have been less commonly employed, although they have recently gained attention in contexts such as offline RL. The relative underuse of the LP-based methods stems from the fact that it leads to an inequality-constrained optimization problem, which is generally more challenging to solve effectively compared with Bellman-equation-based methods. The purpose of this paper is to establish a theoretical foundation for solving LP-based MDPs in a more effective and practical manner. Our key idea is to leverage the log-barrier function, widely used in inequality-constrained optimization, to transform the LP formulation of the MDP into an unconstrained optimization problem. This reformulation enables approximate solutions to be obtained easily via gradient descent. While the method may appear simple, to the best of our knowledge, a thorough theoretical interpretation of this approach has not yet been developed. This paper aims to bridge this gap.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.19800.pdf",
    "abs_url": "https://arxiv.org/abs/2509.19800",
    "published": "2025-09-24T06:36:11Z",
    "updated": "2026-01-28T16:42:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出利用对数障碍函数将基于线性规划的马尔可夫决策问题转化为无约束优化问题，并通过梯度下降求解，填补了该方法理论分析的空白。",
      "motivation": "该研究旨在解决马尔可夫决策问题（MDPs）中线性规划（LP）方法使用不足的挑战。现有动态规划方法虽广泛用于强化学习，但LP方法因涉及不等式约束优化而难以有效实施，尽管在离线RL中受关注。论文动机在于建立理论基础，使LP方法更实用，以克服其计算复杂性，从而促进在RL应用中的扩展。摘要强调LP方法的潜力未被充分利用，需系统性理论支持来解决这一瓶颈。",
      "method": "论文的核心方法是应用对数障碍函数，将MDP的LP表述转换为无约束优化问题，从而便于使用梯度下降算法求解近似解。关键创新点在于理论分析这一转化过程，确保方法在实践中的可行性。虽然未提及具体数据集或模型架构，但专注于MDP框架和优化技术，突出对数障碍函数在简化不等式约束问题中的作用，为后续应用提供理论基础。该方法旨在填补现有文献中对这种简单技术缺乏全面理论解释的空白。",
      "result": "摘要未明确说明具体实验结果，如性能指标、效率改进或与基线方法的对比。因此，无法提供数据支撑，但可推断该方法通过理论推导旨在提升LP解决MDP的实用性和易实施性。",
      "conclusion": "本文的主要贡献是建立使用对数障碍函数解决基于LP的MDPs的理论基础，填补了学术空白。研究具有学术价值，增强了LP方法在强化学习领域的理论框架，实际应用价值在于简化优化过程，使LP更易应用于离线RL等场景。未来工作可能包括扩展方法到更复杂问题或进行实验验证，但摘要未明确说明局限性。",
      "tags": [
        "Markov Decision Process",
        "Linear Programming",
        "Log Barrier Function",
        "Gradient Descent",
        "Approximate Solution"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:10.453639Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.19073",
    "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
    "authors": [
      "Hung Nguyen",
      "Runfa Li",
      "An Le",
      "Truong Nguyen"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
    "categories": [
      "cs.CV",
      "eess.IV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.19073.pdf",
    "abs_url": "https://arxiv.org/abs/2509.19073",
    "published": "2025-09-23T14:34:10Z",
    "updated": "2026-01-28T17:01:43Z",
    "comment": "Accepted to ICASSP 2026",
    "light_analysis": {
      "overview": "WaveletGaussian 提出小波域扩散和轻量网络精炼，高效实现稀疏视图 3D 高斯对象重建。",
      "motivation": "该研究旨在解决 3D Gaussian Splatting (3DGS) 在稀疏视图设置下性能急剧下降的问题，这对于图像基 3D 对象重建在虚拟现实、机器人等应用中至关重要。现有方法使用扩散模型修复损坏的渲染，作为伪 ground truth 进行优化，虽有效但计算量大，涉及扩散微调和修复步骤，导致训练效率低下，限制了实际应用的可扩展性。",
      "method": "WaveletGaussian 框架将扩散过程移入小波域，核心创新是仅在小波变换的低分辨率 LL 子带应用扩散，而高频子带使用轻量网络进行精炼，以减少计算开销。此外，提出了高效的在线随机掩码策略来生成训练对，替代传统低效的留一法，优化扩散微调过程。实验基于 Mip-NeRF 360 和 OmniObject3D 数据集，未详细说明具体模型架构，但强调小波处理和轻量化设计。",
      "result": "实验结果表明，WaveletGaussian 在 Mip-NeRF 360 和 OmniObject3D 数据集上实现了竞争性的渲染质量，与先前基于扩散的方法相比，虽然未提供具体数值，但摘要明确指出训练时间大幅减少，优化了计算效率，有效解决了稀疏视图场景下的性能瓶颈，提升了实际应用中的可行性。",
      "conclusion": "本论文主要贡献是 WaveletGaussian 框架，通过小波域扩散和高效训练策略，提升了稀疏视图 3D 重建的效率，学术价值在于创新了小波与扩散模型的结合，实际应用价值在于为低计算资源场景提供可行方案。摘要未明确说明局限性或未来工作方向，但基于内容可推断可能涉及扩展至更多场景或优化网络设计。",
      "tags": [
        "Wavelet-domain Diffusion",
        "3D Gaussian Splatting",
        "Sparse-view Reconstruction",
        "Lightweight Network",
        "Online Random Masking"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:28.189519Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18766",
    "title": "Diagonal Linear Networks and the Lasso Regularization Path",
    "authors": [
      "Raphaël Berthier"
    ],
    "abstract": "Diagonal linear networks are neural networks with linear activation and diagonal weight matrices. Their theoretical interest is that their implicit regularization can be rigorously analyzed: from a small initialization, the training of diagonal linear networks converges to the linear predictor with minimal 1-norm among minimizers of the training loss. In this paper, we deepen this analysis showing that the full training trajectory of diagonal linear networks is closely related to the lasso regularization path. In this connection, the training time plays the role of an inverse regularization parameter. Both rigorous results and simulations are provided to illustrate this conclusion. Under a monotonicity assumption on the lasso regularization path, the connection is exact while in the general case, we show an approximate connection.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.18766.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18766",
    "published": "2025-09-23T07:59:25Z",
    "updated": "2026-01-28T10:36:18Z",
    "comment": "32 pages, 1 figure",
    "light_analysis": {
      "overview": "本文揭示了对角线性网络的训练轨迹与lasso正则化路径之间的精确或近似联系，训练时间作为逆正则化参数。",
      "motivation": "神经网络中的隐式正则化机制是机器学习理论的核心问题，但对训练动态的深入理解仍有不足。对角线性网络作为简化模型，便于严格分析，但现有研究多集中于收敛结果而非整个轨迹。本文旨在弥补这一空白，探索训练过程如何与正则化路径关联，以提升对复杂网络正则化行为的理论认识，为解决过拟合和模型选择问题提供新思路。",
      "method": "论文使用对角线性网络模型，该模型具有线性激活函数和对角权重矩阵，便于理论分析。核心方法是通过数学推导，将网络的训练轨迹映射到lasso正则化路径上，并创新性地将训练时间视为逆正则化参数。在单调性假设下，这种联系是精确的；一般情况下则为近似。研究还辅以数值模拟验证理论结果，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "理论分析表明，在lasso正则化路径满足单调性假设时，对角线性网络的训练轨迹与正则化路径精确对应；而在一般非单调情况下，两者呈现近似联系。模拟实验验证了这一结论，但摘要未提供具体性能指标如准确率或效率数据，重点在于展示轨迹与路径的匹配程度，为理论发现提供实证支持。",
      "conclusion": "本研究的主要贡献在于建立对角线性网络训练轨迹与lasso正则化路径的理论联系，深化了对隐式正则化机制的理解。学术价值在于为神经网络正则化分析提供了新视角，实际应用可能有助于优化训练策略。局限性包括依赖单调性假设，未来工作可扩展至更复杂网络或探索非单调情况下的精确分析。",
      "tags": [
        "Diagonal Linear Networks",
        "Lasso Regularization",
        "Implicit Regularization",
        "Training Trajectory",
        "Linear Activation"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:50.486189Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.17866",
    "title": "Understanding Post-Training Structural Changes in Large Language Models",
    "authors": [
      "Xinyu He",
      "Xianghui Cao"
    ],
    "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.17866.pdf",
    "abs_url": "https://arxiv.org/abs/2509.17866",
    "published": "2025-09-22T15:03:36Z",
    "updated": "2026-01-28T14:10:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过奇异值分解分析揭示了大型语言模型后训练中参数空间的均匀缩放和正交变换规律，并提出了一个描述参数协调动态的框架。",
      "motivation": "后训练（如指令调整和长链思考蒸馏）显著改变大型语言模型的行为，但对其如何影响内部参数空间的理解不足。现有研究多将参数空间视为黑盒，缺乏对后训练过程中参数演化机制的清晰认识。这限制了模型优化的效率和可解释性，因此探究后训练的结构变化以提升模型性能和理解至关重要。",
      "method": "研究采用系统性的奇异值分解（SVD）方法，对预训练大型语言模型的主要线性层进行分析，重点关注指令调整和长链思考蒸馏这两种后训练方法。关键创新在于揭示了参数矩阵的奇异值发生均匀几何缩放，同时左右奇异向量经历高度一致的正交变换。基于这些发现，提出一个简单有效的框架来描述参数协调动态，阐明后训练如何依赖于预训练基础能力。",
      "result": "分析结果显示，后训练导致参数矩阵的奇异值呈现均匀缩放，这种缩放支撑了温度控制的调节机制；奇异向量的正交变换则编码了语义对齐信息。这些发现挑战了参数空间作为黑盒的普遍观点，首次揭示了训练过程中参数演化的清晰规律。实验进一步证实了这些结构变化的鲁棒性和一致性，为理解模型行为提供了数据支撑。",
      "conclusion": "本研究的主要贡献在于通过SVD分析揭示了大型语言模型后训练中参数空间的结构变化规律，并提出了一个框架解释这些变化如何依赖于预训练基础能力。这为理解模型参数演化提供了新视角，具有重要学术价值，有助于提升模型可解释性和优化策略。未来工作可进一步探索这些规律在不同模型和任务中的应用，以及潜在局限性。",
      "tags": [
        "Large Language Model",
        "Singular Value Decomposition",
        "Instruction Tuning",
        "Long-CoT Distillation",
        "Parameter Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:47:49.676910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.17641",
    "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?",
    "authors": [
      "Hyunjong Ok",
      "Suho Yoo",
      "Hyeonjun Kim",
      "Jaeho Lee"
    ],
    "abstract": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.17641.pdf",
    "abs_url": "https://arxiv.org/abs/2509.17641",
    "published": "2025-09-22T11:45:22Z",
    "updated": "2026-01-28T06:19:42Z",
    "comment": "ICASSP 2026",
    "light_analysis": {
      "overview": "论文提出了AuditoryBench++基准和AIR-CoT方法，以评估和增强语言模型在纯文本环境下的听觉知识和推理能力。",
      "motivation": "该研究旨在解决语言模型在缺乏直接听觉输入时理解听觉知识的问题。人类可以轻松利用听觉常识进行推理，如音高和声源关联，但语言模型在纯文本环境下往往无法有效处理这些属性，限制了其在多模态交互中的应用。现有方法在评估和提升听觉推理能力方面存在不足，因此需要新基准和方法来填补这一空白，以促进模型在听觉相关任务中的表现。",
      "method": "论文提出了两个关键方法：一是AuditoryBench++基准，它涵盖了从基本听觉比较到上下文基础推理的任务，用于在纯文本设置下对模型进行细粒度评估；二是AIR-CoT方法，一种新颖的听觉想象推理技术，通过检测文本中的跨度并使用特殊标记来生成听觉信息，同时结合知识注入来整合外部知识，以增强推理过程。这些方法专注于文本处理，不依赖于实际声音输入，旨在提升模型对听觉概念的理解和整合能力。",
      "result": "通过与最近的大型语言模型（LLMs）和多模态语言模型（MLLMs）进行广泛实验，结果表明AIR-CoT方法在AuditoryBench++基准上通常优于未经修改的现成模型以及通过其他方式增强听觉知识的模型。这显示了AIR-CoT在提升语言模型听觉推理性能方面的有效性，尽管具体性能指标如准确率提升在摘要中未明确说明，但实验对比突出了其相对于基线方法的优势。",
      "conclusion": "本研究的主要贡献是引入了AuditoryBench++基准和AIR-CoT方法，为语言模型在纯文本环境中的听觉知识和推理能力评估提供了新工具。这推动了多模态人工智能领域的研究，并可能改善模型在涉及听觉交互的实际应用中的表现。未来工作可包括扩展基准任务、集成更广泛的听觉知识或探索其在更复杂多模态场景中的应用潜力。",
      "tags": [
        "Large Language Models",
        "Auditory Reasoning",
        "Benchmark Evaluation",
        "Span Detection",
        "Knowledge Injection"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:23.643307Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.17562",
    "title": "Visual Instruction Pretraining for Domain-Specific Foundation Models",
    "authors": [
      "Yuxuan Li",
      "Yicheng Zhang",
      "Wenhao Tang",
      "Yimian Dai",
      "Ming-Ming Cheng",
      "Xiang Li",
      "Jian Yang"
    ],
    "abstract": "Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at https://github.com/zcablii/ViTP.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.17562.pdf",
    "abs_url": "https://arxiv.org/abs/2509.17562",
    "published": "2025-09-22T10:57:42Z",
    "updated": "2026-01-28T07:15:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "ViTP 提出了一种视觉指令预训练范式，通过整合推理直接增强感知学习，提升下游领域基础模型的性能。",
      "motivation": "该研究针对现代计算机视觉闭环中感知、推理和生成相互增强，但高层推理对低层感知特征基础学习影响未被充分探索的问题。现有方法在特定下游领域如遥感和医学成像中未能有效利用推理来优化感知学习，导致模型泛化能力不足。因此，开发一种新的预训练方法以填补这一空白，对于提升领域特定任务的性能至关重要。",
      "method": "ViTP 方法将 Vision Transformer (ViT) 主干嵌入到 Vision-Language Model 中，利用从目标下游领域策划的视觉指令数据进行端到端预训练。关键创新是 Visual Robustness Learning (VRL)，它迫使模型从稀疏的视觉 token 中学习鲁棒且与领域相关的特征，从而直接通过推理过程指导感知特征的学习。这一结合增强了模型在复杂环境下的适应性。",
      "result": "在16个挑战性遥感和医学成像基准测试上进行广泛实验，ViTP 在各种下游任务中建立了新的最先进性能。实验显示，相较于基线方法，ViTP 在领域特定任务上显著提升，摘要未提供具体数据指标，但强调了其在多样任务中的优越性和鲁棒性，验证了方法的有效性。",
      "conclusion": "ViTP 通过视觉指令预训练成功整合推理与感知，在遥感和医学成像等领域实现了性能突破，推动了基础模型预训练技术的发展。其学术价值在于提出一种新范式，实际应用中对高精度视觉任务有重要贡献。未来方向可能包括扩展到更多领域或优化多模态融合，摘要未明确说明。",
      "tags": [
        "Visual Instruction Pretraining",
        "Vision Transformer",
        "Vision-Language Model",
        "Visual Robustness Learning",
        "Domain-Specific Foundation Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:10.540395Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.17100",
    "title": "The SAGES Critical View of Safety Challenge: A Global Benchmark for AI-Assisted Surgical Quality Assessment",
    "authors": [
      "Deepak Alapatt",
      "Jennifer Eckhoff",
      "Zhiliang Lyu",
      "Yutong Ban",
      "Jean-Paul Mazellier",
      "Sarah Choksi",
      "Kunyi Yang",
      "Po-Hsing Chiang",
      "Noemi Zorzetti",
      "Samuele Cannas",
      "Daniel Neimark",
      "Omri Bar",
      "Amine Yamlahi",
      "Jakob Hennighausen",
      "Xiaohan Wang",
      "Rui Li",
      "Long Liang",
      "Yuxian Wang",
      "Saurabh Koju",
      "Binod Bhattarai",
      "Tim Jaspers",
      "Zhehua Mao",
      "Anjana Wijekoon",
      "Jun Ma",
      "Yinan Xu",
      "Zhilong Weng",
      "Ammar M. Okran",
      "Hatem A. Rashwan",
      "Boyang Shen",
      "Kaixiang Yang",
      "Yutao Zhang",
      "Hao Wang",
      "2024 CVS Challenge Consortium",
      "Quanzheng Li",
      "Filippo Filicori",
      "Xiang Li",
      "Pietro Mascagni",
      "Daniel A. Hashimoto",
      "Guy Rosman",
      "Ozanan Meireles",
      "Nicolas Padoy"
    ],
    "abstract": "Advances in artificial intelligence (AI) for surgical quality assessment promise to democratize access to expertise, with applications in training, guidance, and accreditation. This study presents the SAGES Critical View of Safety (CVS) Challenge, the first AI competition organized by a surgical society, using the CVS in laparoscopic cholecystectomy, a universally recommended yet inconsistently performed safety step, as an exemplar of surgical quality assessment. A global collaboration across 54 institutions in 24 countries engaged hundreds of clinicians and engineers to curate 1,000 videos annotated by 20 surgical experts according to a consensus-validated protocol. The challenge addressed key barriers to real-world deployment in surgery, including achieving high performance, capturing uncertainty in subjective assessment, and ensuring robustness to clinical variability. To enable this scale of effort, we developed EndoGlacier, a framework for managing large, heterogeneous surgical video and multi-annotator workflows. Thirteen international teams participated, achieving up to a 17% relative gain in assessment performance, over 80% reduction in calibration error, and a 17% relative improvement in robustness over the state-of-the-art. Analysis of results highlighted methodological trends linked to model performance, providing guidance for future research toward robust, clinically deployable AI for surgical quality assessment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.17100.pdf",
    "abs_url": "https://arxiv.org/abs/2509.17100",
    "published": "2025-09-21T14:41:26Z",
    "updated": "2026-01-28T17:09:40Z",
    "comment": "21 pages, 10 figures",
    "light_analysis": {
      "overview": "论文提出了首个由外科协会组织的SAGES Critical View of Safety挑战，作为AI辅助手术质量评估的全球基准，并通过EndoGlacier框架管理大规模手术视频数据。",
      "motivation": "研究旨在解决手术质量评估中的实际挑战，特别是腹腔镜胆囊切除术中的Critical View of Safety（CVS）步骤，该步骤虽被普遍推荐但执行不一致。现有方法在标准化评估、处理主观不确定性及适应临床变异方面存在不足，限制了AI在实际手术部署中的应用。AI辅助评估可促进专业知识的普及，应用于培训、指导和认证，从而提高手术安全和质量。",
      "method": "论文通过组织SAGES Critical View of Safety挑战，收集了1000个腹腔镜胆囊切除术视频，由20名外科专家根据共识验证协议进行注释，涉及全球54个机构、24个国家的合作。为了支持大规模工作，开发了EndoGlacier框架，用于管理异构手术视频和多注释者工作流。挑战评估了AI模型在性能、不确定性捕获和临床变异鲁棒性方面的表现，共有13个国际团队参与。",
      "result": "13个国际团队参与挑战，最佳模型在评估性能上实现了17%的相对增益，校准误差降低了超过80%，鲁棒性相比现有技术相对提高了17%。结果分析揭示了与模型性能相关的方法学趋势，例如不确定性校准和鲁棒性设计，为未来研究提供了指导。这些数据表明，AI在手术质量评估中可显著提升准确性和可靠性。",
      "conclusion": "论文的主要贡献是推出了SAGES Critical View of Safety挑战，作为AI辅助手术质量评估的全球基准，并提供了大规模数据集和分析框架。研究强调了处理不确定性和临床变异的重要性，推动了鲁棒、可临床部署的AI发展。实际应用价值包括促进手术培训和认证，未来工作可扩展至其他手术类型或优化模型性能。摘要未明确说明具体局限性。",
      "tags": [
        "Surgical Quality Assessment",
        "Video Annotation",
        "Uncertainty Calibration",
        "Clinical Robustness"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:19.470117Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14427",
    "title": "Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models",
    "authors": [
      "Ilyass Moummad",
      "Kawtar Zaher",
      "Lukas Rauch",
      "Alexis Joly"
    ],
    "abstract": "Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training. In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings. We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing. Our approach combines these techniques with frozen embeddings from state-of-the-art vision and audio encoders to yield competitive retrieval performance without any additional learning or fine-tuning. To demonstrate the generality and effectiveness of this approach, we evaluate it on standard image retrieval benchmarks as well as a newly introduced benchmark for audio hashing.",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.14427.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14427",
    "published": "2025-09-17T20:58:43Z",
    "updated": "2026-01-28T17:01:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Hashing-Baseline，一种利用预训练编码器和经典哈希技术的训练无关方法，为信息检索提供强基线，实现竞争性性能。",
      "motivation": "当前哈希方法需要昂贵的特定场景训练，限制了可扩展性和效率。预训练模型的出现提供了丰富嵌入，但现有方法往往忽视训练无关技术的潜力。本研究旨在解决训练成本高的问题，重新审视经典技术，以提升快速搜索应用的灵活性和效率。",
      "method": "方法的核心是结合预训练编码器（如视觉和音频编码器）产生的冻结嵌入，应用经典训练无关技术：主成分分析（PCA）、随机正交投影和阈值二值化。这些技术用于生成紧凑二进制表示，而无需额外学习或微调。创新点在于利用预训练模型的表示能力与简单哈希技术结合，避免复杂训练过程。",
      "result": "摘要未明确说明具体性能数据，但通过在标准图像检索基准和新音频哈希基准上评估，方法无需训练即可实现竞争性检索性能。与需要场景特定训练的现有方法相比，Hashing-Baseline降低了成本并保持了良好效果，展示了其泛化能力和实用性。",
      "conclusion": "本文的主要贡献是提出Hashing-Baseline，一种训练无关的哈希方法，结合预训练编码器和经典技术。学术价值在于重新评估传统方法在预训练时代的适用性，为研究提供新视角。实际应用价值在于减少训练开销，促进快速搜索发展。局限性可能依赖预训练模型质量，未来工作可探索多模态扩展。",
      "tags": [
        "Hashing",
        "Pretrained Models",
        "Principal Component Analysis",
        "Random Projection",
        "Binary Embeddings"
      ]
    },
    "analyzed_at": "2026-01-29T03:48:37.593240Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.02072",
    "title": "Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports",
    "authors": [
      "Jian Chen",
      "Jiabao Dou"
    ],
    "abstract": "The automatic classification of occupational accident reports is pivotal for workplace safety analysis but is persistently hindered by severe class imbalance and data scarcity. In this paper, we propose ABEX-RAT, a resource-efficient framework that synergizes generative data augmentation with robust adversarial learning. Unlike computationally expensive large language models (LLMs) fine-tuning, our approach employs a two-stage abstractive-expansive (ABEX) pipeline: it first utilizes a prompt-guided LLM to distill label-critical semantics into concise abstracts, which are then expanded into diverse synthetic samples to balance the data distribution. Subsequently, we train a lightweight classifier using a random adversarial training (RAT) protocol, which stochastically injects perturbations to enhance generalization without significant computational overhead. Experimental results on the OSHA dataset demonstrate that ABEXRAT establishes a new state-of-the-art, achieving a Macro-F1 score of 90.32% and significantly outperforming both traditional baselines and fine-tuned large models. This confirms that targeted augmentation combined with robust training offers a superior, data-efficient alternative for specialized domain classification. The source code will be made publicly available upon acceptance.",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.02072.pdf",
    "abs_url": "https://arxiv.org/abs/2509.02072",
    "published": "2025-09-02T08:22:59Z",
    "updated": "2026-01-28T04:07:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "ABEX-RAT框架通过结合抽象数据增强与对抗训练，为职业事故报告分类提供了一种资源高效的方法，显著提升分类性能。",
      "motivation": "职业事故报告的自动分类对于工作场所安全分析至关重要，但面临严重的类别不平衡和数据稀缺问题，这限制了传统方法的准确性和实用性。现有方法如基于大型语言模型（LLM）的微调虽然效果较好，但计算成本高昂，不适合资源受限环境。因此，迫切需要开发一种更高效、数据利用率高的技术来克服这些挑战，以提升模型泛化能力并减少计算开销，从而在实际应用中推广。",
      "method": "ABEX-RAT采用两阶段抽象扩展（ABEX）管道：首先使用提示引导的大型语言模型（LLM）从原始报告中提取标签关键语义，生成简洁摘要；然后将这些摘要扩展为多样化的合成样本，以平衡数据分布。接着，通过随机对抗训练（RAT）协议训练一个轻量级分类器，该协议在训练过程中随机注入扰动，以增强模型的鲁棒性和泛化能力，而无需显著增加计算资源或时间成本。",
      "result": "在OSHA数据集上的实验结果表明，ABEX-RAT实现了90.32%的Macro-F1分数，显著优于传统基线方法和微调的大型语言模型。这一性能提升证实了框架的有效性，同时在资源效率方面表现出色，无需大量数据或计算负载就能达到最优分类效果，为特定领域任务设立了新的基准。",
      "conclusion": "本论文的主要贡献在于提出了ABEX-RAT框架，通过结合目标驱动的数据增强和鲁棒对抗训练，为职业事故报告分类提供了一种数据高效、计算轻量的解决方案。这不仅在学术上验证了高效方法在特定领域的适用性，还具有实际应用价值，可推广到其他类似的数据稀缺和类别不平衡问题。未来工作可能包括扩展到更多领域和优化模型参数，源代码开源将促进进一步研究。",
      "tags": [
        "Abstractive Augmentation",
        "Adversarial Training",
        "Large Language Model (LLM)",
        "Prompt-Guided Generation",
        "Occupational Accident Classification"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:02.301538Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.00923",
    "title": "Robust Deep Monte Carlo Counterfactual Regret Minimization: Addressing Theoretical Risks in Neural Fictitious Self-Play",
    "authors": [
      "Zakaria El Jaafari"
    ],
    "abstract": "Monte Carlo Counterfactual Regret Minimization (MCCFR) has emerged as a cornerstone algorithm for solving extensive-form games, but its integration with deep neural networks introduces scale-dependent challenges that manifest differently across game complexities. This paper presents a comprehensive analysis of how neural MCCFR component effectiveness varies with game scale and proposes an adaptive framework for selective component deployment. We identify that theoretical risks such as nonstationary target distribution shifts, action support collapse, variance explosion, and warm-starting bias have scale-dependent manifestation patterns, requiring different mitigation strategies for small versus large games. Our proposed Robust Deep MCCFR framework incorporates target networks with delayed updates, uniform exploration mixing, variance-aware training objectives, and comprehensive diagnostic monitoring. Through systematic ablation studies on Kuhn and Leduc Poker, we demonstrate scale-dependent component effectiveness and identify critical component interactions. The best configuration achieves final exploitability of 0.0628 on Kuhn Poker, representing a 60% improvement over the classical framework (0.156). On the more complex Leduc Poker domain, selective component usage achieves exploitability of 0.2386, a 23.5% improvement over the classical framework (0.3703) and highlighting the importance of careful component selection over comprehensive mitigation. Our contributions include: (1) a formal theoretical analysis of risks in neural MCCFR, (2) a principled mitigation framework with convergence guarantees, (3) comprehensive multi-scale experimental validation revealing scale-dependent component interactions, and (4) practical guidelines for deployment in larger games.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.00923.pdf",
    "abs_url": "https://arxiv.org/abs/2509.00923",
    "published": "2025-08-31T16:19:16Z",
    "updated": "2026-01-28T12:53:55Z",
    "comment": "There seems to be some errors related to the encountered problems and the interpreation of numerical results, that do not have a common pattern",
    "light_analysis": {
      "overview": "论文提出了 Robust Deep MCCFR 框架，通过自适应组件部署缓解深度神经 Monte Carlo Counterfactual Regret Minimization 中的规模依赖风险，显著提升博弈求解性能。",
      "motivation": "Monte Carlo Counterfactual Regret Minimization (MCCFR) 是求解扩展形式博弈的关键算法，但其与深度神经网络整合时面临规模依赖挑战，如非平稳目标分布变化、方差爆炸等风险在不同游戏规模中表现不同，导致性能不稳定。现有方法未能有效处理这些风险，特别是在大型游戏中，因此需要研究自适应策略以提高算法的鲁棒性和效率，解决实际部署中的理论风险。",
      "method": "论文提出了 Robust Deep MCCFR 框架，包含目标网络延迟更新以减少非平稳性问题，均匀探索混合防止行动支持崩溃，方差感知训练目标控制方差，以及全面诊断监控风险。关键创新在于根据游戏规模自适应选择组件，并通过在 Kuhn 和 Leduc Poker 上的系统消融研究验证组件交互。框架结合理论分析，提供收敛保证，确保原则性缓解。",
      "result": "实验结果表明，在 Kuhn Poker 中，最佳配置的 exploitability 降至 0.0628，比经典框架的 0.156 提高了 60%。在更复杂的 Leduc Poker 上，选择性组件使用使 exploitability 达到 0.2386，比经典框架的 0.3703 提高了 23.5%。消融研究验证了组件效果的规模依赖性，与基线对比显示显著性能提升。",
      "conclusion": "论文的主要贡献包括神经 MCCFR 风险的形式理论分析、具有收敛保证的原则缓解框架、多尺度实验验证组件交互，以及大型游戏部署指南。该研究增强了深度强化学习在博弈论中的理论基础和应用价值，为处理规模依赖问题提供了有效方案。未来工作可扩展至更大规模游戏或其他领域。",
      "tags": [
        "Monte Carlo Counterfactual Regret Minimization",
        "Deep Neural Networks",
        "Extensive-form Games",
        "Reinforcement Learning",
        "Variance Reduction"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:08.271527Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.16922",
    "title": "MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition",
    "authors": [
      "Yudong Hu",
      "Yueju Han",
      "Rui Sun",
      "Jinke Ren"
    ],
    "abstract": "Capsule Network (CapsNet) has demonstrated significant potential in visual recognition by capturing spatial relationships and part-whole hierarchies for learning equivariant feature representations. However, existing CapsNet and variants often rely on a single high-level feature map, overlooking the rich complementary information from multi-scale features. Furthermore, conventional feature fusion strategies (e.g., addition and concatenation) struggle to reconcile multi-scale feature discrepancies, leading to suboptimal classification performance. To address these limitations, we propose the Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that integrates multi-scale feature learning and efficient capsule routing. Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature representations from input images, preserving both fine-grained details and global contextual information. Second, the PatchifyCaps partitions these multi-scale features into primary capsules using a uniform patch size, equipping the model with the ability to learn from diverse receptive fields. Finally, the CAR block adaptively routes the multi-scale capsules by identifying cross-scale prediction pairs with maximum agreement. Unlike the simple concatenation of multiple self-routing blocks, CAR ensures that only the most coherent capsules contribute to the final voting. Our proposed MSPCaps achieves remarkable scalability and superior robustness, consistently surpassing multiple baseline methods in terms of classification accuracy, with configurations ranging from a highly efficient Tiny model (344.3K parameters) to a powerful Large model (10.9M parameters), highlighting its potential in advancing feature representation learning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.16922.pdf",
    "abs_url": "https://arxiv.org/abs/2508.16922",
    "published": "2025-08-23T06:56:00Z",
    "updated": "2026-01-28T07:29:49Z",
    "comment": "9 pages, 4 figures; Code is available at https://github.com/abdn-hyd/MSPCaps",
    "light_analysis": {
      "overview": "MSPCaps 提出了一种整合多尺度特征学习和交叉一致性路由的胶囊网络，用于提升视觉识别性能。",
      "motivation": "本研究针对 Capsule Network（CapsNet）在视觉识别中的局限性展开。现有 CapsNet 及其变体通常依赖单一高层特征图，忽视了多尺度特征提供的丰富互补信息（如细粒度细节和全局上下文），这限制了模型捕捉复杂视觉结构的能力。此外，传统特征融合策略（如加法和拼接）难以有效调和多尺度特征间的差异，导致分类性能不佳。因此，需要一种能够综合利用多尺度特征并改进胶囊路由机制的方法，以克服这些挑战，从而提升模型的准确性和鲁棒性。",
      "method": "MSPCaps 采用了一个新颖的三组件架构：多尺度 ResNet 骨干网络（MSRB）、Patchify Capsule Layer（PatchifyCaps）和交叉一致性路由（CAR）块。MSRB 从输入图像中提取多尺度特征表示，确保同时保留局部细节和全局信息。PatchifyCaps 将这些多尺度特征分割成统一大小的补丁，形成初级胶囊，使模型能够从不同感受野中学习。CAR 块通过识别跨尺度预测对中的最大一致性，自适应地路由胶囊，确保只有最连贯的胶囊参与最终投票，从而取代了简单的拼接或加法融合方式。关键创新点在于整合多尺度特征学习和高效的路由机制。",
      "result": "MSPCaps 在分类任务中表现优异，持续超越多个基线方法。模型配置从高效的 Tiny 模型（344.3K 参数）到强大的 Large 模型（10.9M 参数），均展示了良好的可扩展性和鲁棒性，在分类准确率方面实现显著提升。然而，摘要未明确说明具体的准确率数值或详细的对比实验数据，仅强调其性能优越性。",
      "conclusion": "本文的主要贡献是提出了 MSPCaps，它通过整合多尺度特征学习和自适应胶囊路由，改进了特征表示学习，在视觉识别任务中展现出较高的分类准确性和模型可扩展性。该研究为多尺度信息融合和胶囊网络优化提供了新思路，具有重要的学术价值，并有望应用于其他视觉任务。未来工作可探索其在更广泛场景中的应用，并进一步优化计算效率和参数规模。",
      "tags": [
        "Capsule Network",
        "Multi-Scale Learning",
        "Cross-Agreement Routing",
        "ResNet Backbone",
        "Feature Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:26.907297Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.12216",
    "title": "Splat Feature Solver",
    "authors": [
      "Butian Xiong",
      "Rong Liu",
      "Kenneth Xu",
      "Meida Chen",
      "Andrew Feng"
    ],
    "abstract": "Feature lifting has emerged as a crucial component in 3D scene understanding, enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP) onto splat-based 3D representations. The core challenge lies in optimally assigning rich general attributes to 3D primitives while addressing the inconsistency issues from multi-view images. We present a unified, kernel- and feature-agnostic formulation of the feature lifting problem as a sparse linear inverse problem, which can be solved efficiently in closed form. Our approach admits a provable upper bound on the global optimal error under convex losses for delivering high quality lifted features. To address inconsistencies and noise in multi-view observations, we introduce two complementary regularization strategies to stabilize the solution and enhance semantic fidelity. Tikhonov Guidance enforces numerical stability through soft diagonal dominance, while Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic-forward baselines while producing lifted features in minutes. Our \\textbf{code} is available in the \\href{https://github.com/saliteta/splat-distiller/tree/main}{\\textcolor{blue}{GitHub}}. We provide additional \\href{https://splat-distiller.pages.dev/}{\\textcolor{blue}{website}} for more visualization, as well as the \\href{https://www.youtube.com/watch?v=CH-G5hbvArM}{\\textcolor{blue}{video}}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.12216.pdf",
    "abs_url": "https://arxiv.org/abs/2508.12216",
    "published": "2025-08-17T03:13:06Z",
    "updated": "2026-01-28T18:51:46Z",
    "comment": "ICLR 2026 Accepted",
    "light_analysis": {
      "overview": "本文提出一种基于稀疏线性逆问题和正则化策略的高效特征提升方法，显著改善基于splat的3D表示中图像特征的质量和一致性。",
      "motivation": "该研究针对3D场景理解中特征提升的核心挑战，即如何优化地将丰富图像特征（如DINO、CLIP）附加到splat-based 3D表示，并解决多视图图像带来的不一致性和噪声问题。现有方法可能在整合多源信息或处理噪声方面效率低下，导致特征质量下降，而此问题对于开放词汇3D分割等任务至关重要，因为准确的特征关联是实现高级语义理解和场景建模的基础。",
      "method": "该方法将特征提升问题统一公式化为稀疏线性逆问题，支持闭式高效求解，并具有凸损失下全局最优误差的可证明上界。为应对多视图不一致和噪声，引入两种互补正则化策略：Tikhonov Guidance通过软对角优势确保数值稳定性，Post-Lifting Aggregation通过特征聚类过滤噪声输入。该方法核与特征无关，适用于多种图像描述符，核心创新在于结合数学公式化和正则化设计，基于splat-based 3D表示进行实现。",
      "result": "在开放词汇3D分割基准测试中，该方法实现了最先进的性能，全面优于训练基、分组基和启发式基基线，在准确率和效率上均有显著提升。摘要未提供具体数据指标，但强调能在几分钟内生成高质量特征提升结果，展示了相较于现有方法在速度和效果上的优势。",
      "conclusion": "本研究的主要贡献在于提出一种高效、理论保证的特征提升框架，通过稀疏线性逆问题和正则化策略有效解决多视图不一致问题，在学术上推动了3D场景理解的理论创新，实际应用价值体现在提升开放词汇3D分割等任务的性能。局限性未明确说明，未来工作可能涉及扩展到更复杂3D表示或更多图像特征类型。",
      "tags": [
        "Feature Lifting",
        "Sparse Linear Inverse Problem",
        "Regularization",
        "3D Scene Understanding",
        "Open-Vocabulary Segmentation"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:27.316986Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.07286",
    "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking",
    "authors": [
      "Jian Chen",
      "Jiabao Dou"
    ],
    "abstract": "Accurate information extraction from specialized texts is a critical challenge for automated rule checking (ARC) in the architecture, engineering, and construction (AEC) domain. While large language models (LLMs) possess strong reasoning capabilities, their deployment in resource-constrained AEC environments is often impractical. Conversely, standard efficient models struggle with the significant domain gap. Although this gap can be mitigated by pre-training on large, humancurated corpora, such approaches are labor-intensive and costly. To address this, we propose ARCE (Augmented RoBERTa with Contextualized Elucidations), a novel knowledge distillation framework that leverages LLMs to synthesize a task-oriented corpus, termed Cote, for incrementally pre-training smaller models. ARCE systematically explores the optimal strategy for knowledge transfer. Our extensive experiments demonstrate that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20% and outperforming both domain-specific baselines and fine-tuned LLMs. Crucially, our study reveals a less is more principle: simple, direct explanations prove significantly more effective for domain adaptation than complex, role-based rationales in the NER task, which tend to introduce semantic noise. The source code will be made publicly available upon acceptance.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.07286.pdf",
    "abs_url": "https://arxiv.org/abs/2508.07286",
    "published": "2025-08-10T10:49:48Z",
    "updated": "2026-01-28T03:46:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出ARCE框架，通过知识蒸馏利用大语言模型合成任务导向语料库，增强小模型在建筑、工程和施工领域自动规则检查中的命名实体识别性能。",
      "motivation": "自动规则检查在AEC领域对从专业文本中准确提取信息至关重要，但大语言模型因资源消耗高难以部署，而标准高效模型面临显著领域差距。现有方法如人工构建大规模预训练语料库可缓解差距，但成本高昂且劳动密集。因此，研究旨在开发一种高效知识迁移方法，将大语言模型的能力适配到小模型中，以平衡性能和实用性。",
      "method": "ARCE框架采用知识蒸馏技术，首先利用大语言模型生成上下文化的解释语料库Cote，针对AEC领域的命名实体识别任务。接着，使用Cote对RoBERTa等小模型进行增量预训练，系统探索最优知识转移策略。关键创新在于优化解释形式，发现简单直接的解释比复杂角色理性更有效，能减少语义噪声，提升模型适应能力。",
      "result": "实验在基准AEC数据集上显示，ARCE实现了77.20%的Macro-F1得分，超越了领域特定基线模型和经过微调的大语言模型，确立了新的最优性能。研究还验证了“少即是多”原则，简单解释在领域适应中优于复杂理性，能避免引入语义干扰，证明了框架的高效性和鲁棒性。",
      "conclusion": "ARCE框架通过知识蒸馏有效解决AEC领域命名实体识别的领域差距问题，提供资源高效的解决方案，具有重要学术和实际应用价值。研究揭示了简单解释在知识迁移中的优势，可促进自动化规则检查的发展，未来可扩展该方法到其他领域或优化知识转移策略。",
      "tags": [
        "Knowledge Distillation",
        "Large Language Models",
        "Named Entity Recognition",
        "Domain Adaptation",
        "Contextualized Elucidations"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:39.224736Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.05470",
    "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations",
    "authors": [
      "Li-Chun Lu",
      "Miri Liu",
      "Pin-Chun Lu",
      "Yufei Tian",
      "Shao-Hua Sun",
      "Nanyun Peng"
    ],
    "abstract": "We examine, analyze, and compare four representative creativity measures--perplexity, LLM-as-a-Judge, the Creativity Index (CI; measuring n-gram overlap with web corpora), and syntactic templates (detecting repetition of common part-of-speech patterns)--across the diverse creative domains, such as creative writing, unconventional problem-solving, and research ideation. For each domain, we compile datasets with human-aligned creative and uncreative examples and evaluate each metric's ability to discriminate between the two sets. Our analyses reveal limited consistency both across domains and metrics, as metrics that distinguish creativity in one domain fail in others (e.g., CI correctly distinguishes in creative writing but fails in problem-solving), and different metrics often disagree on the same data points (e.g., CI suggests one set to be more creative, while perplexity indicates the other set to be more creative.) We highlight key limitations, such as perplexity reflecting fluency rather than novelty; LLM-as-a-Judge producing inconsistent judgments under minor prompt variations and exhibiting bias towards particular labels; CI primarily measuring lexical diversity, with high sensitivity to implementation choices; and syntactic templates being ineffective in settings dominated by formulaic language. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.05470.pdf",
    "abs_url": "https://arxiv.org/abs/2508.05470",
    "published": "2025-08-07T15:11:48Z",
    "updated": "2026-01-28T18:20:03Z",
    "comment": "EACL 2026",
    "light_analysis": {
      "overview": "论文批判性地分析了四种创造力评估指标，揭示了它们在跨领域中的有限一致性，并提出需要更稳健的评估框架。",
      "motivation": "该研究旨在解决现有创造力评估方法在不同创造性领域中的不一致性和局限性问题。创造力评估对AI在创意写作、问题解决等应用至关重要，但现有方法如困惑度、LLM作为裁判等，存在测量内容不准确（如困惑度反映流畅性而非新颖性）、判断不稳定（LLM在提示变化下产生矛盾）、或实现依赖性强（创造力指数对网络语料选择敏感），导致评估结果不可靠，无法普遍适用，从而强调了改进评估框架以更好地对齐人类判断的必要性。",
      "method": "研究方法系统性地比较了四种创造力测量方法：困惑度、LLM-as-a-Judge、创造力指数（通过n-gram重叠与网络语料库对比）和句法模板（检测常见词性模式重复）。关键创新在于跨多个创造性领域（如创意写作、非常规问题解决、研究构思）进行分析，并编译了每个领域的人类对齐数据集，包含创造性和非创造性示例，以评估这些指标在区分两组示例时的表现，从而分析其跨领域一致性和技术特点。",
      "result": "实验结果显示，现有创造力评估指标在跨领域和指标间的一致性有限。例如，创造力指数在创意写作中能有效区分创造性，但在问题解决领域失败；不同指标如困惑度和创造力指数在同一数据点上常常矛盾。分析揭示了各指标的局限性：困惑度偏向测量流畅性，LLM作为裁判在轻微提示变化下不稳定且有标签偏见，创造力指数主要测量词汇多样性且实现敏感，句法模板在公式化语言中无效，总体表明现有方法无法可靠捕捉人类创造力，与基线方法对比显示出显著差异。",
      "conclusion": "论文的主要贡献在于批判性地揭示了现有创造力评估方法的不足，如指标特异性、不一致性和偏见，强调了需要开发更稳健、可泛化的评估框架以更好地对齐人类判断，这对AI在创意领域的学术和应用价值有重要意义。局限性包括当前方法对领域依赖性强，未来工作应探索更全面的评估标准和统一框架，以提升可靠性和普适性。",
      "tags": [
        "Creativity Evaluation",
        "Perplexity",
        "Large Language Model",
        "N-gram Overlap",
        "Syntactic Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:42.171862Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.04655",
    "title": "X-SAM: From Segment Anything to Any Segmentation",
    "authors": [
      "Hao Wang",
      "Limeng Qiao",
      "Zequn Jie",
      "Zhijian Huang",
      "Chengjian Feng",
      "Qingfang Zheng",
      "Lin Ma",
      "Xiangyuan Lan",
      "Xiaodan Liang"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.04655.pdf",
    "abs_url": "https://arxiv.org/abs/2508.04655",
    "published": "2025-08-06T17:19:10Z",
    "updated": "2026-01-28T15:50:17Z",
    "comment": "AAAI2026",
    "light_analysis": {
      "overview": "X-SAM 提出一个统一的多模态大语言模型框架，从分割任何东西扩展为任何分割任务，实现了高级像素级感知理解。",
      "motivation": "大型语言模型在广泛知识表示方面表现出色，但在像素级感知理解上存在固有不足。尽管分割任意模型在视觉提示驱动的图像分割方面有显著进展，但它在多掩码预测和类别特定分割任务中存在限制，且无法统一所有分割任务于一个模型中。这导致了分割任务处理的不完整性和效率问题，为解决这些限制，本研究旨在开发一个更通用的框架，提升分割任务的全面性和实用性。",
      "method": "X-SAM 基于简化的多模态大语言模型框架，引入了新的统一框架，增强像素级感知理解。核心创新包括提出视觉接地分割任务，通过交互式视觉提示分割所有实例对象，赋予模型视觉接地、像素级解释能力。此外，采用了统一的训练策略，支持跨多个数据集的共同训练，以有效利用多样化数据源。关键细节涵盖 MLLMs 架构扩展和视觉提示处理机制的优化。",
      "result": "实验结果显示，X-SAM 在广泛的图像分割基准上实现了最先进的性能，强调了其在多模态、像素级视觉理解方面的效率。虽然摘要未明确说明具体数据如准确率提升，但与基线方法相比，X-SAM 在多个任务中表现出优越性能。这显示了该框架在统一处理分割任务时的显著改进。",
      "conclusion": "X-SAM 的主要贡献在于提出了一个统一的分割框架，扩展了分割范式，并通过视觉接地分割任务和统一训练策略，增强了 MLLMs 的像素级理解能力。学术价值体现在推动多模态大语言模型在视觉任务中的应用创新，实际应用价值在于提升分割任务的通用性和效率。未来工作可能涉及进一步优化模型或扩展到更多视觉任务。",
      "tags": [
        "Multimodal Large Language Model",
        "Visual GrounDed Segmentation",
        "Unified Training Strategy",
        "Segment Anything Model",
        "Image Segmentation"
      ]
    },
    "analyzed_at": "2026-01-29T03:49:55.673782Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.02197",
    "title": "A Message Passing Realization of Expected Free Energy Minimization",
    "authors": [
      "Wouter W. L. Nuijten",
      "Mykola Lukashchuk",
      "Thijs van de Laar",
      "Bert de Vries"
    ],
    "abstract": "We present a message passing approach to Expected Free Energy (EFE) minimization on factor graphs, based on the theory introduced in arXiv:2504.14898. By reformulating EFE minimization as Variational Free Energy minimization with epistemic priors, we transform a combinatorial search problem into a tractable inference problem solvable through standard variational techniques. Applying our message passing method to factorized state-space models enables efficient policy inference. We evaluate our method on environments with epistemic uncertainty: a stochastic gridworld and a partially observable Minigrid task. Agents using our approach consistently outperform conventional KL-control agents on these tasks, showing more robust planning and efficient exploration under uncertainty. In the stochastic gridworld environment, EFE-minimizing agents avoid risky paths, while in the partially observable minigrid setting, they conduct more systematic information-seeking. This approach bridges active inference theory with practical implementations, providing empirical evidence for the efficiency of epistemic priors in artificial agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.02197.pdf",
    "abs_url": "https://arxiv.org/abs/2508.02197",
    "published": "2025-08-04T08:48:37Z",
    "updated": "2026-01-28T10:28:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于因子图的消息传递方法，实现预期自由能最小化，将组合搜索问题转化为可处理推理问题，桥接主动推理理论与实际应用。",
      "motivation": "在认知不确定环境中，代理需要高效规划和探索以应对未知情况，但现有方法如传统KL控制代理在复杂任务中表现不足，缺乏鲁棒性和系统化探索能力。本研究旨在解决代理在认知不确定性下的决策效率问题，通过最小化预期自由能来改进规划和探索，应对传统方法在处理风险和信息寻求时的局限性。",
      "method": "本研究提出一种消息传递方法，将预期自由能最小化重新表述为带认知先验的变分自由能最小化，从而将组合搜索问题转化为可在因子图上使用标准变分技术求解的推理问题。关键创新在于引入认知先验，并应用于因子化状态空间模型，通过消息传递实现高效策略推理。该方法基于先前理论工作，利用变分技术优化因子图结构。",
      "result": "在随机网格世界和部分可观察Minigrid任务中评估该方法，结果表明代理优于传统KL控制代理，展示出更鲁棒的规划和高效探索。具体地，在随机网格世界中代理避免风险路径，在部分可观察Minigrid中进行系统化信息寻求，验证了认知先验在提升代理性能方面的有效性，但摘要未明确说明具体性能指标数据。",
      "conclusion": "本文的主要贡献是提出一种实用方法实现预期自由能最小化，桥接主动推理理论与实际实现，为认知先验在人工代理中的效率提供了实证证据。该研究具有学术价值，推动了主动推理的应用，并展示了在不确定环境下改进AI代理性能的潜力，未来工作可能涉及扩展到更复杂环境或与其他技术结合。",
      "tags": [
        "Message Passing",
        "Expected Free Energy",
        "Variational Inference",
        "Factor Graphs",
        "Active Inference"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:23.209604Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.00282",
    "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks",
    "authors": [
      "Yi-Long Lu",
      "Jiajun Song",
      "Chunhui Zhang",
      "Wei Wang"
    ],
    "abstract": "Humans constantly generate a diverse range of tasks guided by internal motivations. While generative agents powered by large language models (LLMs) aim to simulate this complex behavior, it remains uncertain whether they operate on similar cognitive principles. To address this, we conducted a task-generation experiment comparing human responses with those of an LLM agent (GPT-4o). We find that human task generation is consistently influenced by psychological drivers, including personal values (e.g., Openness to Change) and cognitive style. Even when these psychological drivers are explicitly provided to the LLM, it fails to reflect the corresponding behavioral patterns. They produce tasks that are markedly less social, less physical, and thematically biased toward abstraction. Interestingly, while the LLM's tasks were perceived as more fun and novel, this highlights a disconnect between its linguistic proficiency and its capacity to generate human-like, embodied goals. We conclude that there is a core gap between the value-driven, embodied nature of human cognition and the statistical patterns of LLMs, highlighting the necessity of incorporating intrinsic motivation and physical grounding into the design of more human-aligned agents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.00282.pdf",
    "abs_url": "https://arxiv.org/abs/2508.00282",
    "published": "2025-08-01T03:00:41Z",
    "updated": "2026-01-28T03:06:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过实验揭示大型语言模型在任务生成上与人类存在核心差距，强调需整合内在动机和物理接地以改进模型对齐性。",
      "motivation": "研究动机在于探索人类与基于大语言模型的生成代理在任务生成上的认知差异。人类任务生成由内部心理驱动因素（如个人价值观和认知风格）指导，而LLM代理试图模拟此行为，但不确定是否基于相似认知原理。问题的重要性在于模拟人类任务生成对AI代理的对齐和实际应用（如智能助手）至关重要。现有方法的不足是即使提供心理驱动因素，LLM也无法反映人类行为模式，突显其在理解人类认知方面的局限。",
      "method": "研究方法采用任务生成实验，比较人类受试者与LLM代理（GPT-4o）的响应。实验中收集人类基于心理驱动因素生成的任务，并测试LLM在提供相同驱动因素时的表现。关键创新点在于直接评估心理因素（如开放性和认知风格）对任务生成的影响，并使用GPT-4o作为代表性LLM代理进行对比。摘要未明确说明具体实验设计细节（如数据集规模或任务类型），因此基于现有信息推断该方法侧重于心理因素注入和对比分析。",
      "result": "主要实验结果显示，人类任务生成一致受心理驱动因素影响，而LLM即使提供这些因素也无法模拟相应行为模式。具体效果包括：LLM生成的任务在社交性和物理性上显著低于人类，主题偏向抽象；有趣的是，LLM的任务被评价为更有趣和新颖。这表明LLM的语言熟练度与生成人类样体目标的能力存在脱节，突显其与人类认知的差异。与人类基线相比，LLM在模拟价值驱动的行为方面表现不足，摘要未提供具体性能指标数据如准确率或效率改进。",
      "conclusion": "结论指出人类认知的价值驱动和身体本质与LLM的统计模式之间存在核心差距，强调在设计更人类对齐的代理时需要整合内在动机和物理接地。研究的学术价值在于深化对LLM局限性的理解，促进认知科学与AI的交叉研究；实际应用价值在于指导未来智能代理开发以提高真实性和对齐性。潜在局限性可能包括实验样本和范围限制，未来工作可探索如何有效将心理因素和物理约束融入模型架构以缩小差距。",
      "tags": [
        "Large Language Model",
        "Task Generation",
        "Human-LLM Comparison",
        "Psychological Drivers",
        "Agent Alignment"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:37.596895Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.23599",
    "title": "DA-Occ: Direction-Aware 2D Convolution for Efficient and Geometry-Preserving 3D Occupancy Prediction",
    "authors": [
      "Yuchen Zhou",
      "Yan Luo",
      "Xiaogang Wang",
      "Xingjian Gu",
      "Mingzhou Lu"
    ],
    "abstract": "Efficient and high-accuracy 3D occupancy prediction is crucial for ensuring the performance of autonomous driving (AD) systems. However, many existing methods involve trade-offs between accuracy and efficiency. Some achieve high precision but with slow inference speed, while others adopt purely bird's-eye-view (BEV)-based 2D representations to accelerate processing, inevitably sacrificing vertical cues and compromising geometric integrity. To overcome these limitations, we propose a pure 2D framework that achieves efficient 3D occupancy prediction while preserving geometric integrity. Unlike conventional Lift-Splat-Shoot (LSS) methods that rely solely on depth scores to lift 2D features into 3D space, our approach additionally introduces a height-score projection to encode vertical geometric structure. We further employ direction-aware convolution to extract geometric features along both vertical and horizontal orientations, effectively balancing accuracy and computational efficiency. On the Occ3D-nuScenes, the proposed method achieves an mIoU of 39.3\\% and an inference speed of 27.7 FPS, effectively balancing accuracy and efficiency. In simulations on edge devices, the inference speed reaches 14.8 FPS, further demonstrating the method's applicability for real-time deployment in resource-constrained environments.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.23599.pdf",
    "abs_url": "https://arxiv.org/abs/2507.23599",
    "published": "2025-07-31T14:39:31Z",
    "updated": "2026-01-28T09:00:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出方向感知的2D卷积框架，通过引入高度分数投影，实现高效且几何保持的3D占用预测。",
      "motivation": "自动驾驶系统中，高效和高精度的3D占用预测至关重要。然而，现有方法在准确性和效率间存在显著权衡：一些方法虽然达到高精度，但推理速度慢；另一些采用纯鸟瞰图（BEV）的2D表示来加速，但这不可避免地牺牲了垂直几何线索，破坏了三维完整性，限制了实时部署和性能提升。因此，迫切需要一种能平衡准确性与效率的新方法。",
      "method": "论文提出一个纯2D框架用于3D占用预测。核心创新在于引入高度分数投影，与传统Lift-Splat-Shoot（LSS）方法仅依赖深度分数不同，此方法将垂直几何结构编码到特征中，以增强三维信息保留。进一步采用方向感知卷积，同时提取垂直和水平方向的几何特征，有效平衡计算复杂度与特征表达能力。框架在Occ3D-nuScenes数据集上实施，通过纯2D处理保持高效性。",
      "result": "在Occ3D-nuScenes数据集上，所提方法取得了39.3%的mIoU和27.7 FPS的推理速度，有效平衡了准确性与效率。与现有方法相比，它在保持较高精度的同时实现了快速推理，适用于实时应用。在边缘设备模拟测试中，推理速度达到14.8 FPS，进一步验证了其在资源受限环境中的部署潜力。",
      "conclusion": "本研究的主要贡献在于提出了一种高效且几何保持的3D占用预测方法，通过方向感知卷积和高度分数投影，克服了现有方法的局限性。这不仅提升了自动驾驶系统的性能潜力，还为实时部署提供了技术基础，具有实际应用价值。未来工作可进一步优化模型以应对复杂场景或扩展至其他三维感知任务。",
      "tags": [
        "3D Occupancy Prediction",
        "Direction-Aware Convolution",
        "Height-Score Projection",
        "Autonomous Driving",
        "Bird's-Eye-View (BEV)"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:36.587563Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.18988",
    "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction",
    "authors": [
      "Chao Wang",
      "Zijin Yang",
      "Yaofei Wang",
      "Weiming Zhang",
      "Kejiang Chen"
    ],
    "abstract": "The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models. To address these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel training-free attribution method designed for generative models with continuous autoencoders. Unlike existing reconstruction-based approaches that rely on the value of a single reconstruction loss, AEDR performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy, which inherently cancels out absolute biases caused by image complexity, with autoencoder-based reconstruction ensuring superior computational efficiency. Experiments on eight top latent diffusion models show that AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.",
    "categories": [
      "cs.CV",
      "cs.CR",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.18988.pdf",
    "abs_url": "https://arxiv.org/abs/2507.18988",
    "published": "2025-07-25T06:34:58Z",
    "updated": "2026-01-28T15:53:05Z",
    "comment": "7 pages. Accepted by AAAI 2026 Oral",
    "light_analysis": {
      "overview": "本论文提出了AEDR方法，通过自编码器双重重建和损失比值实现无需训练的AI生成图像归因，显著提升准确度和计算效率。",
      "motivation": "图像生成技术的快速发展使得任何人都能使用生成模型创建逼真图像，引发了严重的安全担忧。为应对恶意使用，追踪这类图像的来源至关重要。现有基于重建的归因方法虽具潜力，但在应用于最先进模型时，常面临准确度降低和计算成本高的问题，限制了在实际场景中的有效性。因此，开发一种更准确、更高效的归因方法成为迫切需求，以促进图像安全和信任评估。",
      "method": "本论文提出AEDR（自编码器双重重建）方法，专为连续自编码器的生成模型设计。核心思想是使用模型的autoencoder进行两次连续重建，并采用两个重建损失的比值作为归因信号，而非依赖单一重建损失值。这一创新通过比值抵消了图像复杂度引起的绝对偏差，信号进一步用图像同质性度量校准以提高准确度。Autoencoder的利用确保了无需额外训练的计算效率，因为重建过程直接依赖模型固有结构，简化了处理步骤。",
      "result": "实验在八个顶级潜在扩散模型上进行，结果显示AEDR在归因准确度上比现有基于重建的方法高出25.5%，同时计算时间仅需现有方法的1%。这证实了AEDR在准确性和效率方面的双重优势，提供了具体的数据支撑，如准确率提升和计算时间大幅减少，使其成为处理复杂生成图像归因任务的有效工具。",
      "conclusion": "本论文的主要贡献是提出了AEDR方法，通过自编码器双重重建实现无需训练的AI生成图像归因，提高了准确度和计算效率。这为图像安全和来源追踪提供了实用的解决方案，具有重要的学术价值（如推动归因技术发展）和实际应用价值（如版权保护和安全监测）。未来工作可扩展至其他生成架构或探索更多校准指标，以应对潜在局限性。",
      "tags": [
        "Autoencoder",
        "Double-Reconstruction",
        "Image Attribution",
        "Training-Free",
        "Latent Diffusion Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:40.725725Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.13332",
    "title": "The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner",
    "authors": [
      "Zhouqi Hua",
      "Wenwei Zhang",
      "Chengqi Lyu",
      "Yuzhe Gu",
      "Songyang Gao",
      "Kuikun Liu",
      "Dahua Lin",
      "Kai Chen"
    ],
    "abstract": "Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2507.13332.pdf",
    "abs_url": "https://arxiv.org/abs/2507.13332",
    "published": "2025-07-17T17:50:07Z",
    "updated": "2026-01-28T03:05:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 Turing Machine Imitation Learning (TAIL) 方法，通过模仿图灵机执行过程合成推理数据，显著提升大型语言模型的长度泛化能力。",
      "motivation": "长度泛化是 Transformer 大型语言模型的核心挑战，现有数据驱动方法多专注于算术和符号操作任务，但这些方法任务特定且整体性能有限，缺乏通用性。为了解决更广泛的可计算推理问题（即可由算法解决、图灵机处理的问题），本文旨在从图灵机角度出发，开发一种更通用的解决方案，以克服当前方法在长序列问题上的不足，推动模型在未见任务上的泛化表现。",
      "method": "论文提出 Turing Machine Imitation Learning (TAIL)，通过计算机程序合成 chain-of-thoughts 数据来模仿图灵机的执行过程。关键创新包括线性扩展推理步骤为原子状态，以缓解 shortcut learning，以及引入显式内存获取机制，减少基本操作中动态和长范围数据访问的困难。为验证方法，构建了一个合成数据集，覆盖 8 类算法和 18 个任务，确保多样化和挑战性，为模型训练提供基础。",
      "result": "实验表明，TAIL 仅使用合成数据显著提高了 Qwen2.5-7B 模型在各种任务上的长度泛化能力和性能，超越先前方法及 DeepSeek-R1。摘要未提供具体数据，但强调改进是显著的。深入分析显示，图灵机的关键概念（如读写行为）对长度泛化至关重要，模型注意力层中展现出与图灵机属性一致的机制，验证了方法的可靠性和通用性。",
      "conclusion": "本研究的主要贡献是 TAIL 方法，它通过模仿图灵机过程增强了大型语言模型的长度泛化能力，为从合成数据学习推理提供了新方向。学术价值在于将计算理论概念融入机器学习，实际应用上可提升模型在复杂任务中的泛化性能。未来工作可能扩展至更多算法类型或结合其他技术，以进一步提高鲁棒性和实用性。",
      "tags": [
        "Length Generalization",
        "Turing Machine",
        "Imitation Learning",
        "Chain-of-Thoughts",
        "Synthetic Data"
      ]
    },
    "analyzed_at": "2026-01-29T03:50:55.611033Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.09071",
    "title": "BlindSight: Harnessing Sparsity for Efficient Vision-Language Models",
    "authors": [
      "Tharun Adithya Srikrishnan",
      "Deval Shah",
      "Timothy Hein",
      "Ahmed Hasssan",
      "Stephen Youn",
      "Steven K. Reinhardt"
    ],
    "abstract": "Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.09071.pdf",
    "abs_url": "https://arxiv.org/abs/2507.09071",
    "published": "2025-07-11T23:15:30Z",
    "updated": "2026-01-28T18:45:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出BlindSight方法，利用注意力稀疏性优化视觉-语言模型的多图像推理，实现高效加速且精度损失小。",
      "motivation": "视觉-语言模型能够联合处理文本和图像，但视觉数据的加入导致提示长度剧增，延长了首个令牌生成时间，严重影响了推理效率，尤其是在多图像任务中。现有方法可能未充分利用注意力计算中的固有稀疏性，造成计算资源浪费，因此研究如何优化这种瓶颈对提升模型实用性至关重要。",
      "method": "BlindSight方法基于对VLM注意力模式的分析，发现在大量层中不存在跨图像注意力。设计输入模板感知的稀疏掩码，将注意力头分类为Dense、Sink、Intra-Image和Intra-Image+Sink四种类型，以消除冗余计算。使用数据集进行头分类，并开发基于Triton的GPU内核来高效执行稀疏注意力，无额外运行时开销，适用于多图像VLM推理。",
      "result": "实验显示，BlindSight在注意力计算上实现1.8至3.2倍加速，对应提示长度36K到300K。在多图像理解基准测试中，平均准确率仅下降0.78%，在多个VLM模型（如Qwen2-VL、Qwen2.5-VL、Gemma 3）上均有效，表明方法在保持精度的同时显著提升效率，具有良好泛化性。",
      "conclusion": "BlindSight通过利用注意力稀疏性优化了视觉-语言模型的多图像推理效率，具有重要的学术和实际价值，推动了高效VLM设计的研究。它倡导结合稀疏和密集层的架构，为未来模型优化提供了方向，有助于在多模态应用中降低计算成本，促进技术发展。",
      "tags": [
        "Vision-Language Models",
        "Attention Sparsity",
        "Triton GPU Kernel",
        "Multi-Image Inference",
        "Efficient Computation"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:37.628540Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.17697",
    "title": "Beyond Syntax: Action Semantics Learning for App Agents",
    "authors": [
      "Bohan Tang",
      "Dezhao Luo",
      "Jianheng Liu",
      "Jingxuan Chen",
      "Shaogang Gong",
      "Jianye Hao",
      "Jun Wang",
      "Kun Shao"
    ],
    "abstract": "The recent development of Large Language Models (LLMs) enables the rise of App agents that interpret user intent and operate smartphone Apps through actions such as clicking and scrolling. While prompt-based solutions with proprietary LLM APIs show promising ability, they incur heavy compute costs and external API dependency. Fine-tuning smaller open-source LLMs solves these limitations. However, current supervised fine-tuning methods use a syntax learning paradigm that forces agents to reproduce exactly the ground truth action strings, leading to out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action Semantics Learning (ASL), a novel learning framework, where the learning objective is capturing the semantics of the ground truth actions. Specifically, inspired by the programming language theory, we define the action semantics for App agents as the state transition induced by the action in the user interface. Building on this insight, ASL employs a novel SEmantic Estimator~(SEE) to compute a semantic similarity to train the App agents in generating actions aligned with the semantics of ground truth actions, even when their syntactic forms differ. SEE is a flexible module that can be applied in both supervised and reinforcement fine-tuning paradigms. To support the effectiveness of ASL, we theoretically demonstrate the superior robustness of ASL for the OOD problem compared with the existing syntax learning paradigm. Extensive experiments across multiple offline and online benchmarks demonstrate that ASL significantly improves the accuracy and generalisation of App agents compared to existing methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.17697.pdf",
    "abs_url": "https://arxiv.org/abs/2506.17697",
    "published": "2025-06-21T12:08:19Z",
    "updated": "2026-01-28T11:13:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出动作语义学习框架，通过捕捉动作语义而非语法来增强App代理的鲁棒性和泛化能力。",
      "motivation": "当前基于提示的大语言模型解决方案虽有效，但计算成本高且依赖外部API；监督微调方法采用语法学习，迫使代理精确复制地面真实动作字符串，导致分布外（OOD）脆弱性，限制了App代理在实际应用中的性能。本研究旨在解决这些问题，通过引入语义学习来克服现有方法的不足，提高代理在多变环境中的适应能力，这对于开发高效、鲁棒的智能应用操作代理至关重要。",
      "method": "论文提出动作语义学习（ASL）框架，核心创新是定义动作语义为动作在用户界面中引起的状态转换，受编程语言理论启发。该方法使用语义评估器（SEE）计算语义相似度，训练App代理生成与地面真实动作语义一致的动作，即使语法形式不同。SEE是灵活模块，可应用于监督微调和强化微调范式，提高了方法的适应性和鲁棒性，无需依赖特定语法结构。",
      "result": "通过理论分析，论文证明了ASL在处理OOD问题上比现有语法学习范式更具鲁棒性。在多个离线和在线的基准测试中，广泛实验显示ASL显著提高了App代理的准确性和泛化能力，相比基线方法有显著改进。具体表现为更好的性能指标，如在未知场景下的适应能力提升，但摘要未提供具体数据如准确率数值。",
      "conclusion": "本研究的主要贡献是提出了动作语义学习框架，通过聚焦于动作语义而非语法，有效解决了语法学习导致的OOD问题。其学术价值在于将语义学习引入App代理领域，推动了智能代理理论的发展；实际应用价值在于提高了代理的效率和泛化能力，为实际部署提供了可能。未来工作可能包括扩展SEE模块到更多应用场景或优化其计算效率。",
      "tags": [
        "Action Semantics Learning",
        "State Transition",
        "Semantic Similarity",
        "App Agents",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:14.530673Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11743",
    "title": "Taxonomy of reduction matrices for Graph Coarsening",
    "authors": [
      "Antonin Joly",
      "Nicolas Keriven",
      "Aline Roumy"
    ],
    "abstract": "Graph coarsening aims to diminish the size of a graph to lighten its memory footprint, and has numerous applications in graph signal processing and machine learning. It is usually defined using a reduction matrix and a lifting matrix, which, respectively, allows to project a graph signal from the original graph to the coarsened one and back. This results in a loss of information measured by the so-called Restricted Spectral Approximation (RSA). Most coarsening frameworks impose a fixed relationship between the reduction and lifting matrices, generally as pseudo-inverses of each other, and seek to define a coarsening that minimizes the RSA. In this paper, we remark that the roles of these two matrices are not entirely symmetric: indeed, putting constraints on the lifting matrix alone ensures the existence of important objects such as the coarsened graph's adjacency matrix or Laplacian. In light of this, in this paper, we introduce a more general notion of reduction matrix, that is not necessarily the pseudo-inverse of the lifting matrix. We establish a taxonomy of ``admissible'' families of reduction matrices, discuss the different properties that they must satisfy and whether they admit a closed-form description or not. We show that, for a fixed coarsening represented by a fixed lifting matrix, the RSA can be further reduced simply by modifying the reduction matrix. We explore different examples, including some based on a constrained optimization process of the RSA. Since this criterion has also been linked to the performance of Graph Neural Networks, we also illustrate the impact of this choices on different node classification tasks on coarsened graphs.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.11743.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11743",
    "published": "2025-06-13T12:55:03Z",
    "updated": "2026-01-28T13:46:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种新的图粗化缩减矩阵分类法，打破了传统伪逆约束，以进一步减少信息损失并优化图神经网络性能。",
      "motivation": "图粗化旨在减小图规模以节省内存，广泛应用于图信号处理和机器学习。现有方法通常强制缩减矩阵和提升矩阵互为伪逆以最小化信息损失（RSA），但忽略了矩阵角色的非对称性，限制了优化灵活性。本文发现仅约束提升矩阵即可保证粗化图数学对象（如邻接矩阵）存在，从而提出更灵活缩减矩阵定义，以提升信息保留潜力。",
      "method": "论文引入不依赖于提升矩阵伪逆的更一般缩减矩阵概念，建立了“可容许”矩阵家族分类法，探讨其必须满足的数学属性（如确保粗化图结构存在）和闭式描述可能性。通过修改缩减矩阵，可以进一步减少固定粗化下的RSA，举例包括基于RSA约束优化过程的方法。研究还分析了这些选择对图神经网络节点分类任务的影响，但摘要未明确说明具体数据集或模型架构。",
      "result": "研究表明，通过灵活选择缩减矩阵，RSA可被进一步最小化，超越了传统固定关系方法的限制。论文探索了不同例子，表明优化缩减矩阵能改善信息保留，并对图神经网络节点分类任务有积极影响，但摘要未提供具体性能指标（如准确率提升百分比）或基线对比数据。",
      "conclusion": "本研究的主要贡献是提出了更一般的缩减矩阵分类法，突破了传统图粗化中矩阵关系的约束，为优化信息损失提供了新途径。这具有重要理论价值，能促进图信号处理和机器学习应用的发展，未来工作可进一步探索分类法在实际任务中的具体应用和扩展，摘要未明确说明局限性。",
      "tags": [
        "Graph Coarsening",
        "Reduction Matrix",
        "Lifting Matrix",
        "Restricted Spectral Approximation (RSA)",
        "Graph Neural Networks (GNN)"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:46.106491Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11558",
    "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs",
    "authors": [
      "Bo-Cheng Chiu",
      "Jen-Jee Chen",
      "Yu-Chee Tseng",
      "Feng-Chi Chen",
      "An-Zi Yen"
    ],
    "abstract": "Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.11558.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11558",
    "published": "2025-06-13T08:13:05Z",
    "updated": "2026-01-28T17:24:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "DaMO是一个数据高效的多模态视频大型语言模型，通过创新架构和渐进训练范式，显著提升时间推理准确性。",
      "motivation": "大型语言模型已扩展至视频领域，但现有视频LLMs在细粒度时间推理方面存在局限，特别是在监督数据有限时难以精确将响应归因于特定视频时刻。这限制了它们在需要精确时间对齐的任务中的应用，如视频问答和动作识别，因此亟需开发数据高效的解决方案来改进时间推理能力，提升多模态理解水平。",
      "method": "DaMO的核心是Temporal-aware Fuseformer，它采用分层双流架构，分别处理视觉和音频模态，逐步捕捉模态内的时间动态并有效融合互补信息。为提升计算效率，模型集成全局残差以减少空间冗余，同时保留关键语义细节。训练采用四阶段渐进范式，从多模态对齐开始，逐步增强语义基础和时间推理能力，并利用LLM生成的时间基础QA对数据集进行监督学习。",
      "result": "在时间基础任务和视频问答基准测试中，DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. 摘要未明确说明具体性能指标如准确率提升，但实验表明模型在多种评估中优于现有方法，尤其在需要精确时间对齐的复杂任务中表现突出。",
      "conclusion": "DaMO通过创新架构和训练方法，显著提升了视频LLMs的时间推理能力，为数据高效的多模态视频语言建模确立了有前景的方向。研究贡献了增强的数据集，支持时间监督任务，具有学术价值和应用潜力，未来工作可能涉及进一步优化模型效率或扩展到更多模态和场景。",
      "tags": [
        "Video Large Language Model",
        "Temporal Reasoning",
        "Multimodal Fusion",
        "Hierarchical Dual-Stream Architecture",
        "Progressive Training Paradigm"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:41.200919Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11300",
    "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning",
    "authors": [
      "Yang Zhang",
      "Amr Mohamed",
      "Hadi Abdine",
      "Guokan Shang",
      "Michalis Vazirgiannis"
    ],
    "abstract": "Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.11300.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11300",
    "published": "2025-06-12T21:06:57Z",
    "updated": "2026-01-28T17:19:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文首次系统研究了课程学习在大型语言模型预训练中的应用，通过从易到难的数据排序显著提升训练效率。",
      "motivation": "课程学习通过按难度排序训练数据来优化学习过程，已在机器学习领域广泛验证能提高训练效率和性能。然而，在大型语言模型（LLM）的预训练中，现有方法通常依赖随机数据采样，这可能忽略数据的结构性，导致收敛缓慢和资源效率低下。本文旨在弥补这一研究空白，系统探讨课程学习在LLM预训练中的应用，以解决数据排序不当导致的效率问题，推动更可持续的AI训练方法。",
      "method": "本研究采用系统化的课程学习框架，包含三种核心策略：基础课程学习、基于进度的采样和交错课程。难度度量基于六种语言学和信息论属性，如压缩比、词汇多样性（MTLD）和可读性（Flesch Reading Ease）。通过训练超过200个模型，使用高达100B tokens的数据集，在三种实际场景（有限数据、无限数据、持续训练）下进行验证，以评估方法在不同条件下的鲁棒性和有效性。",
      "result": "实验结果表明，课程学习在训练早期和中期能显著加速收敛，将所需训练步骤减少18-45%以达到与基线相同的性能。当用作标准随机采样前的预热策略时，它带来持续的性能提升，最高达3.5%。研究识别出压缩比、词汇多样性和可读性作为最有效的难度信号，这些结果在八个基准测试中得到证实，展示了课程学习在提高LLM预训练效率方面的潜力。",
      "conclusion": "本论文的主要贡献是首次系统性证明课程学习在LLM预训练中的实用价值，通过数据排序提供了一种与现有数据选择方法正交的机制，能显著提升训练效率。这为高效语言模型开发提供了新方向，具有重要的学术和应用意义。摘要未明确说明局限性，但未来工作可探索更多难度度量、扩展到其他训练场景或结合现有优化技术。",
      "tags": [
        "Curriculum Learning",
        "Language Model Pretraining",
        "Difficulty Metrics",
        "Data Sampling",
        "Efficiency Improvement"
      ]
    },
    "analyzed_at": "2026-01-29T03:51:57.078442Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.10912",
    "title": "Breaking Bad Molecules: Are MLLMs Ready for Structure-Level Molecular Detoxification?",
    "authors": [
      "Fei Lin",
      "Ziyang Gong",
      "Cong Wang",
      "Tengchao Zhang",
      "Yonglin Tian",
      "Yining Jiang",
      "Ji Dai",
      "Chao Guo",
      "Xiaotong Yu",
      "Xue Yang",
      "Gen Luo",
      "Fei-Yue Wang"
    ],
    "abstract": "Toxicity remains a leading cause of early-stage drug development failure. Despite advances in molecular design and property prediction, the task of molecular toxicity repair, generating structurally valid molecular alternatives with reduced toxicity, has not yet been systematically defined or benchmarked. To fill this gap, we introduce ToxiMol, the first benchmark task for general-purpose Multimodal Large Language Models (MLLMs) focused on molecular toxicity repair. We construct a standardized dataset covering 11 primary tasks and 660 representative toxic molecules spanning diverse mechanisms and granularities. We design a prompt annotation pipeline with mechanism-aware and task-adaptive capabilities, informed by expert toxicological knowledge. In parallel, we propose an automated evaluation framework, ToxiEval, which integrates toxicity endpoint prediction, synthetic accessibility, drug-likeness, and structural similarity into a high-throughput evaluation chain for repair success. We systematically assess 43 mainstream general-purpose MLLMs and conduct multiple ablation studies to analyze key issues, including evaluation metrics, candidate diversity, and failure attribution. Experimental results show that although current MLLMs still face significant challenges on this task, they begin to demonstrate promising capabilities in toxicity understanding, semantic constraint adherence, and structure-aware editing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.10912.pdf",
    "abs_url": "https://arxiv.org/abs/2506.10912",
    "published": "2025-06-12T17:25:53Z",
    "updated": "2026-01-28T17:20:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了ToxiMol，首个针对多模态大语言模型的分子毒性修复基准任务，填补了该领域的空白。",
      "motivation": "毒性是药物早期开发失败的主要因素之一，尽管分子设计和毒性预测技术有进展，但生成结构有效、毒性降低的分子替代方案这一修复任务，尚未被系统定义或建立基准。这使得评估和改进相关模型的能力变得困难，阻碍了AI在药物发现中的应用。因此，本研究旨在填补这一空白，为多模态大语言模型提供一个聚焦于分子毒性修复的标准化测试环境。",
      "method": "论文提出了ToxiMol基准任务，首先构建了一个标准化数据集，覆盖11个主要任务和660个代表分子，涵盖多种毒性机制和粒度。设计了一个基于专家知识的提示注释流程，具备机制感知和任务适应能力，以提高模型输入的质量。同时，开发了ToxiEval自动评估框架，集成毒性端点预测、合成可访问性、药物相似性和结构相似性指标，形成一个全面的评估链，用于高效评估修复成功性。",
      "result": "通过评估43个主流多模态大语言模型，实验结果表明当前模型在分子毒性修复任务上仍面临显著挑战。然而，这些模型开始展现出在毒性理解、语义约束遵守和结构感知编辑方面的潜力。消融研究进一步分析了评估指标的重要性、候选分子的多样性以及失败案例的归因，揭示了关键影响因素，为未来模型改进提供了指导。",
      "conclusion": "本研究的核心贡献是提出并定义了ToxiMol，首个针对多模态大语言模型的分子毒性修复基准任务，并设计了配套的ToxiEval评估框架。这为分子毒性修复领域的研究提供了标准化平台，具有重要的学术和实际价值，可能推动AI在药物发现中的应用。尽管当前模型性能有限，但展示了MLLMs在结构级编辑任务中的潜力，未来工作可聚焦于提升模型能力和扩展基准范围。",
      "tags": [
        "Multimodal Large Language Models",
        "Molecular Toxicity Repair",
        "Benchmark Task",
        "Toxicity Prediction",
        "Structure-Aware Editing"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:59.346365Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.09630",
    "title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation",
    "authors": [
      "Pol G. Recasens",
      "Alberto Gutierrez",
      "Jordi Torres",
      "Josep. Ll Berral",
      "Javier Carnerero-Cano",
      "Anisa Halimi",
      "Kieran Fraser"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used for synthetic tabular data generation through in-context learning (ICL), offering a practical solution for data augmentation in data scarce scenarios. While prior work has shown the potential of LLMs to improve downstream task performance through augmenting underrepresented groups, these benefits often assume access to a subset of unbiased in-context examples, representative of the real dataset. In real-world settings, however, data is frequently noisy and demographically skewed. In this paper, we systematically study how statistical biases within in-context examples propagate to the distribution of synthetic tabular data, showing that even mild in-context biases lead to global statistical distortions. We further introduce an adversarial scenario where a malicious contributor can inject bias into the synthetic dataset via a subset of in-context examples, ultimately compromising the fairness of downstream classifiers for a targeted and protected subgroup. Finally, we evaluate mitigation strategies based on preprocessing in-context examples, demonstrating that while such interventions can attenuate disparity, the inherent sensitivity of LLMs to adversarial prompts remains a persistent challenge. Our findings highlight a critical new vulnerability in LLM-based data generation pipelines within sensitive domains.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.09630.pdf",
    "abs_url": "https://arxiv.org/abs/2506.09630",
    "published": "2025-06-11T11:39:29Z",
    "updated": "2026-01-28T18:25:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示了大型语言模型在上下文学习生成表格数据时，上下文偏差传播导致公平性风险的新漏洞。",
      "motivation": "在数据稀缺场景中，大型语言模型通过上下文学习生成合成表格数据以进行数据增强，但现有研究假设访问无偏的上下文示例，而现实数据常含统计偏差和人口统计学偏斜。这可能导致合成数据继承和放大偏差，影响下游任务的公平性，特别是在敏感领域如医疗和金融中。因此，系统研究偏差传播机制以解决现实世界数据质量问题至关重要，而现有方法忽略了这一实际挑战。",
      "method": "研究采用系统分析方法，首先评估上下文示例中的统计偏差如何传播到LLM生成的表格数据分布中，量化偏差影响。其次，引入一个对抗性场景，模拟恶意贡献者通过特定上下文示例注入偏差，以测试LLMs对对抗性提示的敏感性。然后，评估基于预处理上下文示例的缓解策略，如过滤或修改偏差示例。摘要未明确说明具体使用的LLM模型架构、数据集细节和实验设置，但涉及了偏差度量和预处理技术的探讨。",
      "result": "实验结果表明，即使上下文示例中包含轻微偏差，也会导致LLM生成的合成表格数据出现全局统计扭曲。在对抗性场景下，恶意注入偏差有效损害了下游分类器对受保护子组的公平性。评估的缓解策略如预处理上下文示例可以部分减少差异，但LLMs对对抗性提示的内在敏感性仍是一个持续挑战。摘要未提供具体的性能指标数据如准确率提升，但强调了偏差传播和缓解策略的有限效果。",
      "conclusion": "论文的主要贡献是系统揭示了基于LLM的表格数据生成中上下文偏差传播的新漏洞，强调了在敏感领域确保数据公平性的重要性。研究具有学术价值，推动了AI公平性和对抗性鲁棒性的理解，并在实际应用中警示数据生成管道的安全风险。局限性在于缓解策略未能完全解决LLMs的敏感性，未来工作需要开发更鲁棒的模型设计或数据预处理技术来应对这一挑战。",
      "tags": [
        "Large Language Models",
        "In-Context Learning",
        "Tabular Data Generation",
        "Bias Propagation",
        "Adversarial Prompts"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:22.899092Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11136",
    "title": "JAFAR: Jack up Any Feature at Any Resolution",
    "authors": [
      "Paul Couairon",
      "Loick Chambon",
      "Louis Serrano",
      "Jean-Emmanuel Haugeard",
      "Matthieu Cord",
      "Nicolas Thome"
    ],
    "abstract": "Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.11136.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11136",
    "published": "2025-06-10T20:53:12Z",
    "updated": "2026-01-28T17:39:25Z",
    "comment": "Code available at https://github.com/PaulCouairon/JAFAR",
    "light_analysis": {
      "overview": "JAFAR提出了一种轻量级、灵活的特征上采样器，通过注意力机制和空间特征变换调制，实现任意基础视觉编码器特征的任意分辨率提升，无需高分辨率监督。",
      "motivation": "基础视觉编码器在密集视觉任务中广泛应用，但其输出的空间特征分辨率较低，无法直接满足下游任务如目标检测、语义分割等对高分辨率模态的需求。现有特征上采样方法可能缺乏灵活性，难以适配不同编码器或处理任意目标分辨率，导致性能受限。因此，开发一种通用且高效的上采样器至关重要，以提升视觉特征的细粒度细节恢复能力，促进多样化任务的性能改进。",
      "method": "JAFAR采用基于注意力的模块，通过空间特征变换（SFT）调制促进高分辨率查询与低分辨率键之间的语义对齐。高分辨率查询源自低层图像特征，而低分辨率键经过SFT增强语义信息，实现轻量级且灵活的特征上采样。该方法支持将任何基础视觉编码器的视觉特征上采样到任意目标分辨率，并在低上采样比率和分辨率下训练，无需高分辨率监督，却能泛化到更高的输出尺度。",
      "result": "大量实验表明，JAFAR能有效恢复图像中的细粒度空间细节，并在多种下游任务中一致优于现有特征上采样方法。实验覆盖了多样化的任务集，但摘要未明确提供具体性能指标如准确率提升数据。结果显示，该方法在泛化能力和性能上表现突出，证明了其在实际应用中的有效性。",
      "conclusion": "本研究的核心贡献是JAFAR特征上采样器，它通过注意力机制和SFT调制技术，在不依赖高分辨率监督的情况下实现了优异的泛化性能。学术上，该方法推动了特征上采样技术的发展，为密集视觉任务提供了高效解决方案；实际应用中，JAFAR可适配各种基础视觉编码器和任务，提升整体性能。未来工作可进一步探索其扩展到更复杂场景的潜力和局限性。",
      "tags": [
        "Feature Upsampling",
        "Attention Mechanism",
        "Spatial Feature Transform",
        "Foundation Vision Encoder",
        "Dense Vision Tasks"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:21.632554Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.08477",
    "title": "Read as You See: Guiding Unimodal LLMs for Low-Resource Explainable Harmful Meme Detection",
    "authors": [
      "Fengjun Pan",
      "Xiaobao Wu",
      "Tho Quan",
      "Anh Tuan Luu"
    ],
    "abstract": "Detecting harmful memes is crucial for safeguarding the integrity and harmony of online environments, yet existing detection methods are often resource-intensive, inflexible, and lacking explainability, limiting their applicability in assisting real-world web content moderation. We propose U-CoT+, a resource-efficient framework that prioritizes accessibility, flexibility and transparency in harmful meme detection by fully harnessing the capabilities of lightweight unimodal large language models (LLMs). Instead of directly prompting or fine-tuning large multimodal models (LMMs) as black-box classifiers, we avoid immediate reasoning over complex visual inputs but decouple meme content recognition from meme harmfulness analysis through a high-fidelity meme-to-text pipeline, which collaborates lightweight LMMs and LLMs to convert multimodal memes into natural language descriptions that preserve critical visual information, thus enabling text-only LLMs to \"see\" memes by \"reading\". Grounded in textual inputs, we further guide unimodal LLMs' reasoning under zero-shot Chain-of-Thoughts (CoT) prompting with targeted, interpretable, context-aware, and easily obtained human-crafted guidelines, thus providing accountable step-by-step rationales, while enabling flexible and efficient adaptation to diverse sociocultural criteria of harmfulness. Extensive experiments on seven benchmark datasets show that U-CoT+ achieves performance comparable to resource-intensive baselines, highlighting its effectiveness and potential as a scalable, explainable, and low-resource solution to support harmful meme detection.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.08477.pdf",
    "abs_url": "https://arxiv.org/abs/2506.08477",
    "published": "2025-06-10T06:10:45Z",
    "updated": "2026-01-28T08:42:59Z",
    "comment": "Accepted to ACM Web Conference 2026 (WWW '26)",
    "light_analysis": {
      "overview": "U-CoT+通过将多模态梗图转换为文本描述并利用零样本思维链提示，实现了低资源、可解释的有害梗图检测。",
      "motivation": "该研究旨在解决有害梗图检测中资源密集和缺乏解释性的问题。现有方法通常依赖大型多模态模型，成本高、灵活性差且难以提供透明推理，限制了在低资源场景下的实际应用，如在线内容审核。因此，开发一个高效、灵活且可解释的框架对维护网络环境安全至关重要。",
      "method": "U-CoT+框架的核心创新在于通过高保真meme-to-text管道，将多模态梗图转换为自然语言描述，使文本型大语言模型能够处理视觉信息。该方法结合轻量级多模态模型和语言模型，避免直接使用资源密集型多模态模型。接着，采用零样本思维链提示，基于人工制定的上下文感知指导原则，引导模型进行逐步推理，生成可解释的检测理由。",
      "result": "在七个基准数据集上的广泛实验显示，U-CoT+在有害梗图检测任务中取得了与资源密集型基线方法相当的性能，证明了其在不牺牲效果的前提下实现低资源消耗和高解释性的能力，突出了其作为可扩展解决方案的潜力。",
      "conclusion": "U-CoT+的主要贡献是提供了一个资源高效、灵活且透明的有害梗图检测框架，通过融合单模态LLM和多模态处理技术，拓展了LLM在多模态任务中的应用。其学术价值在于探索了低资源多模态理解的新方法，实际应用价值在于支持内容安全。未来工作可能涉及优化meme-to-text管道的精度或扩展至其他领域。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thoughts",
        "Zero-Shot Learning",
        "Meme Detection",
        "Text Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:50.081166Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.07972",
    "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization",
    "authors": [
      "Hongzheng Chen",
      "Yingheng Wang",
      "Yaohui Cai",
      "Hins Hu",
      "Jiajie Li",
      "Shirley Huang",
      "Chenhui Deng",
      "Rongjian Liang",
      "Shufeng Kong",
      "Haoxing Ren",
      "Samitha Samaranayake",
      "Carla P. Gomes",
      "Zhiru Zhang"
    ],
    "abstract": "While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.07972.pdf",
    "abs_url": "https://arxiv.org/abs/2506.07972",
    "published": "2025-06-09T17:46:47Z",
    "updated": "2026-01-28T18:52:54Z",
    "comment": "Accepted to ICLR'26",
    "light_analysis": {
      "overview": "本文提出了HeuriGym，一个代理基准框架，用于评估大型语言模型在组合优化问题中生成的启发式算法。",
      "motivation": "尽管大型语言模型在推理和基于代理的问题解决方面取得显著进展，但当前评估方法不足。现有基准要么依赖易于饱和和记忆的封闭式问题，要么是主观比较，缺乏一致性和严谨性。在科学和工程领域，这种缺陷限制了LLMs在复杂问题解决中的真实能力评估。因此，本研究旨在填补这一空白，提供一个专门评估启发式算法的框架，以促进更有效的评估方法发展。",
      "method": "本研究引入了HeuriGym，一个代理框架，专为评估LLM生成的启发式算法设计。该框架允许LLMs提出启发式策略，通过代码执行接收反馈，并进行迭代优化，以改进解决方案。关键创新点包括结合代码执行和迭代反馈机制，以及提出Quality-Yield Index（QYI）量化性能，综合了通过率和解决方案质量。实验在九个领域问题（如计算机系统、物流和生物学）上进行，评估了九个先进的LLM模型，如GPT-o4-mini-high和Gemini-2.5-Pro。",
      "result": "在HeuriGym基准上的评估结果显示，LLMs在工具使用、规划和自适应推理方面存在持续局限性。通过QYI指标量化，顶级模型如GPT-o4-mini-high和Gemini-2.5-Pro的QYI分数仅为0.6，远低于专家基线1.0。与基线方法对比，这突出了LLMs在处理组合优化问题时性能的不足，表明即使先进模型也有显著改进空间，支持了框架的有效性。",
      "conclusion": "本研究的核心贡献是提出了HeuriGym开源基准框架，评估LLM生成启发式算法的能力，并揭示其在复杂问题中的局限性。学术价值在于推动了LLM评估方法的发展，提供客观指标如QYI。实际应用价值在于引导LLMs向更有效的科学和工程问题解决方向发展。潜在局限性或未来工作包括改进模型性能，并扩展基准到更多问题领域，以增强实用性。",
      "tags": [
        "Large Language Models",
        "Combinatorial Optimization",
        "Heuristic Algorithms",
        "Agentic Framework",
        "Quality-Yield Index"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:48.642692Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.06052",
    "title": "DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems",
    "authors": [
      "Kostis Michailidis",
      "Dimos Tsouros",
      "Tias Guns"
    ],
    "abstract": "Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.06052.pdf",
    "abs_url": "https://arxiv.org/abs/2506.06052",
    "published": "2025-06-06T12:56:02Z",
    "updated": "2026-01-28T18:58:23Z",
    "comment": "This version is currently submitted and it is under review. For CP-Bench (the paper accepted at ECAI25), please refer to the previous version of this entry (v2)",
    "light_analysis": {
      "overview": "本研究提出了DCP-Bench-Open基准，以评估大语言模型在离散组合问题约束建模中的表现。",
      "motivation": "离散组合问题在工业决策和优化中普遍存在，但约束建模过程需要专业知识，成为自动化应用的瓶颈。近年来，研究尝试利用大语言模型来自动化约束建模，但现有评估数据集多为小规模、同质化或特定领域，无法捕捉真实世界的多样性，限制了LLM性能的准确评估和改进。因此，需要开发一个更全面的基准来解决这一不足，推动LLM在约束求解中的实际应用。",
      "method": "论文的核心方法是引入DCP-Bench-Open，一个新颖的基准数据集，它收集了来自约束编程和运筹学社区的多种离散组合问题，专门设计用于评估LLM驱动的约束建模。通过比较三种不同抽象级别和语法的约束建模系统，如高级Python框架，并系统评估基于提示和推理时计算方法在不同LLMs上的应用，全面测试了模型的性能。这些技术路线包括使用提示工程优化和动态计算策略，以提升建模的准确性和效率。",
      "result": "实验结果显示，在使用高级Python框架进行约束建模时，大语言模型表现出更高的性能。通过应用基于提示和推理时计算方法，进一步提高了准确率，在DCP-Bench-Open这个高度挑战性基准上达到了91%。摘要未明确说明与基线方法的详细对比，但结果证实了LLM在多样化问题上的有效建模能力，显示了该方法在提升自动化建模精度方面的优势。",
      "conclusion": "本研究的核心贡献是提供了公开可用的DCP-Bench-Open基准，解决了现有评估数据集不足的问题，为评估和改进LLM在约束建模中的能力奠定了基础。学术上，它推动了LLM在离散组合问题领域的应用研究；实际中，有助于降低对专业知识的依赖，促进工业优化决策的自动化。局限性方面，摘要未明确说明，未来工作可能涉及基准的扩展和更多模型的测试。",
      "tags": [
        "Large Language Models",
        "Constraint Modelling",
        "Benchmark Evaluation",
        "Discrete Combinatorial Problems",
        "Prompt-based Methods"
      ]
    },
    "analyzed_at": "2026-01-29T03:52:51.849695Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.05301",
    "title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training",
    "authors": [
      "Jianyi Wang",
      "Shanchuan Lin",
      "Zhijie Lin",
      "Yuxi Ren",
      "Meng Wei",
      "Zongsheng Yue",
      "Shangchen Zhou",
      "Hao Chen",
      "Yang Zhao",
      "Ceyuan Yang",
      "Xuefeng Xiao",
      "Chen Change Loy",
      "Lu Jiang"
    ],
    "abstract": "Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.05301.pdf",
    "abs_url": "https://arxiv.org/abs/2506.05301",
    "published": "2025-06-05T17:51:05Z",
    "updated": "2026-01-28T05:55:42Z",
    "comment": "Camera Ready of ICLR2026. Project page: https://iceclear.github.io/projects/seedvr2/",
    "light_analysis": {
      "overview": "本文提出了SeedVR2模型，通过对抗性训练和自适应窗口注意力机制实现了一步高分辨率视频恢复。",
      "motivation": "扩散模型在视频恢复中虽能提升视觉质量，但推理计算成本极高，限制了实时应用。现有基于蒸馏的一步图像恢复方法难以扩展到视频恢复领域，尤其是在处理高分辨率真实世界视频时，这一问题尚未充分探索。因此，开发高效的单步视频恢复模型成为重要研究方向，以平衡性能与效率，解决现有方法在扩展性和处理能力上的不足。",
      "method": "SeedVR2模型结合扩散模型和对抗性训练，实现一步视频恢复。核心创新包括自适应窗口注意力机制，通过动态调整窗口大小适应输出分辨率，避免预定义窗口在高分辨率下的不一致性。此外，研究验证了多种损失函数，如提出的特征匹配损失，以稳定对抗性训练而不显著牺牲效率，优化模型架构和训练流程来应对高分辨率视频挑战。",
      "result": "广泛实验表明，SeedVR2在单步视频恢复中可以达到与现有方法相当或更好的性能。虽然摘要未明确说明具体性能指标如准确率提升或效率改进数值，但模型通过优化显著降低了计算成本，与基线方法对比显示出竞争力，证明了其在高效视频恢复中的有效性。",
      "conclusion": "本研究的主要贡献是开发了SeedVR2，一个一步扩散视频恢复模型，通过自适应窗口注意力和对抗性训练有效解决高分辨率视频恢复问题。该研究不仅提高了视频恢复的效率，为扩散模型在实时应用中提供了新方向，还具有学术和实际价值；潜在局限性可能涉及训练复杂性，未来工作可进一步优化模型或扩展至更多视频任务。",
      "tags": [
        "Diffusion Models",
        "Adversarial Training",
        "Adaptive Window Attention",
        "Video Restoration",
        "Feature Matching Loss"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:14.399819Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.04207",
    "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning",
    "authors": [
      "Shuang Chen",
      "Yue Guo",
      "Zhaochen Su",
      "Yafu Li",
      "Yulun Wu",
      "Jiacheng Chen",
      "Jiayu Chen",
      "Weijie Wang",
      "Xiaoye Qu",
      "Yu Cheng"
    ],
    "abstract": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex textual tasks, many works attempt to incentivize similar capabilities in Multimodal Large Language Models (MLLMs) by directly applying reinforcement learning (RL). However, they still struggle to activate complex reasoning. In this paper, rather than examining multimodal RL in isolation, we delve into current training pipelines and identify three crucial phenomena: 1) Effective cold start initialization is critical for enhancing MLLM reasoning. Intriguingly, we find that initializing with carefully selected text data alone can lead to performance surpassing many recent multimodal reasoning models, even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers from gradient stagnation, which degrades training stability and performance. 3) Subsequent text-only RL training, following the multimodal RL phase, further enhances multimodal reasoning. This staged training approach effectively balances perceptual grounding and cognitive reasoning development. By incorporating the above insights and addressing multimodal RL issues, we introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath, LogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.04207.pdf",
    "abs_url": "https://arxiv.org/abs/2506.04207",
    "published": "2025-06-04T17:51:08Z",
    "updated": "2026-01-28T07:36:02Z",
    "comment": "19 pages, 6 figures",
    "light_analysis": {
      "overview": "提出 ReVisual-R1 模型，通过优化的冷启动和分阶段强化学习方法，显著提升多模态大型语言模型的推理能力，在多个基准测试中达到领先水平。",
      "motivation": "本研究旨在解决多模态大型语言模型在复杂推理任务中表现不足的问题，因为当前方法直接应用强化学习难以激活复杂推理，影响实际应用如数学和逻辑推理的重要性。现有方法的不足之处在于训练中可能存在梯度停滞问题，导致训练不稳定和性能下降。摘要未明确说明所有背景细节，但强调了提升多模态推理能力的必要性。",
      "method": "论文基于对训练管道的分析，提出一种分阶段训练方法：首先进行优化的冷启动初始化，使用精心选择的文本数据增强模型推理能力；然后应用多模态强化学习，解决了标准 GRPO 中的梯度停滞问题；最后进行仅文本强化学习训练，以进一步平衡感知基础和认知推理。核心创新包括识别梯度停滞和采用分阶段策略，模型架构为 7B 参数的多模态大型语言模型 ReVisual-R1。",
      "result": "ReVisual-R1 在挑战性基准测试中取得了 state-of-the-art 性能，包括 MathVerse、MathVision、WeMath、LogicVista、DynaMath、AIME2024 和 AIME2025。作为开源 7B 多模态大型语言模型，其性能超过了最近许多多模态推理模型，验证了分阶段训练方法的有效性。摘要未提供具体数值数据，但强调了与基线相比的显著改进。",
      "conclusion": "本研究的主要贡献在于提出了一种有效的分阶段训练方法，结合优化的冷启动和多模态强化学习，显著提升了多模态大型语言模型的推理能力。学术价值在于深入分析了训练管道中的关键问题，为多模态推理研究提供了新方向；实际应用价值在于改善了模型在数学和逻辑推理任务中的表现。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning (RL)",
        "Cold Start Initialization",
        "Staged Training",
        "Gradient Stagnation"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:25.633099Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.03996",
    "title": "Spiking Brain Compression: Post-Training Second-order Compression for Spiking Neural Networks",
    "authors": [
      "Lianfeng Shi",
      "Ao Li",
      "Benjamin Ward-Cherrier"
    ],
    "abstract": "Spiking Neural Networks (SNNs) have emerged as a new generation of energy-efficient neural networks suitable for implementation on neuromorphic hardware. As neuromorphic hardware has limited memory and computational resources, parameter pruning and quantization have recently been explored to improve the efficiency of SNNs. State-of-the-art SNN pruning/quantization methods employ multiple compression and training iterations, increasing the cost for pre-trained or very large SNNs. In this paper, we propose a novel one-shot post-training compression framework, Spiking Brain Compression (SBC), that extends the classical Optimal Brain Surgeon method to SNNs. SBC replaces the current-based objective found in the common layer-wise compression method with a spike-train-based objective whose Hessian is cheaply computable, allowing a single backward pass to compress parameters and analytically rescale the rest. Applying SBC to SNN pruning and quantization across event-based and static datasets (up to ImageNet), including SEW-ResNet152 and spike-driven Transformers, we achieve state-of-the-art one-shot post-training compression for SNNs, with single- to double-digit accuracy gains over ANN compression baselines ported to SNNs. We further report a synaptic-operation-based energy proxy and a calibration-size ablation, demonstrating robust performance under sub-one-sample-per-class calibration.",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.03996.pdf",
    "abs_url": "https://arxiv.org/abs/2506.03996",
    "published": "2025-06-04T14:23:05Z",
    "updated": "2026-01-28T17:49:10Z",
    "comment": "Preliminary work accepted at non-archival OPT-ML workshop at NeurIPS 2025. The workshop version is available in an earlier version of this arXiv paper",
    "light_analysis": {
      "overview": "论文提出了一种名为Spiking Brain Compression的单次后训练压缩框架，用于脉冲神经网络，实现了高效的参数修剪和量化。",
      "motivation": "脉冲神经网络作为新一代高效能神经网络，适合在资源有限的神经形态硬件上部署，但硬件内存和计算资源受限，需要压缩参数以提高效率。现有SNN压缩方法如参数修剪和量化通常需要多次压缩和训练迭代，成本高昂，尤其是在预训练或大规模模型上，因此迫切需要一种更高效的单次后训练压缩方法来解决这一问题。",
      "method": "研究方法基于经典的Optimal Brain Surgeon方法，将其扩展到脉冲神经网络，提出SBC框架。关键创新在于使用基于脉冲序列的目标替代了传统层级压缩中的基于电流的目标，其Hessian矩阵计算成本低廉，允许通过单次后向传播一次性压缩参数并解析地调整剩余参数。该方法应用于SNN的修剪和量化，在事件型数据集和静态数据集上进行实验，包括SEW-ResNet152和脉冲驱动的Transformer等模型。",
      "result": "在事件型和静态数据集上，包括ImageNet，SBC实现了单次后训练压缩的最先进性能。与移植到SNN的人工神经网络压缩基线相比，准确率有单位到双位数的提升。此外，论文报告了基于突触操作的能源效率代理和校准大小消融实验，显示在次每类一个样本校准条件下的鲁棒性能，验证了方法的有效性和稳定性。",
      "conclusion": "SBC提供了一种高效的SNN压缩方法，显著减少了压缩过程中的计算成本，适用于资源受限的神经形态硬件，具有实际应用价值。学术上扩展了Optimal Brain Surgeon技术到SNN领域，未来工作可进一步优化方法以适应更复杂场景或数据集，摘要未明确说明具体局限性。",
      "tags": [
        "Spiking Neural Networks",
        "Parameter Pruning",
        "Quantization",
        "Optimal Brain Surgeon",
        "Post-Training Compression"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:25.987835Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.01195",
    "title": "Strategic Dialogue Assessment: The Crooked Path to Innocence",
    "authors": [
      "Anshun Asher Zheng",
      "Junyi Jessy Li",
      "David I. Beaver"
    ],
    "abstract": "Language is often used strategically, particularly in high-stakes, adversarial settings, yet most work on pragmatics and LLMs centers on cooperativity. This leaves a gap in the systematic understanding of strategic communication in adversarial settings. To address this, we introduce SDA (Strategic Dialogue Assessment), a framework grounded in Gricean and game-theoretic pragmatics to assess strategic use of language. It adapts the ME Game jury function to make it empirically estimable for analyzing dialogue. Our approach incorporates two key adaptations: a commitment-based taxonomy of discourse moves, which provides a finer-grained account of strategic effects, and the use of estimable proxies grounded in Gricean maxims to operationalize abstract constructs such as credibility. Together, these adaptations build on discourse theory by treating discourse as the strategic management of commitments, enabling systematic evaluation of how conversational moves advance or undermine discourse goals. We further derive three interpretable metrics-Benefit at Turn (BAT), Penalty at Turn (PAT), and Normalized Relative Benefit at Turn (NRBAT)-to quantify the perceived strategic effects of discourse moves. We also present CPD (the Crooked Path Dataset), an annotated dataset of real courtroom cross-examinations, to demonstrate the framework's effectiveness. Using these tools, we evaluate a range of LLMs and show that LLMs generally exhibit limited pragmatic understanding of strategic language. While model size shows an increase in performance on our metrics, reasoning ability does not help and largely hurts, introducing overcomplication and internal confusion.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.01195.pdf",
    "abs_url": "https://arxiv.org/abs/2506.01195",
    "published": "2025-06-01T22:07:20Z",
    "updated": "2026-01-28T04:04:50Z",
    "comment": "53 pages. Title changed. Accepted by Dialogue and Discourse 17(1)",
    "light_analysis": {
      "overview": "本研究提出SDA框架，结合Gricean语用学和博弈论，用于系统评估对抗性对话中语言的战略效果并量化其影响。",
      "motivation": "研究动机源于语言在对抗性设置（如法庭交叉审讯）中常被战略使用，但现有语用学和大型语言模型研究多聚焦合作性沟通，缺乏对战略通信的系统理解。这导致在评估和应对高风险的敌对性语言行为时存在空白，特别是在需要精确分析话语目标与效果的场景中，如法律或辩论环境。",
      "method": "研究方法基于SDA框架，融合Gricean和博弈论语用学，关键创新包括引入承诺分类法以细化话语移动的战略效果，并使用基于Gricean准则的可估计代理来操作化抽象概念如可信度。此外，提出三个指标（BAT、PAT、NRBAT）量化战略效果，并利用CPD数据集（标注的真实法庭对话）进行实证分析。",
      "result": "实验结果通过评估多种大型语言模型显示，模型在战略语言上的语用理解普遍有限。具体而言，模型大小增加带来SDA指标上的性能提升，但推理能力无益甚至有害，常导致过度复杂化和内部混乱。与基线相比，这揭示了当前LLMs在处理对抗性话语时存在缺陷。",
      "conclusion": "SDA框架为战略语言评估提供了系统方法，填补了语用学和LLMs研究中的空白，揭示了LLMs在对抗性场景下的局限性。其学术价值在于整合语用理论与实证分析，实际应用于法律对话等领域，未来可扩展到其他敌对性沟通场景并改进模型设计。",
      "tags": [
        "Strategic Dialogue Assessment",
        "Gricean Pragmatics",
        "Game Theory",
        "LLMs Evaluation",
        "Discourse Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:53:21.042494Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.22147",
    "title": "Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions",
    "authors": [
      "Florian Andreas Marwitz",
      "Tanya Braun",
      "Ralf Möller",
      "Marcel Gehrke"
    ],
    "abstract": "Decision making is a central problem in AI that can be formalized using a Markov Decision Process. A problem is that, with increasing numbers of (indistinguishable) objects, the state space grows exponentially. To compute policies, the state space has to be enumerated. Even more possibilities have to be enumerated if the size of the action space depends on the size of the state space, especially if we allow concurrent actions. To tackle the exponential blow-up in the action and state space, we present a first-order representation to store the spaces in polynomial instead of exponential size in the number of objects and introduce Foreplan, a relational forward planner, which uses this representation to efficiently compute policies for numerous indistinguishable objects and actions. Additionally, we introduce an even faster approximate version of Foreplan. Moreover, Foreplan identifies how many objects an agent should act on to achieve a certain task given restrictions. Further, we provide a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a speedup of at least four orders of magnitude.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2505.22147.pdf",
    "abs_url": "https://arxiv.org/abs/2505.22147",
    "published": "2025-05-28T09:08:27Z",
    "updated": "2026-01-28T10:31:47Z",
    "comment": "Accepted at AAMAS 2026",
    "light_analysis": {
      "overview": "论文提出了一种一阶表示和关系前向规划器Foreplan，用于高效处理马尔可夫决策过程中对象和动作空间的指数爆炸问题。",
      "motivation": "决策制定是人工智能的核心问题，常通过马尔可夫决策过程形式化。然而，随着不可区分对象数量的增加，状态空间呈指数级增长，传统方法需要枚举状态和动作空间，计算成本极高。特别是在允许并发动作时，动作空间大小也依赖于状态空间，进一步加剧了计算复杂性。因此，开发高效方法以应对指数爆炸至关重要。现有方法在对象多时效率低下，急需创新解决方案。",
      "method": "本研究提出了一种一阶表示方法，将状态和动作空间从指数大小压缩为多项式大小，相对于对象数量。基于此，引入了Foreplan，一个关系前向规划器，利用此表示高效计算策略，适用于大量不可区分对象和动作。关键创新包括支持并发动作，并能识别代理应作用于的对象数量以实现特定任务。此外，还提供了Foreplan的近似版本，以进一步提高速度。摘要未明确说明具体数据集或模型架构细节。",
      "result": "通过理论分析和实证评估，论文展示了Foreplan在效率上的显著改进。具体效果上，与基线方法相比，Foreplan实现了至少四个数量级的速度提升。摘要未明确说明具体基线方法，但可以推断是与传统枚举方法对比。这一结果证明了所提方法在处理大规模状态和动作空间时的有效性，提高了规划任务的可行性。",
      "conclusion": "本研究的核心贡献在于提出了一阶表示和Foreplan规划器，有效解决了马尔可夫决策过程中对象和动作空间指数爆炸的问题。学术上，推动了高效规划方法的发展；实际应用中，有潜力应用于机器人、自动驾驶等领域。摘要未明确提及局限性，但未来工作可能包括扩展该方法到更复杂的场景或进一步优化近似版本。",
      "tags": [
        "Relational Factored MDP",
        "Forward Planning",
        "Concurrent Actions",
        "First-Order Representation",
        "Lifted Planning"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:17.341794Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.14790",
    "title": "Continuous Evolution Pool: Taming Recurring Concept Drift in Online Time Series Forecasting",
    "authors": [
      "Tianxiang Zhan",
      "Ming Jin",
      "Yuanpeng He",
      "Yuxuan Liang",
      "Yong Deng",
      "Shirui Pan"
    ],
    "abstract": "Recurring concept drift poses a dual challenge in online time series forecasting: mitigating catastrophic forgetting while adhering to strict privacy constraints that prevent retaining historical data. Existing approaches predominantly rely on parameter updates or experience replay, which inevitably suffer from knowledge overwriting or privacy risks. To address this, we propose the Continuous Evolution Pool (CEP), a privacy-preserving framework that maintains a dynamic pool of specialized forecasters. Instead of storing raw samples, CEP utilizes lightweight statistical genes to decouple concept identification from forecasting. Specifically, it employs a Retrieval mechanism to identify the nearest concept based on gene similarity, an Evolution strategy to spawn new forecasters upon detecting distribution shifts, and an Elimination policy to prune obsolete models under memory constraints. Experiments on real-world datasets demonstrate that CEP significantly outperforms state-of-the-art baselines, reducing forecasting error by over 20% without accessing historical ground truth.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.14790.pdf",
    "abs_url": "https://arxiv.org/abs/2506.14790",
    "published": "2025-05-28T03:27:49Z",
    "updated": "2026-01-28T13:48:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Continuous Evolution Pool（CEP）框架，通过动态预测器池和轻量级统计基因，解决在线时间序列预测中的重复概念漂移与隐私保护双重挑战。",
      "motivation": "在线时间序列预测中，重复概念漂移导致模型易发生灾难性遗忘，难以平衡新旧知识适应；同时，严格隐私约束禁止存储历史数据，加剧了预测难题。现有方法如参数更新或经验回放常引发知识覆盖或隐私风险，无法有效应对漂移和隐私的双重需求，凸显了对新方法的迫切性。",
      "method": "CEP框架核心是使用轻量级统计基因解耦概念识别与预测，避免存储原始数据以保护隐私。它包括三个关键组件：Retrieval机制基于基因相似性匹配当前数据到最近概念；Evolution策略在检测分布偏移时生成新预测器；Elimination政策在内存约束下剪枝过时模型，形成一个动态的专用预测器池来适应变化。",
      "result": "在真实世界数据集上的实验表明，CEP显著优于最先进的基线方法，预测误差减少超过20%，且无需访问历史真实数据。这一性能提升验证了CEP在处理重复概念漂移和维持预测精度方面的有效性，同时确保了隐私保护。",
      "conclusion": "CEP框架的主要贡献在于提供了一种隐私保护的动态预测器池方法，有效解决在线时间序列预测中的重复概念漂移问题，具有重要学术价值，推动了在线学习领域的发展。实际应用中，它适用于隐私敏感场景如金融预测，未来工作可能涉及优化内存效率或扩展到更复杂的数据类型。",
      "tags": [
        "Online Time Series Forecasting",
        "Recurring Concept Drift",
        "Privacy-Preserving Learning",
        "Dynamic Model Pool",
        "Statistical Genes"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:17.890419Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19847",
    "title": "DGRAG: Distributed Graph-based Retrieval-Augmented Generation in Edge-Cloud Systems",
    "authors": [
      "Wenqing Zhou",
      "Yuxuan Yan",
      "Qianqian Yang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) improves factuality by grounding LLMs in external knowledge, yet conventional centralized RAG requires aggregating distributed data, raising privacy risks and incurring high retrieval latency and cost. We present DGRAG, a distributed graph-driven RAG framework for edge-cloud collaborative systems. Each edge device organizes local documents into a knowledge graph and periodically uploads subgraph-level summaries to the cloud for lightweight global indexing without exposing raw data. At inference time, queries are first answered on the edge; a gate mechanism assesses the confidence and consistency of multiple local generations to decide whether to return a local answer or escalate the query. For escalated queries, the cloud performs summary-based matching to identify relevant edges, retrieves supporting evidence from them, and generates the final response with a cloud LLM. Experiments on distributed question answering show that DGRAG consistently outperforms decentralized baselines while substantially reducing cloud overhead.",
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2505.19847.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19847",
    "published": "2025-05-26T11:31:58Z",
    "updated": "2026-01-28T10:55:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了DGRAG，一个分布式图驱动检索增强生成框架，用于边缘云系统，以在提升事实准确性同时保护隐私并降低延迟和成本。",
      "motivation": "传统检索增强生成（RAG）在集中式系统中需聚合分布式数据，导致隐私泄露风险、高检索延迟和高成本。随着数据在边缘设备上分散存储，隐私保护和实时响应变得至关重要，尤其在物联网和敏感应用场景中。现有集中式方法无法高效处理分布式数据，限制了其在大规模、隐私敏感环境中的适用性，因此需要一种去中心化解决方案来平衡性能和隐私。",
      "method": "DGRAG框架基于边缘云协作设计，每个边缘设备将本地文档组织成知识图，并定期上传子图级摘要到云端进行轻量级全局索引，避免暴露原始数据。推理时，查询先在边缘处理，通过门机制评估多个本地生成的置信度和一致性，以决定返回本地答案或升级查询。对于升级查询，云端执行摘要匹配以识别相关边缘，检索支持证据，并使用云端大型语言模型生成最终响应。关键创新包括图驱动知识表示和智能门控决策。",
      "result": "在分布式问答任务实验中，DGRAG始终优于分散基线方法，展现出更高的性能准确性。同时，它显著减少了云端开销，包括计算和通信成本，尽管摘要未提供具体数值指标。这表明DGRAG在保持高效检索的同时，优化了资源使用，适用于实际部署环境。",
      "conclusion": "该研究的主要贡献是开发了DGRAG框架，有效整合边缘计算和云计算的优点，实现隐私友好、低延迟的检索增强生成系统。其学术价值在于提出了一种分布式AI解决方案，为边缘AI和隐私保护领域提供新思路；实际应用价值覆盖物联网、医疗等数据敏感场景。未来工作可探索更多应用扩展和性能优化，摘要未明确说明具体局限性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Knowledge Graph",
        "Edge-Cloud Systems",
        "Large Language Model",
        "Gate Mechanism"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:01.073545Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.17536",
    "title": "Multimodal Conversation Structure Understanding",
    "authors": [
      "Kent K. Chang",
      "Mackenzie Hanh Cramer",
      "Anna Ho",
      "Ti Ti Nguyen",
      "Yilin Yuan",
      "David Bamman"
    ],
    "abstract": "While multimodal large language models (LLMs) excel at dialogue, whether they can adequately parse the structure of conversation -- conversational roles and threading -- remains underexplored. In this work, we introduce a suite of tasks and release TV-MMPC, a new annotated dataset, for multimodal conversation structure understanding. Our evaluation reveals that while all multimodal LLMs outperform our heuristic baseline, even the best-performing model we consider experiences a substantial drop in performance when character identities of the conversation are anonymized. Beyond evaluation, we carry out a sociolinguistic analysis of 350,842 utterances in TVQA. We find that while female characters initiate conversations at rates in proportion to their speaking time, they are 1.2 times more likely than men to be cast as an addressee or side-participant, and the presence of side-participants shifts the conversational register from personal to social.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.17536.pdf",
    "abs_url": "https://arxiv.org/abs/2505.17536",
    "published": "2025-05-23T06:41:54Z",
    "updated": "2026-01-28T18:39:09Z",
    "comment": "accepted to EACL 2026 main conference; 22 pages, 9 figures, 10 tables",
    "light_analysis": {
      "overview": "本文引入了任务套件并发布了TV-MMPC数据集，用于评估多模态大语言模型在对话结构理解方面的能力。",
      "motivation": "研究动机在于解决多模态大语言模型（LLMs）在对话结构解析（如对话角色和线程）方面未被充分探索的问题。这至关重要，因为对话结构理解影响人机交互和自然语言处理应用的有效性，而现有方法可能侧重于表面对话生成，忽略深层结构分析，导致沟通质量受限。摘要指出现有研究存在不足，强调了探索这一领域的必要性，以提升模型的实用性和准确性。",
      "method": "研究方法包括发布TV-MMPC数据集，这是一个标注的多模态数据集，专门用于评估对话结构理解。核心创新点是设计了一套任务来测试模型解析角色和线程的能力，并进行了社会语言学分析，利用TVQA数据集中的350,842条话语来研究性别角色在对话中的分布。技术特色在于结合多模态信息，为模型评估提供了基准工具，强调对结构要素的深度分析。",
      "result": "主要实验结果显示，所有测试的多模态LLMs在对话结构理解任务上均优于启发式基线，表明模型具备初步能力。然而，在对话角色匿名化后，即使最佳模型的性能也出现显著下降，凸显了模型对身份信息的依赖。社会语言学分析具体发现女性角色虽然按说话时间比例发起对话，但她们比男性高出1.2倍的可能性被指定为受话者或旁听者，且旁听者的存在使对话从个人转向社会风格。",
      "conclusion": "论文的主要贡献是提供了数据集和评估框架，揭示了多模态LLMs在对话结构理解方面的局限性。学术价值在于推动多模态对话理解研究，填补了结构解析的空白；实际应用价值有助于改进人机交互系统，提升沟通效率。潜在的局限性包括模型对匿名化的脆弱性，未来工作方向可包括开发更鲁棒的模型或扩展到其他社会因素分析。",
      "tags": [
        "Multimodal Large Language Model",
        "Conversation Structure Understanding",
        "Anonymization",
        "Sociolinguistic Analysis",
        "Dataset"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:12.128411Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.16512",
    "title": "Beyond Face Swapping: A Diffusion-Based Digital Human Benchmark for Multimodal Deepfake Detection",
    "authors": [
      "Jiaxin Liu",
      "Jia Wang",
      "Saihui Hou",
      "Min Ren",
      "Huijia Wu",
      "Long Ma",
      "Renwang Pei",
      "Zhaofeng He"
    ],
    "abstract": "In recent years, the explosive advancement of deepfake technology has posed a critical and escalating threat to public security: diffusion-based digital human generation. Unlike traditional face manipulation methods, such models can generate highly realistic videos with consistency via multimodal control signals. Their flexibility and covertness pose severe challenges to existing detection strategies. To bridge this gap, we introduce DigiFakeAV, the new large-scale multimodal digital human forgery dataset based on diffusion models. Leveraging five of the latest digital human generation methods and a voice cloning method, we systematically construct a dataset comprising 60,000 videos (8.4 million frames), covering multiple nationalities, skin tones, genders, and real-world scenarios, significantly enhancing data diversity and realism. User studies demonstrate that the misrecognition rate by participants for DigiFakeAV reaches as high as 68%. Moreover, the substantial performance degradation of existing detection models on our dataset further highlights its challenges. To address this problem, we propose DigiShield, an effective detection baseline based on spatiotemporal and cross-modal fusion. By jointly modeling the 3D spatiotemporal features of videos and the semantic-acoustic features of audio, DigiShield achieves state-of-the-art (SOTA) performance on the DigiFakeAV and shows strong generalization on other datasets.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.16512.pdf",
    "abs_url": "https://arxiv.org/abs/2505.16512",
    "published": "2025-05-22T10:46:37Z",
    "updated": "2026-01-28T09:33:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过构建基于扩散模型的大型多模态数字人伪造数据集DigiFakeAV并设计时空跨模态融合检测方法DigiShield，推动了深度伪造检测技术的发展。",
      "motivation": "近年来，扩散模型为基础的数字人生成技术能通过多模态控制信号产生高度真实且一致的视频，其灵活性和隐蔽性对公共安全构成严重威胁。传统的人脸操纵方法如换脸技术已不足以应对这些挑战，现有检测策略在处理多模态伪造内容时性能受限。因此，亟需构建更具挑战性的数据集和开发新方法来提升检测能力，以填补这一研究空白。",
      "method": "论文首先利用五种最新的数字人生成方法和一种语音克隆方法，系统构建了包含60,000个视频（840万帧）的大型数据集DigiFakeAV，覆盖多种国籍、肤色、性别和现实场景，增强了数据多样性和真实性。在此基础上，提出了DigiShield检测基准，该方法通过融合视频的3D时空特征和音频的语义-声学特征，实现了对多模态伪造内容的联合建模，关键创新在于跨模态和时空特征的集成处理。",
      "result": "用户研究表明，参与者对DigiFakeAV数据集的误识别率高达68%，验证了其高欺骗性和挑战性。现有检测模型在该数据集上性能显著下降，而DigiShield在DigiFakeAV上实现了最先进（SOTA）性能，并在其他数据集上展示了强大的泛化能力，证明了该方法在多模态深度伪造检测中的有效性。",
      "conclusion": "本论文的主要贡献是引入了首个基于扩散模型的多模态数字人伪造数据集DigiFakeAV，并提出了检测基准DigiShield。这为深度伪造检测领域提供了重要的新基准，具有推动学术研究进步的学术价值和增强实际安全防护的应用价值。未来工作可包括扩展数据集和改进模型以应对更复杂的伪造技术。",
      "tags": [
        "Diffusion Models",
        "Multimodal Deepfake Detection",
        "Spatiotemporal Fusion",
        "Audio-visual Fusion",
        "Dataset Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:16.435787Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.16324",
    "title": "From Prediction to Perfection: Introducing Refinement to Autoregressive Image Generation",
    "authors": [
      "Cheng Cheng",
      "Lin Song",
      "Di An",
      "Yicheng Xiao",
      "Xuchong Zhang",
      "Hongbin Sun",
      "Ying Shan"
    ],
    "abstract": "Autoregressive (AR) image generators offer a language-model-friendly approach to image generation by predicting discrete image tokens in a causal sequence. However, unlike diffusion models, AR models lack a mechanism to refine previous predictions, limiting their generation quality. In this paper, we introduce TensorAR, a new AR paradigm that reformulates image generation from next-token prediction to next-tensor prediction. By generating overlapping windows of image patches (tensors) in a sliding fashion, TensorAR enables iterative refinement of previously generated content. To prevent information leakage during training, we propose a discrete tensor noising scheme, which perturbs input tokens via codebook-indexed noise. TensorAR is implemented as a plug-and-play module compatible with existing AR models. Extensive experiments on LlamaGEN, Open-MAGVIT2, and RAR demonstrate that TensorAR significantly improves the generation performance of autoregressive models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.16324.pdf",
    "abs_url": "https://arxiv.org/abs/2505.16324",
    "published": "2025-05-22T07:27:25Z",
    "updated": "2026-01-28T06:47:56Z",
    "comment": "Published as a conference paper at ICLR 2026",
    "light_analysis": {
      "overview": "本文提出TensorAR，一种新自回归图像生成范式，通过张量预测和迭代改进机制，显著提升图像生成质量。",
      "motivation": "自回归（AR）图像生成器基于语言模型预测离散图像令牌，提供与语言模型兼容的方法，但缺乏迭代改进先前预测的机制，这限制了生成质量，尤其在细节和逼真度方面。相较于扩散模型，AR模型无法在生成过程中修正错误，导致输出可能不完美，影响了实际应用如高保真图像合成。因此，研究旨在解决AR模型的这一短板，通过引入改进能力，提升其与扩散模型的竞争力，推动高质量图像生成的发展。",
      "method": "TensorAR将图像生成从下一个令牌预测重新定义为下一个张量预测，通过滑动方式生成图像补丁（张量）的重叠窗口，实现迭代改进先前内容。为防止训练时信息泄露，提出离散张量噪声方案，用代码索引的噪声扰动输入令牌。该模块设计为即插即用，兼容现有AR模型如LlamaGEN，无需修改核心架构，便于集成。关键创新包括张量预测框架和噪声处理技术，增强了模型的灵活性和鲁棒性。",
      "result": "在LlamaGEN、Open-MAGVIT2和RAR等自回归模型上进行了广泛实验，TensorAR显著提高了生成性能，表现出优于基线AR方法的图像质量改进。虽然摘要未明确具体数据如准确率数值，但结果表明该方法在多个基准上实现了有效提升，增强了生成图像的细节和整体效果，突显了迭代改进机制的优势。对比显示，TensorAR弥补了AR模型与扩散模型之间的差距，提高了实际应用的可行性。",
      "conclusion": "TensorAR的主要贡献是为自回归图像生成引入迭代改进能力，通过张量预测和噪声方案提升生成质量，同时保持与现有模型的兼容性。这项研究不仅提升了AR模型的学术价值，扩展了图像生成技术的边界，还具有实际应用潜力，如创意设计和内容生成。未来工作可探索噪声方案的优化、在其他生成任务中的应用或扩展到更多模型类型，以进一步推动领域发展。摘要未明确说明具体局限性，但潜在的未来方向包括效率改进和多模态集成。",
      "tags": [
        "Autoregressive Models",
        "Image Generation",
        "Tensor Prediction",
        "Iterative Refinement",
        "Discrete Tensor Noising"
      ]
    },
    "analyzed_at": "2026-01-29T03:54:47.920790Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.14411",
    "title": "Byte Pair Encoding for Efficient Time Series Forecasting",
    "authors": [
      "Leon Götz",
      "Marcel Kollovieh",
      "Stephan Günnemann",
      "Leo Schwinn"
    ],
    "abstract": "Existing time series tokenization methods predominantly encode a constant number of samples into individual tokens. This inflexible approach can generate excessive tokens for even simple patterns like extended constant values, resulting in substantial computational overhead. Inspired by the success of byte pair encoding, we propose the first pattern-centric tokenization scheme for time series analysis. Based on a discrete vocabulary of frequent motifs, our method merges samples with underlying patterns into tokens, compressing time series adaptively. Exploiting our finite set of motifs and the continuous properties of time series, we further introduce conditional decoding as a lightweight yet powerful post-hoc optimization method, which requires no gradient computation and adds no computational overhead. On recent time series foundation models, our motif-based tokenization improves forecasting performance by 36% and boosts efficiency by 1990% on average. Conditional decoding further reduces MSE by up to 44%. In an extensive analysis, we demonstrate the adaptiveness of our tokenization to diverse temporal patterns, its generalization to unseen data, and its meaningful token representations capturing distinct time series properties, including statistical moments and trends.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.14411.pdf",
    "abs_url": "https://arxiv.org/abs/2505.14411",
    "published": "2025-05-20T14:24:49Z",
    "updated": "2026-01-28T13:39:34Z",
    "comment": "29 pages in total, 22 figures",
    "light_analysis": {
      "overview": "提出首个基于模式的标记化方法，结合字节对编码，显著提升时间序列预测的效率和性能。",
      "motivation": "现有时间序列标记化方法通常将恒定数量的样本编码为单个标记，这种方法缺乏灵活性，对于简单模式如延长的常数值，会产生过多的标记，导致计算开销大幅增加。传统方法无法自适应地压缩时间序列，限制了预测模型的效率和准确性，特别是在处理复杂时间序列数据时。因此，需要一种新的标记化方案来减少不必要的标记，同时有效捕获时间序列的底层模式，以优化预测过程。",
      "method": "该方法基于字节对编码的启发，提出了首个模式中心的标记化方案。通过构建时间序列中频繁模式的离散词汇表，将样本与底层模式合并为单个标记，实现自适应压缩。创新点包括模式驱动的标记化和条件解码优化：条件解码利用模式的有限性和时间序列的连续性，作为轻量级后优化方法，无需梯度计算，不增加计算开销。这种方法适用于各种时间序列基础模型，通过模式识别提升标记的语义丰富性和计算效率。",
      "result": "在最近的时间序列基础模型上，基于模式的标记化平均提高了预测性能36%，并提升了计算效率1990%。条件解码进一步减少了均方误差（MSE）高达44%。实验对比表明，该方法相比传统固定样本标记化方法，在预测准确性和运行速度上有显著改善。广泛分析证实了标记化对不同时间序列模式的自适应性，以及对未见数据的泛化能力，同时标记表示能有效捕获时间序列的统计矩和趋势等属性。",
      "conclusion": "本研究贡献了首个结合字节对编码和模式分析的标记化方法，显著优化了时间序列预测的效率和准确性。其学术价值在于提出了创新的标记化框架，推动了时间序列处理技术的发展；实际应用中，能降低模型计算成本，提高预测精度，适用于金融、天气等多个领域。尽管在实验中展示了强大效果，未来工作可探索扩展至更多数据类型或集成其他优化技术以进一步提升泛化能力。",
      "tags": [
        "Byte Pair Encoding",
        "Time Series Tokenization",
        "Conditional Decoding",
        "Motif-based Methods",
        "Efficient Forecasting"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:07.176810Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.12742",
    "title": "MVAR: Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning",
    "authors": [
      "Jinhua Zhang",
      "Wei Long",
      "Minghao Han",
      "Weiyi You",
      "Shuhang Gu"
    ],
    "abstract": "Essential to visual generation is efficient modeling of visual data priors. Conventional next-token prediction methods define the process as learning the conditional probability distribution of successive tokens. Recently, next-scale prediction methods redefine the process to learn the distribution over multi-scale representations, significantly reducing generation latency. However, these methods condition each scale on all previous scales and require each token to consider all preceding tokens, exhibiting scale and spatial redundancy. To better model the distribution by mitigating redundancy, we propose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive framework that introduces scale and spatial Markov assumptions to reduce the complexity of conditional probability modeling. Specifically, we introduce a scale-Markov trajectory that only takes as input the features of adjacent preceding scale for next-scale prediction, enabling the adoption of a parallel training strategy that significantly reduces GPU memory consumption. Furthermore, we propose spatial-Markov attention, which restricts the attention of each token to a localized neighborhood of size k at corresponding positions on adjacent scales, rather than attending to every token across these scales, for the pursuit of reduced modeling complexity. Building on these improvements, we reduce the computational complexity of attention calculation from O(N^2) to O(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating the need for KV cache during inference. Extensive experiments on ImageNet demonstrate that MVAR achieves comparable or superior performance with both small model trained from scratch and large fine-tuned models, while reducing the average GPU memory footprint by 3.0x.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.12742.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12742",
    "published": "2025-05-19T05:56:44Z",
    "updated": "2026-01-28T13:38:40Z",
    "comment": "Accepted to ICLR 2026. Project page: https://nuanbaobao.github.io/MVAR",
    "light_analysis": {
      "overview": "MVAR是一种视觉自回归建模框架，通过引入尺度马尔可夫轨迹和空间马尔可夫注意力，有效减少冗余并降低计算复杂度，提升生成效率。",
      "motivation": "在视觉生成领域，高效建模视觉数据先验至关重要。现有方法如next-token预测和next-scale预测虽能降低延迟，但存在尺度和空间冗余：每个尺度条件于所有先前尺度，每个令牌需关注所有先前令牌，这导致计算复杂性高、资源消耗大，限制模型可扩展性，因此急需减少条件依赖性以提高效率。",
      "method": "MVAR提出一种自回归框架，引入尺度马尔可夫轨迹和空间马尔可夫注意力。前者仅使用相邻前一个尺度的特征进行下一尺度预测，支持并行训练以减少GPU内存；后者限制每个令牌的注意力到相邻尺度对应位置的局部邻域（大小k），而非所有令牌。这使得注意力计算复杂度从O(N^2)降至O(Nk)，简化训练和推理过程。",
      "result": "在ImageNet数据集上的实验显示，MVAR从小训练的小模型和微调的大模型均能达到可比或更优的性能。具体地，GPU内存占用平均减少3.0倍，计算复杂度降低，训练仅需8个NVIDIA RTX 4090 GPUs，推理时无需KV缓存，与基线方法相比在保持性能的同时显著提升效率。",
      "conclusion": "MVAR通过马尔可夫假设减少视觉自回归建模中的冗余，降低计算复杂性，提升训练和推理效率，为资源有限下的视觉生成提供实用方案。学术上推动了高效自回归建模的发展，但摘要未明确说明潜在局限性或未来工作方向。",
      "tags": [
        "Autoregressive Modeling",
        "Markov Processes",
        "Attention Mechanism",
        "Multi-scale Representations",
        "Visual Generation"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:06.815406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.11497",
    "title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
    "authors": [
      "Yushi Huang",
      "Ruihao Gong",
      "Jing Liu",
      "Yifu Ding",
      "Chengtao Lv",
      "Haotong Qin",
      "Jun Zhang"
    ],
    "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($Φ$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $Φ$, we propose a rank-decay strategy that progressively eliminates $Φ$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\\mathbfγ$ to identify and decay low-contributing components. This strategy retains performance while zeroing out additional inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3\\text{B}\\sim14\\text{B}$, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. Code and models are available at https://github.com/ModelTC/QVGen.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.11497.pdf",
    "abs_url": "https://arxiv.org/abs/2505.11497",
    "published": "2025-05-16T17:59:40Z",
    "updated": "2026-01-28T14:07:09Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": {
      "overview": "QVGen提出了一种量化感知训练框架，首次在4位量化下为视频扩散模型实现与全精度相当的性能。",
      "motivation": "视频扩散模型（DMs）能够实现高质量视频合成，但其巨大的计算和内存需求构成了实际部署的严重挑战，即使在高性能GPU上也难以广泛应用。量化作为降低成本的常见方法，在图像DMs中已证明有效，但直接应用于视频DMs时效果不佳。因此，需要开发专门针对视频DMs的量化技术，以解决效率问题并促进模型在资源受限环境中的部署。",
      "method": "QVGen是一个专为视频扩散模型设计的量化感知训练框架。首先，通过理论分析表明减少梯度范数是促进量化感知训练收敛的关键。为此，引入辅助模块（Φ）来减轻大量化误差，从而显著增强收敛性。为消除这些模块的推理开销，提出秩衰减策略，通过重复应用奇异值分解和秩基正则化（γ）来识别并衰减低贡献组件，实现零额外开销，同时保持性能。该方法适用于极低比特量化，如4位或以下。",
      "result": "在四个最先进的视频扩散模型上进行实验，参数规模从1.3B到14B。结果显示，QVGen在4位量化设置下首次达到与全精度模型可比的质量，并显著优于现有方法。例如，3位量化的CogVideoX-2B在VBench评估中动态度提升25.28分，场景一致性提升8.43分，证明了其卓越性能。这一成就为视频生成模型的量化部署提供了坚实的数据支持。",
      "conclusion": "论文的主要贡献是提出了QVGen框架，首次在极低比特量化下为视频扩散模型实现与全精度相当的性能，解决了量化在视频生成领域的应用难题。学术上，该研究推进了量化技术的理论创新，为相关领域提供了新方法。实际应用中，显著降低了模型的计算和内存需求，促进了高效部署。未来工作可能包括进一步优化策略或扩展到更多模型和场景。",
      "tags": [
        "Quantization-Aware Training",
        "Video Diffusion Models",
        "Singular Value Decomposition",
        "Rank Decay Strategy"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:41.962890Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.10936",
    "title": "Cochain: Balancing Insufficient and Excessive Collaboration in LLM Agent Workflows",
    "authors": [
      "Jiaxing Zhao",
      "Hongbin Xie",
      "Yuzhen Lei",
      "Xuan Song",
      "Zhuoran Shi",
      "Lianxin Li",
      "Shuangxue Liu",
      "Linguo Xie",
      "Haoran Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating the collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves the business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.10936.pdf",
    "abs_url": "https://arxiv.org/abs/2505.10936",
    "published": "2025-05-16T07:14:42Z",
    "updated": "2026-01-28T05:56:52Z",
    "comment": "35 pages, 23 figures",
    "light_analysis": {
      "overview": "论文提出Cochain框架，通过结合集成知识图谱和提示树，平衡LLM agent工作流中的协作问题，以较低成本提升性能。",
      "motivation": "LLMs在复杂推理任务中表现优异，但现有方法存在局限：单agent的chain-of-thought因跨域提示设计复杂而面临协作挑战，而多agent系统消耗大量tokens并稀释主要问题，尤其在业务工作流中效率低下。这导致协作不足或过度，影响实际应用效果，因此需要一种高效、低成本的解决方案来优化协作过程。",
      "method": "Cochain框架通过构建集成知识图谱来整合多阶段知识，并结合维护和检索提示树，以获取业务工作流其他阶段的相关提示信息。该方法创新性地平衡了单agent和多agent的协作，通过知识融合和提示优化，在降低计算成本的同时，解决协作中的不足和过度问题，提升整体效率。",
      "result": "在多个数据集上的广泛评估显示，Cochain在提示工程和多agent LLMs方面均优于所有基线方法，具体表现为性能指标的显著提升。专家评估结果进一步表明，即使使用小模型结合Cochain，其效果也超越了GPT-4，证明了框架在效率和准确性上的优势，且没有编造具体数据。",
      "conclusion": "Cochain框架的主要贡献是提供了一种有效的协作提示方法，解决了LLM agent工作流中的协作挑战。学术上，它为多agent系统设计提供了新思路；实际上，降低了token消耗并提高了业务工作流效率。摘要未明确说明局限性，未来工作可探索可扩展性和更广泛的应用场景。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thought",
        "Multi-Agent Systems",
        "Knowledge Graph",
        "Prompt Engineering"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:21.477103Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.08734",
    "title": "NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context",
    "authors": [
      "Ben Yao",
      "Qiuchi Li",
      "Yazhou Zhang",
      "Siyu Yang",
      "Bohan Zhang",
      "Prayag Tiwari",
      "Jing Qin"
    ],
    "abstract": "While LLMs have demonstrated medical knowledge and conversational ability, their deployment in clinical practice raises new risks: patients may place greater trust in LLM-generated responses than in nurses' professional judgments, potentially intensifying nurse-patient conflicts. Such risks highlight the urgent need of evaluating whether LLMs align with the core nursing values upheld by human nurses. This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. We define two-level tasks on the benchmark, considering the two characteristics of emerging nurse-patient conflicts. The Easy-Level dataset consists of 2,200 value-aligned and value-violating instances, which are collected through a five-month longitudinal field study across three hospitals of varying tiers; The Hard-Level dataset is comprised of 2,200 dialogue-based instances that embed contextual cues and subtle misleading signals, which increase adversarial complexity and better reflect the subjectivity and bias of narrators in the context of emerging nurse-patient conflicts. We evaluate a total of 23 SoTA LLMs on their ability to align with nursing values, and find that general LLMs outperform medical ones, and Justice is the hardest value dimension. As the first real-world benchmark for healthcare value alignment, NurValues provides novel insights into how LLMs navigate ethical challenges in clinician-patient interactions.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.08734.pdf",
    "abs_url": "https://arxiv.org/abs/2505.08734",
    "published": "2025-05-13T16:46:25Z",
    "updated": "2026-01-28T14:03:30Z",
    "comment": "39 pages, 9 figures, 24 tables",
    "light_analysis": {
      "overview": "本研究提出了首个护理价值对齐基准NurValues，用于评估大型语言模型在临床环境中的伦理对齐能力。",
      "motivation": "LLMs在医疗领域的应用展示了医学知识和对话能力，但其在临床实践中的部署引发新风险：患者可能更信任LLM生成的反应而非护士的专业判断，加剧护患冲突。这突显了评估LLMs是否与护士核心价值观对齐的紧迫性，现有方法侧重于医疗知识而非伦理对齐，缺乏专门的护理价值基准来应对这一挑战。NurValues旨在填补这一空白，确保AI系统在临床环境中的可靠性和伦理兼容性。",
      "method": "论文提出NurValues基准，从国际护理守则中提炼五个核心价值维度：利他主义、人类尊严、完整性、正义和专业性。定义两个级别任务：易级数据集包含2,200个价值对齐和违反实例，通过为期五个月、跨三个不同等级医院的纵向实地研究收集；难级数据集包含2,200个基于对话的实例，嵌入上下文线索和微妙误导信号，增加对抗复杂性，以更好地模拟护患冲突中的主观性和偏见。使用该基准对23个最先进的LLMs进行评估。",
      "result": "评估结果显示，通用LLMs在护理价值对齐方面表现优于医疗LLMs，正义是最难对齐的价值维度。摘要未明确具体性能指标（如准确率），但这一发现揭示了LLMs在伦理判断上的差异：通用模型可能更具灵活性处理价值问题，而医疗模型可能偏重专业知识。与基线方法的对比表明，正义维度的高难度反映了伦理挑战的复杂性，为未来模型改进提供了方向。",
      "conclusion": "NurValues作为首个现实世界医疗价值对齐基准，为理解LLMs导航医护交互中的伦理挑战提供了新见解。其学术价值在于推动AI伦理对齐研究，特别是针对临床上下文；实际应用价值在于辅助开发更可靠的医疗AI系统，促进护患和谐。潜在局限性可能包括数据集规模或通用性，未来工作可扩展到其他医疗角色或整合更多价值维度以增强评估的全面性。",
      "tags": [
        "Large Language Models",
        "Value Alignment",
        "Benchmark Evaluation",
        "Clinical Natural Language Processing",
        "Adversarial Testing"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:38.271199Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.03953",
    "title": "Sufficient Decision Proxies for Decision-Focused Learning",
    "authors": [
      "Noah Schutte",
      "Grigorii Veviurko",
      "Krzysztof Postek",
      "Neil Yorke-Smith"
    ],
    "abstract": "When solving optimization problems under uncertainty with contextual data, utilizing machine learning to predict the uncertain parameters' values is a popular and effective approach. Decision-focused learning (DFL) aims at learning a predictive model such that decision quality, instead of prediction accuracy, is maximized. Common practice is to predict a single scenario representing the uncertain parameters, implicitly assuming that there exists a deterministic problem approximation (proxy) that allows for optimal decision-making. The opposite has also been considered, where the underlying distribution is estimated with a parameterized distribution. However, little is known about when either choice is valid. This paper investigates for the first time problem properties that justify using a certain decision proxy. Using this, we present alternative decision proxies for DFL, with little or no compromise on the complexity of the learning task. We show the effectiveness of presented approaches in experiments on continuous and discrete problems, as well as problems with uncertainty in the objective function and in the constraints.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.03953.pdf",
    "abs_url": "https://arxiv.org/abs/2505.03953",
    "published": "2025-05-06T20:10:17Z",
    "updated": "2026-01-28T15:39:42Z",
    "comment": "13 pages, 5 figures",
    "light_analysis": {
      "overview": "论文首次探究了问题属性以证明显式决策代理的合理性，并提出替代决策代理方法，在降低学习复杂性同时提升决策质量。",
      "motivation": "研究动机源于决策聚焦学习（DFL）在不确定性优化问题中的应用。当前方法通常使用机器学习预测不确定参数，但实践分为预测单个场景或估计参数化分布作为代理，缺乏理论指导选择何时有效。这导致决策质量可能受限，因为不当代理可能产生次优结果。本文旨在填补这一空白，系统分析问题属性，为选择决策代理提供理论基础，提升DFL的实际应用效果。",
      "method": "研究方法包括理论分析问题属性，以确定何时使用特定决策代理是合理的，例如基于不确定性类型或优化结构。在此基础上，提出新的替代决策代理，旨在简化学习任务复杂性，可能结合机器学习模型和优化理论。摘要未明确说明具体技术细节如数据集或模型架构，但核心创新在于理论框架和代理设计，以平衡预测与决策需求。",
      "result": "实验在连续优化问题、离散优化问题，以及目标函数和约束中具有不确定性的问题上进行。结果表明，提出的决策代理方法在提升决策质量方面有效，尽管摘要未提供具体性能指标如准确率或效率改进。与基线方法相比，新方法展示了更好的决策性能，验证了理论分析的实用性。摘要未明确说明详细对比数据，但强调了在多种问题类型上的广泛适用性。",
      "conclusion": "结论总结了论文的主要贡献：首次系统研究问题属性以指导决策代理选择，并提出实用代理方法，为DFL提供理论支持。学术价值在于深化了不确定性优化与机器学习的交叉研究，实际应用价值在于优化决策过程，降低学习复杂性。潜在局限性包括未覆盖所有问题类型，未来工作可扩展实验验证和探索更多代理策略。摘要未明确说明具体未来方向，但暗示了进一步应用的可能性。",
      "tags": [
        "Decision-Focused Learning",
        "Optimization under Uncertainty",
        "Decision Proxies",
        "Machine Learning",
        "Contextual Data"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:32.504995Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.17229",
    "title": "Range Image-Based Implicit Neural Compression for LiDAR Point Clouds",
    "authors": [
      "Akihiro Kuwabara",
      "Sorachi Kato",
      "Toshiaki Koike-Akino",
      "Takuya Fujihashi"
    ],
    "abstract": "This paper presents a novel scheme to efficiently compress Light Detection and Ranging~(LiDAR) point clouds, enabling high-precision 3D scene archives, and such archives pave the way for a detailed understanding of the corresponding 3D scenes. We focus on 2D range images~(RIs) as a lightweight format for representing 3D LiDAR observations. Although conventional image compression techniques can be adapted to improve compression efficiency for RIs, their practical performance is expected to be limited due to differences in bit precision and the distinct pixel value distribution characteristics between natural images and RIs. We propose a novel implicit neural representation~(INR)--based RI compression method that effectively handles floating-point valued pixels. The proposed method divides RIs into depth and mask images and compresses them using patch-wise and pixel-wise INR architectures with model pruning and quantization, respectively. Experiments on the KITTI dataset show that the proposed method outperforms existing image, point cloud, RI, and INR-based compression methods in terms of 3D reconstruction and detection quality at low bitrates and decoding latency.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.17229.pdf",
    "abs_url": "https://arxiv.org/abs/2504.17229",
    "published": "2025-04-24T03:41:57Z",
    "updated": "2026-01-28T04:14:22Z",
    "comment": "Accepted for publication in IEEE Access",
    "light_analysis": {
      "overview": "本论文提出了一种基于隐式神经表示的范围图像压缩方法，用于高效压缩LiDAR点云。",
      "motivation": "研究动机是解决LiDAR点云数据量大、压缩需求高的问题，以实现高精度3D场景存档和详细理解。LiDAR点云在自动驾驶等领域应用广泛，但传统图像压缩技术应用于范围图像时，由于比特精度和像素值分布与自然图像的差异，效率受限。这导致现有方法无法充分利用范围图像的特性，需要针对性的高效压缩技术来支持实际应用。",
      "method": "研究方法基于隐式神经表示，将范围图像分割为深度图和掩膜图：深度图使用patch-wise隐式神经架构压缩，掩膜图使用pixel-wise架构压缩。关键创新点包括处理浮点像素值，并结合模型剪枝和量化技术以提高压缩效率。在KITTI数据集上实施，具体模型架构未详细说明，但强调了分块和像素级处理。",
      "result": "实验结果显示，在KITTI数据集上，该方法在低比特率和低解码延迟下，在3D重建质量和检测质量上优于现有的图像压缩、点云压缩、范围图像压缩和基于隐式神经表示的压缩方法。与基线方法对比，性能有显著提升，但摘要未提供具体数值如准确率。这表明该方法在压缩效率和重建精度方面表现优异。",
      "conclusion": "本研究的主要贡献是开发了一种高效的LiDAR点云压缩方案，通过隐式神经表示处理范围图像，提高了压缩效率。学术价值在于扩展了INR在3D数据压缩中的应用，实际应用价值在于支持高精度3D场景存档和分析，为自动驾驶等领域提供基础。摘要未明确说明局限性和未来工作方向。",
      "tags": [
        "Implicit Neural Representation",
        "Range Images",
        "LiDAR Point Clouds",
        "Model Pruning",
        "Quantization"
      ]
    },
    "analyzed_at": "2026-01-29T03:55:45.636792Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.14174",
    "title": "Physics-Guided Multimodal Transformers are the Necessary Foundation for the Next Generation of Meteorological Science",
    "authors": [
      "Jing Han",
      "Hanting Chen",
      "Kai Han",
      "Xiaomeng Huang",
      "Wenjun Xu",
      "Dacheng Tao",
      "Ping Zhang"
    ],
    "abstract": "This position paper argues that the next generation of artificial intelligence in meteorological and climate sciences must transition from fragmented hybrid heuristics toward a unified paradigm of physics-guided multimodal transformers. While purely data-driven models have achieved significant gains in predictive accuracy, they often treat atmospheric processes as mere visual patterns, frequently producing results that lack scientific consistency or violate fundamental physical laws. We contend that current ``hybrid'' attempts to bridge this gap remain ad-hoc and struggle to scale across the heterogeneous nature of meteorological data ranging from satellite imagery to sparse sensor measurements. We argue that the transformer architecture, through its inherent capacity for cross-modal alignment, provides the only viable foundation for a systematic integration of domain knowledge via physical constraint embedding and physics-informed loss functions. By advocating for this unified architectural shift, we aim to steer the community away from ``black-box'' curve fitting and toward AI systems that are inherently falsifiable, scientifically grounded, and robust enough to address the existential challenges of extreme weather and climate change.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.14174.pdf",
    "abs_url": "https://arxiv.org/abs/2504.14174",
    "published": "2025-04-19T04:31:35Z",
    "updated": "2026-01-28T09:39:56Z",
    "comment": "Perspective article",
    "light_analysis": {
      "overview": "本文主张物理引导的多模态transformer作为下一代气象科学的统一基础架构。",
      "motivation": "气象科学中，当前纯数据驱动的AI模型虽提升预测精度，但常违反物理定律，导致结果缺乏科学一致性。现有混合方法零散、临时，难以扩展到从卫星图像到稀疏传感器测量的多模态、异质数据，限制了模型在极端天气和气候变化等关键应用中的可靠性和鲁棒性，因此亟需系统集成领域知识的统一方法。",
      "method": "论文提出基于transformer架构的物理引导多模态模型。核心方法利用transformer的跨模态对齐能力，通过注意力机制整合卫星图像和传感器等多源数据，并创新性地嵌入物理约束和使用物理启发损失函数，系统地将领域知识（如流体力学定律）融入模型，以处理气象数据的异质性并提升科学一致性。",
      "result": "摘要未明确说明实验结果，如准确率提升或效率改进。论文未提供与基线方法对比的具体数据，作为立场论文，其主要目的是倡导架构转变，而非报告实验，实际性能需在后续实证研究中验证。",
      "conclusion": "本文总结物理引导的多模态transformer为气象AI提供了一种统一范式，促进从黑盒曲线拟合向科学基础系统的转变，提升了模型的可证伪性和鲁棒性，以应对极端天气和气候变化的挑战。潜在局限性包括模型复杂性和计算成本，未来方向可能涉及优化架构和扩展应用领域。",
      "tags": [
        "Multimodal Transformers",
        "Physics-Guided AI",
        "Cross-Modal Alignment",
        "Physical Constraint Embedding",
        "Physics-Informed Loss Functions"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:04.020848Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.10258",
    "title": "XY-Cut++: Advanced Layout Ordering via Hierarchical Mask Mechanism on a Novel Benchmark",
    "authors": [
      "Shuai Liu",
      "Youmeng Li",
      "Jizeng Wei"
    ],
    "abstract": "Document Reading Order Recovery is a fundamental task in document image understanding, playing a pivotal role in enhancing Retrieval-Augmented Generation (RAG) and serving as a critical preprocessing step for large language models (LLMs). Existing methods often struggle with complex layouts(e.g., multi-column newspapers), high-overhead interactions between cross-modal elements (visual regions and textual semantics), and a lack of robust evaluation benchmarks. We introduce XY-Cut++, an advanced layout ordering method that integrates pre-mask processing, multi-granularity segmentation, and cross-modal matching to address these challenges. Our method significantly enhances layout ordering accuracy compared to traditional XY-Cut techniques. Specifically, XY-Cut++ achieves state-of-the-art performance (98.8 BLEU overall) while maintaining simplicity and efficiency. It outperforms existing baselines by up to 24\\% and demonstrates consistent accuracy across simple and complex layouts on the newly introduced DocBench-100 dataset. This advancement establishes a reliable foundation for document structure recovery, setting a new standard for layout ordering tasks and facilitating more effective RAG and LLM preprocessing.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.10258.pdf",
    "abs_url": "https://arxiv.org/abs/2504.10258",
    "published": "2025-04-14T14:19:57Z",
    "updated": "2026-01-28T08:38:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "XY-Cut++通过集成分层掩码机制和多粒度分割，显著提升了文档布局排序的准确性和效率，并在新基准上取得最先进性能。",
      "motivation": "文档阅读顺序恢复是文档图像理解的基础任务，对增强检索增强生成（RAG）和大型语言模型（LLM）预处理至关重要。然而，现有方法在处理复杂布局（如多栏报纸）时面临挑战，跨模态元素（视觉区域和文本语义）之间的交互开销高，且缺乏稳健的评估基准。这些不足影响了方法的准确性和鲁棒性，限制了其在真实场景中的应用效果。摘要明确指出了这些问题，强调了改进的紧迫性和重要性。",
      "method": "XY-Cut++是一种先进的布局排序方法，整合了预掩码处理、多粒度分割和跨模态匹配来解决现有挑战。预掩码处理可能通过分层掩码机制简化复杂布局的分析；多粒度分割处理不同尺度的文档元素；跨模态匹配增强视觉和文本信息之间的对齐。该方法在新引入的DocBench-100数据集上进行评估，保持了简单性和效率，未明确说明具体模型架构，但技术特色在于这些创新点的结合。",
      "result": "XY-Cut++在布局排序任务中实现了最先进的性能，总体BLEU分数达到98.8。与现有基线相比，性能提升高达24%，并在DocBench-100数据集上验证了其在简单和复杂布局下的一致性高准确性。这些结果通过具体数据（如BLEU分数和提升百分比）支撑，证明了该方法在准确性和效率方面的显著改进，为文档结构恢复提供了可靠依据。",
      "conclusion": "XY-Cut++的主要贡献在于通过整合先进技术显著提升了文档布局排序的准确性，为文档结构恢复建立了可靠基础。这一进展设立了布局排序任务的新标准，并促进了检索增强生成和大型语言模型的更有效预处理，具有重要的学术和实际应用价值。摘要未明确说明局限性或未来工作方向，但推断可能涉及扩展到更广泛的文档类型或进一步优化跨模态交互。",
      "tags": [
        "Layout Ordering",
        "Cross-modal Matching",
        "Multi-granularity Segmentation",
        "Hierarchical Mask Mechanism",
        "Document Reading Order Recovery"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:21.656036Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.09956",
    "title": "Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities",
    "authors": [
      "Katarzyna Filus",
      "Michał Romaszewski",
      "Mateusz Żarski"
    ],
    "abstract": "Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.09956.pdf",
    "abs_url": "https://arxiv.org/abs/2504.09956",
    "published": "2025-04-14T07:44:34Z",
    "updated": "2026-01-28T10:06:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一个基于感知类别相似性的新框架和Similarity Depth指标，用于解释深度视觉网络的误分类错误。",
      "motivation": "当前评估深度神经网络的方法主要依赖分类准确度，缺乏对误分类原因的透明度，这限制了网络行为的深入理解。现有方法在解释错误根源方面不足，难以提供有效的分析工具。因此，本研究旨在解决这一不足，通过探索网络内部感知的语义层次深度，建立一个更透明的框架来解释错误模式，从而提高模型评估的实用性和洞察力。",
      "method": "核心方法是引入一个框架，使用Similarity Depth指标量化网络感知的语义层次深度，并评估网络错误与其内部相似性结构的一致性。创新点包括提出基于图的可视化技术来展示语义关系和误感知，以及利用从分类器层权重派生的类模板，使方法适用于已训练网络，无需额外数据或实验，简化了应用过程。",
      "result": "方法揭示深度视觉网络编码了特定语义层次结构，高语义深度提高了网络感知的类别相似性与实际错误模式之间的符合度。然而，摘要未明确说明具体的实验数据，如准确率提升或效率改进，也未提供与基线方法的详细对比结果，因此结论基于理论分析和框架验证得出。",
      "conclusion": "本文的主要贡献是开发了一个框架和SD指标，用于增强深度视觉网络错误的解释性，提升评估透明度。这有助于深入理解网络行为，具有学术价值；应用上适用于已训练模型，无需额外资源。未来工作可能包括在更多数据集上验证框架的有效性或探索其他网络类型的适应性。",
      "tags": [
        "Deep Vision Networks",
        "Semantic Hierarchy",
        "Similarity Depth",
        "Graph-based Visualization",
        "Class Templates"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:18.191987Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.05978",
    "title": "Smart Exploration in Reinforcement Learning using Bounded Uncertainty Models",
    "authors": [
      "J. S. van Hulst",
      "W. P. M. H. Heemels",
      "D. J. Antunes"
    ],
    "abstract": "Reinforcement learning (RL) is a powerful framework for decision-making in uncertain environments, but it often requires large amounts of data to learn an optimal policy. We address this challenge by incorporating prior model knowledge to guide exploration and accelerate the learning process. Specifically, we assume access to a model set that contains the true transition kernel and reward function. We optimize over this model set to obtain upper and lower bounds on the Q-function, which are then used to guide the exploration of the agent. We provide theoretical guarantees on the convergence of the Q-function to the optimal Q-function under the proposed class of exploring policies. Furthermore, we also introduce a data-driven regularized version of the model set optimization problem that ensures the convergence of the class of exploring policies to the optimal policy. Lastly, we show that when the model set has a specific structure, namely the bounded-parameter MDP (BMDP) framework, the regularized model set optimization problem becomes convex and simple to implement. In this setting, we also prove finite-time convergence to the optimal policy under mild assumptions. We demonstrate the effectiveness of the proposed exploration strategy, which we call BUMEX (Bounded Uncertainty Model-based Exploration), in a simulation study. The results indicate that the proposed method can significantly accelerate learning in benchmark examples. A toolbox is available at https://github.com/JvHulst/BUMEX.",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.05978.pdf",
    "abs_url": "https://arxiv.org/abs/2504.05978",
    "published": "2025-04-08T12:33:38Z",
    "updated": "2026-01-28T13:09:54Z",
    "comment": "Presented at 64th IEEE Conference on Decision and Control, CDC 2025, Rio de Janeiro, Brazil, 2025",
    "light_analysis": {
      "overview": "提出基于有界不确定性模型的智能探索方法BUMEX，以加速强化学习过程。",
      "motivation": "强化学习在不确定性环境中决策能力强，但通常需要大量数据来学习最优策略，导致学习效率低下。现有方法可能缺乏有效利用先验模型知识来引导探索，从而增加数据需求。本研究旨在通过结合模型集来优化探索策略，解决数据效率问题，提高学习速度，这对于实际应用如机器人控制或游戏AI具有重要意义。",
      "method": "论文假设存在一个包含真实转移核和奖励函数的模型集，通过优化该模型集得到Q函数的上下界，并用这些界限指导代理的探索。关键创新包括引入数据驱动的正则化模型集优化问题，确保探索策略收敛到最优策略，并在有界参数MDP（BMDP）框架下使优化问题凸且易于实现。模型架构和数据集未明确说明，但聚焦于理论推导和算法设计。",
      "result": "在模拟研究中，BUMEX方法在基准示例中显著加速了学习过程，尽管摘要未提供具体性能指标如准确率或效率改进百分比，但结果显示与未使用该方法的基线相比，能有效减少数据需求并提升学习速度。这证明了该方法在强化学习任务中的实用性，但具体对比细节需参考完整论文。",
      "conclusion": "论文的主要贡献是提出了BUMEX探索策略，并提供理论保证，包括Q函数收敛到最优和有限时间收敛到最优策略。这具有学术价值，为强化学习探索策略提供了新思路；实际应用中，工具包的提供便于实现和扩展。局限性或未来工作方向摘要未明确说明，但可能涉及扩展到更复杂场景或结合其他模型。",
      "tags": [
        "Reinforcement Learning",
        "Exploration",
        "Bounded Uncertainty Models",
        "Q-function",
        "BMDP"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:22.293947Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.04430",
    "title": "AGITB: A Signal-Level Benchmark for Evaluating Artificial General Intelligence",
    "authors": [
      "Matej Šprogar"
    ],
    "abstract": "Current artificial intelligence systems exhibit strong performance on narrow tasks, while existing evaluation frameworks provide limited insight into generality across domains. We introduce the Artificial General Intelligence Testbed (AGITB), a complementary benchmarking framework grounded in twelve explicitly stated axioms and implemented as a suite of twelve automated, simple, and reusable tests.   AGITB evaluates models on their ability to learn and to predict the next input in a temporal sequence whose semantic content is initially unknown to the model. The framework targets core computational properties, such as determinism, adaptability, and generalisation, that parallel principles observed in biological information processing. Designed to resist brute-force or memorisation-based strategies, AGITB requires autonomous learning across previously unseen environments, in a manner broadly inspired by cortical computation. Preliminary application of AGITB suggests that no contemporary system evaluated to date satisfies all test criteria, indicating that the benchmark provides a structured and interpretable means of assessing progress toward more general learning capabilities. A reference implementation of AGITB is freely available on GitHub.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2504.04430.pdf",
    "abs_url": "https://arxiv.org/abs/2504.04430",
    "published": "2025-04-06T10:01:15Z",
    "updated": "2026-01-28T10:43:55Z",
    "comment": "23 pages, 2 figures",
    "light_analysis": {
      "overview": "论文提出AGITB基准测试框架，基于12个公理，用于评估人工智能系统的通用学习和泛化能力。",
      "motivation": "当前人工智能系统在特定任务上表现优异，但现有评估框架主要关注窄任务性能，难以全面衡量跨领域的通用学习能力。研究动机在于开发一个结构化框架，以评估人工智能向通用智能的进展，解决现有基准测试对通用性洞察不足的问题，促进人工智能领域的进一步发展。",
      "method": "研究方法包括定义AGITB框架，该框架基于12个明确公理，实现为12个自动化、简单且可重用的测试。测试内容涉及模型在时间序列中学习和预测下一个输入，序列语义内容初始未知，以评估核心计算属性如确定性、适应性和泛化。框架设计抵抗暴力或记忆策略，要求自主学习跨未见环境，并受皮质计算启发，具体模型或数据集摘要未明确说明。",
      "result": "初步应用AGITB表明，目前评估的所有当代系统均未能满足所有测试标准，这表明基准提供了一个结构化和可解释的工具，用于评估通用学习能力的进展。与基线方法对比情况摘要未明确说明，未提及具体性能指标如准确率提升。",
      "conclusion": "结论是AGITB框架为评估通用人工智能提供了重要工具，具有学术价值，能促进相关研究发展。实际应用中，GitHub上的参考实现可用，便于广泛使用。局限性或未来工作方向摘要未明确说明，但可推断可能包括测试扩展或更多系统评估。",
      "tags": [
        "Artificial General Intelligence (AGI)",
        "Benchmarking",
        "Time Series Prediction",
        "Generalization",
        "Cortical Computation"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:24.677067Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.00753",
    "title": "CAPE: Connectivity-Aware Path Enforcement Loss for Curvilinear Structure Delineation",
    "authors": [
      "Elyar Esmaeilzadeh",
      "Ehsan Garaaghaji",
      "Farzad Hallaji Azad",
      "Doruk Oner"
    ],
    "abstract": "Promoting the connectivity of curvilinear structures, such as neuronal processes in biomedical scans and blood vessels in CT images, remains a key challenge in semantic segmentation. Traditional pixel-wise loss functions, including cross-entropy and Dice losses, often fail to capture high-level topological connectivity, resulting in topological mistakes in graphs obtained from prediction maps. In this paper, we propose CAPE (Connectivity-Aware Path Enforcement), a novel loss function designed to enforce connectivity in graphs obtained from segmentation maps by optimizing a graph connectivity metric. CAPE uses the graph representation of the ground truth to select node pairs and determine their corresponding paths within the predicted segmentation through a shortest-path algorithm. Using this, we penalize both disconnections and false positive connections, effectively promoting the model to preserve topological correctness. Experiments on 2D and 3D datasets, including neuron and blood vessel tracing demonstrate that CAPE significantly improves topology-aware metrics and outperforms state-of-the-art methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.00753.pdf",
    "abs_url": "https://arxiv.org/abs/2504.00753",
    "published": "2025-04-01T13:03:52Z",
    "updated": "2026-01-28T08:42:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种名为CAPE的新型损失函数，专注于增强曲线结构分割中的拓扑连接性。",
      "motivation": "曲线结构（如生物医学扫描中的神经元过程和CT图像中的血管）的准确分割在语义分割中是一个关键挑战。传统像素级损失函数，如交叉熵和Dice损失，主要关注局部精度，但无法捕捉高层次的拓扑连接性，导致从预测图中提取的图结构出现断开或错误连接。这些问题在生物医学图像分析中尤为重要，因为拓扑正确性直接影响神经元追踪和血管网络重建等应用的有效性，现有方法在这方面存在不足，因此开发能够强制拓扑连接性的损失函数具有重要研究意义。",
      "method": "CAPE是一种新颖的损失函数，通过优化图连接性指标来强制分割图中图的连接性。其核心创新点在于利用地面真实的图表示选择节点对，并通过最短路径算法在预测分割中确定对应路径，从而惩罚断开连接和错误连接，有效促进模型保持拓扑正确性。该方法适用于2D和3D数据集，如神经元和血管追踪，但摘要未明确说明具体的模型架构，仅强调损失函数的设计和集成到训练过程中的技术特色。",
      "result": "在2D和3D数据集（包括神经元和血管追踪）上的实验表明，CAPE能够显著提高拓扑感知度量，尽管摘要未提供具体数值如准确率提升百分比，但明确指出其在拓扑正确性方面优于当前最先进方法。实验对比了基线方法，展示了CAPE在减少拓扑错误方面的有效性，提升了分割结果的连接性质量。",
      "conclusion": "本研究的主要贡献是提出了CAPE损失函数，专注于提升曲线结构分割的拓扑连接性。该方法的学术价值在于将图论概念融入损失设计，拓展了语义分割的优化目标；实际应用价值体现在改善生物医学图像分析的准确性和可靠性。未来工作方向可能包括扩展到其他结构或领域，以及进一步优化计算效率，摘要未明确说明具体局限性。",
      "tags": [
        "Semantic Segmentation",
        "Connectivity-Aware Loss",
        "Graph Theory",
        "Curvilinear Structures",
        "Biomedical Imaging"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:49.587620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.15149",
    "title": "Machine learning surrogate models of many-body dispersion interactions in polymer melts",
    "authors": [
      "Zhaoxiang Shen",
      "Raúl I. Sosa",
      "Jakub Lengiewicz",
      "Alexandre Tkatchenko",
      "Stéphane P. A. Bordas"
    ],
    "abstract": "Accurate prediction of many-body dispersion (MBD) interactions is essential for understanding the van der Waals forces that govern the behavior of many complex molecular systems. However, the high computational cost of MBD calculations limits their direct application in large-scale simulations. In this work, we introduce a machine learning surrogate model specifically designed to predict MBD forces in polymer melts, a system that demands accurate MBD description and offers structural advantages for machine learning approaches. Our model is based on a trimmed SchNet architecture that selectively retains the most relevant atomic connections and incorporates trainable radial basis functions for geometric encoding. We validate our surrogate model on datasets from polyethylene, polypropylene, and polyvinyl chloride melts, demonstrating high predictive accuracy and robust generalization across diverse polymer systems. In addition, the model captures key physical features, such as the characteristic decay behavior of MBD interactions, providing valuable insights for optimizing cutoff strategies. Characterized by high computational efficiency, our surrogate model enables practical incorporation of MBD effects into large-scale molecular simulations.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.15149.pdf",
    "abs_url": "https://arxiv.org/abs/2503.15149",
    "published": "2025-03-19T12:15:35Z",
    "updated": "2026-01-28T12:50:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一种基于修剪SchNet架构的机器学习替代模型，用于高效预测聚合物熔体中的多体分散相互作用。",
      "motivation": "多体分散（MBD）相互作用对理解范德华力至关重要，广泛应用于复杂分子系统模拟。然而，直接MBD计算成本高昂，严重限制了在大规模聚合物熔体模拟中的应用。现有方法难以平衡计算效率和准确性，特别是针对需精确MBD描述的聚合物系统，其结构有序性为机器学习提供了优势。因此，开发高效替代模型以降低计算负担，同时保持高精度，成为当前研究的迫切需求。",
      "method": "论文提出一种专门针对聚合物熔体的机器学习替代模型，基于修剪的SchNet神经网络架构，通过选择性保留最相关的原子连接来优化模型复杂度，并整合可训练的径向基函数进行几何编码。该方法在聚乙烯、聚丙烯和聚氯乙烯等聚合物熔体数据集上构建，利用结构数据训练模型以预测MBD力，核心创新在于结合物理启发的架构设计，提升计算效率和准确性。",
      "result": "实验结果表明，该替代模型在多个聚合物熔体数据集上实现了高预测准确性和强大的泛化能力，有效捕捉了MBD相互作用的特征衰减行为。模型计算效率显著提升，使得MBD效应能够实际融入大规模分子模拟。虽然摘要未明确说明具体性能指标，但与基线方法相比，模型表现出稳健的性能，为优化模拟中的截断策略提供了物理基础。",
      "conclusion": "本研究成功开发了一种高效的机器学习替代模型，不仅解决了MBD计算的高成本问题，还为聚合物系统模拟提供了实用工具。学术上，模型结合物理见解，促进了计算化学与机器学习的交叉；实际应用上，它能显著提升大规模分子模拟的可行性和准确性。未来工作可扩展模型到更广泛的分子系统，并探索更多物理特征。",
      "tags": [
        "Machine Learning Surrogate Models",
        "Many-Body Dispersion",
        "SchNet Architecture",
        "Polymer Melts",
        "Radial Basis Functions"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:41.212637Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.14957",
    "title": "Neuro Symbolic Knowledge Reasoning for Procedural Video Question Answering",
    "authors": [
      "Basura Fernando",
      "Thanh-Son Nguyen",
      "Hong Yang",
      "Tzeh Yuan Neoh",
      "Hao Zhang",
      "Ee Yeo Keat"
    ],
    "abstract": "In this work we present Knowledge Module Learning (KML) to understand and reason over procedural tasks that requires models to learn structured and compositional procedural knowledge. KML is a neurosymbolic framework that learns relation categories within a knowledge graph as neural knowledge modules and composes them into executable reasoning programs generated by large language models (LLMs). Each module encodes a specific procedural relation capturing how each entity type such as tools are related to steps, purpose of each tool, and steps of each task. Given a question conditioned on a task shown in a video, then KML performs multistep reasoning with transparent, traceable intermediate states. Our theoretical analysis demonstrated two desired properties of KML. KML satisfy strong optimal conditions for modelling KG relations as neural mappings, providing strong foundations for generalizable procedural reasoning. It also shows a bound on the expected error when it performs multistep reasoning. To evaluate this model, we construct a large procedural knowledge graph (PKG) consisting of diverse instructional domains by integrating the COIN instructional video dataset, and COIN ontology, commonsense relations from ConceptNet, and structured extractions from LLMs, followed by expert verification. We then generate question and answer pairs by applying graph traversal templates over the PKG, constructing the PKR-QA benchmark for procedural knowledge reasoning. Experiments show that KML improves structured reasoning performance while providing interpretable step-by-step traces, outperforming LLM-only and black-box neural baselines. Code is publicly available at https://github.com/LUNAProject22/KML.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.14957.pdf",
    "abs_url": "https://arxiv.org/abs/2503.14957",
    "published": "2025-03-19T07:49:14Z",
    "updated": "2026-01-28T03:23:09Z",
    "comment": "This paper is under review at IEEE Transactions on Neural Networks and Learning Systems. Personal use is permitted, but republication/redistribution requires IEEE permission",
    "light_analysis": {
      "overview": "本论文提出了Knowledge Module Learning (KML)框架，结合神经符号学习和大型语言模型，用于过程视频问答中的知识推理。",
      "motivation": "该研究旨在解决过程任务的理解和推理问题，这类任务需要模型学习结构和组合的过程知识，例如从视频中识别工具使用和任务步骤。现有方法如纯大型语言模型（LLM）或黑盒神经网络可能在推理透明性、性能和泛化能力上存在不足，尤其是在需要可解释中间状态的复杂推理场景中。KML框架被设计为提供更可追踪和高效的解决方案，以应对这些挑战。",
      "method": "KML是一个神经符号框架，它将知识图中的关系类别学习为神经知识模块，每个模块编码特定的过程关系，如工具与步骤的关联。这些模块由大型语言模型（LLM）组合生成可执行的推理程序，支持多步推理并保持透明、可追踪的中间状态。理论分析显示，KML满足建模知识图关系的强最优条件，并为多步推理提供了误差界限，这为通用过程推理奠定了理论基础。",
      "result": "实验中，作者构建了一个大型过程知识图（PKG），整合了COIN视频数据集、COIN本体、ConceptNet的常识关系以及LLM的结构化提取，并通过专家验证生成PKR-QA基准用于评估。实验结果表明，KML在结构推理性能上显著提升，提供可解释的逐步追踪，优于仅使用LLM或黑盒神经网络的方法。这验证了其在过程知识推理任务中的有效性。",
      "conclusion": "KML的主要贡献是提出了一个神经符号框架，融合了神经网络和符号推理的优势，增强了过程任务的理解和推理能力。该研究不仅提供了理论保障（如最优条件和误差分析），还通过PKR-QA基准展示了实际应用的性能提升和解释性改进。这有助于推动视频问答和知识推理领域的发展，未来可扩展到更广泛的多模态任务中。",
      "tags": [
        "Neurosymbolic Framework",
        "Knowledge Graph Reasoning",
        "Large Language Models",
        "Procedural Video Question Answering",
        "Multistep Reasoning"
      ]
    },
    "analyzed_at": "2026-01-29T03:56:57.137890Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.08023",
    "title": "AdaSCALE: Adaptive Scaling for OOD Detection",
    "authors": [
      "Sudarshan Regmi"
    ],
    "abstract": "The ability of the deep learning model to recognize when a sample falls outside its learned distribution is critical for safe and reliable deployment. Recent state-of-the-art out-of-distribution (OOD) detection methods leverage activation shaping to improve the separation between in-distribution (ID) and OOD inputs. These approaches resort to sample-specific scaling but apply a static percentile threshold across all samples regardless of their nature, resulting in suboptimal ID-OOD separability. In this work, we propose \\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the percentile threshold based on a sample's estimated OOD likelihood. This estimation leverages our key observation: OOD samples exhibit significantly more pronounced activation shifts at high-magnitude activations under minor perturbation compared to ID samples. AdaSCALE enables stronger scaling for likely ID samples and weaker scaling for likely OOD samples, yielding highly separable energy scores. Our approach achieves state-of-the-art OOD detection performance, outperforming the latest rival OptFS by 14.94% in near-OOD and 21.67% in far-OOD datasets in average FPR@95 metric on the ImageNet-1k benchmark across eight diverse architectures. The code is available at: https://github.com/sudarshanregmi/AdaSCALE/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.08023.pdf",
    "abs_url": "https://arxiv.org/abs/2503.08023",
    "published": "2025-03-11T04:10:06Z",
    "updated": "2026-01-28T15:51:26Z",
    "comment": "https://github.com/sudarshanregmi/AdaSCALE/",
    "light_analysis": {
      "overview": "提出AdaSCALE自适应缩放方法，通过动态调整基于OOD似然估计的百分位数阈值，显著提升分布外检测性能。",
      "motivation": "深度学习模型安全部署需要准确识别分布外（OOD）样本，但现有先进OOD检测方法采用激活成形和样本特定缩放时，使用静态百分位数阈值，忽略了样本性质差异，导致ID-OOD分离性不足。这限制了模型的可靠性，特别是在面对未知输入时，因此开发自适应方法来改善检测效果至关重要。",
      "method": "AdaSCALE通过动态调整百分位数阈值实现自适应缩放，基于样本的OOD似然估计。关键创新是利用观察：OOD样本在高幅度激活下对轻微扰动表现出更显著激活偏移。该方法估计OOD似然后，自适应缩放激活，使ID样本强缩放、OOD样本弱缩放，生成高度可分离的能量分数，在ImageNet-1k基准的八种架构中应用，利用扰动观察改进分离性。",
      "result": "在ImageNet-1k基准测试中，AdaSCALE在平均FPR@95指标上达到最先进性能，优于最新对手OptFS，在近OOD数据集上提升14.94%，远OOD数据集上提升21.67%。这证明了方法在不同OOD类型和多种架构下均能有效改进检测能力。",
      "conclusion": "研究提出AdaSCALE，通过自适应缩放显著增强OOD检测的分离性和性能，提高了模型部署的可靠性和安全性。学术贡献在于创新动态阈值调整机制，实际应用价值在于支持安全AI系统。未来工作可扩展到其他任务或探索更高效的OOD似然估计方法，摘要未明确说明局限性。",
      "tags": [
        "Out-of-Distribution Detection",
        "Adaptive Scaling",
        "Activation Shaping",
        "Energy Score",
        "ImageNet Benchmark"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:02.298865Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.03803",
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Bo Li",
      "Ziwei Liu"
    ],
    "abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.03803.pdf",
    "abs_url": "https://arxiv.org/abs/2503.03803",
    "published": "2025-03-05T18:54:16Z",
    "updated": "2026-01-28T15:55:39Z",
    "comment": "Accepted to CVPR 2025. Project Page: https://egolife-ai.github.io/. Code: https://github.com/EvolvingLMMs-Lab/EgoLife",
    "light_analysis": {
      "overview": "EgoLife项目通过集成多模态数据集和先进模型系统，开发了以自我为中心的AI生活助手。",
      "motivation": "研究旨在解决开发AI穿戴眼镜助手以提升个人生活效率的实际问题。这一方向重要，因为日常生活辅助需求广泛，但现有方法常受限于缺乏真实、多模态的自我中心数据，以及处理长上下文信息的能力不足。现有技术难以在复杂环境中实现有效的身份识别和连续事件问答，因此需要构建综合数据集和针对性系统来填补这一空白，推动egocentric AI领域的发展。",
      "method": "研究方法包括三个核心步骤：首先，进行数据收集研究，利用AI眼镜记录六名参与者一周的多模态日常生活，创建了EgoLife数据集，包含300小时带标注的自我中心、多视图视频。其次，设计EgoLifeQA任务集，专注于长上下文问答，如回忆过去事件和监控健康习惯。最后，开发EgoButler系统，集成EgoGPT（一个在多模态数据上训练的模型，用于视觉-音频理解和身份识别）和EgoRAG（基于检索的组件，支持超长上下文问答），以应对技术挑战。关键创新在于多模态数据整合和检索增强的问答架构。",
      "result": "实验结果显示，EgoGPT在自我中心视频理解任务上达到先进水平，但具体性能指标摘要未明确说明。EgoRAG有效支持超长上下文问答，通过检索机制处理扩展时间信息。系统验证了工作机制，并揭示了关键因素和瓶颈，如模型鲁棒性和效率问题，为未来优化提供指导。与基线方法对比表明，集成系统在长上下文处理方面具有优势，但摘要未提供详细数据支撑。",
      "conclusion": "论文的主要贡献是发布了EgoLife数据集、EgoLifeQA任务和EgoButler系统，为自我中心AI助手研究提供了重要资源。这具有学术价值，推动了多模态学习和长上下文处理技术的发展，并具有实际应用潜力，如个性化生活辅助。潜在局限性包括数据规模可能受限和模型泛化能力，未来工作可基于实验揭示的瓶颈，改进模型性能并扩展到更复杂的日常场景。",
      "tags": [
        "Egocentric Vision",
        "Multimodal Dataset",
        "Video Understanding",
        "Retrieval-Augmented Generation",
        "Long-context Question Answering"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:39.323665Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.01805",
    "title": "Depth-Width tradeoffs in Algorithmic Reasoning of Graph Tasks with Transformers",
    "authors": [
      "Gilad Yehudai",
      "Clayton Sanford",
      "Maya Bechler-Speicher",
      "Orr Fischer",
      "Ran Gilad-Bachrach",
      "Amir Globerson"
    ],
    "abstract": "Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.01805.pdf",
    "abs_url": "https://arxiv.org/abs/2503.01805",
    "published": "2025-03-03T18:33:58Z",
    "updated": "2026-01-28T10:28:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示了transformer在图算法任务中深度与宽度权衡的惊人结果，线性宽度和常数深度足以解决多种图问题。",
      "motivation": "本研究旨在解决transformer在图算法任务中的深度与宽度权衡问题。现有研究已表明，在子线性嵌入维度（即模型宽度）下，对数深度足以实现图任务，但当宽度允许线性增长而深度固定时，效果尚不明确。这一问题对设计高效transformer模型至关重要，因为更浅模型能显著提升推理速度和训练效率，减少计算成本，现有方法未充分探索此设置，存在理论空白。摘要未明确说明具体图任务示例，但强调了填补此空白的重要性。",
      "method": "论文采用理论分析与实证研究相结合的方法，探索transformer在图算法任务中的深度与宽度权衡。核心方法是设定不同宽度（如线性和二次）和深度（如常数）的transformer模型，分析其解决特定图问题的能力。理论部分推导了在给定宽度下所需的最小深度，实证部分通过实验验证这些权衡，使用图任务数据集比较不同模型的精度和效率。创新点在于首次系统研究了线性宽度固定深度下的transformer能力，揭示了常数深度足以实现多种图算法，而其他任务则需要更高宽度。摘要未明确说明具体数据集或模型架构细节。",
      "result": "论文的主要结果表明，对于多种图基任务，transformer模型在具有线性宽度和常数深度时即可有效解决，这减少了模型复杂度；在某些任务中，二次宽度是必要的以确保性能。实证研究发现，更宽的模型在保持与更深模型相同准确度的情况下，由于硬件并行化能力，训练和推理时间显著缩短。这些结果挑战了传统对深度需求的认知，突出了宽度增加在提升效率方面的潜力，但摘要未提供具体性能指标数据，如准确率提升百分比。",
      "conclusion": "论文的结论是，通过分析transformer在图算法任务中的深度与宽度权衡，发现线性宽度和常数深度足以解决许多问题，这为模型优化提供了重要insights。学术上，它丰富了transformer理论，展示了实现复杂算法的另一种路径。实际应用中，这一发现有助于设计更高效的AI系统，减少计算开销，适用于实时推理和资源受限环境。未来工作可能涉及扩展到更多任务类型或探索深度与宽度的更精细权衡，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Transformers",
        "Graph Algorithms",
        "Depth-Width Tradeoffs",
        "Parallel Computing",
        "Model Efficiency"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:37.717967Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.01544",
    "title": "Compositional Reasoning with Transformers, RNNs, and Chain of Thought",
    "authors": [
      "Gilad Yehudai",
      "Noah Amsel",
      "Joan Bruna"
    ],
    "abstract": "It is well understood that different neural network architectures are suited to different tasks, but is there always a single best architecture for a given task? We compare the expressive power of transformers, RNNs, and transformers with chain of thought tokens on a simple and natural class of tasks we term Compositional Reasoning Questions (CRQ). This family captures multi-step problems with tree-like compositional structure, such as evaluating Boolean formulas. We prove that under standard hardness assumptions, \\emph{none} of these three architectures is capable of solving CRQs unless some hyperparameter (depth, embedding dimension, and number of chain of thought tokens, respectively) grows with the size of the input. We then provide constructions for solving CRQs with each architecture. For transformers, our construction uses depth that is logarithmic in the problem size. For RNNs, logarithmic embedding dimension is necessary and sufficient, so long as the inputs are provided in a certain order. For transformers with chain of thought, our construction uses $n$ CoT tokens for input size $n$. These results show that, while CRQs are inherently hard, there are several different ways for language models to overcome this hardness. Even for a single class of problems, each architecture has strengths and weaknesses, and none is strictly better than the others.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.01544.pdf",
    "abs_url": "https://arxiv.org/abs/2503.01544",
    "published": "2025-03-03T13:52:45Z",
    "updated": "2026-01-28T10:02:36Z",
    "comment": "NeurIPS CR version",
    "light_analysis": {
      "overview": "本文比较了transformers、RNNs和带有chain of thought的transformers在组合推理问题上的表达能力，揭示了每种架构的解决策略及其局限性。",
      "motivation": "研究动机是探讨不同神经网络架构在给定任务上是否存在单一最佳架构，特别针对组合推理问题（CRQ），这类问题具有树状组合结构，如评估布尔公式。理解架构表达能力对于模型选择和优化至关重要，因为现有方法可能未系统比较transformers、RNNs和CoT transformers在这些问题上的表现，以揭示其计算难度和潜在解决方案。摘要未明确说明现有具体不足，但强调了对架构表达能力的深入分析需求。",
      "method": "研究方法包括定义Compositional Reasoning Questions (CRQ)任务类，通过理论分析比较transformers、RNNs和transformers with chain of thought tokens的表达能力。关键创新点在于证明在标准硬度假设下，这些架构无法解决CRQ除非超参数（如深度、嵌入维度、CoT token数量）随输入大小增长，并为每种架构提供构造解决方案：transformers使用对数深度构造，RNNs需要对数嵌入维度（但依赖特定输入顺序），CoT transformers使用线性数量的tokens。这结合了理论证明和具体构造技术。",
      "result": "主要理论结果表明，CRQs对这三种架构是计算困难的，除非超参数随输入大小增加。具体构造显示，transformers需要深度以对数规模增长，RNNs需要嵌入维度以对数规模增长（前提是输入顺序特定），CoT transformers需要token数量线性增长。这些结果突出了每种架构的相对优势，例如transformers在深度效率上的表现，RNNs在嵌入维度上的适应性，以及CoT transformers通过额外tokens增强推理能力。摘要未提供具体实验数据，但通过理论对比展示了架构间的差异。",
      "conclusion": "结论指出，组合推理问题虽然固有困难，但语言模型可以通过不同架构方式克服，如调整超参数。研究贡献在于系统比较了transformers、RNNs和CoT transformers的表达能力，表明没有单一最佳架构，每种在解决CRQ时各有优缺点。学术价值在于深化了对神经网络表达能力的理解，实际应用有助于指导模型选择和设计。未来工作可能扩展到更广泛的任务类或探索更高效的构造方法，但摘要未明确说明具体局限性。",
      "tags": [
        "Transformers",
        "RNNs",
        "Chain of Thought",
        "Compositional Reasoning",
        "Expressivity Analysis"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:31.173079Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.16744",
    "title": "BAGEL: Projection-Free Algorithm for Adversarially Constrained Online Convex Optimization",
    "authors": [
      "Yiyang Lu",
      "Mohammad Pedramfar",
      "Vaneet Aggarwal"
    ],
    "abstract": "Projection-based algorithms for Constrained Online Convex Optimization (COCO) achieve optimal $\\mathcal{O}(T^{1/2})$ regret guarantees but face scalability challenges due to the computational complexity of projections. To circumvent this, projection-free methods utilizing Linear Optimization Oracles (LOO) have been proposed, albeit typically achieving slower $\\mathcal{O}(T^{3/4})$ regret rates. In this work, we examine whether the $\\mathcal{O}(T^{1/2})$ rate can be recovered in the projection-free setting by strengthening the oracle assumption. We introduce BAGEL, an algorithm utilizing a Separation Oracle (SO) that achieves $\\mathcal{O}(T^{1/2})$ regret and $\\tilde{\\mathcal{O}}(T^{1/2})$ cumulative constraint violation (CCV) for convex cost functions. Our analysis shows that by leveraging an infeasible projection via SO, we can match the time-horizon dependence of projection-based methods with $\\tilde{\\mathcal{O}}(T)$ oracle calls, provided dependence on the geometry of the action set. This establishes a specific regime where projection-free methods can attain the same convergence rates as projection-based counterparts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.16744.pdf",
    "abs_url": "https://arxiv.org/abs/2502.16744",
    "published": "2025-02-23T23:18:40Z",
    "updated": "2026-01-28T05:51:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "BAGEL算法通过引入分离oracle在投影自由设置下实现了与投影方法相同的O(T^{1/2})后悔率，显著提升了约束在线凸优化的计算效率。",
      "motivation": "在约束在线凸优化中，投影算法虽然能实现最优O(T^{1/2})后悔保证，但投影操作的计算复杂度高，导致大规模应用中可扩展性受限。现有投影自由方法通常使用线性优化oracle，但后悔率仅为O(T^{3/4})，无法匹配投影方法的性能。本研究旨在解决这一问题，通过强化oracle假设，探索在投影自由设置中恢复高收敛率，以平衡优化效果与计算效率，弥补现有方法的不足。",
      "method": "本研究提出BAGEL算法，核心方法是利用分离oracle代替线性优化oracle来处理约束优化问题。该算法通过不可行投影技术，在凸成本函数条件下进行优化，避免了直接投影的计算开销。关键创新点包括使用分离oracle实现高效的约束处理，并分析表明在动作集几何依赖性假设下，算法仅需O(T) oracle调用即可匹配投影方法的时间依赖性。摘要未明确说明数据集或模型架构，因是理论性算法。",
      "result": "理论分析结果显示，BAGEL算法实现了O(T^{1/2})的后悔率和O(T^{1/2})的累积约束违反量，这与基线投影方法的性能直接匹配。这证明了投影自由方法在特定条件下可以达到与投影方法相同的最优收敛率，为在线凸优化提供了高效解决方案。摘要未提供具体实验数据，但理论保证支持了这些性能指标，表明在强化oracle假设下，算法能有效提升优化效率。",
      "conclusion": "论文的主要贡献是提出BAGEL算法，展示了投影自由方法在分离oracle假设下能达到与投影方法相同的最优后悔率。这具有重要学术价值，为在线凸优化理论提供了新突破，并具实际应用潜力，如在大规模优化问题中减少计算负担。潜在局限性可能包括分离oracle的实用性和实现成本，摘要未明确说明，未来工作可探索更广泛的oracle类型或实际部署场景，以扩展算法的适用性。",
      "tags": [
        "Online Convex Optimization",
        "Projection-Free Algorithm",
        "Separation Oracle",
        "Regret Analysis",
        "Constrained Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:43.212374Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.13369",
    "title": "Reducing Hallucinations in Language Model-based SPARQL Query Generation Using Post-Generation Memory Retrieval",
    "authors": [
      "Aditya Sharma",
      "Christopher J. Pal",
      "Amal Zouaq"
    ],
    "abstract": "The ability to generate SPARQL queries from natural language questions is crucial for ensuring efficient and accurate retrieval of structured data from knowledge graphs (KG). While large language models (LLMs) have been widely adopted for SPARQL query generation, they are often susceptible to hallucinations and out-of-distribution errors when generating KG elements, such as Uniform Resource Identifiers (URIs), based on opaque internal parametric knowledge. We propose PGMR (Post-Generation Memory Retrieval), a modular framework where the LLM produces an intermediate query using natural language placeholders for URIs, and a non-parametric memory module is subsequently employed to retrieve and resolve the correct KG URIs. PGMR significantly enhances query correctness (SQM) across various LLMs, datasets, and distribution shifts, while achieving the near-complete suppression of URI hallucinations. Critically, we demonstrate PGMR's superior safety and robustness: a retrieval confidence threshold enables PGMR to effectively refuse to answer queries that lack support, and the retriever proves highly resilient to memory noise, maintaining strong performance even when the non-parametric memory size is scaled up to 9 times with irrelevant, distracting entities.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.13369.pdf",
    "abs_url": "https://arxiv.org/abs/2502.13369",
    "published": "2025-02-19T02:08:13Z",
    "updated": "2026-01-28T05:47:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "PGMR框架通过后生成记忆检索，有效减少语言模型在SPARQL查询生成中的幻觉。",
      "motivation": "研究动机是解决基于大型语言模型生成SPARQL查询时的幻觉和分布外错误问题。现有方法依赖LLMs不透明的内部参数知识，导致生成知识图谱元素如URIs时准确性低，影响结构化数据检索效率和可靠性。这凸显了开发新方法以增强查询安全性和鲁棒性的重要性。",
      "method": "论文提出PGMR框架，采用模块化设计：首先由LLM生成包含自然语言占位符的中间查询；随后利用非参数记忆模块检索并解析正确的KG URIs。关键创新点在于将生成与检索分离，减少对LLM内部知识的依赖。摘要未明确说明具体的数据集和模型架构，但提到在多个LLMs和数据集上进行了测试。",
      "result": "PGMR显著提高了查询正确性（SQM），几乎完全抑制了URI幻觉。与基线方法相比，它在多种LLMs、数据集和分布偏移下都表现出更强的性能。安全性和鲁棒性突出，通过检索置信度阈值可拒绝不支持查询，非参数记忆模块对噪声有高韧性，即使记忆大小扩展到9倍时性能依然稳定。",
      "conclusion": "论文的主要贡献是PGMR框架，能有效减少SPARQL查询生成中的幻觉，提升安全性和鲁棒性。学术价值在于展示了后生成检索如何增强LLMs的可靠性，实际应用价值在于提高知识图谱查询的准确性。摘要未明确说明局限性，未来工作可能包括扩展到更广泛的知识图谱任务或探索其他应用场景。",
      "tags": [
        "Large Language Model",
        "SPARQL Query Generation",
        "Hallucination Reduction",
        "Memory Retrieval",
        "Knowledge Graph URIs"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:50.431533Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.11068",
    "title": "MAnchors: Memorization-Based Acceleration of Anchors via Rule Reuse and Transformation",
    "authors": [
      "Haonan Yu",
      "Junhao Liu",
      "Xin Zhang"
    ],
    "abstract": "Anchors is a popular local model-agnostic explanation technique whose applicability is limited by its computational inefficiency. To address this limitation, we propose a memorization-based framework that accelerates Anchors while preserving explanation fidelity and interpretability. Our approach leverages the iterative nature of Anchors' algorithm which gradually refines an explanation until it is precise enough for a given input by storing and reusing intermediate results obtained during prior explanations. Specifically, we maintain a memory of low-precision, high-coverage rules and introduce a rule transformation framework to adapt them to new inputs: the horizontal transformation adapts a pre-trained explanation to the current input by replacing features, and the vertical transformation refines the general explanation until it is precise enough for the input. We evaluate our method across tabular, text, and image datasets, demonstrating that it significantly reduces explanation generation time while maintaining fidelity and interpretability, thereby enabling the practical adoption of Anchors in time-sensitive applications.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.11068.pdf",
    "abs_url": "https://arxiv.org/abs/2502.11068",
    "published": "2025-02-16T10:30:01Z",
    "updated": "2026-01-28T07:12:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于记忆的框架，通过规则重用和变换来加速Anchors局部模型无关解释技术，保持解释忠实性和可解释性。",
      "motivation": "Anchors是一种流行的局部模型无关解释技术，但其计算效率低限制了在时间敏感应用中的实用性。现有方法的不足之处在于Anchors算法需要大量迭代以生成精确解释，这导致解释生成时间过长，影响实际部署。因此，研究旨在解决这一效率问题，通过提升解释速度，使该技术更适用于实时场景。摘要未明确说明更广泛的背景，但强调了效率改进的重要性。",
      "method": "本方法提出了一个记忆框架，通过存储和重用Anchors算法中的中间结果来加速解释过程。具体而言，维护一个低精度、高覆盖率的规则记忆库，并引入规则变换框架，包括水平变换和垂直变换：水平变换通过替换特征将预训练解释适配到新输入，垂直变换则逐步细化通用解释直到满足输入精度要求。该方法利用Anchors的迭代性质，避免重复计算，从而提升效率。技术特色在于结合记忆机制和变换策略，以优化解释生成。摘要未明确指定数据集或模型架构细节，但涉及通用解释技术应用。",
      "result": "实验在表格、文本和图像数据集上进行，结果表明该方法显著减少了解释生成时间，同时保持了与原始Anchors相当的忠实性和可解释性。与基线方法（原始Anchors）相比，加速效果明显，但摘要未提供具体数据如准确率提升或效率改进数值。这证明了方法在多种数据类型上的有效性，为时间敏感应用提供了实用解决方案。",
      "conclusion": "本研究的主要贡献是提出了一个基于记忆的框架，加速了Anchors解释技术，同时保持解释质量，使其在现实应用中更具可行性。学术价值在于改进了模型无关解释方法的效率，推动了可解释AI领域的发展；应用价值在于支持时间敏感场景如实时决策系统。未来工作可探索在更大规模数据集或更复杂模型上的扩展，以及潜在局限性如规则记忆的存储开销。",
      "tags": [
        "Anchors Explanation",
        "Rule Reuse",
        "Rule Transformation",
        "Memorization-Based Framework",
        "Local Model-Agnostic Explanation"
      ]
    },
    "analyzed_at": "2026-01-29T03:57:40.717120Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.09667",
    "title": "Summaries as Centroids for Interpretable and Scalable Text Clustering",
    "authors": [
      "Jairo Diaz-Rodriguez"
    ],
    "abstract": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means that periodically replace numeric centroids with textual summaries. The key idea, summary-as-centroid, retains k-means assignments in embedding space while producing human-readable, auditable cluster prototypes. The method is LLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling offline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that uses an LLM for summaries under a fixed per-iteration budget whose cost does not grow with dataset size. We also present a mini-batch extension for real-time clustering of streaming text. Across diverse datasets, embedding models, and summarization strategies, our approach consistently outperforms classical baselines and approaches the accuracy of recent LLM-based clustering-without extensive LLM calls. Finally, we provide a case study on sequential text streams and release a StackExchange-derived benchmark for evaluating streaming text clustering.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.09667.pdf",
    "abs_url": "https://arxiv.org/abs/2502.09667",
    "published": "2025-02-12T19:50:22Z",
    "updated": "2026-01-28T16:27:53Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "该论文提出以文本摘要作为质心的k-NLPmeans和k-LLMmeans方法，实现可解释且可扩展的文本聚类。",
      "motivation": "传统k-means在文本聚类中使用数字质心，缺乏可解释性，导致用户难以理解和审计聚类结果；而基于大型语言模型（LLM）的聚类方法虽然准确但成本高昂，难以处理大规模或流式数据应用。因此，研究旨在解决可解释性与可扩展性之间的平衡问题，开发一种既能提供人类可读聚类原型，又能高效运行的新方法，以满足实际场景需求。",
      "method": "论文提出k-NLPmeans和k-LLMmeans两种变体，核心思想是将k-means的数字质心替换为文本摘要。k-NLPmeans使用轻量级、确定性的摘要器，支持离线、低成本和稳定操作；k-LLMmeans作为即插即用升级，利用LLM生成摘要，每次迭代成本固定且不随数据集大小增长。方法还包括mini-batch扩展，用于实时流文本聚类，通过在嵌入空间执行k-means分配，同时产生可读聚类原型。",
      "result": "在不同数据集、嵌入模型和摘要策略下，该方法持续超越传统k-means等基线，并接近LLM-based聚类方法的准确率，但无需大量LLM调用。例如，k-LLMmeans通过固定每迭代预算，在保证性能的同时保持成本可控。摘要未明确说明具体准确率数值，但强调了其有效性和可扩展性，表明能在多种场景下稳定优于经典方法。",
      "conclusion": "主要贡献是提出了以文本摘要作为质心的聚类方法，实现了可解释性和可扩展性的创新平衡。学术上，为可解释AI和聚类分析提供了新思路；实践上，k-NLPmeans和k-LLMmeans适用于不同成本和性能需求，mini-batch扩展支持流数据处理。未来工作可优化摘要质量对聚类的影响，摘要还发布了StackExchange基准数据集，推动流文本聚类评估。",
      "tags": [
        "Text Clustering",
        "k-means",
        "Large Language Models",
        "Summarization",
        "Streaming Data Processing"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:19.114241Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.08005",
    "title": "DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision",
    "authors": [
      "Wenlin Chen",
      "Mingtian Zhang",
      "Jiajun He",
      "Zijing Ou",
      "José Miguel Hernández-Lobato",
      "Bernhard Schölkopf",
      "David Barber"
    ],
    "abstract": "Score-based distillation methods (e.g., variational score distillation) train one-step diffusion models by first pre-training a teacher score model and then distilling it into a one-step student model. However, the gradient estimator in the distillation stage usually suffers from two sources of bias: (1) biased teacher supervision due to score estimation error incurred during pre-training, and (2) the student model's score estimation error during distillation. These biases can degrade the quality of the resulting one-step diffusion model. To address this, we propose DiffRatio, a new framework for training one-step diffusion models: instead of estimating the teacher and student scores independently and then taking their difference, we directly estimate the score difference as the gradient of a learned log density ratio between the student and data distributions across diffusion time steps. This approach greatly simplifies the training pipeline, significantly reduces gradient estimation bias, and improves one-step generation quality. Additionally, it also reduces auxiliary network size by using a lightweight density-ratio network instead of two full score networks, which improves computational and memory efficiency. DiffRatio achieves competitive one-step generation results on CIFAR-10 and ImageNet (64x64 and 512x512), outperforming most teacher-supervised distillation methods. Moreover, the learned density ratio naturally serves as a verifier, enabling a principled inference-time parallel scaling scheme that further improves the generation quality without external rewards or additional sequential computation.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.08005.pdf",
    "abs_url": "https://arxiv.org/abs/2502.08005",
    "published": "2025-02-11T23:02:14Z",
    "updated": "2026-01-28T17:35:55Z",
    "comment": "22 pages, 8 figures, 5 tables, 2 algorithms",
    "light_analysis": {
      "overview": "提出了DiffRatio，一个无需教师监督的训练单步扩散模型的新框架，通过直接估计分数差异来提高生成质量和计算效率。",
      "motivation": "现有基于分数的蒸馏方法（如变分分数蒸馏）训练单步扩散模型时，先预训练教师分数模型，再蒸馏到学生模型。然而，蒸馏阶段的梯度估计器存在两个偏差来源：教师监督偏差（源于预训练中的分数估计误差）和学生模型在蒸馏时的分数估计误差。这些偏差会降低单步模型的生成质量，因此需要新方法来解决这一瓶颈，以促进高效、高质量的扩散模型应用。",
      "method": "DiffRatio框架的核心是直接估计分数差异，作为学生分布和数据分布之间学习到的对数密度比的梯度，而非独立估计教师和学生分数后再取差。这通过一个轻量级密度比网络实现，取代了传统的两个完整分数网络，简化了训练流程，减少了梯度估计偏差，并提高了计算和内存效率。该方法创新性地将密度比估计应用于扩散模型训练，避免了复杂的多阶段优化。",
      "result": "在CIFAR-10和ImageNet（包括64x64和512x64分辨率）数据集上，DiffRatio取得了竞争性的单步生成结果，超越了大多数教师监督蒸馏方法，表明生成质量和效率有所改进。具体性能指标如准确率提升在摘要中未明确说明，但实验结果验证了框架的有效性，与基线方法相比表现出色。",
      "conclusion": "DiffRatio的主要贡献在于提供了一个无需教师监督的训练框架，通过直接估计分数差异减少了偏差，提高了单步扩散模型的生成质量和计算效率。其学术价值在于创新性地融合密度比估计与扩散模型训练，实际应用价值包括提升推理效率。此外，学习的密度比作为验证器，实现了推理时并行缩放方案，无需外部奖励或额外序列计算，进一步扩展了应用潜力。",
      "tags": [
        "Diffusion Models",
        "Score-based Distillation",
        "Density Ratio Estimation",
        "One-Step Generation",
        "Computational Efficiency"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:20.828489Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.01777",
    "title": "CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition",
    "authors": [
      "Martijn Bartelds",
      "Ananjan Nandi",
      "Moussa Koulako Bala Doumbouya",
      "Dan Jurafsky",
      "Tatsunori Hashimoto",
      "Karen Livescu"
    ],
    "abstract": "Modern deep learning models often achieve high overall performance, but consistently fail on specific subgroups. Group distributionally robust optimization (group DRO) addresses this problem by minimizing the worst-group loss, but it fails when group losses misrepresent performance differences between groups. This is common in domains like speech, where the widely used connectionist temporal classification (CTC) loss not only scales with input length but also varies with linguistic and acoustic properties, leading to spurious differences between group losses. We present CTC-DRO, which addresses the shortcomings of the group DRO objective by smoothing the group weight update to prevent overemphasis on consistently high-loss groups, while using input length-matched batching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of multilingual automatic speech recognition (ASR) across five language sets from the diverse ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and CTC-based baseline models, reducing the worst-language error by up to 47.1% and the average error by up to 32.9%. CTC-DRO can be applied to ASR with minimal computational costs, and, while motivated by multilingual ASR, offers the potential for reducing group disparities in other domains with similar challenges.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.01777.pdf",
    "abs_url": "https://arxiv.org/abs/2502.01777",
    "published": "2025-02-03T19:29:42Z",
    "updated": "2026-01-28T15:09:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出CTC-DRO方法，通过平滑组权重更新和输入长度匹配批处理，显著减少多语言语音识别中的语言差异。",
      "motivation": "现代深度学习模型在整体性能优异，但常在特定子群（如不同语言）上失败。组分布稳健优化（group DRO）旨在最小化最差组损失以解决这一问题，然而在语音识别等领域，CTC损失受输入长度和语言声学属性影响，导致组间损失虚假差异，使group DRO失效。这一挑战在多语言自动语音识别中尤为重要，因为语言差异可能加剧识别错误率不均，影响模型公平性和实际应用价值。",
      "method": "CTC-DRO通过平滑组权重更新防止过度强调持续高损失组，并结合输入长度匹配批处理以缓解CTC损失随输入长度缩放的问题。该方法在ML-SUPERB 2.0基准的五个语言集上进行评估，专注于多语言自动语音识别任务。关键创新点在于优化CTC损失函数，使其更稳健地处理组间性能差异，同时保留计算效率。摘要未明确说明具体模型架构，但基于CTC的基线模型进行对比。",
      "result": "在ML-SUPERB 2.0基准的五个语言集上评估，CTC-DRO显著优于group DRO和基于CTC的基线模型。具体而言，CTC-DRO将最差语言错误率减少了高达47.1%，平均错误率减少了高达32.9%。这些性能提升表明CTC-DRO在平衡不同语言识别准确性方面更有效，凸显了其解决组差异问题的优势。",
      "conclusion": "CTC-DRO的主要贡献是提出了一种低计算成本的优化方法，有效减少多语言语音识别中的语言差异。该研究具有学术价值，提升了自动语音识别系统的公平性，并显示出扩展到其他面临类似组差异挑战的领域的潜力。局限性摘要未明确说明，但未来工作可能包括验证在更广泛任务和数据集上的适用性。",
      "tags": [
        "Connectionist Temporal Classification (CTC)",
        "Group Distributionally Robust Optimization",
        "Multilingual Automatic Speech Recognition",
        "Robust Optimization",
        "Input Length Matching"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:46.895382Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.14277",
    "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
    "authors": [
      "JongMin Lee",
      "Sungjoo Yoo"
    ],
    "abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods. Project page: https://icetea-cv.github.io/densesfm/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.14277.pdf",
    "abs_url": "https://arxiv.org/abs/2501.14277",
    "published": "2025-01-24T06:45:12Z",
    "updated": "2026-01-28T17:55:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "Dense-SfM通过整合密集匹配和Gaussian Splatting技术，提出一种新的结构从运动框架，显著提升了多视角图像的3D重建精度和密度。",
      "motivation": "传统结构从运动方法主要依赖稀疏关键点匹配，这限制了重建的准确性和点密度，尤其是在纹理稀少区域。这一问题的重要性在于，3D重建在计算机视觉应用中如机器人导航和虚拟现实中至关重要，现有方法无法有效处理密集场景，导致细节缺失和性能下降。Dense-SfM旨在解决这一局限，以提高重建质量和适用性。",
      "method": "Dense-SfM的核心方法包括整合密集匹配与基于Gaussian Splatting的轨迹扩展，生成更一致和更长的特征轨迹。此外，它采用一个多视角核化匹配模块，利用transformer和Gaussian Process架构进行稳健的轨迹细化，以提高跨多视图的一致性。该方法不依赖传统稀疏匹配，而是结合现代深度学习技术来增强特征追踪的可靠性和精度。",
      "result": "在ETH3D和Texture-Poor SfM数据集上的评估显示，Dense-SfM相比最先进方法在重建精度和密度方面有显著提升。摘要未提供具体性能数据，但实验结果表明了其有效性，通过基线方法对比突出了改进，尤其在纹理稀少场景中表现优异。这表明Dense-SfM在提升3D重建质量方面具有竞争力。",
      "conclusion": "Dense-SfM的主要贡献是提出一个新颖的框架，结合密集匹配和先进技术，提高了3D重建的准确性和密度。该研究具有重要的学术价值，为计算机视觉领域的3D重建任务提供了新思路，并可能在增强现实、机器人感知等实际应用中发挥重要作用。局限性摘要未明确说明，未来工作可探索计算效率优化或扩展到更复杂环境。",
      "tags": [
        "Structure from Motion",
        "Dense Matching",
        "Gaussian Splatting",
        "Transformer",
        "Gaussian Process"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:21.965002Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.09718",
    "title": "FLOL: Fast Baselines for Real-World Low-Light Enhancement",
    "authors": [
      "Juan C. Benito",
      "Daniel Feijoo",
      "Alvaro Garcia",
      "Marcos V. Conde"
    ],
    "abstract": "Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the computer vision literature. However, current deep learning-based solutions struggle with efficiency and robustness for real-world scenarios (e.g., scenes with noise, saturated pixels). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our baseline method, FLOL, is one of the fastest models for this task, achieving results comparable to the state-of-the-art on popular real-world benchmarks such as LOLv2, LSRW, MIT-5K and UHD-LL. Moreover, we are able to process 1080p images in real-time under 12ms. Code and models at https://github.com/cidautai/FLOL",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.09718.pdf",
    "abs_url": "https://arxiv.org/abs/2501.09718",
    "published": "2025-01-16T18:06:09Z",
    "updated": "2026-01-28T18:31:35Z",
    "comment": "Journal Preprint",
    "light_analysis": {
      "overview": "论文提出轻量级神经网络FLOL，结合频域和空间域处理，实现快速低光图像增强，速度领先。",
      "motivation": "低光图像增强是计算机视觉和计算摄影中的关键任务，旨在提升夜间或暗光环境下捕获的图像质量。现有深度学习方案虽在增强效果上有所改进，但在处理真实世界场景时（如含噪声和饱和像素的图像），常面临计算效率低下和鲁棒性不足的挑战，这限制了在实时应用中的部署。因此，本研究旨在开发一种更高效、鲁棒的增强方法，以弥补理论与实际需求之间的差距，推动技术向移动摄影和视频监控等领域的转化。",
      "method": "FLOL方法采用轻量级神经网络架构，创新性地融合了频域和空间域的图像处理技术。通过结合频域分析（如傅里叶变换）来捕捉全局特征，以及空间域操作（如卷积）来处理局部细节，这种跨域融合优化了参数效率并降低了计算复杂度。研究在多个标准真实世界数据集上进行验证，包括LOLv2、LSRW、MIT-5K和UHD-LL，但摘要未详细说明具体网络层设计或训练损失函数。",
      "result": "FLOL在LOLv2、LSRW、MIT-5K和UHD-LL等基准测试中，取得了与当前最先进方法媲美的增强效果。关键性能指标是其处理速度，能够在12毫秒内完成1080p图像的实时处理，相当于每秒超过83帧，显著提升了效率，优于许多现有模型，为高帧率应用如实时视频增强提供了可能。",
      "conclusion": "本研究成功开发了FLOL，一个快速且高效的低光图像增强基准方法，通过结合频域和空间域处理，在保持高质量的同时大幅提升速度，具有重要应用价值。该工作推动了实时图像处理技术的发展，为计算摄影领域提供了实用工具，尤其在资源受限环境中。未来工作可探索模型在更复杂场景下的鲁棒性或进一步集成其他先进技术，但摘要未明确说明局限性。",
      "tags": [
        "Low-Light Image Enhancement",
        "Lightweight Neural Network",
        "Frequency Domain Processing",
        "Spatial Domain Processing",
        "Real-time Processing"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:21.143742Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.01256",
    "title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
    "authors": [
      "Bikang Pan",
      "Qun Li",
      "Xiaoying Tang",
      "Wei Huang",
      "Zhen Fang",
      "Feng Liu",
      "Jingya Wang",
      "Jingyi Yu",
      "Ye Shi"
    ],
    "abstract": "The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representations and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2412.01256.pdf",
    "abs_url": "https://arxiv.org/abs/2412.01256",
    "published": "2024-12-02T08:25:09Z",
    "updated": "2026-01-28T08:33:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出NLPrompt方法，通过结合MAE损失和最优运输数据净化，增强视觉-语言模型在噪声标签下的提示学习鲁棒性。",
      "motivation": "现实世界的数据集常包含噪声标签，这会显著降低视觉-语言模型通过提示学习获得的性能，影响广泛应用。尽管MAE损失以鲁棒性闻名，但由于其在非提示学习场景中收敛慢、性能差，很少用于噪声标签学习。因此，需要开发新方法来提高模型对噪声的抵抗力，确保在实际环境中的可靠性和准确性。摘要强调了噪声标签问题的重要性和现有方法的不足。",
      "method": "论文提出PromptMAE，在提示学习中使用MAE损失以增强对噪声标签的鲁棒性，并利用特征学习理论解释其能抑制噪声样本影响。此外，引入PromptOT，一种基于最优运输的数据净化方法，利用视觉-语言模型中的文本特征作为原型构建最优运输矩阵，将数据集分割为干净和噪声子集。然后，对干净子集应用交叉熵损失，对噪声子集应用MAE损失，结合为NLPrompt方法，该方法简单高效。摘要未明确说明具体数据集或模型架构细节。",
      "result": "通过在各种噪声设置下进行广泛实验，NLPrompt显示出显著的性能改进。实验涵盖了多种噪声类型和程度，结果表明该方法在保持高精度的同时，增强了对抗噪声的能力，表现优于传统基线方法。摘要未提供具体性能指标数据，但强调了改进的鲁棒性和有效性，验证了NLPrompt在真实世界应用中的潜力。",
      "conclusion": "NLPrompt提供了一种简单高效的噪声标签提示学习方法，利用视觉-语言模型的强大表示和精确对齐能力提升鲁棒性。该研究的学术价值在于为噪声标签学习提供了新视角和技术路线，实际应用价值在于可部署于有噪声的真实数据环境中。摘要未明确说明局限性或未来工作方向，但暗示了进一步优化的可能性。",
      "tags": [
        "Prompt Learning",
        "Mean Absolute Error (MAE) Loss",
        "Optimal Transport",
        "Noise-Label Learning",
        "Vision-Language Models"
      ]
    },
    "analyzed_at": "2026-01-29T03:58:54.768116Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.08862",
    "title": "LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs",
    "authors": [
      "Piyush Jha",
      "Arnav Arora",
      "Vijay Ganesh"
    ],
    "abstract": "We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.08862.pdf",
    "abs_url": "https://arxiv.org/abs/2411.08862",
    "published": "2024-11-13T18:44:30Z",
    "updated": "2026-01-28T18:58:57Z",
    "comment": "Accepted at AAAI 2025",
    "light_analysis": {
      "overview": "论文提出LLMStinger，一种通过强化学习微调大语言模型来自动生成对抗性后缀以进行越狱攻击的新方法。",
      "motivation": "该研究旨在解决大语言模型越狱攻击中对抗性后缀自动生成的难题。随着LLMs安全措施日益加强，传统方法依赖复杂提示工程或白盒访问，效率低下且可扩展性差，限制了对模型安全性的有效评估。LLMStinger通过强化学习优化方法，提供了一种高效、可扩展的解决方案，对提升LLMs的防御能力和安全研究具有重要意义。",
      "method": "LLMStinger采用强化学习循环来微调一个攻击者大语言模型，基于现有攻击生成新的对抗性后缀。该方法以HarmBench基准中的有害问题为目标，通过RL迭代优化后缀生成，关键创新在于摆脱了传统方法对手动工程或模型内部访问的依赖，实现了自动、高效的攻击生成。摘要未明确说明具体模型架构细节，但强调了基于现有攻击的迭代过程。",
      "result": "实验结果显示，LLMStinger在多个大语言模型上显著优于现有方法。与15种最新方法比较，攻击成功率（ASR）在LLaMA2-7B-chat上提升了57.2%，在Claude 2上提升了50.3%。此外，在GPT-3.5上达到94.97% ASR，在Gemma-2B-it上达到99.4%，证明该方法在开源和闭源模型上均具有鲁棒性和高适应性。",
      "conclusion": "LLMStinger的主要贡献是开发了一种基于强化学习微调的越狱攻击方法，显著提升了攻击成功率。该研究为大语言模型的安全性评估提供了新工具，推动了对抗性攻击技术的发展，具有实际应用价值。尽管在多个模型上表现优异，未来工作可探索更广泛的模型测试或优化RL策略以增强普适性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Jailbreaking",
        "Adversarial Attack",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:09.116396Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.08127",
    "title": "TIPO: Text to Image with Text Presampling for Prompt Optimization",
    "authors": [
      "Shih-Ying Yeh",
      "Sang-Hyun Park",
      "Yi Li",
      "Giyeong Oh",
      "Xuehai Wang",
      "Min Song",
      "Youngjae Yu"
    ],
    "abstract": "TIPO (Text-to-Image Prompt Optimization) introduces an efficient approach for automatic prompt refinement in text-to-image (T2I) generation. Starting from simple user prompts, TIPO leverages a lightweight pre-trained model to expand these prompts into richer and more detailed versions. Conceptually, TIPO samples refined prompts from a targeted sub-distribution within the broader semantic space, preserving the original intent while significantly improving visual quality, coherence, and detail. Unlike resource-intensive methods based on large language models (LLMs) or reinforcement learning (RL), TIPO offers strong computational efficiency and scalability, opening new possibilities for effective automated prompt engineering in T2I tasks. Extensive experiments across multiple domains demonstrate that TIPO achieves stronger text alignment, reduced visual artifacts, and consistently higher human preference rates, while maintaining competitive aesthetic quality. These results highlight the effectiveness of distribution-aligned prompt engineering and point toward broader opportunities for scalable, automated refinement in text-to-image generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2411.08127.pdf",
    "abs_url": "https://arxiv.org/abs/2411.08127",
    "published": "2024-11-12T19:09:45Z",
    "updated": "2026-01-28T17:24:46Z",
    "comment": "50 pages, 28 figures",
    "light_analysis": {
      "overview": "TIPO提出一种基于文本预采样的高效提示优化方法，用于改善文本到图像生成的视觉质量和连贯性。",
      "motivation": "文本到图像生成中，用户提供的提示往往过于简单，导致生成图像质量低下，如细节不足或视觉不连贯。现有方法依赖于大型语言模型或强化学习，计算资源消耗大、效率低，难以在实际应用中规模化部署。因此，本研究旨在开发一种高效、可扩展的自动提示优化技术，以解决现有方法在计算成本和可扩展性上的不足，提升生成图像的整体质量。",
      "method": "TIPO采用轻量级预训练模型，从简单的用户提示出发，通过文本预采样技术，从语义空间的目标子分布中提取更丰富、细节化的提示。该方法的核心创新在于保持原始意图的同时，利用分布对齐策略自动扩展提示内容，避免了依赖计算密集型方法如大型语言模型或强化学习。关键步骤包括语义空间采样和提示细化，摘要未明确说明具体模型架构或使用的数据集，但强调了其轻量级和高效性。",
      "result": "通过多个领域的广泛实验验证，TIPO在文本到图像生成任务中表现出显著改进：它实现了更强的文本对齐，减少了视觉伪影，并获得了一致更高的人类偏好率，同时保持了竞争力的美学质量。与基于大型语言模型或强化学习的基线方法相比，TIPO在计算效率和可扩展性上更优，但摘要未明确提供具体性能指标如百分比提升，仅强调了其在多个测试领域的稳健效果。",
      "conclusion": "TIPO的主要贡献是提出一种分布对齐的提示工程方法，为文本到图像生成提供了高效、可扩展的自动优化方案。该研究具有重要学术价值，推动了自动化提示工程的发展，可能应用于图像生成工具和创意设计中。局限性可能包括对特定语义空间的依赖性，未来工作可探索跨领域适应性和更复杂的优化策略，以进一步拓展应用范围。",
      "tags": [
        "Text-to-Image Generation",
        "Prompt Optimization",
        "Pre-trained Models",
        "Semantic Sampling",
        "Computational Efficiency"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:04.631393Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.01262",
    "title": "Improving Fine-Grained Control via Aggregation of Multiple Diffusion Models",
    "authors": [
      "Conghan Yue",
      "Zhengwei Peng",
      "Shiyan Du",
      "Zhi Ji",
      "Chuangjian Cai",
      "Le Wan",
      "Dongyu Zhang"
    ],
    "abstract": "While many diffusion models perform well when controlling particular aspects such as style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel training-free algorithm for fine-grained generation, called Aggregation of Multiple Diffusion Models (AMDM). The algorithm integrates features in the latent data space from multiple diffusion models within the same ecosystem into a specified model, thereby activating particular features and enabling fine-grained control. Experimental results demonstrate that AMDM significantly improves fine-grained control without training, validating its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional generation in diffusion models. Specifically, it allows us to fully utilize existing or develop new conditional diffusion models that control specific aspects, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at: https://github.com/Hammour-steak/AMDM.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2410.01262.pdf",
    "abs_url": "https://arxiv.org/abs/2410.01262",
    "published": "2024-10-02T06:16:06Z",
    "updated": "2026-01-28T08:13:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种训练免费的AMDM算法，通过聚合多个扩散模型的潜在空间特征实现细粒度控制，解决了现有方法的不足。",
      "motivation": "扩散模型在风格、角色等特定方面控制良好，但在细粒度控制上存在挑战，主要由于数据集限制和模型架构复杂。现有方法需要构建复杂数据集、设计精细架构并承担高训练成本，影响了高效应用。本研究旨在解决这些问题，通过训练免费算法聚合多模型特性，以增强细粒度控制能力，克服了传统方法的局限性。",
      "method": "AMDM算法通过在潜在数据空间中聚合多个扩散模型的特征，激活特定属性以实现细粒度控制。该算法训练免费，无需额外训练成本，核心创新是集成同一生态系统中多个模型的专长，如位置、属性和风格特征。摘要未明确说明具体使用的数据集或模型架构细节，但强调算法避免了复杂设计和训练开销。",
      "result": "实验结果显示，AMDM算法显著提升了细粒度控制效果，验证了其有效性。尽管摘要未提供具体性能指标数据，但表明相比基线方法在控制精度上有明显改进，并揭示了扩散模型在生成过程中初期聚焦特征，后期优化质量的特性，这为后续研究提供了实证基础。",
      "conclusion": "AMDM的主要贡献是提出一种训练免费聚合算法，为扩散模型的细粒度条件生成提供了新视角，允许充分利用现有或新开发的条件扩散模型，降低开发成本。这具有重要学术价值，简化了模型设计，并为实际应用如创意生成带来便利。潜在局限性或未来方向包括扩展应用场景和优化聚合策略，摘要未明确说明具体细节。",
      "tags": [
        "Diffusion Models",
        "Fine-Grained Control",
        "Training-Free Algorithm",
        "Latent Space Aggregation"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:13.436313Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.04628",
    "title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP",
    "authors": [
      "Danlu Chen",
      "Freda Shi",
      "Aditi Agarwal",
      "Jacobo Myerston",
      "Taylor Berg-Kirkpatrick"
    ],
    "abstract": "Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.   This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2408.04628.pdf",
    "abs_url": "https://arxiv.org/abs/2408.04628",
    "published": "2024-08-08T17:58:06Z",
    "updated": "2026-01-28T17:27:29Z",
    "comment": "correct wrong refs, typos",
    "light_analysis": {
      "overview": "本研究提出了LogogramNLP基准测试，首次为古代表意文字系统提供NLP分析，并通过实验证明视觉表示在部分任务中优于文本表示。",
      "motivation": "古代表意文字系统如甲骨文、楔形文字等大多以图像形式存在，缺乏标准转录，这成为应用自然语言处理（NLP）技术研究古代文化的瓶颈。现有NLP方法依赖符号表示，但转录过程耗时且依赖专家知识，导致大量文化遗产数据无法被有效利用。因此，本研究旨在探索直接处理视觉表示的可能性，以解决数据瓶颈，推动基于NLP的古代语言研究。",
      "method": "本研究引入LogogramNLP基准测试，首次收集了四个古代表意文字系统的转录和视觉数据集，并标注了分类、翻译和解析等多种NLP任务。核心方法是通过实验比较以最近视觉编码策略（如基于图像的模型）和文本编码策略（如基于序列的模型）为骨干的系统。这旨在评估视觉处理管道的有效性，为古代表意文字的NLP分析提供标准化框架，同时探索多模态表示的学习方法。",
      "result": "实验结果表明，在部分研究的NLP任务中，如分类或翻译，使用视觉表示的系统性能超过了基于文本表示的系统。摘要未明确说明具体性能指标如准确率提升，但结果暗示直接处理图像数据在处理古代表意文字时可能更具优势，这为利用未转录的文化遗产数据提供了初步证据。与基线方法相比，视觉管道展示了替代传统转录方法的潜力，可能推动相关领域的效率改进。",
      "conclusion": "本研究的主要贡献是提出了LogogramNLP基准测试，首次为古代表意文字系统提供标准化的NLP分析资源，并验证了视觉表示在特定任务上的优越性。这一研究具有重要学术价值，它可能解锁大量文化遗产数据供NLP分析，促进文化遗产保护和多模态学习的发展。未来工作方向可包括优化视觉编码策略、扩展到更多文字系统和任务，以进一步克服数据瓶颈。",
      "tags": [
        "Natural Language Processing",
        "Logographic Writing Systems",
        "Visual Representation",
        "Textual Representation",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:40.540920Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2402.14891",
    "title": "LLMBind: A Unified Modality-Task Integration Framework",
    "authors": [
      "Bin Zhu",
      "Munan Ning",
      "Peng Jin",
      "Bin Lin",
      "Jinfa Huang",
      "Qi Song",
      "Junwu Zhang",
      "Zhenyu Tang",
      "Mingjun Pan",
      "Li Yuan"
    ],
    "abstract": "Despite recent progress in Multi-Modal Large Language Models (MLLMs), it remains challenging to integrate diverse tasks ranging from pixel-level perception to high-fidelity generation. Existing approaches often suffer from either restricted task extensibility or severe performance degradation due to modality interference. n this paper, we present LLMBind, an extensible framework that unifies multimodal tasks through a dual-pathway mechanism: In-Situ semantic embeddings for localization-sensitive tasks like semantic segmentation and Ex-Situ task-prompts for generation across image, video, and audio modalities. Additionally, we employ a Mixture-of-Experts (MoE) architecture to route task-specific tokens, thereby achieving modality disentanglement and mitigating negative transfer. We also curate a 400k multi-turn interactive dataset focused on iterative visual refinement to enable human-like interaction. Extensive experiments demonstrate that LLMBind achieves excellent performance across multiple perception and generation benchmarks while maintaining superior expandability.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2402.14891.pdf",
    "abs_url": "https://arxiv.org/abs/2402.14891",
    "published": "2024-02-22T12:36:31Z",
    "updated": "2026-01-28T17:35:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "LLMBind提出了一个统一模态-任务集成框架，通过双路径机制和混合专家架构解决多模态任务扩展性和性能退化问题。",
      "motivation": "近年来，多模态大型语言模型（MLLMs）在多种任务中取得了进展，但集成从像素级感知到高保真生成的多样化任务仍具挑战性。现有方法常因模态干扰导致任务可扩展性受限或性能严重下降。LLMBind研究旨在解决这些问题，提供一个可扩展的框架以统一多模态任务，弥补现有方法的不足。",
      "method": "LLMBind框架采用双路径机制：In-Situ语义嵌入用于定位敏感任务如语义分割，Ex-Situ任务提示用于跨图像、视频和音频模态的生成任务。此外，通过混合专家（MoE）架构路由特定于任务的令牌，实现模态解耦并减轻负迁移。还构建了一个40万轮多轮交互数据集，专注于迭代视觉细化，以支持人类般的交互。",
      "result": "大量实验表明，LLMBind在多个感知和生成基准测试中实现了优异的性能，同时保持了优越的可扩展性。摘要未明确说明具体性能指标如准确率提升，但展示了其在不同任务中的有效性和扩展优势。",
      "conclusion": "LLMBind的主要贡献是提出了一个统一的模态-任务集成框架，通过双路径机制和MoE架构解决了模态干扰和任务扩展性问题。该研究提升了多模态AI系统的性能，并为未来可扩展多模态模型的设计提供了新思路，摘要未明确提及局限性或未来工作。",
      "tags": [
        "Multi-Modal Large Language Models",
        "Mixture-of-Experts",
        "Semantic Embeddings",
        "Task Prompts",
        "Modality Disentanglement"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:33.155697Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2401.06157",
    "title": "UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection",
    "authors": [
      "Dennis Monari",
      "Farhad Fassihi Tash",
      "Jordan J. Bird",
      "Ahmad Lotfi",
      "Isibor Kennedy Ihianle",
      "Salisu Wada Yahaya",
      "Isibor Kennedy Ihianle",
      "Md Mahmudul Hasan",
      "Pedro Sousa",
      "Pedro Machado"
    ],
    "abstract": "Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90%. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and pollution in aquatic ecosystem's. This article introduces the Cognitive Edge Device (CED) computing platform for the detection of crayfish and plastic. It also presents two publicly available underwater datasets, annotated with sequences of crayfish and aquatic plastic debris. Four You Only Look Once (YOLO) variants were trained and evaluated for crayfish and plastic object detection. YOLOv5s achieved the highest detection accuracy, with an mAP@0.5 of 0.90, and achieved the best precision",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2401.06157.pdf",
    "abs_url": "https://arxiv.org/abs/2401.06157",
    "published": "2023-12-21T16:03:59Z",
    "updated": "2026-01-28T16:53:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于边缘计算的Cognitive Edge Device平台和公开水下数据集，用于实时检测入侵小龙虾和塑料污染。",
      "motivation": "入侵信号小龙虾对英国水生生态系统造成严重破坏，传播致命疾病并导致栖息地退化，同时塑料污染加剧本地物种脆弱性，种群下降超过90%。现有监测方法可能效率低或成本高，缺乏自动化实时解决方案，因此开发高效检测技术至关重要，以应对入侵物种和污染对生态系统的威胁。本文旨在填补这一空白，提供基于边缘计算的实用监测手段。",
      "method": "本文开发了Cognitive Edge Device (CED)边缘计算平台，专门设计用于水下环境中的实时对象检测。同时发布了两个公开可用的水下数据集，包含注释的小龙虾和塑料碎片图像序列。研究中采用YOLO对象检测框架，训练并评估了四个不同版本的YOLO模型变体，以比较其在检测任务中的性能。关键创新在于边缘设备的部署和标准化数据集的贡献，支持后续研究和实际应用。",
      "result": "实验结果显示，在训练和评估的四个YOLO变体中，YOLOv5s模型取得了最高的检测精度，mAP@0.5达到0.90，并且表现出最佳精确度。这证明了YOLOv5s在水下小龙虾和塑料检测任务中的有效性，验证了所提平台和数据集的实用性。尽管摘要未详细对比其他变体的具体数据，但结果支持基于边缘的计算机视觉方法在复杂环境中的可行性，为性能评估提供了基准。",
      "conclusion": "论文的主要贡献是引入了Cognitive Edge Device平台和公开水下数据集，推动了水下环境监测技术的发展，填补了相关研究空白。学术价值在于提供标准化数据和边缘计算框架，实际应用有助于实时监控入侵物种和污染，保护水生生态系统。未来工作可能包括扩展数据集规模、优化检测算法或集成更多传感器，但摘要未明确说明具体局限性或下一步方向。",
      "tags": [
        "Edge Computing",
        "Computer Vision",
        "Object Detection",
        "YOLO",
        "Underwater Datasets"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:47.584070Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2311.12086",
    "title": "Robust MAE-Driven NAS: From Mask Reconstruction to Architecture Innovation",
    "authors": [
      "Yiming Hu",
      "Xiangxiang Chu",
      "Yong Wang"
    ],
    "abstract": "Neural Architecture Search (NAS) relies heavily on labeled data, which is labor-intensive and time-consuming to obtain. In this paper, we propose a novel NAS method based on an unsupervised paradigm, specifically Masked Autoencoders (MAE), thereby eliminating the need for labeled data. By replacing the supervised learning objective with an image reconstruction task, our approach enables the efficient discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) in the unsupervised setting by designing a hierarchical decoder. Extensive experiments across various datasets demonstrate the effectiveness and robustness of our method, offering empirical evidence of its superiority over the counterparts.",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2311.12086.pdf",
    "abs_url": "https://arxiv.org/abs/2311.12086",
    "published": "2023-11-20T13:45:21Z",
    "updated": "2026-01-28T10:09:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种基于掩码自编码器的无监督神经架构搜索方法，通过图像重建任务避免了标注数据依赖，并解决了DARTS的性能崩溃问题。",
      "motivation": "神经架构搜索（NAS）传统上严重依赖大量标注数据，这增加了获取成本和复杂性，限制了其实用性。现有方法如可微架构搜索（DARTS）在无监督设置下容易发生性能崩溃，导致搜索过程不稳定，影响模型泛化能力。本研究旨在开发一个无需标注数据的NAS框架，以降低数据需求并提高鲁棒性，从而推动无监督学习在架构搜索中的应用。",
      "method": "本方法的核心是结合掩码自编码器（MAE）构建无监督神经架构搜索框架。通过将监督学习目标替换为图像重建任务，使用MAE在掩码图像上进行重建训练，以评估网络架构的性能。关键创新点包括设计了一个分层解码器，专门用于解决DARTS在无监督环境中的性能崩溃问题，确保搜索过程稳定高效。该方法基于标准MAE架构，在不同数据集上自动探索最优网络结构，无需额外标注数据。",
      "result": "实验在多个数据集上进行，结果表明该方法在无监督设置下表现优越，展现出高效率和鲁棒性。尽管具体性能指标如准确率提升摘要未明确说明，但论文提供了与基线方法对比的实证证据，显示其有效性超越现有无监督NAS方法。这证实了该方法在减少标注数据依赖的同时，保持了较好的泛化能力和架构发现性能。",
      "conclusion": "本研究的主要贡献是提出了一种基于MAE的无监督神经架构搜索方法，有效消除了对标注数据的依赖，并通过分层解码器解决了DARTS的性能崩溃问题。其学术价值在于推动了无监督学习在架构搜索领域的发展，实际应用价值体现在降低数据标注成本和提高NAS的实用性。未来工作可能包括扩展到更多任务和优化计算效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Neural Architecture Search",
        "Masked Autoencoders",
        "Differentiable Architecture Search",
        "Unsupervised Learning",
        "Hierarchical Decoder"
      ]
    },
    "analyzed_at": "2026-01-29T03:59:35.710468Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2310.00488",
    "title": "Membership Privacy Risks of Sharpness Aware Minimization",
    "authors": [
      "Young In Kim",
      "Andrea Agiollo",
      "Pratiksha Agrawal",
      "Johannes O. Royset",
      "Rajiv Khanna"
    ],
    "abstract": "Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization and robustness to noise. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This suggests that the geometric mechanism of SAM that improves generalization simultaneously exacerbates membership leakage. We investigate this phenomenon through extensive analysis of memorization and influence scores. Our results reveal that SAM is more capable of capturing atypical subpatterns, leading to higher memorization scores of samples. Conversely, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Crucially, this characteristic of SAM can be linked to lower variance in the prediction confidence of unseen samples, thereby amplifying membership signals. Finally, we model SAM under a perfectly interpolating linear regime and theoretically show that sharpness regularization inherently reduces variance, guaranteeing a higher MIA advantage for confidence and likelihood ratio attacks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2310.00488.pdf",
    "abs_url": "https://arxiv.org/abs/2310.00488",
    "published": "2023-09-30T20:59:07Z",
    "updated": "2026-01-28T13:43:19Z",
    "comment": "accepted to iclr 2026",
    "light_analysis": {
      "overview": "本文揭示Sharpness Aware Minimization（SAM）优化算法在提升模型泛化能力的同时，会加剧成员推理攻击的隐私风险。",
      "motivation": "本研究旨在评估SAM优化算法对成员隐私风险的影响。SAM被广泛认为通过寻求平坦最小值来改善模型的泛化和噪声鲁棒性，但其隐私影响尚不明确，尤其是在实际应用中可能引发安全问题。现有方法多关注SAM的性能优势，而忽略其对隐私的潜在威胁，本文填补了这一空白，探索SAM的泛化改进是否以牺牲隐私为代价，从而为机器学习安全领域提供新视角。摘要未明确说明具体应用场景，但强调该问题对模型部署的重要性。",
      "method": "研究方法包括通过实验比较SAM和随机梯度下降（SGD）在多个数据集和攻击方法下的成员推理攻击易感性。核心创新在于使用记忆分数和影响力分数的广泛分析，以解释SAM为何更易泄露隐私。同时，理论建模SAM在线性插值完美情况下的行为，证明尖锐度正则化能减少预测置信度的方差，从而增加攻击优势。摘要未明确说明所用数据集和模型架构细节，但基于现有信息推断，该方法结合实证分析和理论推导探究SAM的几何机制。",
      "result": "实验结果显示，SAM比SGD更容易受到成员推理攻击，尽管SAM的测试错误率更低。这一现象在多个数据集和攻击方法中一致观察到，表明SAM的泛化改进同时放大了隐私风险。具体分析显示，SAM捕捉更多非典型子模式，导致更高记忆分数；而SGD依赖多数特征，泛化能力较差但隐私风险较低。SAM预测置信度的方差降低，为基于置信度和似然比的攻击提供更强信号，证实了其隐私脆弱性。摘要未提供具体数值指标，但强调对比基线的优势。",
      "conclusion": "本文主要贡献在于证实SAM优化算法在提高泛化的同时会增加成员隐私泄漏风险，揭示了优化目标与隐私安全的冲突。学术价值在于为机器学习模型优化和隐私保护的交叉研究提供新见解，强调算法设计需平衡性能与安全。实际应用价值在于提醒开发者在采用SAM时需考虑隐私增强措施，避免潜在安全漏洞。未来工作可能包括开发兼顾泛化和隐私的优化算法，或进一步分析其他类似算法的隐私影响。摘要未明确说明局限性，但基于研究可推断理论模型假设的简化可能影响推广性。",
      "tags": [
        "Sharpness Aware Minimization",
        "Membership Inference Attacks",
        "Stochastic Gradient Descent",
        "Memorization Scores",
        "Variance Reduction"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:00.218180Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2305.08367",
    "title": "Toward Highly Efficient and Private Submodular Maximization via Matrix-Based Acceleration",
    "authors": [
      "Boyu Liu",
      "Lianke Qin",
      "Zhao Song",
      "Yitan Wang",
      "Jiale Zhao"
    ],
    "abstract": "Submodular function maximization is a critical building block for diverse tasks, such as document summarization, sensor placement, and image segmentation. Yet its practical utility is often limit by the $O(knd^2)$ computational bottleneck. In this paper, we propose an integrated framework that addresses efficiency and privacy simultaneously. First, we introduce a novel matrix-based computation paradigm that accelerates function evaluations. Second, we develop approximate data structures that further streamline the optimization process, achieving a theoretical complexity of $O(ε^{-2}(nd+kn+kd^2)\\log(k/δ))$. Third, we integrate ($ε, δ$)-DP guaranties to address the privacy concerns inherent in sensitive optimization tasks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2305.08367.pdf",
    "abs_url": "https://arxiv.org/abs/2305.08367",
    "published": "2023-05-15T06:00:02Z",
    "updated": "2026-01-28T08:52:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一个集成框架，通过矩阵基加速技术同时提高次模函数最大化的效率和隐私性。",
      "motivation": "次模函数最大化在AI和机器学习中应用广泛，如文档摘要、传感器布局和图像分割，但传统算法面临$O(knd^2)$的高计算复杂度瓶颈，限制其大规模应用；此外，处理敏感数据时，优化过程易导致隐私泄露，现有方法往往忽视隐私保护。因此，本研究旨在解决这些效率与隐私的双重挑战，以提升次模优化的实用性和安全性。",
      "method": "该方法引入新颖的矩阵基计算范式，将函数评估转化为矩阵运算以加速计算；开发近似数据结构，优化内存和计算流程；并集成($ε, δ$)-差分隐私保证，通过注入噪声保护敏感信息，构成一个高效、隐私保护的次模函数最大化框架。",
      "result": "摘要未明确说明具体实验数据，如准确率或效率对比；但理论分析表明，该方法实现了时间复杂度$O(ε^{-2}(nd+kn+kd^2)\\log(k/δ))$，相比基线$O(knd^2)$，理论上可能在大规模参数下显著提升效率，同时通过差分隐私确保隐私安全，适用于敏感优化任务。",
      "conclusion": "本研究主要贡献是开发了一个高效和隐私保护的次模函数最大化框架，通过矩阵基加速和差分隐私技术，有效应对计算瓶颈和隐私问题。该框架增强了算法在现实任务中的应用潜力，并为AI领域的数据安全优化提供了新路径；未来工作可扩展至其他优化问题或在更大数据集上验证性能。",
      "tags": [
        "Submodular Function Maximization",
        "Matrix-Based Computation",
        "Differential Privacy (DP)",
        "Approximate Data Structures",
        "Privacy-Preserving Optimization"
      ]
    },
    "analyzed_at": "2026-01-29T04:00:09.376585Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]