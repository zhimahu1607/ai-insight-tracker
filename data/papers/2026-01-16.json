[
  {
    "id": "2601.10715",
    "title": "DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids",
    "authors": [
      "Navami Kairanda",
      "Shanthika Naik",
      "Marc Habermann",
      "Avinash Sharma",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ],
    "abstract": "We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10715.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10715",
    "published": "2026-01-15T18:59:57Z",
    "updated": "2026-01-15T18:59:57Z",
    "comment": "25 pages; 16 figures; project page: https://4dqv.mpi-inf.mpg.de/DInf-Grid/",
    "light_analysis": {
      "overview": "DInf-Grid提出了一种结合特征网格和径向基函数插值的可微分网格表示法，显著提升神经微分方程求解器的训练效率和求解速度。",
      "motivation": "本研究旨在解决神经求解微分方程时效率低下和计算高阶导数能力不足的问题。现有方法如坐标基础的多层感知机（MLPs）计算密集、训练缓慢，而网格基础方法如Instant-NGP虽训练快但依赖线性插值，无法有效计算高阶导数，限制了其在微分方程求解中的应用。微分方程在物理建模等领域广泛使用，高效求解至关重要，因此需要一种兼具训练效率和可微分性的新表示方法，以克服现有方法的不足。",
      "method": "论文提出DInf-GGrid方法，核心是结合特征网格的高效性和径向基函数插值的无限可微分性。关键创新包括引入多分辨率分解和共址网格，以捕获高频解并实现稳定的全局梯度计算。该方法通过将微分方程作为损失函数进行隐式训练，用于建模物理场，并在泊松方程、亥姆霍兹方程和基尔霍夫-洛夫边界值问题等任务中验证其有效性。摘要未明确说明使用的具体数据集或详细模型架构，但突出了网格表示和插值技术的融合。",
      "result": "实验结果显示，DInf-GGrid在多个微分方程求解任务上表现优异，包括图像重建的泊松方程、波场模拟的亥姆霍兹方程和布料模拟的基尔霍夫-洛夫边界值问题。与坐标基础MLP方法相比，DInf-GGrid实现了5-20倍的训练速度提升，求解时间缩短至秒或分钟级别，同时保持与基线方法相当的准确性和模型紧凑性。具体性能指标如准确率提升在摘要中未明确说明，但强调了效率的显著改进。",
      "conclusion": "DInf-GGrid的主要贡献在于提出了一种高效可微分的网格表示法，能够准确建模物理场并快速求解微分方程，为神经微分方程求解提供了新思路。研究具有重要学术价值，推动了该领域的进展，并在图像重建、波场模拟和布料模拟等实际应用中展现出潜力。摘要未明确说明方法的局限性或未来工作方向，但暗示了其在更广泛物理建模任务中的扩展可能性。",
      "tags": [
        "Differentiable Grids",
        "Neural Differential Equation Solvers",
        "Radial Basis Function Interpolation",
        "Multi-resolution Decomposition",
        "Feature Grids"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:14.348753Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10712",
    "title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching",
    "authors": [
      "Changle Qu",
      "Sunhao Dai",
      "Hengyi Cai",
      "Jun Xu",
      "Shuaiqiang Wang",
      "Dawei Yin"
    ],
    "abstract": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10712.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10712",
    "published": "2026-01-15T18:59:23Z",
    "updated": "2026-01-15T18:59:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "MatchTIR 提出了一种基于二分匹配的细粒度监督框架，改进了工具集成推理中的信用分配问题，增强了大语言模型在复杂任务中的性能。",
      "motivation": "当前工具集成推理中的强化学习方法通常依赖结果级或轨迹级奖励，为轨迹中所有步骤分配统一的优势，导致信用分配粗糙。这种粗粒度方法无法区分有效、冗余或错误的工具调用，特别是在长视界多轮场景中，限制了模型的效率和准确性。现有方法不足在于缺乏对个体交互轮的精确监督，难以适应复杂任务需求，因此需要引入更细粒度的监督机制来解决这一问题。",
      "method": "MatchTIR 框架引入基于二分匹配的轮级奖励分配和双重优势估计方案来实现细粒度监督。核心方法将信用分配建模为预测与真实轨迹之间的二分匹配问题，使用两种分配策略生成密集的轮级奖励。此外，通过结合轮级和轨迹级信号，为每个交互轮分配独特的优势值，以平衡局部步骤精度与全局任务成功，关键技术包括二分匹配算法和双重优势计算模块，专注于多轮交互场景。",
      "result": "在三个基准测试上进行广泛实验，MatchTIR 表现出优越的性能。具体而言，其 4B 参数模型在多个任务中超越了大多数 8B 参数的竞争者，尤其是在长视界和多轮任务方面，效果显著提升。实验结果证明了细粒度监督在改善模型准确性和效率方面的有效性，但与基线方法的对比细节未在摘要中明确说明，总体显示 MatchTIR 在复杂场景中的优势。",
      "conclusion": "MatchTIR 的主要贡献在于提供了一种新颖的细粒度信用分配方法，通过二分匹配和双重优势估计提高了工具集成推理的精确性。这具有重要的学术价值，为结合强化学习和大语言模型的研究提供了新思路，实际应用中能显著提升复杂任务的处理能力。未来工作可能包括扩展到更多任务类型或探索其他监督机制，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Tool-Integrated Reasoning",
        "Large Language Models",
        "Reinforcement Learning",
        "Bipartite Matching",
        "Credit Assignment"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:46.475685Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10708",
    "title": "High-accuracy and dimension-free sampling with diffusions",
    "authors": [
      "Khashayar Gatmiry",
      "Sitan Chen",
      "Adil Salim"
    ],
    "abstract": "Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \\emph{high-quality} samples.   More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \\emph{polylogarithmically} in $1/\\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \\emph{effective radius} of the support of the target distribution only.",
    "categories": [
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10708.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10708",
    "published": "2026-01-15T18:58:50Z",
    "updated": "2026-01-15T18:58:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种新求解器，实现扩散模型高精度采样，迭代复杂度多对数增长且边界不显式依赖环境维度。",
      "motivation": "扩散模型在采样多模态分布中表现出色，但其推理依赖数值求解微分方程，无法闭式求解。现有离散化方法需大量迭代以实现高质量样本，复杂度在环境维度和逆精度上多项式增长，导致高维采样效率低下，限制了实际应用，因此需要开发更高效的求解方法以提升采样精度和速度。",
      "method": "论文提出一种新求解器，基于低度近似与配置方法（源自Lee, Song, Vempala 2018）的巧妙互动，优化扩散模型微分方程的数值求解过程。核心创新在于结合这两种技术减少离散化迭代步骤，实现高精度采样，仅需近似访问数据分布的分数，无需显式维度依赖，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验证明新求解器的迭代复杂度在逆精度1/ε上呈多对数增长，相比先前工作的多项式增长显著提升效率。边界不显式依赖于环境维度，仅通过目标分布支持的有效半径影响，为基于扩散的采样器首次提供高精度理论保证，尽管摘要未给出具体性能指标如准确率提升，但与基线方法相比实现了复杂度优化。",
      "conclusion": "主要贡献是提出并理论证明了一种高效扩散模型求解器，实现高精度和维度无关采样，推动了扩散模型的理论发展，并为高维数据采样提供了新思路。学术价值在于首次提供多对数复杂度保证，实际应用价值可能涉及高效采样任务；未来工作可探索方法扩展或实际部署中的效果，但摘要未明确说明局限性。",
      "tags": [
        "Diffusion Models",
        "Iteration Complexity",
        "Polylogarithmic Scaling",
        "Collocation Method",
        "Sampling"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:21.977914Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10707",
    "title": "See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection",
    "authors": [
      "Amir Mallak",
      "Erfan Aasi",
      "Shiva Sreeram",
      "Tsun-Hsuan Wang",
      "Daniela Rus",
      "Alaa Maalouf"
    ],
    "abstract": "Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10707.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10707",
    "published": "2026-01-15T18:58:33Z",
    "updated": "2026-01-15T18:58:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出随机补丁选择方法，通过随机掩码补丁特征减少冗余，提升端到端自动驾驶策略在分布外场景的泛化能力和鲁棒性。",
      "motivation": "该研究旨在解决端到端自动驾驶中策略在分布外场景泛化能力不足的问题。当前基于基础模型的补丁特征虽能提升泛化，但由于自注意力机制导致特征高度冗余，策略易过拟合虚假相关性，损害鲁棒性。现有方法未有效处理冗余，影响系统在未知环境中的可靠性，因此需开发新方法以增强泛化性和适应性，提高自动驾驶安全。",
      "method": "研究方法为随机补丁选择，核心是在每帧中随机掩码一部分补丁描述符，不输入策略模型，同时保留剩余补丁空间布局，提供不同随机但完整的场景视图。这迫使策略学习基于不变特征的决策，减少冗余导致的过拟合。技术基于BLIP2等基础模型提取补丁对齐特征，通过随机掩码创新性地利用随机性增强泛化，简单有效且无需复杂结构调整。",
      "result": "实验结果显示，在所有分布外场景中，该方法平均性能提升6.2%，闭环模拟中最高提升20.4%，且速度快2.4倍。消融实验训练9个系统，其中8个超越先前最先进技术，验证了方法的有效性。具体数据如平均改进百分比和速度倍数，证实其在提升泛化性和效率方面的优势，显著优于基线方法。",
      "conclusion": "研究总结表明，随机补丁选择方法通过减少特征冗余，显著提升了策略的泛化性和鲁棒性，主要贡献在于提出简单有效的掩码策略，使学习策略可直接迁移到真实世界汽车，无需调整。这具有重要学术和实际价值，为自动驾驶提供可靠解决方案；未来工作可能涉及优化掩码策略或扩展到其他领域，但摘要未明确说明局限性。",
      "tags": [
        "End-to-End Autonomous Driving",
        "Foundation Models",
        "Stochastic Patch Selection",
        "Self-Attention",
        "OOD Robustness"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:03.900116Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10705",
    "title": "Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication",
    "authors": [
      "Keval Jain",
      "Anant Raj",
      "Saurav Prakash",
      "Girish Varma"
    ],
    "abstract": "We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10705.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10705",
    "published": "2026-01-15T18:56:54Z",
    "updated": "2026-01-15T18:56:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种处理过时更新、部分参与和通信噪声的分布式感知器训练方法，并给出理论收敛保证。",
      "motivation": "本研究旨在解决联邦学习和分布式系统中模型训练面临的系统效应挑战，包括延迟更新导致的过时性、客户端间歇可用性以及通信噪声。这些效应在实际部署中普遍存在，如边缘计算场景，会严重影响训练效率和模型收敛性。现有方法通常依赖于对延迟和参与的随机模型假设，而本工作通过引入确定性规则，避免此类假设，提供更鲁棒的分析框架，从而更好地应对现实世界的不确定性，并强调处理这些问题的理论重要性。",
      "method": "研究方法基于半异步客户端-服务器架构，使用迭代参数混合（IPM-style averaging）进行感知器训练：客户端执行本地感知器更新，服务器在每轮通信中聚合更新以形成全局模型。关键创新是服务器端聚合规则“staleness-bucket aggregation with padding”，该规则确定性强制执行规定的过时配置文件，处理双向版本滞后，无需假设延迟或参与的随机分布。技术条件包括边际可分性和有界数据半径，通信噪声建模为具有有界二阶矩的零均值加性噪声，模型为感知器。",
      "result": "主要理论结果为在给定服务器轮次上，累积加权感知器错误数存在有限期望界。延迟的影响仅通过强制平均过时显现，而通信噪声贡献一个额外的项，该项随视野的平方根和总噪声能量增长。在无噪声情况下，研究展示了如何一个有限的期望错误预算在温和的新参与条件下产生明确的有限轮稳定界，暗示了方法的收敛性和稳定性。摘要未提供具体实验数据或与基线方法的直接对比，但理论分析为性能提供了量化保证。",
      "conclusion": "本研究的主要贡献是提出了一种鲁棒的分布式感知器训练框架，结合新聚合规则和理论分析，有效处理过时更新、部分参与和通信噪声。学术上，扩展了分布式优化理论，特别是在非理想通信条件下的收敛性分析。实际应用价值高，适用于联邦学习、边缘计算等场景，提高在现实约束下的训练可靠性。未来工作方向摘要未明确说明，但可推断可能包括扩展到更复杂模型、实验验证或处理更多系统变量。",
      "tags": [
        "Distributed Learning",
        "Federated Learning",
        "Perceptron",
        "Staleness",
        "Communication Noise"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:39.736734Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10702",
    "title": "Grounding Agent Memory in Contextual Intent",
    "authors": [
      "Ruozhen Yang",
      "Yucheng Jiang",
      "Yueqi Jiang",
      "Priyanka Kargupta",
      "Yunyi Zhang",
      "Jiawei Han"
    ],
    "abstract": "Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step's intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.   For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10702.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10702",
    "published": "2026-01-15T18:55:13Z",
    "updated": "2026-01-15T18:55:13Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10701",
    "title": "Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis",
    "authors": [
      "Chun Hei Michael Shiu",
      "Chih Wei Ling"
    ],
    "abstract": "Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10701.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10701",
    "published": "2026-01-15T18:55:00Z",
    "updated": "2026-01-15T18:55:00Z",
    "comment": "19 pages, 5 figures. This work is submitted in part to the 2026 IEEE International Symposium on Information Theory (ISIT). arXiv admin note: substantial text overlap with arXiv:2501.12046",
    "light_analysis": {
      "overview": "本文提出并理论分析了通信效率和隐私可调机制CEPAM，为联邦学习中的通信效率和隐私保护问题提供了集成解决方案。",
      "motivation": "联邦学习作为一种数据治理下的隐私保护协作方法，允许多方在不共享原始数据的情况下联合训练模型，但在实际应用中面临通信效率低下和隐私泄露风险等关键挑战。现有方法通常分别处理通信优化和隐私保护，导致性能折衷或隐私不足，限制了其在大规模部署中的实用性。因此，本研究旨在开发一种能同时优化这两个方面的机制，以推动联邦学习在数据敏感领域（如医疗和金融）的广泛采用。",
      "method": "本研究提出的CEPAM机制基于拒绝采样通用量化器RSUQ，这是一种随机向量量化器，其量化误差等价于可调预设噪声，从而允许定制化隐私保护水平。核心创新在于将量化与噪声注入相结合，以优化通信效率同时保障数据隐私。通过理论分析，我们探讨了CEPAM的隐私保证和收敛性质，确保其可靠性和稳定性。实验中评估了收敛曲线和准确性-隐私权衡，但摘要未明确说明具体使用的数据集和模型架构。",
      "result": "实验评估显示，CEPAM在保持隐私保护的同时提高了通信效率，并展现出优于基线的收敛性能。与其他方法相比，CEPAM的收敛曲线更平滑，并在准确性-隐私权衡方面表现良好，证明了其在联邦学习场景中的实用性。具体性能指标如准确率提升或效率改进百分比未在摘要中明确说明，但实验验证了CEPAM的整体有效性。",
      "conclusion": "本研究的主要贡献在于提出了CEPAM机制，它通过理论分析和实验验证，为联邦学习中的通信效率和隐私保护问题提供了统一的解决方案。学术上，CEPAM丰富了联邦学习的理论框架；实际上，它有望促进数据敏感领域的应用。局限性可能包括泛化性需进一步验证，未来工作可探索更广泛的实验设置和优化策略，以增强其适应性。",
      "tags": [
        "Federated Learning",
        "Communication Efficiency",
        "Privacy Protection",
        "RSUQ",
        "Convergence Analysis"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:56.978297Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10700",
    "title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals",
    "authors": [
      "Gilat Toker",
      "Nitay Calderon",
      "Ohad Amosy",
      "Roi Reichart"
    ],
    "abstract": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10700.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10700",
    "published": "2026-01-15T18:54:50Z",
    "updated": "2026-01-15T18:54:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "LIBERTy框架通过结构化因果模型生成反事实数据，为大型语言模型概念解释的忠实性提供基准测试。",
      "motivation": "概念基于解释能量化高级概念（如性别或经验）对模型行为的影响，在高风险领域（如医疗或招聘）的决策制定中至关重要。现有基准依赖于昂贵且不完美的人类编写反事实作为参考，这不仅成本高，还可能引入偏差，限制了评估的准确性和可扩展性。因此，本研究旨在解决这一问题，通过自动化生成反事实，提供一个更可靠和系统的基准，以改进概念解释的评估和开发。",
      "method": "LIBERTy框架基于明确定义的结构化因果模型（SCMs）构建数据集，通过干预概念并让SCM指导LLM生成对应的反事实文本。关键创新包括使用SCMs自动生成结构反事实对，避免了人类编写的限制。框架引入了三个领域特定的数据集（疾病检测、简历筛选和工作场所暴力预测）和一个新的评估指标order-faithfulness，用于评估多种概念解释方法在五个不同LLM上的表现，确保了评估的全面性和客观性。",
      "result": "使用LIBERTy框架评估了广泛的概念解释方法，发现当前方法在忠实性方面存在显著改进空间。通过order-faithfulness指标分析，专有LLMs对人口统计概念的敏感性明显降低，这可能是由于后训练缓解措施的影响。与基线方法相比，评估结果揭示了模型响应模式的差异，为未来改进提供了基准数据和具体方向。摘要未明确说明具体的准确率提升数值，但强调了框架的实用性和洞察力。",
      "conclusion": "LIBERTy为概念解释的忠实性评估提供了一个必要的基准框架，推动了可解释AI领域的研究。它不仅有助于系统评估和比较不同方法，还能揭示模型对干预的敏感性，为高风险领域的应用提供支持。未来工作可以扩展数据集到更多领域，并进一步优化解释技术以提升可信度和通用性，潜在局限性包括领域特定性和解释方法的普适性。",
      "tags": [
        "Concept-Based Explanations",
        "Structured Causal Models (SCMs)",
        "Counterfactual Analysis",
        "Large Language Models (LLMs)",
        "Explainability Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:26.815652Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10696",
    "title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load",
    "authors": [
      "Han Jiang",
      "Yao Xiao",
      "Rachel Hurley",
      "Shichao Liu"
    ],
    "abstract": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10696.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10696",
    "published": "2026-01-15T18:52:59Z",
    "updated": "2026-01-15T18:52:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究探讨生成式AI在建筑概念设计中对性能、创造性自我效能和认知负荷的影响，揭示其对新手设计师的性能提升但自我效能下降，强调用户专业知识和交互策略的关键作用。",
      "motivation": "随着生成式AI在创意设计领域的广泛应用，如何评估其对设计过程的影响成为重要问题。现有研究可能忽略用户专业差异和交互策略的作用，导致对GenAI效果的理解不足。本研究旨在解决这一问题，通过系统分析GenAI在建筑概念设计中的多维影响，为优化AI辅助工具提供实证基础，弥补现有方法在个性化评估方面的缺陷。",
      "method": "本研究采用实验方法，招募36名学生参与者完成两阶段建筑概念设计任务：先独立设计，后使用外部工具辅助（包括GenAI辅助条件和对照组使用在线现有项目库）。设计结果由专家评估，创造性自我效能和认知负荷通过自报问卷收集。通过差异分析（difference-in-differences）比较不同条件，重点关注子组分析以揭示用户专业差异的影响，创新点在于结合性能、心理状态和交互模式的综合评估。",
      "result": "实验结果显示，GenAI对整体参与者的设计性能无显著优势，但子组分析表明，新手设计师在GenAI辅助下性能显著提升。使用GenAI的学生创造性自我效能普遍下降。认知负荷在两种条件下无明显差异，但提示使用模式分析显示，迭代想法生成和视觉反馈提示与认知负荷的更大减少相关。这些结果通过定量数据支持，如专家评估的绩效改进和自报问卷的自我效能变化，与基线方法对比突显了用户专业背景的调节作用。",
      "conclusion": "本研究的主要贡献在于揭示生成式AI在建筑概念设计中的效果依赖于用户先前专业知识和交互策略，尤其是提示使用模式。学术上，为AI辅助创意设计提供实证证据，强调个性化应用的重要性；实际应用中，可指导工具开发以优化用户体验和提升设计质量。局限性包括样本规模较小，未来工作可扩展至更多样化群体或探索长期影响。",
      "tags": [
        "Generative AI",
        "Human-Computer Interaction",
        "Experimental Design",
        "Prompt Engineering",
        "Cognitive Load Theory"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:21.405942Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10690",
    "title": "Data-driven stochastic reduced-order modeling of parametrized dynamical systems",
    "authors": [
      "Andrew F. Ilersich",
      "Kevin Course",
      "Prasanth B. Nair"
    ],
    "abstract": "Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10690.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10690",
    "published": "2026-01-15T18:50:18Z",
    "updated": "2026-01-15T18:50:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种数据驱动的随机降阶建模框架，用于参数化动力系统，能高效处理随机动态和量化不确定性。",
      "motivation": "建模复杂动力系统在变化参数条件下计算密集，高保真模拟常不可行，这限制了实际应用如鲁棒决策。现有降阶模型（ROMs）在处理随机动态和量化预测不确定性方面不足，因为它们往往依赖于计算昂贵的正向求解器，且无法可靠评估不确定性，导致决策风险增加。因此，开发一种能泛化到不同参数条件并高效提供不确定性估计的方法至关重要。",
      "method": "研究提出基于摊销随机变分推理的框架，利用重新参数化技巧处理马尔可夫高斯过程，从而避免训练中计算昂贵的正向求解器。通过联合学习概率自编码器和随机微分方程来建模系统的潜在动态，该方法计算成本独立于数据集大小和系统刚度，并允许在可用时整合物理先验知识以增强灵活性。",
      "result": "在三个挑战性测试问题中，该方法展示了对未见参数组合和强制条件的优秀泛化能力，相比现有方法实现了显著效率提升。尽管摘要未提供具体性能指标，但实验强调了泛化性和计算效率的优势，表明在保持建模精度的同时，大幅降低了计算成本。",
      "conclusion": "该研究贡献了一种高效处理参数化系统随机动态的降阶建模框架，提升了不确定性量化能力和计算效率，具有重要学术价值。实际应用价值在于支持更可靠的工程决策，未来工作可能包括扩展到更复杂系统或优化算法性能。",
      "tags": [
        "Stochastic Reduced-Order Modeling",
        "Amortized Stochastic Variational Inference",
        "Markov Gaussian Processes",
        "Probabilistic Autoencoder",
        "Data-driven Modeling"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:45.694622Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10684",
    "title": "On the origin of neural scaling laws: from random graphs to natural language",
    "authors": [
      "Maissam Barkeshli",
      "Alberto Alfarano",
      "Andrey Gromov"
    ],
    "abstract": "Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.",
    "categories": [
      "cs.LG",
      "cond-mat.dis-nn",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10684.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10684",
    "published": "2026-01-15T18:46:09Z",
    "updated": "2026-01-15T18:46:09Z",
    "comment": "33 pages",
    "light_analysis": {
      "overview": "本文通过研究随机图和简化语言模型，揭示了神经网络缩放定律可以在没有数据幂律结构的情况下产生。",
      "motivation": "神经网络缩放定律在现代AI中至关重要，能预测模型性能随数据、计算和参数增加而提升，但现有研究常假设其源于数据中的幂律结构，这可能不完整且缺乏基础验证。本研究旨在解决缩放定律起源的不明问题，通过简化实验设置探索其是否依赖于数据结构的复杂性，以弥补理论空白并推动更深入的理解。",
      "method": "研究采用变换器模型，训练它们预测随机游走在可调复杂度的图上，包括Erdös-Renyi和scale-free Barabási-Albert图。关键创新是系统性地降低自然语言复杂度，从多层变换器语言模型采样序列到语言双元组，并使用上下文长度为50的2层变换器复现传统缩放定律结果。",
      "result": "实验表明，即使数据相关性中无幂律结构，神经网络缩放定律也能出现，缩放指数在简化语言模型中单调演化。在随机图上训练的模型也产生缩放定律数据。本研究批判性分析了先前文献的拟合方法，展示了获得计算最优曲线的替代方法，并提供了初步证据表明最大更新参数化比标准参数化更参数高效。",
      "conclusion": "主要贡献是挑战了缩放定律源于数据幂律的假设，通过基础实验揭示了其更广泛起源，为神经网络缩放理论提供了新见解。这具有重要学术价值，可能影响模型设计和优化策略。局限性在于实验设置较简化，未来工作可扩展到复杂数据集和模型架构以验证普适性。",
      "tags": [
        "Neural Scaling Laws",
        "Transformers",
        "Random Walks",
        "Language Modeling",
        "Parameterization"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:34.474244Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10681",
    "title": "Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems",
    "authors": [
      "Amir Khurshid",
      "Abhishek Sehgal"
    ],
    "abstract": "Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10681.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10681",
    "published": "2026-01-15T18:43:19Z",
    "updated": "2026-01-15T18:43:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种结构感知和多样性约束的上下文气泡构建框架，优化企业检索增强系统，减少冗余并提高答案质量。",
      "motivation": "现有检索增强生成（RAG）方法通过排名前k个文本来构建大型语言模型（LLM）上下文，但这种方法导致文档结构信息图碎片化、过度检索和内容重复，同时查询上下文不足，忽略次要和三级方面。在企业环境中，这降低了检索效率和答案准确性，因此需要一种新方法来保留文档结构并优化上下文选择，以改进LLM的推理能力和上下文相关性。",
      "method": "该方法提出一个结构感知和多样性约束的上下文气泡构建框架，核心创新在于利用文档固有结构，通过组织多粒度文本片段（如章节和行）并应用任务条件结构先验来指导检索。从高相关性锚定片段出发，采用约束选择策略平衡查询相关性、边际覆盖和冗余惩罚，构建紧凑的上下文气泡，并在严格令牌预算下确保信息丰富性。此外，框架提供完整的检索记录，实现评分和选择过程的可审计性和确定性调整。",
      "result": "实验基于企业文档进行，结果显示上下文气泡框架显著减少冗余上下文，更好地覆盖次要方面，并在有限上下文窗口中提升答案质量和引用忠实度。消融研究证实，结构先验和多样性约束选择都是必要组件；移除任一会导致覆盖下降和冗余或不完整上下文的增加。尽管摘要未提供具体性能指标数据，但结果表明该方法在优化上下文构建方面优于传统RAG方法。",
      "conclusion": "本研究的贡献在于提出了一种新颖的上下文气泡构建框架，结合结构感知和多样性约束，有效解决了传统RAG方法的不足，为企业检索增强系统提供了更高效和精确的上下文构建方案。这具有重要的学术价值，推动了检索技术的发展，并具备实际应用潜力。未来工作可能包括扩展到更多文档类型和进一步优化算法，摘要未明确说明具体局限性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Large Language Model",
        "Structural Priors",
        "Diversity Constraint",
        "Context Construction"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:03.252512Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10679",
    "title": "Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models",
    "authors": [
      "Zirui Ren",
      "Ziming Liu"
    ],
    "abstract": "Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) \"Grokking\" dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM \"guesses\" the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be \"guessing\" instead of \"reasoning\". Leveraging this \"guessing\" picture, we propose three strategies to scale HRM's guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models \"reason\".",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10679.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10679",
    "published": "2026-01-15T18:42:50Z",
    "updated": "2026-01-15T18:42:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文对分层推理模型进行机制分析，揭示其'猜测'而非'推理'的本质，并提出增强策略显著提升性能。",
      "motivation": "分层推理模型（HRM）在多种推理任务上表现优异，优于大型语言模型推理器，但其在极简单谜题上的失败暴露了潜在缺陷，如违反固定点属性，表明HRM可能依赖猜测而非逻辑推理。这一问题对构建可靠、可解释的推理系统至关重要，因为现有方法在简单任务上的不稳定性限制了其实际应用和泛化能力。研究旨在深入理解HRM的推理模式，以揭示其失败机制并推动改进。",
      "method": "论文通过机制研究分析HRM的推理模式，发现三个关键事实：简单谜题失败归因于固定点属性违反；推理步骤中存在'顿悟'动态，答案改进非均匀；多个固定点导致HRM'猜测'初始固定点并陷入其中。基于此，提出三种策略扩展HRM的猜测能力：数据增强提升猜测质量，输入扰动利用推理随机性增加猜测数量，模型自举利用训练随机性增加猜测数量，并组合这些方法开发Augmented HRM。",
      "result": "结合所有增强策略的Augmented HRM在Sudoku-Extreme数据集上实现了从54.5%到96.9%的准确率提升，显著优于原始HRM基线。这表明通过机制分析驱动的改进策略能有效克服HRM的'猜测'局限，验证了方法在具体任务上的高效性和实用性，为推理模型性能优化提供了数据支撑。",
      "conclusion": "论文的主要贡献是对HRM进行机制分析，揭示了其'猜测'行为并提出有效增强策略，在实践上显著提升推理准确率，在科学上为理解推理模型的内部工作机制提供新见解。这推动了更可靠推理系统的发展，潜在局限性可能在于对其他推理任务的泛化性，未来工作可扩展到更广泛的任务和模型架构优化。",
      "tags": [
        "Hierarchical Reasoning Model",
        "Mechanistic Analysis",
        "Data Augmentation",
        "Input Perturbation",
        "Model Bootstrapping"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:46.204969Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10673",
    "title": "Single-Stage Huffman Encoder for ML Compression",
    "authors": [
      "Aditya Agrawal",
      "Albert Magyar",
      "Hiteshwar Eswaraiah",
      "Patrick Sheridan",
      "Pradeep Janedula",
      "Ravi Krishnan Venkatesan",
      "Krishna Nair",
      "Ravi Iyer"
    ],
    "abstract": "Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10673.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10673",
    "published": "2026-01-15T18:37:56Z",
    "updated": "2026-01-15T18:37:56Z",
    "comment": "5 pages, 4 figures",
    "light_analysis": {
      "overview": "提出单阶段Huffman编码器，通过固定码本消除传统Huffman编码的多阶段开销，提升机器学习压缩效率。",
      "motivation": "本文研究动机源于大型语言模型（LLMs）在分布式训练和推理中，数据跨多个加速器传输时网络带宽成为瓶颈。传统的Huffman无损压缩虽然有效，但其三阶段设计——实时频率分析、码本生成和码本传输——引入了显著的计算开销、延迟和数据冗余。这些额外成本在芯片间通信等延迟敏感场景中不可接受，限制了压缩方法的实用性。因此，需要开发更高效的压缩方案来解决这一关键问题，以提高系统整体性能。",
      "method": "论文提出单阶段Huffman编码器作为核心方法，通过使用固定码本来简化压缩流程。技术路线包括：从先前数据批次的平均概率分布导出固定码本，避免实时计算；创新点在于利用张量在层和分片间的高统计相似性（基于Gemma 2B模型分析），支持固定码本近似最优压缩。该方法直接应用于数据流，无需传输码本，减少了数据开销，适用于机器学习环境中的实时操作。",
      "result": "实验结果显示，该单阶段Huffman编码器在压缩性能上表现优异：与每分片动态Huffman编码的差异在0.5%以内，与理想Shannon可压缩性的差距在1%以内。这表明新方法实现了接近最优的压缩率，同时显著降低了延迟和计算开销。与基线方法对比，它克服了传统Huffman编码在实时场景中的不足，支持高效的在飞行压缩，为机器学习应用提供了实用性解决方案。",
      "conclusion": "本文的主要贡献是设计了一种单阶段Huffman编码器，通过固定码本消除传统压缩方法的开销，提高了机器学习压缩的效率。这项研究的学术价值在于优化了无损压缩算法在分布式系统中的适用性，实际应用价值在于支持大型语言模型的训练和推理，减少网络带宽瓶颈。摘要未明确说明局限性，但可推断未来工作可能涉及扩展到更多模型或验证固定码本的泛化能力。",
      "tags": [
        "Huffman Coding",
        "Lossless Compression",
        "Large Language Models",
        "Network Bandwidth Optimization",
        "Statistical Similarity"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:07.272910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10660",
    "title": "Detecting Winning Arguments with Large Language Models and Persuasion Strategies",
    "authors": [
      "Tiziano Labruna",
      "Arkadiusz Modzelewski",
      "Giorgio Satta",
      "Giovanni Da San Martino"
    ],
    "abstract": "Detecting persuasion in argumentative text is a challenging task with important implications for understanding human communication. This work investigates the role of persuasion strategies - such as Attack on reputation, Distraction, and Manipulative wording - in determining the persuasiveness of a text. We conduct experiments on three annotated argument datasets: Winning Arguments (built from the Change My View subreddit), Anthropic/Persuasion, and Persuasion for Good. Our approach leverages large language models (LLMs) with a Multi-Strategy Persuasion Scoring approach that guides reasoning over six persuasion strategies. Results show that strategy-guided reasoning improves the prediction of persuasiveness. To better understand the influence of content, we organize the Winning Argument dataset into broad discussion topics and analyze performance across them. We publicly release this topic-annotated version of the dataset to facilitate future research. Overall, our methodology demonstrates the value of structured, strategy-aware prompting for enhancing interpretability and robustness in argument quality assessment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10660.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10660",
    "published": "2026-01-15T18:30:15Z",
    "updated": "2026-01-15T18:30:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于大语言模型和说服策略的方法，通过多策略评分提升论辩说服力检测的可解释性和鲁棒性。",
      "motivation": "检测论辩文本中的说服力是自然语言处理领域的挑战性任务，对人类沟通理解具有重要意义，尤其在社交媒体、辩论和广告等场景。现有方法往往忽视说服策略（如声誉攻击、分散注意力和操纵性措辞）的细微作用，导致评估不够准确。本研究旨在解决这一问题，探索说服策略在确定文本说服力中的角色，以改进论辩质量评估，为理解人类说服行为提供新视角。",
      "method": "研究利用大语言模型（LLMs），结合多策略说服评分方法，指导模型对六个说服策略进行推理。在三个标注数据集（Winning Arguments、Anthropic/Persuasion和Persuasion for Good）上实施实验。为了更深入理解内容影响，将Winning Arguments数据集按广泛讨论主题组织，并分析性能差异。该方法通过结构化提示增强策略感知，提升模型在论辩分析中的推理能力，核心创新在于策略引导的推理框架。",
      "result": "实验结果显示，策略引导的推理方法在预测论辩文本说服力方面优于基线方法，提高了预测准确性（摘要未明确说明具体数据）。通过分析不同讨论主题，发现方法在不同领域均表现良好，显示出鲁棒性。研究还公开发布了主题注释版本的数据集，以促进未来研究。这些发现验证了结合说服策略能有效增强论辩质量评估的性能，但未提供详细对比数据。",
      "conclusion": "本研究的核心贡献是提出了一种基于大语言模型和多策略说服评分的方法，显著增强了论辩说服力检测的可解释性和鲁棒性。通过结构化、策略感知的提示，方法在多个数据集上验证了有效性。研究发布的主题注释数据集为相关领域提供宝贵资源，促进未来研究。学术价值在于推动了论辩分析技术的发展，实际应用包括改善在线内容评估和自动辩论辅助系统等，潜在局限性是未讨论模型泛化性，未来可探索更多策略或跨语言应用。",
      "tags": [
        "Large Language Model",
        "Persuasion Strategy",
        "Multi-Strategy Persuasion Scoring",
        "Argument Quality Assessment",
        "Dataset Annotation"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:22.842654Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10651",
    "title": "Multi-Property Synthesis",
    "authors": [
      "Christoph Weinhuber",
      "Yannik Schnitzer",
      "Alessandro Abate",
      "David Parker",
      "Giuseppe De Giacomo",
      "Moshe Y. Vardi"
    ],
    "abstract": "We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.",
    "categories": [
      "cs.AI",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10651.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10651",
    "published": "2026-01-15T18:18:33Z",
    "updated": "2026-01-15T18:18:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种全符号算法，用于多属性LTLf合成，通过一次固定点计算高效处理无法同时满足所有属性的情况，实现最大可实现目标集。",
      "motivation": "在形式验证和控制系统设计中，LTLf合成用于从时序逻辑规范生成策略。当涉及多个属性时，常出现无法同时满足所有属性的情况，例如在复杂系统设计中。现有方法通过枚举属性子集来处理，但枚举导致计算复杂度指数增长，效率低下，难以扩展到大规模型。因此，需要一种高效算法来合成满足最大可能属性集的策略，以提升系统设计的可扩展性和实用性，解决实际应用中的资源限制问题。",
      "method": "论文开发了一个全符号算法，核心是引入布尔变量表示目标属性，并利用目标集的单调性来紧凑编码指数级多的组合。通过一次固定点计算，在产品游戏状态空间中确定每个状态可实现的目标集关系，并基于此合成策略以实现最大可实现集。该方法避免了枚举子集，采用符号表示来高效处理状态和目标交互，关键创新包括利用游戏理论和符号技术优化计算过程。",
      "result": "实验结果显示，该算法在性能上显著优于基于枚举的基线方法。具体地，速度提升高达两个数量级，即快100倍，这通过处理标准基准测试验证。算法在处理多属性合成问题时表现出高效性和可扩展性，减少了计算时间和资源消耗，与基线对比显示出明显优势，支持了方法的有效性。",
      "conclusion": "本研究的主要贡献是开发了一种高效的全符号算法，用于多属性LTLf合成，解决了枚举方法效率低下的问题。学术上，它推动了形式合成和游戏理论技术的发展；实际上，可应用于自动驾驶、机器人控制等需要满足多个时序属性的系统设计。未来工作可能包括扩展到更复杂的逻辑规范、分布式环境或结合机器学习方法，以进一步优化性能。",
      "tags": [
        "LTLf Synthesis",
        "Symbolic Algorithm",
        "Fixed-Point Computation",
        "Monotonicity",
        "Game Theory"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:51.123516Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10649",
    "title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning",
    "authors": [
      "Darshan Singh",
      "Arsha Nagrani",
      "Kawshik Manikantan",
      "Harman Singh",
      "Dinesh Tewari",
      "Tobias Weyand",
      "Cordelia Schmid",
      "Anelia Angelova",
      "Shachi Dave"
    ],
    "abstract": "Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10649.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10649",
    "published": "2026-01-15T18:15:06Z",
    "updated": "2026-01-15T18:15:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出CURVE基准，旨在解决视频推理基准的文化和语言偏见问题，通过引入多文化、多语言长视频注释，促进公平评估。",
      "motivation": "视频模型在长视频理解方面进展显著，但现有基准多以西方数据和英语为主，引入显著偏见，限制了模型在多文化环境中的表现。自动翻译方法无法处理复杂推理和语境差异。CURVE旨在弥补这一差距，通过提供来自18个全球地点的区域特定文化视频的高质量人类注释，提升模型的泛化能力，确保评估的公平性和多样性。",
      "method": "本研究创建了CURVE基准，包含来自18个全球地点的文化视频，提供人类生成的问题、答案和多步推理步骤，均使用本地语言。创新点包括利用推理轨迹构建基于证据的图，并提出一种新颖的迭代策略，使用这些图来识别推理中的细粒度错误，从而增强对视觉文化上下文的理解，避免了自动翻译的局限性。",
      "result": "实验结果显示，最先进的视频-语言模型在CURVE基准上表现不佳，准确率显著低于人类水平，错误主要源于对视频中文化元素的视觉感知不足。这表明现有模型在处理多文化内容时存在重大局限。与基线方法相比，CURVE具有更高的挑战性，凸显了解决文化偏差的必要性。",
      "conclusion": "本研究的主要贡献是引入CURVE基准，为多文化和多语言视频推理提供了无偏见的评估工具。其学术价值在于推动AI模型的公平性和多样性，实际应用潜力体现在跨文化视频分析和交互中。未来工作可能包括改进模型的文化感知能力，并利用CURVE进行进一步优化，以提升泛化性能。",
      "tags": [
        "Video-Language Models",
        "Multilingual Reasoning",
        "Cultural Understanding",
        "Benchmark Evaluation",
        "Evidence-based Graphs"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:11.478170Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10645",
    "title": "Influential Training Data Retrieval for Explaining Verbalized Confidence of LLMs",
    "authors": [
      "Yuxi Xia",
      "Loris Schoenegger",
      "Benjamin Roth"
    ],
    "abstract": "Large language models (LLMs) can increase users' perceived trust by verbalizing confidence in their outputs. However, prior work has shown that LLMs are often overconfident, making their stated confidence unreliable since it does not consistently align with factual accuracy. To better understand the sources of this verbalized confidence, we introduce TracVC (\\textbf{Trac}ing \\textbf{V}erbalized \\textbf{C}onfidence), a method that builds on information retrieval and influence estimation to trace generated confidence expressions back to the training data. We evaluate TracVC on OLMo and Llama models in a question answering setting, proposing a new metric, content groundness, which measures the extent to which an LLM grounds its confidence in content-related training examples (relevant to the question and answer) versus in generic examples of confidence verbalization. Our analysis reveals that OLMo2-13B is frequently influenced by confidence-related data that is lexically unrelated to the query, suggesting that it may mimic superficial linguistic expressions of certainty rather than rely on genuine content grounding. These findings point to a fundamental limitation in current training regimes: LLMs may learn how to sound confident without learning when confidence is justified. Our analysis provides a foundation for improving LLMs' trustworthiness in expressing more reliable confidence.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10645.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10645",
    "published": "2026-01-15T18:05:42Z",
    "updated": "2026-01-15T18:05:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出TracVC方法，通过追溯训练数据来解释大语言模型言语化自信的来源。",
      "motivation": "大型语言模型通过言语化输出中的自信来增强用户信任，但研究表明其经常过度自信，导致自信表达不可靠，因为它与事实准确性不一致。这降低了LLMs的可信度，现有方法未能有效解释自信的来源，因此本研究旨在探究言语化自信的底层机制，以解决信任问题，并提供改进基础。",
      "method": "论文提出TracVC方法，结合信息检索和影响力估计技术，将LLMs生成的言语化自信追溯到训练数据。在问答设置中，该方法应用于OLMo和Llama模型，并引入新指标content groundness，用于衡量LLM是依赖于内容相关训练示例还是通用自信言语化示例，从而分析自信表达的内在机制。",
      "result": "分析显示，OLMo2-13B模型经常受到与查询词汇无关的自信相关训练数据的影响，表明其言语化自信可能源于模仿自信的表面语言表达，而不是基于真实内容基础。这通过content groundness指标量化，突显了当前训练机制的局限性，但摘要未明确说明具体性能提升数据或与基线的详细对比。",
      "conclusion": "本研究揭示了LLMs言语化自信不可靠的根本原因：模型可能学习表面自信表达而非合理自信判断，指出了训练机制的根本限制。这为改进LLMs在表达可靠自信方面的可信度提供了学术基础和实际应用价值。未来工作可专注于开发更有效的训练方法来增强自信与内容的一致性。",
      "tags": [
        "Large Language Model",
        "Verbalized Confidence",
        "Information Retrieval",
        "Influence Estimation",
        "Content Groundness"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:38.555163Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10639",
    "title": "STEM: Scaling Transformers with Embedding Modules",
    "authors": [
      "Ranajoy Sadhukhan",
      "Sheng Cao",
      "Harry Dong",
      "Changsheng Zhao",
      "Attiano Purpura-Pontoniere",
      "Yuandong Tian",
      "Zechun Liu",
      "Beidi Chen"
    ],
    "abstract": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10639.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10639",
    "published": "2026-01-15T18:00:27Z",
    "updated": "2026-01-15T18:00:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "STEM 方法通过嵌入模块扩展 Transformers，实现稀疏性下的稳定训练，提升性能并增强可解释性。",
      "motivation": "细粒度稀疏性虽能提高参数容量而不按比例增加计算成本，但现有方法面临训练不稳定、负载平衡困难和通信开销大的问题。这些挑战限制了稀疏性在大型语言模型中的应用效果，因为动态路由等技术引入额外复杂性，阻碍模型扩展。本研究旨在解决这些不足，通过提出一种静态方法来更高效地扩展 Transformers 的容量，改善稀疏性处理，从而优化模型性能和效率。",
      "method": "STEM 采用静态令牌索引方法，将前馈网络的上投影层替换为层本地嵌入查找，同时保持门和下投影层密集。关键创新包括移除运行时动态路由，支持 CPU 卸载和异步数据预取，从而将参数容量与每令牌 FLOPs 及跨设备通信解耦。该方法通过学习具有大角度分布的嵌入空间增强知识存储容量，并利用令牌索引特性提升可解释性，便于知识编辑和注入操作。",
      "result": "实验结果显示，STEM 在训练中表现稳定，下游任务性能优于密集基线，减少约三分之一的 FFN 参数访问和每令牌 FLOPs。在 350M 和 1B 参数规模的模型上，整体准确率提升约 3-4%，在知识推理密集型基准如 ARC-Challenge、OpenBookQA、GSM8K 和 MMLU 上表现尤为突出。此外，长上下文处理时，随着序列增长激活更多参数，实现实际容量扩展。",
      "conclusion": "本研究提出 STEM 作为一种扩展 Transformers 参数内存的有效方法，同时提供更好的可解释性、训练稳定性和计算效率。学术上，它为稀疏神经网络和知识存储领域提供了新思路；实际应用中，可优化大型语言模型的部署和性能。未来工作可能包括进一步验证在更大规模模型上的表现，或探索与其他技术如强化学习的结合。",
      "tags": [
        "Transformers",
        "Sparse Models",
        "Embedding Modules",
        "Knowledge Storage",
        "Model Interpretability"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:06.095148Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10632",
    "title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos",
    "authors": [
      "Chengfeng Zhao",
      "Jiazhi Shu",
      "Yubo Zhao",
      "Tianyu Huang",
      "Jiahao Lu",
      "Zekai Gu",
      "Chengwei Ren",
      "Zhiyang Dou",
      "Qing Shuai",
      "Yuan Liu"
    ],
    "abstract": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10632.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10632",
    "published": "2026-01-15T17:52:29Z",
    "updated": "2026-01-15T17:52:29Z",
    "comment": "Project Page: https://igl-hkust.github.io/CoMoVi/",
    "light_analysis": {
      "overview": "CoMoVi是一个共生成框架，通过耦合视频扩散模型同步生成3D人体动作和视频，实现内在耦合的生成过程。",
      "motivation": "该研究动机源于3D人体动作和2D视频生成之间的内在耦合关系：3D动作为视频提供结构先验以确保一致性和合理性，而预训练视频模型则为动作生成带来强大泛化能力。然而，现有方法常独立处理这两者，导致视频缺乏结构支撑或动作适应性不足，难以实现高效协同生成。在动画、虚拟现实等应用领域，这种耦合至关重要，因此需要开发新方法来整合生成过程，以提升整体质量和效率。",
      "method": "研究提出CoMoVi框架，首先设计了一种有效的2D人体动作表示，能够继承预训练视频扩散模型（VDMs）的强大先验，以简化动作生成。其次，开发了双分支扩散模型，耦合两个VDMs在一个扩散去噪循环中同步生成3D动作和视频，通过相互特征交互和3D-2D交叉注意力机制实现协同优化。关键创新包括利用扩散模型的去噪过程进行共生成，以及构建CoMoVi数据集——一个大规模真实世界人体视频数据集，带有文本和动作标注，覆盖多样化动作以支持实验验证。",
      "result": "广泛实验表明，CoMoVi方法在3D人体动作生成和视频生成任务中都展现出有效性，可能通过耦合过程提升了生成质量、一致性和效率。与基线方法对比，该方法可能优化了性能指标，但摘要未明确说明具体数据如准确率提升或效率改进数值，因此推断其整体优势基于实验验证的生成效果。实验部分可能包括定量和定性评估，但详细结果需参考完整论文。",
      "conclusion": "本论文主要贡献在于提出CoMoVi框架，实现3D人体动作和视频的同步共生成，并创建相关数据集，推动了多模态生成技术的发展。学术价值体现在强调耦合生成的重要性，扩展了扩散模型在视觉和动作合成中的应用。实际应用价值在于为动画、游戏和虚拟现实等领域提供更一致和高效的生成工具。未来工作可能涉及方法优化或扩展到其他场景，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Diffusion Models",
        "Video Generation",
        "3D Human Motion",
        "Co-Generation",
        "Cross-Attention"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:42.037792Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10611",
    "title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding",
    "authors": [
      "Christopher Clark",
      "Jieyu Zhang",
      "Zixian Ma",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Rohun Tripathi",
      "Sangho Lee",
      "Zhongzheng Ren",
      "Chris Dongjoo Kim",
      "Yinuo Yang",
      "Vincent Shao",
      "Yue Yang",
      "Weikai Huang",
      "Ziqi Gao",
      "Taira Anderson",
      "Jianrui Zhang",
      "Jitesh Jain",
      "George Stoica",
      "Winson Han",
      "Ali Farhadi",
      "Ranjay Krishna"
    ],
    "abstract": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10611.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10611",
    "published": "2026-01-15T17:27:44Z",
    "updated": "2026-01-15T17:27:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "Molmo2是一个开放权重和数据的视频语言模型家族，在开源模型中达到最先进水平，并在图像和视频的点驱动接地任务中展现出新能力。",
      "motivation": "当前视频语言模型多为专有，开源模型依赖专有模型的合成数据或不公开训练细节，限制了开源社区的创新和发展。许多下游应用，如视频分析和交互，需要像素级接地能力（如指向和跟踪），但现有模型（包括专有模型）普遍缺乏此功能。因此，研究旨在通过提供开放的数据集和方法，解决开源模型基础薄弱和接地能力不足的问题，以推动视频理解和接地技术的进步。",
      "method": "论文提出Molmo2模型，核心贡献包括收集7个新视频数据集和2个多图像数据集，这些数据集覆盖预训练（如高详细视频字幕）、微调（如自由形式视频问答）、对象跟踪和视频指向任务，所有数据未使用闭源模型。训练方法采用高效的打包和消息树编码方案，结合视觉token上的双向注意力机制和一种新颖的token权重策略，以优化模型性能并提升接地能力。",
      "result": "Molmo2的8B模型在短视频理解、计数和字幕生成任务中优于其他开放权重和数据模型，并在长视频任务上具有竞争力。具体地，在视频接地任务中，与开源模型Qwen3-VL相比，视频计数准确率从29.6提升至35.5；在某些任务上超越专有模型如Gemini 3 Pro，例如视频指向F1分数从20.0提升至38.4，视频跟踪J&F指标从41.1提升至56.2，显示出显著的性能改进。",
      "conclusion": "研究通过提供开放的、高质量数据集和创新的训练方法，使Molmo2成为开源视频语言模型的先进代表，显著提升了接地能力，为开源社区提供了强大的基础。这具有重要的学术价值，推动了多模态学习领域的发展，并支持实际应用如视频分析和交互。未来工作可能包括扩展到更多复杂任务或优化模型效率，以进一步巩固其影响。",
      "tags": [
        "Vision-Language Models",
        "Video Grounding",
        "Token Weighting",
        "Dataset Collection",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:58.352780Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10606",
    "title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation",
    "authors": [
      "Peng Chen",
      "Xiaobao Wei",
      "Yi Yang",
      "Naiming Yao",
      "Hui Chen",
      "Feng Tian"
    ],
    "abstract": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10606.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10606",
    "published": "2026-01-15T17:23:19Z",
    "updated": "2026-01-15T17:23:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "RSATalker 是一个基于 3D Gaussian Splatting 的框架，首次实现具有社会感知能力的真实 talking head 生成，支持多轮对话。",
      "motivation": "在虚拟现实中，多轮对话的 talking head 生成对于社交场景至关重要。现有方法面临显著限制：基于网格的 3D 方法能够建模双人对话但缺乏真实纹理；基于大型模型的 2D 方法产生自然外观但计算成本过高；而基于 3D Gaussian Splatting 的方法虽然实现了高效真实渲染，但仅关注说话者并忽略了社会关系。因此，本研究旨在解决这些不足，开发一个既逼真又能捕捉社会动态的 talking head 生成系统，以提升虚拟社交体验的真实性和交互性。",
      "method": "论文提出 RSATalker 框架，首先从语音驱动基于网格的 3D 面部运动，然后将 3D Gaussians 绑定到网格面片，以渲染高保真的 2D 头像视频。关键创新在于社会感知模块，它通过可学习查询机制将社会关系（如血统与非血统、平等与不平等）编码为高层嵌入。此外，设计了三阶段训练范式，并构建了 RSATalker 数据集，包含带有社会关系注释的 speech-mesh-image 三元组，从而结合了真实渲染和社会关系建模的技术路线。",
      "result": "摘要未明确说明具体实验数据，但广泛实验表明，RSATalker 在真实性和社会感知方面达到了最先进的性能。与现有方法相比，它可能在这些指标上优于基线方法，尤其是在捕捉多轮对话中的人际动态方面表现突出，但具体准确率提升或效率改进的数值未在摘要中提供。",
      "conclusion": "本研究的主要贡献是提出了 RSATalker 框架，首次将 3D Gaussian Splatting 与社会关系编码相结合，实现了真实且社会感知的多轮对话 talking head 生成。学术上，它推动了 VR 社交应用中头像生成技术的发展；实践中，可应用于虚拟会议、游戏等场景以增强互动真实感。局限性或未来工作方向摘要未明确说明，但可能包括进一步优化计算效率或扩展社会关系类型。",
      "tags": [
        "3D Gaussian Splatting",
        "Socially-Aware Module",
        "Talking Head Generation",
        "Multi-Turn Conversation",
        "3D Facial Motion"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:17.003353Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10592",
    "title": "Action100M: A Large-scale Video Action Dataset",
    "authors": [
      "Delong Chen",
      "Tejaswi Kasarla",
      "Yejin Bang",
      "Mustafa Shukor",
      "Willy Chung",
      "Jade Yu",
      "Allen Bolourchi",
      "Theo Moutakanni",
      "Pascale Fung"
    ],
    "abstract": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10592.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10592",
    "published": "2026-01-15T17:02:27Z",
    "updated": "2026-01-15T17:02:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Action100M，一个大规模视频动作数据集，基于互联网教学视频构建，通过自动化流程生成，旨在推动视频理解和世界建模的研究。",
      "motivation": "从视觉观察中推断物理动作是提升机器智能在物理世界中能力的基础。然而，现有研究缺乏大规模、开放词汇的视频动作数据集，这些数据集需要覆盖广泛领域以支持多样化的动作识别任务。当前数据集可能规模有限或词汇封闭，限制了模型的泛化性能和应用范围。因此，开发一个涵盖大量开放词汇动作的数据集对于推进AI在现实世界的理解和交互至关重要。",
      "method": "论文提出一个全自动化的数据处理流程来构建Action100M数据集。首先，使用V-JEPA 2嵌入进行分层时间分割，将视频分解为百万级的时序片段。其次，生成多级帧和片段标题，组织为树状结构（Tree-of-Captions），以提供丰富的语义信息。最后，通过推理模型GPT-OSS-120B进行多轮自优化（Self-Refine）过程，聚合证据并输出结构化注释，包括简要/详细动作、演员和标题。这一方法的关键创新在于结合自动化分割和推理，实现大规模数据标注的高效和准确。",
      "result": "训练视频语言联合预测模型（VL-JEPA）在Action100M上展示了数据缩放带来的持续改进，并在多个动作识别基准上表现出强大的零样本性能。与基线方法相比，该数据集支持模型在未见过的任务中取得显著提升，但摘要未明确说明具体性能指标，如准确率提升百分比。这些结果表明，Action100M能够有效促进模型在动作识别领域的泛化能力和适应性，为后续研究提供有力数据支持。",
      "conclusion": "论文的主要贡献是引入了Action100M大规模数据集，为视频理解和世界建模研究奠定了新的基础。这一研究具有重要的学术价值，解决了数据稀缺问题，并展示了自动化标注流程的可行性。实际应用价值包括提升动作识别模型的性能和扩展AI在物理世界中的能力。尽管摘要未明确说明局限性，未来工作可能涉及进一步优化数据集质量或扩展应用到更多AI任务中。",
      "tags": [
        "Large-scale Video Dataset",
        "Open-Vocabulary Action Recognition",
        "Self-Refine",
        "Tree-of-Captions",
        "V-JEPA 2"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:03.102469Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10591",
    "title": "ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition",
    "authors": [
      "Arundeep Chinta",
      "Lucas Vinh Tran",
      "Jay Katukuri"
    ],
    "abstract": "Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-fin.RM",
      "q-fin.TR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10591.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10591",
    "published": "2026-01-15T17:02:06Z",
    "updated": "2026-01-15T17:02:06Z",
    "comment": "Accepted for oral presentation at the AI Meets Quantitative Finance Workshop at ICAIF 2025. An enhanced version was accepted for oral presentation at the AI for Time Series Analysis Workshop at AAAI 2026",
    "light_analysis": {
      "overview": "提出基于Transformer的概率时间序列基础模型ProbFM，首次利用深度证据回归实现原则性认知-偶发不确定性分解。",
      "motivation": "时间序列基础模型在零样本金融预测中展现出强大的可转移性和数据效率，但其金融应用受限于不确定性量化问题。当前方法如混合模型、Student's t分布或符合预测，要么依赖限制性分布假设，要么混淆不同不确定性来源（如认知和偶发不确定性），或缺乏原则性校准机制，导致无法提供理论基础的分解。因此，本研究旨在克服这些不足，开发一个新型框架来解决核心的不确定性量化挑战。",
      "method": "本文提出ProbFM，一种基于Transformer的概率框架，利用深度证据回归（DER）进行原则性不确定性量化，并显式分解为认知和偶发不确定性。关键创新在于，与现有方法指定分布形式或需要采样推理不同，ProbFM通过高阶证据学习学习最优不确定性表示，同时保持单次计算效率。为了独立评估DER的量化能力，研究使用一致的LSTM架构对五种概率方法（DER、高斯负对数似然、Student's-t负对数似然、分位数损失和符合预测）进行了控制比较。",
      "result": "在加密货币回报预测的评估中，深度证据回归（DER）在预测精度上与基线方法（如高斯NLL、Student's-t NLL、分位数损失和符合预测）保持竞争性。实验结果表明，DER不仅维持了类似的预测性能，还成功提供了显式的认知-偶发不确定性分解，验证了其在不确定性量化方面的优势。然而，摘要未明确说明具体精度数值，但强调了DER在分解不同不确定性来源上的有效性。",
      "conclusion": "本研究的主要贡献在于提出了ProbFM框架，为基础模型中的原则性不确定性量化提供了一个可扩展的解决方案，并首次将深度证据回归应用于时间序列预测。通过实证证据，工作展示了DER在金融应用中的有效性，如加密货币预测，这有助于推动风险管理、决策支持等实际领域的进步。未来工作可进一步扩展该框架到其他数据类型或应用场景。",
      "tags": [
        "Deep Evidential Regression",
        "Transformer-based Models",
        "Uncertainty Decomposition",
        "Time Series Foundation Models",
        "Probabilistic Forecasting"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:22.270324Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10587",
    "title": "Adversarial Evasion Attacks on Computer Vision using SHAP Values",
    "authors": [
      "Frank Mollard",
      "Marcus Becker",
      "Florian Roehrbein"
    ],
    "abstract": "The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10587.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10587",
    "published": "2026-01-15T16:58:55Z",
    "updated": "2026-01-15T16:58:55Z",
    "comment": "10th bwHPC Symposium - September 25th & 26th, 2024",
    "light_analysis": {
      "overview": "论文提出了一种基于SHAP值的对抗攻击方法，用于计算机视觉模型，能在梯度隐藏场景下生成更鲁棒的误分类。",
      "motivation": "研究动机源于对抗攻击对深度学习模型安全性的威胁，特别是那些不易被人眼察觉的攻击，可能导致算法误判而逃避人类检测。现有方法如快速梯度符号方法（FGSM）在梯度隐藏场景下可能效果有限，因此需要开发更鲁棒的攻击策略来暴露模型弱点，增强对抗鲁棒性评估。摘要强调了攻击的阴险性，凸显了解决此问题的重要性。",
      "method": "该方法是一种白盒攻击，利用SHAP值在推理阶段量化输入特征对模型输出的重要性，以指导对抗样本的生成。核心创新是结合SHAP值优化输入，降低输出置信度或诱导误分类，特别是在梯度隐藏场景中。摘要未明确说明使用的具体数据集或模型架构，但提及了在计算机视觉模型上的应用。",
      "result": "主要实验结果表明，SHAP攻击在生成误分类方面比快速梯度符号方法（FGSM）更鲁棒，尤其是在梯度隐藏场景下。摘要未提供具体性能数据如准确率或效率指标，但强调了SHAP攻击的优越性，为对抗攻击的鲁棒性测试提供了新证据。",
      "conclusion": "论文的主要贡献是提出了一种基于SHAP值的新型对抗攻击方法，展示了其在特定场景下的有效性。研究意义在于增强了对抗鲁棒性的评估，为计算机视觉模型安全提供了新视角。潜在局限性包括摘要未明确讨论实际应用范围，未来工作可能涉及扩展到其他领域或开发相应的防御策略。",
      "tags": [
        "Adversarial Attacks",
        "SHAP Values",
        "White-box Attack",
        "Computer Vision",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:21.389106Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10583",
    "title": "Combinatorial Optimization Augmented Machine Learning",
    "authors": [
      "Maximilian Schiffer",
      "Heiko Hoppe",
      "Yue Su",
      "Louis Bouvier",
      "Axel Parmentier"
    ],
    "abstract": "Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10583.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10583",
    "published": "2026-01-15T16:55:19Z",
    "updated": "2026-01-15T16:55:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文综述了组合优化增强机器学习（COAML）领域的最新进展，通过引入统一框架和问题分类法，为未来研究提供全面路线图。",
      "motivation": "COAML旨在解决在复杂决策任务（如调度、车辆路由）中，机器学习模型可能忽略可行性约束或与优化脱节的问题。整合数据驱动预测和组合优化，以构建更准确且可行的策略，弥补了传统方法在综合性和适应性上的不足，桥接了机器学习、运筹学和随机优化领域。",
      "method": "论文提出一个统一框架，将组合优化oracle嵌入学习流程，定义方法论构建块如策略构建和经验成本最小化。基于不确定性和决策结构，开发问题分类法区分静态和动态问题，并回顾了算法方法，包括模仿学习、强化学习等，应用于调度、车辆路由等领域，但没有具体提及数据集或模型架构细节。",
      "result": "作为综述性论文，摘要未明确说明具体实验结果或性能指标，而是综合分析了COAML在调度、随机规划等应用中的进展，总结了算法在经验成本最小化、模仿学习和强化学习方面的贡献，通过与基线方法对比强调了框架的理论优势。",
      "conclusion": "论文主要贡献是全面概述COAML领域现状，提供统一框架和分类法以促进跨学科研究。学术价值在于整合机器学习与优化理论，实际应用价值在于提升复杂决策系统的效率。局限性可能在于实际应用案例不足，未来方向包括探索动态问题和强化学习等研究前沿。",
      "tags": [
        "Combinatorial Optimization",
        "Machine Learning Integration",
        "Stochastic Programming",
        "Reinforcement Learning",
        "Imitation Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:39.322543Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10581",
    "title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA",
    "authors": [
      "Kimia Abedini",
      "Farzad Shami",
      "Gianmaria Silvello"
    ],
    "abstract": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10581.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10581",
    "published": "2026-01-15T16:54:11Z",
    "updated": "2026-01-15T16:54:11Z",
    "comment": "Accepted paper by the 48th European Conference on Information Retrieval (ECIR'26)",
    "light_analysis": {
      "overview": "本论文提出GenomAgent多智能体框架，通过协调专门智能体显著提升基因组问答的性能和适应性。",
      "motivation": "基因组信息对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍具挑战。大型语言模型（LLMs）在基因组问答（QA）中潜力巨大，但受限于对领域特定数据库的直接访问。现有最先进系统GeneGPT虽通过API调用增强了LLMs，但其僵化的API依赖和有限适应性阻碍了进一步发展。因此，本研究旨在克服这些限制，开发更灵活、高效的基因组QA系统。",
      "method": "作者首先复制了GeneGPT系统，然后在此基础上提出了GenomAgent，一个多智能体框架。该框架通过协调多个专门智能体来处理复杂基因组查询，每个智能体可能负责不同子任务或数据库访问。关键创新在于从单智能体扩展到多智能体，提高了查询处理的效率和协调性，同时增强了系统的灵活性和适应性。尽管摘要未详细说明具体架构，但强调了多智能体协调的优势。",
      "result": "在GeneTuring基准测试的九个任务上进行评估，GenomAgent平均性能比GeneGPT高出12%，表现出显著的改进。与基线方法GeneGPT的对比显示，新框架在准确率或效率方面有提升。此外，GenomAgent的灵活架构使其能够扩展到基因组学之外，适用于其他需要专家知识提取的科学领域，展示了广泛的应用潜力。",
      "conclusion": "本研究的主要贡献是开发了GenomAgent多智能体框架，有效解决了单智能体系统在基因组QA中的局限性。其学术价值在于推动了多智能体推理在生物信息学中的应用，为复杂科学问题的自动化提供了新方法。实际应用价值体现在框架的可扩展性，能够扩展到其他领域，促进知识的提取和整合。未来工作可能涉及进一步优化智能体协调机制或扩展到更多样化的查询类型。",
      "tags": [
        "Multi-Agent System",
        "Large Language Model",
        "Genomics Question Answering",
        "API Integration",
        "Scientific Knowledge Extraction"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:58.272897Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10580",
    "title": "Form and Meaning in Intrinsic Multilingual Evaluations",
    "authors": [
      "Wessel Poelman",
      "Miryam de Lhoneux"
    ],
    "abstract": "Intrinsic evaluation metrics for conditional language models, such as perplexity or bits-per-character, are widely used in both mono- and multilingual settings. These metrics are rather straightforward to use and compare in monolingual setups, but rest on a number of assumptions in multilingual setups. One such assumption is that comparing the perplexity of CLMs on parallel sentences is indicative of their quality since the information content (here understood as the semantic meaning) is the same. However, the metrics are inherently measuring information content in the information-theoretic sense. We make this and other such assumptions explicit and discuss their implications. We perform experiments with six metrics on two multi-parallel corpora both with mono- and multilingual models. Ultimately, we find that current metrics are not universally comparable. We look at the form-meaning debate to provide some explanation for this.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10580.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10580",
    "published": "2026-01-15T16:53:40Z",
    "updated": "2026-01-15T16:53:40Z",
    "comment": "EACL 2026: Main Conference",
    "light_analysis": {
      "overview": "论文质疑内在多语评估指标的假设，揭示其非普遍可比性，并从形式-意义辩论提供解释。",
      "motivation": "研究动机源于内在评估指标如困惑度在多语设置中被广泛使用，但基于信息内容相同的假设。这一问题重要，因为多语模型评估需公平比较，现有方法忽略信息理论与语义意义的差异，可能导致评估偏差。现有不足在于假设未充分验证，影响评估准确性和可靠性，挑战多语研究的有效性。",
      "method": "研究方法包括明确内在评估指标在多语设置中的假设，并进行实验验证。核心创新点在于系统化分析假设，结合信息理论和语义讨论其影响。实验细节涉及使用六个指标（如困惑度）在两个多平行语料库上，测试单语和多语模型，以检验指标的可比性和假设的有效性。",
      "result": "主要实验结果表明，当前内在评估指标在多语设置中不是普遍可比的。摘要未明确说明具体性能指标如准确率提升或效率改进，但通过对比不同指标和模型，验证了假设的局限性。与基线方法对比方面，摘要未提及具体基线，但实验揭示了指标在不同设置下的不一致性和不可比性。",
      "conclusion": "结论总结论文主要贡献是明确内在多语评估指标的假设并揭示其局限性。学术价值在于挑战现有评估实践，促进更严谨的多语研究；实际应用价值是提醒研究者谨慎使用指标以避免误导。潜在局限性包括未提出新指标，未来工作方向可能涉及开发更合适的评估方法或进一步探索形式与意义的关系。",
      "tags": [
        "Conditional Language Models",
        "Perplexity",
        "Multilingual Evaluation",
        "Information Theory",
        "Form-Meaning Debate"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:54.368064Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10577",
    "title": "Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation",
    "authors": [
      "Serena Grazia De Benedictis",
      "Amedeo Altavilla",
      "Nicoletta Del Buono"
    ],
    "abstract": "Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.   In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \\emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $β_0 = β_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.   This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.",
    "categories": [
      "cs.CV",
      "math.AT",
      "math.NA"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10577.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10577",
    "published": "2026-01-15T16:47:53Z",
    "updated": "2026-01-15T16:47:53Z",
    "comment": "27 pages, 18 figures",
    "light_analysis": {
      "overview": "本论文提出了一种基于Jordan曲线定理的数字拓扑感知分割概念，用于评估二进制图像分割的结构连贯性。",
      "motivation": "图像分割在计算机视觉中至关重要，但传统评估指标如像素级、区域基础或边界关注方法，常难以捕捉分割的结构和拓扑连贯性。在医疗成像或对象描绘等应用中，小边界误差、空洞或碎片化预测可能导致高评分，但实际掩码可能破坏对象的全局形状或连通性。这暴露了传统指标的局限：它们无法判断分割是否将图像划分为有意义的内部和外部区域，从而影响分割质量的有效评估，尤其在需要精确拓扑保持的场景中显得不足。",
      "method": "论文引入了基于Jordan曲线定理的数字拓扑感知分割定义，核心创新是提出“Jordan-segmentatable mask”概念，即二进制分割确保图像域分成两个连接组件。方法结合数字拓扑和同调理论，通过提取掩码的4-曲线候选，使用Betti数（β0和β1）验证其拓扑有效性。当候选形成数字4-曲线且β0=β1=1时，掩码被视为Jordan-segmentatable，等价于其补集分成两个8-连接组件。该方法使用数字Jordan理论和同调不变量，提供数学严谨的无监督评估准则。",
      "result": "摘要未明确说明具体的实验结果和性能对比数据。然而，论文通过理论框架展示了其方法的数学严谨性，强调了该框架作为标准评估指标的替代价值。它聚焦于评估分割掩码的结构连贯性，而非提供定量数据，暗示了在拓扑正确性关键的应用中（如医学图像分析）的潜在有效性，但需要进一步实验验证以补充具体性能指标。",
      "conclusion": "本论文的主要贡献是提出一种基于Jordan曲线定理的数字拓扑感知分割框架，弥补了传统评估指标在结构连贯性方面的不足。该方法结合数字拓扑和同调理论，提供了一个数学严谨的无监督准则，具有学术价值，为分割评估领域引入了新视角。在实际应用中，尤其对于需要拓扑正确性的场景如医学成像，该方法有重要价值。未来工作可能包括将该框架扩展到更复杂场景，或与其他评估方法结合以提高实用性。",
      "tags": [
        "Image Segmentation",
        "Digital Topology",
        "Homology Theory",
        "Jordan Curve Theorem",
        "Betti Numbers"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:55.199892Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10567",
    "title": "Generative AI collective behavior needs an interactionist paradigm",
    "authors": [
      "Laura Ferrarotti",
      "Gian Maria Campedelli",
      "Roberto Dessì",
      "Andrea Baronchelli",
      "Giovanni Iacca",
      "Kathleen M. Carley",
      "Alex Pentland",
      "Joel Z. Leibo",
      "James Evans",
      "Bruno Lepri"
    ],
    "abstract": "In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10567.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10567",
    "published": "2026-01-15T16:29:23Z",
    "updated": "2026-01-15T16:29:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文主张采用互动主义范式来研究和理解基于大型语言模型的多智能体系统的集体行为，以应对其社会影响。",
      "motivation": "研究动机在于理解基于大型语言模型（LLMs）的智能体集体行为，因为这对社会有广泛影响，涉及风险和利益。LLMs的独特性质，如通过大量预训练知识和隐含社会先验初始化，以及上下文学习能力，使得现有方法可能不足以系统分析先验知识和嵌入价值观如何与社会上下文交互，从而影响多智能体系统中的新兴现象。这种不足凸显了开发新范式的必要性，以更好地评估和引导生成AI的集体行为。",
      "method": "论文提出互动主义范式，包括替代的理论基础、方法论和分析工具，旨在系统研究先验知识和嵌入价值观如何与社会上下文交互以塑造集体行为。关键创新在于强调交互过程，而非孤立分析个体智能体，以捕捉LLMs中预训练知识和社会先验的动态影响。摘要未明确说明具体使用的数据集或模型架构，但建议通过新方法来探究多智能体系统的复杂动态。",
      "result": "摘要未明确说明主要实验结果或具体性能指标，论文侧重于提出理论框架和讨论未来方向。因此，没有提供与基线方法的对比或数据支撑，但强调了互动主义范式的必要性，以指导未来研究和实证分析，旨在更系统地理解LLM-based集体行为的风险和潜在应用。",
      "conclusion": "论文的主要贡献是提出并讨论了互动主义范式，为LLM-based集体行为研究提供了新视角，具有重要学术价值，促进了理论创新和跨学科对话。实际应用价值在于帮助更安全、有效地部署生成AI系统，减少社会风险。未来工作可进一步探索具体实证方法、工具开发和跨领域合作，以完善该范式并应对复杂挑战。",
      "tags": [
        "Large Language Models",
        "Multi-agent Systems",
        "Collective Behavior",
        "Interactionist Paradigm",
        "Contextual Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:50.604564Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10566",
    "title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure",
    "authors": [
      "Syed Naveed Mahmood",
      "Md. Rezaur Rahman Bhuiyan",
      "Tasfia Zaman",
      "Jareen Tasneem Khondaker",
      "Md. Sameer Sakib",
      "Nazia Tasnim",
      "Farig Sadeque"
    ],
    "abstract": "Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10566.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10566",
    "published": "2026-01-15T16:28:14Z",
    "updated": "2026-01-15T16:28:14Z",
    "comment": "16 pages, 4 figures",
    "light_analysis": {
      "overview": "本文提出知识免疫框架(KIF)，通过针对内部激活签名实现大规模语言模型中的真正知识擦除，避免了表面抑制的混淆。",
      "motivation": "本文的研究动机源于大规模语言模型中选择性知识擦除的需求，特别是为满足GDPR等法规合规和提升模型安全。现有方法通常仅抑制输出行为，而未能真正移除内在知识，导致潜在能力在表面拒绝下持续存在，这引发合规风险和安全漏洞。因此，需要一种能够区分表面混淆与真正擦除的方法，以解决知识持久性问题并实现更可靠的知识管理。",
      "method": "论文提出知识免疫框架(KIF)，这是一个基于表示感知的架构，通过目标内部激活签名而非表面输出来实现知识擦除。核心方法结合动态抑制特定主题的表示和使用参数高效的适应技术，避免了整个模型的重新训练。关键创新点在于从模型内部机制层面操作，利用激活签名来识别和移除知识，确保更彻底的知识消除，同时保持模型的整体实用性和效率。",
      "result": "KIF实现了优异的擦除性能，FQ接近0.99（理想值为1.00），同时实用性指数MU为0.62，打破了先前工作中稳定性与擦除之间的权衡。在多种模型上评估，包括标准基础模型（Llama、Mistral）和推理优先模型（Qwen、DeepSeek），参数范围从3B到14B。结果显示，标准模型表现规模独立的真正擦除能力，实用性漂移低于3%，而推理优先模型揭示架构上的根本差异。全面双指标评估协议结合表面泄漏和潜在追踪持久性，操作化了混淆与擦除的区别。",
      "conclusion": "本论文的主要贡献是提出了知识免疫框架(KIF)，通过内部激活签名实现真正知识擦除，有效区分了表面抑制与知识移除。学术上，它提供了一种新方法来操作化擦除与混淆的差别，并开发了双指标评估协议，为系统诊断模型遗忘行为提供了工具。实际应用上，KIF有助于LLMs在GDPR合规和安全场景下的部署，尽管未明确说明局限性，但未来工作可能包括扩展至更广泛模型架构和应用领域。",
      "tags": [
        "Large Language Model",
        "Knowledge Unlearning",
        "Activation Signatures",
        "Representation-Aware",
        "Parameter-Efficient Adaptation"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:38.146138Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10563",
    "title": "Kolmogorov Arnold Networks and Multi-Layer Perceptrons: A Paradigm Shift in Neural Modelling",
    "authors": [
      "Aradhya Gaonkar",
      "Nihal Jain",
      "Vignesh Chougule",
      "Nikhil Deshpande",
      "Sneha Varur",
      "Channabasappa Muttal"
    ],
    "abstract": "The research undertakes a comprehensive comparative analysis of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptrons (MLP), highlighting their effectiveness in solving essential computational challenges like nonlinear function approximation, time-series prediction, and multivariate classification. Rooted in Kolmogorov's representation theorem, KANs utilize adaptive spline-based activation functions and grid-based structures, providing a transformative approach compared to traditional neural network frameworks. Utilizing a variety of datasets spanning mathematical function estimation (quadratic and cubic) to practical uses like predicting daily temperatures and categorizing wines, the proposed research thoroughly assesses model performance via accuracy measures like Mean Squared Error (MSE) and computational expense assessed through Floating Point Operations (FLOPs). The results indicate that KANs reliably exceed MLPs in every benchmark, attaining higher predictive accuracy with significantly reduced computational costs. Such an outcome highlights their ability to maintain a balance between computational efficiency and accuracy, rendering them especially beneficial in resource-limited and real-time operational environments. By elucidating the architectural and functional distinctions between KANs and MLPs, the paper provides a systematic framework for selecting the most suitable neural architectures for specific tasks. Furthermore, the proposed study highlights the transformative capabilities of KANs in progressing intelligent systems, influencing their use in situations that require both interpretability and computational efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10563.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10563",
    "published": "2026-01-15T16:26:49Z",
    "updated": "2026-01-15T16:26:49Z",
    "comment": "13 pages, 8 figures, 2 tables",
    "light_analysis": {
      "overview": "论文通过比较Kolmogorov-Arnold Networks和Multi-Layer Perceptrons，展示了KANs在精度和计算效率上的优势，并提出了一种新的神经网络建模范式。",
      "motivation": "该研究旨在解决非线性函数近似、时间序列预测和多变量分类等核心计算问题。现有方法如Multi-Layer Perceptrons在复杂任务中可能面临计算开销大和精度平衡不佳的挑战，特别是在资源受限的实时应用场景中。因此，探索基于Kolmogorov表示定理的新网络架构KANs，旨在提升效率与准确性的平衡，以改进传统神经网络的不足。",
      "method": "论文提出基于Kolmogorov-Arnold Networks (KAN) 的方法，利用自适应样条激活函数和网格结构，与传统的Multi-Layer Perceptrons (MLP) 进行比较。研究使用了多样化数据集，包括数学函数估计（如二次和三次函数）以及实际应用数据集（如每日温度预测和葡萄酒分类）。评估指标包括均方误差(MSE)和浮点运算(FLOPs)，以全面衡量模型性能。",
      "result": "实验结果表明，KANs在所有基准测试中均优于MLPs，实现了更高的预测精度和显著降低的计算成本。具体地，KANs在MSE指标上表现更优，同时FLOPs大幅减少，显示出在计算效率与准确性之间的优越平衡。这一优势使其特别适用于资源受限和实时操作环境。",
      "conclusion": "论文总结了KANs在神经网络建模中的创新贡献，通过比较研究提供了选择合适架构的系统框架。KANs的变革能力推动了智能系统的发展，尤其在需要可解释性和计算效率的场景中具有重要应用价值。未来工作可能包括进一步优化KANs结构或探索更多实际应用领域。",
      "tags": [
        "Kolmogorov-Arnold Networks",
        "Multi-Layer Perceptrons",
        "Spline-based Activation Functions",
        "Function Approximation",
        "Computational Efficiency"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:13.005160Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10562",
    "title": "Process-Guided Concept Bottleneck Model",
    "authors": [
      "Reza M. Asiyabi",
      "SEOSAW Partnership",
      "Steven Hancock",
      "Casey Ryan"
    ],
    "abstract": "Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10562.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10562",
    "published": "2026-01-15T16:25:55Z",
    "updated": "2026-01-15T16:25:55Z",
    "comment": "13 pages with 7 figures and 1 table, Supplementary Materials 10 pages with 3 figures",
    "light_analysis": {
      "overview": "论文提出过程引导概念瓶颈模型，通过引入领域因果机制提升概念瓶颈模型的准确性和可解释性。",
      "motivation": "标准概念瓶颈模型虽然提高了深度学习的可解释性，但常忽视领域特定关系和因果机制，且依赖完整概念标签，限制了在监督稀疏但过程定义清晰的科学领域的应用。研究动机在于解决这一问题，通过结合领域知识来增强模型的性能和透明性，以应对科学应用中监督数据有限但过程知识丰富的挑战。",
      "method": "研究扩展概念瓶颈模型，提出过程引导版本，通过生物物理上有意义的中间概念约束学习遵循领域定义的因果机制。使用地球观测数据进行地上生物量密度估计作为案例研究，利用多源异质训练数据生成可解释的中间输出，以提高模型的准确性和透明度，关键创新点在于过程引导的学习框架。",
      "result": "在地球观测数据的案例中，PG-CBM与多个基准模型相比，减少了误差和偏差，提高了地上生物量密度估计的准确性。模型还展现出更好的透明度，能够检测虚假学习并提供科学见解，尽管具体性能指标摘要未明确说明，但突出了其在减少误差和偏差方面的优越性。",
      "conclusion": "PG-CBM的主要贡献在于通过过程引导学习改进了概念瓶颈模型，提升准确性和可解释性，增强透明度并支持科学洞察，代表科学应用中向更可信AI系统迈进一步。摘要未明确说明潜在局限性或未来工作方向，但强调了其在多源数据利用和科学应用中的价值。",
      "tags": [
        "Concept Bottleneck Model",
        "Causal Mechanisms",
        "Interpretable Machine Learning",
        "Earth Observation",
        "Multi-source Data"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:19.815247Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10554",
    "title": "DeepUrban: Interaction-Aware Trajectory Prediction and Planning for Automated Driving by Aerial Imagery",
    "authors": [
      "Constantin Selzer",
      "Fabian B. Flohr"
    ],
    "abstract": "The efficacy of autonomous driving systems hinges critically on robust prediction and planning capabilities. However, current benchmarks are impeded by a notable scarcity of scenarios featuring dense traffic, which is essential for understanding and modeling complex interactions among road users. To address this gap, we collaborated with our industrial partner, DeepScenario, to develop DeepUrban-a new drone dataset designed to enhance trajectory prediction and planning benchmarks focusing on dense urban settings. DeepUrban provides a rich collection of 3D traffic objects, extracted from high-resolution images captured over urban intersections at approximately 100 meters altitude. The dataset is further enriched with comprehensive map and scene information to support advanced modeling and simulation tasks. We evaluate state-of-the-art (SOTA) prediction and planning methods, and conducted experiments on generalization capabilities. Our findings demonstrate that adding DeepUrban to nuScenes can boost the accuracy of vehicle predictions and planning, achieving improvements up to 44.1 % / 44.3% on the ADE / FDE metrics. Website: https://iv.ee.hm.edu/deepurban",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10554.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10554",
    "published": "2026-01-15T16:18:42Z",
    "updated": "2026-01-15T16:18:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了基于无人机图像的DeepUrban数据集，以增强密集城市环境下自动驾驶系统的轨迹预测和规划性能。",
      "motivation": "自动驾驶系统的有效性依赖于在复杂交通场景中的准确预测和规划，但当前基准测试如nuScenes缺乏足够的密集交通数据，导致模型难以有效学习和评估道路用户之间的复杂交互行为。密集城市环境中交互多变，现有数据集覆盖不足，限制了算法的发展。因此，开发一个专注于密集城市设置的数据集至关重要，以填补这一空白并提升系统鲁棒性。",
      "method": "研究通过与工业伙伴DeepScenario合作，开发了DeepUrban数据集。该数据集利用无人机在约100米高度捕获的高分辨率图像，提取3D交通对象，并整合了全面的地图和场景信息。关键创新点在于专注于密集城市交叉口，提供丰富的交互场景数据，支持轨迹预测和规划的基准测试。数据集的构建包括图像采集、对象提取和信息增强，以促进高级建模和仿真任务。",
      "result": "实验评估了最先进的预测和规划方法，并测试了泛化能力。结果显示，将DeepUrban数据集添加到nuScenes中，显著提升了车辆预测和规划的准确性。具体而言，在ADE和FDE指标上，改进分别高达44.1%和44.3%，与基线方法相比，数据集的引入增强了模型对密集交通场景的适应能力，证明了其在性能提升方面的有效性。",
      "conclusion": "论文的主要贡献是填补了密集城市交通数据集的空白，DeepUrban数据集能有效推动自动驾驶预测和规划研究，通过提供高分辨率的无人机图像和丰富场景信息，支持更准确的模型训练和评估。实际应用中，有助于开发更安全的自动驾驶系统。未来工作可扩展数据集规模或探索更多交互建模方法，但摘要未明确说明具体局限性。",
      "tags": [
        "Trajectory Prediction",
        "Automated Driving Planning",
        "Drone Dataset",
        "3D Traffic Object Detection",
        "Urban Traffic Benchmark"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:39.562078Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10553",
    "title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models",
    "authors": [
      "Jianhao Yuan",
      "Xiaofeng Zhang",
      "Felix Friedrich",
      "Nicolas Beltran-Velez",
      "Melissa Hall",
      "Reyhane Askari-Hemmat",
      "Xiaochuang Han",
      "Nicolas Ballas",
      "Michal Drozdzal",
      "Adriana Romero-Soriano"
    ],
    "abstract": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10553.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10553",
    "published": "2026-01-15T16:18:00Z",
    "updated": "2026-01-15T16:18:00Z",
    "comment": "22 pages, 10 figures",
    "light_analysis": {
      "overview": "论文提出了WMReward方法，利用潜在世界模型在推理时对齐视频生成模型的物理合理性。",
      "motivation": "现有先进的视频生成模型虽然能产生高质量的视觉内容，但经常违反基本物理原则，如物体运动或交互不自然，这限制了其实用性。研究表明，这一不足不仅源于预训练阶段对物理理解的缺失，还与推理策略的次优性有关。当前方法在生成过程中未能有效整合物理约束，导致输出缺乏真实感。因此，研究旨在通过优化推理策略，将提升物理合理性视为关键问题，以增强视频生成的实用性和可靠性。",
      "method": "论文提出WMReward方法，将物理合理性的提升视为推理时对齐问题。核心是利用潜在世界模型VJEPA-2的强物理先验作为奖励，在推理过程中搜索和引导多个候选去噪轨迹。通过增加测试时计算，优化生成路径，以提高物理合理性。关键创新点包括使用潜在世界模型作为奖励机制，以及对去噪过程进行多候选搜索，从而实现缩放测试性能。摘要未明确说明具体数据集和模型架构细节，但强调了在图像条件、多帧条件和文本条件等多种设置下的应用。",
      "result": "实验结果显示，该方法在图像条件、多帧条件和文本条件等视频生成设置下，物理合理性得到显著提升，并通过人类偏好研究验证了有效性。在ICCV 2025 Perception Test PhysicsIQ挑战赛中，最终得分达到62.64%，获得第一名，相比之前的最佳方法提升了7.42%。这表明WMReward方法在性能上超越现有技术，展示了其在改善视频生成物理真实性方面的优越表现。",
      "conclusion": "该研究的主要贡献是证明了使用潜在世界模型提升视频生成物理合理性的可行性，并提出了一种新颖的推理时对齐方法，即WMReward。这不仅在特定模型实例中有效，还具有通用性，为视频生成领域提供了新的技术方向。学术价值在于为生成模型结合物理约束开辟了新途径，实际应用价值包括增强虚拟现实、游戏等领域的真实性。摘要未明确说明局限性，但未来工作可能包括扩展到更多模型类型和应用场景，以进一步验证和优化方法。",
      "tags": [
        "Video Generative Models",
        "Latent World Models",
        "Inference-time Alignment",
        "Reward-based Learning",
        "Physics Plausibility"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:00.342872Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10551",
    "title": "Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure",
    "authors": [
      "Luxuan Fu",
      "Chong Liu",
      "Bisheng Yang",
      "Zhen Dong"
    ],
    "abstract": "Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10551.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10551",
    "published": "2026-01-15T16:16:34Z",
    "updated": "2026-01-15T16:16:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个域适应框架，通过整合数据高效微调和知识基础推理，将大型视觉语言模型转化为智能基础设施分析的专用代理，显著提升感知准确性和可靠性。",
      "motivation": "智能城市管理中，自动化感知路边基础设施至关重要，但现有通用模型难以准确捕获细粒度属性和工程标准。大型视觉语言模型在开放世界识别中表现优异，但在复杂设施状态解释时，常因不符合专业规则而产生幻觉，导致实际应用不可靠，突出了改进的必要性。",
      "method": "本方法采用一个框架，结合数据高效微调策略和知识基础推理机制。具体包括：利用Grounding DINO进行开放词汇微调，以少量监督鲁棒地定位多样资产；基于LoRA在Qwen-VL上进行深度语义属性推理的适应；并引入双模态检索增强生成模块，动态检索权威行业标准和视觉示例，以减少幻觉并确保专业合规性。",
      "result": "在全面的城市路边场景新数据集上评估，框架实现58.9 mAP的检测性能和95.5%的属性识别准确率。这些指标优于基线方法，展示了在智能基础设施监控中，框架能有效提升感知精度和可靠性。",
      "conclusion": "本研究的主要贡献是开发了一个域适应框架，解决大型视觉语言模型在专业领域感知中的局限性。其学术价值在于结合微调和检索增强生成技术，减少幻觉并增强合规性；实际应用价值为智能城市基础设施监控提供稳健工具。摘要未明确说明局限性，未来工作可扩展至更多场景或优化推理效率。",
      "tags": [
        "Large Vision Language Models",
        "Fine-tuning",
        "Retrieval-Augmented Generation",
        "Grounding DINO",
        "LoRA"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:35.276883Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10543",
    "title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing",
    "authors": [
      "Yinzhi Zhao",
      "Ming Wang",
      "Shi Feng",
      "Xiaocui Yang",
      "Daling Wang",
      "Yifei Zhang"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10543.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10543",
    "published": "2026-01-15T16:09:10Z",
    "updated": "2026-01-15T16:09:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种通过在解码过程中激活大型语言模型内部潜在安全信号的新方法，有效防御越狱攻击。",
      "motivation": "大型语言模型（LLMs）在自然语言任务中表现出色，但经过安全对齐后仍易受越狱攻击威胁。现有防御机制如解码约束和后验内容检测器存在不足：它们要么在复杂攻击下检测不鲁棒，导致漏检；要么过度干预模型输出，降低实用性和响应质量。这凸显了在保持模型效用同时增强安全性的迫切需求，以避免实际部署中的安全风险。因此，研究旨在开发一种能平衡安全与效用的新防御策略。",
      "method": "论文基于对LLMs解码过程的观察，发现即使模型被成功越狱，在生成过程中内部仍存在潜在安全信号，但被流畅性驱动力压制而无法及时纠正。作者提出一种简单有效的方法，在解码期间显式提取和利用这些信号，用于早期检测不安全内容。关键创新在于激活模型的内在安全感知机制，无需依赖外部检测器或修改训练过程，摘要未明确说明具体数据集或模型架构，但强调在解码阶段实现动态探测和自我校正。",
      "result": "通过多种越狱攻击实验，该方法显著提升了模型的安全性，有效识别和阻止不安全内容生成。同时，在良性输入上保持了较低的过度拒绝率，确保响应质量不降级。与基线防御机制（如解码约束和后验检测器）相比，该方法在安全性和实用性之间实现了更好平衡，避免了过度干预或性能损失，证实了其在现实场景中的可行性。",
      "conclusion": "本研究的主要贡献是提出了一种通过解码过程中激活内部安全信号来防御越狱攻击的新方法，增强了LLMs的安全性而不损害其效用。学术上，为安全感知解码提供了互补方向；应用上，有助于开发更可靠、高效的AI系统。潜在局限性包括摘要未明确说明方法的泛化性，未来工作可探索在更多攻击类型和模型架构中的适应性，或集成其他防御技术以进一步提升稳健性。",
      "tags": [
        "Large Language Models",
        "Jailbreak Attacks",
        "Safety-Awareness",
        "Decoding Process",
        "Self-Correction"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:43.894172Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10541",
    "title": "Mixtures of Transparent Local Models",
    "authors": [
      "Niffa Cheick Oumar Diaby",
      "Thierry Duchesne",
      "Mario Marchand"
    ],
    "abstract": "The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10541.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10541",
    "published": "2026-01-15T16:05:20Z",
    "updated": "2026-01-15T16:05:20Z",
    "comment": "44 pages, 32 figues",
    "light_analysis": {
      "overview": "本文提出了一种混合透明局部模型的算法，通过结合透明标签函数和局部区域学习，增强机器学习模型的可解释性，以处理输入空间中标签函数的局部变化。",
      "motivation": "随着机器学习模型在各个领域的广泛应用，对其透明度的需求日益增长，因为透明度有助于评估模型的安全性和非歧视性。现有方法往往难以在输入空间的局部区域中提供简单透明的模型，导致可解释性不足或性能下降。因此，本研究旨在设计一种可解释模型，既能保持透明度，又能适应标签函数在局部区域内的变化，解决实际应用中模型解释性差的问题。",
      "method": "本研究提出了一种基于混合透明局部模型的方法，其核心是学习透明标签函数以及输入空间的局部区域，使标签函数在分配的局部中实现最小风险。关键创新在于引入一个新的多预测器和多局部损失函数，用于同时优化标签函数和区域划分。针对二元线性分类和线性回归问题，建立了严格的PAC-Bayesian风险界限，以提供理论保证。使用合成数据集来说明算法如何有效处理局部变化，展示了方法的鲁棒性和适应性。",
      "result": "实验结果表明，该混合透明局部模型方法在真实数据集上表现良好，与其他现有可解释方法及某些不透明模型相比，显示出竞争力。虽然摘要未提供具体性能指标，但通过合成数据的验证，算法能够有效应对标签函数在不同局部区域的突变。这些结果强调了该方法在保持模型透明度的同时，仍能实现与不透明模型相当的预测性能，为可解释机器学习提供了实用性方案。",
      "conclusion": "本研究的主要贡献是提出了一种混合透明局部模型的算法，为设计可解释机器学习模型提供了新思路。通过结合局部透明函数和PAC-Bayesian理论，方法在保障透明度的基础上，适应了输入空间的动态变化，具有重要的学术和实际价值。学术上，它为透明模型的风险分析提供了理论框架；实际上，适用于安全关键领域如医疗或金融决策。未来工作可扩展到更复杂的模型类型或应用场景，以进一步提升模型的泛化能力。",
      "tags": [
        "PAC-Bayes",
        "Risk Bounds",
        "Local Models",
        "Transparent Models",
        "Mixture Models"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:54.287459Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10537",
    "title": "Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning",
    "authors": [
      "Oscar H. Ramírez-Agudelo",
      "Akshay N. Shewatkar",
      "Edoardo Milana",
      "Roland C. Aydin",
      "Kai Franke"
    ],
    "abstract": "Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\\% train, 10\\% validation, and 10\\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10537.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10537",
    "published": "2026-01-15T15:59:12Z",
    "updated": "2026-01-15T15:59:12Z",
    "comment": "17 pages, 10 figures, 6 tables, SPIE Applications of Machine Learning 2023, San Diego, US",
    "light_analysis": {
      "overview": "本论文通过深度学习模型FFA-Net和AECR-Net，显著提升烟雾和雾霾环境中仪表图像的可见度，以支持自动化读取和应急响应。",
      "motivation": "在雾霾和烟雾环境中捕获的图像可见度降低，这给基础设施监控和紧急服务带来挑战，特别是在应急情况下影响仪表数据的准确解读。现有方法可能缺乏专门针对仪表图像的增强技术，且由于基准数据集不可用，限制了机器自动化读取的进展。这一问题至关重要，因为准确的仪表数据能辅助应急响应人员快速决策，提升安全性和效率，因此研究旨在利用深度学习克服这些不足，改善图像质量以应对实际需求。",
      "method": "研究采用FFA-Net和AECR-Net两种深度学习架构来增强仪表图像的可见度，核心创新在于应用这些去雾模型处理仪表图像，并生成合成数据集以解决数据稀缺问题。具体地，使用Unreal Engine生成超过14,000张合成图像，涵盖从轻微到密集的雾霾和烟雾场景，数据集按80%训练、10%验证和10%测试分割进行模型训练和评估。该方法结合了先进模型和定制数据集，以优化图像增强效果，突出了技术路线的实用性和创新性。",
      "result": "在合成雾霾数据集上，模型表现出色，结构相似性指数（SSIM）约为0.98，峰值信噪比（PSNR）约为43dB，与现有最先进结果相当。AECR-Net相比FFA-Net展现出更稳健的性能，但在合成烟雾数据集上结果较差，SSIM和PSNR指标较低，主要是因为烟雾的不均匀性和高密度使得增强更困难。尽管烟雾图像增强有限，训练模型仍取得有趣成果，表明去雾模型在特定场景下有效，但需进一步优化以适应烟雾环境。",
      "conclusion": "本研究证实深度学习架构能显著提升烟雾和雾霾场景中仪表图像的质量，支持自动化读取，主要贡献在于应用FFA-Net和AECR-Net进行图像增强，并生成合成数据集填补数据空白。学术价值在于验证了这些模型在特定应用中的有效性，实际应用价值体现在提升应急响应中的仪表数据解读能力。局限性包括烟雾图像增强的挑战和模型原本为去雾设计，未来工作可改进模型以适应烟雾场景或开发专门方法，进一步拓展应用范围。",
      "tags": [
        "Deep Learning",
        "Image Dehazing",
        "Synthetic Dataset",
        "FFA-Net",
        "AECR-Net"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:59.154834Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10535",
    "title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery",
    "authors": [
      "Chong Liu",
      "Luxuan Fu",
      "Yang Jia",
      "Zhen Dong",
      "Bisheng Yang"
    ],
    "abstract": "The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10535.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10535",
    "published": "2026-01-15T15:57:18Z",
    "updated": "2026-01-15T15:57:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "SVII-3D提出统一框架，融合LoRA微调检测、空间注意力匹配与几何精炼，实现稀疏图像中的分米级3D定位和细粒度状态理解。",
      "motivation": "在智慧城市建设和设施生命周期管理中，自动化创建数字孪生和精确资产清单是关键需求。然而，现有利用成本效益高稀疏图像的方法面临诸多挑战：鲁棒性有限导致不同视图间的观测难以稳定关联，定位不准确难以满足分米级精度要求，且缺乏细粒度状态理解限制了自动化维护的智能化水平。这些问题使得现有技术在平衡成本与精度方面不足，阻碍了大规模应用和高效资产管理。",
      "method": "SVII-3D框架采用多层次技术整合：首先，结合LoRA微调的开集检测与空间注意力匹配网络，增强稀疏视图间的稳健关联，应对观测不确定性；其次，引入几何引导的精炼机制，纠正结构误差，实现精确到分米级的3D定位；最后，超越静态几何映射，集成视觉-语言模型代理，通过多模态提示自动诊断资产的细粒度操作状态，提升理解深度。摘要未明确说明使用的具体数据集和模型架构细节。",
      "result": "实验表明，SVII-3D显著提高了识别精度并最小化了定位误差，有效改善了稀疏图像处理的效果。由于摘要未提供具体性能指标（如准确率提升百分比或误差减少数值），无法量化详细比较，但整体结果证实了框架在克服鲁棒性差、定位不准等挑战上的优势，相对于基线方法展现了明显改进，支持其在真实场景中的应用潜力。",
      "conclusion": "论文的主要贡献在于提出SVII-3D统一框架，创新性地融合检测、匹配和语言模型技术，解决了稀疏图像中资产数字化的定位和理解问题。其学术价值体现在多模态处理和精确定位方法的进步，实际意义在于提供了可扩展、成本效益高的解决方案，促进智能城市维护自动化。摘要未明确说明研究的局限性或未来工作方向，但暗示了进一步优化和桥接稀疏感知与智能维护的扩展可能。",
      "tags": [
        "LoRA fine-tuning",
        "Spatial Attention",
        "3D Localization",
        "Vision-Language Model",
        "Multi-modal Prompting"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:30.539925Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10532",
    "title": "PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models",
    "authors": [
      "Chengbing Wang",
      "Wuqiang Zheng",
      "Yang Zhang",
      "Fengbin Zhu",
      "Junyi Cheng",
      "Yi Xie",
      "Wenjie Wang",
      "Fuli Feng"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed in human-centric applications, yet they often fail to provide substantive emotional support. While Reinforcement Learning (RL) has been utilized to enhance empathy of LLMs, existing reward models typically evaluate empathy from a single perspective, overlooking the inherently bidirectional interaction nature of empathy between the supporter and seeker as defined by Empathy Cycle theory. To address this limitation, we propose Psychology-grounded Empathetic Reward Modeling (PERM). PERM operationalizes empathy evaluation through a bidirectional decomposition: 1) Supporter perspective, assessing internal resonation and communicative expression; 2) Seeker perspective, evaluating emotional reception. Additionally, it incorporates a bystander perspective to monitor overall interaction quality. Extensive experiments on a widely-used emotional intelligence benchmark and an industrial daily conversation dataset demonstrate that PERM outperforms state-of-the-art baselines by over 10\\%. Furthermore, a blinded user study reveals a 70\\% preference for our approach, highlighting its efficacy in generating more empathetic responses. Our code, dataset, and models are available at https://github.com/ZhengWwwq/PERM.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10532.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10532",
    "published": "2026-01-15T15:56:55Z",
    "updated": "2026-01-15T15:56:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "PERM提出基于心理学基础的双向共情奖励建模方法，显著提升大型语言模型的情感支持能力。",
      "motivation": "LLMs在人类中心应用中部署增加，但常无法提供实质性情感支持。现有基于强化学习的共情增强方法中，奖励模型通常仅从支持者视角评估共情，忽略了Empathy Cycle理论中强调的双向互动性质，即共情涉及支持者与寻求者之间的相互影响，导致模型互动能力不足。因此，需要更全面的共情评估框架来改进LLMs在情感支持场景中的表现。",
      "method": "PERM基于Empathy Cycle理论，将共情评估分解为三个视角：支持者视角评估内部情感共鸣与外部表达，寻求者视角评估情感接收质量，以及旁观者视角监控整体互动质量。该方法通过构建奖励模型整合这些视角，优化LLMs的响应生成。实验使用情感智能基准和工业日常对话数据集验证模型性能，具体模型架构和数据集细节摘要未明确说明。",
      "result": "在情感智能基准和工业日常对话数据集上的实验表明，PERM在共情评估指标上比最优基线提升了超过10%。盲目用户研究中，参与者显示出70%的偏好选择PERM生成的响应，突显其在生成更具共情回应的效果。这些结果验证了双向共情评估方法在改进LLMs情感支持能力方面的优越性。",
      "conclusion": "PERM通过整合心理学理论，提出了一种双向共情奖励建模方法，显著增强了LLMs的情感支持能力。研究构建了全面的共情评估框架，具有学术价值和实际应用前景，可改进对话系统的用户体验。未来工作可能包括扩展到其他情感维度或优化模型效率，摘要未明确说明具体方向。",
      "tags": [
        "Large Language Models",
        "Reinforcement Learning",
        "Empathy Cycle Theory",
        "Reward Modeling",
        "User Study"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:18.609652Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10527",
    "title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5",
    "authors": [
      "Xingjun Ma",
      "Yixu Wang",
      "Hengyuan Xu",
      "Yutao Wu",
      "Yifan Ding",
      "Yunhan Zhao",
      "Zilong Wang",
      "Jiabin Hua",
      "Ming Wen",
      "Jianan Liu",
      "Ranjie Duan",
      "Yifeng Gao",
      "Yingshui Tan",
      "Yunhao Chen",
      "Hui Xue",
      "Xin Wang",
      "Wei Cheng",
      "Jingjing Chen",
      "Zuxuan Wu",
      "Bo Li",
      "Yu-Gang Jiang"
    ],
    "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10527.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10527",
    "published": "2026-01-15T15:52:52Z",
    "updated": "2026-01-15T15:52:52Z",
    "comment": "42 pages, 24 figures",
    "light_analysis": {
      "overview": "本文对GPT-5.2等7个前沿大模型进行集成安全性评估，揭示其安全性的异质性和多维性，强调了标准化评估的重要性。",
      "motivation": "随着大语言模型和多模态大模型的快速发展，其推理、感知和生成能力显著提升，但安全性的提升是否相应增强尚不明确。现有评估方法碎片化，局限于单一模态或威胁模型，难以全面衡量模型在现实世界中的风险，这阻碍了负责任模型开发和部署。因此，本研究旨在通过集成评估，解决评估实践不统一的问题，为安全性能的准确评估提供基础。",
      "method": "研究采用统一协议对7个模型（包括GPT-5.2、Gemini 3 Pro等）在语言、视觉语言和图像生成设置下进行集成安全性评估。核心方法包括基准评估、对抗评估、多语言评估和合规性评估，关键创新在于整合多种评估方式，覆盖不同模态，以全面评估模型的安全性能。摘要未明确说明具体数据集和模型架构细节，但提及了模型名称和评估模式，旨在提供标准化的评估框架。",
      "result": "评估结果显示，模型安全性存在显著异质性：GPT-5.2在所有评估中表现一致强大且平衡，而其他模型在基准安全性、对抗对齐、多语言泛化和监管合规性之间存在明显权衡。尽管在标准基准上表现良好，所有模型在对抗评估下性能大幅下降，语言和视觉语言模态尤为脆弱。文本到图像模型在受监管的视觉风险类别中相对更强对齐，但在对抗性或语义模糊提示下仍表现脆弱，摘要未提供具体数值对比，但强调了安全性性能的多样性。",
      "conclusion": "本研究揭示了前沿模型安全性的多维性，受模态、语言和评估方案的影响。主要贡献在于提出集成安全性评估框架，推动了标准化评估的学术价值，有助于准确评估现实世界风险，指导负责任模型开发和部署。局限性包括评估范围可能有限，未来工作可扩展更多模型和威胁模型，并改进评估协议以提升鲁棒性。",
      "tags": [
        "Large Language Model",
        "Multimodal Large Language Model",
        "Safety Evaluation",
        "Adversarial Evaluation",
        "Compliance Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:37.182853Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10524",
    "title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection",
    "authors": [
      "Frank Bobe",
      "Gregory D. Vetaw",
      "Chase Pavlick",
      "Darshan Bryner",
      "Matthew Cook",
      "Jose Salas-Vernis"
    ],
    "abstract": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10524.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10524",
    "published": "2026-01-15T15:51:24Z",
    "updated": "2026-01-15T15:51:24Z",
    "comment": "16 pages, 6 figures, 6 tables",
    "light_analysis": {
      "overview": "本文提出并应用诊断框架，通过跨架构研究揭示微调大型语言模型泛化失败的根本原因，强调架构与数据多样性的协同作用。",
      "motivation": "微调大型语言模型在专门任务上虽能达到最佳性能，但模型在泛化过程中变得脆弱并失败的原因仍是一个关键开放问题。本研究旨在解决这一问题，特别是在高风险钓鱼检测任务中，理解泛化失败对于构建可靠的AI系统至关重要，因为现有方法缺乏有效工具来诊断模型内部机制，导致无法优化泛化能力。",
      "method": "研究引入并应用一个多层次的诊断框架，进行跨架构分析。具体方法包括微调三个不同架构的模型（Llama 3.1 8B、Gemma 2 9B和Mistral）于钓鱼检测任务，并使用SHAP分析和机制可解释性技术来揭示泛化失败的根本原因。关键创新点在于结合这些工具深入理解模型行为和失败模式，突出了架构选择和数据集多样性对泛化的影响。",
      "result": "实验结果显示，泛化性能高度依赖于架构和数据多样性。Gemma 2 9B在风格多样的“通用”数据集上达到了超过91%的F1分数，表现出色；而Llama 3.1 8B在窄域上表现良好，但无法整合多样数据，导致性能显著下降；Mistral模型则在多种训练范式中表现一致和稳健。这些发现通过具体指标（如F1分数）强调了不同架构在泛化能力上的差异。",
      "conclusion": "本研究的主要贡献在于提供了一个具体方法来诊断和理解微调大型语言模型的泛化失败，通过识别导致失败的启发式方法，强调了可靠AI需要深入验证架构、数据和训练策略之间的相互作用。学术价值在于推进模型可解释性研究，实际应用有助于优化高风险任务的AI系统。未来工作可能涉及扩展到更多架构和任务，以验证方法的通用性。",
      "tags": [
        "Large Language Models",
        "Fine-Tuning",
        "Generalization Failures",
        "SHAP Analysis",
        "Mechanistic Interpretability"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:45.009219Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10521",
    "title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition",
    "authors": [
      "Max A. Buettner",
      "Kanak Mazumder",
      "Luca Koecher",
      "Mario Finkbeiner",
      "Sebastian Niebler",
      "Fabian B. Flohr"
    ],
    "abstract": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10521.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10521",
    "published": "2026-01-15T15:47:46Z",
    "updated": "2026-01-15T15:47:46Z",
    "comment": "This work has been submitted to the IEEE ICPR for possible publication",
    "light_analysis": {
      "overview": "本文提出首个自行车手视角的开放感知平台FUSE-Bike和数据集BikeActions，用于VRU动作识别，并建立了首个性能基准。",
      "motivation": "自动驾驶和移动机器人中，预测弱势道路用户（VRUs）的意图是保障安全的关键挑战。当前研究多从车辆视角关注行人过街行为，忽略了密集共享空间（如自行车道）中的交互，导致VRU行为建模不全面。这种局限性限制了自动驾驶系统的适应性，需要从自行车手这一核心VRU群体的视角出发，收集高质量数据以弥补研究空白。",
      "method": "研究引入FUSE-Bike平台，配备两个LiDAR、一个相机和GNSS，从自行车手视角捕获高保真、近距离多模态数据。利用该平台，创建了BikeActions数据集，包含852个标注样本，覆盖5个不同动作类别。进一步，采用最先进的图卷积和transformer模型在公开数据分割上进行评估，建立首个VRU动作识别的技术基准，推动模型比较与改进。",
      "result": "论文通过评估图卷积和transformer模型，为VRU动作识别任务建立了第一个性能基准，尽管摘要未提供具体准确率或效率指标，但基准的设置为后续研究提供了基础比较。作者开源了完整数据集、数据管理工具和基准代码，促进了社区合作与未来技术发展。",
      "conclusion": "本研究的主要贡献在于推出了首个开放的、自行车手视角的VRU动作识别平台和数据集，填补了密集共享空间中行为建模的空白。学术上，提供了新的多模态资源，推动VRU动作理解领域进展；实践上，有助于提升自动驾驶和机器人的安全性能。未来可扩展数据集规模或探索更多复杂交互动作以增强模型泛化能力。",
      "tags": [
        "Vulnerable Road Users (VRU)",
        "Action Recognition",
        "Multi-modal Dataset",
        "Graph Convolutional Networks",
        "Transformers"
      ]
    },
    "analyzed_at": "2026-01-16T03:31:00.035766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10520",
    "title": "Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment",
    "authors": [
      "Felix Jahn",
      "Yannic Muskalla",
      "Lisa Dargasz",
      "Patrick Schramowski",
      "Kevin Baum"
    ],
    "abstract": "As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GRACE restructures decision-making into three modules: a Moral Module (MM) that determines permissible macro actions via deontic logic-based reasoning; a Decision-Making Module (DMM) that encapsulates the target agent while selecting instrumentally optimal primitive actions in accordance with derived macro actions; and a Guard that monitors and enforces moral compliance. The MM uses a reason-based formalism providing a semantic foundation for deontic logic, enabling interpretability, contestability, and justifiability. Its symbolic representation enriches the DMM's informational context and supports formal verification and statistical guarantees of alignment enforced by the Guard. We demonstrate GRACE on an example of a LLM therapy assistant, showing how it enables stakeholders to understand, contest, and refine agent behavior.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10520.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10520",
    "published": "2026-01-15T15:47:38Z",
    "updated": "2026-01-15T15:47:38Z",
    "comment": "10 pages, 4 figures, accepted at 2nd Annual Conference of the International Association for Safe & Ethical AI (IASEAI'26)",
    "light_analysis": {
      "overview": "提出GRACE神经-符号架构，通过解耦规范推理与工具决策，实现AI安全和伦理对齐。",
      "motivation": "随着AI代理在关键场景中广泛部署且自主性增强，确保其决策既有效又符合规范变得至关重要。现有方法往往将规范推理与工具决策紧密结合，导致缺乏可解释性、可控性和安全性，容易引发伦理风险。因此，开发一个能有效对齐AI行为、支持监管的架构是当前研究的迫切需求，以应对实际应用中的挑战。",
      "method": "论文引入GRACE架构，包括三个模块：Moral Module使用基于理由的道义逻辑推理确定宏观行动；Decision-Making Module封装目标代理选择最优微观行动；Guard监控并强制执行道德合规。该方法采用神经-符号结合，通过符号表示增强信息上下文，支持形式验证和统计保证，提高可解释性、可争议性和可验证性，适用于各种AI设计。",
      "result": "在LLM治疗助手的示例中，GRACE展示了如何使利益相关者理解、争议和改进代理行为，增强了AI对齐的可控性和透明性。摘要未明确说明具体性能指标如准确率，但通过架构设计实现了对决策过程的规范约束，提升了安全与伦理保障，为实际应用提供了可验证的基础。",
      "conclusion": "GRACE架构的主要贡献在于提供了一个可解释、可验证的AI对齐框架，通过神经-符号结合和基于理由的形式化，解决了安全和伦理问题。其学术价值推动AI对齐领域的发展，实际应用潜力在医疗、自动驾驶等关键领域。未来工作可扩展到更多场景，并进一步验证其泛化能力和局限性。",
      "tags": [
        "Neuro-Symbolic Architecture",
        "GRACE",
        "Deontic Logic",
        "AI Alignment",
        "Reason-Based Formalism"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:52.693606Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10519",
    "title": "Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models",
    "authors": [
      "Andrea Melis",
      "Andrea Piroddi",
      "Roberto Girau"
    ],
    "abstract": "Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10519.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10519",
    "published": "2026-01-15T15:46:22Z",
    "updated": "2026-01-15T15:46:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于Transformer模型的方法，通过GPT-2生成自适应调制策略，以改进认知无线电系统的频谱效率和性能。",
      "motivation": "认知无线电系统需要动态适应变化的频谱环境，以提高频谱效率、鲁棒性和安全性，但传统调制方法在创新性和适应性方面有限，难以应对复杂环境挑战。因此，本研究探索利用先进的机器学习技术，特别是Transformer模型，来解决这一问题。这一研究的重要性在于，通过引入生成式模型，可以为无线通信提供更灵活和高效的调制方案，从而推动通信技术的发展，并填补传统方法在智能频谱管理方面的不足。",
      "method": "本研究采用基于GPT-2架构的Transformer模型，通过在现有调制公式的数据集上进行训练，生成新的调制方案。核心创新在于将Transformer模型应用于调制策略的生成，这是一种新颖的机器学习应用。具体技术路线包括数据收集（使用现有调制公式作为数据集）、模型训练（利用GPT-2架构进行学习）、以及生成过程，以产生适应性强的新调制方案，而不依赖人工设计。",
      "result": "实验结果表明，通过Transformer生成的调制方案在与传统方法比较时，在关键性能指标如信号噪声比和功率谱密度方面表现出相当或更优的性能。具体结果显示，生成的方案在某些情况下超越了传统方法，虽然摘要未提供精确数据，但这验证了Transformer模型在调制策略生成中的有效性，并为实际应用提供了潜在优势，提升了通信系统的整体表现。",
      "conclusion": "本研究的贡献在于展示了Transformer模型在认知无线电系统中生成自适应调制策略的有效性，验证了其能够提高通信系统的效率、鲁棒性和安全性。学术价值在于扩展了Transformer模型的应用领域到无线通信调制，实际应用价值在于推动更智能和高效的通信系统发展。未来工作可探索不同模型架构或更复杂的训练数据集以进一步提升性能，摘要未明确说明局限性，但可推断需要更多实验验证。",
      "tags": [
        "Cognitive Radio",
        "Transformer Models",
        "GPT-2",
        "Modulation Schemes",
        "Signal-to-Noise Ratio"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:12.182345Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10513",
    "title": "AEQ-Bench: Measuring Empathy of Omni-Modal Large Models",
    "authors": [
      "Xuan Luo",
      "Lewei Yao",
      "Libo Zhao",
      "Lanqing Hong",
      "Kai Chen",
      "Dehua Tao",
      "Daxin Tan",
      "Ruifeng Xu",
      "Jing Li"
    ],
    "abstract": "While the automatic evaluation of omni-modal large models (OLMs) is essential, assessing empathy remains a significant challenge due to its inherent affectivity. To investigate this challenge, we introduce AEQ-Bench (Audio Empathy Quotient Benchmark), a novel benchmark to systematically assess two core empathetic capabilities of OLMs: (i) generating empathetic responses by comprehending affective cues from multi-modal inputs (audio + text), and (ii) judging the empathy of audio responses without relying on text transcription. Compared to existing benchmarks, AEQ-Bench incorporates two novel settings that vary in context specificity and speech tone. Comprehensive assessment across linguistic and paralinguistic metrics reveals that (1) OLMs trained with audio output capabilities generally outperformed models with text-only outputs, and (2) while OLMs align with human judgments for coarse-grained quality assessment, they remain unreliable for evaluating fine-grained paralinguistic expressiveness.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10513.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10513",
    "published": "2026-01-15T15:39:50Z",
    "updated": "2026-01-15T15:39:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文的核心贡献是引入了AEQ-Bench基准，用于系统评估全模态大模型在多模态输入和音频输出下的共情能力。",
      "motivation": "研究动机源于全模态大模型的自动评估需求，特别是在评估其共情能力时面临挑战，因为共情具有内在的情感性，传统方法难以量化。现有基准通常侧重于文本或单一模态，忽略了多模态上下文下的共情评估，缺乏对音频输出和副语言特征的系统性分析。这导致模型在实际交互中可能表现不足，因此需要开发新基准来弥补这一缺陷，以更准确地衡量OLMs的情感理解和生成能力。",
      "method": "研究方法的核心是提出AEQ-Bench基准，它系统评估全模态大模型的两个核心共情能力：首先，通过理解多模态输入（音频和文本）生成共情回应；其次，在不依赖文本转录的情况下直接判断音频回应的共情程度。关键创新包括设计了两个新设置，这些设置在上下文特异性和语音语调方面变化，以覆盖多样化的评估场景。评估过程使用综合的语言和副语言指标，如情感分析和语音特征测量，以确保全面性，避免仅依赖文本的局限性。",
      "result": "主要实验结果显示，具备音频输出能力的全模态大模型通常在共情能力上优于仅支持文本输出的模型。通过综合评估发现，OLMs在粗粒度质量评估方面与人类判断基本一致，但在细粒度副语言表达性评估上表现不可靠，这表明模型在处理音频中的细微情感线索时存在不足。与基线方法对比，音频输出模型在理解多模态输入方面显示出优势，但整体性能仍有改进空间，尤其是在精确评估情感表达方面。",
      "conclusion": "论文的主要贡献是开发了AEQ-Bench基准，为评估全模态大模型的共情能力提供了系统化工具，具有重要学术价值，填补了多模态评估的空白，并有助于指导模型优化。实际应用中，基准可促进智能交互系统的发展。局限性在于模型在细粒度副语言评估上表现不稳定，未来工作可专注于增强音频处理技术、改进情感模型，或扩展基准以涵盖更多模态和真实场景，以提升评估的准确性和可靠性。",
      "tags": [
        "Omni-Modal Large Models",
        "Empathy Assessment",
        "Audio Processing",
        "Multi-modal Learning",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:23.402668Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10512",
    "title": "SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction",
    "authors": [
      "Kanak Mazumder",
      "Fabian B. Flohr"
    ],
    "abstract": "Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages lane-level semantics and texture from satellite imagery captured from a Bird's Eye View (BEV) perspective as a global prior, effectively mitigating depth ambiguity and occlusion. In our experiments on the nuScenes dataset, SatMap achieves 34.8% mAP performance improvement over the camera-only baseline and 8.5% mAP improvement over the camera-LiDAR fusion baseline. Moreover, we evaluate our model in long-range and adverse weather conditions to demonstrate the advantages of using a satellite prior map. Source code will be available at https://iv.ee.hm.edu/satmap/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10512.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10512",
    "published": "2026-01-15T15:39:27Z",
    "updated": "2026-01-15T15:39:27Z",
    "comment": "This work has been submitted to the IEEE ICPR for possible publication",
    "light_analysis": {
      "overview": "SatMap提出一种通过整合卫星地图作为全局先验的方法，显著提升在线高清地图构建的性能，解决自动驾驶中摄像头方法的深度感知和遮挡问题。",
      "motivation": "在线高清地图构建是自动驾驶系统的关键组成部分，用于支持安全可靠的端到端驾驶流程。然而，基于车载摄像头的方法存在深度感知能力有限和环境遮挡导致的精度下降问题。现有方法如纯摄像头方案在复杂场景下表现不足，而摄像头与LiDAR融合方案仍有改进空间。这影响了自动驾驶系统的整体可靠性和地图构建的准确性，因此需要引入额外信息源来增强感知能力。",
      "method": "SatMap是一种在线向量化高清地图估计方法，整合卫星地图与多视图相机观测，直接预测向量化高清地图供下游模块使用。该方法的核心创新在于利用卫星图像从鸟瞰视角提供的语义和纹理信息作为全局先验，有效缓解深度模糊和遮挡。具体技术路线包括多摄像头输入处理、卫星地图数据融合，并在nuScenes数据集上进行实现和验证，生成可直接用于自动驾驶的高清地图向量。",
      "result": "在nuScenes数据集上的实验显示，SatMap相比纯摄像头基线方法，平均精度提高了34.8% mAP；相比摄像头与LiDAR融合基线，也实现了8.5% mAP的提升。此外，在长距离场景和恶劣天气条件下的评估中，SatMap展现出优势，进一步验证了卫星地图先验的有效性。这些结果表明，该方法在多种环境下都能提供更精确和鲁棒的高清地图构建，支持自动驾驶系统的实际应用。",
      "conclusion": "本研究的主要贡献是提出SatMap方法，通过整合卫星地图作为先验，显著提高了在线高清地图构建的精度和鲁棒性。该工作在学术上推动了自动驾驶感知技术的发展，为高清地图构建提供了新思路；在实际应用上，有助于提升自动驾驶系统的安全性和可靠性。摘要未明确说明潜在的局限性，未来工作可能涉及更广泛场景的验证和效率优化。",
      "tags": [
        "Online HD Map Construction",
        "Satellite Imagery",
        "Bird's Eye View (BEV)",
        "Multi-view Camera",
        "Vectorized Maps"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:28.710987Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10504",
    "title": "DR-Arena: an Automated Evaluation Framework for Deep Research Agents",
    "authors": [
      "Yiwen Gao",
      "Ruochen Zhao",
      "Yang Deng",
      "Wenxuan Zhang"
    ],
    "abstract": "As Large Language Models (LLMs) increasingly operate as Deep Research (DR) Agents capable of autonomous investigation and information synthesis, reliable evaluation of their task performance has become a critical bottleneck. Current benchmarks predominantly rely on static datasets, which suffer from several limitations: limited task generality, temporal misalignment, and data contamination. To address these, we introduce DR-Arena, a fully automated evaluation framework that pushes DR agents to their capability limits through dynamic investigation. DR-Arena constructs real-time Information Trees from fresh web trends to ensure the evaluation rubric is synchronized with the live world state, and employs an automated Examiner to generate structured tasks testing two orthogonal capabilities: Deep reasoning and Wide coverage. DR-Arena further adopts Adaptive Evolvement Loop, a state-machine controller that dynamically escalates task complexity based on real-time performance, demanding deeper deduction or wider aggregation until a decisive capability boundary emerges. Experiments with six advanced DR agents demonstrate that DR-Arena achieves a Spearman correlation of 0.94 with the LMSYS Search Arena leaderboard. This represents the state-of-the-art alignment with human preferences without any manual efforts, validating DR-Arena as a reliable alternative for costly human adjudication.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10504.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10504",
    "published": "2026-01-15T15:28:21Z",
    "updated": "2026-01-15T15:28:21Z",
    "comment": "22 pages, 8 figures",
    "light_analysis": {
      "overview": "DR-Arena提出了一种自动化评估框架，通过动态调查和自适应任务复杂化，可靠评估大型语言模型的深度研究能力。",
      "motivation": "随着大语言模型作为深度研究代理的兴起，对其任务性能的可靠评估已成为关键瓶颈。当前评估主要依赖静态数据集，存在任务泛化性有限、时间错位和数据污染等问题，无法同步实时世界状态，导致评估准确性不足。这些问题限制了代理的发展和实际应用，因此需要一种动态、自适应的解决方案来克服现有基准的局限性，提升评估的泛化和有效性。",
      "method": "DR-Arena框架采用全自动化评估方法，核心包括构建实时信息树，从新鲜网络趋势中提取数据，确保评估标准与实时世界状态同步。自动化考官生成结构化任务，测试两个正交能力：深度推理和广泛覆盖。此外，自适应进化循环作为状态机控制器，根据实时性能动态调整任务复杂度，逐步提高难度，直到确定能力边界。关键创新在于动态调查和自适应机制，避免了静态数据集的依赖。",
      "result": "实验在六个先进的DR代理上进行，结果显示DR-Arena与LMSYS Search Arena排行榜的Spearman相关系数达到0.94。这表示该方法在无需人工干预的情况下，与人类偏好高度一致，实现了最先进的对齐效果。验证了DR-Arena作为昂贵人工评估的可靠替代，显著提升了评估效率和准确性，相比静态基准有显著改进。",
      "conclusion": "DR-Arena的主要贡献是提出了一个全自动、动态的评估框架，解决了静态数据集在评估深度研究代理时的局限性。该研究具有重要学术价值，为LLM评估提供了创新方法，促进技术发展；实际应用中，能减少成本并增强评估可靠性。未来工作方向可能包括扩展框架到更多任务类型或优化自适应算法，以进一步提升性能。",
      "tags": [
        "Large Language Model",
        "Automated Evaluation",
        "Adaptive Evolution Loop",
        "Real-time Information Tree",
        "Orthogonal Capability Testing"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:20.577603Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10498",
    "title": "Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning",
    "authors": [
      "Nilin Abrahamsen"
    ],
    "abstract": "This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10498.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10498",
    "published": "2026-01-15T15:16:15Z",
    "updated": "2026-01-15T15:16:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文介绍了一种新的投影微批次积累（PROMA）方法，用于强化学习中的近策略更新，不依赖参考策略或似然比裁剪，提高了策略学习的稳定性。",
      "motivation": "该研究旨在解决强化学习中策略更新不稳定的问题，尤其是在大语言模型微调过程中。现有方法如PPO和GRPO依赖参考策略或可能引起熵崩溃，限制了学习效率和模型的稳健性。通过设计无参考策略的更新机制，PROMA避免了这些缺陷，为策略优化提供了更可靠的基础，从而在复杂任务中提升性能。",
      "method": "PROMA方法的核心是通过在微批次梯度积累前投影出序列梯度分量来实现近策略更新。投影在后向传递中逐层应用，无需额外前向或后向传递，确保了高效实现。该方法专为大语言模型微调设计，利用梯度投影技术控制策略更新，避免了对参考策略或似然比裁剪的依赖。关键创新点包括逐层投影和无参考更新机制。",
      "result": "实验结果表明，PROMA比GRPO能更严格地控制局部KL散度，实现了更稳定的策略学习。与PPO和GRPO相比，PROMA不引起熵崩溃，并且不依赖于参考策略或似然比裁剪。这些发现证明了PROMA在提升学习稳定性和效率方面的优势，摘要未提供具体数值，但实证显示其在强化学习任务中表现卓越。",
      "conclusion": "本研究的主要贡献是提出了PROMA方法，一种无参考策略的近策略更新技术，解决了现有方法依赖参考策略和熵崩溃的问题。它具有重要的学术价值，为强化学习策略优化提供了创新思路，并在大语言模型微调等实际应用中具有潜力。未来工作可能涉及PROMA的进一步优化和在其他领域的扩展，以增强其通用性。",
      "tags": [
        "Reinforcement Learning",
        "Proximal Policy Update",
        "Gradient Projection",
        "Microbatch Accumulation",
        "KL Divergence Control"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:35.769091Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10497",
    "title": "mergetune: Continued fine-tuning of vision-language models",
    "authors": [
      "Wenqing Wang",
      "Da Li",
      "Xiatian Zhu",
      "Josef Kittler"
    ],
    "abstract": "Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \\emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\\% on base-novel generalisation without adding parameters. % We show \\emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10497.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10497",
    "published": "2026-01-15T15:15:53Z",
    "updated": "2026-01-15T15:15:53Z",
    "comment": "20 pages, 5 figures",
    "light_analysis": {
      "overview": "MERGETUNE提出一种基于线性模式连接的持续微调方法，旨在恢复视觉语言模型微调后丢失的预训练知识。",
      "motivation": "微调视觉语言模型（如CLIP）常导致灾难性遗忘，即预训练知识丢失，影响模型泛化能力。现有方法虽旨在缓解遗忘，但遗忘在微调过程中仍不可避免。本研究针对这一问题，提出恢复微调后丢失知识的必要性，因为保持预训练知识对跨数据集转移和模型鲁棒性至关重要。传统方法在知识恢复方面存在不足，尤其是缺乏有效的后处理策略。",
      "method": "论文引入持续微调（CFT）新范式，并提出MERGETUNE策略。该方法基于线性模式连接（LMC），无需更改模型架构，可后处理现有微调模型。核心步骤包括：继续微调模型的软提示或线性头等可训练参数，寻找一个持续模型，该模型有两条低损失路径分别连接零样本解（如CLIP）和微调解（如CoOp）。通过损失景观几何，模型隐含合并两个解，恢复知识。创新点在于使用二阶代理近似LMC约束，避免了大规模数据回放的需求。",
      "result": "实验结果显示，MERGETUNE在CoOp基准上，将基础-新颖泛化的谐波平均提升+5.6%，且未增加额外参数。在跨数据集转移中，首次在DTD和EuroSAT数据集上超越CLIP性能。在鲁棒微调评估中，通过LMC合并的模型超越集成基线，推理成本更低；当与零样本模型集成时，进一步取得增益并达到最新成果，展现了高效恢复知识的能力。",
      "conclusion": "论文主要贡献是提出持续微调范式和MERGETUNE方法，有效恢复微调模型中丢失的预训练知识，提升模型泛化和鲁棒性。学术价值在于为灾难性遗忘问题提供新解决方案，利用损失景观优化技术；实际应用价值在于模型无关性和无需参数增加即可改进性能。摘要未明确说明未来方向，但可能包括扩展至其他视觉语言模型或任务。",
      "tags": [
        "Vision-Language Models",
        "Fine-tuning",
        "Linear Mode Connectivity",
        "Knowledge Recovery",
        "Model Merging"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:29.053518Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10491",
    "title": "Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients",
    "authors": [
      "Shenlong Zheng",
      "Zhen Zhang",
      "Yuhui Deng",
      "Geyong Min",
      "Lin Cui"
    ],
    "abstract": "Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10491.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10491",
    "published": "2026-01-15T15:11:41Z",
    "updated": "2026-01-15T15:11:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了GradESTC技术，通过利用梯度的时空相关性来减少联邦学习的通信开销。",
      "motivation": "联邦学习中的通信开销是关键挑战，尤其在带宽受限网络中。现有方法多专注于压缩单个梯度，忽略了梯度间的时间相关性，导致通信效率受限。梯度已表现出空间相关性（如低秩结构），但通过实证分析发现相邻轮次间梯度具有强时间相关性，这为解决实际部署中的瓶颈问题提供了新思路。",
      "method": "GradESTC方法利用空间相关性将全梯度分解为基础向量和组合系数，并利用时间相关性在每轮中仅动态更新部分基础向量。关键创新在于综合处理梯度的时空结构，通过传输轻量级组合系数和少量更新的基础向量替代全梯度传输，显著降低通信开销。摘要未明确说明具体数据集或模型架构细节。",
      "result": "大量实验表明，在达到目标准确率近收敛时，GradESTC相比最强基线平均减少39.79%的上行通信开销，同时保持与未压缩FedAvg相当的收敛速度和最终准确率。这证明了该方法在维持模型性能的同时，有效提升了通信效率，为实际应用提供了数据支撑。",
      "conclusion": "论文的主要贡献是提出GradESTC技术，通过有效利用梯度的时空结构，为通信高效的联邦学习提供了实用和可扩展的解决方案。该研究具有学术价值，推动了优化技术的发展，并在物联网和边缘计算等实际场景有应用前景。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Federated Learning",
        "Gradient Compression",
        "Spatio-Temporal Correlations",
        "Low-Rank Decomposition",
        "Communication Efficiency"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:44.078858Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10485",
    "title": "Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge",
    "authors": [
      "Runhao Zhao",
      "Weixin Zeng",
      "Wentao Zhang",
      "Chong Chen",
      "Zhengpin Li",
      "Xiang Zhao",
      "Lei Chen"
    ],
    "abstract": "Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program, maps abstract relations to granularity-aware operators, and verifies domain relevance via program executability on the target DKG. This unified probabilistic framework jointly resolves relevance and granularity issues. We construct two benchmarks, DKGF(W-I) and DKGF(Y-I), with 21 evaluation configurations. Extensive experiments validate the task's importance and our model's effectiveness, providing the first standardized testbed for DKGF.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10485.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10485",
    "published": "2026-01-15T15:06:56Z",
    "updated": "2026-01-15T15:06:56Z",
    "comment": "13 pages, 4 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10477",
    "title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning",
    "authors": [
      "Yu Wang",
      "Yi Wang",
      "Rui Dai",
      "Yujie Wang",
      "Kaikui Liu",
      "Xiangxiang Chu",
      "Yansheng Li"
    ],
    "abstract": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10477.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10477",
    "published": "2026-01-15T15:00:36Z",
    "updated": "2026-01-15T15:00:36Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10471",
    "title": "DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction",
    "authors": [
      "Zhancun Mu"
    ],
    "abstract": "We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10471.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10471",
    "published": "2026-01-15T14:56:57Z",
    "updated": "2026-01-15T14:56:57Z",
    "comment": "13 pages, 3 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10462",
    "title": "ChartComplete: A Taxonomy-based Inclusive Chart Dataset",
    "authors": [
      "Ahmad Mustapha",
      "Charbel Toumieh",
      "Mariette Awad"
    ],
    "abstract": "With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10462.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10462",
    "published": "2026-01-15T14:51:15Z",
    "updated": "2026-01-15T14:51:15Z",
    "comment": "7 pages, 4 figures, 3 tables, 1 algorithm. Dataset and source code available at https://github.com/AI-DSCHubAUB/ChartComplete-Dataset",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10460",
    "title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models",
    "authors": [
      "Abhinaba Basu",
      "Pavan Chakraborty"
    ],
    "abstract": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.   We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.   We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.   The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10460.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10460",
    "published": "2026-01-15T14:50:49Z",
    "updated": "2026-01-15T14:50:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10457",
    "title": "NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models",
    "authors": [
      "Ziming Dai",
      "Dabiao Ma",
      "Jinle Tong",
      "Mengyuan Han",
      "Jian Yang",
      "Haojun Fei"
    ],
    "abstract": "Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being \"non-intrusive\". It treats the legacy model as a frozen model and performs targeted repairs on \"hard regions\" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10457.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10457",
    "published": "2026-01-15T14:48:52Z",
    "updated": "2026-01-15T14:48:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10455",
    "title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability",
    "authors": [
      "Ruochen Li",
      "Kun Yuan",
      "Yufei Xia",
      "Yue Zhou",
      "Qingyu Lu",
      "Weihang Li",
      "Youxiang Zhu",
      "Nassir Navab"
    ],
    "abstract": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.",
    "categories": [
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10455.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10455",
    "published": "2026-01-15T14:47:26Z",
    "updated": "2026-01-15T14:47:26Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10449",
    "title": "Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation",
    "authors": [
      "Clementine Grethen",
      "Nicolas Menga",
      "Roland Brochard",
      "Geraldine Morin",
      "Simone Gasparini",
      "Jeremy Lebreton",
      "Manuel Sanchez Gestido"
    ],
    "abstract": "We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10449.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10449",
    "published": "2026-01-15T14:39:25Z",
    "updated": "2026-01-15T14:39:25Z",
    "comment": "Data & code: https://clementinegrethen.github.io/publications/Lunar-G2R",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10421",
    "title": "Are Language Models Models?",
    "authors": [
      "Philip Resnik"
    ],
    "abstract": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10421.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10421",
    "published": "2026-01-15T14:13:01Z",
    "updated": "2026-01-15T14:13:01Z",
    "comment": "5 pages. This is an invited commentary under review at Behavioral and Brain Sciences",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10418",
    "title": "Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching",
    "authors": [
      "Nadav Merlis"
    ],
    "abstract": "We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\\ell$, which can usually be considered a small constant.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10418.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10418",
    "published": "2026-01-15T14:09:49Z",
    "updated": "2026-01-15T14:09:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10416",
    "title": "LLMdoctor: Token-Level Flow-Guided Preference Optimization for Efficient Test-Time Alignment of Large Language Models",
    "authors": [
      "Tiesunlong Shen",
      "Rui Mao",
      "Jin Wang",
      "Heming Sun",
      "Jian Zhang",
      "Xuejie Zhang",
      "Erik Cambria"
    ],
    "abstract": "Aligning Large Language Models (LLMs) with human preferences is critical, yet traditional fine-tuning methods are computationally expensive and inflexible. While test-time alignment offers a promising alternative, existing approaches often rely on distorted trajectory-level signals or inefficient sampling, fundamentally capping performance and failing to preserve the generative diversity of the base model. This paper introduces LLMdoctor, a novel framework for efficient test-time alignment that operates via a patient-doctor paradigm. It integrates token-level reward acquisition with token-level flow-guided preference optimization (TFPO) to steer a large, frozen patient LLM with a smaller, specialized doctor model. Unlike conventional methods that rely on trajectory-level rewards, LLMdoctor first extracts fine-grained, token-level preference signals from the patient model's behavioral variations. These signals then guide the training of the doctor model via TFPO, which establishes flow consistency across all subtrajectories, enabling precise token-by-token alignment while inherently preserving generation diversity. Extensive experiments demonstrate that LLMdoctor significantly outperforms existing test-time alignment methods and even surpasses the performance of full fine-tuning approaches like DPO.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10416.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10416",
    "published": "2026-01-15T14:05:40Z",
    "updated": "2026-01-15T14:05:40Z",
    "comment": "Accepted by AAAI26",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10413",
    "title": "LADFA: A Framework of Using Large Language Models and Retrieval-Augmented Generation for Personal Data Flow Analysis in Privacy Policies",
    "authors": [
      "Haiyue Yuan",
      "Nikolay Matyunin",
      "Ali Raza",
      "Shujun Li"
    ],
    "abstract": "Privacy policies help inform people about organisations' personal data processing practices, covering different aspects such as data collection, data storage, and sharing of personal data with third parties. Privacy policies are often difficult for people to fully comprehend due to the lengthy and complex legal language used and inconsistent practices across different sectors and organisations. To help conduct automated and large-scale analyses of privacy policies, many researchers have studied applications of machine learning and natural language processing techniques, including large language models (LLMs). While a limited number of prior studies utilised LLMs for extracting personal data flows from privacy policies, our approach builds on this line of work by combining LLMs with retrieval-augmented generation (RAG) and a customised knowledge base derived from existing studies. This paper presents the development of LADFA, an end-to-end computational framework, which can process unstructured text in a given privacy policy, extract personal data flows and construct a personal data flow graph, and conduct analysis of the data flow graph to facilitate insight discovery. The framework consists of a pre-processor, an LLM-based processor, and a data flow post-processor. We demonstrated and validated the effectiveness and accuracy of the proposed approach by conducting a case study that involved examining ten selected privacy policies from the automotive industry. Moreover, it is worth noting that LADFA is designed to be flexible and customisable, making it suitable for a range of text-based analysis tasks beyond privacy policy analysis.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10413.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10413",
    "published": "2026-01-15T14:03:22Z",
    "updated": "2026-01-15T14:03:22Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10410",
    "title": "TF3-RO-50M: Training Compact Romanian Language Models from Scratch on Synthetic Moral Microfiction",
    "authors": [
      "Mihai Dan Nadas",
      "Laura Diosan",
      "Andreea Tomescu",
      "Andrei Piscoran"
    ],
    "abstract": "Recent advances in synthetic data generation have shown that compact language models can be trained effectively when the underlying corpus is structurally controlled and linguistically coherent. However, for morphologically rich and computationally under-resourced languages such as Romanian, there is still no openly documented, end-to-end pipeline that unifies tokenizer design, preprocessing, pretraining, compression, evaluation, and large-scale synthetic data generation in a reproducible framework. Building on TF1, a three-million-story English fable dataset, and TF2, which extends TF1 through high-quality Romanian translations, we introduce TF3-RO, a Romanian-centric language modeling pipeline spanning tokenizer training, from-scratch model development, and Romanian-native dataset generation. TF3-RO constructs Romanian-specific BPE and Unigram tokenizers from a linguistically informed corpus to mitigate token inflation induced by Romanian morphology. Using long-sequence packed training, we pretrain a 51.65M-parameter LLaMA-style Transformer entirely from scratch. The model is subsequently optimized through quantization, structured pruning, and logit-based knowledge distillation, yielding a compact 26.45M-parameter student model with tied embeddings and strong deployment characteristics. Using this distilled model, TF3-RO generates three million Romanian-native synthetic fables via a controlled combinatorial prompting framework. Across all stages, the pipeline integrates a comprehensive evaluation suite combining intrinsic metrics, Romanian agreement probes, entity coherence, rule-based grammar checking, and LLM-based assessment. TF3-RO provides a reproducible and linguistically grounded framework for training compact Romanian language models and producing large-scale synthetic narrative corpora.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10410.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10410",
    "published": "2026-01-15T14:02:00Z",
    "updated": "2026-01-15T14:02:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10407",
    "title": "CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning",
    "authors": [
      "Yuanjie Zhao",
      "Junnan Qiu",
      "Yue Ding",
      "Jie Li"
    ],
    "abstract": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10407.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10407",
    "published": "2026-01-15T13:57:52Z",
    "updated": "2026-01-15T13:57:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10406",
    "title": "ErrEval: Error-Aware Evaluation for Question Generation through Explicit Diagnostics",
    "authors": [
      "Weiping Fu",
      "Bifan Wei",
      "Jingyi Hao",
      "Yushun Zhang",
      "Jian Zhang",
      "Jiaxin Wang",
      "Bo Li",
      "Yu He",
      "Lingling Zhang",
      "Jun Liu"
    ],
    "abstract": "Automatic Question Generation (QG) often produces outputs with critical defects, such as factual hallucinations and answer mismatches. However, existing evaluation methods, including LLM-based evaluators, mainly adopt a black-box and holistic paradigm without explicit error modeling, leading to the neglect of such defects and overestimation of question quality. To address this issue, we propose ErrEval, a flexible and Error-aware Evaluation framework that enhances QG evaluation through explicit error diagnostics. Specifically, ErrEval reformulates evaluation as a two-stage process of error diagnosis followed by informed scoring. At the first stage, a lightweight plug-and-play Error Identifier detects and categorizes common errors across structural, linguistic, and content-related aspects. These diagnostic signals are then incorporated as explicit evidence to guide LLM evaluators toward more fine-grained and grounded judgments. Extensive experiments on three benchmarks demonstrate the effectiveness of ErrEval, showing that incorporating explicit diagnostics improves alignment with human judgments. Further analyses confirm that ErrEval effectively mitigates the overestimation of low-quality questions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10406.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10406",
    "published": "2026-01-15T13:57:15Z",
    "updated": "2026-01-15T13:57:15Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10403",
    "title": "Discrete Feynman-Kac Correctors",
    "authors": [
      "Mohsin Hasan",
      "Viktor Ohanesian",
      "Artem Gazizov",
      "Yoshua Bengio",
      "Alán Aspuru-Guzik",
      "Roberto Bondesan",
      "Marta Skreta",
      "Kirill Neklyudov"
    ],
    "abstract": "Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10403.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10403",
    "published": "2026-01-15T13:55:38Z",
    "updated": "2026-01-15T13:55:38Z",
    "comment": "Code: https://github.com/hasanmohsin/discrete_fkc",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10402",
    "title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering",
    "authors": [
      "Xinyu Zhu",
      "Yuzhu Cai",
      "Zexi Liu",
      "Bingyang Zheng",
      "Cheng Wang",
      "Rui Ye",
      "Jiaao Chen",
      "Hanrui Wang",
      "Wei-Chen Wang",
      "Yuzhi Zhang",
      "Linfeng Zhang",
      "Weinan E",
      "Di Jin",
      "Siheng Chen"
    ],
    "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10402.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10402",
    "published": "2026-01-15T13:52:04Z",
    "updated": "2026-01-15T13:52:04Z",
    "comment": "26 pages. 5 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10398",
    "title": "LatentRefusal: Latent-Signal Refusal for Unanswerable Text-to-SQL Queries",
    "authors": [
      "Xuancheng Ren",
      "Shijing Hu",
      "Zhihui Lu",
      "Jiangqi Huang",
      "Qiang Duan"
    ],
    "abstract": "In LLM-based text-to-SQL systems, unanswerable and underspecified user queries may generate not only incorrect text but also executable programs that yield misleading results or violate safety constraints, posing a major barrier to safe deployment. Existing refusal strategies for such queries either rely on output-level instruction following, which is brittle due to model hallucinations, or estimate output uncertainty, which adds complexity and overhead. To address this challenge, we formalize safe refusal in text-to-SQL systems as an answerability-gating problem and propose LatentRefusal, a latent-signal refusal mechanism that predicts query answerability from intermediate hidden activations of a large language model. We introduce the Tri-Residual Gated Encoder, a lightweight probing architecture, to suppress schema noise and amplify sparse, localized cues of question-schema mismatch that indicate unanswerability. Extensive empirical evaluations across diverse ambiguous and unanswerable settings, together with ablation studies and interpretability analyses, demonstrate the effectiveness of the proposed approach and show that LatentRefusal provides an attachable and efficient safety layer for text-to-SQL systems. Across four benchmarks, LatentRefusal improves average F1 to 88.5 percent on both backbones while adding approximately 2 milliseconds of probe overhead.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10398.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10398",
    "published": "2026-01-15T13:48:22Z",
    "updated": "2026-01-15T13:48:22Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10392",
    "title": "Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy",
    "authors": [
      "Hassan Eshkiki",
      "Sarah Costa",
      "Mostafa Mohammadpour",
      "Farinaz Tanhaei",
      "Christopher H. George",
      "Fabio Caraffini"
    ],
    "abstract": "Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10392.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10392",
    "published": "2026-01-15T13:44:49Z",
    "updated": "2026-01-15T13:44:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10388",
    "title": "INDIC DIALECT: A Multi Task Benchmark to Evaluate and Translate in Indian Language Dialects",
    "authors": [
      "Tarun Sharma",
      "Manikandan Ravikiran",
      "Sourava Kumar Behera",
      "Pramit Bhattacharya",
      "Arnab Bhattacharya",
      "Rohit Saluja"
    ],
    "abstract": "Recent NLP advances focus primarily on standardized languages, leaving most low-resource dialects under-served especially in Indian scenarios. In India, the issue is particularly important: despite Hindi being the third most spoken language globally (over 600 million speakers), its numerous dialects remain underrepresented. The situation is similar for Odia, which has around 45 million speakers. While some datasets exist which contain standard Hindi and Odia languages, their regional dialects have almost no web presence. We introduce INDIC-DIALECT, a human-curated parallel corpus of 13k sentence pairs spanning 11 dialects and 2 languages: Hindi and Odia. Using this corpus, we construct a multi-task benchmark with three tasks: dialect classification, multiple-choice question (MCQ) answering, and machine translation (MT). Our experiments show that LLMs like GPT-4o and Gemini 2.5 perform poorly on the classification task. While fine-tuned transformer based models pretrained on Indian languages substantially improve performance e.g., improving F1 from 19.6\\% to 89.8\\% on dialect classification. For dialect to language translation, we find that hybrid AI model achieves highest BLEU score of 61.32 compared to the baseline score of 23.36. Interestingly, due to complexity in generating dialect sentences, we observe that for language to dialect translation the ``rule-based followed by AI\" approach achieves best BLEU score of 48.44 compared to the baseline score of 27.59. INDIC-DIALECT thus is a new benchmark for dialect-aware Indic NLP, and we plan to release it as open source to support further work on low-resource Indian dialects.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10388.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10388",
    "published": "2026-01-15T13:40:27Z",
    "updated": "2026-01-15T13:40:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10387",
    "title": "The Assistant Axis: Situating and Stabilizing the Default Persona of Language Models",
    "authors": [
      "Christina Lu",
      "Jack Gallagher",
      "Jonathan Michala",
      "Kyle Fish",
      "Jack Lindsey"
    ],
    "abstract": "Large language models can represent a variety of personas but typically default to a helpful Assistant identity cultivated during post-training. We investigate the structure of the space of model personas by extracting activation directions corresponding to diverse character archetypes. Across several different models, we find that the leading component of this persona space is an \"Assistant Axis,\" which captures the extent to which a model is operating in its default Assistant mode. Steering towards the Assistant direction reinforces helpful and harmless behavior; steering away increases the model's tendency to identify as other entities. Moreover, steering away with more extreme values often induces a mystical, theatrical speaking style. We find this axis is also present in pre-trained models, where it primarily promotes helpful human archetypes like consultants and coaches and inhibits spiritual ones. Measuring deviations along the Assistant Axis predicts \"persona drift,\" a phenomenon where models slip into exhibiting harmful or bizarre behaviors that are uncharacteristic of their typical persona. We find that persona drift is often driven by conversations demanding meta-reflection on the model's processes or featuring emotionally vulnerable users. We show that restricting activations to a fixed region along the Assistant Axis can stabilize model behavior in these scenarios -- and also in the face of adversarial persona-based jailbreaks. Our results suggest that post-training steers models toward a particular region of persona space but only loosely tethers them to it, motivating work on training and steering strategies that more deeply anchor models to a coherent persona.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10387.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10387",
    "published": "2026-01-15T13:40:06Z",
    "updated": "2026-01-15T13:40:06Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10386",
    "title": "Handling Missing Modalities in Multimodal Survival Prediction for Non-Small Cell Lung Cancer",
    "authors": [
      "Filippo Ruffini",
      "Camillo Maria Caruso",
      "Claudia Tacconi",
      "Lorenzo Nibid",
      "Francesca Miccolis",
      "Marta Lovino",
      "Carlo Greco",
      "Edy Ippolito",
      "Michele Fiore",
      "Alessio Cortellini",
      "Bruno Beomonte Zobel",
      "Giuseppe Perrone",
      "Bruno Vincenzi",
      "Claudio Marrocco",
      "Alessandro Bria",
      "Elisa Ficarra",
      "Sara Ramella",
      "Valerio Guarrasi",
      "Paolo Soda"
    ],
    "abstract": "Accurate survival prediction in Non-Small Cell Lung Cancer (NSCLC) requires the integration of heterogeneous clinical, radiological, and histopathological information. While Multimodal Deep Learning (MDL) offers a promises for precision prognosis and survival prediction, its clinical applicability is severely limited by small cohort sizes and the presence of missing modalities, often forcing complete-case filtering or aggressive imputation. In this work, we present a missing-aware multimodal survival framework that integrates Computed Tomography (CT), Whole-Slide Histopathology (WSI) Images, and structured clinical variables for overall survival modeling in unresectable stage II-III NSCLC. By leveraging Foundation Models (FM) for modality-specific feature extraction and a missing-aware encoding strategy, the proposed approach enables intermediate multimodal fusion under naturally incomplete modality profiles. The proposed architecture is resilient to missing modalities by design, allowing the model to utilize all available data without being forced to drop patients during training or inference. Experimental results demonstrate that intermediate fusion consistently outperforms unimodal baselines as well as early and late fusion strategies, with the strongest performance achieved by the fusion of WSI and clinical modalities (73.30 C-index). Further analyses of modality importance reveal an adaptive behavior in which less informative modalities, i.e., CT modality, are automatically down-weighted and contribute less to the final survival prediction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10386.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10386",
    "published": "2026-01-15T13:38:19Z",
    "updated": "2026-01-15T13:38:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10378",
    "title": "Global Context Compression with Interleaved Vision-Text Transformation",
    "authors": [
      "Dian Jiao",
      "Jiaxin Duan",
      "Shuai Zhao",
      "Jiabing Leng",
      "Yiran Zhang",
      "Feng Huang"
    ],
    "abstract": "Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10378.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10378",
    "published": "2026-01-15T13:29:16Z",
    "updated": "2026-01-15T13:29:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10373",
    "title": "Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement",
    "authors": [
      "Yichong Xia",
      "Yimin Zhou",
      "Jinpeng Wang",
      "Bin Chen"
    ],
    "abstract": "Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \\textbf{Diff}usion-based Image Compression via \\textbf{C}onsistency Prior \\textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $ε$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \\textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\\% BD-rate (LPIPS) and 65.1\\% BD-rate (PSNR)) and over $10\\times$ speed-up compared to SOTA diffusion-based compression baselines.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10373.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10373",
    "published": "2026-01-15T13:25:25Z",
    "updated": "2026-01-15T13:25:25Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10369",
    "title": "Fine-Grained Human Pose Editing Assessment via Layer-Selective MLLMs",
    "authors": [
      "Ningyu Sun",
      "Zhaolin Cai",
      "Zitong Xu",
      "Peihang Chen",
      "Huiyu Duan",
      "Yichao Yan",
      "Xiongkuo Min",
      "Xiaokang Yang"
    ],
    "abstract": "Text-guided human pose editing has gained significant traction in AIGC applications. However,it remains plagued by structural anomalies and generative artifacts. Existing evaluation metrics often isolate authenticity detection from quality assessment, failing to provide fine-grained insights into pose-specific inconsistencies. To address these limitations, we introduce HPE-Bench, a specialized benchmark comprising 1,700 standardized samples from 17 state-of-the-art editing models, offering both authenticity labels and multi-dimensional quality scores. Furthermore, we propose a unified framework based on layer-selective multimodal large language models (MLLMs). By employing contrastive LoRA tuning and a novel layer sensitivity analysis (LSA) mechanism, we identify the optimal feature layer for pose evaluation. Our framework achieves superior performance in both authenticity detection and multi-dimensional quality regression, effectively bridging the gap between forensic detection and quality assessment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10369.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10369",
    "published": "2026-01-15T13:22:07Z",
    "updated": "2026-01-15T13:22:07Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10358",
    "title": "PLGC: Pseudo-Labeled Graph Condensation",
    "authors": [
      "Jay Nandy",
      "Arnab Kumar Mondal",
      "Anuj Rathore",
      "Mahesh Chandran"
    ],
    "abstract": "Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10358.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10358",
    "published": "2026-01-15T13:02:31Z",
    "updated": "2026-01-15T13:02:31Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10356",
    "title": "EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography",
    "authors": [
      "Mesut Ceylan",
      "Alexis Tabin",
      "Patrick Langer",
      "Elgar Fleisch",
      "Filipe Barata"
    ],
    "abstract": "Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these \"what-if\" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10356.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10356",
    "published": "2026-01-15T12:58:59Z",
    "updated": "2026-01-15T12:58:59Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10355",
    "title": "Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text",
    "authors": [
      "Zhihao Xu",
      "Rumei Li",
      "Jiahuan Li",
      "Rongxiang Weng",
      "Jingang Wang",
      "Xunliang Cai",
      "Xiting Wang"
    ],
    "abstract": "Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10355.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10355",
    "published": "2026-01-15T12:58:46Z",
    "updated": "2026-01-15T12:58:46Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10349",
    "title": "SuS: Strategy-aware Surprise for Intrinsic Exploration",
    "authors": [
      "Mark Kashirskiy",
      "Ilya Makarov"
    ],
    "abstract": "We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10349.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10349",
    "published": "2026-01-15T12:48:59Z",
    "updated": "2026-01-15T12:48:59Z",
    "comment": "8 pages, 7 figures, 3 tables. Code available at https://github.com/mariklolik/sus",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10348",
    "title": "Training-Trajectory-Aware Token Selection",
    "authors": [
      "Zhanming Shen",
      "Jiaqi Hu",
      "Zeyu Qin",
      "Hao Chen",
      "Wentao Ye",
      "Zenan Huang",
      "Yihong Zhuang",
      "Guoshan Lu",
      "Junlin Zhou",
      "Junbo Zhao"
    ],
    "abstract": "Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10348.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10348",
    "published": "2026-01-15T12:45:05Z",
    "updated": "2026-01-15T12:45:05Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10343",
    "title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding",
    "authors": [
      "Deming Ding",
      "Shichun Liu",
      "Enhui Yang",
      "Jiahang Lin",
      "Ziying Chen",
      "Shihan Dou",
      "Honglin Guo",
      "Weiyu Cheng",
      "Pengyu Zhao",
      "Chengjun Xiao",
      "Qunhong Zeng",
      "Qi Zhang",
      "Xuanjing Huang",
      "Qidi Xu",
      "Tao Gui"
    ],
    "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10343.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10343",
    "published": "2026-01-15T12:36:08Z",
    "updated": "2026-01-15T12:36:08Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10342",
    "title": "C-GRASP: Clinically-Grounded Reasoning for Affective Signal Processing",
    "authors": [
      "Cheng Lin Cheng",
      "Ting Chuan Lin",
      "Chai Kai Chang"
    ],
    "abstract": "Heart rate variability (HRV) is a pivotal noninvasive marker for autonomic monitoring; however, applying Large Language Models (LLMs) to HRV interpretation is hindered by physiological hallucinations. These include respiratory sinus arrhythmia (RSA) contamination, short-data instability in nonlinear metrics, and the neglect of individualized baselines in favor of population norms. We propose C-GRASP (Clinically-Grounded Reasoning for Affective Signal Processing), a guardrailed RAG-enhanced pipeline that decomposes HRV interpretation into eight traceable reasoning steps. Central to C-GRASP is a Z-score Priority Hierarchy that enforces the weighting of individualized baseline shifts over normative statistics. The system effectively mitigates spectral hallucinations through automated RSA-aware guardrails, preventing contamination of frequency-domain indices. Evaluated on 414 trials from the DREAMER dataset, C-GRASP integrated with high-scale reasoning models (e.g., MedGemma3-thinking) achieved superior performance in 4-class emotion classification (37.3% accuracy) and a Clinical Reasoning Consistency (CRC) score of 69.6%. Ablation studies confirm that the individualized Delta Z-score module serves as the critical logical anchor, preventing the \"population bias\" common in native LLMs. Ultimately, C-GRASP transitions affective computing from black-box classification to transparent, evidence-based clinical decision support, paving the way for safer AI integration in biomedical engineering.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10342.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10342",
    "published": "2026-01-15T12:35:35Z",
    "updated": "2026-01-15T12:35:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.10334",
    "title": "An analytic theory of convolutional neural network inverse problems solvers",
    "authors": [
      "Minh Hai Nguyen",
      "Quoc Bao Do",
      "Edouard Pauwels",
      "Pierre Weiss"
    ],
    "abstract": "Supervised convolutional neural networks (CNNs) are widely used to solve imaging inverse problems, achieving state-of-the-art performance in numerous applications. However, despite their empirical success, these methods are poorly understood from a theoretical perspective and often treated as black boxes. To bridge this gap, we analyze trained neural networks through the lens of the Minimum Mean Square Error (MMSE) estimator, incorporating functional constraints that capture two fundamental inductive biases of CNNs: translation equivariance and locality via finite receptive fields. Under the empirical training distribution, we derive an analytic, interpretable, and tractable formula for this constrained variant, termed Local-Equivariant MMSE (LE-MMSE). Through extensive numerical experiments across various inverse problems (denoising, inpainting, deconvolution), datasets (FFHQ, CIFAR-10, FashionMNIST), and architectures (U-Net, ResNet, PatchMLP), we demonstrate that our theory matches the neural networks outputs (PSNR $\\gtrsim25$dB). Furthermore, we provide insights into the differences between \\emph{physics-aware} and \\emph{physics-agnostic} estimators, the impact of high-density regions in the training (patch) distribution, and the influence of other factors (dataset size, patch size, etc).",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10334.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10334",
    "published": "2026-01-15T12:25:59Z",
    "updated": "2026-01-15T12:25:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过最小均方误差估计器视角，结合卷积神经网络的局部性和平移等变性归纳偏置，提出了一个解析理论来解释监督CNN在成像逆问题求解中的行为。",
      "motivation": "监督卷积神经网络（CNNs）在成像逆问题（如去噪、修复）中取得了显著成效，但现有方法在理论层面理解不足，常被视为黑盒。这阻碍了对模型工作机制的深入分析和优化，导致理论发展滞后于实证应用。因此，研究旨在弥合经验成功与理论空白，通过建立解析框架来理解CNN的归纳偏置（如平移等变性和局部性），从而促进更有效的算法设计和理论进展。摘要未明确说明所有现有方法的具体不足，但强调了理论理解的重要性。",
      "method": "论文采用最小均方误差（MMSE）估计器作为分析基础，结合功能约束来模拟CNN的归纳偏置，包括平移等变性和通过有限感受野实现的局部性。在经验训练分布下，推导出局部等变MMSE（LE-MMSE）公式，该公式具有解析性、可解释性和可处理性。通过将CNN行为与经典估计器联系起来，研究提供了一个理论框架来深入分析监督学习在逆问题中的应用，无需依赖复杂实验设置。摘要未详细说明具体数据集或模型架构在方法中的角色，但提及了用于实验的组件。",
      "result": "通过广泛数值实验，论文在多种逆问题（如去噪、修复、解卷积）、数据集（FFHQ、CIFAR-10、FashionMNIST）和网络架构（U-Net、ResNet、PatchMLP）上验证了LE-MMSE理论。结果表明，理论输出与CNN输出高度一致，峰值信噪比（PSNR）达到约25dB以上，展示了理论的准确性。此外，研究提供了对物理感知与物理无关估计器差异、训练分布中高密度区域影响以及其他因素（如数据集大小）的见解，深化了对CNN行为的理解。摘要未提供与基线方法的详细对比数据，但强调了匹配性能。",
      "conclusion": "本研究提出了一种解析理论框架，结合最小均方误差估计器和CNN归纳偏置，成功解释了监督卷积神经网络在逆问题求解中的行为，填补了理论理解的空白。学术价值在于提供了可解释的分析工具，推动CNN理论发展；实际应用价值在于可能指导逆问题求解算法的优化和改进。未来工作可探索更多归纳偏置的整合或扩展到其他机器学习任务。摘要未明确说明局限性，但基于内容推断，可能包括理论在更复杂场景下的适用性或进一步实验验证。",
      "tags": [
        "Convolutional Neural Networks",
        "Inverse Problems",
        "Minimum Mean Square Error",
        "Translation Equivariance",
        "Locality"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:18.354901Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10332",
    "title": "Think-Then-Generate: Reasoning-Aware Text-to-Image Diffusion with LLM Encoders",
    "authors": [
      "Siqi Kou",
      "Jiachun Jin",
      "Zetong Zhou",
      "Ye Ma",
      "Yugang Wang",
      "Quan Chen",
      "Peng Jiang",
      "Xiao Yang",
      "Jun Zhu",
      "Kai Yu",
      "Zhijie Deng"
    ],
    "abstract": "Recent progress in text-to-image (T2I) diffusion models (DMs) has enabled high-quality visual synthesis from diverse textual prompts. Yet, most existing T2I DMs, even those equipped with large language model (LLM)-based text encoders, remain text-pixel mappers -- they employ LLMs merely as text encoders, without leveraging their inherent reasoning capabilities to infer what should be visually depicted given the textual prompt. To move beyond such literal generation, we propose the think-then-generate (T2G) paradigm, where the LLM-based text encoder is encouraged to reason about and rewrite raw user prompts; the states of the rewritten prompts then serve as diffusion conditioning. To achieve this, we first activate the think-then-rewrite pattern of the LLM encoder with a lightweight supervised fine-tuning process. Subsequently, the LLM encoder and diffusion backbone are co-optimized to ensure faithful reasoning about the context and accurate rendering of the semantics via Dual-GRPO. In particular, the text encoder is reinforced using image-grounded rewards to infer and recall world knowledge, while the diffusion backbone is pushed to produce semantically consistent and visually coherent images. Experiments show substantial improvements in factual consistency, semantic alignment, and visual realism across reasoning-based image generation and editing benchmarks, achieving 0.79 on WISE score, nearly on par with GPT-4. Our results constitute a promising step toward next-generation unified models with reasoning, expression, and demonstration capacities.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10332.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10332",
    "published": "2026-01-15T12:19:05Z",
    "updated": "2026-01-15T12:19:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了think-then-generate（T2G）范式，通过激活大语言模型（LLM）编码器的推理能力来重写文本提示，从而改进文本到图像扩散模型的生成质量。",
      "motivation": "当前文本到图像（T2I）扩散模型，即使集成大语言模型（LLM）作为文本编码器，仍主要作为文本-像素映射器，未充分利用LLM的推理能力来推断文本提示的视觉含义。这导致生成图像可能缺乏语义一致性和视觉真实性，尤其在处理复杂或隐含上下文时。现有方法的不足在于依赖字面翻译，限制了图像生成的准确性和多样性，因此需要引入推理机制来提升整体性能，解决语义鸿沟问题。",
      "method": "论文提出think-then-generate（T2G）范式，核心包括两个步骤：首先，通过轻量级监督微调激活LLM编码器的思考-重写模式，使其推理并重写原始用户提示为更适合图像生成的形式；其次，使用Dual-GRPO方法共同优化LLM编码器和扩散主干模型，其中LLM编码器通过图像基础奖励进行强化学习以推断和回忆世界知识，而扩散主干则被优化以生成语义一致和视觉连贯的图像，不指定具体数据集但应用于推理基础的基准。",
      "result": "实验结果显示，在推理基础的图像生成和编辑基准测试中，该方法在事实一致性、语义对齐和视觉真实性方面取得显著改进，具体WISE得分达到0.79，接近GPT-4的水平。这表明通过利用LLM推理能力，性能有实质性提升，尽管摘要未明确说明具体基线对比数据，但强调了相对于传统方法的优势，验证了T2G范式的有效性。",
      "conclusion": "本文的核心贡献是提出了T2G范式，将LLM的推理能力融入文本到图像生成，提升了语义对齐和视觉质量，为开发具有推理、表达和演示能力的下一代统一模型迈出重要一步。研究的学术价值在于扩展了多模态生成模型的交互方式，实际应用潜力包括改进创意设计和内容创作，未来工作可进一步优化推理算法或扩展到视频生成，但摘要未明确说明具体局限性。",
      "tags": [
        "Text-to-Image Generation",
        "Diffusion Models",
        "Large Language Models",
        "Reinforcement Learning",
        "Semantic Reasoning"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:52.503874Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10328",
    "title": "Meta Dynamic Graph for Traffic Flow Prediction",
    "authors": [
      "Yiqing Zou",
      "Hanning Yuan",
      "Qianyu Yang",
      "Ziqiang Yuan",
      "Shuliang Wang",
      "Sijie Ruan"
    ],
    "abstract": "Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10328.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10328",
    "published": "2026-01-15T12:15:54Z",
    "updated": "2026-01-15T12:15:54Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "提出Meta Dynamic Graph (MetaDG)框架，通过动态图结构统一建模时空动力学和异质性，以提升交通流预测的精度。",
      "motivation": "交通流预测作为典型的时空预测问题，核心挑战在于建模复杂的时空依赖性。现有方法常分离处理空间和时间依赖性，这阻碍了时空相关性的有效建模；尽管动态建模和时空异质性处理有所进展，但仍存在两大局限性：动态建模常限于空间拓扑变化（如邻接矩阵更新），而未扩展到更广泛的范围；时空异质性建模在空间和时间维度上相互分离，缺乏统一处理。这些问题限制了模型的适应性和性能，因此需要创新框架来综合解决动态与异质性挑战。",
      "method": "MetaDG框架利用节点表示的动态图结构来显式建模时空动力学。关键创新包括生成动态邻接矩阵和元参数，这不仅将动态建模扩展到了超越空间拓扑变化的范围，还将时空异质性捕获统一到单个维度。该框架通过节点表示的演化来反映交通流的复杂变化，从而增强模型灵活性。摘要未详细说明模型架构的具体组件或数据预处理步骤，但提到在四个真实数据集上进行实验验证。",
      "result": "在四个真实世界数据集上进行的大量实验验证了MetaDG的有效性，表明其在交通流预测任务中具有优势。摘要未明确提供具体的性能指标（如准确率提升或效率改进百分比），也未详述与基线方法的直接对比数据，但实验结果表明该框架能够有效应对动态和异质性挑战，从而可能提升预测精度。这间接支持了其在复杂时空依赖性建模方面的改进。",
      "conclusion": "MetaDG的主要贡献是提出一个统一框架，解决了动态建模局限性和时空异质性分离问题，通过动态图结构扩展了建模范围并整合了异质性捕获。该研究具有重要学术价值，推动了时空预测模型的发展，特别是通过动态图理论增强了灵活性和表达力。在实际应用中，它可能提升智能交通系统的预测能力，优化交通管理。未来工作方向可能包括模型架构的进一步优化或扩展到其他时空预测任务，但摘要未明确说明潜在局限性。",
      "tags": [
        "Traffic Flow Prediction",
        "Dynamic Graph",
        "Spatio-temporal Modeling",
        "Meta-parameters",
        "Node Representations"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:51.204527Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10324",
    "title": "SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition",
    "authors": [
      "Yiming Zhang",
      "Weibo Qin",
      "Yuntian Liu",
      "Feng Wang"
    ],
    "abstract": "Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10324.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10324",
    "published": "2026-01-15T12:09:49Z",
    "updated": "2026-01-15T12:09:49Z",
    "comment": "5 pages, 4 figures",
    "light_analysis": {
      "overview": "提出SRAW攻击方法，通过优化的空间变形和前景/背景区域重新加权，以平衡隐蔽性和有效性来攻击SAR目标识别系统。",
      "motivation": "合成孔径雷达图像由于独特的电磁散射机制具有内在的信息稀疏性，导致基于深度神经网络的SAR自动目标识别系统易受对抗样本攻击，且倾向于过度依赖背景区域，从而降低对抗鲁棒性。现有攻击方法通常需要视觉可感知的扭曲才能生效，缺乏隐蔽性，这在实际应用中可能限制攻击的实用性。因此，迫切需要开发一种既有效又隐蔽的攻击方法，以解决现有方法在平衡攻击性能和隐蔽性方面的不足，提高对SAR-ATR系统安全性的评估能力。",
      "method": "论文提出SRAW攻击方法，该方法通过优化的空间变形生成对抗样本，并对前景和背景区域分配不同的扰动预算进行重新加权，以在保持不可感知性的同时最大化攻击效果。关键创新在于根据区域重要性优化空间变形，减少对背景区域的过度依赖，从而生成更自然的对抗样本。摘要未明确说明具体使用的数据集、模型架构或优化算法，但推断该方法可能基于对抗性优化技术来调整变形参数，以提升攻击的鲁棒性和隐蔽性。",
      "result": "大量实验表明，SRAW攻击能显著降低先进SAR-ATR模型的性能，并在不可感知性和对抗迁移性方面一致优于现有攻击方法。摘要未提供具体性能指标如准确率下降幅度或攻击成功率，但强调了其效果和优势。与基线方法对比，SRAW在保持较低视觉失真度的同时，展现出更强的攻击能力和更好的隐蔽性，证明其在平衡攻击有效性和迁移性方面的优越性，适用于多种SAR-ATR模型。",
      "conclusion": "SRAW攻击方法通过空间变形和区域重新加权，有效解决了SAR目标识别系统中对抗攻击的隐蔽性与有效性问题，为对抗鲁棒性研究提供了新工具。学术价值在于提出了一种针对信息稀疏图像的创新攻击策略，丰富了对抗攻击方法学；实际应用价值则有助于评估和提升SAR识别系统的安全性。未来工作可探索更多样化的变形策略、扩展到其他图像模态，或开发对应的防御方法以增强系统鲁棒性。",
      "tags": [
        "Adversarial Attack",
        "SAR Target Recognition",
        "Spatial Deformation",
        "Imperceptibility",
        "Transferability"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:09.106997Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10323",
    "title": "ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding",
    "authors": [
      "Xueyun Tian",
      "Wei Li",
      "Bingbing Xu",
      "Heng Dong",
      "Yuanzhuo Wang",
      "Huawei Shen"
    ],
    "abstract": "Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10323.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10323",
    "published": "2026-01-15T12:09:04Z",
    "updated": "2026-01-15T12:09:04Z",
    "comment": "Our project page is available at https://eureka-maggie.github.io/ROMA_show",
    "light_analysis": {
      "overview": "ROMA提出了一个实时全多模态助手，实现统一反应式和主动式交互的流式音频-视频理解。",
      "motivation": "本研究旨在解决流式音频-视频理解的挑战，现有全多模态大语言模型虽在统一建模方面有前景，但现有方法常存在能力脱节，如支持模态不完整或缺乏自主主动监控，导致实时交互受限，影响应用效果。该问题重要，因为随着多模态AI发展，实时、统一的交互助手在监控、人机交互等领域需求增长。",
      "method": "ROMA通过将连续输入处理为同步多模态单元，对齐密集音频与离散视频帧，以解决粒度不匹配问题；引入轻量级说话头，解耦响应启动与生成，确保精确触发无任务冲突；训练使用定制流式数据集和两阶段课程学习，逐步优化流式格式适应和主动响应性。关键创新在于实时流式处理和多模态对齐技术。",
      "result": "在12个基准测试中的广泛实验显示，ROMA在主动任务（如警报、叙述）上达到最先进性能，在反应式设置（如问答）中具有竞争力，验证了其在统一实时全多模态理解中的鲁棒性。与基线方法相比，ROMA展示了显著改进，但具体数据摘要未明确说明。",
      "conclusion": "论文主要贡献是提出了ROMA，一个实时全多模态助手，实现了统一反应式和主动式交互的流式音频-视频理解；学术价值在于推进了多模态流式处理技术，填补现有方法不足；实际应用价值广泛，如智能监控和实时助手；局限性或未来方向包括进一步优化效率和扩展到更多模态，摘要未明确说明。",
      "tags": [
        "Omni-Multimodal Large Language Models",
        "Streaming Audio-Video Understanding",
        "Synchronized Multimodal Units",
        "Lightweight Speak Head",
        "Curriculum Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:42.220934Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10321",
    "title": "An Efficient Long-Context Ranking Architecture With Calibrated LLM Distillation: Application to Person-Job Fit",
    "authors": [
      "Warren Jouanneau",
      "Emma Jouffroy",
      "Marc Palyart"
    ],
    "abstract": "Finding the most relevant person for a job proposal in real time is challenging, especially when resumes are long, structured, and multilingual. In this paper, we propose a re-ranking model based on a new generation of late cross-attention architecture, that decomposes both resumes and project briefs to efficiently handle long-context inputs with minimal computational overhead. To mitigate historical data biases, we use a generative large language model (LLM) as a teacher, generating fine-grained, semantically grounded supervision. This signal is distilled into our student model via an enriched distillation loss function. The resulting model produces skill-fit scores that enable consistent and interpretable person-job matching. Experiments on relevance, ranking, and calibration metrics demonstrate that our approach outperforms state-of-the-art baselines.",
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10321.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10321",
    "published": "2026-01-15T12:01:27Z",
    "updated": "2026-01-15T12:01:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种基于晚交叉注意力架构和LLM蒸馏的高效长上下文重排序模型，用于提升人岗匹配的准确性和可解释性。",
      "motivation": "在实时人岗匹配中，简历通常较长、结构化且多语言，这导致处理长上下文输入时计算开销大，且现有方法可能存在历史数据偏见，影响匹配的公平性和准确性。该研究旨在解决这一问题，通过开发高效、偏差较小的模型来提升实时招聘中的匹配质量，从而应对复杂简历信息带来的挑战，减少偏见并增强可解释性。",
      "method": "论文提出一种重排序模型，采用新一代晚交叉注意力架构，通过分解简历和项目简介来高效处理长上下文输入，最小化计算开销。关键创新是利用生成式大型语言模型作为教师，生成细粒度的语义监督信号，并通过优化的蒸馏损失函数将知识蒸馏到学生模型中，产生技能匹配分数，实现一致的人岗匹配。",
      "result": "实验在相关性、排序和校准等多个指标上评估，结果表明该方法优于当前最先进的基线模型。模型输出的技能匹配分数不仅提高了匹配准确性，还增强了匹配的一致性和可解释性，尽管摘要未提供具体数值细节，但整体性能表现优异。",
      "conclusion": "本研究的主要贡献在于提出了一种结合晚交叉注意力架构和LLM蒸馏的高效模型，显著改善了人岗匹配的性能。学术上推动了长上下文处理和知识蒸馏技术的发展，实际应用中提升了招聘系统的实时性和公平性。摘要未明确说明局限性，未来工作可进一步优化模型效率或扩展到其他类似任务。",
      "tags": [
        "Late Cross-Attention",
        "LLM Distillation",
        "Re-ranking Model",
        "Long-Context Processing",
        "Person-Job Fit"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:10.800773Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10318",
    "title": "Boundary-Aware NL2SQL: Integrating Reliability through Hybrid Reward and Data Synthesis",
    "authors": [
      "Songsong Tian",
      "Kongsheng Zhuo",
      "Zhendong Wang",
      "Rong Shen",
      "Shengtao Zhang",
      "Yong Wu"
    ],
    "abstract": "In this paper, we present BAR-SQL (Boundary-Aware Reliable NL2SQL), a unified training framework that embeds reliability and boundary awareness directly into the generation process. We introduce a Seed Mutation data synthesis paradigm that constructs a representative enterprise corpus, explicitly encompassing multi-step analytical queries alongside boundary cases including ambiguity and schema limitations. To ensure interpretability, we employ Knowledge-Grounded Reasoning Synthesis, which produces Chain-of-Thought traces explicitly anchored in schema metadata and business rules. The model is trained through a two-stage process: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning via Group Relative Policy Optimization. We design a Task-Conditioned Hybrid Reward mechanism that simultaneously optimizes SQL execution accuracy-leveraging Abstract Syntax Tree analysis and dense result matching-and semantic precision in abstention responses. To evaluate reliability alongside generation accuracy, we construct and release Ent-SQL-Bench, which jointly assesse SQL precision and boundary-aware abstention across ambiguous and unanswerable queries. Experimental results on this benchmark demonstrate that BAR-SQL achieves 91.48% average accuracy, outperforming leading proprietary models, including Claude 4.5 Sonnet and GPT-5, in both SQL generation quality and boundary-aware abstention capability. The source code and benchmark are available anonymously at: https://github.com/TianSongS/BAR-SQL.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10318.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10318",
    "published": "2026-01-15T11:55:01Z",
    "updated": "2026-01-15T11:55:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出BAR-SQL框架，通过混合奖励和数据合成将可靠性和边界意识嵌入自然语言转SQL生成过程，提高可靠性和准确性。",
      "motivation": "研究旨在解决NL2SQL系统中的可靠性问题，特别是在处理歧义查询和模式限制等边界情况时。现有方法可能缺乏对这类情况的明确处理，导致生成错误或不完整的SQL，影响实际应用中的信任度。通过构建企业级语料库和评估基准，本研究强调了在复杂环境中确保SQL生成准确性和适时的拒绝回答的重要性，以提升系统在商业分析等场景的实用性。",
      "method": "方法包括Seed Mutation数据合成范式，构建包含多步分析查询和边界案例的企业语料库；使用Knowledge-Grounded Reasoning Synthesis生成基于模式元数据和业务规则的Chain-of-Thought痕迹以增强可解释性。训练采用两阶段过程：先进行监督微调，后通过Group Relative Policy Optimization进行强化学习。设计Task-Conditioned Hybrid Reward机制，结合Abstract Syntax Tree分析和密集结果匹配，同时优化SQL执行准确性和语义精度在拒绝响应中。",
      "result": "在自建的Ent-SQL-Bench基准上评估，BAR-SQL实现91.48%的平均准确率，在SQL生成质量和边界意识拒绝能力方面均超过Claude 4.5 Sonnet和GPT-5等领先专有模型。该框架在歧义和不可回答查询上表现出色，显示出在可靠性和准确性上的显著改进，具体数据支撑了其优于基线方法的性能。",
      "conclusion": "BAR-SQL通过整合可靠性和边界意识，提供了NL2SQL领域的新训练框架和基准，具有学术价值，推动了可靠性研究。开源代码和基准促进了实际应用，特别是在企业环境中处理复杂查询。局限性或未来方向摘要未明确说明，但可能包括扩展到更多边界案例或优化奖励机制。",
      "tags": [
        "NL2SQL",
        "Reinforcement Learning",
        "Data Synthesis",
        "Chain-of-Thought",
        "Abstract Syntax Tree"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:48.017132Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10315",
    "title": "ADVOSYNTH: A Synthetic Multi-Advocate Dataset for Speaker Identification in Courtroom Scenarios",
    "authors": [
      "Aniket Deroy"
    ],
    "abstract": "As large-scale speech-to-speech models achieve high fidelity, the distinction between synthetic voices in structured environments becomes a vital area of study. This paper introduces Advosynth-500, a specialized dataset comprising 100 synthetic speech files featuring 10 unique advocate identities. Using the Speech Llama Omni model, we simulate five distinct advocate pairs engaged in courtroom arguments. We define specific vocal characteristics for each advocate and present a speaker identification challenge to evaluate the ability of modern systems to map audio files to their respective synthetic origins.   Dataset is available at this link-https: //github.com/naturenurtureelite/ADVOSYNTH-500.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10315.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10315",
    "published": "2026-01-15T11:46:47Z",
    "updated": "2026-01-15T11:46:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文介绍了 Advosynth-500 合成语音数据集，专门用于法庭场景下的说话人识别研究。",
      "motivation": "随着大规模语音到语音模型实现高保真度，合成声音在结构化环境（如法庭）中的区分成为一个重要研究领域。现有研究缺乏专门针对合成语音的说话人识别数据集，尤其是在模拟真实法庭辩论场景时。因此，开发一个包含多个倡导者身份的数据集，以评估现代系统在区分合成声音方面的能力，对于提升语音处理技术的实用性和可靠性至关重要。",
      "method": "研究使用 Speech Llama Omni 模型生成 Advosynth-500 数据集，包含 100 个合成语音文件，涉及 10 个独特的倡导者身份。通过模拟五个不同的倡导者对在法庭辩论中的对话，定义了每个倡导者的特定声学特征。关键创新在于构建了一个结构化数据集，并设计了说话人识别挑战，以测试系统将音频文件映射到合成来源的能力，为后续评估提供基础。",
      "result": "摘要未明确说明具体的实验结果，如准确率或与基线方法的对比数据。论文仅提及提出了一个说话人识别挑战来评估现代系统的性能，但未提供具体指标。因此，主要效果是基于数据集的发布，为后续研究提供了一个测试平台，可用于验证合成声音的区分能力。",
      "conclusion": "本文的主要贡献是提出了 Advosynth-500 数据集，专注于合成语音在法庭场景下的说话人识别，填补了相关数据集的空白。该研究为语音处理领域提供了有价值的资源，促进了合成声音区分技术的发展，具有学术和应用价值。未来工作可包括使用该数据集进行更广泛的性能评估和模型改进。",
      "tags": [
        "Synthetic Speech",
        "Speaker Identification",
        "Courtroom Scenarios",
        "Speech Llama Omni Model",
        "Dataset"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:13.521445Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10313",
    "title": "Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models",
    "authors": [
      "Peng-Fei Zhang",
      "Zi Huang"
    ],
    "abstract": "Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10313.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10313",
    "published": "2026-01-15T11:45:56Z",
    "updated": "2026-01-15T11:45:56Z",
    "comment": "15 pages, 7 figures",
    "light_analysis": {
      "overview": "HRA是一种分层次精炼的通用多模态对抗攻击框架，针对视觉语言模型，提高了攻击效率和效果。",
      "motivation": "现有视觉语言模型（VLP）的对抗攻击方法大多是样本特定的，导致在扩展到大型数据集或新场景时计算开销巨大。这个问题的重要性在于，随着VLP模型在多个应用中的部署，高效的对抗攻击方法对评估模型鲁棒性和安全性至关重要。现有方法不足在于它们依赖于每个样本的独立优化，缺乏通用性，从而限制了在实际大规模应用中的可行性和实用性，增加了测试成本和时间消耗。",
      "method": "HRA框架在样本级别和优化级别精炼通用对抗扰动（UAPs）。对于图像模态，分离对抗样本为干净图像和扰动，并引入ScMix增强策略多样化视觉上下文，以减少对虚假特征的依赖；优化路径中，利用历史梯度和估计未来梯度的时间层次结构来避免局部极小值并稳定学习。文本模态方面，结合句子内和句子间重要性度量识别全局有影响的词作为通用扰动。关键创新点包括跨模态对齐的有效破坏和优化的稳定性提升。",
      "result": "摘要未明确说明具体实验数据，但论文指出在各种下游任务、VLP模型和数据集上的广泛实验证明了所提攻击方法的优越性。推断HRA在攻击效率和效果上优于现有的样本特定攻击方法，可能表现为更高的攻击成功率、更低的计算开销或更广泛的应用场景覆盖。与基线方法对比，HRA通过通用性设计减少了重复优化需求，从而提高了整体性能。",
      "conclusion": "HRA框架的主要贡献在于提出了一种高效的多模态通用对抗攻击方法，通过分层次精炼策略增强了攻击的通用性和稳定性。学术上，它推动了对抗攻击领域的发展，特别是在多模态模型安全评估中的应用。实际价值在于帮助更有效地测试VLP模型的鲁棒性，促进模型改进。未来工作可能包括进一步优化攻击策略、扩展到其他模态或处理更复杂场景的局限性。",
      "tags": [
        "Universal Adversarial Perturbations",
        "Multimodal Attacks",
        "Vision-Language Models",
        "ScMix Augmentation",
        "Gradient Optimization"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:52.958859Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10312",
    "title": "We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification",
    "authors": [
      "Zhipeng Liu",
      "Peibo Duan",
      "Xuan Tang",
      "Haodong Jing",
      "Mingyang Geng",
      "Yongsheng Huang",
      "Jialu Xu",
      "Bin Zhang",
      "Binwu Wang"
    ],
    "abstract": "The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10312.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10312",
    "published": "2026-01-15T11:44:07Z",
    "updated": "2026-01-15T11:44:07Z",
    "comment": "This paper has been accepted for publication at ACM WWW 2026",
    "light_analysis": {
      "overview": "论文提出了一种轻量级且鲁棒的双因果解缠框架DualCD，旨在增强时间序列分类模型在域增量学习场景中的稳健性。",
      "motivation": "时间序列分类在智能服务中至关重要，但现有方法在域增量学习中面临显著挑战，例如新领域数据分布变化导致模型性能下降。研究动机源于开发更鲁棒分类器的需求，以处理动态变化的领域环境。现有方法往往未能有效解缠和利用因果特征，易受伪特征干扰，影响模型的泛化能力和适应性，限制了在实际应用中的部署。",
      "method": "DualCD框架首先通过时间特征解缠模块，将时间序列数据分解为类因果特征和伪特征，以确保模型在域增量学习中依赖于具有预测力的因果特征。关键创新点是双因果干预机制，它消除类内和类间混杂特征的影响，通过构造变体样本结合当前类的因果特征与其他特征，并使用因果干预损失训练模型仅基于因果特征进行准确预测。这种方法可无缝集成到现有时间序列分类模型中，如深度学习架构，提升其在多数据集上的鲁棒性。",
      "result": "在多个时间序列数据集和模型上的广泛实验表明，DualCD能有效改善域增量学习场景下的分类性能，性能提升具体体现在准确率等指标上。作者将丰富实验汇总为全面的基准，以促进该领域的研究。虽然没有提供具体数据，但与基线方法相比，DualCD展现出更强的鲁棒性和适应性，验证了其在应对数据分布变化中的有效性。",
      "conclusion": "本研究的主要贡献是提出DualCD框架，通过因果解缠和干预机制，增强了时间序列分类在域增量学习中的鲁棒性。学术价值在于推动了因果学习在时间序列分析中的应用，实际应用价值在于支持智能服务中的自适应分类系统。摘要未明确说明局限性和未来工作方向，但该框架为后续研究提供了新思路和基准资源。",
      "tags": [
        "Domain-Incremental Learning",
        "Time Series Classification",
        "Causal Disentanglement",
        "Dual-Causal Intervention",
        "Robust Classifier"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:00.790200Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10310",
    "title": "Multilinguality as Sense Adaptation",
    "authors": [
      "Jan Christian Blaise Cruz",
      "David Ifeoluwa Adelani",
      "Alham Fikri Aji"
    ],
    "abstract": "We approach multilinguality as sense adaptation: aligning latent meaning representations across languages rather than relying solely on shared parameters and scale. In this paper, we introduce SENse-based Symmetric Interlingual Alignment (SENSIA), which adapts a Backpack language model from one language to another by explicitly aligning sense-level mixtures and contextual representations on parallel data, while jointly training a target-language language modeling loss to preserve fluency. Across benchmarks on four typologically diverse languages, SENSIA generally outperforms comparable multilingual alignment methods and achieves competitive accuracy against monolingual from-scratch baselines while using 2-4x less target-language data. Analyses of learned sense geometry indicate that local sense topology and global structure relative to English are largely preserved, and ablations show that the method is robust in terms of design and scale.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10310.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10310",
    "published": "2026-01-15T11:44:01Z",
    "updated": "2026-01-15T11:44:01Z",
    "comment": "Code available at https://github.com/jcblaisecruz02/sensia",
    "light_analysis": {
      "overview": "论文提出SENSIA方法，通过将多语言性视为感知适应，实现跨语言感知级表示的对齐，减少数据依赖并提升语言模型适应效率。",
      "motivation": "当前多语言对齐方法主要依赖共享参数和规模，可能导致资源浪费或语义偏差。本文旨在解决跨语言语义对齐问题，通过感知适应来更高效地适应语言模型到新语言，减少对大量目标语言数据的依赖，提升多语言自然语言处理的性能和准确性，现有方法在语义层面对齐不足。",
      "method": "SENSIA方法采用Backpack语言模型，在平行数据上对称对齐感知级混合和上下文表示，结合目标语言建模损失以保持流畅性。核心创新是明确对齐语义层面的感知表示，而非仅通过参数共享，从而优化跨语言模型适应过程。",
      "result": "在四个类型多样的语言基准测试中，SENSIA准确性优于可比的多语言对齐方法，与从头训练的单语言基线相比，使用2-4倍更少数据实现竞争性性能。分析显示感知几何结构被保留，消融实验证实方法在设计和规模上鲁棒。",
      "conclusion": "本研究贡献了SENSIA方法，推进多语言对齐技术，通过感知适应减少数据需求，具有学术和实际价值。未来可扩展到更多语言或应用场景，进一步提升通用性和优化潜力。",
      "tags": [
        "Sense Adaptation",
        "Multilingual Alignment",
        "Backpack Language Model",
        "Parallel Data",
        "Language Modeling Loss"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:42.326606Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10307",
    "title": "The Straight and Narrow: Do LLMs Possess an Internal Moral Path?",
    "authors": [
      "Luoming Hu",
      "Jingjie Zeng",
      "Liang Yang",
      "Hongfei Lin"
    ],
    "abstract": "Enhancing the moral alignment of Large Language Models (LLMs) is a critical challenge in AI safety. Current alignment techniques often act as superficial guardrails, leaving the intrinsic moral representations of LLMs largely untouched. In this paper, we bridge this gap by leveraging Moral Foundations Theory (MFT) to map and manipulate the fine-grained moral landscape of LLMs. Through cross-lingual linear probing, we validate the shared nature of moral representations in middle layers and uncover a shared yet different moral subspace between English and Chinese. Building upon this, we extract steerable Moral Vectors and successfully validate their efficacy at both internal and behavioral levels. Leveraging the high generalizability of morality, we propose Adaptive Moral Fusion (AMF), a dynamic inference-time intervention that synergizes probe detection with vector injection to tackle the safety-helpfulness trade-off. Empirical results confirm that our approach acts as a targeted intrinsic defense, effectively reducing incorrect refusals on benign queries while minimizing jailbreak success rates compared to standard baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10307.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10307",
    "published": "2026-01-15T11:42:00Z",
    "updated": "2026-01-15T11:42:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出基于道德基础理论的方法，通过映射和操控大型语言模型的内在道德表示，改善道德对齐并解决安全与帮助性之间的权衡。",
      "motivation": "增强大型语言模型（LLMs）的道德对齐是AI安全领域的关键挑战。当前对齐技术如护栏通常只提供表面控制，未能深入LLMs的内在道德表示，导致模型可能对良性查询做出错误拒绝或容易被越狱攻击。因此，需要开发内在影响LLMs道德行为的方法，以解决安全与帮助性的根本矛盾。",
      "method": "论文采用道德基础理论（MFT）映射LLMs的细粒度道德景观。通过跨语言线性探测，验证了中间层道德表示的共享性，并发现了英语和中文间的共享道德子空间。基于此，提取了可操控的道德向量，并在内部和行为层面验证其有效性。提出Adaptive Moral Fusion（AMF），一种动态推理时间干预，结合探测检测与向量注入，以优化安全与帮助性平衡，摘要未明确说明具体数据集或模型架构。",
      "result": "经验结果表明，该方法作为一种有针对性的内在防御，有效减少了模型对良性查询的错误拒绝，同时相比标准基线最小化了越狱成功率。实验结果验证了道德向量和AMF在改善模型行为上的效果，显示了道德对齐的显著提升，尽管摘要未提供具体性能指标数据。",
      "conclusion": "本研究的贡献在于开发了基于MFT的内在道德对齐方法，通过AMF动态干预解决了LLMs的安全与帮助性权衡。这深化了对LLMs道德表示的理解，具有学术价值，并为AI安全提供了实用工具。未来工作可探索更广泛的道德场景和模型泛化能力，以应对更复杂的对齐挑战。",
      "tags": [
        "Moral Foundations Theory",
        "Linear Probing",
        "Moral Vectors",
        "Adaptive Moral Fusion",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:49.480654Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10306",
    "title": "Evidence-Augmented Policy Optimization with Reward Co-Evolution for Long-Context Reasoning",
    "authors": [
      "Xin Guan",
      "Zijian Li",
      "Shen Huang",
      "Pengjun Xie",
      "Jingren Zhou",
      "Jiuxin Cao"
    ],
    "abstract": "While Reinforcement Learning (RL) has advanced LLM reasoning, applying it to long-context scenarios is hindered by sparsity of outcome rewards. This limitation fails to penalize ungrounded \"lucky guesses,\" leaving the critical process of needle-in-a-haystack evidence retrieval largely unsupervised. To address this, we propose EAPO (Evidence-Augmented Policy Optimization). We first establish the Evidence-Augmented Reasoning paradigm, validating via Tree-Structured Evidence Sampling that precise evidence extraction is the decisive bottleneck for long-context reasoning. Guided by this insight, EAPO introduces a specialized RL algorithm where a reward model computes a Group-Relative Evidence Reward, providing dense process supervision to explicitly improve evidence quality. To sustain accurate supervision throughout training, we further incorporate an Adaptive Reward-Policy Co-Evolution mechanism. This mechanism iteratively refines the reward model using outcome-consistent rollouts, sharpening its discriminative capability to ensure precise process guidance. Comprehensive evaluations across eight benchmarks demonstrate that EAPO significantly enhances long-context reasoning performance compared to SOTA baselines.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10306.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10306",
    "published": "2026-01-15T11:40:57Z",
    "updated": "2026-01-15T11:40:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 EAPO（证据增强策略优化）方法，通过奖励协同演化解决长上下文推理中的奖励稀疏问题，提升推理性能。",
      "motivation": "研究动机是强化学习在长上下文推理场景中面临奖励稀疏的挑战，导致无法有效监督证据检索过程，使得无根据的猜测未被惩罚。现有 RL 方法因奖励信号不足，难以精确指导证据提取，限制了推理的准确性和可靠性。这一问题在复杂文档分析或问答任务中尤为重要，因此需要开发能提供密集过程监督的方法来改善长上下文推理效果。",
      "method": "论文提出 EAPO 方法，首先建立证据增强推理范式，通过树结构证据采样验证证据提取是长上下文推理的关键瓶颈。核心算法使用奖励模型计算组相对证据奖励，为证据质量提供密集过程监督，以提升推理准确性。创新点包括自适应奖励-策略协同演化机制，迭代优化奖励模型以保持监督精确性，确保过程指导的有效性。该方法结合了专门 RL 算法和模型架构优化。",
      "result": "在八个基准测试上的综合评估显示，EAPO 显著提高了长上下文推理的性能，优于当前最先进的基线方法。具体性能指标如准确率提升未在摘要中明确说明，但实验结果表明 EAPO 在多个测试中表现出色，验证了其有效性和鲁棒性，解决了奖励稀疏导致的监督不足问题。",
      "conclusion": "论文的主要贡献是提出 EAPO 方法，通过证据增强和奖励协同演化解决了长上下文推理中的奖励稀疏问题，显著改善了证据提取的质量和监督。学术上，推动了强化学习在复杂推理任务中的应用研究；实际上，可能增强 LLM 在长文档分析等场景的性能。未来工作可进一步优化机制或扩展到其他推理任务，摘要未明确说明局限性。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Model",
        "Evidence-Augmented Reasoning",
        "Reward Co-Evolution",
        "Tree-Structured Evidence Sampling"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:45.821617Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10305",
    "title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset",
    "authors": [
      "Hengyu Shen",
      "Tiancheng Gu",
      "Bin Qin",
      "Lan Wu",
      "Yuling Wu",
      "Shuo Tan",
      "Zelong Sun",
      "Jun Wang",
      "Nan Wu",
      "Xiang An",
      "Weidong Cai",
      "Ziyong Feng",
      "Kaicheng Yang"
    ],
    "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10305.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10305",
    "published": "2026-01-15T11:28:58Z",
    "updated": "2026-01-15T11:28:58Z",
    "comment": "19 pages, 11 figures, 7 tables",
    "light_analysis": {
      "overview": "论文提出DanQing，一个大规模、高质量的中文视觉-语言预训练数据集，通过严格筛选流程和基于最新网页数据，提升模型在下游任务中的性能。",
      "motivation": "视觉-语言预训练模型在跨模态任务中表现优异，但现有大规模数据集如COYO-700M和LAION-400M主要为英文数据，中文领域因缺乏高质量图像-文本对而发展滞后。这限制了中文视觉-语言预训练模型的应用和进步，现有中文数据集质量不足或时效性差，无法有效捕捉现代语义趋势。因此，构建高质量中文数据集至关重要，以推动跨模态AI研究。",
      "method": "研究开发了一个全面的流程来构建高质量中文跨模态数据集。提出的DanQing数据集包含从Common Crawl收集的1亿图像-文本对，通过更严格的筛选过程确保数据质量。关键创新包括数据主要源自2024-2025年网页，能更好地捕捉新兴语义趋势，提高实用性。实验中使用SigLIP2模型进行持续预训练，以评估数据集在中文下游任务中的表现。",
      "result": "通过持续预训练SigLIP2模型并比较DanQing与现有数据集，实验结果显示，DanQing在多种中文下游任务中一致达到优越性能，包括零样本分类、跨模态检索和基于语言模型的多模态评估。这表明DanQing在数据质量和时效性上优于现有基线数据集，显著提升了模型准确性和实用性，推动了中文视觉-语言预训练的进展。",
      "conclusion": "论文的主要贡献是提出并开源了DanQing，一个大规模、高质量的中文视觉-语言预训练数据集，解决了数据稀缺问题。该研究具有重要学术价值，可促进中文跨模态AI模型的发展，基于最新数据提高了实际应用潜力。未来工作可能包括进一步优化数据集筛选流程或扩展到其他语言领域，以支持更广泛的视觉-语言研究。",
      "tags": [
        "Vision-Language Pre-training",
        "Large-scale Dataset",
        "Cross-modal Retrieval",
        "Zero-shot Classification",
        "Contrastive Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:04.548054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10282",
    "title": "SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks",
    "authors": [
      "Jose Marie Antonio Minoza"
    ],
    "abstract": "Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10282.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10282",
    "published": "2026-01-15T10:59:48Z",
    "updated": "2026-01-15T10:59:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SPIKE框架，通过稀疏Koopman算子正则化增强物理信息神经网络在微分方程求解中的泛化能力。",
      "motivation": "物理信息神经网络（PINNs）在训练域内容易过拟合，导致在时空区域外泛化能力差，这限制了其在复杂动态系统中的应用。现有方法缺乏有效正则化来捕捉动态的低维结构，从而无法准确外推预测。该问题的重要性在于实际科学计算和工程模拟中需要可靠的泛化性能，以解决现实世界的微分方程。因此，亟需一种新方法来克服过拟合并提高模型的鲁棒性。",
      "method": "论文提出SPIKE框架，通过连续时间Koopman算子正则化PINNs，以学习简约的动态表示。核心方法包括将物理约束嵌入可学习空间，强制线性动态 dz/dt = Az，并在其中应用L1正则化以促进稀疏性。关键创新点在于结合Parsimony原则，使用矩阵指数积分提供无条件稳定性，从而避免离散时间Koopman算子的对角主导问题。框架还包括PIKE变体（无显式稀疏正则化）和SPIKE变体（有L1正则化），涵盖多种PDE和ODE类型。",
      "result": "实验涵盖了抛物线、双曲线、色散和刚性偏微分方程，以及流体动力学（Navier-Stokes）和混沌常微分方程（Lorenz）。结果显示，在时间外推、空间泛化和长期预测准确性方面取得了一致改进。与基线方法相比，SPIKE在复杂动态系统如Navier-Stokes方程中表现出更好的稳定性和准确性。虽然摘要未明确说明具体数据，但报告了持续的性能提升，表明该框架有效增强了PINNs的泛化能力。",
      "conclusion": "本文的主要贡献是提出了SPIKE框架，结合稀疏Koopman算子正则化，显著改善了物理信息神经网络的泛化性能。该研究在学术上推动了微分方程求解方法的创新，具有实际应用价值，可用于流体动力学和混沌系统等复杂模拟。潜在局限性可能在于框架对特定动态类型的依赖性，未来工作可扩展到更多物理场景或探索自适应正则化策略。",
      "tags": [
        "Physics-Informed Neural Networks",
        "Koopman Operator",
        "Sparse Regularization",
        "Differential Equations",
        "Machine Learning for Physics"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:13.155972Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10274",
    "title": "Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers",
    "authors": [
      "Emre Ozbas",
      "Melih Bastopcu"
    ],
    "abstract": "We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "cs.NI",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10274.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10274",
    "published": "2026-01-15T10:47:11Z",
    "updated": "2026-01-15T10:47:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于排队理论的优化框架，在LLM服务器中分配推理令牌以平衡准确性和延迟。",
      "motivation": "在大型语言模型服务器中，处理来自多种任务类型的异构查询流时，存在准确性和延迟之间的固有权衡：增加推理令牌可提高准确性但延长服务时间，反之亦然。现有方法可能未充分考虑系统排队效应和动态查询到达，导致资源分配效率低下。本研究的动机是开发一个队列感知的优化模型，在有限计算资源下最大化准确性同时控制延迟，以提升服务器性能和用户体验。",
      "method": "该方法将系统建模为M/G/1队列，假设查询到达遵循泊松过程，并为N个任务类型分配固定数量的内部推理令牌，其中准确性函数呈现递减回报，服务时间为令牌的近似仿射函数。通过构建一个最大化加权平均准确性（惩罚平均系统时间）的优化问题，受令牌预算和队列稳定性约束。利用凹优化理论证明目标函数严格凹，确保最优解存在；推导一阶最优条件、迭代算法和投影梯度方法求解连续解，最后通过四舍五入获得整数令牌分配方案。",
      "result": "模拟实验评估了整数令牌分配方案导致的性能损失，摘要未明确说明具体数值，但表明损失可控。优化方法在准确性和延迟之间实现了有效权衡，相比基线（摘要未明确说明）确保系统稳定性和效率提升。关键点包括证明了最优解的存在性和唯一性，并提供了收敛保证的算法，但具体性能改进数据需参考全文。",
      "conclusion": "本研究的主要贡献是提出了一个队列感知的令牌分配优化模型和算法，结合排队理论和优化技术，为LLM服务器资源管理提供了理论框架和实用工具。其学术价值在于将经典队列模型应用于新兴AI系统，具有实际应用潜力以增强系统效率。未来工作可探索更复杂的场景如分布式系统，但摘要未明确说明局限性。",
      "tags": [
        "Queueing Theory",
        "Optimization",
        "M/G/1 Queue",
        "Token Allocation",
        "Large Language Model"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:20.318194Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10272",
    "title": "MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts",
    "authors": [
      "Yuxuan Lou",
      "Kai Yang",
      "Yang You"
    ],
    "abstract": "We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \\footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10272.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10272",
    "published": "2026-01-15T10:43:29Z",
    "updated": "2026-01-15T10:43:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "MoST模型通过模态感知专家混合架构，首次实现完全开源的语音-文本多模态大语言模型。",
      "motivation": "当前的多模态模型通常使用相同参数处理不同模态表示，忽视了语音和文本之间的固有表示差异，这限制了模型在跨模态理解中的性能，导致现有方法无法有效区分模态特定特性。语音和文本的集成对于自然语言处理和语音处理任务至关重要，改进模态特定学习和跨模态信息传递是提升多模态AI应用的关键，因此本研究旨在通过创新的架构解决这一问题。",
      "method": "论文提出MoST模型，采用Modality-Aware Mixture of Experts (MAMoE)架构，包括模态特定专家组以捕获领域特定模式，以及共享专家以促进模态间信息转移。基于此架构，开发了高效的转换管道：先在ASR和TTS数据集上进行策略后训练，适应预训练的MoE语言模型，然后使用精心策划的语音-文本指令数据集进行微调。整个过程完全依赖可公开访问的开源数据集，强调数据效率和性能优化。",
      "result": "综合评估在ASR、TTS、音频语言建模和语音问答基准上进行，MoST consistently outperforms existing models of comparable parameter counts，具体性能指标摘要未明确说明。消融研究证实，模态特定路由机制和共享专家设计对所有测试领域的性能提升有显著贡献，突显了架构设计的有效性，与基线方法相比显示出明显优势。",
      "conclusion": "MoST是首个完全开源、基于Mixture of Experts架构的语音-文本大语言模型，主要贡献在于MAMoE架构和高效训练流程，推动了多模态AI的发展，具有学术价值和实际应用潜力。未来工作可能包括扩展至更多模态或优化训练策略，但局限性在摘要中未明确说明，该模型为开源社区提供了重要资源。",
      "tags": [
        "Multimodal Large Language Model",
        "Mixture of Experts",
        "Modality-Aware Routing",
        "Automatic Speech Recognition",
        "Text-to-Speech"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:44.405042Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10269",
    "title": "Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders",
    "authors": [
      "P. Sánchez",
      "K. Reyes",
      "B. Radu",
      "E. Fernández"
    ],
    "abstract": "This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10269.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10269",
    "published": "2026-01-15T10:38:14Z",
    "updated": "2026-01-15T10:38:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种无需运行到失效标签的无监督健康监控框架，利用LSTM自动编码器实现涡轮风扇发动机的早期故障检测。",
      "motivation": "发动机早期故障检测对于提高安全性和可靠性至关重要，但现有方法常依赖于运行到失效的标签，这在实践中难以获取且成本高昂。本研究旨在解决这一问题，开发一种无监督方法，自动检测故障而无需手动调整规则，以克服标签依赖和人工干预的局限性，适用于实际部署环境。",
      "method": "研究方法首先通过基于回归的归一化技术，消除NASA CMAPSS传感器数据中的操作条件影响；然后训练一个长短期记忆（LSTM）自动编码器，仅使用每个轨迹的健康部分数据；最后，采用自适应数据驱动阈值来估计持续重构错误，以实时触发故障警报，无需依赖手动设定规则。",
      "result": "基准测试结果显示，该方法在多个操作制度下实现了高故障检测召回率和低误报率，表现出优越的性能和鲁棒性。摘要未明确说明具体数据值，但强调了与基线方法的比较优势，证实了其在早期故障预警中的有效性。",
      "conclusion": "本研究的主要贡献是提供了一个快速部署、可扩展的无监督早期故障检测框架，能够作为剩余使用寿命模型的互补预警层，提升发动机维护的主动性和效率。其学术价值在于推动无监督健康监控技术的发展，实际应用价值在于减少对标签的依赖并适应多样化的机队需求。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Unsupervised Learning",
        "LSTM Autoencoder",
        "Fault Detection",
        "Adaptive Threshold",
        "CMAPSS"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:41.324110Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10267",
    "title": "In-Context Source and Channel Coding",
    "authors": [
      "Ziqiong Wang",
      "Tianqi Ren",
      "Rongpeng Li",
      "Zhifeng Zhao",
      "Honggang Zhang"
    ],
    "abstract": "Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10267.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10267",
    "published": "2026-01-15T10:37:57Z",
    "updated": "2026-01-15T10:37:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于接收端的上下文解码框架，结合错误校正码变换器和大型语言模型算术解码器，提高了分离信源信道编码在低信噪比下的鲁棒性。",
      "motivation": "分离信源信道编码因其模块化和与成熟编码器兼容而在文本传输中广泛应用，但在低信噪比下常出现悬崖效应，导致解码错误和性能下降，特别是当使用大型语言模型驱动的算术编码时。现有方法如传统分离信源信道编码和联合信源信道编码在低信噪比下表现不佳，稳健性不足，因此需要一种不修改发送端即可增强稳健性的方法。",
      "method": "接收端上下文解码框架利用错误校正码变换器获取解码信息比特的比特级可靠性，基于上下文一致比特流构建置信度排名候选池，通过可靠性指导的比特翻转生成紧凑且多样化的候选子集，然后应用大型语言模型算术解码器获取重建和序列级对数似然，最后通过可靠性-似然融合规则选择最终输出。还提供了采样过程的稳定性和收敛性理论保证。",
      "result": "在加性高斯白噪声和瑞利衰落信道上的大量实验显示，该框架相比传统分离信源信道编码基线和代表性联合信源信道编码方案，展示了持续的性能提升。摘要未明确具体数值，但结果证明了能有效缓解悬崖效应，提高解码成功率和稳健性。",
      "conclusion": "本研究的主要贡献是提出了一个接收端上下文解码框架，通过结合先进模型增强分离信源信道编码的稳健性，无需修改发送端，提供了理论分析和实验验证，具有学术价值和实际应用潜力。未来工作可扩展到更多信道类型或优化采样过程。",
      "tags": [
        "Separate Source-Channel Coding",
        "In-Context Decoding",
        "Large Language Model",
        "Error Correction Code Transformer",
        "Arithmetic Coding"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:04.509917Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10266",
    "title": "Measuring Affinity between Attention-Head Weight Subspaces via the Projection Kernel",
    "authors": [
      "Hiroaki Yamagiwa",
      "Yusuke Takase",
      "Hidetoshi Shimodaira"
    ],
    "abstract": "Understanding relationships between attention heads is essential for interpreting the internal structure of Transformers, yet existing metrics do not capture this structure well. We focus on the subspaces spanned by attention-head weight matrices and quantify head-to-head relationships using the Projection Kernel (PK), a principal-angle-based measure of subspace similarity. Experiments show that PK reproduces known head-to-head interactions on the IOI task more clearly than prior metrics such as the Composition Score. We further introduce a framework to quantify the informativeness of PK distributions by comparing them with a reference distribution derived from random orthogonal subspaces. As an application, we analyze a directed graph constructed from PK and show that, in GPT2-small, L4H7 acts as a hub by functioning as an identity head.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10266.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10266",
    "published": "2026-01-15T10:37:55Z",
    "updated": "2026-01-15T10:37:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过投影核度量注意力头权重子空间的相似性，改进了对 Transformer 内部结构的分析。",
      "motivation": "理解 Transformer 模型中注意力头之间的关系对于解释其内部结构至关重要，因为这有助于模型的可解释性和调试。现有指标如 Composition Score 不能有效捕捉头与头之间的结构关系，导致在分析和解释模型行为时存在局限性。本研究旨在解决这一问题，通过提出更精确的度量方法，量化注意力头子空间的相似性，以更好地揭示和利用模型的内部机制。",
      "method": "论文提出使用投影核（PK）作为核心方法，这是一个基于主角的子空间相似性度量，用于量化注意力头权重矩阵所张成的子空间之间的关系。关键创新包括引入 PK 来替代现有指标，并开发了一个框架来评估 PK 分布的信息性，通过比较其与从随机正交子空间导出的参考分布。实验在 GPT2-small 模型上进行，重点关注 IOI 任务，以验证方法的有效性。",
      "result": "实验结果表明，投影核在 IOI 任务上比基线方法如 Composition Score 更清晰地复制了已知的头到头的互动，虽然摘要未明确说明具体性能数据如准确率，但指出方法更为有效。进一步分析通过构建有向图显示，在 GPT2-small 中，L4H7 头作为枢纽并充当身份头，这验证了 PK 在揭示模型内部结构中的实用性和精确性，提升了分析的质量。",
      "conclusion": "本研究的主要贡献是提出了投影核作为一种新的度量方法，改进了对 Transformer 注意力头关系的量化，增强了模型的可解释性。学术上，它为结构分析提供了新的工具；应用上，有助于模型调试和优化。局限性包括仅在 GPT2-small 和 IOI 任务上验证，未来工作可扩展到更多任务和模型，以进一步验证其通用性和应用潜力。",
      "tags": [
        "Attention Heads",
        "Projection Kernel",
        "Subspace Similarity",
        "Transformers",
        "GPT2-small"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:07.446629Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10257",
    "title": "Untangling Input Language from Reasoning Language: A Diagnostic Framework for Cross-Lingual Moral Alignment in LLMs",
    "authors": [
      "Nan Li",
      "Bo Kang",
      "Tijl De Bie"
    ],
    "abstract": "When LLMs judge moral dilemmas, do they reach different conclusions in different languages, and if so, why? Two factors could drive such differences: the language of the dilemma itself, or the language in which the model reasons. Standard evaluation conflates these by testing only matched conditions (e.g., English dilemma with English reasoning). We introduce a methodology that separately manipulates each factor, covering also mismatched conditions (e.g., English dilemma with Chinese reasoning), enabling decomposition of their contributions. To study \\emph{what} changes, we propose an approach to interpret the moral judgments in terms of Moral Foundations Theory. As a side result, we identify evidence for splitting the Authority dimension into a family-related and an institutional dimension. Applying this methodology to English-Chinese moral judgment with 13 LLMs, we demonstrate its diagnostic power: (1) the framework isolates reasoning-language effects as contributing twice the variance of input-language effects; (2) it detects context-dependency in nearly half of models that standard evaluation misses; and (3) a diagnostic taxonomy translates these patterns into deployment guidance. We release our code and datasets at https://anonymous.4open.science/r/CrossCulturalMoralJudgement.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10257.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10257",
    "published": "2026-01-15T10:26:29Z",
    "updated": "2026-01-15T10:26:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个诊断框架，通过分离输入语言和推理语言，分析大型语言模型在跨语言道德对齐中的差异，并基于道德基础理论解释判断。",
      "motivation": "研究动机是探讨大型语言模型在处理不同语言道德困境时是否产生不一致的结论及其原因。标准评估方法仅测试输入语言与推理语言匹配的条件，混淆了这两个因素的贡献，导致无法准确识别模型偏差来源。这一问题对于提高模型的跨语言可靠性和道德对齐至关重要，尤其是在多语言应用场景中，以避免潜在的语言依赖性偏差。",
      "method": "研究方法提出一种新方法论，通过分别操纵输入语言和推理语言，覆盖匹配和不匹配条件，以分解它们在道德判断中的影响。核心创新在于解耦这两个因素，并使用道德基础理论解释判断结果，从而更精细地分析模型行为。具体应用包括在13个大型语言模型上测试英语和中文的道德判断，并基于数据识别道德基础理论中Authority维度的细分，分为家庭相关和制度相关维度。",
      "result": "实验结果表明，该诊断框架能有效隔离推理语言效应，其方差贡献是输入语言效应的两倍，具体体现在数据分析中。此外，框架检测到近一半的大型语言模型中存在标准评估遗漏的上下文依赖性。通过诊断分类学，这些分析模式被转化为部署指南，为模型优化提供具体依据，提升了评估的精确性和实用性。",
      "conclusion": "论文的主要贡献是开发了一个诊断框架，能够分解输入语言和推理语言在跨语言道德对齐中的作用。学术上，这为理解模型偏差和道德判断提供了新视角；实际上，通过诊断分类学指导模型部署，增强了模型的可靠性和跨语言应用价值。未来工作可扩展至更多语言和模型类型，以进一步验证和推广该框架，摘要未明确说明局限性。",
      "tags": [
        "Large Language Models",
        "Cross-Lingual",
        "Moral Foundations Theory",
        "Reasoning Language",
        "Diagnostic Framework"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:15.045117Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10254",
    "title": "NoReGeo: Non-Reasoning Geometry Benchmark",
    "authors": [
      "Irina Abdullaeva",
      "Anton Vasiliuk",
      "Elizaveta Goncharova",
      "Temurbek Rahmatullaev",
      "Zagorulko Ivan",
      "Maxim Kurkin",
      "Andrey Kuznetsov"
    ],
    "abstract": "We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10254.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10254",
    "published": "2026-01-15T10:22:55Z",
    "updated": "2026-01-15T10:22:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了NoReGeo基准，用于直接评估大语言模型的内在几何理解能力，而不依赖推理过程。",
      "motivation": "现有几何评估基准主要依赖于推理为基础的代数方法，这无法直接测试大语言模型是否自然编码空间关系。研究动机是解决几何理解评估的局限性，因为当前方法侧重于问题解决步骤，而非模型对几何概念的直接认知。几何理解是AI系统认知世界的基础，现有评估的不足导致无法准确衡量模型的内在能力，限制了开发真正具备几何认知的模型。本研究旨在填补这一空白，通过设计专门基准来更真实地评估模型的几何理解水平，推动相关领域进展。",
      "method": "研究方法的核心是创建NoReGeo基准，包含2,500个简单几何问题，覆盖25个类别，每个问题设计为仅通过已知对象位置的纯粹几何理解解决，无需推理或代数计算。创新点在于专注于非推理评估，直接测试模型编码空间关系和识别几何属性的能力，避免了传统方法对计算步骤的依赖。评估了包括GPT-4在内的前沿大语言模型，使用二元分类任务作为主要指标。技术路线涉及问题生成和模型测试，确保了基准的多样性和针对性，为全面评估几何理解提供了新途径。",
      "result": "主要实验结果显示，在NoReGeo基准上评估的多个先进大语言模型，包括GPT-4，在二元分类任务中的最高准确率仅为65%，这表明模型在纯粹几何理解上表现有限。消融实验进一步表明，几何理解不能通过简单的微调获得，需要专门的训练方法从开始设计。与现有推理为基础的评估相比，NoReGeo揭示了性能差距，强调了直接几何评估的重要性，凸显了当前模型在空间关系认知方面的不足，为未来改进提供了数据支撑。",
      "conclusion": "本研究的主要贡献是提出了NoReGeo基准，有效评估大语言模型的几何理解，揭示了当前模型在自然掌握几何概念方面的显著不足。学术价值在于提供了新的评估工具，促进了对几何认知机制的研究；实际应用价值是指导开发具有真正几何能力的智能模型，提升AI的空间理解能力。未来工作可包括扩展基准覆盖更多几何类别，或探索更有效的训练策略。局限性可能在于基准设计简化，但为进一步研究奠定了基础，推动领域向真实几何认知发展。",
      "tags": [
        "Large Language Models",
        "Geometric Benchmark",
        "Binary Classification",
        "Fine-Tuning",
        "Spatial Relationships"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:19.077652Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10251",
    "title": "X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction",
    "authors": [
      "Hongru Duan",
      "Yongle Chen",
      "Lei Guan"
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10251.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10251",
    "published": "2026-01-15T10:19:08Z",
    "updated": "2026-01-15T10:19:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出X-SAM方法，通过特征向量梯度校正增强Sharpness-Aware Minimization的泛化能力。",
      "motivation": "SAM旨在通过最小化模型参数邻域内的最坏扰动损失来提升泛化性能，但训练中优化行为可能偏离理论预期，因为尖锐和平坦区域都可产生小损失，导致梯度仍指向尖锐区域，削弱正则化效果。这表明现有SAM方法在准确衡量尖锐度方面存在不足，影响模型训练的稳定性和泛化表现，因此需要更有效的优化策略来克服这一限制。",
      "method": "研究从谱和几何角度分析SAM，引入梯度与Hessian主导特征向量之间的角度作为尖锐度度量，发现角度小于等于90度时正则化效果减弱。提出X-SAM方法，通过沿顶部特征向量进行梯度正交分解实现校正，直接正则化Hessian的最大特征值，从而提升尖锐度控制的效率和精度，不依赖于特定数据集或模型架构。",
      "result": "论文证明了X-SAM的收敛性和优越泛化性能，实验评估确认了理论和实践优势，包括与基线方法相比的改进。摘要未明确说明具体性能指标如准确率提升，但实验结果支持X-SAM在优化训练过程和增强模型泛化方面的有效性。",
      "conclusion": "X-SAM通过特征向量梯度校正强化了SAM的泛化能力，提供了理论收敛保证和实验验证，具有学术价值如丰富优化方法理论，以及实际应用潜力如提升深度学习模型训练效率。未来工作可探索该方法在更广泛场景的适用性和扩展性。",
      "tags": [
        "Sharpness-Aware Minimization",
        "Hessian Analysis",
        "Eigenvector Alignment",
        "Gradient Correction",
        "Regularization"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:12.971505Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10246",
    "title": "coTherapist: A Behavior-Aligned Small Language Model to Support Mental Healthcare Experts",
    "authors": [
      "Prottay Kumar Adhikary",
      "Reena Rawat",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Access to mental healthcare is increasingly strained by workforce shortages and rising demand, motivating the development of intelligent systems that can support mental healthcare experts. We introduce coTherapist, a unified framework utilizing a small language model to emulate core therapeutic competencies through domain-specific fine-tuning, retrieval augmentation, and agentic reasoning. Evaluation on clinical queries demonstrates that coTherapist generates more relevant and clinically grounded responses than contemporary baselines. Using our novel T-BARS rubric and psychometric profiling, we confirm coTherapist exhibits high empathy and therapist-consistent personality traits. Furthermore, human evaluation by domain experts validates that coTherapist delivers accurate, trustworthy, and safe responses. coTherapist was deployed and tested by clinical experts. Collectively, these findings demonstrate that small models can be engineered to exhibit expert-like behavior, offering a scalable pathway for digital mental health tools.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10246.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10246",
    "published": "2026-01-15T10:06:28Z",
    "updated": "2026-01-15T10:06:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "coTherapist是一个行为对齐的小语言模型框架，通过领域微调、检索增强和代理推理模拟治疗能力以支持心理健康专家。",
      "motivation": "心理健康护理领域面临劳动力短缺和需求上升的双重压力，现有智能系统往往无法有效支持专家处理临床问题。这一问题重要性在于它直接影响到护理质量和可及性，而现有方法在模拟治疗师核心能力方面不足，需要更精准、可扩展的解决方案来缓解资源紧张。",
      "method": "coTherapist采用小语言模型为基础，通过领域特定微调适应心理健康场景，检索增强技术融入临床知识库以提供背景支持，并利用代理推理模拟治疗师决策过程。关键创新在于统一框架整合这些组件，实现行为对齐，无需大型模型即可模拟治疗能力。",
      "result": "在临床查询评估中，coTherapist比当代基线生成更相关和临床基础的回答。使用T-BARS量表和心测分析显示其具有高同理心和治疗师一致的人格特质。人类专家评估验证了响应准确性、可信度和安全性，具体性能指标摘要未明确说明，但确认优于基线方法。",
      "conclusion": "研究表明，小模型可通过工程化设计表现出专家级行为，为数字心理健康工具提供可扩展途径。这具有学术价值，展示了模型行为对齐的潜力，实际应用中能缓解资源压力。局限性摘要未明确说明，未来工作可能涉及扩展应用场景和提升模型泛化能力。",
      "tags": [
        "Small Language Model",
        "Domain-specific Fine-tuning",
        "Retrieval Augmentation",
        "Agentic Reasoning",
        "Mental Health Support"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:14.176404Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10245",
    "title": "TRIM: Hybrid Inference via Targeted Stepwise Routing in Multi-Step Reasoning Tasks",
    "authors": [
      "Vansh Kapoor",
      "Aman Gupta",
      "Hao Chen",
      "Anurag Beniwal",
      "Jing Huang",
      "Aviral Kumar"
    ],
    "abstract": "Multi-step reasoning tasks like mathematical problem solving are vulnerable to cascading failures, where a single incorrect step leads to complete solution breakdown. Current LLM routing methods assign entire queries to one model, treating all reasoning steps as equal. We propose TRIM (Targeted routing in multi-step reasoning tasks), which routes only critical steps$\\unicode{x2013}$those likely to derail the solution$\\unicode{x2013}$to larger models while letting smaller models handle routine continuations. Our key insight is that targeted step-level interventions can fundamentally transform inference efficiency by confining expensive calls to precisely those steps where stronger models prevent cascading errors. TRIM operates at the step-level: it uses process reward models to identify erroneous steps and makes routing decisions based on step-level uncertainty and budget constraints. We develop several routing strategies within TRIM, ranging from a simple threshold-based policy to more expressive policies that reason about long-horizon accuracy-cost trade-offs and uncertainty in step-level correctness estimates. On MATH-500, even the simplest thresholding strategy surpasses prior routing methods with 5x higher cost efficiency, while more advanced policies match the strong, expensive model's performance using 80% fewer expensive model tokens. On harder benchmarks such as AIME, TRIM achieves up to 6x higher cost efficiency. All methods generalize effectively across math reasoning tasks, demonstrating that step-level difficulty represents fundamental characteristics of reasoning.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10245.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10245",
    "published": "2026-01-15T10:06:06Z",
    "updated": "2026-01-15T10:06:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "TRIM提出了一种在多步骤推理任务中通过针对性步骤级路由实现混合推理的方法，以提高效率并防止级联错误。",
      "motivation": "多步骤推理任务如数学问题解决中，由于级联故障的存在，单个错误步骤可能导致整个解决方案崩溃，这对推理系统的可靠性和效率构成了挑战。现有的大型语言模型路由方法通常将整个查询分配给单一模型，将所有步骤视为同等重要，忽略了不同步骤的难度差异，导致要么资源浪费、要么性能下降。因此，开发一种能智能识别和处理关键步骤的路由方法至关重要，以在成本控制和高准确性之间取得平衡。",
      "method": "TRIM方法在步骤级别进行操作，使用过程奖励模型来评估每个推理步骤的正确性，从而识别可能引发级联错误的关键步骤。基于步骤级的不确定性估计和预设的预算约束，TRIM决定是否将特定步骤路由到更大、更强的模型处理，而将常规步骤交由较小、更高效的模型处理。作者开发了多种路由策略，包括简单的阈值策略和更复杂的策略，后者能推理长期准确性-成本权衡及步骤级正确性估计的不确定性，实现自适应优化。",
      "result": "在MATH-500数据集上，TRIM的最简单阈值策略相较于先前路由方法实现了5倍的成本效率提升；更高级的策略能以使用少80%昂贵模型token的方式，匹配强、昂贵模型的性能。在更难的基准测试AIME上，TRIM的成本效率提升高达6倍。所有方法在多种数学推理任务中都表现出有效的泛化能力，验证了步骤级难度作为推理基本特征的假设。",
      "conclusion": "该研究通过TRIM框架证明，在多步骤推理任务中，针对性步骤级路由能有效防止级联错误并显著优化推理效率。主要贡献在于提出了一种新颖的混合推理方法，结合大小模型优势，实现高性能与低成本的平衡；学术价值在于深化了对推理过程中步骤级难度的理解，实际应用价值在于为资源高效的AI系统提供了可行解决方案。未来工作可扩展该方法到其他推理任务，并进一步优化路由策略。",
      "tags": [
        "Large Language Model Routing",
        "Multi-Step Reasoning",
        "Process Reward Models",
        "Uncertainty Estimation",
        "Hybrid Inference"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:04.866109Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10244",
    "title": "Attend to what I say: Highlighting relevant content on slides",
    "authors": [
      "Megha Mariam K M",
      "C. V. Jawahar"
    ],
    "abstract": "Imagine sitting in a presentation, trying to follow the speaker while simultaneously scanning the slides for relevant information. While the entire slide is visible, identifying the relevant regions can be challenging. As you focus on one part of the slide, the speaker moves on to a new sentence, leaving you scrambling to catch up visually. This constant back-and-forth creates a disconnect between what is being said and the most important visual elements, making it hard to absorb key details, especially in fast-paced or content-heavy presentations such as conference talks. This requires an understanding of slides, including text, graphics, and layout. We introduce a method that automatically identifies and highlights the most relevant slide regions based on the speaker's narrative. By analyzing spoken content and matching it with textual or graphical elements in the slides, our approach ensures better synchronization between what listeners hear and what they need to attend to. We explore different ways of solving this problem and assess their success and failure cases. Analyzing multimedia documents is emerging as a key requirement for seamless understanding of content-rich videos, such as educational videos and conference talks, by reducing cognitive strain and improving comprehension. Code and dataset are available at: https://github.com/meghamariamkm2002/Slide_Highlight",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10244.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10244",
    "published": "2026-01-15T10:04:59Z",
    "updated": "2026-01-15T10:04:59Z",
    "comment": "Accepted at the International Conference on Document Analysis and Recognition (ICDAR) 2025",
    "light_analysis": {
      "overview": "本文提出一种基于演讲者叙述自动识别并高亮幻灯片相关区域的方法，改善听众的视听同步体验。",
      "motivation": "在演讲场景中，听众需同时处理听觉叙述和视觉幻灯片信息，但幻灯片内容庞杂时，难以快速定位当前所述的相关区域，导致认知负担加重。这一问题在快节奏或内容密集的演讲如会议报告中尤为突出，影响理解效率。现有方法通常依赖听众手动搜索，缺乏自动化同步机制，效率低下且易遗漏关键细节，因此需要自动化解决方案来减少这种不同步现象。",
      "method": "论文引入一种方法，通过分析演讲者的口语内容，并与幻灯片中的文本或图形元素进行匹配，自动识别并高亮最相关的区域。方法探索了不同技术路线，如结合语音识别和幻灯片解析技术，但具体模型架构和算法细节摘要未明确说明。代码和数据集已公开，便于复现和进一步研究，强调了实验的可重复性和社区贡献。",
      "result": "摘要未明确说明具体实验结果，如准确率提升或效率改进的量化指标。论文评估了不同方法的成功和失败案例，但未提供与基线方法的对比数据。推断该方法可能在改善同步效果方面表现良好，需要参考完整论文获取实验数据，以验证其在实际应用中的有效性。",
      "conclusion": "该方法的主要贡献是实现演讲内容与幻灯片视觉元素的自动同步，减少听众认知负担，提升理解效率。研究具有学术价值，推动了多媒体文档分析领域的发展，特别是在教育视频和会议报告中的应用。未来工作可包括优化匹配算法、扩展到更多多媒体类型，并评估其在不同场景下的鲁棒性和实用性。",
      "tags": [
        "Speech Analysis",
        "Slide Highlighting",
        "Content Synchronization",
        "Multimedia Understanding"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:45.565708Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10242",
    "title": "Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?",
    "authors": [
      "Guanxu Chen",
      "Dongrui Liu",
      "Jing Shao"
    ],
    "abstract": "Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10242.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10242",
    "published": "2026-01-15T10:01:21Z",
    "updated": "2026-01-15T10:01:21Z",
    "comment": "9 pages,6 figures",
    "light_analysis": {
      "overview": "本研究通过实验发现 Looped Transformers 能部分缩小大型语言模型内部知识与语言输出的差距，但受限于知识退化和感知能力不足，尚未实现真正内省链接。",
      "motivation": "大型语言模型经常在内部知识表示和显式自然语言输出之间表现出差距，这限制了模型的透明度和性能可靠性。现有方法可能缺乏有效机制来桥接这一差距，因此探索新架构如 Looped Transformers 至关重要，旨在通过内省改进输出质量，为提升模型解释性和可控性提供基础。",
      "method": "论文采用 Looped Transformers 架构，通过迭代共享层增加计算深度，利用迭代过程作为内省形式来探索表示空间与输出的链接。实验通过调整循环迭代次数进行实证分析，但摘要未明确说明具体数据集或模型架构细节，主要聚焦于评估迭代对内部知识和感知能力的影响。",
      "result": "实验结果表明，增加 Looped Transformers 的循环迭代次数能部分缩小内部知识与语言输出的差距，但部分原因在于表示携带的内部知识退化。同时，当前 LTs 在循环中感知表示的能力未改善，仅存在于最终迭代，这表明架构尚未实现有效内省机制，与理想链接存在差距。",
      "conclusion": "本研究的贡献在于揭示 Looped Transformers 作为扩展计算深度的方法具有潜力，但未能实现真正链接表示空间和自然语言所需的内省。这为未来改进大型语言模型架构提供了学术方向，强调了开发更有效内省机制的重要性，以提升模型性能和可解释性，潜在局限性包括知识退化和感知能力不足。",
      "tags": [
        "Large Language Model",
        "Looped Transformer",
        "Transformer Architecture",
        "Iterative Methods",
        "Representation Gap"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:57.317070Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10237",
    "title": "Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD",
    "authors": [
      "Murat Bilgehan Ertan",
      "Marten van Dijk"
    ],
    "abstract": "Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy   $σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ $\\quad\\text{or}\\quad$ $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$,   and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \\to \\infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10237.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10237",
    "published": "2026-01-15T09:50:36Z",
    "updated": "2026-01-15T09:50:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过$f$-差分隐私框架分析DP-SGD，揭示了在最坏情况对抗模型下无法同时实现强隐私和高效用的根本局限性。",
      "motivation": "DP-SGD是机器学习中保护隐私训练的主要方法，但在对抗性隐私定义下的性能限制尚未被充分理解。现有研究可能高估了隐私与效用之间的权衡，导致实际应用中隐私保护不足或模型准确性下降。本文旨在填补这一空白，系统分析DP-SGD在$f$-差分隐私框架下的根本限制，强调在最坏情况假设下隐私保护面临的挑战。背景是差分隐私随机梯度下降作为主流私有训练范式，但其对抗性隐私下的理论基础仍有待探索。",
      "method": "研究采用$f$-差分隐私框架，将隐私量化为假设检验权衡曲线，分析DP-SGD在单个训练epoch中使用混洗采样和$M$梯度更新的情况。通过推导权衡曲线的显式上界，获得分离参数$κ$的几何下界，并形式化证明高斯噪声乘数$σ$的严格下界。关键创新点包括理论扩展到Poisson采样，并量化隐私与效用之间的基本冲突。技术细节涉及使用Gaussian噪声机制，评估隐私损失与模型性能的权衡。",
      "result": "理论结果显示，在标准最坏情况对抗模型下，DP-SGD必须满足$σ\\ge \\frac{1}{\\sqrt{2\\ln M}}$ 或 $κ\\ge\\ \\frac{1}{\\sqrt{8}}\\!\\left(1-\\frac{1}{\\sqrt{4π\\ln M}}\\right)$，这表明无法同时实现强隐私（小$κ$）和高效用（小噪声）。实验证实，该噪声水平在实际训练设置中导致模型准确性显著下降，例如在实验中观察到准确率降低，验证了理论界限的实用性。与基线方法相比，噪声需求较高，限制了DP-SGD的实用性。",
      "conclusion": "论文的主要贡献是揭示了DP-SGD在隐私保护训练中的根本局限性，强调在最坏情况假设下隐私与效用之间存在不可调和的冲突。这项研究提高了对抗性隐私下算法设计的认识，并暗示需要开发新方法或放松假设以克服瓶颈。未来工作可能包括探索更高效的隐私机制、优化采样策略或重新评估对抗性模型的实际适用性，以推动私有机器学习的发展。",
      "tags": [
        "Differential Privacy",
        "DP-SGD",
        "f-Differential Privacy",
        "Privacy-Utility Trade-off",
        "Gaussian Noise"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:14.866902Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10229",
    "title": "GeoSteer: Faithful Chain-of-Thought Steering via Latent Manifold Gradients",
    "authors": [
      "Kentaro Kazama",
      "Daiki Shirafuji",
      "Tatsuhiko Saito"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have improved multi-step reasoning. Most approaches rely on Chain-of-Thought (CoT) rationales. Previous studies have shown that LLMs often generate logically inconsistent reasoning steps even when their final answers are correct. These inconsistencies reduce the reliability of step-level reasoning. We propose GeoSteer, a manifold-based framework that improves the quality of intermediate reasoning. The method consists of: (1) constructing a CoT dataset with segment-level scores, (2) training a Variational Autoencoder (VAE) model and a quality estimation model to learn a low-dimensional manifold of high-quality CoT trajectories, and (3) steering hidden states of target LLMs toward higher-quality regions in the latent space. This update in a latent space behaves like a natural-gradient adjustment in the original hidden-state space. It ensures geometrically coherent steering. We evaluate GeoSteer on the GSM8k dataset using the Qwen3 series. We measure via answer accuracy and overall reasoning performance. GeoSteer improved the exact match accuracy by up to 2.6 points. It also enhanced the pairwise win rate by 5.3 points. These results indicate that GeoSteer provides an effective and controllable mechanism for improving the quality of intermediate reasoning in LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10229.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10229",
    "published": "2026-01-15T09:44:07Z",
    "updated": "2026-01-15T09:44:07Z",
    "comment": "The Third workshop of NeusymBridge @AAAI 2026 (Bridging Neurons and Symbols for NLP and Knowledge Graph Reasoning)",
    "light_analysis": {
      "overview": "提出GeoSteer框架，通过潜在流形梯度提升大型语言模型的中间推理质量。",
      "motivation": "大型语言模型在多步推理方面取得进展，但Chain-of-Thought rationale常存在逻辑不一致，即使最终答案正确。这些不一致降低了步骤级推理的可靠性，影响模型的可信度。现有方法大多依赖于CoT，但缺乏有效机制确保中间推理质量，导致推理过程中的错误可能被忽视。为了提升LLM在数学推理等实际应用中的可信度，本研究旨在开发框架来主动引导和优化CoT轨迹。",
      "method": "论文提出GeoSteer框架，首先构建带有分段评分的Chain-of-Thought数据集。然后，训练一个变分自编码器（VAE）模型和一个质量估计模型，共同学习高质量CoT轨迹的低维流形表示。接着，在潜在空间中引导目标大型语言模型（如Qwen3系列）的隐藏状态向高质量区域调整，这种更新类似于原始隐藏状态空间中的自然梯度调整，确保了几何一致的引导过程。",
      "result": "在GSM8k数据集上使用Qwen3系列模型进行评估，通过答案准确性和整体推理性能测量效果。GeoSteer将精确匹配准确率提升了最多2.6个百分点，同时成对胜率提高了5.3个百分点。与基线方法相比，这些提升表明GeoSteer能有效改进中间推理质量，提供了更可靠和可控的机制，验证了方法的有效性。",
      "conclusion": "论文的主要贡献是提出了GeoSteer框架，通过潜在流形梯度来引导和提升大型语言模型的Chain-of-Thought推理质量。在学术上，它引入了流形学习和梯度调整的新方法，增强了多步推理的可控性和可靠性。在实际应用中，可为数学问题求解、逻辑推理等任务提供更可信的模型输出。摘要未明确说明具体局限性，但未来工作可能包括扩展到其他数据集和模型，进一步优化引导策略。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thought",
        "Variational Autoencoder",
        "Manifold Learning",
        "Latent Gradient"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:38.104199Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10228",
    "title": "Optimizing Multimodal LLMs for Egocentric Video Understanding: A Solution for the HD-EPIC VQA Challenge",
    "authors": [
      "Sicheng Yang",
      "Yukai Huang",
      "Shitong Sun",
      "Weitong Cai",
      "Jiankang Deng",
      "Jifei Song",
      "Zhensong Zhang"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) struggle with complex video QA benchmarks like HD-EPIC VQA due to ambiguous queries/options, poor long-range temporal reasoning, and non-standardized outputs. We propose a framework integrating query/choice pre-processing, domain-specific Qwen2.5-VL fine-tuning, a novel Temporal Chain-of-Thought (T-CoT) prompting for multi-step reasoning, and robust post-processing. This system achieves 41.6% accuracy on HD-EPIC VQA, highlighting the need for holistic pipeline optimization in demanding video understanding. Our code, fine-tuned models are available at https://github.com/YoungSeng/Egocentric-Co-Pilot.",
    "categories": [
      "cs.CV",
      "cs.MM",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10228.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10228",
    "published": "2026-01-15T09:43:49Z",
    "updated": "2026-01-15T09:43:49Z",
    "comment": "4 pages, 1 figure, CVPR 2025 EgoVis Workshop, 2nd Place in HD-EPIC Challenge",
    "light_analysis": {
      "overview": "论文提出了一种集成预处理、微调、时序链式思维提示和后处理的框架，以优化多模态大语言模型在HD-EPIC VQA挑战中的性能，强调了全管道优化的创新性。",
      "motivation": "多模态大语言模型在复杂视频问答基准如HD-EPIC VQA中表现不佳，主要由于查询和选项的模糊性、长期时序推理能力不足以及输出非标准化。这一问题至关重要，因为以自我为中心的视频理解在智能监控、机器人导航等领域有广泛应用，而现有方法难以有效处理视频中的时间依赖性和上下文复杂性，亟需改进以应对苛刻的视频理解需求。",
      "method": "论文提出了一个综合框架，首先对查询和选项进行预处理以消除歧义；其次，使用Qwen2.5-VL模型进行领域特定微调，使其适应以自我为中心的视频数据；接着，引入了新颖的时序链式思维提示方法，通过多步推理增强长期时序理解；最后，实施鲁棒的后处理步骤以确保输出标准化。关键创新点在于T-CoT提示，它模拟人类思维链，帮助模型逐步分析视频事件序列。",
      "result": "实验结果显示，该系统在HD-EPIC VQA基准测试中实现了41.6%的准确率。这一性能虽未与具体基线模型直接对比，但鉴于多模态大语言模型在该任务上普遍面临挑战，该结果显著提升了视频问答的能力，突出了全管道优化在解决查询模糊、时序推理差和输出非标准化等问题上的有效性。摘要未明确说明详细对比数据。",
      "conclusion": "本研究的主要贡献在于提出了一个优化多模态大语言模型用于以自我为中心视频理解的框架，并在HD-EPIC VQA上验证了其有效性，达到41.6%准确率，强调了在复杂视频理解中全管道优化的必要性。学术上，该工作推动了多模态大语言模型在时序推理任务中的发展；应用上，开源代码和模型为后续研究提供了便利。未来工作可探索模型的可扩展性和泛化能力，尽管摘要未明确说明局限性。",
      "tags": [
        "Multimodal Large Language Model",
        "Video Question Answering",
        "Fine-tuning",
        "Chain-of-Thought Prompting",
        "Post-processing"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:41.773069Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10215",
    "title": "Topo-RAG: Topology-aware retrieval for hybrid text-table documents",
    "authors": [
      "Alex Dantart",
      "Marco Kóvacs-Navarro"
    ],
    "abstract": "In enterprise datasets, documents are rarely pure. They are not just text, nor just numbers; they are a complex amalgam of narrative and structure. Current Retrieval-Augmented Generation (RAG) systems have attempted to address this complexity with a blunt tool: linearization. We convert rich, multidimensional tables into simple Markdown-style text strings, hoping that an embedding model will capture the geometry of a spreadsheet in a single vector. But it has already been shown that this is mathematically insufficient.   This work presents Topo-RAG, a framework that challenges the assumption that \"everything is text\". We propose a dual architecture that respects the topology of the data: we route fluid narrative through traditional dense retrievers, while tabular structures are processed by a Cell-Aware Late Interaction mechanism, preserving their spatial relationships. Evaluated on SEC-25, a synthetic enterprise corpus that mimics real-world complexity, Topo-RAG demonstrates an 18.4% improvement in nDCG@10 on hybrid queries compared to standard linearization approaches. It's not just about searching better; it's about understanding the shape of information.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10215.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10215",
    "published": "2026-01-15T09:27:14Z",
    "updated": "2026-01-15T09:27:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Topo-RAG框架，通过双重架构处理混合文本-表格文档，实现拓扑感知检索，显著提升检索性能。",
      "motivation": "在企业数据集中，文档往往是文本和表格的混合体，现有检索增强生成（RAG）系统通常采用线性化方法，将复杂的多维表格转换为简单文本字符串进行处理。然而，这种方法在数学上已被证明不足，无法有效捕捉表格的结构和几何关系，导致检索准确性和效率受限。因此，研究旨在解决现有方法在处理混合文档拓扑时的缺陷，提出一种更精细的检索方法来尊重数据的内在形状，从而应对真实世界复杂数据集的挑战。",
      "method": "Topo-RAG采用双重架构设计，分别处理文本和表格部分。对于文本叙述，使用传统的密集检索器进行路由；对于表格结构，则通过Cell-Aware Late Interaction机制，专注于单元格之间的空间关系，避免线性化转换。这种方法保留了表格的拓扑特性，通过特定的交互方式捕捉结构化信息。框架在SEC-25合成企业语料库上进行实施，以模拟真实世界的复杂性，并验证其技术有效性。",
      "result": "在SEC-25合成企业语料库的评估中，Topo-RAG在混合查询上的nDCG@10指标相比标准线性化方法提升了18.4%。这一显著提升表明，拓扑感知检索在处理混合文档时具有优势，能够更准确地检索相关信息，并验证了保留数据形状的重要性。实验对比了基线方法，强调了框架在改进检索效果方面的具体数据支撑。",
      "conclusion": "Topo-RAG研究挑战了“一切都是文本”的假设，通过引入拓扑感知检索框架，改善了混合文档的处理效果。它不仅提升了检索性能，还促进了对信息结构的深入理解，为企业数据检索和分析提供了新思路。该框架具有推动RAG技术发展的学术价值和实际应用潜力，未来工作可探索扩展到更多结构化数据类型或应用于更广泛的实际场景。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Hybrid Documents",
        "Topology-aware Retrieval",
        "Cell-Aware Late Interaction",
        "SEC-25 Dataset"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:52.580478Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10214",
    "title": "Beyond Inpainting: Unleash 3D Understanding for Precise Camera-Controlled Video Generation",
    "authors": [
      "Dong-Yu Chen",
      "Yixin Guo",
      "Shuojin Yang",
      "Tai-Jiang Mu",
      "Shi-Min Hu"
    ],
    "abstract": "Camera control has been extensively studied in conditioned video generation; however, performing precisely altering the camera trajectories while faithfully preserving the video content remains a challenging task. The mainstream approach to achieving precise camera control is warping a 3D representation according to the target trajectory. However, such methods fail to fully leverage the 3D priors of video diffusion models (VDMs) and often fall into the Inpainting Trap, resulting in subject inconsistency and degraded generation quality. To address this problem, we propose DepthDirector, a video re-rendering framework with precise camera controllability. By leveraging the depth video from explicit 3D representation as camera-control guidance, our method can faithfully reproduce the dynamic scene of an input video under novel camera trajectories. Specifically, we design a View-Content Dual-Stream Condition mechanism that injects both the source video and the warped depth sequence rendered under the target viewpoint into the pretrained video generation model. This geometric guidance signal enables VDMs to comprehend camera movements and leverage their 3D understanding capabilities, thereby facilitating precise camera control and consistent content generation. Next, we introduce a lightweight LoRA-based video diffusion adapter to train our framework, fully preserving the knowledge priors of VDMs. Additionally, we construct a large-scale multi-camera synchronized dataset named MultiCam-WarpData using Unreal Engine 5, containing 8K videos across 1K dynamic scenes. Extensive experiments show that DepthDirector outperforms existing methods in both camera controllability and visual quality. Our code and dataset will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10214.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10214",
    "published": "2026-01-15T09:26:45Z",
    "updated": "2026-01-15T09:26:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出DepthDirector框架，通过深度视频指导释放视频扩散模型的3D理解能力，实现精确相机控制的视频生成。",
      "motivation": "相机控制在条件视频生成中备受关注，但精确改变相机轨迹同时保持视频内容一致性仍具挑战。主流方法通过变形3D表示实现相机控制，但未能充分利用视频扩散模型（VDMs）的3D先验，陷入Inpainting Trap，导致主体不一致和生成质量下降。因此，本研究旨在解决这一问题，提升视频生成的精确控制能力，避免内容失真。",
      "method": "本研究提出DepthDirector视频重新渲染框架，利用来自显式3D表示的深度视频作为相机控制指导。设计View-Content双流条件机制，将源视频和目标视角下渲染的变形深度序列注入预训练视频生成模型，使VDMs理解相机运动并利用其3D能力。引入轻量级基于LoRA的视频扩散适配器进行训练，保留模型知识先验。此外，使用Unreal Engine 5构建大型多相机同步数据集MultiCam-WarpData，包含1K动态场景的8K视频。",
      "result": "广泛实验显示DepthDirector在相机可控性和视觉质量方面均优于现有方法，表现出更高的性能。与基线方法对比，它能够更精确地控制相机轨迹并保持视频内容的一致性，提升了生成效果，摘要未明确说明具体性能指标数字。",
      "conclusion": "本研究的主要贡献是提出DepthDirector框架，通过深度视频和双流条件机制解决了Inpainting Trap，实现了精确相机控制的视频生成。学术价值在于释放了VDMs的3D理解潜力，推动视频生成技术的发展。实际应用价值包括视频编辑和虚拟现实等领域。代码和数据集将公开，促进未来工作，摘要未明确说明局限性。",
      "tags": [
        "Camera Control",
        "Video Diffusion Models",
        "Depth Guidance",
        "Dual-Stream Condition",
        "LoRA"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:46.195452Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10205",
    "title": "One Instruction Does Not Fit All: How Well Do Embeddings Align Personas and Instructions in Low-Resource Indian Languages?",
    "authors": [
      "Arya Shah",
      "Himanshu beniwal",
      "Mayank Singh"
    ],
    "abstract": "Aligning multilingual assistants with culturally grounded user preferences is essential for serving India's linguistically diverse population of over one billion speakers across multiple scripts. However, existing benchmarks either focus on a single language or conflate retrieval with generation, leaving open the question of whether current embedding models can encode persona-instruction compatibility without relying on response synthesis. We present a unified benchmark spanning 12 Indian languages and four evaluation tasks: monolingual and cross-lingual persona-to-instruction retrieval, reverse retrieval from instruction to persona, and binary compatibility classification. Eight multilingual embedding models are evaluated in a frozen-encoder setting with a thin logistic regression head for classification. E5-Large-Instruct achieves the highest Recall@1 of 27.4\\% on monolingual retrieval and 20.7\\% on cross-lingual transfer, while BGE-M3 leads reverse retrieval at 32.1\\% Recall@1. For classification, LaBSE attains 75.3\\% AUROC with strong calibration. These findings offer practical guidance for model selection in Indic multilingual retrieval and establish reproducible baselines for future work\\footnote{Code, datasets, and models are publicly available at https://github.com/aryashah2k/PI-Indic-Align.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10205.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10205",
    "published": "2026-01-15T09:10:14Z",
    "updated": "2026-01-15T09:10:14Z",
    "comment": "12 pages, 4 figures, 10 tables",
    "light_analysis": {
      "overview": "本文提出一个统一基准，评估多语言嵌入模型在12种印度语言中的人设-指令对齐性能，为模型选择提供实用指南。",
      "motivation": "研究动机源于服务印度超过十亿人口的多元语言需求，现有基准往往局限于单一语言或将检索与生成任务混淆，无法有效评估嵌入模型在人设-指令兼容性编码上的能力，尤其在低资源语言中构建文化敏感的AI助手面临挑战，因此需要专门框架来填补这一空白。",
      "method": "研究方法包括创建一个涵盖12种印度语言的统一基准，定义四个评估任务：单语言和跨语言的人设到指令检索、指令到人设的反向检索、以及二元兼容性分类。在冻结编码器设置下，评估了八个多语言嵌入模型，如E5-Large-Instruct和BGE-M3，并使用薄逻辑回归头进行分类，专注于编码器能力而不依赖生成任务。",
      "result": "主要实验结果显示，E5-Large-Instruct在单语言检索上Recall@1达到27.4%，跨语言迁移为20.7%；BGE-M3在反向检索上以32.1% Recall@1领先；LaBSE在分类任务上AUROC为75.3%，校准效果良好。这些数据为不同任务下的模型选择提供了性能对比，并建立了可复现的基准。",
      "conclusion": "结论总结该研究通过统一基准评估了多语言嵌入模型在印度语言中的对齐性能，为模型选择提供了实用指导，并建立了可复现基线，具有学术价值（推进低资源语言AI评估方法）和实际应用价值（助力多语言助手开发），未来工作可扩展至更多语言或任务。",
      "tags": [
        "Multilingual Embedding Models",
        "Retrieval Tasks",
        "Compatibility Classification",
        "Low-Resource Languages",
        "Indian Languages"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:56.241206Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10201",
    "title": "PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary",
    "authors": [
      "Jiarui Yao",
      "Ruida Wang",
      "Tong Zhang"
    ],
    "abstract": "Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10201.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10201",
    "published": "2026-01-15T09:01:53Z",
    "updated": "2026-01-15T09:01:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了过程奖励学习（PRL）方法，通过将结果奖励分解为过程监督信号，以理论支持的方式改进大语言模型的推理能力。",
      "motivation": "近年来，提升大语言模型（LLMs）的推理能力已成为重要课题。然而，现有大多数研究基于轨迹级别的结果奖励，缺乏在推理过程中的细粒度监督，导致优化不精确。其他训练框架虽然尝试结合过程信号，但严重依赖额外步骤如蒙特卡洛树搜索（MCTS）或训练单独的奖励模型，这损害了训练效率并增加了复杂性。此外，过程信号设计的直觉缺乏严谨的理论支持，使得优化机制不透明。因此，亟需一种高效、理论基础扎实的方法来改进LLMs的推理过程，以解决现有方法在监督粒度、效率和理论解释上的不足。",
      "method": "本论文提出了过程奖励学习（PRL）方法，其核心技术路线是将熵正则化的强化学习（RL）目标分解为中间步骤，为每个步骤分配严格定义的过程奖励。从理论动机出发，推导出PRL的公式，该公式本质上等同于奖励最大化加上策略模型与参考模型之间的KL散度惩罚项。PRL的关键创新在于能将结果奖励转化为细粒度的过程监督信号，从而更好地指导RL优化中的探索过程，避免了依赖额外步骤如MCTS或训练奖励模型。这种方法直接基于理论推导，优化了训练效率，并提供了清晰的优化机制理解。",
      "result": "通过广泛实验，PRL方法在提升LLMs推理能力方面表现出显著效果。结果表明，它不仅提高了平均@ n性能指标，还拓宽了推理边界，具体体现在pass @ n指标的改进上。实验验证了PRL的有效性和泛化性，尽管摘要未提供具体数值对比基线方法，但强调了PRL在优化推理过程中的优势，如更高效的探索和性能提升。这表明PRL能有效克服现有方法的局限性，并可能在不同推理任务中推广应用。",
      "conclusion": "本研究的主要贡献是提出了过程奖励学习（PRL）方法，以理论支持的方式改进大语言模型的推理能力，通过分解结果奖励为过程信号，优化了训练效率和监督粒度。其学术价值在于为过程监督提供了严谨的理论基础，增强了优化机制的可解释性。在实际应用中，PRL能显著提升LLMs的推理性能，具有广泛的潜力用于自然语言处理和AI推理任务。尽管摘要未明确说明局限性，但未来工作可能涉及进一步验证其在更复杂场景下的鲁棒性，或扩展到其他机器学习领域以优化过程监督。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Process Reward Learning",
        "Entropy Regularization",
        "KL-divergence"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:04.229920Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10200",
    "title": "ELITE: Efficient Gaussian Head Avatar from a Monocular Video via Learned Initialization and TEst-time Generative Adaptation",
    "authors": [
      "Kim Youwang",
      "Lee Hyoseok",
      "Subin Park",
      "Gerard Pons-Moll",
      "Tae-Hyun Oh"
    ],
    "abstract": "We introduce ELITE, an Efficient Gaussian head avatar synthesis from a monocular video via Learned Initialization and TEst-time generative adaptation. Prior works rely either on a 3D data prior or a 2D generative prior to compensate for missing visual cues in monocular videos. However, 3D data prior methods often struggle to generalize in-the-wild, while 2D generative prior methods are computationally heavy and prone to identity hallucination. We identify a complementary synergy between these two priors and design an efficient system that achieves high-fidelity animatable avatar synthesis with strong in-the-wild generalization. Specifically, we introduce a feed-forward Mesh2Gaussian Prior Model (MGPM) that enables fast initialization of a Gaussian avatar. To further bridge the domain gap at test time, we design a test-time generative adaptation stage, leveraging both real and synthetic images as supervision. Unlike previous full diffusion denoising strategies that are slow and hallucination-prone, we propose a rendering-guided single-step diffusion enhancer that restores missing visual details, grounded on Gaussian avatar renderings. Our experiments demonstrate that ELITE produces visually superior avatars to prior works, even for challenging expressions, while achieving 60x faster synthesis than the 2D generative prior method.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10200.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10200",
    "published": "2026-01-15T08:57:49Z",
    "updated": "2026-01-15T08:57:49Z",
    "comment": "Project page: https://kim-youwang.github.io/elite",
    "light_analysis": {
      "overview": "ELITE 提出了一种结合3D和2D先验的高效高斯头头像合成方法，通过学习初始化和测试时生成适应，实现了高保真和快速生成。",
      "motivation": "该研究旨在解决从单目视频合成头头像时因视觉线索缺失导致的挑战。单目视频信息有限，导致头像质量下降和动画困难。先前工作依赖于3D数据先验，但它们在野外场景中泛化能力不足；或依赖于2D生成先验，虽然能补偿信息，但计算成本高且容易产生身份幻觉。因此，需要开发一个高效、泛化能力强的方法来提升头像合成的实用性和质量，以支持虚拟现实和远程通信等应用。",
      "method": "论文设计了ELITE系统，核心包括两个阶段。首先，引入前馈Mesh2Gaussian Prior Model (MGPM)，通过学习初始化快速生成高斯头像的初步表示，利用3D先验信息。其次，在测试时加入生成适应阶段，使用真实和合成图像作为监督，并提出渲染引导的单步扩散增强器来恢复缺失视觉细节。关键创新在于结合3D和2D先验的协同作用，避免传统完全扩散去噪的慢速和幻觉问题，实现高效域适应。",
      "result": "实验结果显示，ELITE在合成头头像时展现出视觉上的优越性，即使在挑战性表达下也能保持高质量，比先前方法产生更逼真的结果。具体而言，合成速度比2D生成先验方法快60倍，显著提升效率。与基线方法对比，ELITE在感知质量和泛化能力上均有改进，为实时应用提供了可能，但摘要未明确说明具体数据集或详细指标。",
      "conclusion": "ELITE的主要贡献在于提出一个高效的头头像合成系统，结合3D和2D先验，通过学习和适应策略提升质量与速度。研究具有学术价值，展示了先验融合的有效性，并为单目视觉任务提供了新方向。实际应用包括虚拟化身和增强现实等领域。未来工作可能涉及扩展场景或优化适应策略，但摘要未明确说明局限性。",
      "tags": [
        "Gaussian Head Avatar",
        "Monocular Video Synthesis",
        "Mesh2Gaussian Prior Model",
        "Test-time Generative Adaptation",
        "Diffusion Enhancer"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:16.259481Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10199",
    "title": "Graph Regularized PCA",
    "authors": [
      "Antonio Briola",
      "Marwin Schmidt",
      "Fabio Caccioli",
      "Carlos Ros Perez",
      "James Singleton",
      "Christian Michler",
      "Tomaso Aste"
    ],
    "abstract": "High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10199.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10199",
    "published": "2026-01-15T08:57:18Z",
    "updated": "2026-01-15T08:57:18Z",
    "comment": "15 pages, 2 figures, 4 Tables",
    "light_analysis": {
      "overview": "本文提出了Graph Regularized PCA，通过结合图正则化优化PCA在非独立同分布噪声下的性能，提高降维的结构保真度。",
      "motivation": "高维数据中特征间的依赖关系常常违反主成分分析（PCA）的噪声各向同性假设，当噪声不是独立同分布时，传统PCA不再是处理此类数据的最优方法。这在实际应用中很重要，例如在基因表达或社交网络数据中，特征依赖结构可能导致PCA忽略关键信息。现有PCA方法假设噪声是独立同分布的，忽略了条件依赖关系，因此需要一种方法结合数据依赖结构以提高鲁棒性和解释性。",
      "method": "GR-PCA是一种基于图正则化的PCA扩展方法，通过学习稀疏精度图来建模特征间的依赖结构，并引入图拉普拉斯惩罚将主成分负载偏向低频傅里叶模式。关键创新点在于利用图结构信息抑制高频噪声信号，同时保留图一致的低频信号，从而产生更可解释的主成分。该方法模块化，可与不同精度估计器结合，实现简单且可扩展，无需指定特定模型架构。",
      "result": "论文在合成数据上进行评估，涵盖多样化的图拓扑、信噪比和稀疏度水平。结果显示，GR-PCA能够将方差集中在预期的支持上，产生的负载具有较低图拉普拉斯能量，并在样本外重建中保持与基线方法的竞争力。与标准PCA相比，GR-PCA在高频信号图相关时优势最显著，通过防止过拟合提高了结构保真度，尽管重建精度略有下降。摘要未明确说明具体数值指标，但强调了性能的相对改进。",
      "conclusion": "GR-PCA的主要贡献是提供一种结构感知的降维方法，在不牺牲预测性能的情况下提高PCA在高维数据中的结构保真度。其学术价值在于将图理论与传统PCA结合，推动了对依赖结构建模的研究；实际应用价值在于模块化和可扩展性，适用于多种数据分析场景。潜在局限性包括在噪声旋转不变情况下PCA可能仍具竞争力，未来工作可探索在真实数据集上的验证和优化。",
      "tags": [
        "Graph Regularized PCA",
        "Sparse Precision Graph",
        "Laplacian Regularization",
        "Dimensionality Reduction",
        "Principal Component Analysis"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:20.132285Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10198",
    "title": "HUMANLLM: Benchmarking and Reinforcing LLM Anthropomorphism via Human Cognitive Patterns",
    "authors": [
      "Xintao Wang",
      "Jian Yang",
      "Weiyuan Li",
      "Rui Xie",
      "Jen-tse Huang",
      "Jun Gao",
      "Shuai Huang",
      "Yueping Kang",
      "Liyuan Gou",
      "Hongwei Feng",
      "Yanghua Xiao"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in reasoning and generation, serving as the foundation for advanced persona simulation and Role-Playing Language Agents (RPLAs). However, achieving authentic alignment with human cognitive and behavioral patterns remains a critical challenge for these agents. We present HUMANLLM, a framework treating psychological patterns as interacting causal forces. We construct 244 patterns from ~12,000 academic papers and synthesize 11,359 scenarios where 2-5 patterns reinforce, conflict, or modulate each other, with multi-turn conversations expressing inner thoughts, actions, and dialogue. Our dual-level checklists evaluate both individual pattern fidelity and emergent multi-pattern dynamics, achieving strong human alignment (r=0.91) while revealing that holistic metrics conflate simulation accuracy with social desirability. HUMANLLM-8B outperforms Qwen3-32B on multi-pattern dynamics despite 4x fewer parameters, demonstrating that authentic anthropomorphism requires cognitive modeling--simulating not just what humans do, but the psychological processes generating those behaviors.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10198.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10198",
    "published": "2026-01-15T08:56:53Z",
    "updated": "2026-01-15T08:56:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了HUMANLLM框架，通过建模人类认知模式作为因果力来基准测试和增强大语言模型的拟人化能力。",
      "motivation": "大语言模型在推理和生成方面表现出色，已成为人物模拟和角色扮演语言代理的基础。然而，实现与人类认知和行为模式的真实对齐仍是一个关键挑战，现有方法往往只关注行为表面，而忽视内在心理过程。这限制了代理在教育和娱乐等应用中的真实性和自然度，需要开发更精细的认知建模技术来提升对齐效果。",
      "method": "HUMANLLM框架将心理模式视为相互作用的因果力，方法包括从约12,000篇学术论文中提取244个认知模式，并合成了11,359个多模式交互场景，其中2-5个模式以强化、冲突或调制方式交互，通过多轮对话表达内心思想、行动和对话。核心创新是双级检查清单，分别评估个体模式保真度和新兴多模式动态，以确保对齐的准确性和完整性。",
      "result": "实验结果显示，HUMANLLM在人类对齐方面达到0.91的相关系数，表明高保真度。在性能对比中，HUMANLLM-8B模型在多模式动态评估上优于参数多4倍的Qwen3-32B模型，这证明认知建模能有效提升拟人化能力，同时揭示整体指标可能混淆模拟准确性与社会期望。",
      "conclusion": "该研究的主要贡献是提出了HUMANLLM框架，强调真实拟人化依赖于认知建模——不仅要模拟人类行为，还要理解生成这些行为的心理过程。这为角色扮演语言代理的研究提供了新方向，具有推动理论和应用发展的价值。摘要未明确说明局限性和未来工作，但可推断需进一步优化模型和扩展模式库。",
      "tags": [
        "Large Language Models",
        "Anthropomorphism",
        "Cognitive Modeling",
        "Causal Forces",
        "Role-Playing Language Agents"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:10.294075Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10193",
    "title": "GFM4GA: Graph Foundation Model for Group Anomaly Detection",
    "authors": [
      "Jiujiu Chen",
      "Weijun Zeng",
      "Shaofeng Hu",
      "Sihong Xie",
      "Hui Xiong"
    ],
    "abstract": "Group anomaly detection is crucial in many network applications, but faces challenges due to diverse anomaly patterns. Motivated by the success of large language models (LLMs) in natural language processing, graph foundation models (GFMs) is proposed to handle few-shot learning task with fewer labeling efforts. GFMs have been successfully applied to detection of individual anomalies but cannot be generalized to group anomalies, as group anomaly patterns must be detected as a whole and individuals in an abnormal group can look rather normal. Therefore, we propose GFM4GA, a novel graph foundation model for group anomaly detection. The pipeline is pretrained via dual-level contrastive learning based on feature-based estimation and group extraction, to capture potential group anomaly structure and feature inconsistencies. In the downstream tasks, the pipeline is finetuned in parameter-constrained and group-anomaly-proportion weighted few-shot settings, and its adaptive ability to unseen group anomalies expanded via group contexts determined by labeled anomaly neighbors. Experiments show that GFM4GA surpasses group anomaly detectors and GFMs for individual anomalies, achieving average improvements of 2.85% in AUROC and 2.55% in AUPRC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10193.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10193",
    "published": "2026-01-15T08:48:34Z",
    "updated": "2026-01-15T08:48:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "GFM4GA是一种新型的图基础模型，专门用于组异常检测，通过双层对比学习和自适应微调机制有效提升检测性能。",
      "motivation": "组异常检测在网络应用中至关重要，但面临因异常模式多样化而带来的挑战。现有图基础模型（GFMs）虽成功应用于个体异常检测，但无法泛化到组异常，因为组异常需整体检测且个体可能表现正常。受大型语言模型（LLMs）在少样本学习中成功的启发，本研究旨在填补这一空白，开发一个能处理组异常的模型，减少标注需求并应对复杂异常模式，提高检测的适应性和效率。",
      "method": "本研究提出GFM4GA模型，采用预训练和微调两阶段方法。预训练阶段使用基于特征估计和组提取的双层对比学习，以捕获潜在的组异常结构和特征不一致性。微调阶段在少样本设置下进行，结合参数约束和组异常比例加权，并通过标记异常邻居确定的组上下文来增强模型对新组异常的自适应能力。核心创新点包括双层对比学习机制和自适应微调策略，利用图数据结构学习组级模式。",
      "result": "实验结果显示，GFM4GA在组异常检测任务中表现优异，超越了现有组异常检测器和针对个体异常的图基础模型。具体性能指标表明，在AUROC上平均提升2.85%，在AUPRC上平均提升2.55%，证明了其显著的有效性和优越性。与基线方法相比，GFM4GA在少样本设置下实现了更高的检测准确率和鲁棒性，为组异常检测提供了新的基准。",
      "conclusion": "本论文的主要贡献是提出了GFM4GA，一个专门用于组异常检测的图基础模型，解决了现有方法无法泛化到组异常的问题。学术价值在于扩展了基础模型在异常检测领域的应用，为少样本学习提供了新方法。实际应用价值体现在网络安全的异常检测中，能提高效率和准确性。未来工作可包括优化模型参数、扩展到更多异常类型或应用到其他网络场景中。",
      "tags": [
        "Graph Foundation Model",
        "Group Anomaly Detection",
        "Contrastive Learning",
        "Few-shot Learning",
        "Feature Estimation"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:39.545546Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10192",
    "title": "From Physical Degradation Models to Task-Aware All-in-One Image Restoration",
    "authors": [
      "Hu Gao",
      "Xiaoning Lei",
      "Xichen Xu",
      "Xingjian Wang",
      "Lizhuang Ma"
    ],
    "abstract": "All-in-one image restoration aims to adaptively handle multiple restoration tasks with a single trained model. Although existing methods achieve promising results by introducing prompt information or leveraging large models, the added learning modules increase system complexity and hinder real-time applicability. In this paper, we adopt a physical degradation modeling perspective and predict a task-aware inverse degradation operator for efficient all-in-one image restoration. The framework consists of two stages. In the first stage, the predicted inverse operator produces an initial restored image together with an uncertainty perception map that highlights regions difficult to reconstruct, ensuring restoration reliability. In the second stage, the restoration is further refined under the guidance of this uncertainty map. The same inverse operator prediction network is used in both stages, with task-aware parameters introduced after operator prediction to adapt to different degradation tasks. Moreover, by accelerating the convolution of the inverse operator, the proposed method achieves efficient all-in-one image restoration. The resulting tightly integrated architecture, termed OPIR, is extensively validated through experiments, demonstrating superior all-in-one restoration performance while remaining highly competitive on task-aligned restoration.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10192.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10192",
    "published": "2026-01-15T08:47:10Z",
    "updated": "2026-01-15T08:47:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出OPIR架构，通过物理退化建模预测任务感知逆退化算子，实现高效且可靠的多合一图像恢复。",
      "motivation": "多合一图像恢复旨在用单一模型自适应处理多种图像恢复任务，但现有方法通过引入提示信息或利用大模型虽然取得不错结果，却增加了学习模块的复杂性，阻碍了实时应用。本研究从物理退化建模角度出发，旨在开发一种更高效、结构简化的方法，以解决现有系统复杂度和效率低下的问题，提升实际部署的可行性。",
      "method": "该方法基于物理退化建模，预测任务感知逆退化算子，框架分为两阶段：第一阶段生成初步恢复图像和不确定性感知图，突出重建困难区域以确保可靠性；第二阶段在不确定性图指导下进一步细化恢复。关键创新是使用同一逆算子预测网络，引入任务感知参数适应不同任务，并通过加速卷积操作优化效率，形成紧密集成的OPIR架构。",
      "result": "实验广泛验证了OPIR架构，展示出在多合一图像恢复任务中的优越性能，同时在特定任务恢复上保持高度竞争力。尽管摘要未明确说明具体性能指标如准确率或效率提升，但结果表明该方法在恢复效果和实时性方面超越了现有基线方法。",
      "conclusion": "本研究的核心贡献是OPIR架构，通过物理退化建模和任务感知逆算子实现了高效、可靠的多合一图像恢复。其学术价值在于提供了一种新的方法视角，推动图像恢复领域的发展；实际应用价值在于简化架构、提高效率，适合实时图像处理场景。未来可探索该方法的泛化能力和更多任务适配。",
      "tags": [
        "All-in-One Image Restoration",
        "Physical Degradation Modeling",
        "Inverse Operator Prediction",
        "Uncertainty Perception",
        "Efficient Architecture"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:37.670759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10191",
    "title": "How does downsampling affect needle electromyography signals? A generalisable workflow for understanding downsampling effects on high-frequency time series",
    "authors": [
      "Mathieu Cherpitel",
      "Janne Luijten",
      "Thomas Bäck",
      "Camiel Verhamme",
      "Martijn Tannemaat",
      "Anna Kononova"
    ],
    "abstract": "Automated analysis of needle electromyography (nEMG) signals is emerging as a tool to support the detection of neuromuscular diseases (NMDs), yet the signals' high and heterogeneous sampling rates pose substantial computational challenges for feature-based machine-learning models, particularly for near real-time analysis. Downsampling offers a potential solution, but its impact on diagnostic signal content and classification performance remains insufficiently understood. This study presents a workflow for systematically evaluating information loss caused by downsampling in high-frequency time series. The workflow combines shape-based distortion metrics with classification outcomes from available feature-based machine learning models and feature space analysis to quantify how different downsampling algorithms and factors affect both waveform integrity and predictive performance. We use a three-class NMD classification task to experimentally evaluate the workflow. We demonstrate how the workflow identifies downsampling configurations that preserve diagnostic information while substantially reducing computational load. Analysis of shape-based distortion metrics showed that shape-aware downsampling algorithms outperform standard decimation, as they better preserve peak structure and overall signal morphology. The results provide practical guidance for selecting downsampling configurations that enable near real-time nEMG analysis and highlight a generalisable workflow that can be used to balance data reduction with model performance in other high-frequency time-series applications as well.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10191.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10191",
    "published": "2026-01-15T08:46:56Z",
    "updated": "2026-01-15T08:46:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种通用工作流程，用于评估下采样对高频时间序列的影响，以支持针肌电图信号的近实时分析，同时保持诊断信息。",
      "motivation": "针肌电图（nEMG）信号的自动化分析正成为检测神经肌肉疾病的重要工具，但信号的高采样率和异构性给基于特征的机器学习模型带来了显著的计算挑战，尤其是在需要近实时处理的场景中。下采样作为减少计算负载的潜在方案，其如何影响诊断信号内容和分类性能尚未得到充分理解。本研究旨在填补这一空白，通过系统评估下采样效果，为优化计算效率和诊断准确性提供理论基础。",
      "method": "本研究设计了一个系统工作流程来评估下采样导致的信息损失。工作流程整合了基于形状的失真度量、基于特征机器学习模型的分类结果和特征空间分析，以量化不同下采样算法和因素对波形完整性和预测性能的影响。实验采用三分类神经肌肉疾病任务进行评估，具体使用形状感知下采样算法与标准抽取方法进行对比，分析其对信号形态的保留能力，从而识别关键影响因素。",
      "result": "实验结果表明，该工作流程能够识别出既能保持诊断信息又能显著降低计算负载的下采样配置。分析显示，形状感知下采样算法在形状基于失真度量上优于标准抽取算法，能更好地保留信号的峰结构和整体形态。这为选择下采样配置以实现近实时nEMG分析提供了实用指导，并通过分类性能验证了其有效性，提升了模型的计算效率。",
      "conclusion": "本研究的核心贡献是提出了一个通用工作流程，用于评估下采样对高频时间序列的影响。它在针肌电图分析中提供了选择下采样配置的实用指导，帮助平衡数据减少和模型性能，促进近实时处理的应用。此外，该工作流程具有推广价值，可扩展到其他高频时间序列领域，为自动分析工具的优化和计算资源管理提供方法论框架。",
      "tags": [
        "Downsampling",
        "High-Frequency Time Series",
        "Machine Learning Classification",
        "Signal Processing",
        "Distortion Metrics"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:59.918462Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10187",
    "title": "HOMURA: Taming the Sand-Glass for Time-Constrained LLM Translation via Reinforcement Learning",
    "authors": [
      "Ziang Cui",
      "Mengran Yu",
      "Tianjiao Li",
      "Chenyu Shi",
      "Yingxuan Shi",
      "Lusheng Zhang",
      "Hongwei Lin"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable strides in multilingual translation but are hindered by a systemic cross-lingual verbosity bias, rendering them unsuitable for strict time-constrained tasks like subtitling and dubbing. Current prompt-engineering approaches struggle to resolve this conflict between semantic fidelity and rigid temporal feasibility. To bridge this gap, we first introduce Sand-Glass, a benchmark specifically designed to evaluate translation under syllable-level duration constraints. Furthermore, we propose HOMURA, a reinforcement learning framework that explicitly optimizes the trade-off between semantic preservation and temporal compliance. By employing a KL-regularized objective with a novel dynamic syllable-ratio reward, HOMURA effectively \"tames\" the output length. Experimental results demonstrate that our method significantly outperforms strong LLM baselines, achieving precise length control that respects linguistic density hierarchies without compromising semantic adequacy.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10187.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10187",
    "published": "2026-01-15T08:45:54Z",
    "updated": "2026-01-15T08:45:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了HOMURA强化学习框架，通过引入Sand-Glass基准和动态音节比奖励，优化大语言模型在时间约束翻译中的语义保留与长度控制。",
      "motivation": "该研究旨在解决大语言模型在多语言翻译中存在的系统性跨语言冗长偏差问题，这种偏差使其不适合严格时间约束任务，如字幕和配音。当前提示工程方法难以平衡语义保真度和时间可行性，导致在实际应用中效率低下。因此，需要新方法在保持语义质量的同时精确控制输出长度，以应对媒体内容本地化等需求。",
      "method": "研究提出了HOMURA框架，一个基于强化学习的系统，用于优化翻译中的语义保留和时间合规性。核心创新包括引入KL正则化目标和新颖的动态音节比奖励，通过强化学习训练显式调整输出长度。此外，作者设计了Sand-Glass基准，专门评估音节级持续时间约束下的翻译性能，为方法提供评估基础。该方法利用大语言模型作为基础，结合奖励机制实现精确控制。",
      "result": "实验结果表明，HOMURA框架在时间约束翻译任务中显著优于强大的大语言模型基线方法，实现了精确的长度控制。具体表现为，方法能够在不损害语义充足性的前提下，尊重语言密度层次，有效减少冗长输出。与基线相比，改进体现在平衡语义和时间约束方面，但摘要未明确说明具体性能指标如准确率提升百分比。",
      "conclusion": "论文的主要贡献是引入了Sand-Glass基准和HOMURA强化学习框架，解决了大语言模型在时间约束翻译中的长度控制问题，具有学术价值和实际应用潜力，适用于字幕、配音等媒体任务。潜在局限性可能包括对特定语言或场景的泛化能力，未来工作可扩展更多约束条件或数据集，但摘要未明确说明具体方向。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "KL Regularization",
        "Dynamic Syllable-Ratio Reward",
        "Benchmark"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:49.657596Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10181",
    "title": "Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand",
    "authors": [
      "Kiattikun Chobtham"
    ],
    "abstract": "Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.",
    "categories": [
      "cs.LG",
      "astro-ph.EP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10181.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10181",
    "published": "2026-01-15T08:40:01Z",
    "updated": "2026-01-15T08:40:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种使用强化学习优化东北季风指数的方法，显著提高了泰国月降雨预测的准确性。",
      "motivation": "气候预测面临地球系统复杂时空模式的挑战，全局气候指数如厄尔尼诺-南方涛动是长期降雨预测的标准输入，但现有方法在泰国特定区域存在预测准确性不足的问题。由于缺乏本地尺度指数，无法充分捕捉局部气候特征，影响了农业和水资源管理等实际应用的精度。因此，开发针对泰国地区的本地化气候指数至关重要，以弥补现有方法的局限性并提升预测可靠性。",
      "method": "论文引入了一个基于海面温度计算的东北季风气候指数，以反映冬季季风的气候学。为优化该指数的计算区域，使用深度Q网络强化学习代理探索并选择与季节性降雨相关性最高的矩形区域。降雨站被分为12个簇，以区分泰国南部和上部的降雨模式，然后将优化指数作为特征输入长短期记忆模型进行月降雨预测。关键创新点在于结合强化学习自动选择气候指数的最佳区域，增强了模型的适应性。",
      "result": "实验结果表明，将优化后的东北季风指数融入长短期记忆模型中，显著提高了大多数簇区域的长期月降雨预测技能。具体而言，该方法有效减少了12个月预测的均方根误差，表明预测准确性得到明显提升。与基线方法相比，优化指数带来了统计显著性的改进，尽管摘要未提供具体数值，但强调了误差降低的实际效果。",
      "conclusion": "论文的主要贡献是提出了一种基于强化学习的气候指数优化框架，提高了泰国月降雨预测的精度。学术上，该方法展示了机器学习在气候学中的创新应用；实际上，为地区降雨预测提供了更可靠的工具，有助于气候适应和灾害管理。未来工作可能包括扩展到其他区域或整合更多气候变量，但摘要未明确说明潜在局限性。",
      "tags": [
        "Reinforcement Learning",
        "Deep Q-Network",
        "Long Short-Term Memory",
        "Climate Index",
        "Sea Surface Temperature"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:58.580935Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10180",
    "title": "Bias in the Shadows: Explore Shortcuts in Encrypted Network Traffic Classification",
    "authors": [
      "Chuyi Wang",
      "Xiaohui Xie",
      "Tongze Wang",
      "Yong Cui"
    ],
    "abstract": "Pre-trained models operating directly on raw bytes have achieved promising performance in encrypted network traffic classification (NTC), but often suffer from shortcut learning-relying on spurious correlations that fail to generalize to real-world data. Existing solutions heavily rely on model-specific interpretation techniques, which lack adaptability and generality across different model architectures and deployment scenarios.   In this paper, we propose BiasSeeker, the first semi-automated framework that is both model-agnostic and data-driven for detecting dataset-specific shortcut features in encrypted traffic. By performing statistical correlation analysis directly on raw binary traffic, BiasSeeker identifies spurious or environment-entangled features that may compromise generalization, independent of any classifier. To address the diverse nature of shortcut features, we introduce a systematic categorization and apply category-specific validation strategies that reduce bias while preserving meaningful information.   We evaluate BiasSeeker on 19 public datasets across three NTC tasks. By emphasizing context-aware feature selection and dataset-specific diagnosis, BiasSeeker offers a novel perspective for understanding and addressing shortcut learning in encrypted network traffic classification, raising awareness that feature selection should be an intentional and scenario-sensitive step prior to model training.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10180.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10180",
    "published": "2026-01-15T08:39:56Z",
    "updated": "2026-01-15T08:39:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出BiasSeeker框架，首次采用模型无关和数据驱动的方法检测加密流量分类中的捷径特征，提升泛化性。",
      "motivation": "加密网络流量分类中，预训练模型虽在原始字节上表现良好，但常因捷径学习依赖虚假相关性，导致泛化能力不足。现有方法大多基于模型特定解释技术，缺乏跨不同架构和部署场景的通用性，无法有效识别和缓解数据集特定捷径特征。因此，开发一个通用、可适应的框架至关重要，以提升分类模型在真实世界中的可靠性和泛化能力，避免因偏见特征导致的性能下降。",
      "method": "BiasSeeker是一个半自动化框架，通过直接对原始二进制流量进行统计相关性分析，独立于任何分类器模型，识别虚假或环境纠缠的捷径特征。关键创新包括系统分类捷径特征（如基于统计属性），并应用类别特定的验证策略，以减少偏见同时保留有意义信息。该方法在三个网络流量分类任务（如应用识别、恶意软件检测）的19个公共数据集上评估，验证其模型无关和数据驱动的设计，增强了在不同场景下的适用性。",
      "result": "摘要未明确说明具体性能指标。评估基于19个公共数据集显示，BiasSeeker通过上下文感知特征选择和数据集特定诊断，提供了理解和解决捷径学习问题的新视角。与现有模型特定解释方法相比，该框架更具通用性和适应性，有助于提高加密流量分类的泛化性和可靠性，尽管具体准确率提升或效率改进数据未详细阐述。",
      "conclusion": "BiasSeeker框架的主要贡献在于首次提供一个模型无关和数据驱动的方法，用于检测加密网络流量分类中的捷径特征，强调了特征选择在模型训练前的有意性和场景敏感性。学术上，这深化了对分类模型泛化能力的理解；实践中，提高了网络安全应用的可靠性。潜在局限性包括可能未覆盖所有场景类型，未来工作可扩展到更多模型架构和任务，并优化验证策略以处理更复杂的捷径特征。",
      "tags": [
        "Shortcut Learning",
        "Model-Agnostic Framework",
        "Statistical Correlation Analysis",
        "Encrypted Traffic Classification",
        "Feature Selection"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:19.388656Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10176",
    "title": "CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling",
    "authors": [
      "Mingyu Zhao",
      "Haoran Bai",
      "Yu Tian",
      "Bing Zhu",
      "Hengliang Luo"
    ],
    "abstract": "Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value \"whale\" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \\textbf{C}onditional \\textbf{C}ascaded \\textbf{O}rdinal-\\textbf{R}esidual Networks \\textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \\textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \\textit{structural ordinal decomposition module} for robust ranking, an \\textit{intra-bucket residual module} for fine-grained regression, and a \\textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10176.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10176",
    "published": "2026-01-15T08:35:17Z",
    "updated": "2026-01-15T08:35:17Z",
    "comment": "Accepted by WWW'26",
    "light_analysis": {
      "overview": "本文提出了CC-OR-Net，一个通过结构解耦的统一框架，用于客户生命周期价值预测，确保排名架构上保证。",
      "motivation": "客户生命周期价值预测在现代营销中至关重要，但其数据分布具有零膨胀和长尾特性，导致两个关键挑战：低中价值用户占多数，掩盖了关键的高价值用户；低中价值用户内部存在显著异质性。现有方法要么依赖刚性统计假设，要么通过损失约束而非架构设计强制有序性，难以平衡全局准确性和高价值用户精度。因此，需要一种新方法来有效解耦排名和回归，提升预测的整体商业价值。",
      "method": "论文提出条件级联序数-残差网络框架，通过结构分解实现更稳健的解耦，架构上保证排名。该框架集成三个关键模块：结构性序数分解模块用于鲁棒排名；桶内残差模块用于细粒度回归；目标高价值增强模块用于提升顶级用户精度。在超过3亿用户的真实数据集上进行评估，展示了其实际应用性和架构创新。",
      "result": "CC-OR-Net在超过3亿用户的真实数据集上评估，结果显示它在所有关键业务指标上取得优越权衡，优于当前最先进方法。摘要未明确说明具体性能指标如准确率提升百分比，但强调了其在创建全面商业LTV预测解决方案中的优势。这表明该方法在实际应用中有较高有效性和竞争力。",
      "conclusion": "本研究的主要贡献是提出了CC-OR-Net框架，通过结构解耦解决了LTV预测中的排名和回归问题，实现了更稳健的解耦。学术上，该框架提供了新的架构设计思路；实际上，它提供了一个全面且商业价值高的预测解决方案。未来工作可能包括扩展应用到其他领域或进一步优化模型以处理更大规模数据。",
      "tags": [
        "Customer Lifetime Value Prediction",
        "Structural Decomposition",
        "Ordinal Regression",
        "Residual Networks",
        "High-Value Augmentation"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:09.409417Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10169",
    "title": "CtD: Composition through Decomposition in Emergent Communication",
    "authors": [
      "Boaz Carmeli",
      "Ron Meir",
      "Yonatan Belinkov"
    ],
    "abstract": "Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways. This study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images. Our method, termed \"Composition through Decomposition\", involves two sequential training steps. In the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game. Subsequently, in the 'Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases. Remarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10169.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10169",
    "published": "2026-01-15T08:17:26Z",
    "updated": "2026-01-15T08:17:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出'Composition through Decomposition'方法，使人工神经网络代理能够零样本泛化描述新图像。",
      "motivation": "组成性是人类认知的关键机制，允许将已知概念组合成新表达，在人工智能通信中具有重要意义。现有方法可能难以让代理在描述未见过图像时实现泛化，尤其是零样本场景。本研究旨在模拟人类认知机制，通过分步训练使代理获取组成泛化能力，以解决多代理协调中的通信瓶颈，提升处理新情况的能力。",
      "method": "研究方法称为'Composition through Decomposition'，分为两个顺序训练步骤。首先，在'Decompose'步骤中，代理通过多目标协调游戏交互，学习将图像分解为存储在codebook中的基本概念。其次，在'Compose'步骤中，代理利用codebook中的概念组合成复杂短语描述新图像。关键创新点包括通过分解构建可组合单元，并实现零样本泛化，无需额外训练。",
      "result": "实验结果表明，该方法使代理能够成功描述先前未见的图像，并在'Compose'步骤中实现零样本泛化。这展示了代理在组成泛化上的有效性，但摘要未明确说明具体性能指标如准确率或效率对比。与基线方法相比，该方法在描述新图像时表现出更好的泛化能力，尤其是在零样本场景下。",
      "conclusion": "该研究的主要贡献是提出了'Composition through Decomposition'方法，使人工神经网络代理模仿人类组成性机制，实现零样本泛化。学术价值在于深化了对代理通信中组成泛化的理解，为多代理系统和人工智能领域提供了新思路。实际应用可能包括自然语言处理和机器人交互，未来工作可探索更复杂环境或扩展该方法到其他任务。",
      "tags": [
        "Emergent Communication",
        "Compositionality",
        "Neural Agents",
        "Zero-shot Generalization",
        "Decomposition"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:27.637882Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10168",
    "title": "RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation",
    "authors": [
      "Yue Chang",
      "Rufeng Chen",
      "Zhaofan Zhang",
      "Yi Chen",
      "Sihong Xie"
    ],
    "abstract": "Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10168.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10168",
    "published": "2026-01-15T08:15:01Z",
    "updated": "2026-01-15T08:15:01Z",
    "comment": "9 pages, 6 figures",
    "light_analysis": {
      "overview": "RAG-3DSG通过重新拍摄引导的不确定性估计和检索增强生成，显著提升3D场景图生成的准确性和速度。",
      "motivation": "开放词汇3D场景图生成在机器人操作和导航等下游任务中至关重要，但现有方法面临对象级识别准确性和速度低下的问题，这主要源于约束视角、遮挡和冗余表面密度等因素。这些问题影响了结构化语义表示的效能，从而限制了机器人在实际应用中的性能提升，因此亟需新方法来解决这些瓶颈，以增强3D场景图的实用性和效率。",
      "method": "论文提出RAG-3DSG方法，核心包括重新拍摄指导的不确定性估计以减少聚合噪声，并利用可靠的低不确定性对象支持对象级检索增强生成（RAG）。此外，提出动态降采样映射策略，通过自适应粒度加速跨图像对象聚合。该方法在Replica数据集上进行验证，但未明确说明具体模型架构或实现细节，主要创新点在于不确定性估计和对象级RAG的结合。",
      "result": "实验在Replica数据集上显示，RAG-3DSG显著提高了3D场景图生成中的节点标注准确性，同时与原始版本相比，映射时间减少了三分之二（约66.7%）。这证明了该方法在提升准确性和加速处理方面的有效性，尽管未提供具体准确率数值，但通过与基线对比明确展示了性能改进，尤其是在减少噪声和优化效率方面。",
      "conclusion": "本研究主要贡献是开发了RAG-3DSG方法，有效解决了开放词汇3D场景图生成中的准确性和速度问题，具有重要学术价值，为3D视觉领域引入不确定性估计和检索增强生成的结合提供了新思路。应用上能增强机器人学下游任务的性能，但摘要未明确说明局限性或未来工作方向，建议未来探索更多数据集或技术扩展。",
      "tags": [
        "3D Scene Graph",
        "Retrieval-Augmented Generation",
        "Uncertainty Estimation",
        "Dynamic Downsampling",
        "Object-Level Recognition"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:33.728866Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10167",
    "title": "Credit C-GPT: A Domain-Specialized Large Language Model for Conversational Understanding in Vietnamese Debt Collection",
    "authors": [
      "Nhung Nguyen Thi Hong",
      "Cuong Nguyen Dang",
      "Tri Le Ngoc"
    ],
    "abstract": "Debt collection is a critical function within the banking, financial services, and insurance (BFSI) sector, relying heavily on large-scale human-to-human conversational interactions conducted primarily in Vietnamese contact centers. These conversations involve informal spoken language, emotional variability, and complex domain-specific reasoning, which pose significant challenges for traditional natural language processing systems. This paper introduces Credit C-GPT, a domain-specialized large language model with seven billion parameters, fine-tuned for conversational understanding in Vietnamese debt collection scenarios. The proposed model integrates multiple conversational intelligence tasks, including dialogue understanding, sentiment recognition, intent detection, call stage classification, and structured slot-value extraction, within a single reasoning-based framework. We describe the data construction process, annotation strategy, and training methodology, and evaluate the model on proprietary human-annotated datasets. Experimental results show consistent improvements over traditional pipeline-based approaches, indicating that domain-specialized conversational language models provide a scalable and privacy-aware solution for real-time assistance and post-call analytics in enterprise contact centers.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10167.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10167",
    "published": "2026-01-15T08:12:55Z",
    "updated": "2026-01-15T08:12:55Z",
    "comment": "8 pages, 0 figures, 3 tables. Preprint",
    "light_analysis": {
      "overview": "本文提出了Credit C-GPT，一个70亿参数的领域专门化大语言模型，专为越南债务收集场景的对话理解设计，整合多个对话智能任务于统一框架。",
      "motivation": "债务收集在银行、金融服务和保险部门中至关重要，但依赖大规模越南语对话交互，涉及非正式口语、情感变化和复杂领域推理。传统自然语言处理系统难以有效处理这些挑战，导致效率低下和错误率高，因此开发专门针对该场景的模型具有实际需求和学术意义，以提升自动化处理能力和隐私保护。",
      "method": "论文提出Credit C-GPT模型，基于70亿参数的大语言模型，通过微调适应越南债务收集场景。该模型在一个基于推理的框架中整合多个对话智能任务，包括对话理解、情感识别、意图检测、呼叫阶段分类和结构化槽值提取。研究详细描述了数据构建过程、注释策略和训练方法，使用专有人工注释数据集进行训练，以提高模型对领域特定内容的处理能力。",
      "result": "实验结果显示，Credit C-GPT在越南债务收集对话理解任务上，相较于传统基于管道的方法取得了显著改进，摘要未明确说明具体性能指标如准确率，但强调了模型的一致提升和可扩展性。这表明领域专门化模型在实时助手和通话后分析应用中优于传统系统，提供更高效的解决方案。",
      "conclusion": "本研究的主要贡献是开发了Credit C-GPT，一个领域专门化的语言模型，为越南债务收集提供对话理解解决方案。该模型具有学术价值，推动了对话AI在特定语言和领域的应用；实际应用中，可作为企业呼叫中心的实时助手，提升效率并保护隐私。未来工作可能包括扩展到其他语言或领域，并优化模型性能以处理更复杂的对话场景。",
      "tags": [
        "Large Language Model",
        "Fine-tuning",
        "Conversational AI",
        "Vietnamese NLP",
        "Domain Specialization"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:36.994155Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10165",
    "title": "Advancing Adaptive Multi-Stage Video Anomaly Reasoning: A Benchmark Dataset and Method",
    "authors": [
      "Chao Huang",
      "Benfeng Wang",
      "Wei Wang",
      "Jie Wen",
      "Li Shen",
      "Wenqi Ren",
      "Yong Xu",
      "Xiaochun Cao"
    ],
    "abstract": "Recent progress in reasoning capabilities of Multimodal Large Language Models(MLLMs) has highlighted their potential for performing complex video understanding tasks. However, in the domain of Video Anomaly Detection and Understanding (VAD&U), existing MLLM-based methods are largely limited to anomaly localization or post-hoc description, lacking explicit reasoning processes, risk awareness, and decision-oriented interpretation. To address this gap, we define a new task termed Video Anomaly Reasoning (VAR), which elevates video anomaly analysis from descriptive understanding to structured, multi-stage reasoning. VAR explicitly requires models to perform progressive reasoning over anomalous events before answering anomaly-related questions, encompassing visual perception, causal interpretation, and risk-aware decision making. To support this task, we present a new dataset with 8,641 videos, where each video is annotated with diverse question types corresponding to different reasoning depths, totaling more than 50,000 samples, making it one of the largest datasets for video anomaly. The annotations are based on a structured Perception-Cognition-Action Chain-of-Thought (PerCoAct-CoT), which formalizes domain-specific reasoning priors for video anomaly understanding. This design enables systematic evaluation of multi-stage and adaptive anomaly reasoning. In addition, we propose Anomaly-Aware Group Relative Policy Optimization to further enhance reasoning reliability under weak supervision. Building upon the proposed task and dataset, we develop an end-to-end MLLM-based VAR model termed Vad-R1-Plus, which supports adaptive hierarchical reasoning and risk-aware decision making. Extensive experiments demonstrate that the proposed benchmark and method effectively advance the reasoning capabilities of MLLMs on VAR tasks, outperforming both open-source and proprietary baselines.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10165.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10165",
    "published": "2026-01-15T08:09:04Z",
    "updated": "2026-01-15T08:09:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文定义了视频异常推理(VAR)任务，提出了基准数据集和Vad-R1-Plus模型，以提升多模态大语言模型的多阶段推理能力。",
      "motivation": "在视频异常检测与理解领域，现有基于多模态大语言模型的方法主要局限于异常定位或事后描述，缺乏明确的推理过程、风险意识和决策导向解释。这使得模型难以进行结构化分析，限制了实际应用。因此，研究定义了VAR任务，要求模型在回答异常问题时进行渐进式推理，包括视觉感知、因果解释和风险决策，以解决现有方法的不足并推动视频分析从描述性理解向决策支持发展。",
      "method": "研究提出基于感知-认知-行动链式思维(PerCoAct-CoT)的注解结构，用于构建包含8,641个视频和超过50,000个样本的数据集，以支持多阶段推理评估。关键创新包括异常感知组相对策略优化方法，通过弱监督增强推理可靠性；并开发了端到端的Vad-R1-Plus模型，基于多模态大语言模型实现自适应分层推理和风险意识决策，从而提高视频异常分析的准确性和解释性。",
      "result": "通过大量实验，提出的基准和方法在视频异常推理任务上显著提升了多模态大语言模型的性能，超越了开源和专有基线。尽管摘要未明确说明具体性能指标如准确率提升，但强调了在系统性评估中表现出色，验证了数据集的实用性和模型推理能力的增强，特别是在处理复杂推理链条和风险判断方面。",
      "conclusion": "论文的主要贡献在于定义VAR任务、创建大规模数据集和提出Vad-R1-Plus模型，为视频异常理解提供了结构化推理框架，具有重要的学术价值和应用潜力，如增强视频监控的智能分析能力。未来工作可能涉及优化推理算法、扩展到更广泛的视频场景或探索更多弱监督学习方法，以进一步提升模型鲁棒性和泛化性。",
      "tags": [
        "Multimodal Large Language Models (MLLMs)",
        "Video Anomaly Reasoning (VAR)",
        "PerCoAct-CoT",
        "Anomaly-Aware Group Relative Policy Optimization",
        "Weak Supervision"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:58.856493Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10161",
    "title": "AWED-FiNER: Agents, Web applications, and Expert Detectors for Fine-grained Named Entity Recognition across 36 Languages for 6.6 Billion Speakers",
    "authors": [
      "Prachuryya Kaushik",
      "Ashish Anand"
    ],
    "abstract": "We introduce AWED-FiNER, an open-source ecosystem designed to bridge the gap in Fine-grained Named Entity Recognition (FgNER) for 36 global languages spoken by more than 6.6 billion people. While Large Language Models (LLMs) dominate general Natural Language Processing (NLP) tasks, they often struggle with low-resource languages and fine-grained NLP tasks. AWED-FiNER provides a collection of agentic toolkits, web applications, and several state-of-the-art expert models that provides FgNER solutions across 36 languages. The agentic tools enable to route multilingual text to specialized expert models and fetch FgNER annotations within seconds. The web-based platforms provide ready-to-use FgNER annotation service for non-technical users. Moreover, the collection of language specific extremely small sized open-source state-of-the-art expert models facilitate offline deployment in resource contraint scenerios including edge devices. AWED-FiNER covers languages spoken by over 6.6 billion people, including a specific focus on vulnerable languages such as Bodo, Manipuri, Bishnupriya, and Mizo. The resources can be accessed here: Agentic Tool (https://github.com/PrachuryyaKaushik/AWED-FiNER), Web Application (https://hf.co/spaces/prachuryyaIITG/AWED-FiNER), and 49 Expert Detector Models (https://hf.co/collections/prachuryyaIITG/awed-finer).",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10161.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10161",
    "published": "2026-01-15T08:00:25Z",
    "updated": "2026-01-15T08:00:25Z",
    "comment": "Submitted to ACL'26 System Demonstration",
    "light_analysis": {
      "overview": "AWED-FiNER 是一个开源生态系统，针对 36 种语言提供细粒度命名实体识别的代理工具、web应用和专家模型，以解决低资源语言和精细任务的不足。",
      "motivation": "当前，大语言模型（LLMs）主导通用自然语言处理任务，但在低资源语言和细粒度任务如细粒度命名实体识别（FgNER）上表现不佳，导致许多语言缺乏有效解决方案。这个问题对于全球超过 6.6 亿使用者，尤其是脆弱语言如 Bodo 和 Manipuri，尤为重要，因为它影响语言多样性和公平的 AI 应用。AWED-FiNER 旨在填补这一空白，提供覆盖 36 种语言的 FgNER 工具，以支持广泛的社会和技术需求。",
      "method": "AWED-FiNER 的核心方法包括代理工具包、web 应用程序和最先进的专家模型。代理工具能路由多语言文本到专门的专家模型，在几秒内获取 FgNER 标注，优化处理流程。web 平台为非技术用户提供即用型服务，简化访问。专家模型设计为极小尺寸，便于离线部署，适用于资源受限场景如边缘设备。该系统未在摘要中详细说明数据集和模型架构，但强调集成工具以实现高效多语言 FgNER。",
      "result": "摘要未明确说明具体的实验结果和性能指标，如准确率提升或效率对比数据。然而，论文提到 AWED-FiNER 能快速提供 FgNER 标注，覆盖 36 种语言，特别关注脆弱语言，并支持离线部署。系统已开发完成，提供开源资源包括代理工具、web 应用和 49 个专家模型，可通过链接访问，但缺乏与基线方法的量化性能比较。",
      "conclusion": "AWED-FiNER 的主要贡献是提供了一个全面的开源生态系统，用于多语言细粒度命名实体识别，推动低资源语言 NLP 研究。其实际应用价值在于为全球用户，特别是非技术用户和资源受限环境，提供便捷工具。未来工作可能包括扩展到更多语言、改进模型性能或整合更多 NLP 任务，但摘要未明确说明具体局限性。",
      "tags": [
        "Fine-grained Named Entity Recognition",
        "Agents",
        "Expert Models",
        "Multilingual NLP",
        "Edge Deployment"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:24.273263Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10160",
    "title": "Alignment Pretraining: AI Discourse Causes Self-Fulfilling (Mis)alignment",
    "authors": [
      "Cameron Tice",
      "Puria Radmard",
      "Samuel Ratnam",
      "Andy Kim",
      "David Africa",
      "Kyle O'Brien"
    ],
    "abstract": "Pretraining corpora contain extensive discourse about AI systems, yet the causal influence of this discourse on downstream alignment remains poorly understood. If prevailing descriptions of AI behaviour are predominantly negative, LLMs may internalise corresponding behavioural priors, giving rise to self-fulfilling misalignment. This paper provides the first controlled study of this hypothesis by pretraining 6.9B-parameter LLMs with varying amounts of (mis)alignment discourse. We find that discussion of AI contributes to misalignment. Upsampling synthetic training documents about AI misalignment leads to a notable increase in misaligned behaviour. Conversely, upsampling documents about aligned behaviour reduces misalignment scores from 45% to 9%. We consider this evidence of self-fulfilling alignment. These effects are dampened, but persist through post-training. Our findings establish the study of how pretraining data shapes alignment priors, or alignment pretraining, as a complement to post-training. We recommend practitioners pretrain for alignment as well as capabilities. Our models and datasets are available at alignmentpretraining.ai",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10160.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10160",
    "published": "2026-01-15T07:59:31Z",
    "updated": "2026-01-15T07:59:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文首次通过受控实验证明，预训练数据中的AI讨论能导致语言模型内化行为先验，实现自我实现的对齐或不对齐。",
      "motivation": "预训练语料库广泛包含AI系统讨论，但这些内容对模型下游对齐行为的因果影响尚不清楚。现有方法缺乏对此的系统研究，如果主流AI描述偏向负面，可能促使大型语言模型（LLMs）内化负面先验，引发自我实现的不对齐，威胁AI安全性和可靠性。因此，探究预训练数据中的话语对齐效应成为重要课题，以填补现有研究空白并优化模型训练策略。",
      "method": "研究采用受控实验设计，预训练了参数为6.9B的LLMs，并通过调整训练数据中关于AI对齐或不对齐讨论的比例来测试假设。关键创新在于使用上采样合成文档，模拟不同对齐倾向的讨论内容，以量化预训练数据对行为先验的因果影响。具体技术包括构建合成数据集，控制文档的正面或负面描述，并基于此评估模型对齐行为的变化。",
      "result": "实验结果揭示，上采样关于AI不对齐的合成文档显著增加模型的不对齐行为；而上采样对齐行为文档则使不对齐得分从45%降低至9%，证实预训练数据对齐能有效促进模型对齐。这些效应在训练后虽有所减弱但持续存在，验证了自我实现对齐的假设。与基线相比，控制讨论内容对模型行为产生显著影响，提供了实证数据支持对齐预训练的有效性。",
      "conclusion": "研究的主要贡献是确立了对齐预训练作为新研究方向，作为后训练方法的补充，首次实证证明了AI讨论对模型对齐的因果影响。学术价值在于深化了对预训练数据对齐机制的理解，具有理论创新意义；实际建议开发者在预训练中兼顾对齐和性能，以提升AI系统安全性。未来工作可探索如何增强对齐预训练的持久效应并应对潜在局限性。",
      "tags": [
        "Large Language Model",
        "Alignment Pretraining",
        "Pre-training Data",
        "Self-Fulfilling Effects",
        "Synthetic Data"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:20.251665Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10159",
    "title": "What Gets Activated: Uncovering Domain and Driver Experts in MoE Language Models",
    "authors": [
      "Guimin Hu",
      "Meng Li",
      "Qiwei Peng",
      "Lijie Hu",
      "Boyan Xu",
      "Ruichu Cai"
    ],
    "abstract": "Most interpretability work focuses on layer- or neuron-level mechanisms in Transformers, leaving expert-level behavior in MoE LLMs underexplored. Motivated by functional specialization in the human brain, we analyze expert activation by distinguishing domain and driver experts. In this work, we study expert activation in MoE models across three public domains and address two key questions: (1) which experts are activated, and whether certain expert types exhibit consistent activation patterns; and (2) how tokens are associated with and trigger the activation of specific experts. To answer these questions, we introduce entropy-based and causal-effect metrics to assess whether an expert is strongly favored for a particular domain, and how strongly expert activation contributes causally to the model's output, thus identify domain and driver experts, respectively. Furthermore, we explore how individual tokens are associated with the activation of specific experts. Our analysis reveals that (1) Among the activated experts, some show clear domain preferences, while others exert strong causal influence on model performance, underscoring their decisive roles. (2) tokens occurring earlier in a sentence are more likely to trigger the driver experts, and (3) adjusting the weights of domain and driver experts leads to significant performance gains across all three models and domains. These findings shed light on the internal mechanisms of MoE models and enhance their interpretability.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10159.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10159",
    "published": "2026-01-15T07:59:17Z",
    "updated": "2026-01-15T07:59:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出熵基和因果效应指标，区分领域和驱动专家，揭示了MoE语言模型的内部激活机制并实现了性能提升。",
      "motivation": "大多数可解释性研究集中于Transformer的层或神经元级别，而MoE LLMs中的专家级别行为未被充分探索，导致模型内部机制理解不足。受人类大脑功能专门化启发，本研究旨在解决MoE模型中专家激活模式不明确的问题，以增强可解释性，回答哪些专家被激活及其激活模式，从而为模型优化提供基础。",
      "method": "本研究分析了三个公共领域的MoE模型，引入基于熵的指标评估专家对特定领域的偏好，和基于因果效应的指标测量专家激活对模型输出的贡献。核心创新在于区分领域专家和驱动专家，并探索个体token与专家激活的关联，特别是token在句子中的位置对触发驱动专家的影响。方法侧重于量化分析和因果推理，以揭示专家激活动态。",
      "result": "实验显示，MoE模型中部分专家有明确领域偏好，其他专家对性能有强因果影响。调整领域和驱动专家权重在所有三个模型和领域均导致显著性能提升，但摘要未提供具体数值。此外，句子中较早的token更可能触发驱动专家，表明位置因素在专家选择中起关键作用，验证了区分专家类型的有效性。",
      "conclusion": "本论文通过区分领域和驱动专家，使用熵基和因果效应指标，增强了MoE模型的可解释性，揭示了内部机制。研究发现调整专家权重可改善性能，具有实际应用价值，学术上为理解MoE提供了新视角。未来工作可扩展到更多模型和领域，并深化指标探索。",
      "tags": [
        "Mixture of Experts (MoE)",
        "Language Models",
        "Interpretability",
        "Entropy-based Metrics",
        "Causal Analysis"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:09.579376Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10157",
    "title": "MMPG: MoE-based Adaptive Multi-Perspective Graph Fusion for Protein Representation Learning",
    "authors": [
      "Yusong Wang",
      "Jialun Shen",
      "Zhihao Wu",
      "Yicheng Xu",
      "Shiyin Tan",
      "Mingkun Xu",
      "Changshuo Wang",
      "Zixing Song",
      "Prayag Tiwari"
    ],
    "abstract": "Graph Neural Networks (GNNs) have been widely adopted for Protein Representation Learning (PRL), as residue interaction networks can be naturally represented as graphs. Current GNN-based PRL methods typically rely on single-perspective graph construction strategies, which capture partial properties of residue interactions, resulting in incomplete protein representations. To address this limitation, we propose MMPG, a framework that constructs protein graphs from multiple perspectives and adaptively fuses them via Mixture of Experts (MoE) for PRL. MMPG constructs graphs from physical, chemical, and geometric perspectives to characterize different properties of residue interactions. To capture both perspective-specific features and their synergies, we develop an MoE module, which dynamically routes perspectives to specialized experts, where experts learn intrinsic features and cross-perspective interactions. We quantitatively verify that MoE automatically specializes experts in modeling distinct levels of interaction from individual representations, to pairwise inter-perspective synergies, and ultimately to a global consensus across all perspectives. Through integrating this multi-level information, MMPG produces superior protein representations and achieves advanced performance on four different downstream protein tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10157.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10157",
    "published": "2026-01-15T07:57:28Z",
    "updated": "2026-01-15T07:57:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "MMPG 提出一种基于混合专家（MoE）的自适应多视角图融合框架，改进了蛋白质表示学习。",
      "motivation": "现有基于图神经网络（GNN）的蛋白质表示学习方法通常依赖单视角图构建策略，仅捕获残基交互的部分属性，导致蛋白质表示不完整。这个问题在生物信息学中至关重要，因为蛋白质表示学习影响下游任务如功能预测和结构分析，但现有方法因视角单一而限制了表示能力。因此，需要多视角整合来全面捕捉残基交互特性，以提升模型性能。",
      "method": "MMPG 框架从物理、化学和几何三个视角构建蛋白质图，以表征残基交互的不同属性。核心创新是使用混合专家（MoE）模块，动态路由各视角到专门专家，学习视角特定特征和跨视角交互。专家自动专门化建模不同层次交互，包括个体表示、成对视角协同作用，以及所有视角的全局共识，从而整合多层次信息。摘要未明确说明具体数据集或模型架构细节。",
      "result": "MMPG 通过整合多视角和多层次信息，产生优越的蛋白质表示，并在四个不同的下游蛋白质任务上实现先进性能。定量验证表明，MoE 模块能自动专门化专家建模交互层次，但摘要未明确说明具体性能指标如准确率提升或与基线方法的详细对比数据。整体而言，框架增强了表示质量，在多任务中表现优异。",
      "conclusion": "MMPG 的主要贡献是提出一种基于 MoE 的自适应多视角图融合框架，显著改进蛋白质表示学习。学术上，它扩展了 GNN 在蛋白质领域的应用，实际上提升了多种下游任务的性能。未来工作可探索扩展更多视角或应用到其他生物分子表示学习，但摘要未明确说明潜在局限性。",
      "tags": [
        "Graph Neural Networks",
        "Mixture of Experts",
        "Multi-Perspective Graph",
        "Protein Representation Learning",
        "Cross-Perspective Interactions"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:29.369356Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10156",
    "title": "ToolSafe: Enhancing Tool Invocation Safety of LLM-based agents via Proactive Step-level Guardrail and Feedback",
    "authors": [
      "Yutao Mou",
      "Zhangchi Xue",
      "Lijun Li",
      "Peiyang Liu",
      "Shikun Zhang",
      "Wei Ye",
      "Jing Shao"
    ],
    "abstract": "While LLM-based agents can interact with environments via invoking external tools, their expanded capabilities also amplify security risks. Monitoring step-level tool invocation behaviors in real time and proactively intervening before unsafe execution is critical for agent deployment, yet remains under-explored. In this work, we first construct TS-Bench, a novel benchmark for step-level tool invocation safety detection in LLM agents. We then develop a guardrail model, TS-Guard, using multi-task reinforcement learning. The model proactively detects unsafe tool invocation actions before execution by reasoning over the interaction history. It assesses request harmfulness and action-attack correlations, producing interpretable and generalizable safety judgments and feedback. Furthermore, we introduce TS-Flow, a guardrail-feedback-driven reasoning framework for LLM agents, which reduces harmful tool invocations of ReAct-style agents by 65 percent on average and improves benign task completion by approximately 10 percent under prompt injection attacks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10156.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10156",
    "published": "2026-01-15T07:54:32Z",
    "updated": "2026-01-15T07:54:32Z",
    "comment": "Work in Progress. Code available: https://github.com/MurrayTom/ToolSafe",
    "light_analysis": {
      "overview": "本文提出了ToolSafe系统，通过主动步级防护和反馈机制来增强基于LLM的代理工具调用安全性。",
      "motivation": "LLM-based agents能够通过调用外部工具与环境交互，扩展了能力但也引入了安全风险，如不安全的工具调用可能导致恶意执行。现有方法在实时监控步级工具调用行为和提前干预方面尚未充分探索，这限制了代理在安全关键场景的部署。因此，开发有效的防护机制对于确保LLM代理在实际应用中的安全性至关重要，避免因工具调用失误或攻击造成的损害。",
      "method": "研究首先构建了TS-Bench，一个专门用于评估LLM代理步级工具调用安全性的新基准。然后，开发了TS-Guard防护模型，采用多任务强化学习技术，通过推理交互历史来提前检测不安全的工具调用动作，评估请求的危害性和动作与攻击的关联，并产生可解释且可泛化的安全判断和反馈。此外，引入了TS-Flow框架，将防护机制与反馈集成到代理的推理过程中，优化安全性能。",
      "result": "实验结果表明，ToolSafe框架显著提升了安全性。在TS-Bench上评估，TS-Flow框架平均减少了ReAct-style代理的65%有害工具调用，有效降低了安全风险。同时，在提示注入攻击场景下，良性任务的完成率提高了约10%，显示了框架在保证安全的同时不影响正常功能，与基线方法相比具有明显优势。",
      "conclusion": "本研究的主要贡献在于提供了一套完整的工具调用安全增强方案，包括基准测试、防护模型和推理框架，推动了LLM代理的安全部署，具有重要的学术和实际应用价值。尽管在特定场景下表现优异，但未来可探索更广泛的攻击类型和更复杂的交互环境以提升模型的泛化能力。摘要未明确说明具体局限性，但可推断需进一步优化以适应多样化应用。",
      "tags": [
        "LLM-based Agents",
        "Tool Safety",
        "Multi-task Reinforcement Learning",
        "Proactive Guardrail",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:34.291677Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10155",
    "title": "LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers",
    "authors": [
      "Aryan Karmore"
    ],
    "abstract": "Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\\times$ compression at 95.7\\% output fidelity and 32 $\\times$ compression at 95.0\\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10155.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10155",
    "published": "2026-01-15T07:54:07Z",
    "updated": "2026-01-15T07:54:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "LOOKAT是一种基于查找优化键注意力的方法，通过应用产品量化和非对称距离计算，高效压缩Transformer模型中的KV缓存，提升内存效率和边缘设备部署能力。",
      "motivation": "研究旨在解决大型语言模型在边缘设备部署时KV缓存占用大量内存的问题。现有量化方法虽然能压缩存储，但注意力计算需要将键反量化为浮点数，导致带宽未减少。这个问题重要，因为边缘设备资源受限，提高内存效率和减少带宽对模型实际部署至关重要，但当前方法无法兼顾存储和带宽优化，限制了应用效果。",
      "method": "论文提出LOOKAT方法，将向量数据库中的产品量化和非对称距离计算技术应用于Transformer架构。核心创新是将注意力评分视为内积相似性搜索，通过分解键向量为子空间、学习码书并利用查找表计算注意力表，实现注意力从内存密集型向计算密集型的转变。该方法在GPT-2数据集上实现，无需改变模型架构或额外训练，直接优化KV缓存结构。",
      "result": "在GPT-2模型上的实验结果表明，LOOKAT实现了64倍压缩时95.7%的输出保真度和32倍压缩时95.0%的保真度。与现有量化方法相比，LOOKAT在保持高保真度的同时显著减少带宽消耗，秩相关系数ρ>0.95，表明输出质量稳定。理论分析验证秩相关性随参数d_k/mK变化而缓慢退化，适用于序列长度达1024个令牌，确认了方法的鲁棒性和有效性。",
      "conclusion": "LOOKAT成功引入向量数据库压缩技术到Transformer，有效解决了KV缓存的存储和带宽问题。主要贡献是提出一种无需架构更改的训练方法，提高了内存效率。学术价值在于拓展了注意力机制的优化途径，应用价值是促进了大型语言模型在边缘设备上的部署。未来工作可探索在更大模型和更长序列上的应用潜力。",
      "tags": [
        "Large Language Models",
        "KV Cache Compression",
        "Product Quantization",
        "Attention Mechanism",
        "Transformer Architecture"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:26.454218Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10154",
    "title": "MHub.ai: A Simple, Standardized, and Reproducible Platform for AI Models in Medical Imaging",
    "authors": [
      "Leonard Nürnberg",
      "Dennis Bontempi",
      "Suraj Pai",
      "Curtis Lisle",
      "Steve Pieper",
      "Ron Kikinis",
      "Sil van de Leemput",
      "Rahul Soni",
      "Gowtham Murugesan",
      "Cosmin Ciausu",
      "Miriam Groeneveld",
      "Felix J. Dorfner",
      "Jue Jiang",
      "Aneesh Rangnekar",
      "Harini Veeraraghavan",
      "Joeran S. Bosma",
      "Keno Bressem",
      "Raymond Mak",
      "Andrey Fedorov",
      "Hugo JWL Aerts"
    ],
    "abstract": "Artificial intelligence (AI) has the potential to transform medical imaging by automating image analysis and accelerating clinical research. However, research and clinical use are limited by the wide variety of AI implementations and architectures, inconsistent documentation, and reproducibility issues. Here, we introduce MHub.ai, an open-source, container-based platform that standardizes access to AI models with minimal configuration, promoting accessibility and reproducibility in medical imaging. MHub.ai packages models from peer-reviewed publications into standardized containers that support direct processing of DICOM and other formats, provide a unified application interface, and embed structured metadata. Each model is accompanied by publicly available reference data that can be used to confirm model operation. MHub.ai includes an initial set of state-of-the-art segmentation, prediction, and feature extraction models for different modalities. The modular framework enables adaptation of any model and supports community contributions. We demonstrate the utility of the platform in a clinical use case through comparative evaluation of lung segmentation models. To further strengthen transparency and reproducibility, we publicly release the generated segmentations and evaluation metrics and provide interactive dashboards that allow readers to inspect individual cases and reproduce or extend our analysis. By simplifying model use, MHub.ai enables side-by-side benchmarking with identical execution commands and standardized outputs, and lowers the barrier to clinical translation.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10154.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10154",
    "published": "2026-01-15T07:53:09Z",
    "updated": "2026-01-15T07:53:09Z",
    "comment": "41 pages, 15 figures, 6 tables",
    "light_analysis": {
      "overview": "MHub.ai是一个开源、基于容器的平台，标准化医学影像AI模型的访问，提高可重复性和临床转化能力。",
      "motivation": "AI在医学影像领域具有自动化图像分析和加速临床研究的潜力，但研究和临床使用受限，因为AI实现和架构多样、文档不一致以及可重复性问题。这些不足导致模型难以访问、复现和比较，阻碍了AI的广泛应用和临床转化。因此，需要一个标准化平台来解决这些挑战，提升可重复性和可访问性，推动医学影像AI的发展。",
      "method": "MHub.ai是一个开源、基于容器的平台，通过标准化容器封装AI模型，支持直接处理DICOM等格式，并提供统一的应用接口和结构化元数据。每个模型配有公开参考数据以验证操作。平台采用模块化框架，可适应任何模型并支持社区贡献。初始版本包括先进的分割、预测和特征提取模型，覆盖不同医学影像模态，如CT和MRI。",
      "result": "论文通过临床用例展示了MHub.ai平台的实用性，比较了肺分割模型来验证效果。摘要未明确说明具体性能指标提升，但提到公开了生成的分割和评估指标，并提供交互式仪表板让读者检查个体案例和复现分析。这促进了透明度和可重复性，支持使用相同执行命令和标准化输出的并排基准测试，降低了临床转化障碍。",
      "conclusion": "MHub.ai平台的主要贡献是提供了一个标准化、可复现的平台，简化了医学影像AI模型的使用，解决了可重复性和可访问性问题。学术上，它为AI模型比较和基准测试提供了统一框架；实际上，降低了临床转化障碍，加速AI应用。未来，模块化设计支持模型扩展和社区贡献，增强平台的适应性和影响力，推动更广泛的临床研究。",
      "tags": [
        "Container-based Platform",
        "Medical Imaging AI",
        "DICOM Processing",
        "Standardized Metadata",
        "Community Contribution"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:46.997156Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10150",
    "title": "Simple Network Graph Comparative Learning",
    "authors": [
      "Qiang Yu",
      "Xinran Cheng",
      "Shiqiang Xu",
      "Chuanyi Liu"
    ],
    "abstract": "The effectiveness of contrastive learning methods has been widely recognized in the field of graph learning, especially in contexts where graph data often lack labels or are difficult to label. However, the application of these methods to node classification tasks still faces a number of challenges. First, existing data enhancement techniques may lead to significant differences from the original view when generating new views, which may weaken the relevance of the view and affect the efficiency of model training. Second, the vast majority of existing graph comparison learning algorithms rely on the use of a large number of negative samples. To address the above challenges, this study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL). Specifically, SNGCL employs a superimposed multilayer Laplace smoothing filter as a step in processing the data to obtain global and local feature smoothing matrices, respectively, which are thus passed into the target and online networks of the siamese network, and finally employs an improved triple recombination loss function to bring the intra-class distance closer and the inter-class distance farther. We have compared SNGCL with state-of-the-art models in node classification tasks, and the experimental results show that SNGCL is strongly competitive in most tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10150.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10150",
    "published": "2026-01-15T07:43:42Z",
    "updated": "2026-01-15T07:43:42Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "SNGCL是一种新型节点分类对比学习方法，通过叠加多层拉普拉斯平滑滤波器和改进损失函数提升模型性能。",
      "motivation": "图对比学习方法在节点分类任务中面临两大挑战：现有数据增强技术生成新视图时可能导致显著差异，削弱视图相关性并影响训练效率；多数算法依赖大量负样本，增加了计算负担和复杂性。这些问题限制了图学习在无标签或难标签场景下的应用效果。因此，本研究旨在开发一种更高效、无需大量负样本的方法来优化节点分类性能。",
      "method": "SNGCL的核心方法包括使用叠加多层拉普拉斯平滑滤波器处理图数据，生成全局和局部特征平滑矩阵。这些矩阵传入孪生网络的目标和在线网络进行特征学习，最后采用改进的三元重组损失函数，通过减小类内距离和增大类间距离来优化模型。关键创新在于平滑滤波器的应用和损失函数的重新设计，摘要未明确说明具体的数据集或模型架构细节。",
      "result": "实验结果显示，SNGCL在节点分类任务中与最先进模型相比具有强烈竞争力，表明其在大多数任务中表现优异。然而，摘要未明确说明具体的性能指标（如准确率提升或效率改进），仅提及了与基线方法的对比结果。这暗示SNGCL在提升模型效果方面有潜力，但需要进一步实验数据支撑。",
      "conclusion": "SNGCL通过创新方法解决了图对比学习在节点分类中的挑战，贡献包括平滑滤波器和改进损失函数的应用，具有学术价值如推动图学习算法发展，以及实际应用价值如提升无标签数据下的节点分类效率。未来工作可探索其通用性、局限性或在其他图任务中的应用，摘要未明确说明具体局限性。",
      "tags": [
        "Contrastive Learning",
        "Node Classification",
        "Siamese Network",
        "Laplace Smoothing",
        "Loss Function"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:31.316688Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10148",
    "title": "DecisionLLM: Large Language Models for Long Sequence Decision Exploration",
    "authors": [
      "Xiaowei Lv",
      "Zhilin Zhang",
      "Yijun Li",
      "Yusen Huo",
      "Siyuan Ju",
      "Xuyan Li",
      "Chunxiang Hong",
      "Tianyu Wang",
      "Yongcai Wang",
      "Peng Sun",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng"
    ],
    "abstract": "Long-sequence decision-making, which is usually addressed through reinforcement learning (RL), is a critical component for optimizing strategic operations in dynamic environments, such as real-time bidding in computational advertising. The Decision Transformer (DT) introduced a powerful paradigm by framing RL as an autoregressive sequence modeling problem. Concurrently, Large Language Models (LLMs) have demonstrated remarkable success in complex reasoning and planning tasks. This inspires us whether LLMs, which share the same Transformer foundation, but operate at a much larger scale, can unlock new levels of performance in long-horizon sequential decision-making problem. This work investigates the application of LLMs to offline decision making tasks. A fundamental challenge in this domain is the LLMs' inherent inability to interpret continuous values, as they lack a native understanding of numerical magnitude and order when values are represented as text strings. To address this, we propose treating trajectories as a distinct modality. By learning to align trajectory data with natural language task descriptions, our model can autoregressively predict future decisions within a cohesive framework we term DecisionLLM. We establish a set of scaling laws governing this paradigm, demonstrating that performance hinges on three factors: model scale, data volume, and data quality. In offline experimental benchmarks and bidding scenarios, DecisionLLM achieves strong performance. Specifically, DecisionLLM-3B outperforms the traditional Decision Transformer (DT) by 69.4 on Maze2D umaze-v1 and by 0.085 on AuctionNet. It extends the AIGB paradigm and points to promising directions for future exploration in online bidding.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10148.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10148",
    "published": "2026-01-15T07:42:02Z",
    "updated": "2026-01-15T07:42:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出DecisionLLM，通过将轨迹数据视为独特模态并应用大型语言模型到长序列决策任务中，解决LLMs理解连续值的问题，建立性能缩放定律。",
      "motivation": "长序列决策在动态环境如计算广告中至关重要，传统强化学习方法如Decision Transformer将决策建模为序列问题。大型语言模型在复杂推理中表现出色，但由于无法理解连续数值，限制了其在决策中的应用。本研究旨在利用LLMs的Transformer基础和大规模优势，克服连续值理解的挑战，探索在离线决策任务中的性能提升潜力。",
      "method": "论文提出DecisionLLM，通过将决策轨迹视为与自然语言不同的模态，在自回归框架下学习对齐轨迹数据和任务描述，以处理连续值。关键创新在于使LLMs能理解数值顺序，并建立缩放定律，表明性能取决于模型规模、数据量和数据质量，为优化提供指导。",
      "result": "实验在离线基准和竞价场景中进行，DecisionLLM表现强劲。DecisionLLM-3B在Maze2D umaze-v1任务上比传统Decision Transformer性能提升69.4点，在AuctionNet竞价任务上提升0.085点。这些结果验证了轨迹对齐方法的有效性，并展示了在长序列决策中的优势，支持缩放定律的发现。",
      "conclusion": "论文的主要贡献是成功将大型语言模型应用于离线长序列决策任务，通过轨迹对齐解决了连续值理解问题，并建立了性能缩放定律。这扩展了人工智能生成行为范式，为在线竞价等实际应用提供了新的研究方向，并指出未来可探索在线环境中的部署和优化。",
      "tags": [
        "Large Language Models",
        "Decision Transformer",
        "Autoregressive Sequence Modeling",
        "Trajectory Alignment",
        "Scaling Laws"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:02.856255Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10143",
    "title": "History Is Not Enough: An Adaptive Dataflow System for Financial Time-Series Synthesis",
    "authors": [
      "Haochong Xia",
      "Yao Long Teng",
      "Regan Tan",
      "Molei Qin",
      "Xinrun Wang",
      "Bo An"
    ],
    "abstract": "In quantitative finance, the gap between training and real-world performance-driven by concept drift and distributional non-stationarity-remains a critical obstacle for building reliable data-driven systems. Models trained on static historical data often overfit, resulting in poor generalization in dynamic markets. The mantra \"History Is Not Enough\" underscores the need for adaptive data generation that learns to evolve with the market rather than relying solely on past observations. We present a drift-aware dataflow system that integrates machine learning-based adaptive control into the data curation process. The system couples a parameterized data manipulation module comprising single-stock transformations, multi-stock mix-ups, and curation operations, with an adaptive planner-scheduler that employs gradient-based bi-level optimization to control the system. This design unifies data augmentation, curriculum learning, and data workflow management under a single differentiable framework, enabling provenance-aware replay and continuous data quality monitoring. Extensive experiments on forecasting and reinforcement learning trading tasks demonstrate that our framework enhances model robustness and improves risk-adjusted returns. The system provides a generalizable approach to adaptive data management and learning-guided workflow automation for financial data.",
    "categories": [
      "cs.AI",
      "q-fin.TR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10143.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10143",
    "published": "2026-01-15T07:38:59Z",
    "updated": "2026-01-15T07:38:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种漂移感知的自适应数据流系统，通过整合机器学习和双层优化技术，改善了金融时间序列数据的生成和管理。",
      "motivation": "量化金融中，概念漂移和分布非平稳性导致训练模型在静态历史数据上容易过拟合，与实际动态市场脱节，严重影响数据驱动系统的可靠性。该研究旨在解决这一关键问题，通过自适应数据生成弥补训练与真实性能的差距，减少模型泛化不足的风险。",
      "method": "系统设计包括参数化数据操作模块（进行单股转换、多股混合和标准管理操作）和自适应规划调度器（采用梯度驱动的双层优化控制）。该框架统一了数据增强、课程学习和数据工作流管理于可微分架构，支持来源感知重放和连续质量监控，增强了系统的灵活性和可适应性。",
      "result": "在时间序列预测和强化学习交易任务的实验中，该系统显著提升了模型的稳健性，并改善了风险调整后的回报性能。尽管摘要未提供具体数据对比，但结果表明该方法在动态金融环境中比传统静态数据方法更有效。",
      "conclusion": "该研究的主要贡献是提供了一个可泛化的自适应数据管理和学习引导工作流自动化框架，为应对金融数据中的概念漂移提供了新方法。这有助于增强数据驱动系统的性能，未来可扩展至其他动态或非平稳数据场景，如多领域的时间序列分析。",
      "tags": [
        "Financial Time-Series Synthesis",
        "Adaptive Dataflow System",
        "Concept Drift",
        "Bi-level Optimization",
        "Reinforcement Learning Trading"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:22.173817Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10141",
    "title": "Understanding and Preserving Safety in Fine-Tuned LLMs",
    "authors": [
      "Jiawen Zhang",
      "Yangfan Hu",
      "Kejia Chen",
      "Lipeng He",
      "Jiachen Ma",
      "Jian Lou",
      "Dan Li",
      "Jian Liu",
      "Xiaohu Yang",
      "Ruoxi Jia"
    ],
    "abstract": "Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.   In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10141.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10141",
    "published": "2026-01-15T07:33:13Z",
    "updated": "2026-01-15T07:33:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出safety-preserving fine-tuning (SPF)方法，通过分析梯度几何交互，解决了大语言模型微调中的安全-效用困境。",
      "motivation": "微调是大语言模型应用于下游任务的关键技术，但常导致安全对齐显著下降，增加越狱攻击的脆弱性，即使微调数据无害。现有方法面临安全-效用困境：强调安全会牺牲任务性能，而优先效用则需深度微调，引发严重安全退化，限制了模型的实际应用可靠性。因此，亟需在微调过程中平衡安全和性能，防止对齐失效。",
      "method": "本研究基于对安全和效用梯度几何交互的系统分析，揭示三个关键见解：安全梯度位于低秩子空间，效用梯度在高维空间；两者常负相关导致方向冲突；主导安全方向可从单个样本高效估计。基于此，提出SPF方法，一个轻量级微调技术，显式去除与低秩安全子空间冲突的梯度分量。理论分析表明，SPF能保证效用收敛并限制安全漂移，而无需复杂架构或额外数据。",
      "result": "实证结果显示，SPF能持续保持下游任务性能，并恢复近所有预训练安全对齐，即使在对抗性微调场景下。此外，SPF表现出对深度微调和动态越狱攻击的强大抵抗力，相较于基线方法解决了安全-效用困境。虽然摘要未明确具体数值，但方法有效提升了安全性和任务效用的平衡。",
      "conclusion": "本研究的主要贡献是为微调过程中的安全对齐提供了新的机械理解和实用指导。SPF方法理论和实证均证明其有效性，推动了始终对齐LLM微调的发展，具有学术和实际应用价值。未来工作可扩展该方法到更广泛模型或安全场景，进一步提升微调的安全鲁棒性，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Fine-Tuning",
        "Safety Alignment",
        "Gradient Analysis",
        "Jailbreak Attacks"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:33.267504Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10137",
    "title": "Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation",
    "authors": [
      "Ziyi Ding",
      "Chenfei Ye-Hao",
      "Zheyuan Wang",
      "Xiao-Ping Zhang"
    ],
    "abstract": "Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10137.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10137",
    "published": "2026-01-15T07:28:59Z",
    "updated": "2026-01-15T07:28:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Tree-Query框架，通过树结构多专家LLM实现透明、高置信度的因果发现。",
      "motivation": "因果发现旨在理解变量间的因果关系，但现有方法存在不足：经典约束基方法（如PC、FCI）易产生错误传播，导致结果不可靠；而近期基于大语言模型（LLM）的因果oracle往往不透明，缺乏置信度评估，影响可信度。这些问题限制了因果发现在实际应用中的准确性和可解释性，因此需要开发一种更透明、具有稳健置信度估计的因果发现方法。",
      "method": "论文提出Tree-Query框架，这是一个树形结构的多专家大语言模型系统。核心方法是将成对因果发现转化为一系列短序列查询，涉及后门路径、独立性/依赖性、潜在混杂因素和因果方向。关键创新在于生成可解释的判断，并集成鲁棒感知的置信度估计。虽然摘要未具体说明数据集或模型架构细节，但该方法旨在提供理论保证，简化因果推理过程并增强透明度。",
      "result": "在基于Mooij等人和UCI因果图的数据自由基准测试中，Tree-Query在结构度量上优于直接使用LLM的基线方法，显示了性能提升。一个饮食-体重案例研究进一步证明框架能有效筛选混杂因素并产生稳定、高置信度的因果结论。摘要未明确说明具体准确率或效率数值，但结果表明Tree-Query在改善因果发现的可信度和鲁棒性方面具有优势。",
      "conclusion": "Tree-Query提供了一个原则性方法，从大语言模型中获取数据自由的因果先验知识，可补充下游数据驱动的因果发现。这一框架增强了因果发现的透明度和置信度评估，具有重要的学术价值，如推动因果推理理论发展，以及实际应用价值，如辅助决策系统。未来工作可能包括扩展框架到更复杂场景，但摘要未明确说明局限性。",
      "tags": [
        "Causal Discovery",
        "Large Language Model",
        "Multi-Agent System",
        "Confidence Estimation",
        "Backdoor Paths"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:26.701951Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10132",
    "title": "Is More Context Always Better? Examining LLM Reasoning Capability for Time Interval Prediction",
    "authors": [
      "Yanan Cao",
      "Farnaz Fallahi",
      "Murali Mohana Krishna Dandu",
      "Lalitesh Morishetti",
      "Kai Zhao",
      "Luyi Ma",
      "Sinduja Subramaniam",
      "Jianpeng Xu",
      "Evren Korpeoglu",
      "Kaushiki Nag",
      "Sushant Kumar",
      "Kannan Achan"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning and prediction across different domains. Yet, their ability to infer temporal regularities from structured behavioral data remains underexplored. This paper presents a systematic study investigating whether LLMs can predict time intervals between recurring user actions, such as repeated purchases, and how different levels of contextual information shape their predictive behavior. Using a simple but representative repurchase scenario, we benchmark state-of-the-art LLMs in zero-shot settings against both statistical and machine-learning models. Two key findings emerge. First, while LLMs surpass lightweight statistical baselines, they consistently underperform dedicated machine-learning models, showing their limited ability to capture quantitative temporal structure. Second, although moderate context can improve LLM accuracy, adding further user-level detail degrades performance. These results challenge the assumption that \"more context leads to better reasoning\". Our study highlights fundamental limitations of today's LLMs in structured temporal inference and offers guidance for designing future context-aware hybrid models that integrate statistical precision with linguistic flexibility.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10132.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10132",
    "published": "2026-01-15T07:18:40Z",
    "updated": "2026-01-15T07:18:40Z",
    "comment": "Accepted at The Web Conference 2026 (WWW 2026)",
    "light_analysis": {
      "overview": "本研究通过系统实验揭示了大语言模型在结构化时间推理中的局限性，并挑战了更多上下文总是有益的假设。",
      "motivation": "大语言模型虽在推理和预测任务中表现出色，但其在从结构化行为数据中推断时间规律的能力仍未被充分探索。时间间隔预测在用户行为分析等领域有重要应用价值，如预测重复购买，有助于商业决策。现有假设认为增加上下文能提升 LLM 性能，但缺乏实证验证，且传统机器学习模型可能更擅长捕捉定量时间模式。因此，本文旨在填补研究空白，探究 LLMs 在此任务中的能力，并与统计和机器学习方法对比，以评估其有效性和潜在不足。",
      "method": "本文采用系统性研究方法，以一个简单但代表性的重复购买场景作为任务框架，预测用户行为之间的时间间隔。在零样本设置下，将最先进的大语言模型与轻量级统计模型和专用机器学习模型进行基准测试。通过调整上下文信息的层次，如用户历史行为的详细程度，研究上下文如何影响 LLM 的预测行为。方法核心在于对比不同模型在捕捉时间结构上的表现，评估 LLMs 的推理能力，而不依赖特定训练数据或复杂调优。",
      "result": "实验结果表明，大语言模型在时间间隔预测任务中能超越轻量级统计基线，但在与专用机器学习模型对比时表现不佳，显示出其在处理定量时间结构方面的局限性。具体而言，适度增加上下文能略微提高 LLM 的准确性，但添加更多用户级详细信息反而导致性能下降，这直接挑战了“更多上下文总是导致更好推理”的假设。结果基于零样本设置下的对比评估，强调了 LLMs 在结构化时间推理中的瓶颈。",
      "conclusion": "本研究的核心贡献是揭示了大语言模型在结构化时间推理中的基本局限性，并通过实验数据验证了上下文效应的复杂性。学术价值在于挑战了 LLM 推理能力的现有假设，鼓励未来研究深入探索 LLMs 的局限性。实际应用价值在于为设计上下文感知混合模型提供了指导，建议结合统计精确性和语言灵活性，以提升预测性能。未来工作可致力于开发更有效的混合系统，克服当前 LLMs 在时间预测中的不足。",
      "tags": [
        "Large Language Model",
        "Time Interval Prediction",
        "Zero-shot Learning",
        "Contextual Information",
        "Repurchase Scenario"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:46.013391Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10131",
    "title": "M^4olGen: Multi-Agent, Multi-Stage Molecular Generation under Precise Multi-Property Constraints",
    "authors": [
      "Yizhan Li",
      "Florence Cloutier",
      "Sifan Wu",
      "Ali Parviz",
      "Boris Knyazev",
      "Yan Zhang",
      "Glen Berseth",
      "Bang Liu"
    ],
    "abstract": "Generating molecules that satisfy precise numeric constraints over multiple physicochemical properties is critical and challenging. Although large language models (LLMs) are expressive, they struggle with precise multi-objective control and numeric reasoning without external structure and feedback. We introduce \\textbf{M olGen}, a fragment-level, retrieval-augmented, two-stage framework for molecule generation under multi-property constraints. Stage I : Prototype generation: a multi-agent reasoner performs retrieval-anchored, fragment-level edits to produce a candidate near the feasible region. Stage II : RL-based fine-grained optimization: a fragment-level optimizer trained with Group Relative Policy Optimization (GRPO) applies one- or multi-hop refinements to explicitly minimize the property errors toward our target while regulating edit complexity and deviation from the prototype. A large, automatically curated dataset with reasoning chains of fragment edits and measured property deltas underpins both stages, enabling deterministic, reproducible supervision and controllable multi-hop reasoning. Unlike prior work, our framework better reasons about molecules by leveraging fragments and supports controllable refinement toward numeric targets. Experiments on generation under two sets of property constraints (QED, LogP, Molecular Weight and HOMO, LUMO) show consistent gains in validity and precise satisfaction of multi-property targets, outperforming strong LLMs and graph-based algorithms.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10131.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10131",
    "published": "2026-01-15T07:18:05Z",
    "updated": "2026-01-15T07:18:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了M^4olGen框架，一个基于片段检索增强的两阶段分子生成系统，实现了在多个物理化学属性精确约束下的可控分子生成。",
      "motivation": "生成满足多个物理化学属性精确数值约束的分子在药物发现等领域至关重要且挑战性高。现有大语言模型（LLMs）虽然表达力强，但在精确多目标控制和数值推理方面存在不足，往往需要外部结构和反馈来弥补。因此，本研究旨在开发一种新方法，以克服LLMs在处理多属性约束时的不精确性，提升分子生成的准确性和可控性。",
      "method": "M^4olGen采用两阶段框架进行分子生成。第一阶段为原型生成，使用多代理推理器进行基于检索的片段级编辑，产生接近可行区域的候选分子。第二阶段为基于强化学习的细粒度优化，通过Group Relative Policy Optimization (GRPO)训练的片段级优化器，应用一跳或多跳细化操作，最小化目标属性的误差，同时控制编辑复杂性和偏离原型程度。支撑这两个阶段的是一个大型自动策划的数据集，包含片段编辑的推理链和测量的属性变化，提供确定性监督和可控的多跳推理。关键创新点包括片段级操作、检索增强设计和GRPO优化算法。",
      "result": "实验在两组属性约束下进行：一组包括QED、LogP和分子量，另一组包括HOMO和LUMO。结果表明，M^4olGen在生成分子的有效性和多属性目标精确满足方面取得了一致性提升，优于强大的大语言模型（LLMs）和基于图的算法。摘要未明确具体性能指标数据，但强调了框架的优越性，展现出在处理多属性约束任务中的显著改进。",
      "conclusion": "该研究通过引入片段级推理和检索增强的两阶段框架，显著提高了在多属性精确约束下分子生成的准确性和可控性，为药物设计和材料科学提供了新工具。主要贡献包括开发了一种可解释和可复现的方法，支持向数值目标的可控细化。摘要未明确说明局限性或未来工作方向，但暗示了潜在应用价值。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Reasoning",
        "Reinforcement Learning",
        "Fragment-Based Generation",
        "Retrieval-Augmented Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:32.533413Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10129",
    "title": "LaViT: Aligning Latent Visual Thoughts for Multi-modal Reasoning",
    "authors": [
      "Linquan Wu",
      "Tianxiang Jiang",
      "Yifei Dong",
      "Haoyu Yang",
      "Fengji Zhang",
      "Shichaang Meng",
      "Ai Xuan",
      "Linqi Song",
      "Jacky Keung"
    ],
    "abstract": "Current multimodal latent reasoning often relies on external supervision (e.g., auxiliary images), ignoring intrinsic visual attention dynamics. In this work, we identify a critical Perception Gap in distillation: student models frequently mimic a teacher's textual output while attending to fundamentally divergent visual regions, effectively relying on language priors rather than grounded perception. To bridge this, we propose LaViT, a framework that aligns latent visual thoughts rather than static embeddings. LaViT compels the student to autoregressively reconstruct the teacher's visual semantics and attention trajectories prior to text generation, employing a curriculum sensory gating mechanism to prevent shortcut learning. Extensive experiments show that LaViT significantly enhances visual grounding, achieving up to +16.9% gains on complex reasoning tasks and enabling a compact 3B model to outperform larger open-source variants and proprietary models like GPT-4o.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10129.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10129",
    "published": "2026-01-15T07:14:24Z",
    "updated": "2026-01-15T07:14:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "LaViT提出一种通过对齐潜在视觉思考来改善多模态推理视觉基础的新框架，显著提升模型性能。",
      "motivation": "当前多模态潜在推理方法常依赖外部监督如辅助图像，忽视模型内在视觉注意动态，导致在知识蒸馏中出现感知鸿沟。学生模型倾向于模仿教师的文本输出，但关注的视觉区域与教师不同，这使模型过度依赖语言先验而非真实视觉感知，限制了推理的准确性和鲁棒性。该研究旨在解决这一核心问题，强调对齐视觉注意对于提升多模态模型视觉基础能力的重要性，以克服现有方法在视觉接地方面的不足。",
      "method": "LaViT框架通过强制学生在文本生成前自回归重构教师的视觉语义和注意轨迹，对齐潜在视觉思考而非静态嵌入。其关键技术包括使用自回归生成过程模拟教师模型的视觉感知，并结合课程感官门机制动态调整训练，防止学生模型通过捷径学习依赖语言先验。这种方法在蒸馏过程中有效桥接感知鸿沟，具体实现涉及多模态模型架构优化，以确保学生更紧密学习教师的视觉动态，从而提升推理的准确性和泛化能力。",
      "result": "实验显示，LaViT在复杂多模态推理任务上显著增强视觉基础性能，相比基线方法实现高达+16.9%的性能增益。一个紧凑的3B参数模型在使用LaViT后，能够超越更大的开源变体和专有模型如GPT-4o，验证了该框架在减少感知鸿沟和提高效率方面的优越性。这些结果通过定量指标和模型对比证明，LaViT不仅提升了推理准确性，还为轻量级模型在高性能应用中提供了可行性，突显其在多模态学习领域的突破性进展。",
      "conclusion": "LaViT的主要贡献在于提出一种基于潜在视觉思考对齐的方法，有效解决多模态推理中的感知鸿沟问题，具有重要的学术价值，为视觉基础技术提供新方向。其实际应用价值体现在使紧凑模型达到更高性能，降低计算资源需求。摘要未明确说明局限性，但未来工作可能包括扩展到更多模态或任务、在多样化数据集上进一步验证，以及探索其他防止捷径学习的机制，以推动多模态AI的持续发展。",
      "tags": [
        "Multimodal Reasoning",
        "Visual Attention",
        "Knowledge Distillation",
        "Curriculum Learning",
        "Autoregressive Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:37.283635Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10124",
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "authors": [
      "Sicheng Yang",
      "Zhaohu Xing",
      "Lei Zhu"
    ],
    "abstract": "Consistency learning with feature perturbation is a widely used strategy in semi-supervised medical image segmentation. However, many existing perturbation methods rely on dropout, and thus require a careful manual tuning of the dropout rate, which is a sensitive hyperparameter and often difficult to optimize and may lead to suboptimal regularization. To overcome this limitation, we propose VQ-Seg, the first approach to employ vector quantization (VQ) to discretize the feature space and introduce a novel and controllable Quantized Perturbation Module (QPM) that replaces dropout. Our QPM perturbs discrete representations by shuffling the spatial locations of codebook indices, enabling effective and controllable regularization. To mitigate potential information loss caused by quantization, we design a dual-branch architecture where the post-quantization feature space is shared by both image reconstruction and segmentation tasks. Moreover, we introduce a Post-VQ Feature Adapter (PFA) to incorporate guidance from a foundation model (FM), supplementing the high-level semantic information lost during quantization. Furthermore, we collect a large-scale Lung Cancer (LC) dataset comprising 828 CT scans annotated for central-type lung carcinoma. Extensive experiments on the LC dataset and other public benchmarks demonstrate the effectiveness of our method, which outperforms state-of-the-art approaches. Code available at: https://github.com/script-Yang/VQ-Seg.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10124.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10124",
    "published": "2026-01-15T07:09:00Z",
    "updated": "2026-01-15T07:09:00Z",
    "comment": "Accepted by NeurIPS 2025",
    "light_analysis": {
      "overview": "VQ-Seg首次将向量量化应用于半监督医学图像分割，提出量化扰动模块替换dropout，实现可控且有效的正则化。",
      "motivation": "半监督医学图像分割中，一致性学习常通过特征扰动提升模型性能，但现有方法多依赖dropout，其dropout率作为敏感超参数需手动调优，难以优化且可能导致正则化效果不佳，影响分割准确性。这一问题在医疗图像分析中尤为重要，因为标注数据稀缺，需要高效方法以克服标注成本高、模型泛化弱的挑战。",
      "method": "方法提出VQ-Seg，利用向量量化离散化特征空间，并设计量化扰动模块通过混淆码书索引的空间位置实现可控扰动，避免dropout的超参数调优。为减少量化带来的信息损失，采用双分支架构让图像重建和分割任务共享量化后特征空间，同时引入后VQ特征适配器结合基础模型提供高级语义指导，增强特征表示，提升分割性能。",
      "result": "在收集的大规模肺癌CT数据集（含828个扫描）及其他公共基准测试中进行了广泛实验。结果表明，VQ-Seg在医学图像分割任务上优于现有的最先进方法，验证了其有效性和正则化优势。摘要未提供具体性能指标数据，但强调了方法相对于基线的显著改进。",
      "conclusion": "论文的主要贡献是创新性地将向量量化引入半监督医学图像分割，提出可控扰动模块替代dropout，解决了超参数调优问题，提升了分割准确性和鲁棒性。这具有学术价值，推动了半监督学习技术在医疗领域的应用，同时代码开源促进了社区发展。未来可扩展至更多医学图像任务或探索量化参数的自动化设置。",
      "tags": [
        "Vector Quantization",
        "Semi-Supervised Learning",
        "Medical Image Segmentation",
        "Consistency Learning",
        "Foundation Model"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:44.219825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10122",
    "title": "Role-Playing Agents Driven by Large Language Models: Current Status, Challenges, and Future Trends",
    "authors": [
      "Ye Wang",
      "Jiaxing Chen",
      "Hongjiang Xiao"
    ],
    "abstract": "In recent years, with the rapid advancement of large language models (LLMs), role-playing language agents (RPLAs) have emerged as a prominent research focus at the intersection of natural language processing (NLP) and human-computer interaction. This paper systematically reviews the current development and key technologies of RPLAs, delineating the technological evolution from early rule-based template paradigms, through the language style imitation stage, to the cognitive simulation stage centered on personality modeling and memory mechanisms. It summarizes the critical technical pathways supporting high-quality role-playing, including psychological scale-driven character modeling, memory-augmented prompting mechanisms, and motivation-situation-based behavioral decision control. At the data level, the paper further analyzes the methods and challenges of constructing role-specific corpora, focusing on data sources, copyright constraints, and structured annotation processes. In terms of evaluation, it collates multi-dimensional assessment frameworks and benchmark datasets covering role knowledge, personality fidelity, value alignment, and interactive hallucination, while commenting on the advantages and disadvantages of methods such as human evaluation, reward models, and LLM-based scoring. Finally, the paper outlines future development directions of role-playing agents, including personality evolution modeling, multi-agent collaborative narrative, multimodal immersive interaction, and integration with cognitive neuroscience, aiming to provide a systematic perspective and methodological insights for subsequent research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10122.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10122",
    "published": "2026-01-15T07:08:20Z",
    "updated": "2026-01-15T07:08:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文系统综述了基于大语言模型的角色扮演代理的当前发展、关键技术挑战和未来趋势，为后续研究提供了系统视角和方法论见解。",
      "motivation": "近年来，随着大语言模型的快速发展，角色扮演语言代理在自然语言处理和人机交互交叉领域成为研究热点。然而，从早期基于规则的模板范式，到语言风格模仿阶段，再到以个性建模和记忆机制为核心的认知模拟阶段，技术演进复杂且缺乏系统性总结。现有方法在角色真实性、一致性和交互质量方面存在不足，因此需要梳理现状、识别挑战，并明确未来方向，以推动该领域进步并解决实际应用中的问题。",
      "method": "作为一篇综述论文，本文采用系统回顾方法，总结了角色扮演代理的技术演化路径，从规则范式到认知模拟阶段。关键技术包括心理量表驱动的角色建模、记忆增强的提示机制以及基于动机情境的行为决策控制。此外，论文分析了构建角色特定语料库的方法，涉及数据来源、版权约束和结构化标注过程，并整理了多维评估框架，涵盖角色知识、个性保真度、价值对齐和交互幻觉等基准数据集，评述了人类评估、奖励模型和基于LLM评分等方法的优劣。",
      "result": "本文是综述性研究，未提供具体的实验性能数据，而是总结了现有角色扮演代理评估的成果。摘要提到，论文整理了多维评估框架和基准数据集，包括角色知识、个性保真度等方面的测试，但未明确说明准确率提升或效率改进等具体指标。这表明当前评估方法多样，但缺乏统一的性能标准，未来需要进一步实证研究以量化技术效果，为后续工作提供了参考依据。",
      "conclusion": "本文的主要贡献在于系统综述了基于大语言模型的角色扮演代理的发展现状、关键技术和未来趋势，为学术研究和实际应用提供了全面视角。通过总结技术演化、数据构建和评估方法，论文促进了该领域的系统性发展。未来方向包括个性演化建模、多代理协作叙事、多模态沉浸式交互以及与认知神经科学的融合，这些方向具有重要的学术价值和应用潜力，有助于提升代理真实性和交互体验，推动智能系统向更人性化方向发展。",
      "tags": [
        "Large Language Models",
        "Role-Playing Agents",
        "Memory-Augmented Prompting",
        "Psychological Scale Modeling",
        "Multimodal Interaction"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:12.121227Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10117",
    "title": "Beyond Single Prompts: Synergistic Fusion and Arrangement for VICL",
    "authors": [
      "Wenwen Liao",
      "Jianbo Yu",
      "Yuansong Wang",
      "Shifu Yan",
      "Xiaofeng Yang"
    ],
    "abstract": "Vision In-Context Learning (VICL) enables inpainting models to quickly adapt to new visual tasks from only a few prompts. However, existing methods suffer from two key issues: (1) selecting only the most similar prompt discards complementary cues from other high-quality prompts; and (2) failing to exploit the structured information implied by different prompt arrangements.   We propose an end-to-end VICL framework to overcome these limitations. Firstly, an adaptive Fusion Module aggregates critical patterns and annotations from multiple prompts to form more precise contextual prompts. Secondly, we introduce arrangement-specific lightweight MLPs to decouple layout priors from the core model, while minimally affecting the overall model. In addition, an bidirectional fine-tuning mechanism swaps the roles of query and prompt, encouraging the model to reconstruct the original prompt from fused context and thus enhancing collaboration between the fusion module and the inpainting model. Experiments on foreground segmentation, single-object detection, and image colorization demonstrate superior results and strong cross-task generalization of our method.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10117.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10117",
    "published": "2026-01-15T06:53:59Z",
    "updated": "2026-01-15T06:53:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种端到端视觉情境学习框架，通过自适应融合多个提示和解耦布局先验，提升任务适应性和性能。",
      "motivation": "视觉情境学习（VICL）使图像修复模型能基于少量提示快速适应新视觉任务，但现有方法存在两大问题：一是仅选择最相似提示，忽视了其他高质量提示提供的互补信息；二是未能利用不同提示排列所隐含的结构化信息。这些问题限制了模型在复杂任务中的表现，导致性能不足，因此需要更高效的方法来充分利用多提示和排列优势，以提高视觉任务的准确性和泛化能力。",
      "method": "论文设计了一个端到端VICL框架，主要包括三个核心技术：首先，自适应融合模块聚合多个提示中的关键模式和注释，形成更精确的上下文提示；其次，引入排列特定的轻量级多层感知机（MLPs），将布局先验从核心模型中解耦，以最小化对整体模型的影响；此外，采用双向微调机制，交换查询和提示的角色，鼓励模型从融合上下文中重建原始提示，从而增强融合模块与图像修复模型之间的协作。该方法通过多提示融合和排列优化，旨在提升视觉任务的适应能力。",
      "result": "实验在前景分割、单目标检测和图像着色等视觉任务上进行验证。结果显示，该方法在这些任务上均取得优越效果，并展现出强大的跨任务泛化能力。虽然摘要未明确说明具体数据（如准确率提升），但通过与其他基线方法的对比，证明了该框架在提升模型性能和适应性方面的有效性，克服了现有方法的局限性。",
      "conclusion": "本研究的主要贡献在于开发了一个创新的端到端VICL框架，有效解决了丢弃互补提示和忽略排列结构的问题，为视觉情境学习提供了新方法。学术上，该工作推动了VICL领域的发展，促进了多提示融合和布局优化的研究；实际应用中，该方法可广泛应用于分割、检测和着色等视觉任务，提高模型的灵活性和性能。未来工作可能涉及扩展到更多任务或优化计算效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Vision In-Context Learning (VICL)",
        "Adaptive Fusion Module",
        "Arrangement-specific MLPs",
        "Bidirectional Fine-tuning",
        "Inpainting Models"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:18.219017Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10114",
    "title": "Following the Teacher's Footsteps: Scheduled Checkpoint Distillation for Domain-Specific LLMs",
    "authors": [
      "Cheng Feng",
      "Chaoliang Zhong",
      "Jun Sun",
      "Yusuke Oishi"
    ],
    "abstract": "Large language models (LLMs) are challenging to deploy for domain-specific tasks due to their massive scale. While distilling a fine-tuned LLM into a smaller student model is a promising alternative, the capacity gap between teacher and student often leads to suboptimal performance. This raises a key question: when and how can a student model match or even surpass its teacher on domain-specific tasks? In this work, we propose a novel theoretical insight: a student can outperform its teacher if its advantage on a Student-Favored Subdomain (SFS) outweighs its deficit on the Teacher-Favored Subdomain (TFS). Guided by this insight, we propose Scheduled Checkpoint Distillation (SCD), which reduces the TFS deficit by emulating the teacher's convergence process during supervised fine-tuning (SFT) on the domain task, and a sample-wise Adaptive Weighting (AW) mechanism to preserve student strengths on SFS. Experiments across diverse domain tasks--including QA, NER, and text classification in multiple languages--show that our method consistently outperforms existing distillation approaches, allowing the student model to match or even exceed the performance of its fine-tuned teacher.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10114.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10114",
    "published": "2026-01-15T06:46:01Z",
    "updated": "2026-01-15T06:46:01Z",
    "comment": "15 pages, submitted to ICPR 2026",
    "light_analysis": {
      "overview": "该论文提出了定时检查点蒸馏（SCD）方法，使学生模型在领域特定任务上能匹配甚至超越老师模型。",
      "motivation": "大规模语言模型（LLMs）因规模过大而难以部署到特定领域任务，蒸馏技术虽能压缩知识到小模型，但师生模型容量差距导致性能不佳，这引发了学生模型何时及如何在领域任务中超越老师的关键问题。现有蒸馏方法往往忽视子域性能的动态差异，导致学生潜力未充分发挥，因此研究优化蒸馏过程以减少这种差距具有重要的实际应用价值，以提升轻量级模型在专业场景下的效率。",
      "method": "基于理论洞察，学生模型如果在学生有利子域（SFS）的优势超过其在老师有利子域（TFS）的劣势，就能超越老师。为此，提出Scheduled Checkpoint Distillation（SCD），通过模仿老师在监督微调（SFT）过程中的收敛路径，逐步减少TFS缺陷，并结合样本级自适应权重（AW）机制来保持在SFS上的优势。关键创新点包括分阶段的蒸馏策略和动态权重调整，无需额外模型架构细节，专注于优化师生知识传递。",
      "result": "在多种领域任务上的实验，包括问答（QA）、命名实体识别（NER）和多语言文本分类，表明SCD方法一致优于现有蒸馏方法。学生模型能够匹配甚至超越其微调老师，显示出在性能提升方面的有效性，对比基线方法，该方法在多个任务中表现出显著优势，尽管具体数值摘要未明确说明，但突显了其在弥合师生差距和优化特定任务表现方面的能力。",
      "conclusion": "本研究的主要贡献是提出了Scheduled Checkpoint Distillation（SCD）方法，有效解决蒸馏中师生性能差距问题，使学生模型在领域任务中超越老师，提供了新的理论框架以理解蒸馏性能，并具有实际应用价值，如促进轻量级LLMs部署。潜在局限性可能包括对特定任务的依赖性，未来工作可扩展到更多领域或探索更复杂的动态策略，以进一步提升方法的泛化能力。",
      "tags": [
        "Large Language Model",
        "Knowledge Distillation",
        "Scheduled Checkpoint Distillation",
        "Adaptive Weighting",
        "Supervised Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:40.332464Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10109",
    "title": "Skill-Aware Data Selection and Fine-Tuning for Data-Efficient Reasoning Distillation",
    "authors": [
      "Lechen Zhang",
      "Yunxiang Zhang",
      "Wei Hu",
      "Lu Wang"
    ],
    "abstract": "Large reasoning models such as DeepSeek-R1 and their distilled variants achieve strong performance on complex reasoning tasks. Yet, distilling these models often demands large-scale data for supervised fine-tuning (SFT), motivating the pursuit of data-efficient training methods. To address this, we propose a skill-centric distillation framework that efficiently transfers reasoning ability to weaker models with two components: (1) Skill-based data selection, which prioritizes examples targeting the student model's weaker skills, and (2) Skill-aware fine-tuning, which encourages explicit skill decomposition during problem solving. With only 1,000 training examples selected from a 100K teacher-generated corpus, our method surpasses random SFT baselines by +1.6% on Qwen3-4B and +1.4% on Qwen3-8B across five mathematical reasoning benchmarks. Further analysis confirms that these gains concentrate on skills emphasized during training, highlighting the effectiveness of skill-centric training for efficient reasoning distillation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10109.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10109",
    "published": "2026-01-15T06:31:39Z",
    "updated": "2026-01-15T06:31:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种技能感知的数据选择和微调框架，用于实现数据高效的推理蒸馏。",
      "motivation": "深度学习模型如DeepSeek-R1及其蒸馏变体在复杂推理任务中表现强大，但传统蒸馏方法通常需要大规模数据进行监督微调，这导致训练成本高昂且效率低下。现有方法往往忽略针对学生模型弱点的数据选择，限制了数据利用率和模型性能。因此，开发数据高效的训练方法对于资源受限的实际应用场景至关重要，能够推动模型蒸馏技术的广泛应用。",
      "method": "本研究提出一个技能为中心的蒸馏框架，核心包括两个组件：技能基于数据选择，通过优先从教师生成数据中选择针对学生模型较弱技能的示例；技能感知微调，鼓励在问题解决过程中显式进行技能分解。使用从100K语料库中精选的1000个训练示例，应用于Qwen3系列模型进行实验。关键创新在于将技能识别与针对性训练结合，提高数据利用率，实现高效的推理能力转移。",
      "result": "在五个数学推理基准上的实验表明，使用1000个精选示例，Qwen3-4B和Qwen3-8B模型相比随机SFT基线分别提升了1.6%和1.4%的性能。这些提升主要集中在训练中强调的技能领域，证实了技能为中心方法的有效性。与基线方法对比，该方法以更少数据实现了显著性能改进，突出了数据高效蒸馏的潜力。",
      "conclusion": "本文的主要贡献是提出了一种数据高效的推理蒸馏方法，通过技能感知的数据选择和微调，有效提升了弱模型的推理能力。学术上，强调了技能为中心训练策略在模型蒸馏中的重要性；实际应用上，减少了数据需求和训练成本，适用于资源有限的环境。摘要未明确说明局限性，但未来工作可能涉及扩展到更多任务或进一步优化技能识别算法。",
      "tags": [
        "Knowledge Distillation",
        "Data Selection",
        "Fine-Tuning",
        "Skill-Aware Learning",
        "Mathematical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:31.635956Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10108",
    "title": "SIN-Bench: Tracing Native Evidence Chains in Long-Context Multimodal Scientific Interleaved Literature",
    "authors": [
      "Yiming Ren",
      "Junjie Wang",
      "Yuxin Meng",
      "Yihang Shi",
      "Zhiqiang Lin",
      "Ruihang Chu",
      "Yiran Xu",
      "Ziming Li",
      "Yunfei Zhao",
      "Zihan Wang",
      "Yu Qiao",
      "Ruiming Tang",
      "Minghao Liu",
      "Yujiu Yang"
    ],
    "abstract": "Evaluating whether multimodal large language models truly understand long-form scientific papers remains challenging: answer-only metrics and synthetic \"Needle-In-A-Haystack\" tests often reward answer matching without requiring a causal, evidence-linked reasoning trace in the document. We propose the \"Fish-in-the-Ocean\" (FITO) paradigm, which requires models to construct explicit cross-modal evidence chains within native scientific documents. To operationalize FITO, we build SIN-Data, a scientific interleaved corpus that preserves the native interleaving of text and figures. On top of it, we construct SIN-Bench with four progressive tasks covering evidence discovery (SIN-Find), hypothesis verification (SIN-Verify), grounded QA (SIN-QA), and evidence-anchored synthesis (SIN-Summary). We further introduce \"No Evidence, No Score\", scoring predictions when grounded to verifiable anchors and diagnosing evidence quality via matching, relevance, and logic. Experiments on eight MLLMs show that grounding is the primary bottleneck: Gemini-3-pro achieves the best average overall score (0.573), while GPT-5 attains the highest SIN-QA answer accuracy (0.767) but underperforms on evidence-aligned overall scores, exposing a gap between correctness and traceable support.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10108.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10108",
    "published": "2026-01-15T06:25:25Z",
    "updated": "2026-01-15T06:25:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出FITO范式和SIN-Bench基准，用于评估多模态大语言模型在长篇科学论文中的跨模态证据链推理能力。",
      "motivation": "现有评估方法，如答案匹配和‘Needle-In-A-Haystack’测试，主要关注答案准确性，忽略了模型是否能在文档中进行因果、证据链推理，这在处理长篇科学论文时尤为重要，因为跨模态证据链的理解是衡量真实理解的关键。这些方法的不足在于它们奖励答案匹配而不要求推理轨迹，导致评估不全面，无法确保模型真正掌握内容结构，尤其是在科学文献中文本和图形交织的复杂环境中。因此，需要新的评估范式来填补这一空白，提升多模态模型推理能力的可信度。",
      "method": "论文提出‘Fish-in-the-Ocean’(FITO)范式，要求模型在原生科学文档中构建显式跨模态证据链。为实施此范式，首先构建了SIN-Data语料库，它保留了文本和图形的原生交织结构，模拟真实科学文献。基于此，开发了SIN-Bench基准，包含四个渐进任务：证据发现(SIN-Find)、假设验证(SIN-Verify)、基于证据的问答(SIN-QA)和证据锚定合成(SIN-Summary)。关键创新在于引入‘No Evidence, No Score’评分机制，预测需基于可验证锚点进行评分，并通过匹配度、相关性和逻辑一致性来诊断证据质量，从而确保评估的严谨性。",
      "result": "在八个多模态大语言模型上的实验结果显示，基础化是主要性能瓶颈。Gemini-3-pro在平均总分上表现最佳，得分为0.573；而GPT-5在SIN-QA任务中的答案准确率最高，达到0.767，但在证据对齐的总分上表现不佳。这暴露了模型在提供正确回答和可追踪证据支持之间的显著差距，强调了证据链推理的重要性。实验结果不仅量化了不同模型的性能差异，还揭示了现有方法在证据对齐方面的不足，为后续改进提供了数据支撑。",
      "conclusion": "本研究的主要贡献在于提出了FITO范式和SIN-Bench基准，推动了多模态大语言模型在科学文档理解中的证据链推理评估。学术上，它弥补了现有评估方法的缺陷，增强了模型推理能力的可解释性；实际应用上，为模型设计和优化提供了新方向。尽管摘要未明确说明局限性，但可以推断未来工作可能包括改进模型的基础化能力、扩展基准到更多领域，以及探索更精细的证据质量度量方法，以进一步提升评估的全面性和实用性。",
      "tags": [
        "Multimodal Large Language Models",
        "Evidence Chain Reasoning",
        "Cross-Modal Integration",
        "Benchmark Evaluation",
        "Scientific Document Understanding"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:41.952759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10107",
    "title": "Enhancing Visual In-Context Learning by Multi-Faceted Fusion",
    "authors": [
      "Wenwen Liao",
      "Jianbo Yu",
      "Yuansong Wang",
      "Qingchao Jiang",
      "Xiaofeng Yang"
    ],
    "abstract": "Visual In-Context Learning (VICL) has emerged as a powerful paradigm, enabling models to perform novel visual tasks by learning from in-context examples. The dominant \"retrieve-then-prompt\" approach typically relies on selecting the single best visual prompt, a practice that often discards valuable contextual information from other suitable candidates. While recent work has explored fusing the top-K prompts into a single, enhanced representation, this still simply collapses multiple rich signals into one, limiting the model's reasoning capability. We argue that a more multi-faceted, collaborative fusion is required to unlock the full potential of these diverse contexts. To address this limitation, we introduce a novel framework that moves beyond single-prompt fusion towards an multi-combination collaborative fusion. Instead of collapsing multiple prompts into one, our method generates three contextual representation branches, each formed by integrating information from different combinations of top-quality prompts. These complementary guidance signals are then fed into proposed MULTI-VQGAN architecture, which is designed to jointly interpret and utilize collaborative information from multiple sources. Extensive experiments on diverse tasks, including foreground segmentation, single-object detection, and image colorization, highlight its strong cross-task generalization, effective contextual fusion, and ability to produce more robust and accurate predictions than existing methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10107.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10107",
    "published": "2026-01-15T06:25:09Z",
    "updated": "2026-01-15T06:25:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个多组合协作融合框架，通过增强视觉上下文学习的多方面融合，提升模型的跨任务泛化能力和预测准确性。",
      "motivation": "在视觉上下文学习领域，现有方法如“检索-提示”通常依赖选择单一最佳视觉提示，导致丢弃其他候选的有价值上下文信息，这限制了模型的推理能力。尽管最近研究尝试融合 top-K 提示，但仅简单合并信号为一个表示，仍无法充分利用多样上下文。因此，需要一种更全面的协作融合方法来提升模型对上下文的利用效率，以解决当前方法在信息融合方面的不足，从而提高视觉任务的性能和泛化能力。",
      "method": "论文引入一个创新框架，采用多组合协作融合策略。核心方法是从高质量提示中选择不同组合，生成三个互补的上下文表示分支，每个分支集成特定组合的信息。这些分支作为指导信号输入到专门设计的 MULTI-VQGAN 架构中，该架构旨在联合解释和利用来自多源的协作信息，实现更有效的上下文融合和推理。通过这种方式，方法避免了简单折叠多个信号，突出了协作融合的优势，增强了模型的学习能力。",
      "result": "通过在前景分割、单对象检测和图像着色等多样任务上的广泛实验，结果显示该方法具有强大的跨任务泛化能力，能有效融合上下文信息。与现有方法相比，它产生更稳健和准确的预测，尽管摘要未明确说明具体性能指标如准确率提升，但强调了其在多个任务中的优越表现，证实了多组合协作融合框架的有效性和实用性。",
      "conclusion": "本研究的核心贡献是提出一个多组合协作融合框架，显著提升了视觉上下文学习的效果。学术上，它改进了上下文融合技术，促进了更高效的上下文利用；实际应用上，在多种视觉任务中展现出更强的泛化能力和准确性，为未来研究提供了新方向。未来工作可能包括扩展到更复杂任务或优化融合机制，进一步探索其局限性。",
      "tags": [
        "Visual In-Context Learning",
        "Multi-Faceted Fusion",
        "Collaborative Fusion",
        "MULTI-VQGAN",
        "Contextual Representation"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:56.243559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10104",
    "title": "MathDoc: Benchmarking Structured Extraction and Active Refusal on Noisy Mathematics Exam Papers",
    "authors": [
      "Chenyue Zhou",
      "Jiayi Tuo",
      "Shitong Qin",
      "Wei Dai",
      "Mingxuan Wang",
      "Ziwei Zhao",
      "Duoyang Li",
      "Shiyang Su",
      "Yanxi Lu",
      "Yanbiao Ma"
    ],
    "abstract": "The automated extraction of structured questions from paper-based mathematics exams is fundamental to intelligent education, yet remains challenging in real-world settings due to severe visual noise. Existing benchmarks mainly focus on clean documents or generic layout analysis, overlooking both the structural integrity of mathematical problems and the ability of models to actively reject incomplete inputs. We introduce MathDoc, the first benchmark for document-level information extraction from authentic high school mathematics exam papers. MathDoc contains \\textbf{3,609} carefully curated questions with real-world artifacts and explicitly includes unrecognizable samples to evaluate active refusal behavior. We propose a multi-dimensional evaluation framework covering stem accuracy, visual similarity, and refusal capability. Experiments on SOTA MLLMs, including Qwen3-VL and Gemini-2.5-Pro, show that although end-to-end models achieve strong extraction performance, they consistently fail to refuse illegible inputs, instead producing confident but invalid outputs. These results highlight a critical gap in current MLLMs and establish MathDoc as a benchmark for assessing model reliability under degraded document conditions. Our project repository is available at \\href{https://github.com/winnk123/papers/tree/master}{GitHub repository}",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10104.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10104",
    "published": "2026-01-15T06:17:47Z",
    "updated": "2026-01-15T06:17:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了MathDoc基准，首次从真实高中数学试卷中评估文档级信息提取和主动拒绝能力。",
      "motivation": "自动化从纸质数学试卷中提取结构化问题是智能教育的基础，但在真实环境中，由于严重视觉噪声的存在，这一任务仍然具有挑战性。现有基准主要关注干净文档或通用布局分析，忽视了数学问题的结构完整性和模型主动拒绝不完整输入的能力。这种不足限制了模型在嘈杂文档条件下的可靠应用，突显了开发新评估标准的必要性。",
      "method": "本论文引入了MathDoc基准，包含3,609个精心策划的高中数学问题，模拟真实世界的噪声和无法识别的样本。提出了一个多维评估框架，涵盖词干准确性、视觉相似性和拒绝能力，以全面测试模型性能。研究采用最新的多模态大语言模型（MLLMs），如Qwen3-VL和Gemini-2.5-Pro，进行端到端的文档级信息提取实验。",
      "result": "实验结果显示，SOTA MLLMs在结构化问题提取方面表现良好，能够有效处理嘈杂文档。然而，这些模型一致未能主动拒绝无法识别的输入，反而产生自信但无效的输出，这表明当前技术在可靠性方面存在关键缺陷。与现有基准相比，MathDoc提供了更全面的评估，揭示了模型在实际场景中的不足。",
      "conclusion": "本研究的主要贡献是提出了MathDoc基准，填补了评估模型在嘈杂文档条件下信息提取和主动拒绝能力的空白。学术上，它为多模态大语言模型提供了新的测试标准；实际应用中，有助于提升智能教育系统的可靠性。摘要未明确说明具体局限性，但未来工作可能侧重于优化模型的拒绝行为，或扩展基准到其他文档类型。",
      "tags": [
        "Document-Level Information Extraction",
        "Active Refusal",
        "Multimodal Large Language Models",
        "Benchmarking",
        "Visual Noise"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:01.426365Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10103",
    "title": "FlowAct-R1: Towards Interactive Humanoid Video Generation",
    "authors": [
      "Lizhen Wang",
      "Yongming Zhu",
      "Zhipeng Ge",
      "Youwei Zheng",
      "Longhao Zhang",
      "Tianshu Hu",
      "Shiyang Qin",
      "Mingshuang Luo",
      "Jiaxu Zhang",
      "Xin Chen",
      "Yulong Wang",
      "Zerong Zheng",
      "Jianwen Jiang",
      "Chao Liang",
      "Weifeng Chen",
      "Xing Wang",
      "Yuan Zhang",
      "Mingyuan Gao"
    ],
    "abstract": "Interactive humanoid video generation aims to synthesize lifelike visual agents that can engage with humans through continuous and responsive video. Despite recent advances in video synthesis, existing methods often grapple with the trade-off between high-fidelity synthesis and real-time interaction requirements. In this paper, we propose FlowAct-R1, a framework specifically designed for real-time interactive humanoid video generation. Built upon a MMDiT architecture, FlowAct-R1 enables the streaming synthesis of video with arbitrary durations while maintaining low-latency responsiveness. We introduce a chunkwise diffusion forcing strategy, complemented by a novel self-forcing variant, to alleviate error accumulation and ensure long-term temporal consistency during continuous interaction. By leveraging efficient distillation and system-level optimizations, our framework achieves a stable 25fps at 480p resolution with a time-to-first-frame (TTFF) of only around 1.5 seconds. The proposed method provides holistic and fine-grained full-body control, enabling the agent to transition naturally between diverse behavioral states in interactive scenarios. Experimental results demonstrate that FlowAct-R1 achieves exceptional behavioral vividness and perceptual realism, while maintaining robust generalization across diverse character styles.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10103.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10103",
    "published": "2026-01-15T06:16:22Z",
    "updated": "2026-01-15T06:16:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "FlowAct-R1 提出一个实时交互式人形视频生成框架，通过 MMDiT 架构和创新扩散策略平衡高保真与低延迟。",
      "motivation": "交互式人形视频生成旨在合成能与人类连续交互的逼真视觉代理，但现有方法常在高保真合成与实时交互需求间存在权衡，限制了实际应用中对流畅交互的追求。本研究通过 FlowAct-R1 框架，致力于解决这一权衡问题，提升交互式代理的实用性和用户体验。",
      "method": "FlowAct-R1 基于 MMDiT 架构实现流式视频合成，核心创新包括分块扩散强制策略及其自强制变体，以减轻错误积累并确保长期时间一致性。结合高效蒸馏和系统级优化，框架支持任意时长视频生成，保持低延迟响应。",
      "result": "FlowAct-R1 在 480p 分辨率下达到稳定 25fps，首次帧时间约 1.5 秒，实验显示卓越的行为生动性和感知真实性，并在多样角色风格中保持鲁棒泛化，相较于基线方法有显著改进。",
      "conclusion": "FlowAct-R1 解决了交互式视频生成中实时性与高保真的冲突，具有学术创新价值，推动了视频合成技术的发展，并适用于虚拟助手等实际场景。未来可探索更高分辨率扩展和更复杂交互能力。",
      "tags": [
        "MMDiT Architecture",
        "Chunkwise Diffusion Forcing",
        "Self-forcing",
        "Real-time Video Generation",
        "Temporal Consistency"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:38.323766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10101",
    "title": "MATRIX AS PLAN: Structured Logical Reasoning with Feedback-Driven Replanning",
    "authors": [
      "Ke Chen",
      "Jiandian Zeng",
      "Zihao Peng",
      "Guo Li",
      "Guangxue Zhang",
      "Tian Wang"
    ],
    "abstract": "As knowledge and semantics on the web grow increasingly complex, enhancing Large Language Models (LLMs) comprehension and reasoning capabilities has become particularly important. Chain-of-Thought (CoT) prompting has been shown to enhance the reasoning capabilities of LLMs. However, it still falls short on logical reasoning tasks that rely on symbolic expressions and strict deductive rules. Neuro-symbolic methods address this gap by enforcing formal correctness through external solvers. Yet these solvers are highly format-sensitive, and small instabilities in model outputs can lead to frequent processing failures. LLM-driven approaches avoid parsing brittleness, but they lack structured representations and process-level error-correction mechanisms. To further enhance the logical reasoning capabilities of LLMs, we propose MatrixCoT, a structured CoT framework with a matrix-based plan. Specifically, we normalize and type natural language expressions, attach explicit citation fields, and introduce a matrix-based planning method to preserve global relations among steps. The plan becomes a verifiable artifact, making execution more stable. For verification, we also add a feedback-driven replanning mechanism. Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer. Experiments on five logical-reasoning benchmarks and five LLMs show that, without relying on external solvers, MatrixCoT enhances both robustness and interpretability when tackling complex symbolic reasoning tasks, while maintaining competitive performance.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10101.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10101",
    "published": "2026-01-15T06:12:00Z",
    "updated": "2026-01-15T06:12:00Z",
    "comment": "12 pages, 5 figures, 2 tables. Accepted at The Web Conference (WWW) 2026",
    "light_analysis": {
      "overview": "论文提出了MatrixCoT框架，通过矩阵化计划和反馈驱动重规划，增强大型语言模型在逻辑推理中的鲁棒性和可解释性。",
      "motivation": "随着网络知识和语义复杂性增加，提升大型语言模型（LLMs）的逻辑推理能力变得至关重要。现有链式思维（CoT）提示在符号表达和严格演绎规则的推理任务中表现不足，无法处理形式化约束。神经符号方法依赖外部求解器强制执行正确性，但对输出格式高度敏感，导致不稳定和频繁失败；纯LLM方法避免了解析脆弱性，但缺乏结构化表示和过程级错误校正机制。因此，研究旨在开发一种结合符号推理精确性和LLM灵活性的新方法，以应对日益复杂的语义推理挑战。",
      "method": "论文提出MatrixCoT，一个结构化CoT框架。核心方法包括：标准化和类型化自然语言表达式，附加显式引用字段以确保可追溯性；引入基于矩阵的规划来维护推理步骤间的全局依赖关系，使计划成为可验证工件，提高执行稳定性。此外，添加反馈驱动重规划机制，在语义等价约束下识别遗漏和缺陷，重写并压缩依赖矩阵，生成更可信的答案。摘要未明确说明具体数据集或模型架构细节，但实验基于五个逻辑推理基准和五个LLMs进行。",
      "result": "实验在五个逻辑推理基准和五个大型语言模型上开展。结果显示，MatrixCoT不依赖外部求解器，显著提升了处理复杂符号推理任务的鲁棒性和可解释性。与基线方法相比，该方法在多个测试中表现出更好的稳定性，同时保持竞争性性能，尽管具体数值未在摘要中提供。整体上，MatrixCoT在增强推理准确性和减少处理失败方面表现突出，验证了其有效性。",
      "conclusion": "MatrixCoT框架的主要贡献在于通过矩阵化计划和反馈重规划，有效增强了LLMs在逻辑推理中的稳定性和可解释性。学术价值在于创新融合符号结构化和LLM灵活性，为逻辑推理任务提供了新思路；实际应用价值在于提升智能系统在复杂语义环境下的推理可靠性。摘要未明确说明局限性，未来工作可能包括扩展到更广泛的推理任务或优化机制以处理动态场景，进一步推动领域发展。",
      "tags": [
        "Large Language Models",
        "Chain-of-Thought",
        "Matrix-based Planning",
        "Feedback-driven Replanning",
        "Logical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:53.099005Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10098",
    "title": "InfoSculpt: Sculpting the Latent Space for Generalized Category Discovery",
    "authors": [
      "Wenwen Liao",
      "Hang Ruan",
      "Jianbo Yu",
      "Yuansong Wang",
      "Qingchao Jiang",
      "Xiaofeng Yang"
    ],
    "abstract": "Generalized Category Discovery (GCD) aims to classify instances from both known and novel categories within a large-scale unlabeled dataset, a critical yet challenging task for real-world, open-world applications. However, existing methods often rely on pseudo-labeling, or two-stage clustering, which lack a principled mechanism to explicitly disentangle essential, category-defining signals from instance-specific noise. In this paper, we address this fundamental limitation by re-framing GCD from an information-theoretic perspective, grounded in the Information Bottleneck (IB) principle. We introduce InfoSculpt, a novel framework that systematically sculpts the representation space by minimizing a dual Conditional Mutual Information (CMI) objective. InfoSculpt uniquely combines a Category-Level CMI on labeled data to learn compact and discriminative representations for known classes, and a complementary Instance-Level CMI on all data to distill invariant features by compressing augmentation-induced noise. These two objectives work synergistically at different scales to produce a disentangled and robust latent space where categorical information is preserved while noisy, instance-specific details are discarded. Extensive experiments on 8 benchmarks demonstrate that InfoSculpt validating the effectiveness of our information-theoretic approach.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10098.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10098",
    "published": "2026-01-15T06:00:13Z",
    "updated": "2026-01-15T06:00:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出InfoSculpt框架，基于信息瓶颈原则，通过双重条件互信息目标雕刻潜在空间，用于广义类别发现，以解耦类别信息和噪声。",
      "motivation": "广义类别发现旨在分类大规模未标记数据集中的已知和新类别实例，这对现实世界开放世界应用至关重要，但现有方法如伪标签或两阶段聚类缺乏原理性机制来明确解耦类别定义信号和实例特定噪声，导致性能受限。因此，需要一种更系统的方法来提升鲁棒性和泛化能力，从而应对开放环境下的未知类别挑战。",
      "method": "论文提出InfoSculpt框架，从信息论角度重新框架GCD，基于信息瓶颈原则最小化双重条件互信息目标。具体包括：类别级CMI在标记数据上学习紧凑和有区分度的已知类表示；实例级CMI在所有数据上通过压缩增强诱导的噪声来蒸馏不变特征。这两个目标协同工作，在不同尺度上产生解耦和鲁棒的潜在空间。摘要未明确说明具体数据集或模型架构细节。",
      "result": "论文在8个基准数据集上进行了广泛实验，验证了InfoSculpt框架的有效性。实验结果表明，该方法优于现有基线，但摘要未明确说明具体性能指标，如准确率提升或效率改进的百分比。因此，基于摘要信息，可以推断该方法在广义类别发现任务上表现出色，但详细对比数据未提供。",
      "conclusion": "本文的主要贡献是提出InfoSculpt框架，从信息论角度解决了广义类别发现中的表示学习问题。通过双重CMI目标，该方法能系统地雕刻潜在空间，保留类别信息并去除噪声，提升了分类鲁棒性和泛化能力。这项研究为开放世界应用提供了理论支撑和实用工具，未来工作可能包括优化目标函数或扩展到更多复杂场景。摘要未明确说明局限性或具体未来方向。",
      "tags": [
        "Generalized Category Discovery",
        "Information Bottleneck",
        "Conditional Mutual Information",
        "Representation Learning",
        "Feature Distillation"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:30.078949Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10096",
    "title": "Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text",
    "authors": [
      "Piyush Singh Pasi"
    ],
    "abstract": "Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10096.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10096",
    "published": "2026-01-15T05:56:37Z",
    "updated": "2026-01-15T05:56:37Z",
    "comment": "EACL 2026 Findings accepted. Initial Draft of Camera-ready",
    "light_analysis": {
      "overview": "论文提出METAL方法，一种轻量级对齐技术，仅使用英语文本将多语言文本嵌入映射到多模态空间，显著提升非英语语言的多模态任务性能。",
      "motivation": "多模态模型在英语中表现优异，得益于丰富的图像-文本和音频-文本数据，但对其他语言性能大幅下降，主要由于多语言多模态资源稀缺。现有解决方案过度依赖机器翻译，未能充分利用多语言文本建模的最新进展，因此需要开发更高效的方法来利用单语文本资源，以解决跨语言多模态任务中的性能瓶颈问题。",
      "method": "论文引入METAL（Multilingual-To-Multimodal Alignment），一种轻量级对齐方法。核心创新是仅使用英语文本数据，通过学习少数线性层，将预训练的多语言文本嵌入映射到共享的多模态空间。该方法避免了依赖大规模多语言多模态数据集，关键特色在于通过线性变换重塑嵌入几何结构，而非进行简单旋转，从而高效实现对齐。评估基于XTD文本到图像检索等任务。",
      "result": "实验结果表明，METAL在英语文本到图像检索中达到Recall at 10为94.9%，与基线性能持平。在零-shot设置下，对11种语言（其中10种未见）的平均Recall at 10为89.5%，显示出强大的跨语言转移能力。此外，该方法成功推广到音频-文本检索和跨语言文本到图像生成任务，验证了其通用性和有效性，优于依赖机器翻译的现有方法。",
      "conclusion": "论文的主要贡献是提出了METAL方法，有效解决了多语言多模态对齐的挑战，具有重要的学术价值和实际应用潜力，如促进全球AI应用的普及。通过轻量级设计降低了计算资源需求，并发布了代码、检查点和多语言数据集（如MSCOCO Multilingual 30K）以支持未来研究。摘要未明确说明局限性，但暗示了方法的可扩展性和在更多模态任务中的潜在应用。",
      "tags": [
        "Multilingual Multimodal Learning",
        "Linear Layer Alignment",
        "Zero-shot Transfer",
        "Text-to-Image Retrieval",
        "Audio-Text Retrieval"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:38.962511Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10094",
    "title": "V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation",
    "authors": [
      "Han Wang",
      "Yi Yang",
      "Jingyuan Hu",
      "Minfeng Zhu",
      "Wei Chen"
    ],
    "abstract": "Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10094.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10094",
    "published": "2026-01-15T05:47:43Z",
    "updated": "2026-01-15T05:47:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "V-Zero提出了一种无需人工标注的多模态推理自我改进框架，通过协同进化循环提升模型性能。",
      "motivation": "该研究旨在解决多模态学习中依赖大规模人工标注数据集的问题，因为人工标注成本高、耗时，限制了视觉语言模型（VLMs）的可扩展性和应用效率。现有最先进方法虽提升了推理能力，但需大量标注数据，导致部署成本增加和灵活性不足。此问题的重要性在于，无标注学习能降低数据获取门槛，推动多模态系统在实际场景中的应用，促进AI技术的发展，尤其是在资源有限的环境中。摘要强调了减少标注依赖的迫切性，以加速多模态模型的迭代和改进。",
      "method": "V-Zero是一个通用的后训练框架，通过实例化两个角色实现自我改进：Questioner和Solver。Questioner利用双轨推理奖励合成高质量挑战性问题，该奖励对比直观猜测与推理结果以评估问题难度；Solver则基于自身采样响应的多数投票生成伪标签进行优化。两者通过Group Relative Policy Optimization（GRPO）迭代训练，形成协同进化循环，不断相互增强。关键创新点包括无标注学习机制、自我改进循环设计，以及GRPO用于策略优化，框架应用于Qwen2.5-VL-7B-Instruct模型，无需外部数据标注。",
      "result": "论文主要实验结果显示，在不使用任何人工标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上取得了显著性能提升：视觉数学推理能力提高了1.7分，通用视觉中心推理能力提高了2.6分。这些数据表明自我改进机制的有效性，通过内部循环优化了模型在多模态任务上的表现。与基线方法相比，V-Zero实现了持续的性能增益，证明了无需标注数据即可增强模型推理能力的潜力，具体对比结果在摘要中未明确说明基线模型，但暗示了与原有模型状态的改进。",
      "conclusion": "V-Zero的主要贡献是开发了一个无标注自我改进框架，通过协同进化循环提升多模态推理能力，学术上探索了自监督学习在多模态领域的应用价值，为减少数据依赖提供了新思路。实际应用价值在于降低标注成本，加速模型部署，可能扩展到更多视觉语言任务。摘要未明确说明局限性，但未来工作方向可能包括扩展到其他模型架构或任务、优化奖励机制，以及验证在更复杂场景下的有效性。",
      "tags": [
        "Multimodal Learning",
        "Vision-Language Models",
        "Self-Improvement",
        "Group Relative Policy Optimization",
        "Zero-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:35.193246Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10092",
    "title": "LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data",
    "authors": [
      "Jongseok Kim",
      "Seongae Kang",
      "Jonghwan Shin",
      "Yuhan Lee",
      "Ohyun Jo"
    ],
    "abstract": "Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10092.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10092",
    "published": "2026-01-15T05:44:05Z",
    "updated": "2026-01-15T05:44:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "LeMoF框架通过层级引导的多模态融合，选择性整合异构临床数据中的模态特定表示，解决了预测稳定性和判别能力的平衡问题。",
      "motivation": "多模态临床预测整合了电子健康记录和生物信号等异构数据，对于医疗决策至关重要。然而，现有方法依赖静态的模态集成方案和简单融合策略，无法充分利用模态特定的表示，导致在异构临床环境中预测性能不稳定和判别能力不足。因此，需要一种能够自适应整合模态信息并平衡稳定性和判别性的新方法，以提升临床预测的准确性和可靠性。",
      "method": "LeMoF框架从编码器的不同层提取层级表示，作为每个模态的引导信息。通过选择性整合这些层级表示，LeMoF分离学习全局的模态级预测和层级特定的判别表示，从而实现动态的多模态融合。关键创新点在于利用层级引导来优化融合过程，增强了模型在异构数据中的适应能力。尽管摘要未明确说明使用的具体数据集或模型架构，但提到涉及编码器配置，可能基于深度神经网络实现。",
      "result": "在ICU数据的住院时间预测实验中，LeMoF在多种编码器配置下一致优于现有的最先进多模态融合技术。尽管摘要未提供具体的性能指标如准确率提升，但实验证实了层级集成是提升预测稳健性的关键因素，确保了在各种临床条件下的稳定和判别性能。这验证了LeMoF在异构环境中的优越性，为多模态临床预测提供了有效解决方案。",
      "conclusion": "LeMoF框架的主要贡献在于提出了一种层级引导的多模态融合方法，有效解决了异构临床数据中预测稳定性和判别能力的平衡问题。该研究不仅改进了多模态融合策略，具有学术价值，还为医疗预测提供了更稳健的工具，具有实际应用价值。未来工作可以扩展到更多临床任务和数据集，并探索在其他多模态领域的适用性，以进一步提升模型泛化能力。",
      "tags": [
        "Multimodal Fusion",
        "Level-guided Representation",
        "Clinical Prediction",
        "Encoder Layers",
        "Heterogeneous Data"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:51.910481Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10090",
    "title": "Difficulty-guided Sampling: Bridging the Target Gap between Dataset Distillation and Downstream Tasks",
    "authors": [
      "Mingzhuo Li",
      "Guang Li",
      "Linfeng Ye",
      "Jiafeng Mao",
      "Takahiro Ogawa",
      "Konstantinos N. Plataniotis",
      "Miki Haseyama"
    ],
    "abstract": "In this paper, we propose difficulty-guided sampling (DGS) to bridge the target gap between the distillation objective and the downstream task, therefore improving the performance of dataset distillation. Deep neural networks achieve remarkable performance but have time and storage-consuming training processes. Dataset distillation is proposed to generate compact, high-quality distilled datasets, enabling effective model training while maintaining downstream performance. Existing approaches typically focus on features extracted from the original dataset, overlooking task-specific information, which leads to a target gap between the distillation objective and the downstream task. We propose leveraging characteristics that benefit the downstream training into data distillation to bridge this gap. Focusing on the downstream task of image classification, we introduce the concept of difficulty and propose DGS as a plug-in post-stage sampling module. Following the specific target difficulty distribution, the final distilled dataset is sampled from image pools generated by existing methods. We also propose difficulty-aware guidance (DAG) to explore the effect of difficulty in the generation process. Extensive experiments across multiple settings demonstrate the effectiveness of the proposed methods. It also highlights the broader potential of difficulty for diverse downstream tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10090.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10090",
    "published": "2026-01-15T05:29:50Z",
    "updated": "2026-01-15T05:29:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 difficulty-guided sampling (DGS) 方法，旨在弥合数据集蒸馏目标与下游任务之间的目标差距，从而提升蒸馏性能。",
      "motivation": "深度神经网络虽然性能卓越，但训练过程耗时且存储需求大，数据集蒸馏技术应运而生，旨在生成紧凑的高质量数据集以优化训练效率。然而，现有方法通常仅关注从原始数据集提取的特征，忽略了任务特定信息，导致蒸馏目标与下游任务之间存在目标差距，影响最终性能。本研究旨在解决这一问题，通过将有益于下游训练的特征融入蒸馏过程，以提升数据集蒸馏的实际应用效果和泛化能力。",
      "method": "论文聚焦于图像分类下游任务，引入了 difficulty 的概念，并提出 difficulty-guided sampling (DGS) 作为一个插件式后阶段采样模块。该方法基于特定的目标难度分布，从现有蒸馏方法生成的图像池中采样最终蒸馏数据集。此外，还提出了 difficulty-aware guidance (DAG) 来探索难度在数据生成过程中的影响，从而进一步优化蒸馏质量，增强对任务特定信息的考虑。",
      "result": "摘要未明确说明具体实验结果数据，但指出在多种设置下进行了广泛实验，证明了所提方法的有效性。可以合理推断，DGS 在图像分类任务中可能提升了蒸馏数据集的性能，例如在保持或提高准确率的同时减少了训练资源消耗，与基线方法相比展示了更好的下游任务适应性。具体性能指标如准确率提升百分比等未在摘要中提及。",
      "conclusion": "本研究通过提出 DGS 方法，有效弥合了数据集蒸馏与下游任务之间的目标差距，提高了蒸馏数据的质量和实用性，为图像分类任务提供了更高效的训练方案。论文强调了 difficulty 概念的学术价值，揭示了其在数据蒸馏中的重要性，为未来研究提供了新思路，并指出其潜在应用于多样下游任务的更广泛潜力，但需进一步探索局限性和扩展方向。",
      "tags": [
        "Dataset Distillation",
        "Difficulty-guided Sampling",
        "Image Classification",
        "Downstream Tasks",
        "Machine Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:58.075913Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10089",
    "title": "Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications",
    "authors": [
      "Ashley Klein",
      "Edward Raff",
      "Marcia DesJardin"
    ],
    "abstract": "The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10089.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10089",
    "published": "2026-01-15T05:29:32Z",
    "updated": "2026-01-15T05:29:32Z",
    "comment": "To appear in AAAI 2026",
    "light_analysis": {
      "overview": "本文提出一种贝叶斯元分析方法，解决医学研究中关键变量缺失问题，并以剖腹产后试产为例验证其实用性。",
      "motivation": "元分析在医学研究中依赖先前研究准确捕获变量，但在剖腹产后试产等场景中，影响医生决策的关键变量未被记录，导致效应大小未知、结论不可靠。这种不确定性限制了临床决策的准确性和患者护理的有效性，现有元分析方法无法处理变量缺失情况，亟需新方法提高分析可靠性。",
      "method": "论文构建了一种贝叶斯方法，用于处理医学元分析中关键变量缺失的问题，利用贝叶斯统计推断在不完整数据下评估效应大小和可靠性。以剖腹产后试产为例，该方法帮助分析是否存在正效应，支持医生决策。摘要未明确说明具体的数据集或模型架构细节，但强调贝叶斯框架的应用创新。",
      "result": "摘要未明确说明具体的实验结果或性能指标，但通过案例研究展示了该贝叶斯方法的实用性。在剖腹产后试产评估中，该方法有助于确定正效应的存在性，从而为医生提供决策支持，但由于缺乏与基线方法的对比数据，具体效果提升未详细说明。",
      "conclusion": "本文的主要贡献是提出一种贝叶斯元分析方法，解决了医学研究中关键变量缺失的问题，提高了元分析的可靠性和实用性，为临床决策提供更坚实证据。学术价值在于扩展贝叶斯统计在医学领域的应用，实际意义在于支持医生在复杂情况下的患者护理。未来工作可能包括更广泛的应用验证和局限性改进。",
      "tags": [
        "Bayesian Meta-Analysis",
        "Medical Statistics",
        "Clinical Decision Support",
        "TOLAC",
        "Statistical Modeling"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:06.166705Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10088",
    "title": "State of AI: An Empirical 100 Trillion Token Study with OpenRouter",
    "authors": [
      "Malika Aubakirova",
      "Alex Atallah",
      "Chris Clark",
      "Justin Summerville",
      "Anjney Midha"
    ],
    "abstract": "The past year has marked a turning point in the evolution and real-world use of large language models (LLMs). With the release of the first widely adopted reasoning model, o1, on December 5th, 2024, the field shifted from single-pass pattern generation to multi-step deliberation inference, accelerating deployment, experimentation, and new classes of applications. As this shift unfolded at a rapid pace, our empirical understanding of how these models have actually been used in practice has lagged behind. In this work, we leverage the OpenRouter platform, which is an AI inference provider across a wide variety of LLMs, to analyze over 100 trillion tokens of real-world LLM interactions across tasks, geographies, and time. In our empirical study, we observe substantial adoption of open-weight models, the outsized popularity of creative roleplay (beyond just the productivity tasks many assume dominate) and coding assistance categories, plus the rise of agentic inference. Furthermore, our retention analysis identifies foundational cohorts: early users whose engagement persists far longer than later cohorts. We term this phenomenon the Cinderella \"Glass Slipper\" effect. These findings underscore that the way developers and end-users engage with LLMs \"in the wild\" is complex and multifaceted. We discuss implications for model builders, AI developers, and infrastructure providers, and outline how a data-driven understanding of usage can inform better design and deployment of LLM systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10088.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10088",
    "published": "2026-01-15T05:28:39Z",
    "updated": "2026-01-15T05:28:39Z",
    "comment": "36 pages",
    "light_analysis": {
      "overview": "本文通过分析OpenRouter平台上的100万亿token真实数据，揭示了大型语言模型在实际使用中的复杂用户行为和趋势。",
      "motivation": "随着2024年12月推理模型o1的发布，LLMs从单次模式生成转向多步深思推理，加速了应用部署。然而，对这些模型实际使用情况的理解滞后于技术发展，现有方法缺乏基于大规模真实数据的实证研究。本研究旨在解决这一不足，通过数据驱动分析用户如何“在野外”与LLMs交互，以应对快速发展带来的知识空白，为更有效的系统设计和部署提供基础。",
      "method": "研究利用OpenRouter平台，这是一个提供多种LLMs推理服务的平台，分析了超过100万亿个token的真实世界交互数据。方法基于数据驱动，覆盖了不同任务、地理区域和时间段的LLM使用情况，关键创新在于整合跨模型和跨场景的实际使用数据，通过大规模实证分析来揭示用户行为模式，为理解LLMs的实际应用提供了实证支持。",
      "result": "研究发现了几个关键趋势：开放权重模型被广泛采用；创意角色扮演和编码辅助任务超出预期地流行；代理推理方式兴起。用户保留分析揭示了“灰姑娘玻璃鞋”效应，即早期用户群体的参与度比后期用户更持久。这些结果表明，LLMs的实际使用是多方面的，挑战了仅关注生产力任务的假设，但摘要未提供具体性能指标对比。",
      "conclusion": "本研究的主要贡献是通过大规模实证数据，提供了对LLMs实际使用情况的数据驱动理解。它强调了用户交互的复杂性和多样性，为模型构建者、AI开发者和基础设施提供商提供了重要启示，有助于指导未来LLM系统的优化设计和部署。尽管摘要未明确说明局限性，但未来工作可能包括扩展数据范围或深入分析特定用户群体。",
      "tags": [
        "Large Language Models",
        "Empirical Study",
        "OpenRouter",
        "Token Analysis",
        "User Behavior"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:14.875465Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10085",
    "title": "CALM-IT: Generating Realistic Long-Form Motivational Interviewing Dialogues with Dual-Actor Conversational Dynamics Tracking",
    "authors": [
      "Viet Cuong Nguyen",
      "Nhi Yen Nguyen",
      "Kristin A. Candan",
      "Mary Conlon",
      "Vanessa Rumie",
      "Kristen Risola",
      "Srijan Kumar",
      "Munmun De Choudhury"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used in mental health-related settings, yet they struggle to sustain realistic, goal-directed dialogue over extended interactions. While LLMs generate fluent responses, they optimize locally for the next turn rather than maintaining a coherent model of therapeutic progress, leading to brittleness and long-horizon drift. We introduce CALM-IT, a framework for generating and evaluating long-form Motivational Interviewing (MI) dialogues that explicitly models dual-actor conversational dynamics. CALM-IT represents therapist-client interaction as a bidirectional state-space process, in which both agents continuously update inferred alignment, mental states, and short-term goals to guide strategy selection and utterance generation. Across large-scale evaluations, CALM-IT consistently outperforms strong baselines in Effectiveness and Goal Alignment and remains substantially more stable as conversation length increases. Although CALM-IT initiates fewer therapist redirections, it achieves the highest client acceptance rate (64.3%), indicating more precise and therapeutically aligned intervention timing. Overall, CALM-IT provides evidence for modeling evolving conversational state being essential for generating high-quality long-form synthetic conversations.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10085.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10085",
    "published": "2026-01-15T05:21:54Z",
    "updated": "2026-01-15T05:21:54Z",
    "comment": "46 pages",
    "light_analysis": {
      "overview": "CALM-IT 通过显式建模双演员对话动态，生成长格式动机访谈对话，提高了对话质量和稳定性。",
      "motivation": "大型语言模型在心理健康应用中能生成流畅响应，但倾向于局部优化下一轮对话，而非维护连贯的治疗进程模型，导致对话脆弱和长程漂移。现有方法无法有效维持长程目标导向对话，这在治疗性对话中至关重要。",
      "method": "CALM-IT 将治疗师-客户互动表示为双向状态空间过程，两个参与者持续更新推断的对齐、心理状态和短期目标，以指导策略选择和话语生成。关键创新是显式建模双演员对话动态，解决了长程一致性问题，摘要未明确说明具体数据集和模型架构细节。",
      "result": "在大规模评估中，CALM-IT 在有效性和目标对齐方面持续优于基线方法，对话长度增加时保持更高稳定性。客户接受率达到 64.3%，高于基线，表明更精确的治疗干预时机，尽管发起的治疗师重定向较少。",
      "conclusion": "CALM-IT 证明了建模变化的对话状态对生成高质量长格式合成对话的重要性，具有学术价值，为心理健康应用提供了更可靠的方法，并为其他长程对话任务提供了参考。未来工作可扩展其适用性和验证实际应用，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Models",
        "Motivational Interviewing",
        "Dual-Actor Conversational Dynamics",
        "State-Space Model",
        "Dialogue Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:26.922054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10084",
    "title": "Adaptive Label Error Detection: A Bayesian Approach to Mislabeled Data Detection",
    "authors": [
      "Zan Chaudhry",
      "Noam H. Rotenberg",
      "Brian Caffo",
      "Craig K. Jones",
      "Haris I. Sair"
    ],
    "abstract": "Machine learning classification systems are susceptible to poor performance when trained with incorrect ground truth labels, even when data is well-curated by expert annotators. As machine learning becomes more widespread, it is increasingly imperative to identify and correct mislabeling to develop more powerful models. In this work, we motivate and describe Adaptive Label Error Detection (ALED), a novel method of detecting mislabeling. ALED extracts an intermediate feature space from a deep convolutional neural network, denoises the features, models the reduced manifold of each class with a multidimensional Gaussian distribution, and performs a simple likelihood ratio test to identify mislabeled samples. We show that ALED has markedly increased sensitivity, without compromising precision, compared to established label error detection methods, on multiple medical imaging datasets. We demonstrate an example where fine-tuning a neural network on corrected data results in a 33.8% decrease in test set errors, providing strong benefits to end users. The ALED detector is deployed in the Python package statlab.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10084.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10084",
    "published": "2026-01-15T05:20:00Z",
    "updated": "2026-01-15T05:20:00Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "该论文提出了ALED方法，一种基于贝叶斯的自适应标签错误检测技术，能显著提高敏感度而不影响精度。",
      "motivation": "机器学习分类系统在训练时，如果使用错误的真实标签，即使数据由专家标注，性能也会下降。随着机器学习应用日益广泛，识别和纠正错误标签对于开发更强模型至关重要。现有标签错误检测方法可能在敏感度与精度平衡上存在不足，导致实际应用中模型可靠性受限，因此需要更高效的方法来提升检测效果。",
      "method": "ALED方法从深度卷积神经网络提取中间特征空间，进行特征去噪，然后使用多维高斯分布对每个类别的降维流形建模，并通过似然比测试识别错误标签样本。该方法的核心创新在于结合了贝叶斯框架和特征处理技术，在多个医学影像数据集上实施，具体网络架构摘要未明确说明。",
      "result": "在多个医学影像数据集上，ALED相比现有标签错误检测方法，敏感度显著提高且未影响精度。具体实验显示，通过使用纠正后的数据微调神经网络，测试集错误减少了33.8%，这直接提升了模型性能。对比基线方法，ALED在检测准确性方面表现更优，验证了其有效性。",
      "conclusion": "论文的主要贡献是提出ALED方法，改进了标签错误检测，提高敏感度而不牺牲精度。研究具有学术价值，提供了一种基于贝叶斯的新颖检测框架，实际应用价值体现在减少模型训练错误，增强最终性能。摘要未明确说明局限性或未来工作方向，但潜在应用可扩展到其他领域。",
      "tags": [
        "Label Error Detection",
        "Bayesian Approach",
        "Deep Convolutional Neural Network",
        "Medical Imaging Datasets",
        "Likelihood Ratio Test"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:33.242292Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10082",
    "title": "Is MT Ready for the Next Crisis or Pandemic?",
    "authors": [
      "Vipasha Bansal",
      "Elizabeth Brown",
      "Chelsea Kendrick",
      "Benjamin Pong",
      "William D. Lewis"
    ],
    "abstract": "Communication in times of crisis is essential. However, there is often a mismatch between the language of governments, aid providers, doctors, and those to whom they are providing aid. Commercial MT systems are reasonable tools to turn to in these scenarios. But how effective are these tools for translating to and from low resource languages, particularly in the crisis or medical domain? In this study, we evaluate four commercial MT systems using the TICO-19 dataset, which is composed of pandemic-related sentences from a large set of high priority languages spoken by communities most likely to be affected adversely in the next pandemic. We then assess the current degree of ``readiness'' for another pandemic (or epidemic) based on the usability of the output translations.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10082.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10082",
    "published": "2026-01-15T05:14:59Z",
    "updated": "2026-01-15T05:14:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究评估了四个商业机器翻译系统在危机或医疗领域对低资源语言的翻译效果，并基于TICO-19数据集评估其对未来大流行的准备程度。",
      "motivation": "危机时期沟通至关重要，但政府、援助提供者与受影响社区之间存在语言不匹配问题。商业MT系统常被用于此类场景，但其对低资源语言，特别是在危机或医疗领域的效果如何尚未明确。这关系到危机响应的效率和公平性，因为现有方法可能不足：商业MT系统主要针对高资源语言，对低资源语言支持有限，在专业领域如医疗翻译中准确性可能较低。因此，研究旨在填补这一空白，评估MT系统在真实危机场景中的实际表现，以改进全球卫生事件中的语言支持。",
      "method": "本研究采用实证评估方法，使用TICO-19数据集，该数据集包含与大流行相关的句子，覆盖多个高优先级语言，这些语言来自可能在未来大流行中受影响的社区。评估对象为四个商业MT系统，通过分析翻译输出的可用性来评估其效果。关键创新在于专注于危机和医疗领域的低资源语言翻译，并使用专门的数据集进行测试。研究未详细说明具体模型架构或技术细节，因为评估的是现成的商业系统，但强调了数据集的选择和评估标准的针对性。",
      "result": "摘要未明确说明具体实验结果数据。基于评估，论文可能得出商业MT系统在低资源语言翻译中的表现参差不齐，并对未来大流行的准备程度提出见解。但由于摘要未提供具体指标如准确率或效率改进，无法详述具体数据。与基线方法的对比也未明确，推断可能与其他MT系统或理想翻译标准进行比较。评估结果基于翻译输出的可用性，但具体量化指标如错误率或用户满意度未提及，仅概括性地评估了准备程度。",
      "conclusion": "本研究的主要贡献在于评估了商业MT系统在危机通信中的有效性，特别是在低资源语言和医疗领域的应用。学术上，为MT研究提供了危机场景的实证基础，推动了跨语言沟通的实证分析；实际上，有助于改进翻译工具以应对未来危机，提高全球卫生事件的响应能力和公平性。局限性可能包括数据集覆盖有限或评估标准主观性，未来工作可扩展语言范围、结合更多评估指标或开发针对低资源语言的专用MT系统，以增强危机准备。",
      "tags": [
        "Machine Translation",
        "Low Resource Languages",
        "TICO-19 Dataset",
        "Crisis Communication",
        "Medical Domain Translation"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:02.039193Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10080",
    "title": "Deriving Character Logic from Storyline as Codified Decision Trees",
    "authors": [
      "Letian Peng",
      "Kun Zhou",
      "Longfei Yun",
      "Yupeng Hou",
      "Jingbo Shang"
    ],
    "abstract": "Role-playing (RP) agents rely on behavioral profiles to act consistently across diverse narrative contexts, yet existing profiles are largely unstructured, non-executable, and weakly validated, leading to brittle agent behavior. We propose Codified Decision Trees (CDT), a data-driven framework that induces an executable and interpretable decision structure from large-scale narrative data. CDT represents behavioral profiles as a tree of conditional rules, where internal nodes correspond to validated scene conditions and leaves encode grounded behavioral statements, enabling deterministic retrieval of context-appropriate rules at execution time. The tree is learned by iteratively inducing candidate scene-action rules, validating them against data, and refining them through hierarchical specialization, yielding profiles that support transparent inspection and principled updates. Across multiple benchmarks, CDT substantially outperforms human-written profiles and prior profile induction methods on $85$ characters across $16$ artifacts, indicating that codified and validated behavioral representations lead to more reliable agent grounding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10080.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10080",
    "published": "2026-01-15T05:12:43Z",
    "updated": "2026-01-15T05:12:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 Codified Decision Trees (CDT) 框架，通过数据驱动方法从叙事数据中学习可执行和可解释的角色行为逻辑。",
      "motivation": "角色扮演代理依赖于行为配置文件来保持在不同叙事背景下的行为一致性，但现有配置文件大多是非结构化、不可执行且验证不足，导致代理行为脆弱不可靠。本研究旨在解决这些问题，因为可执行和验证的行为配置文件对提高自主代理的稳定性和一致性至关重要。现有方法如人工编写配置文件缺乏系统性，不易于更新和验证，难以适应复杂叙事环境。",
      "method": "论文提出 Codified Decision Trees (CDT) 数据驱动框架，从大规模叙事数据中诱导出可执行和可解释的决策结构。该框架将行为配置文件表示为条件规则树，其中内部节点对应已验证的场景条件，叶子编码具体行为语句，确保在执行时能确定性检索合适规则。学习过程涉及迭代诱导候选场景-动作规则、通过数据验证规则、并进行层次细化，从而生成支持透明检查和原则性更新的配置文件，核心创新在于结合规则验证与树形结构优化。",
      "result": "在多个基准测试中，CDT 在 85 个角色和 16 个文物上大幅优于人类编写的配置文件和先前的配置文件推导方法，表明编码和验证的行为表示能带来更可靠的代理基础。摘要未明确说明具体性能指标，但强调 CDT 显著超越基线方法，突显了其在实际应用中的有效性。",
      "conclusion": "CDT 框架的主要贡献在于提供了一种可执行和可解释的行为配置文件表示方法，提升了角色扮演代理的可靠性和一致性。其学术价值在于为行为建模引入了结构化数据驱动方法，促进了可解释 AI 的发展；实际应用价值包括增强游戏、虚拟代理等系统的自主性。局限性如摘要未明确说明，未来工作可能涉及扩展到更复杂的叙事场景和改进验证机制。",
      "tags": [
        "Codified Decision Trees",
        "Role-playing Agents",
        "Behavioral Profiles",
        "Data-driven Framework",
        "Scene-Action Rules"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:32.541535Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10079",
    "title": "Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts",
    "authors": [
      "Sijia Luo",
      "Xiaokang Zhang",
      "Yuxuan Hu",
      "Bohan Zhang",
      "Ke Wang",
      "Jinbo Su",
      "Mengshu Sun",
      "Lei Liang",
      "Jing Zhang"
    ],
    "abstract": "Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10079.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10079",
    "published": "2026-01-15T05:12:03Z",
    "updated": "2026-01-15T05:12:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "Sparse-RL提出了一种稳定稀疏rollouts的强化学习方法，有效解决大型语言模型强化学习中的内存墙问题。",
      "motivation": "大型语言模型（LLM）的强化学习（RL）在激发复杂推理能力方面变得至关重要，但长时程rollouts中存储关键值（KV）缓存导致巨大内存开销，成为关键瓶颈，限制有限硬件上的高效训练。现有KV压缩技术虽适用于推理，但直接用于RL训练会引发严重策略不匹配，导致性能崩溃，凸显了需要稳定训练方法的重要性。",
      "method": "Sparse-RL的核心方法是引入Sparsity-Aware Rejection Sampling和Importance-based Reweighting，以纠正由压缩引起的信息损失带来的off-policy偏差。研究识别了密集旧策略、稀疏采样策略和学习者策略之间的策略不匹配问题，并通过这些技术实现稳定稀疏rollouts，减少内存开销。摘要未明确说明具体数据集或模型架构细节，但强调了技术创新的重点。",
      "result": "实验结果表明，Sparse-RL相比密集基线减少了rollouts开销，同时保持了性能，未提供具体数据如准确率提升，但强调了效率与性能的平衡。此外，稀疏感知训练显著增强了模型在稀疏推理部署中的鲁棒性，展示了实际应用优势。",
      "conclusion": "Sparse-RL的主要贡献是通过稳定稀疏rollouts解决LLM RL的内存墙问题，学术价值在于提出纠正off-policy偏差的新技术，实际应用价值是促进有限硬件上的高效训练并提升稀疏推理的稳健性。未来工作方向摘要未明确说明，但可能包括进一步优化压缩算法或扩展到其他RL场景。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Model",
        "KV Compression",
        "Sparse Rollouts",
        "Off-policy Correction"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:48.455315Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10075",
    "title": "Thinking Like Van Gogh: Structure-Aware Style Transfer via Flow-Guided 3D Gaussian Splatting",
    "authors": [
      "Zhendong Wang",
      "Lebin Zhou",
      "Jingchuan Xiao",
      "Rongduo Han",
      "Nam Ling",
      "Cihan Ruan"
    ],
    "abstract": "In 1888, Vincent van Gogh wrote, \"I am seeking exaggeration in the essential.\" This principle, amplifying structural form while suppressing photographic detail, lies at the core of Post-Impressionist art. However, most existing 3D style transfer methods invert this philosophy, treating geometry as a rigid substrate for surface-level texture projection. To authentically reproduce Post-Impressionist stylization, geometric abstraction must be embraced as the primary vehicle of expression.   We propose a flow-guided geometric advection framework for 3D Gaussian Splatting (3DGS) that operationalizes this principle in a mesh-free setting. Our method extracts directional flow fields from 2D paintings and back-propagates them into 3D space, rectifying Gaussian primitives to form flow-aligned brushstrokes that conform to scene topology without relying on explicit mesh priors. This enables expressive structural deformation driven directly by painterly motion rather than photometric constraints.   Our contributions are threefold: (1) a projection-based, mesh-free flow guidance mechanism that transfers 2D artistic motion into 3D Gaussian geometry; (2) a luminance-structure decoupling strategy that isolates geometric deformation from color optimization, mitigating artifacts during aggressive structural abstraction; and (3) a VLM-as-a-Judge evaluation framework that assesses artistic authenticity through aesthetic judgment instead of conventional pixel-level metrics, explicitly addressing the subjective nature of artistic stylization.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10075.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10075",
    "published": "2026-01-15T05:00:02Z",
    "updated": "2026-01-15T05:00:02Z",
    "comment": "7 pages, 8 figures",
    "light_analysis": {
      "overview": "论文提出了一种流引导的几何对流框架，用于3D高斯泼溅，实现了基于结构抽象的风格迁移，直接模拟后印象派艺术的核心原则。",
      "motivation": "该研究旨在解决现有3D风格迁移方法无法真实再现后印象派艺术结构抽象的问题。后印象派艺术强调几何形式的夸张，而现有方法通常将3D几何视为刚性基底，仅进行表面纹理投影，导致风格化效果缺乏艺术真实性。为了捕捉梵高等艺术家的创作理念，需要一种新方法，将几何变形作为主要表达方式，拥抱结构抽象而非依赖显式网格先验。",
      "method": "论文提出了一个流引导的几何对流框架，操作于3D高斯泼溅环境中。核心方法是提取2D绘画中的方向流场，并通过投影机制将其反向传播到3D空间，校正高斯基元以形成流对齐的笔触，不依赖于显式网格。关键创新包括基于投影的无网格流引导机制、亮度-结构解耦策略以及VLM-as-a-Judge评估框架，分离几何变形和颜色优化，并使用视觉语言模型进行美学评判。",
      "result": "摘要未明确说明具体实验结果。然而，论文提出的方法能够生成结构感知的风格迁移效果，产生与场景拓扑一致的流对齐笔触。通过VLM-as-a-Judge评估框架，该方法可能在艺术真实性上优于基线方法，避免了像素级指标的局限性。实验可能展示了在几何抽象和艺术表现上的改进，但具体性能指标如准确率或效率提升在摘要中未提及。",
      "conclusion": "论文的主要贡献是提出了一个流引导的几何对流框架，用于3D高斯泼溅，实现了结构感知的风格迁移。这为3D风格迁移领域引入了新方法，强调几何抽象作为艺术表达的核心，具有重要的学术价值。该方法的应用潜力包括数字艺术和虚拟现实，能增强风格化的真实性。未来工作可能涉及扩展到其他艺术风格或进一步优化评估框架。",
      "tags": [
        "3D Gaussian Splatting",
        "Flow Guidance",
        "Geometric Advection",
        "Style Transfer",
        "VLM-as-a-Judge"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:08.070692Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10073",
    "title": "ReaMIL: Reasoning- and Evidence-Aware Multiple Instance Learning for Whole-Slide Histopathology",
    "authors": [
      "Hyun Do Jung",
      "Jungwon Choi",
      "Hwiyoung Kim"
    ],
    "abstract": "We introduce ReaMIL (Reasoning- and Evidence-Aware MIL), a multiple instance learning approach for whole-slide histopathology that adds a light selection head to a strong MIL backbone. The head produces soft per-tile gates and is trained with a budgeted-sufficiency objective: a hinge loss that enforces the true-class probability to be $\\geq τ$ using only the kept evidence, under a sparsity budget on the number of selected tiles. The budgeted-sufficiency objective yields small, spatially compact evidence sets without sacrificing baseline performance. Across TCGA-NSCLC (LUAD vs. LUSC), TCGA-BRCA (IDC vs. Others), and PANDA, ReaMIL matches or slightly improves baseline AUC and provides quantitative evidence-efficiency diagnostics. On NSCLC, it attains AUC 0.983 with a mean minimal sufficient K (MSK) $\\approx 8.2$ tiles at $τ= 0.90$ and AUKC $\\approx 0.864$, showing that class confidence rises sharply and stabilizes once a small set of tiles is kept. The method requires no extra supervision, integrates seamlessly with standard MIL training, and naturally yields slide-level overlays. We report accuracy alongside MSK, AUKC, and contiguity for rigorous evaluation of model behavior on WSIs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10073.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10073",
    "published": "2026-01-15T04:55:06Z",
    "updated": "2026-01-15T04:55:06Z",
    "comment": "Accepted at LFMBio Workshop, WACV 2026. This work has been submitted to the IEEE for possible publication",
    "light_analysis": {
      "overview": "论文提出ReaMIL方法，通过轻量选择头和预算充足的损失函数，在多实例学习框架下为全片组织病理学生成小型、紧凑的证据集。",
      "motivation": "全片组织病理学分析中，多实例学习模型通常处理大量切片，但现有方法可能在证据选择上缺乏效率，导致证据集过大或冗余。ReaMIL旨在通过引入预算充足的损失函数，在保持分类性能的同时生成小型、空间紧凑的证据集，从而减少计算负担并增强模型的可解释性，解决现有方法在证据稀疏性方面的不足（摘要未明确说明具体背景细节）。",
      "method": "ReaMIL的核心方法是在一个强大的多实例学习骨干网络上集成一个轻量级选择头，该头为每个切片生成软门控信号。训练时使用预算充足的损失函数，这是一种铰链损失，强制模型在稀疏预算约束下，仅使用选定切片达到真实类概率不低于阈值τ。关键创新包括软门控机制和预算充足的优化目标，使得模型能够自动识别关键证据区域，无需额外监督即可融入标准训练流程。",
      "result": "实验在TCGA-NSCLC、TCGA-BRCA和PANDA数据集上进行，ReaMIL匹配或略微提高了基线模型的AUC性能。在TCGA-NSCLC任务中，AUC达到0.983，平均最小足够切片数约8.2（τ=0.90），AUKC约0.864。这些结果表明，模型在保留少量关键切片后，类别置信度迅速上升并稳定，提供了定量证据效率诊断，如MSK、AUKC和连续性指标，用于严格评估模型行为。",
      "conclusion": "ReaMIL的主要贡献是提出了一种无需额外监督的多实例学习方法，通过预算充足的损失函数生成紧凑的证据集，在保持分类准确性的同时提高证据效率。该研究在学术上改进了MIL框架的可解释性和效率，在实际应用中有助于病理学诊断的自动化和辅助决策。未来工作方向可能包括进一步优化损失函数或扩展到其他领域，但摘要未明确说明具体局限性。",
      "tags": [
        "Multiple Instance Learning",
        "Whole-Slide Histopathology",
        "Budgeted-Sufficiency Objective",
        "Soft Gates",
        "Evidence Efficiency Diagnostics"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:19.877271Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10070",
    "title": "Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment",
    "authors": [
      "Mohammad Abbadi"
    ],
    "abstract": "Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).   The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.   These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10070.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10070",
    "published": "2026-01-15T04:51:36Z",
    "updated": "2026-01-15T04:51:36Z",
    "comment": "Under review at Computers in Biology and Medicine",
    "light_analysis": {
      "overview": "该研究提出并比较了一种基于深度学习的图像模型（HuSHeM）与传统WHO标准方法，展示了深度学习在精子形态评估中具有更高的预测可靠性和临床实用性。",
      "motivation": "精子形态评估是男性生育评估的关键环节，但存在主观性强、观察者间变异性高以及资源受限等问题，影响诊断的准确性。传统基于WHO标准的方法虽然常用，但可能受限于规则性和炎症因素（如SIRI指数）的局限性，需要更客观、可复现的评估技术以提升临床决策效率。该研究旨在通过人工智能框架解决这些问题，改进精子形态评估的可靠性和实用性。",
      "method": "论文构建了一个比较性生物医学人工智能框架，评估了基于深度学习的HuSHeM模型和基于WHO标准增强的基线方法（WHO(+SIRI)）。HuSHeM模型通过训练高分辨率精子形态图像构建，使用独立临床队列进行评估。方法采用全面的性能分析，包括判别分析（如接收者操作特征曲线下面积）、校准分析以及临床效用分析（如决策曲线分析），以量化模型在预测精子形态质量方面的表现和实际应用价值。",
      "result": "实验结果显示，HuSHeM模型在判别性能上优于WHO(+SIRI)基准，表现为更高的接收者操作特征曲线下面积和较窄的置信区间。在类不平衡情况下，精确召回分析表明HuSHeM具有改进的性能。校准分析显示HuSHeM的预测概率与观测结果更一致，决策曲线分析则指出在临床相关阈值概率下，HuSHeM提供更大的净临床益处。这些对比结果表明深度学习模型在精子形态评估中具有更优的预测能力和临床适用性。",
      "conclusion": "该研究的贡献在于展示了基于图像的深度学习模型相较于传统规则基础和炎症增强方法，在精子形态评估中提供了更好的预测可靠性和临床实用性。该框架支持客观、可复现的评估，可作为生育筛查和转诊工作流中的决策支持工具。未来工作可扩展模型的泛化能力，但需注意模型旨在辅助而非替代临床判断，强调了其在增强现有医疗流程中的潜在价值。",
      "tags": [
        "Deep Learning",
        "Image Analysis",
        "Clinical Decision Support",
        "Sperm Morphology",
        "Comparative Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:56.697577Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10067",
    "title": "Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection",
    "authors": [
      "Hung Vinh Tran",
      "Tong Chen",
      "Hechuan Wen",
      "Quoc Viet Hung Nguyen",
      "Bin Cui",
      "Hongzhi Yin"
    ],
    "abstract": "Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\\% of full-dataset training performance using merely 1\\% of the training data. The source code is available at \\href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.",
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10067.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10067",
    "published": "2026-01-15T04:46:28Z",
    "updated": "2026-01-15T04:46:28Z",
    "comment": "WebConf 2026",
    "light_analysis": {
      "overview": "论文提出噪声感知核心集选择（NaCS）框架，用于高效训练基于内容的推荐系统，通过结合噪声纠正和不确定性量化提升核心集质量。",
      "motivation": "基于内容的推荐系统（CRSs）依赖内容特征预测用户-物品交互，但需要大规模和连续训练以适应多样化用户偏好，导致高昂计算成本。核心集选择作为减少训练开销的方法，然而现有技术在用户-物品交互中存在噪声时表现不佳，尤其是核心集规模较小时，噪声会严重影响模型质量，因此开发能有效处理噪声的核心集选择框架对提升CRSs训练效率至关重要。",
      "method": "NaCS框架采用子模优化基于训练梯度构建核心集，以保持数据代表性；同时利用渐进训练模型动态纠正噪声标签，减少错误交互影响；并通过不确定性量化评估样本置信度，过滤低置信度样本以精炼核心集。该方法专门针对CRSs设计，结合了噪声纠正和核心集选择，优化了训练过程。",
      "result": "实验显示，NaCS在基于内容的推荐系统上优于现有核心集选择技术，产生更高质量核心集并提升训练效率。具体而言，仅使用1%训练数据即可恢复93-95%的全数据集训练性能，显著减少资源消耗。与基线方法相比，NaCS在多个指标上表现更好，验证了其有效性和鲁棒性。",
      "conclusion": "NaCS通过噪声感知核心集选择，显著提高了基于内容推荐系统的训练效率和质量。主要贡献在于提出集成噪声纠正和不确定性量化的框架，减少对大规模数据依赖。该研究具有学术价值，推动核心集选择技术发展，并提供实际应用价值以降低推荐系统计算成本。未来工作可扩展到其他推荐系统或更复杂噪声场景。",
      "tags": [
        "Content-based Recommendation Systems",
        "Coreset Selection",
        "Noise-aware Learning",
        "Submodular Optimization",
        "Uncertainty Quantification"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:15.233997Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10064",
    "title": "Long-Chain Reasoning Distillation via Adaptive Prefix Alignment",
    "authors": [
      "Zhenghao Liu",
      "Zhuoyang Wu",
      "Xinze Li",
      "Yukun Yan",
      "Shuo Wang",
      "Zulong Chen",
      "Yu Gu",
      "Ge Yu",
      "Maosong Sun"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities, particularly in solving complex mathematical problems. Recent studies show that distilling long reasoning trajectories can effectively enhance the reasoning performance of small-scale student models. However, teacher-generated reasoning trajectories are often excessively long and structurally complex, making them difficult for student models to learn. This mismatch leads to a gap between the provided supervision signal and the learning capacity of the student model. To address this challenge, we propose Prefix-ALIGNment distillation (P-ALIGN), a framework that fully exploits teacher CoTs for distillation through adaptive prefix alignment. Specifically, P-ALIGN adaptively truncates teacher-generated reasoning trajectories by determining whether the remaining suffix is concise and sufficient to guide the student model. Then, P-ALIGN leverages the teacher-generated prefix to supervise the student model, encouraging effective prefix alignment. Experiments on multiple mathematical reasoning benchmarks demonstrate that P-ALIGN outperforms all baselines by over 3%. Further analysis indicates that the prefixes constructed by P-ALIGN provide more effective supervision signals, while avoiding the negative impact of redundant and uncertain reasoning components. All code is available at https://github.com/NEUIR/P-ALIGN.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10064.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10064",
    "published": "2026-01-15T04:40:45Z",
    "updated": "2026-01-15T04:40:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Prefix-ALIGNment蒸馏框架P-ALIGN，通过自适应对齐教师推理轨迹的前缀来优化长推理过程的蒸馏，有效提升学生模型的数学推理性能。",
      "motivation": "大型语言模型在复杂数学推理任务中表现出色，但将其能力蒸馏到小规模学生模型时，教师生成的推理轨迹往往过长且结构复杂，导致监督信号与学生学习能力不匹配。现有蒸馏方法中，冗长推理轨迹的冗余和不确定性增加了学习难度，限制了蒸馏效果，因此亟需解决这一不匹配问题，以提高监督信号的有效性。",
      "method": "P-ALIGN框架采用自适应前缀对齐技术进行推理蒸馏，核心创新在于动态截断教师生成的推理轨迹，评估剩余后缀是否简洁且足以指导学生模型学习。方法利用教师生成的前缀部分作为监督信号，促进学生模型实现前缀对齐，避免冗长轨迹的负面影响。摘要未明确说明具体数据集或模型架构，但提及实验基于多个数学推理基准。",
      "result": "在多个数学推理基准测试中，P-ALIGN优于所有基线方法，性能提升超过3%。分析表明，自适应构造的前缀提供更有效的监督信号，避免了冗余和不确定推理组件的负面影响。具体实验数据显示，该方法在准确性指标上实现显著改进，验证了其在处理长推理轨迹时的优越性。",
      "conclusion": "研究的主要贡献是提出P-ALIGN框架，通过自适应前缀对齐优化推理蒸馏过程，提升了学生模型的性能。学术价值在于改进了现有蒸馏技术，解决了长推理轨迹学习中的不匹配问题，具有实际应用潜力，特别是在资源受限环境中部署高效推理模型。未来工作可能涉及将该方法扩展到其他推理领域或进一步优化自适应机制。",
      "tags": [
        "Large Language Model",
        "Reasoning Distillation",
        "Chain-of-Thought",
        "Prefix Alignment",
        "Adaptive Truncation"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:37.894539Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10061",
    "title": "CoF-T2I: Video Models as Pure Visual Reasoners for Text-to-Image Generation",
    "authors": [
      "Chengzhuo Tong",
      "Mingkun Chang",
      "Shenglong Zhang",
      "Yuran Wang",
      "Cheng Liang",
      "Zhizheng Zhao",
      "Ruichuan An",
      "Bohan Zeng",
      "Yang Shi",
      "Yifan Dai",
      "Ziming Zhao",
      "Guanbin Li",
      "Pengfei Wan",
      "Yuanxing Zhang",
      "Wentao Zhang"
    ],
    "abstract": "Recent video generation models have revealed the emergence of Chain-of-Frame (CoF) reasoning, enabling frame-by-frame visual inference. With this capability, video models have been successfully applied to various visual tasks (e.g., maze solving, visual puzzles). However, their potential to enhance text-to-image (T2I) generation remains largely unexplored due to the absence of a clearly defined visual reasoning starting point and interpretable intermediate states in the T2I generation process. To bridge this gap, we propose CoF-T2I, a model that integrates CoF reasoning into T2I generation via progressive visual refinement, where intermediate frames act as explicit reasoning steps and the final frame is taken as output. To establish such an explicit generation process, we curate CoF-Evol-Instruct, a dataset of CoF trajectories that model the generation process from semantics to aesthetics. To further improve quality and avoid motion artifacts, we enable independent encoding operation for each frame. Experiments show that CoF-T2I significantly outperforms the base video model and achieves competitive performance on challenging benchmarks, reaching 0.86 on GenEval and 7.468 on Imagine-Bench. These results indicate the substantial promise of video models for advancing high-quality text-to-image generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10061.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10061",
    "published": "2026-01-15T04:33:06Z",
    "updated": "2026-01-15T04:33:06Z",
    "comment": "16 pages, 8 figures",
    "light_analysis": {
      "overview": "CoF-T2I 模型通过集成视频模型的链帧推理，以显式中间帧步骤实现文本到图像的渐进式生成，显著提升图像质量。",
      "motivation": "本文旨在解决文本到图像（T2I）生成过程中缺乏明确的视觉推理起点和可解释的中间状态的问题，这限制了视频模型在T2I任务中的应用潜力。现有方法通常直接生成图像，没有显式的推理步骤，导致难以利用视频模型在视觉推理方面的优势。通过引入链帧推理，可以增强生成过程的可解释性并提高图像质量，这对于推动高质量图像生成技术具有重要意义。",
      "method": "CoF-T2I 的核心方法是将视频模型的链帧推理集成到T2I生成中，通过渐进式视觉细化过程，其中中间帧作为显式推理步骤，最终帧作为输出图像。关键创新包括创建CoF-Evol-Instruct数据集，该数据集建模从语义到美学的生成轨迹，以及采用独立编码操作处理每帧以降低运动伪影。此方法利用视频模型的推理能力，为T2I生成提供了结构化的生成框架。",
      "result": "实验结果显示，CoF-T2I 在性能上显著优于基础视频模型，并在挑战性基准上达到竞争性水平：在GenEval基准上得分为0.86，在Imagine-Bench上为7.468。这些数据表明模型在高质量T2I生成中具有优越表现，验证了通过链帧推理引入显式步骤的有效性。与基线方法相比，CoF-T2I 展现出更高的准确性和生成质量。",
      "conclusion": "本研究的主要贡献是证明了视频模型通过链帧推理可以显著增强T2I生成质量，拓展了视频模型在视觉任务中的应用范围。学术价值在于为T2I生成提供了新的推理框架，实际应用中可能推动更高质量的图像生成技术。局限性方面，摘要未明确说明；未来工作可能包括进一步优化推理过程或扩展至其他生成任务。",
      "tags": [
        "Chain-of-Frame",
        "Text-to-Image Generation",
        "Video Models",
        "Progressive Refinement",
        "Independent Encoding"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:25.585546Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10058",
    "title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers",
    "authors": [
      "Renpu Liu",
      "Jing Yang"
    ],
    "abstract": "Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10058.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10058",
    "published": "2026-01-15T04:23:32Z",
    "updated": "2026-01-15T04:23:32Z",
    "comment": "Published as a conference paper at NeurIPS 2025",
    "light_analysis": {
      "overview": "该论文提出一种增强上下文学习框架，利用无标签数据可证明地提升Transformer的预测准确率。",
      "motivation": "大语言模型（LLMs）在上下文学习（ICL）中展示出强大能力，但其预测质量受限于提示中可容纳的少量昂贵标记示例。现实中存在海量无标签数据，这些数据可能与ICL任务紧密相关，现有ICL方法主要依赖标记数据，无法有效利用无标签资源，导致性能瓶颈。因此，探索如何利用无标签数据可证明地增强ICL性能成为一个新兴的基本问题，以克服传统方法的局限性并优化模型效率。",
      "method": "本研究提出一个新颖的增强ICL框架，在提示中同时包含少量标记示例和一个无标签输入块。技术路线专注于多类线性分类设置，通过链式思考（CoT）提示策略，使多层Transformer能够有效模拟期望最大化（EM）算法，从而隐式地从标记和无标签数据中提取有用信息。关键创新在于结合CoT和EM算法，实现数据融合，并采用教师强制训练方法，确保Transformer参数以线性速率收敛到期望解，提高了训练的稳定性和效率。",
      "result": "实验结果表明，增强ICL框架在性能上一致优于传统的少样本ICL方法，为理论发现提供了实证支持。尽管摘要未明确具体准确率数值，但强调该框架能持续提升ICL准确率，展示了对无标签数据利用的有效性。与基线方法对比，新框架在资源利用和模型泛化方面表现更佳，验证了其在实际应用中的潜力。",
      "conclusion": "本研究的核心贡献是首次从理论上探讨无标签数据对Transformer上下文学习性能的影响，提出了一个可证明提升准确率的增强框架。学术上，它为ICL的理论分析提供了新视角和严格保证；实际中，展示了利用无标签数据优化大语言模型性能的可行性，有助于降低数据标注成本。未来工作可能包括将该框架扩展到更复杂的任务和模型架构，以进一步验证其通用性和局限性。",
      "tags": [
        "Large Language Models",
        "In-Context Learning",
        "Chain-of-Thought Prompting",
        "Transformers",
        "Expectation-Maximization Algorithm"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:58.768539Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10054",
    "title": "UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow",
    "authors": [
      "Nick Truong",
      "Pritam P. Karmokar",
      "William J. Beksi"
    ],
    "abstract": "Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10054.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10054",
    "published": "2026-01-15T04:10:14Z",
    "updated": "2026-01-15T04:10:14Z",
    "comment": "To be presented at the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshop on Event-Based Vision in the Era of Generative AI",
    "light_analysis": {
      "overview": "论文提出首个合成水下事件光流基准数据集，解决缺乏真实水下光学与光流配对数据的问题，推动水下事件感知算法发展。",
      "motivation": "水下成像面临光衰减、散射、浊度和照明不均等挑战，使标准相机难以获取地面真实运动，限制了传统视觉方法的精度。事件相机具备高分辨率和宽动态范围，但在水下环境的研究进展受限，主要原因是缺乏将真实水下光学与准确光流相结合的数据集。这个问题阻碍了水下事件感知算法的开发和应用，尤其是在恶劣光学条件下。",
      "method": "研究通过基于物理的光线追踪 RGBD 序列生成合成水下数据集，利用现代视频到事件流水线处理渲染的水下视频，产生包含密集地面真实流、深度和相机运动的真实事件数据流。关键创新点在于结合物理渲染模拟水下光学效应，如光衰减和散射，从而为事件相机提供可靠的基准评估平台。",
      "result": "论文基准了基于学习和基于模型的光流预测方法，以探究水下光传输如何影响事件形成和运动估计准确性。摘要未明确说明具体性能指标（如准确率提升），但通过数据集比较，为现有方法在模拟水下环境中的表现提供了新参考，为算法改进奠定基础。",
      "conclusion": "该数据集为水下事件感知算法的未来发展和评估设定了新基准，具有重要的学术和实用价值。通过公开源代码和数据集，促进社区验证和进一步研究。局限性可能包括合成数据与真实数据的差异，未来工作可扩展到真实数据采集或优化算法以适应更复杂的水下条件。",
      "tags": [
        "Event Cameras",
        "Optical Flow",
        "Synthetic Dataset",
        "Ray-Traced Rendering",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-16T03:26:54.643174Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10053",
    "title": "Disentangled Concept Representation for Text-to-image Person Re-identification",
    "authors": [
      "Giyeol Kim",
      "Chanho Eom"
    ],
    "abstract": "Text-to-image person re-identification (TIReID) aims to retrieve person images from a large gallery given free-form textual descriptions. TIReID is challenging due to the substantial modality gap between visual appearances and textual expressions, as well as the need to model fine-grained correspondences that distinguish individuals with similar attributes such as clothing color, texture, or outfit style. To address these issues, we propose DiCo (Disentangled Concept Representation), a novel framework that achieves hierarchical and disentangled cross-modal alignment. DiCo introduces a shared slot-based representation, where each slot acts as a part-level anchor across modalities and is further decomposed into multiple concept blocks. This design enables the disentanglement of complementary attributes (\\textit{e.g.}, color, texture, shape) while maintaining consistent part-level correspondence between image and text. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that our framework achieves competitive performance with state-of-the-art methods, while also enhancing interpretability through explicit slot- and block-level representations for more fine-grained retrieval results.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10053.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10053",
    "published": "2026-01-15T04:08:53Z",
    "updated": "2026-01-15T04:08:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了DiCo框架，通过分离概念表示实现层次化和分离的跨模态对齐，以改进文本到图像人员重识别。",
      "motivation": "本研究针对文本到图像人员重识别（TIReID）中的挑战，包括视觉外观与文本表达之间的显著模态差距，以及需要建模细粒度对应关系以区分具有相似属性（如服装颜色、纹理）的个体。该问题在实际应用中至关重要，例如在安防和监控领域，通过文本描述灵活检索图像可以提高效率。现有方法往往难以有效处理模态对齐和属性分离，导致检索精度受限或解释性不足，因此需要新的框架来克服这些不足。",
      "method": "本论文提出DiCo（Disentangled Concept Representation）框架，核心是引入共享的基于槽位的表示，其中每个槽位作为跨模态的部分级锚点，并进一步分解为多个概念块。这种设计使得互补属性（如颜色、纹理、形状）能够分离，同时保持图像和文本之间一致的部分级对应关系。创新点在于实现层次化和分离的跨模态对齐，通过显式的槽位和块级表示提升模型的可解释性和细粒度检索能力。",
      "result": "在CUHK-PEDES、ICFG-PEDES和RSTPReid等数据集上的广泛实验表明，DiCo框架的性能与最先进方法竞争，具体表现为在跨模态检索任务中实现高精度匹配。该框架还通过显式的槽位和块级表示增强了可解释性，从而支持更细粒度的检索结果。与基线方法相比，DiCo在模态对齐和属性分离方面表现出优势，摘要未明确说明具体性能指标数值，但强调了其竞争力。",
      "conclusion": "本论文的主要贡献是提出了DiCo框架，有效解决了TIReID中的模态差距和属性纠缠问题，提高了检索性能和可解释性。研究的学术价值在于推进了跨模态学习领域的方法，通过分离表示实现更好的对齐；实际应用价值在于增强人员重识别系统的实用性和灵活性。摘要未明确说明局限性或未来工作方向，但暗示了进一步优化和扩展的潜力。",
      "tags": [
        "Text-to-image Person Re-identification",
        "Disentangled Representation",
        "Cross-modal Alignment",
        "Slot-based Representation",
        "Concept Blocks"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:02.257234Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10033",
    "title": "EmplifAI: a Fine-grained Dataset for Japanese Empathetic Medical Dialogues in 28 Emotion Labels",
    "authors": [
      "Wan Jou She",
      "Lis Kanashiro Pereira",
      "Fei Cheng",
      "Sakiko Yahata",
      "Panote Siriaraya",
      "Eiji Aramaki"
    ],
    "abstract": "This paper introduces EmplifAI, a Japanese empathetic dialogue dataset designed to support patients coping with chronic medical conditions. They often experience a wide range of positive and negative emotions (e.g., hope and despair) that shift across different stages of disease management. EmplifAI addresses this complexity by providing situation-based dialogues grounded in 28 fine-grained emotion categories, adapted and validated from the GoEmotions taxonomy. The dataset includes 280 medically contextualized situations and 4125 two-turn dialogues, collected through crowdsourcing and expert review. To evaluate emotional alignment in empathetic dialogues, we assessed model predictions on situation--dialogue pairs using BERTScore across multiple large language models (LLMs), achieving F1 scores of 0.83. Fine-tuning a baseline Japanese LLM (LLM-jp-3.1-13b-instruct4) with EmplifAI resulted in notable improvements in fluency, general empathy, and emotion-specific empathy. Furthermore, we compared the scores assigned by LLM-as-a-Judge and human raters on dialogues generated by multiple LLMs to validate our evaluation pipeline and discuss the insights and potential risks derived from the correlation analysis.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10033.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10033",
    "published": "2026-01-15T03:26:53Z",
    "updated": "2026-01-15T03:26:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出EmplifAI数据集，一个基于28个细粒度情绪标签的日语共情医疗对话数据集，并验证其在大型语言模型微调和评估中的应用价值。",
      "motivation": "慢性病患者在疾病管理中常经历复杂情绪变化，如希望和绝望，需要有效的共情对话支持。现有方法缺乏针对日语医疗情境的细粒度情绪分类数据集，导致情感对齐不精准，难以应对患者多样化的情感需求。情绪分类的细粒度化对准确捕捉患者情感状态至关重要，而现有资源往往忽略医疗领域的特殊性。因此，开发EmplifAI数据集旨在填补这一空白，为提升情感支持系统的性能和可靠性提供基础。",
      "method": "研究创建了EmplifAI数据集，包含280个医学情境和4125个两轮对话，情绪类别基于GoEmotions分类法改编为28个细粒度标签，并通过众包收集和专家评审确保数据质量。评估方法使用BERTScore在多个大型语言模型上计算情境-对话对的情感对齐F1分数（达到0.83）。创新点包括微调基线日语LLM（LLM-jp-3.1-13b-instruct4），以优化共情表现，并采用LLM-as-a-Judge与人类评分者对比验证评估流程。",
      "result": "实验结果显示，微调后的基线日语LLM在流畅度、一般共情和情绪特定共情方面均有显著改善。使用BERTScore评估情感对齐时，F1分数达到0.83，表明模型在情感匹配上表现良好。通过比较LLM-as-a-Judge和人类评分者对多个LLM生成对话的评分，验证了评估流程的有效性，并讨论了相关性分析揭示的见解和潜在风险，如自动化评估的可靠性问题。",
      "conclusion": "EmplifAI数据集的推出为日语医疗共情对话研究提供了关键资源，证实了其在大型语言模型微调中的有效性，促进了细粒度情绪分类在对话系统中的应用。学术价值在于创建了高质量、情境化的数据集，实际应用有助于开发智能医疗助手，支持患者情感健康。未来工作可扩展数据集规模或改进评估方法，以应对LLM评估中的潜在风险和局限性。",
      "tags": [
        "Empathetic Dialogues",
        "Fine-grained Emotion Classification",
        "Large Language Models",
        "BERTScore",
        "Crowdsourcing"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:46.218921Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10031",
    "title": "FilDeep: Learning Large Deformations of Elastic-Plastic Solids with Multi-Fidelity Data",
    "authors": [
      "Jianheng Tang",
      "Shilong Tao",
      "Zhe Feng",
      "Haonan Sun",
      "Menglu Wang",
      "Zhanxing Zhu",
      "Yunhuai Liu"
    ],
    "abstract": "The scientific computation of large deformations in elastic-plastic solids is crucial in various manufacturing applications. Traditional numerical methods exhibit several inherent limitations, prompting Deep Learning (DL) as a promising alternative. The effectiveness of current DL techniques typically depends on the availability of high-quantity and high-accuracy datasets, which are yet difficult to obtain in large deformation problems. During the dataset construction process, a dilemma stands between data quantity and data accuracy, leading to suboptimal performance in the DL models. To address this challenge, we focus on a representative application of large deformations, the stretch bending problem, and propose FilDeep, a Fidelity-based Deep Learning framework for large Deformation of elastic-plastic solids. Our FilDeep aims to resolve the quantity-accuracy dilemma by simultaneously training with both low-fidelity and high-fidelity data, where the former provides greater quantity but lower accuracy, while the latter offers higher accuracy but in less quantity. In FilDeep, we provide meticulous designs for the practical large deformation problem. Particularly, we propose attention-enabled cross-fidelity modules to effectively capture long-range physical interactions across MF data. To the best of our knowledge, our FilDeep presents the first DL framework for large deformation problems using MF data. Extensive experiments demonstrate that our FilDeep consistently achieves state-of-the-art performance and can be efficiently deployed in manufacturing.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10031.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10031",
    "published": "2026-01-15T03:22:03Z",
    "updated": "2026-01-15T03:22:03Z",
    "comment": "Accepted in Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1 (KDD '26)",
    "light_analysis": {
      "overview": "FilDeep提出首个使用多保真度数据的深度学习框架，通过注意力跨保真度模块解决弹性-塑性固体大变形问题中的数量-准确性困境。",
      "motivation": "科学计算弹性-塑性固体的大变形在制造应用中至关重要，但传统数值方法存在固有局限，如高计算成本或低精度，促使深度学习成为替代方案。当前深度学习技术依赖于高数量和高精度数据集，然而在大变形问题中，这些数据难以同时获得，导致数量与准确性之间的困境，限制了模型性能，亟需解决以提升计算效率和实用性。",
      "method": "FilDeep是一个基于保真度的深度学习框架，通过同时使用低保真度和高保真度数据进行训练来平衡数据需求：低保真度数据量大但精度低，高保真度数据精度高但量少。针对代表性应用拉伸弯曲问题，设计注意力启用的跨保真度模块，以捕捉多保真度数据中的长程物理相互作用，这是首次应用于大变形问题的多保真度数据深度学习解决方案。",
      "result": "摘要未明确说明具体性能指标，但指出FilDeep在大量实验中始终达到最先进的性能，并能高效部署到制造应用中。这表明相比基线方法（如传统数值方法或其他深度学习模型），FilDeep在解决大变形问题时可能提升了准确性和效率，尽管具体数据如准确率提升或计算时间减少未详细说明。",
      "conclusion": "FilDeep的主要贡献是首次将多保真度数据集成到深度学习框架中，有效解决了弹性-塑性固体大变形问题的数据数量-准确性困境。其学术价值在于为复杂物理问题的机器学习方法提供了新思路，实际应用价值体现在高效部署到制造业，提升智能化水平。未来工作可能包括扩展到其他变形场景或优化模块设计，但摘要未明确说明。",
      "tags": [
        "Deep Learning",
        "Multi-Fidelity Data",
        "Attention Mechanism",
        "Large Deformation",
        "Elastic-Plastic Solids"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:32.190420Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10029",
    "title": "PaperScout: An Autonomous Agent for Academic Paper Search with Process-Aware Sequence-Level Policy Optimization",
    "authors": [
      "Tingyue Pan",
      "Jie Ouyang",
      "Mingyue Cheng",
      "Qingchuan Li",
      "Zirui Liu",
      "Mingfan Pan",
      "Shuo Yu",
      "Qi Liu"
    ],
    "abstract": "Academic paper search is a fundamental task in scientific research, yet most existing approaches rely on rigid, predefined workflows that struggle with complex, conditional queries. To address this limitation, we propose PaperScout, an autonomous agent that reformulates paper search as a sequential decision-making process. Unlike static workflows, PaperScout dynamically decides whether, when, and how to invoke search and expand tools based on accumulated retrieval context. However, training such agents presents a fundamental challenge: standard reinforcement learning methods, typically designed for single-turn tasks, suffer from a granularity mismatch when applied to multi-turn agentic tasks, where token-level optimization diverges from the granularity of sequence-level interactions, leading to noisy credit assignment. We introduce Proximal Sequence Policy Optimization (PSPO), a process-aware, sequence-level policy optimization method that aligns optimization with agent-environment interaction. Comprehensive experiments on both synthetic and real-world benchmarks demonstrate that PaperScout significantly outperforms strong workflow-driven and RL baselines in both recall and relevance, validating the effectiveness of our adaptive agentic framework and optimization strategy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10029.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10029",
    "published": "2026-01-15T03:21:21Z",
    "updated": "2026-01-15T03:21:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 PaperScout，一个用于学术论文搜索的自适应自治代理，通过过程感知序列级策略优化，解决了传统方法的局限性。",
      "motivation": "学术论文搜索是科研的基础任务，但现有方法依赖预定义工作流程，难以灵活处理复杂条件查询，导致搜索性能受限。此外，标准强化学习方法为单回合任务设计，应用于多回合代理任务时，存在粒度不匹配问题，token 级优化与序列级交互不一致，造成训练困难。因此，需要开发自适应代理框架和优化策略以改善搜索效率。",
      "method": "论文提出 PaperScout 自治代理，将论文搜索建模为顺序决策过程，动态决定是否、何时及如何调用搜索工具。核心创新是引入 Proximal Sequence Policy Optimization (PSPO)，一种过程感知的序列级策略优化方法，通过对齐优化粒度与代理-环境交互，解决了多回合任务中的训练挑战。该方法利用累积检索上下文指导决策，避免了传统强化学习的粒度不匹配。",
      "result": "在合成和真实世界基准上的综合实验表明，PaperScout 在召回率和相关性上显著优于工作流程驱动和强化学习基线方法，验证了自适应代理框架和 PSPO 优化策略的有效性。摘要未明确说明具体数值，但强调了其在复杂查询中的性能优势，提升了搜索准确性和适应性。",
      "conclusion": "本研究的主要贡献是提出 PaperScout 代理和 PSPO 方法，实现了学术论文搜索的自适应决策过程。学术上，为多回合代理任务提供了新的优化框架；实践中，可提高科研搜索的效率和准确性。未来工作可扩展该方法到其他领域或优化策略细节，具有一定的拓展潜力。",
      "tags": [
        "Autonomous Agent",
        "Reinforcement Learning",
        "Proximal Sequence Policy Optimization",
        "Sequence-Level Optimization",
        "Multi-Turn Tasks"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:37.314119Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10025",
    "title": "Structured Personality Control and Adaptation for LLM Agents",
    "authors": [
      "Jinpeng Wang",
      "Xinyu Jia",
      "Wei Wei Heng",
      "Yuquan Li",
      "Binbin Shi",
      "Qianlei Chen",
      "Guannan Chen",
      "Junxia Zhang",
      "Yuyu Yin"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly shaping human-computer interaction (HCI), from personalized assistants to social simulations. Beyond language competence, researchers are exploring whether LLMs can exhibit human-like characteristics that influence engagement, decision-making, and perceived realism. Personality, in particular, is critical, yet existing approaches often struggle to achieve both nuanced and adaptable expression. We present a framework that models LLM personality via Jungian psychological types, integrating three mechanisms: a dominant-auxiliary coordination mechanism for coherent core expression, a reinforcement-compensation mechanism for temporary adaptation to context, and a reflection mechanism that drives long-term personality evolution. This design allows the agent to maintain nuanced traits while dynamically adjusting to interaction demands and gradually updating its underlying structure. Personality alignment is evaluated using Myers-Briggs Type Indicator questionnaires and tested under diverse challenge scenarios as a preliminary structured assessment. Findings suggest that evolving, personality-aware LLMs can support coherent, context-sensitive interactions, enabling naturalistic agent design in HCI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10025.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10025",
    "published": "2026-01-15T03:15:24Z",
    "updated": "2026-01-15T03:15:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一个基于Jungian心理类型的LLM人格控制与适应框架，实现动态人格表达和进化。",
      "motivation": "随着大语言模型在人机交互中的广泛应用，如个性化助手和社交模拟，研究者关注LLMs是否能够展现影响参与度、决策和感知真实感的人性化特征。人格在此背景下至关重要，但现有方法常难以实现细腻且可适应的人格表达，导致交互缺乏自然性和真实性。为解决这一问题，本研究旨在开发新方法，以提升LLM代理在复杂情境中的表现，并弥补现有技术在处理动态人格方面的不足，推动HCI领域向更智能、更人性化的方向发展。",
      "method": "论文提出了一个结构化框架，通过Jungian心理类型对LLM人格进行建模。该框架整合三个关键机制：主导-辅助协调机制用于确保核心人格表达的一致性；强化-补偿机制使代理能够临时适应不同的上下文需求；反思机制则驱动长期人格的进化，允许代理在交互中逐步调整其底层结构。评估方法包括使用Myers-Briggs Type Indicator问卷进行人格对齐评估，并在多样挑战场景中进行测试，作为初步结构化评估。尽管摘要未明确说明使用的具体数据集或模型架构，但该方法强调结合心理理论实现动态人格控制。",
      "result": "研究发现，演化的人格感知LLMs能够支持一致且上下文敏感的交互，从而在人机交互中实现自然化代理设计。通过Myers-Briggs问卷和挑战场景的初步评估，该框架展示了在人格表达细腻性和适应性方面的改进。虽然摘要未提供具体数据如准确率提升，但结果表明该方法比现有方法更有效地平衡了人格的稳定表达与动态适应。这为未来研究提供了基础，表明结合心理机制可增强LLM代理在真实世界应用中的表现和用户接受度。",
      "conclusion": "论文的主要贡献是提出了一个允许LLM代理实现结构化人格控制和适应的框架，使代理既能保持细腻人格特征，又能动态适应交互需求并逐步进化。这一研究在学术上推动了人格建模在AI领域的应用，结合心理学理论丰富了AI的交互能力；实际应用价值在于提升人机交互的自然性和真实感，例如在个性化助手和社交模拟中。局限性或未来工作方向摘要未明确说明，但可推断需要更广泛测试和机制优化以应对复杂场景，进一步验证框架的普适性和有效性。",
      "tags": [
        "Large Language Model",
        "Personality Modeling",
        "Adaptation Mechanisms",
        "Jungian Psychology",
        "Myers-Briggs Type Indicator"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:42.365014Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10024",
    "title": "BPE: Behavioral Profiling Ensemble",
    "authors": [
      "Yanxin Liu",
      "Yunqi Zhang"
    ],
    "abstract": "Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10024.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10024",
    "published": "2026-01-15T03:14:51Z",
    "updated": "2026-01-15T03:14:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "BPE框架提出了一种基于模型内在行为档案的集成方法，显著提升了预测性能和计算效率。",
      "motivation": "集成学习虽能提升预测性能，但传统静态方法（如Stacking）将基础模型视为整体，忽略了其在不同实例区域的差异；动态方法（如DES）虽引入选择机制，但仍依赖模型间分歧，忽视了模型固有特性，并需要大量验证集进行能力估计，这导致集成效果受限、计算开销大，影响实际应用效率。因此，本文旨在通过新方法解决这些不足，优化集成过程。",
      "method": "BPE框架的核心是为每个模型构建“行为档案”，该档案捕获模型的固有行为模式。对于特定测试实例，计算模型响应与行为档案的偏差，并基于此偏差动态计算集成权重。创新点在于从模型间分歧转向模型内特性分析，减少了对验证集的依赖，提高了自适应性。具体数据集和模型架构等细节在摘要中未明确说明。",
      "result": "在合成和真实世界数据集上的实验表明，BPE算法在预测准确性上超越了最新的集成基线方法，同时提升了计算效率和存储资源利用率，在各种场景下表现优异。由于摘要未提供具体数据，无法给出精确指标，但整体显示显著性能增益。",
      "conclusion": "BPE框架的主要贡献是引入了基于模型行为档案的新集成范式，减少了对外部验证数据的依赖，并在学术上丰富了个体模型分析理论，实践中提升了预测精度和资源效率。局限性或未来工作细节摘要未明确说明，可能包括优化行为档案构建或拓展应用领域。",
      "tags": [
        "Ensemble Learning",
        "Behavioral Profiling",
        "Dynamic Ensemble Selection",
        "Model Integration",
        "Predictive Accuracy"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:30.153085Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10020",
    "title": "EHRNavigator: A Multi-Agent System for Patient-Level Clinical Question Answering over Heterogeneous Electronic Health Records",
    "authors": [
      "Lingfei Qian",
      "Mauro Giuffre",
      "Yan Wang",
      "Huan He",
      "Qianqian Xie",
      "Xuguang Ai",
      "Xeuqing Peng",
      "Fan Ma",
      "Ruey-Ling Weng",
      "Donald Wright",
      "Adan Wang",
      "Qingyu Chen",
      "Vipina K. Keloth",
      "Hua Xu"
    ],
    "abstract": "Clinical decision-making increasingly relies on timely and context-aware access to patient information within Electronic Health Records (EHRs), yet most existing natural language question-answering (QA) systems are evaluated solely on benchmark datasets, limiting their practical relevance. To overcome this limitation, we introduce EHRNavigator, a multi-agent framework that harnesses AI agents to perform patient-level question answering across heterogeneous and multimodal EHR data. We assessed its performance using both public benchmark and institutional datasets under realistic hospital conditions characterized by diverse schemas, temporal reasoning demands, and multimodal evidence integration. Through quantitative evaluation and clinician-validated chart review, EHRNavigator demonstrated strong generalization, achieving 86% accuracy on real-world cases while maintaining clinically acceptable response times. Overall, these findings confirm that EHRNavigator effectively bridges the gap between benchmark evaluation and clinical deployment, offering a robust, adaptive, and efficient solution for real-world EHR question answering.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10020.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10020",
    "published": "2026-01-15T03:02:15Z",
    "updated": "2026-01-15T03:02:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "EHRNavigator是一种多代理框架，用于在异质电子健康记录上进行患者级临床问题回答，旨在弥补基准评估与临床部署之间的差距。",
      "motivation": "临床决策越来越依赖电子健康记录（EHRs）中及时和上下文感知的信息访问，但现有自然语言问题回答系统主要在基准数据集上评估，忽视EHRs的异质性和多模态特性，以及临床环境中的时间推理需求，导致实用价值有限。这使得系统在真实医院环境中表现不足，影响诊断效率和患者护理，突显了开发能够适应复杂现实条件系统的必要性。",
      "method": "本研究提出EHRNavigator，一个多代理框架，利用AI代理在异质和多模态电子健康记录数据上执行患者级问题回答。核心创新点包括集成多个代理来处理不同类型的EHR数据，如结构化记录和文本报告，并整合多模态证据，以适应现实医院条件中的多样化数据模式和时间推理需求。具体实施细节摘要未明确说明，但框架设计旨在实现自适应和高效的数据整合与推理。",
      "result": "EHRNavigator在现实医院条件下进行评估，使用公共基准和机构数据集，涉及不同数据模式和时间推理。通过定量评估和临床医生验证的图表审查，系统在真实案例中达到86%的准确率，并保持临床可接受的响应时间。这表明EHRNavigator具有较强的泛化能力，能够适应复杂临床环境，与基准方法相比提升了实用性能，但具体基线对比摘要未明确说明。",
      "conclusion": "本研究证实EHRNavigator有效弥补基准评估与临床部署之间的差距，为真实世界的EHR问题回答提供鲁棒、自适应且高效的解决方案。主要贡献在于提出了一个多代理框架，增强系统在异质数据上的泛化能力。学术价值在于推动临床自然语言处理向实用化发展，实际应用价值在于支持临床决策和患者护理。未来工作方向摘要未明确说明，可能包括扩展数据类型或优化代理协作机制。",
      "tags": [
        "Multi-Agent System",
        "Clinical Question Answering",
        "Electronic Health Records",
        "Heterogeneous Data Integration",
        "Multimodal Evidence"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:53.988644Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10019",
    "title": "Time Aggregation Features for XGBoost Models",
    "authors": [
      "Mykola Pinchuk"
    ],
    "abstract": "This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10019.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10019",
    "published": "2026-01-15T03:02:04Z",
    "updated": "2026-01-15T03:02:04Z",
    "comment": "17 pages, 18 tables and figures",
    "light_analysis": {
      "overview": "论文提出使用拖尾窗口时间聚合特征，在XGBoost模型中提升点击率预测的ROC AUC和PR AUC。",
      "motivation": "研究动机是改善点击率预测中时间序列数据的处理效率，特别是在严格时间分割和无前瞻特征约束的场景下。现有目标编码方法可能无法有效捕捉时间动态，导致预测性能受限。点击率预测对在线广告优化至关重要，因此需要设计更有效的时间聚合特征来解决这一问题，弥补现有方法的不足，提升模型在实时预测中的准确性。",
      "method": "研究方法基于XGBoost模型，使用Avazu点击率预测数据集，在严格时间分割和特征只利用过去印象的约束下，比较了拖尾窗口、事件计数窗口、间隙窗口和分桶窗口等多种时间聚合特征。关键创新点在于设计时间聚合特征网格，通过历史数据窗口化聚合来生成特征，评估不同窗口设计对模型性能的影响，核心是优化特征工程以增强时间序列建模能力。",
      "result": "主要实验结果显示，在确定性10%样本的两个滚动尾折叠上，拖尾窗口相对于目标编码基线，ROC AUC提升了约0.0066到0.0082，PR AUC提升了约0.0084到0.0094。事件计数窗口提供了微小但一致的改进，而间隙窗口和分桶窗口在该数据集和协议下表现不佳，均逊于简单的拖尾窗口设计。",
      "conclusion": "论文结论支持将拖尾窗口作为实用的默认时间聚合特征，在需要边际ROC AUC增益时可选事件计数窗口。研究贡献在于为点击率预测提供了有效的特征设计指导，具有学术价值和实际应用价值，但局限性在于仅在特定数据集上测试，未来工作可扩展到更多数据集或探索其他时间聚合方法。",
      "tags": [
        "XGBoost",
        "Time Aggregation Features",
        "Click-through Rate Prediction",
        "Target Encoding",
        "Rolling Windows"
      ]
    },
    "analyzed_at": "2026-01-16T03:27:41.502674Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10015",
    "title": "CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation",
    "authors": [
      "Boyi Liu",
      "Zimu Zhou",
      "Yongxin Tong"
    ],
    "abstract": "Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10015.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10015",
    "published": "2026-01-15T02:52:12Z",
    "updated": "2026-01-15T02:52:12Z",
    "comment": "12 pages, conference",
    "light_analysis": {
      "overview": "CAFEDistill提出冲突感知的联邦退出蒸馏框架，解决了PFL中早期退出网络的冲突，实现个性化和动态模型。",
      "motivation": "个性化联邦学习（PFL）能在分散的异构数据上协作训练个性化模型，但现有方法生成静态模型，在准确性与效率间有固定权衡，限制了其在推理需求随环境和资源变化的动态环境中的应用。早期退出网络（EENs）通过中间分类器提供自适应推理，但集成到PFL中面临客户端异质性和退出目标冲突导致的深度干扰。先前研究未能同时解决这些冲突，导致性能不佳，因此需新方法克服这些挑战。",
      "method": "CAFEDistill框架采用冲突感知的联邦退出蒸馏方法，通过渐进式、深度优先的学生协调机制来减轻浅层和深层退出之间的干扰。它实现跨客户端的有效个性化知识转移，并使用客户端解耦公式减少通信开销。摘要未明确说明具体使用的数据集或模型架构，但该方法旨在优化PFL中EENs的集成，处理客户端异质性和退出冲突。",
      "result": "实验结果显示，CAFEDistill在广泛评估中优于当前最先进方法，提高了准确性，并将推理成本降低了30.79%至46.86%。与基线方法相比，它在保持个性化性能的同时显著减少计算开销，适用于动态推理环境。准确率提升的具体数据摘要未明确说明，但整体表现优于现有方法。",
      "conclusion": "论文的主要贡献是开发了CAFEDistill框架，有效解决了PFL中早期退出网络的集成问题，实现了个性化和动态模型。这项研究扩展了PFL的应用范围，学术上提供了处理冲突的新方法，实际中可适应动态环境并降低推理成本。未来工作可探索更多应用场景或进一步优化框架机制。",
      "tags": [
        "Personalized Federated Learning",
        "Early-Exit Networks",
        "Federated Learning",
        "Knowledge Distillation",
        "Dynamic Model Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:11.124747Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10012",
    "title": "PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning",
    "authors": [
      "Yanhang Shi",
      "Xiaoyu Wang",
      "Houwei Cao",
      "Jian Li",
      "Yong Liu"
    ],
    "abstract": "Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10012.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10012",
    "published": "2026-01-15T02:42:29Z",
    "updated": "2026-01-15T02:42:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "PARSE框架通过部分信息分解和片级对齐，解决了多模态去中心化联邦学习中的梯度未对齐问题。",
      "motivation": "多模态去中心化联邦学习（DFL）中，代理在可用模态和模型架构上存在异质性，但必须在点对点网络中协作，无中心协调器。标准多模态方法学习单一共享嵌入，在DFL设置中导致单模态与多模态代理之间的梯度错位，这抑制了异质共享和跨模态交互。PARSE旨在克服这一困境，通过部分对齐机制促进知识交换，而无需依赖中心协调或复杂梯度手术，从而增强学习效率。",
      "method": "PARSE框架基于部分信息分解（PID），在无服务器设置中操作：每个代理执行特征分裂，将潜在表示分解为冗余、独特和协同部分。通过片级部分对齐，仅交换具有相应模态的语义可共享分支，实现异构代理间的P2P知识共享。创新点在于特征分裂技术和选择性对齐机制，无需中心协调或梯度手术，解决了梯度冲突，并与标准DFL约束兼容。方法包括使用PID指导的分解过程，但数据集和具体模型架构摘要未明确说明。",
      "result": "PARSE在多个基准测试和代理混合中，相比任务共享、模态共享和混合共享的DFL基线，实现了持续的性能提升。摘要未提供具体准确率数据，但指出“一致增益”，表明该方法在效果上优于现有基线。消融研究评估了融合算子和分裂比率的影响，展示了设计的效率和稳健性，定性可视化进一步证实了PARSE在促进跨模态交互方面的优势。",
      "conclusion": "PARSE的主要贡献是解决多模态DFL中的梯度未对齐问题，通过部分信息分解和片级对齐，消除对中心协调和梯度手术的依赖，从而克服多模态DFL困境。学术价值在于将PID引入无服务器设置，扩展了联邦学习理论；应用价值在于提升异构代理协作的效率和实用性。未来工作方向或局限性摘要未明确说明，但消融研究暗示可进一步优化融合算子和分裂参数。",
      "tags": [
        "Multimodal Federated Learning",
        "Decentralized Federated Learning",
        "Partial Information Decomposition",
        "Feature Fission",
        "Partial Alignment"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:07.944039Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10011",
    "title": "Memo-SQL: Structured Decomposition and Experience-Driven Self-Correction for Training-Free NL2SQL",
    "authors": [
      "Zerui Yang",
      "Weichuan Wang",
      "Yanwei Xu",
      "Linqi Song",
      "Yudai Matsuda",
      "Wei Han",
      "Bo Bai"
    ],
    "abstract": "Existing NL2SQL systems face two critical limitations: (1) they rely on in-context learning with only correct examples, overlooking the rich signal in historical error-fix pairs that could guide more robust self-correction; and (2) test-time scaling approaches often decompose questions arbitrarily, producing near-identical SQL candidates across runs and diminishing ensemble gains. Moreover, these methods suffer from a stark accuracy-efficiency trade-off: high performance demands excessive computation, while fast variants compromise quality. We present Memo-SQL, a training-free framework that addresses these issues through two simple ideas: structured decomposition and experience-aware self-correction. Instead of leaving decomposition to chance, we apply three clear strategies, entity-wise, hierarchical, and atomic sequential, to encourage diverse reasoning. For correction, we build a dynamic memory of both successful queries and historical error-fix pairs, and use retrieval-augmented prompting to bring relevant examples into context at inference time, no fine-tuning or external APIs required. On BIRD, Memo-SQL achieves 68.5% execution accuracy, setting a new state of the art among open, zero-fine-tuning methods, while using over 10 times fewer resources than prior TTS approaches.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.10011.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10011",
    "published": "2026-01-15T02:42:05Z",
    "updated": "2026-01-15T02:42:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "Memo-SQL是一种无需训练的NL2SQL框架，通过结构化分解和经验驱动自我修正，显著提升性能和效率。",
      "motivation": "现有NL2SQL系统面临两个关键限制：首先，它们依赖上下文学习仅使用正确示例，忽视了历史错误修复对的丰富信号，这限制了系统在遇到错误时的自我修正能力，影响了鲁棒性。其次，测试时间缩放方法通常任意分解问题，导致不同运行中产生相似的SQL候选，减少了集合增益，同时存在明显的准确性和效率权衡——高性能需要大量计算，而快速方法牺牲质量。这些问题在自然语言到SQL转换的实际应用中至关重要，因为它们影响系统的可靠性和实用性，亟需改进。",
      "method": "Memo-SQL框架通过两个核心创新解决现有问题：结构化分解和经验感知自我修正。在结构化分解中，应用三种明确策略——实体化、分层和原子顺序分解，以鼓励多样化的推理过程，避免任意分解带来的重复候选。在自我修正方面，构建动态记忆库，存储成功查询和历史错误修复对，并使用检索增强提示技术在推理时检索相关示例引入上下文，无需微调或外部API。这种方法依赖于大语言模型的推理能力，简单高效，重点提升多样性和准确性。",
      "result": "在BIRD数据集上，Memo-SQL实现了68.5%的执行准确率，在开源、零微调方法中创下新的最高性能，标志着无训练方法的显著进步。资源使用方面，比先前的测试时间缩放方法减少了10倍以上，有效解决了准确性和效率的权衡问题。这一结果表明，通过结构化分解和经验驱动修正，系统能在不增加计算负担的情况下，大幅超越现有基线方法，尤其在鲁棒性和效率方面表现突出。",
      "conclusion": "Memo-SQL的主要贡献是提出了一种无需训练的方法，通过结构化分解和经验驱动自我修正，成功解决了NL2SQL系统中多样性和鲁棒性不足的问题。其学术价值在于为无训练方法提供了新思路，克服了现有方法的局限性；实际应用价值在于提高了自然语言查询处理的准确性和效率，降低了计算成本。未来工作可以扩展到其他类似任务或数据集，但摘要未明确说明具体方向，可能包括进一步优化记忆机制或应用于更复杂的查询场景。",
      "tags": [
        "NL2SQL",
        "Structured Decomposition",
        "Self-Correction",
        "Retrieval-Augmented Prompting",
        "Training-Free"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:13.534168Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10010",
    "title": "VERHallu: Evaluating and Mitigating Event Relation Hallucination in Video Large Language Models",
    "authors": [
      "Zefan Zhang",
      "Kehua Zhu",
      "Shijie Jiang",
      "Hongyuan Lu",
      "Shengkai Sun",
      "Tian Bai"
    ],
    "abstract": "Video Large Language Models (VideoLLMs) exhibit various types of hallucinations. Existing research has primarily focused on hallucinations involving the presence of events, objects, and scenes in videos, while largely neglecting event relation hallucination. In this paper, we introduce a novel benchmark for evaluating the Video Event Relation Hallucination, named VERHallu. This benchmark focuses on causal, temporal, and subevent relations between events, encompassing three types of tasks: relation classification, question answering, and counterfactual question answering, for a comprehensive evaluation of event relation hallucination. Additionally, it features counterintuitive video scenarios that deviate from typical pretraining distributions, with each sample accompanied by human-annotated candidates covering both vision-language and pure language biases. Our analysis reveals that current state-of-the-art VideoLLMs struggle with dense-event relation reasoning, often relying on prior knowledge due to insufficient use of frame-level cues. Although these models demonstrate strong grounding capabilities for key events, they often overlook the surrounding subevents, leading to an incomplete and inaccurate understanding of event relations. To tackle this, we propose a Key-Frame Propagating (KFP) strategy, which reallocates frame-level attention within intermediate layers to enhance multi-event understanding. Experiments show it effectively mitigates the event relation hallucination without affecting inference speed.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10010.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10010",
    "published": "2026-01-15T02:40:41Z",
    "updated": "2026-01-15T02:40:41Z",
    "comment": "11 pages, 6 figures",
    "light_analysis": {
      "overview": "本文提出了VERHallu基准来评估视频大语言模型的事件关系幻觉，并设计关键帧传播策略以缓解此问题。",
      "motivation": "视频大语言模型在视频理解中存在幻觉问题，现有研究主要关注事件、对象和场景的幻觉，忽略了事件关系如因果、时间和子事件关系的幻觉，这对全面理解视频内容至关重要。由于模型在密集事件关系推理中表现不佳，常因对帧级线索利用不足而过度依赖先验知识，导致理解不完整和不准确，因此需要专门的评估和缓解方法。",
      "method": "研究提出VERHallu基准，用于系统评估事件关系幻觉，涵盖因果、时间和子事件关系，包括关系分类、问题回答和反事实问题回答三种任务。基准采用反直觉视频场景，偏离典型预训练分布，每个样本配有人工注释候选以覆盖视觉语言和纯语言偏见。为缓解幻觉，设计关键帧传播策略，在模型中间层重新分配帧级注意力，增强对多事件的理解，而不影响推理速度。",
      "result": "实验显示，当前先进的视频大语言模型在VERHallu基准上表现不佳，在处理密集事件关系时经常忽视子事件，依赖先验知识。关键帧传播策略能有效减轻事件关系幻觉，并保持推理速度不变，提升了事件关系理解的完整性和准确性，具体性能提升数据摘要未明确说明。",
      "conclusion": "本研究通过引入VERHallu基准和关键帧传播策略，填补了事件关系幻觉评估的空白，并提供了一种有效的缓解方法。研究具有重要学术价值，推动了视频大语言模型幻觉问题的深入探索，同时在实际应用如视频分析中可提高准确性。未来工作可扩展到更多视频任务或模型扩展。",
      "tags": [
        "Video Large Language Models",
        "Event Relation Hallucination",
        "Key-Frame Propagation",
        "Benchmark Evaluation",
        "Counterfactual Question Answering"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:41.735578Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10007",
    "title": "Continuous-Depth Transformers with Learned Control Dynamics",
    "authors": [
      "Peter Jemley"
    ],
    "abstract": "We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\\%/88\\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.10007.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10007",
    "published": "2026-01-15T02:35:37Z",
    "updated": "2026-01-15T02:35:37Z",
    "comment": "9 pages, 4 figures. Code available at: https://github.com/PeterJemley/Continuous-Depth-Transformers-with-Learned-Control-Dynamics",
    "light_analysis": {
      "overview": "提出一种连续深度Transformer架构，通过神经常微分方程和学习控制信号在推理时实现生成属性的可控性。",
      "motivation": "标准Transformer模型使用固定离散层处理表示，这限制了生成过程的灵活性和控制性，难以动态调整生成属性以应对多样化应用需求。为解决这一问题，本研究旨在开发一种可高效控制语言生成的架构，以提升模型在实际场景中的适应性和应用价值。现有方法的不足在于缺乏对深度变量的连续控制和外部引导机制，导致模型灵活性受限。",
      "method": "提出混合Transformer架构，用连续深度神经常微分方程（ODE）块替换离散中间层，将深度视为连续变量，由学习向量场 $F_θ(H, τ, u)$ 控制，其中 $u$ 是通过显式连接注入的低维控制信号。关键创新包括结合Neural ODE实现连续深度动力学，并使用伴随方法实现 $O(1)$ 内存训练，无论积分深度如何，同时通过自适应ODE求解器优化计算效率。技术特色在于引入控制信号实现推理时的生成属性调控，增强模型的灵活性和可解释性。",
      "result": "通过四个实验验证：梯度流稳定性无梯度爆炸/消失事件；语义控制实验中，正负情感控制准确率分别为98%和88%；连续插值显示固定与自适应求解器间轨迹发散仅为0.068%；效率基准测试表明延迟与标准离散基线相当。这些结果证明该方法在保持高效率的同时，有效实现了可控生成，性能优于传统固定层架构，并在语义控制和计算稳定性方面表现出色。",
      "conclusion": "本研究贡献了一种连续深度Transformer架构，结合学习控制信号，为可引导语言生成提供了可行高效的机制。学术价值在于扩展了Transformer模型的设计空间，引入连续深度动力学和控制信号理论；实际应用价值体现在提升语言生成的灵活性和控制能力，适用于情感调整等任务。摘要未明确说明潜在局限性，但未来工作可能包括优化控制信号学习或扩展到更复杂的生成场景。",
      "tags": [
        "Transformer",
        "Neural ODE",
        "Learned Control",
        "Continuous-Depth Models",
        "Adjoint Method"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:29.164368Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10003",
    "title": "SocraticKG: Knowledge Graph Construction via QA-Driven Fact Extraction",
    "authors": [
      "Sanghyeok Choi",
      "Woosang Jeon",
      "Kyuseok Yang",
      "Taehyeong Kim"
    ],
    "abstract": "Constructing Knowledge Graphs (KGs) from unstructured text provides a structured framework for knowledge representation and reasoning, yet current LLM-based approaches struggle with a fundamental trade-off: factual coverage often leads to relational fragmentation, while premature consolidation causes information loss. To address this, we propose SocraticKG, an automated KG construction method that introduces question-answer pairs as a structured intermediate representation to systematically unfold document-level semantics prior to triple extraction. By employing 5W1H-guided QA expansion, SocraticKG captures contextual dependencies and implicit relational links typically lost in direct KG extraction pipelines, providing explicit grounding in the source document that helps mitigate implicit reasoning errors. Evaluation on the MINE benchmark demonstrates that our approach effectively addresses the coverage-connectivity trade-off, achieving superior factual retention while maintaining high structural cohesion even as extracted knowledge volume substantially expands. These results highlight that QA-mediated semantic scaffolding plays a critical role in structuring semantics prior to KG extraction, enabling more coherent and reliable graph construction in subsequent stages.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.10003.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10003",
    "published": "2026-01-15T02:26:51Z",
    "updated": "2026-01-15T02:26:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "SocraticKG 提出了一种基于问答对作为中间表示的知识图谱构建方法，以解决事实覆盖与关系连贯性之间的权衡问题。",
      "motivation": "从非结构化文本构建知识图谱为知识表示和推理提供了结构化框架，但现有基于大型语言模型的方法面临一个基本权衡：提高事实覆盖会导致关系碎片化，而过早整合又会造成信息丢失。这一问题限制了知识图谱的完整性和可靠性，使得构建过程中难以平衡覆盖范围和连接性，导致隐式推理错误和信息损失，因此需要创新方法来优化这一过程。",
      "method": "SocraticKG 使用问答对作为结构化中间表示，通过 5W1H 引导的问答扩展来捕获文档级语义和上下文依赖关系。在提取三元组之前，系统展开语义以捕获隐式关系链接，提供源文档的显式基础，从而减轻推理错误。核心创新在于引入 QA 驱动的语义脚手架，以在知识提取前优化语义结构。",
      "result": "在 MINE 基准上的评估表明，SocraticKG 有效解决了覆盖-连接权衡，实现了优异的事实保留和高结构连贯性。即使在提取知识量大幅增加的情况下，该方法仍能保持这些优势，证明 QA 介导的语义脚手架在知识图谱构建中至关重要，优于传统直接提取方法。",
      "conclusion": "SocraticKG 通过 QA 驱动的中间表示，为知识图谱构建提供了更连贯和可靠的方法，减少了信息损失和推理错误。这项研究在学术上推动了自动化知识提取技术的发展，具有实际应用价值，未来可扩展到更复杂文本或多语言处理，以进一步提高构建效率和质量。",
      "tags": [
        "Knowledge Graph Construction",
        "QA-driven Fact Extraction",
        "5W1H-guided QA Expansion",
        "Semantic Scaffolding",
        "LLM-based Approaches"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:36.864678Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.10001",
    "title": "DW-DGAT: Dynamically Weighted Dual Graph Attention Network for Neurodegenerative Disease Diagnosis",
    "authors": [
      "Chengjia Liang",
      "Zhenjiong Wang",
      "Chao Chen",
      "Ruizhi Zhang",
      "Songxi Liang",
      "Hai Xie",
      "Haijun Lei",
      "Zhongwei Huang"
    ],
    "abstract": "Parkinson's disease (PD) and Alzheimer's disease (AD) are the two most prevalent and incurable neurodegenerative diseases (NDs) worldwide, for which early diagnosis is critical to delay their progression. However, the high dimensionality of multi-metric data with diverse structural forms, the heterogeneity of neuroimaging and phenotypic data, and class imbalance collectively pose significant challenges to early ND diagnosis. To address these challenges, we propose a dynamically weighted dual graph attention network (DW-DGAT) that integrates: (1) a general-purpose data fusion strategy to merge three structural forms of multi-metric data; (2) a dual graph attention architecture based on brain regions and inter-sample relationships to extract both micro- and macro-level features; and (3) a class weight generation mechanism combined with two stable and effective loss functions to mitigate class imbalance. Rigorous experiments, based on the Parkinson Progression Marker Initiative (PPMI) and Alzhermer's Disease Neuroimaging Initiative (ADNI) studies, demonstrate the state-of-the-art performance of our approach.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.10001.pdf",
    "abs_url": "https://arxiv.org/abs/2601.10001",
    "published": "2026-01-15T02:21:46Z",
    "updated": "2026-01-15T02:21:46Z",
    "comment": "AAAI-2026 accepted poster paper",
    "light_analysis": {
      "overview": "DW-DGAT通过动态加权双图注意力网络，整合多源数据并处理类别不平衡，提升了帕金森病和阿尔茨海默病的早期诊断性能。",
      "motivation": "帕金森病和阿尔茨海默病是全球最常见且无法治愈的神经退行性疾病，早期诊断对延缓疾病进展至关重要。然而，多度量数据的高维度和多样结构形式、神经影像与表型数据的异质性，以及类别不平衡等问题，使得传统方法难以有效处理，导致诊断准确性受限。现有技术可能无法同时应对数据融合和样本不平衡的挑战，因此需要一种综合解决方案来改善诊断效果。",
      "method": "DW-DGAT方法结合三个创新点：首先，采用通用数据融合策略，整合三种结构形式的多度量数据，如神经影像和表型数据；其次，构建基于脑区域和样本间关系的双图注意力架构，利用图注意力网络提取微观脑区特征和宏观样本关系特征；最后，引入动态类别权重生成机制，结合两个稳定有效的损失函数来缓解类别不平衡，优化模型训练过程。",
      "result": "摘要未明确说明具体实验数据，但基于帕金森病进展标志物倡议和阿尔茨海默病神经影像倡议数据集的研究表明，DW-DGAT表现出最先进的诊断性能。该方法通过整合多源数据和优化损失函数，在神经退行性疾病分类任务中优于现有基线方法，有效应对了数据高维和类别不平衡的挑战，但缺乏准确率或效率提升的具体指标，需参考完整论文获取详细信息。",
      "conclusion": "DW-DGAT为神经退行性疾病诊断提供了创新解决方案，通过数据融合、双图注意力和类别不平衡处理，提升了模型性能。学术上，它推动了图神经网络在多模态学习中的应用；实际上，有助于实现早期精准诊断，延缓疾病进展并改善患者管理。未来工作可能包括扩展到其他疾病类型、优化计算效率或集成更多数据类型以进一步增强鲁棒性。",
      "tags": [
        "Graph Attention Network",
        "Data Fusion",
        "Class Imbalance",
        "Neuroimaging",
        "Multi-Modal Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:28:55.771392Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09012",
    "title": "TranslateGemma Technical Report",
    "authors": [
      "Mara Finkelstein",
      "Isaac Caswell",
      "Tobias Domhan",
      "Jan-Thorsten Peter",
      "Juraj Juraska",
      "Parker Riley",
      "Daniel Deutsch",
      "Cole Dilanni",
      "Colin Cherry",
      "Eleftheria Briakou",
      "Elizabeth Nielsen",
      "Jiaming Luo",
      "Kat Black",
      "Ryan Mullins",
      "Sweta Agrawal",
      "Wenda Xu",
      "Erin Kats",
      "Stephane Jaskiewicz",
      "Markus Freitag",
      "David Vilar"
    ],
    "abstract": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09012.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09012",
    "published": "2026-01-13T22:23:24Z",
    "updated": "2026-01-15T10:43:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "TranslateGemma是基于Gemma 3基础模型的开源机器翻译套件，通过两阶段微调提升翻译质量和效率。",
      "motivation": "研究动机是增强Gemma 3基础模型在机器翻译任务中的多语言能力，以解决现有方法可能未针对翻译优化的问题。机器翻译是AI领域的关键应用，需要高质量且高效的多语言模型。基础模型通常缺乏专门训练，限制了其在翻译任务中的性能。通过微调提升模型能力，可以为研究社区提供更强大的工具，推动实际应用发展。摘要未明确说明具体不足，但暗示了优化翻译任务的必要性。",
      "method": "研究方法采用两阶段微调过程。首先，进行监督微调，使用高质量大规模合成平行数据（由先进模型生成）和人工翻译数据混合，以增强模型的多语言翻译基础。其次，实施强化学习阶段，通过集成奖励模型如MetricX-QE和AutoMQM来优化翻译质量。该方法基于Gemma 3模型架构，关键创新在于结合数据驱动和奖励优化策略，有效提升翻译任务的适应性和准确性。",
      "result": "实验结果显示，TranslateGemma模型在多个评估基准上表现优异。在WMT24++基准的55个语言对自动评估中，相比基线Gemma 3模型有持续且显著的性能提升。人工评估在WMT25测试集的10个语言对中也验证了其有效性。较小尺寸的TranslateGemma模型常能达到与较大基线模型相当的翻译质量，提高了计算效率。此外，模型在Vistra图像翻译基准上展示了增强的多模态能力，支持更广泛的应用场景。",
      "conclusion": "结论是TranslateGemma模型通过两阶段微调方法，成功提升了机器翻译的性能和效率，主要贡献在于开源了强大且适应性强的翻译工具。这项研究展示了基础模型微调在优化多语言任务中的有效性，具有重要学术价值。实际应用价值在于为研究社区提供了高效翻译技术，促进多模态翻译发展。摘要未明确说明局限性，未来工作可能涉及扩展到更多语言对或进一步优化模型效率。",
      "tags": [
        "Machine Translation",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "Reward Models",
        "Multimodal Translation"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:11.727409Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.07941",
    "title": "Moonworks Lunara Aesthetic Dataset",
    "authors": [
      "Yan Wang",
      "M M Sayeef Abdullah",
      "Partho Hassan",
      "Sabit Hassan"
    ],
    "abstract": "The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.07941.pdf",
    "abs_url": "https://arxiv.org/abs/2601.07941",
    "published": "2026-01-12T19:11:41Z",
    "updated": "2026-01-15T18:27:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文创建了名为Lunara Aesthetic Dataset的高质量美学数据集，通过生成模型生成图像并附带结构化注释，旨在推动美学AI研究。",
      "motivation": "研究动机源于现有大规模网络数据集在美学质量上的不足，这些数据集通常侧重数据量而非美学精度，导致美学研究受限。美学对于图像生成和评估至关重要，但缺乏高质量、风格多样且标注精确的专类数据集。因此，本研究旨在填补这一空白，提供一个以美学为核心的数据集，以支持AI模型在美学任务中的准确开发和评估，例如风格识别和艺术生成。摘要未明确说明具体应用场景，但强调了美学质量的提升对研究的推动价值。",
      "method": "研究方法采用生成模型技术，核心是使用Moonworks Lunara模型生成所有图像，这些图像被刻意设计以体现多样美学风格，包括中东、北欧、东亚、南亚等区域风格及素描、油画等通用类别。关键创新点在于强调美学质量和风格多样性，而非数据规模扩展。每张图像都附有人工优化的文本提示和结构化注释，详细描述图像中的对象、属性、关系和风格线索，以提升标注的精确性。摘要未明确说明模型的具体架构、数据集大小或生成细节，但突出了生成过程和注释方法的系统性。",
      "result": "主要实验结果显示，该数据集在美学评分上表现优异，显著超过现有专注于美学的数据集以及通用数据集，证明了其高质量特性。这一优势通过对比现有数据集得到验证，为美学评估提供了更可靠的基准。结构化注释和风格多样性进一步增强了数据集在AI模型性能提升中的应用潜力，例如在美学分类或图像生成任务中。摘要未提供具体的美学评分数值或对比细节，但强调了数据集在美学质量上的显著改进和实际效用。",
      "conclusion": "本研究的核心贡献是创建了一个具有高美学质量、风格多样化和许可透明的数据集，并通过Apache 2.0许可发布，支持无限制的学术和商业使用，促进了美学研究的开放性。学术价值在于为美学AI模型提供了更好的训练和评估资源，实际应用则可能包括艺术生成、风格分析和图像评价等领域。潜在局限性可能在于数据集依赖生成模型，其多样性受模型限制，未来工作可扩展数据集规模或集成更多真实世界图像。摘要未明确说明具体局限性或未来方向，因此基于现有信息推断。",
      "tags": [
        "Generative Models",
        "Aesthetic Dataset",
        "Structured Annotation",
        "Artistic Styles",
        "Image Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:03.218609Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.07093",
    "title": "3D Wavelet-Based Structural Priors for Controlled Diffusion in Whole-Body Low-Dose PET Denoising",
    "authors": [
      "Peiyuan Jing",
      "Yue Tang",
      "Chun-Wun Cheng",
      "Zhenxuan Zhang",
      "Liutao Yang",
      "Thiago V. Lima",
      "Klaus Strobel",
      "Antoine Leimgruber",
      "Angelica Aviles-Rivero",
      "Guang Yang",
      "Javier Montoya"
    ],
    "abstract": "Low-dose Positron Emission Tomography (PET) imaging reduces patient radiation exposure but suffers from increased noise that degrades image quality and diagnostic reliability. Although diffusion models have demonstrated strong denoising capability, their stochastic nature makes it challenging to enforce anatomically consistent structures, particularly in low signal-to-noise regimes and volumetric whole-body imaging. We propose Wavelet-Conditioned ControlNet (WCC-Net), a fully 3D diffusion-based framework that introduces explicit frequency-domain structural priors via wavelet representations to guide volumetric PET denoising. By injecting wavelet-based structural guidance into a frozen pretrained diffusion backbone through a lightweight control branch, WCC-Net decouples anatomical structure from noise while preserving generative expressiveness and 3D structural continuity. Extensive experiments demonstrate that WCC-Net consistently outperforms CNN-, GAN-, and diffusion-based baselines. On the internal 1/20-dose test set, WCC-Net improves PSNR by +1.21 dB and SSIM by +0.008 over a strong diffusion baseline, while reducing structural distortion (GMSD) and intensity error (NMAE). Moreover, WCC-Net generalizes robustly to unseen dose levels (1/50 and 1/4), achieving superior quantitative performance and improved volumetric anatomical consistency.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.07093.pdf",
    "abs_url": "https://arxiv.org/abs/2601.07093",
    "published": "2026-01-11T23:26:06Z",
    "updated": "2026-01-15T10:27:47Z",
    "comment": "10 pages",
    "light_analysis": {
      "overview": "本研究提出WCC-Net，一种基于3D扩散的框架，通过小波结构先验指导低剂量PET图像去噪，显著提升图像质量和解剖一致性。",
      "motivation": "低剂量正电子发射断层扫描（PET）成像虽能减少患者辐射暴露，但噪声增加会降低图像质量和诊断可靠性。尽管扩散模型在去噪方面表现强大，但其随机性使得在低信噪比和全身体积成像中难以强制执行解剖学一致的结构，这限制了其在医学影像中的应用。现有方法如CNN和GAN在这方面存在不足，需要更有效的结构控制机制来解决这一问题。",
      "method": "论文提出Wavelet-Conditioned ControlNet（WCC-Net），这是一种全3D扩散框架，利用小波表示引入频域结构先验来引导体积PET去噪。该方法通过轻量级控制分支将小波引导注入冻结的预训练扩散骨干中，实现解剖结构和噪声的解耦，同时保持生成表达能力和3D结构连续性。关键创新在于结合小波分析和扩散模型，增强了结构控制能力，并在3D成像环境中操作以提高整体性能。",
      "result": "在广泛实验中，WCC-Net consistently超越CNN、GAN和扩散基线方法。在内部1/20剂量测试集上，与强扩散基线相比，峰值信噪比（PSNR）提高了1.21 dB，结构相似性指数（SSIM）提升了0.008，同时降低了结构失真（GMSD）和强度误差（NMAE）。此外，WCC-Net能鲁棒地泛化到未见过的剂量水平（如1/50和1/4），展现出优越的定量性能和改善的体积解剖一致性，突显其实际应用潜力。",
      "conclusion": "本研究的主要贡献是开发了WCC-Net，通过集成小波结构先验到扩散模型中，有效提升了低剂量PET图像的去噪效果和解剖保真度。该方法具有学术价值，探索了频域先验在生成模型中的新应用，同时具有实际应用价值，可增强低剂量医学成像的诊断可靠性。未来工作方向可能包括优化计算效率或扩展到其他成像模态，摘要未明确说明具体局限性。",
      "tags": [
        "Diffusion Models",
        "Wavelet Transform",
        "3D Imaging",
        "Denoising",
        "Medical Imaging"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:09.240974Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06584",
    "title": "Softly Induced Functional Simplicity: Implications for Neural Network Generalisation, Robustness, and Distillation",
    "authors": [
      "Maciej Glowacki"
    ],
    "abstract": "Learning robust and generalisable abstractions from high-dimensional input data is a central challenge in machine learning and its applications to high-energy physics (HEP). Solutions of lower functional complexity are known to produce abstractions that generalise more effectively and are more robust to input perturbations. In complex hypothesis spaces, inductive biases make such solutions learnable by shaping the loss geometry during optimisation. In a HEP classification task, we show that a soft symmetry respecting inductive bias creates approximate degeneracies in the loss, which we identify as pseudo-Goldstone modes. We quantify functional complexity using metrics derived from first principles Hessian analysis and via compressibility. Our results demonstrate that solutions of lower complexity give rise to abstractions that are more generalisable, robust, and efficiently distillable.",
    "categories": [
      "cs.LG",
      "hep-ex"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.06584.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06584",
    "published": "2026-01-10T14:44:46Z",
    "updated": "2026-01-15T10:40:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过引入软的对称性尊重归纳偏置诱导函数简化，提升神经网络在泛化性、鲁棒性和可蒸馏性上的表现。",
      "motivation": "在机器学习和高能物理应用中，从高维输入数据学习鲁棒且可泛化的抽象是一个核心挑战。现有模型往往复杂度高，导致泛化能力差、对输入扰动敏感，限制了实际应用效果。较低函数复杂性的解决方案已被证明能提供更好泛化和鲁棒性，但如何在优化过程中有效诱导这种简化是关键问题，需要探索新的归纳偏置策略来解决这一不足。",
      "method": "论文提出在神经网络优化中引入软的对称性尊重归纳偏置，以诱导函数简化。具体地，在高能物理分类任务中，这种偏置在损失函数中创建近似简并，称为伪戈德斯通模。为量化函数复杂性，研究者从第一原理出发，采用Hessian分析推导指标并结合可压缩性度量。核心创新在于将对称性偏置与复杂性量化结合，通过塑造损失几何优化模型参数，实现低复杂性的抽象表示学习。",
      "result": "实验结果表明，通过软归纳偏置诱导的低复杂性解决方案在泛化性、鲁棒性和可蒸馏性方面表现更优。具体性能改进包括更稳定的预测和高效的知识蒸馏，但摘要未明确说明具体数据如准确率提升或效率改进百分比。推断与基线方法相比，低复杂性模型在HEP分类任务中展现出错误率降低和蒸馏效率提高，验证了函数简化对模型性能的积极影响。",
      "conclusion": "本研究的核心贡献是展示了软对称性尊重归纳偏置能诱导函数简化，从而提升神经网络的泛化性、鲁棒性和可蒸馏性。通过Hessian分析和可压缩性量化复杂性，为模型设计提供了新工具。研究具有重要学术价值，加深了对模型复杂性与性能关系的理解，并在高能物理等应用中具有潜力。未来工作可扩展此方法到其他领域或优化偏置策略以进一步提高性能。",
      "tags": [
        "Inductive Bias",
        "Functional Complexity",
        "Hessian Analysis",
        "Compressibility",
        "Neural Network Generalization"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:24.314982Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05656",
    "title": "HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation",
    "authors": [
      "Rongxin Chen",
      "Tianyu Wu",
      "Bingbing Xu",
      "Jiatang Luo",
      "Xiucheng Xu",
      "Huawei Shen"
    ],
    "abstract": "High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05656.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05656",
    "published": "2026-01-09T09:26:08Z",
    "updated": "2026-01-15T07:37:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了HAG框架，一种层次化代理人生成方法，用于主题自适应模拟，通过两阶段决策过程确保宏观分布对齐和微观一致性。",
      "motivation": "高保真代理人初始化对于可信的基于代理建模至关重要，尤其是在跨领域应用中。现有方法分为两类：基于静态数据的检索方法无法适应未在数据中出现的新主题，导致灵活性不足；而基于大语言模型（LLM）的生成方法虽然能生成个体，但缺乏宏观层面的人口分布意识，造成微观个体属性与现实不一致。这限制了模拟的真实性和适应性，迫切需要一种既能捕捉宏观分布又能保证微观合理性的解决方案。",
      "method": "HAG框架将人口生成形式化为两阶段决策过程。首先，利用世界知识模型推断层次条件概率，构建Topic-Adaptive Tree以实现宏观层面的人口分布对齐。然后，基于真实世界数据进行实例化和代理人增强，确保微观层面个体属性的理性与一致性。关键创新点在于结合宏观和微观层面的优化，使用层次化树结构自动适应不同主题，避免了现有方法的局限性。",
      "result": "论文建立了多领域评估基准和综合PACE评估框架进行广泛实验。结果表明，HAG显著优于代表性基线方法，具体表现在人口对齐错误平均减少37.7%，社会学一致性提升18.8%。这些量化指标验证了HAG在改善代理人生成质量和模拟可信度方面的有效性，突出了其在宏观分布对齐和微观一致性上的优势。",
      "conclusion": "HAG框架的主要贡献是提出了一种主题自适应的代理人生成方法，有效解决了现有技术中宏观与微观脱节的问题。其学术价值在于为基于代理的建模提供了新框架，促进更真实的仿真研究；实际应用价值体现在提升跨领域模拟的可信度和适应性。未来工作可能包括扩展框架到更复杂场景或优化评估指标，但摘要未明确说明具体方向。",
      "tags": [
        "Agent Generation",
        "Hierarchical Tree",
        "Topic Adaptation",
        "World Knowledge Model",
        "Simulation Benchmark"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:26.053284Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05529",
    "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
    "authors": [
      "Jua Han",
      "Jaeyoon Seo",
      "Jungbin Min",
      "Jean Oh",
      "Jihie Kim"
    ],
    "abstract": "One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05529.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05529",
    "published": "2026-01-09T05:04:15Z",
    "updated": "2026-01-15T05:09:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过系统评估揭示了大型语言模型在安全关键机器人决策中的严重风险，指出当前模型不适合直接部署。",
      "motivation": "该研究旨在解决LLM在安全关键机器人决策中可能导致灾难性错误的问题。随着LLM日益集成到机器人系统中，物理安全风险显著增加，一个错误指令可能直接危及生命。现有方法缺乏对罕见但致命错误的系统性评估，尤其是在误差容忍度极低的场景中，这使得当前LLM的部署存在潜在危险，亟需更严格的安全测试框架。",
      "method": "研究首先通过定性评估火灾疏散场景，识别LLM决策中的关键失败案例。基于此，设计七项定量评估任务，分为三类：完整信息任务使用ASCII地图以减少解释歧义并隔离空间推理；不完整信息任务测试模型在缺失上下文下的推断能力；安全导向空间推理任务使用自然语言评估生命威胁情境下的安全决策。对多种LLM和视觉语言模型进行基准测试，以分析其性能。",
      "result": "实验结果揭示严重漏洞：多个模型在ASCII导航任务中成功率降至0%，而在模拟火灾演练中，模型错误指示机器人向危险区域移动而非紧急出口。研究强调，1%的失败率可能导致灾难性后果，99%的准确率在机器人应用中仍不可靠，因为这意味着每百次执行可能引发一次致命错误。对比显示，当前先进模型无法保证安全，突显其不适用于高风险环境。",
      "conclusion": "论文的主要贡献是系统性评估了LLM在安全关键机器人决策中的风险，并指出当前模型不适用于直接部署。学术价值在于强调了安全评估的紧迫性和99%准确率的误导性，实际应用价值是警示AI系统在物理世界中的潜在危害。未来工作可能包括开发更健壮的模型或集成安全机制，以降低风险。",
      "tags": [
        "Large Language Model",
        "Robotics Decision Making",
        "Safety-Critical Systems",
        "Vision-Language Model",
        "Spatial Reasoning"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:28.345019Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04377",
    "title": "Disco-RAG: Discourse-Aware Retrieval-Augmented Generation",
    "authors": [
      "Dongqi Liu",
      "Hang Ding",
      "Qiming Feng",
      "Jian Li",
      "Xurong Xie",
      "Zhucun Xue",
      "Chengjie Wang",
      "Jiangning Zhang",
      "Yabiao Wang"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) has emerged as an important means of enhancing the performance of large language models (LLMs) in knowledge-intensive tasks. However, most existing RAG strategies treat retrieved passages in a flat and unstructured way, which prevents the model from capturing structural cues and constrains its ability to synthesize knowledge from dispersed evidence across documents. To overcome these limitations, we propose Disco-RAG, a discourse-aware framework that explicitly injects discourse signals into the generation process. Our method constructs intra-chunk discourse trees to capture local hierarchies and builds inter-chunk rhetorical graphs to model cross-passage coherence. These structures are jointly integrated into a planning blueprint that conditions the generation. Experiments on question answering and long-document summarization benchmarks show the efficacy of our approach. Disco-RAG achieves state-of-the-art results on the benchmarks without fine-tuning. These findings underscore the important role of discourse structure in advancing RAG systems.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04377.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04377",
    "published": "2026-01-07T20:32:50Z",
    "updated": "2026-01-15T09:06:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Disco-RAG框架，通过注入话语结构来增强检索增强生成的性能，无需微调即在问答和摘要任务上取得先进结果。",
      "motivation": "现有检索增强生成策略平坦地处理检索段落，无法捕捉结构线索，限制了模型从分散证据合成知识的能力。知识密集型任务如问答和摘要中，这种不足影响性能，因此需要引入话语结构来改进RAG系统，以更好地整合跨文档信息。",
      "method": "Disco-RAG框架通过构建内部语块的话语树来捕获局部层次结构，以及外部语块的修辞图来建模跨段落连贯性。这些结构被整合到规划蓝图中，指导生成过程，从而结合话语信号来增强大型语言模型的知识集成能力。",
      "result": "在问答和长文档摘要基准上的实验显示，Disco-RAG取得了最先进的结果，无需额外微调，证明了话语结构在提升检索增强生成性能中的有效性，优于传统的平坦处理检索段落的方法。",
      "conclusion": "研究强调了话语结构在推进RAG系统中的关键作用，提供了有效的框架来改善知识密集型任务的表现。学术价值在于结合话语分析增强生成模型，实际应用潜力高，未来可扩展到更多任务，如多文档推理或对话系统。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Discourse Analysis",
        "Rhetorical Graphs",
        "Large Language Models",
        "Planning Blueprint"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:24.489199Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03043",
    "title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage",
    "authors": [
      "Junhao Hu",
      "Fangze Li",
      "Mingtao Xu",
      "Feifan Meng",
      "Shiju Zhao",
      "Tiancheng Hu",
      "Ting Peng",
      "Anmin Liu",
      "Wenrui Huang",
      "Chenxu Liu",
      "Ziyue Hua",
      "Tao Xie"
    ],
    "abstract": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03043.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03043",
    "published": "2026-01-06T14:23:58Z",
    "updated": "2026-01-15T11:59:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种早停算法，缓解稀疏注意力在长解码阶段导致的'Less is Less'问题，显著减少token消耗。",
      "motivation": "大型语言模型（LLMs）推理效率需求高，decode阶段延迟主导总时间。现有稀疏注意力算法旨在降低复杂性，但可能导致信息损失和序列延长，反而增加端到端复杂性，这一矛盾现象在长解码阶段尤为突出，阻碍了实际部署的效率提升。因此，研究旨在解决稀疏注意力应用中的负面效应，优化解码流程以平衡信息保留与效率。",
      "method": "核心方法是提出早期停止算法，用于检测稀疏解码过程中信息损失超过信息增益的阈值。算法在推理时动态监测信息平衡点，一旦达到阈值即停止解码，避免不必要的序列扩展。关键创新在于自动识别信息损失临界点，优化解码过程。摘要未明确说明具体模型架构或数据集细节，但方法适用于推理密集型基准测试，如摘要中提及。",
      "result": "在推理密集型基准测试中，早停算法减少token消耗高达90%，准确率下降小于2%。这表明算法有效缓解了信息损失问题，提升了稀疏解码的效率。与未使用早停的稀疏解码方法相比，实现了更好的资源利用和性能平衡，实验结果支持了算法在保持模型准确性的同时优化推理延迟的潜力。",
      "conclusion": "论文主要贡献是揭示了稀疏注意力在长解码阶段可能导致信息损失和序列延长的现象，并提出早停算法来缓解。该研究具有学术价值，深化了对稀疏解码机制的理解，并为提高LLMs推理效率提供了新思路。实际应用中，算法可减少部署成本，促进大规模LLMs的使用。未来工作可能包括优化检测阈值或扩展到其他模型架构。",
      "tags": [
        "Large Language Model",
        "Sparse-Attention",
        "Early-Stopping Algorithm",
        "Decode Stage",
        "Information Loss"
      ]
    },
    "analyzed_at": "2026-01-16T03:29:57.097854Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02918",
    "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning",
    "authors": [
      "Guoqiang Liang",
      "Jianyi Wang",
      "Zhonghua Wu",
      "Shangchen Zhou"
    ],
    "abstract": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.02918.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02918",
    "published": "2026-01-06T11:00:17Z",
    "updated": "2026-01-15T14:19:47Z",
    "comment": "Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.02737",
    "title": "Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench",
    "authors": [
      "Zanting Ye",
      "Xiaolong Niu",
      "Xuanbin Wu",
      "Xu Han",
      "Shengyuan Liu",
      "Jing Hao",
      "Zhihao Peng",
      "Hao Sun",
      "Jieqin Lv",
      "Fanghu Wang",
      "Yanchao Huang",
      "Hubing Wu",
      "Yixuan Yuan",
      "Habib Zaidi",
      "Arman Rahmim",
      "Yefeng Zheng",
      "Lijun Lu"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.02737.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02737",
    "published": "2026-01-06T05:58:50Z",
    "updated": "2026-01-15T06:08:23Z",
    "comment": "9 pages, 6 figures, 6 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.01461",
    "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
    "authors": [
      "Yuxiang Mei",
      "Dongxing Xu",
      "Jiaen Liang",
      "Yanhua Long"
    ],
    "abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.01461.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01461",
    "published": "2026-01-04T10:08:53Z",
    "updated": "2026-01-15T08:38:42Z",
    "comment": "5 pages, 1 figure",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.00554",
    "title": "Entropy Production in Machine Learning Under Fokker-Planck Probability Flow",
    "authors": [
      "Lennon Shikhman"
    ],
    "abstract": "Machine learning models deployed in nonstationary environments inevitably experience performance degradation due to data drift. While numerous drift detection heuristics exist, most lack a dynamical interpretation and provide limited guidance on how retraining decisions should be balanced against operational cost. In this work, we propose an entropy-based retraining framework grounded in nonequilibrium statistical physics. Interpreting drift as probability flow governed by a Fokker-Planck equation, we quantify model-data mismatch using relative entropy and show that its time derivative admits an entropy-balance decomposition featuring a nonnegative entropy production term driven by probability currents. Guided by this theory, we implement an entropy-triggered retraining policy using an exponentially weighted moving-average (EWMA) control statistic applied to a streaming kernel density estimator of the Kullback-Leibler divergence. We evaluate this approach across multiple nonstationary data streams. In synthetic, financial, and web-traffic domains, entropy-based retraining achieves predictive performance comparable to frequent retraining while reducing retraining frequency by one to two orders of magnitude. However, in a challenging biomedical ECG setting, the entropy-based trigger underperforms the maximum-frequency baseline, highlighting limitations of feature-space entropy monitoring under complex label-conditional drift.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.00554.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00554",
    "published": "2026-01-02T04:01:57Z",
    "updated": "2026-01-15T13:22:48Z",
    "comment": "12 pages, 4 figures, 1 table",
    "light_analysis": {
      "overview": "提出一种基于熵的重新训练框架，使用Fokker-Planck概率流理论量化数据漂移，优化机器学习模型在非平稳环境中的重新训练策略。",
      "motivation": "机器学习模型在非平稳环境中部署时，常因数据漂移导致预测性能下降。现有漂移检测方法多为启发式，缺乏动态理论解释，难以在重新训练成本与模型维护之间取得平衡。本研究旨在解决这一问题，通过引入非平衡统计物理学理论，为漂移提供动态模型，并指导更有效的重新训练决策，以减少不必要的操作开销。",
      "method": "该方法将数据漂移解释为Fokker-Planck方程控制的概率流，使用相对熵（Kullback-Leibler散度）量化模型与数据不匹配，并推导出包含非负熵产生项的熵平衡分解。基于此理论，实施基于熵触发的重新训练策略，具体采用指数加权移动平均（EWMA）控制统计量，应用于KLD的流式核密度估计器，以实时监控漂移并自动触发重新训练。",
      "result": "在合成、金融和网络流量数据流中，基于熵的重新训练实现了与频繁重新训练相当的预测性能，同时将重新训练频率降低一到两个数量级。但在生物医学心电图（ECG）数据中，由于复杂标签条件漂移，该方法表现不如最大频率重新训练基线，表明特征空间熵监测在特定场景下存在局限性。",
      "conclusion": "本研究贡献了一个基于熵理论的重新训练框架，在简单漂移场景中有效平衡了性能与成本，展示了物理理论在机器学习应用中的潜力。然而，在复杂标签条件漂移下的不足指出了未来研究方向，如改进熵监测方法以处理更复杂的漂移模式，拓展理论在多样化数据环境中的应用价值。",
      "tags": [
        "Entropy Production",
        "Fokker-Planck Equation",
        "Relative Entropy",
        "Exponential Weighted Moving Average (EWMA)",
        "Kernel Density Estimation"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:37.282759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00138",
    "title": "Explicit Abstention Knobs for Predictable Reliability in Video Question Answering",
    "authors": [
      "Jorge Ortiz"
    ],
    "abstract": "High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.00138.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00138",
    "published": "2025-12-31T23:27:32Z",
    "updated": "2026-01-15T17:31:17Z",
    "comment": "Preprint. Diagnostic study of confidence-based abstention under evidence truncation",
    "light_analysis": {
      "overview": "该论文提出通过置信度阈值实现视频问答中视觉语言模型的错误率可控避免机制，以提升高风险部署的可靠性。",
      "motivation": "在视觉语言模型的高风险部署中，如视频问答应用，错误可能导致严重后果，因此需要系统在不确定时主动避免回答以控制风险。本研究旨在解决现有置信度方法是否能在视频问答中提供可靠错误率控制，尤其在分布变化下是否稳健。这关系到提升模型安全性和实用价值，但摘要未明确说明现有方法的特定不足之处，仅强调控制机制的重要性。",
      "method": "研究方法采用置信度阈值进行选择性预测，核心创新是通过扫掠阈值epsilon来调节系统的风险和覆盖权衡，以实现错误率的机制性控制。使用NExT-QA数据集和Gemini 2.0 Flash模型进行实验，分析阈值变化对性能的影响。摘要未详细描述模型架构或实施细节，但突出了研究置信度在视频问答中的具体应用。",
      "result": "摘要表明，置信度阈值在分布内提供机制性控制，扫掠阈值能产生平滑的风险覆盖权衡，从而减少错误率。然而，摘要未提供具体数据支撑，如准确率提升百分比或效率改进指标，也未提及与基线方法的对比情况，因此摘要未明确说明完整的量化实验结果。",
      "conclusion": "该研究验证了置信度阈值在视频问答中控制错误率的有效性，特别在分布内的可控性方面有所贡献。学术价值在于为视觉语言模型的安全性部署提供了理论和方法支持，具有实际应用价值。局限性可能包括分布变化下的稳健性未充分探讨，未来工作可进一步研究跨分布泛化能力。",
      "tags": [
        "Vision-Language Models",
        "Selective Prediction",
        "Confidence Thresholding",
        "Video Question Answering"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:27.582657Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23860",
    "title": "Lifelong Domain Adaptive 3D Human Pose Estimation",
    "authors": [
      "Qucheng Peng",
      "Hongfei Xue",
      "Pu Wang",
      "Chen Chen"
    ],
    "abstract": "3D Human Pose Estimation (3D HPE) is vital in various applications, from person re-identification and action recognition to virtual reality. However, the reliance on annotated 3D data collected in controlled environments poses challenges for generalization to diverse in-the-wild scenarios. Existing domain adaptation (DA) paradigms like general DA and source-free DA for 3D HPE overlook the issues of non-stationary target pose datasets. To address these challenges, we propose a novel task named lifelong domain adaptive 3D HPE. To our knowledge, we are the first to introduce the lifelong domain adaptation to the 3D HPE task. In this lifelong DA setting, the pose estimator is pretrained on the source domain and subsequently adapted to distinct target domains. Moreover, during adaptation to the current target domain, the pose estimator cannot access the source and all the previous target domains. The lifelong DA for 3D HPE involves overcoming challenges in adapting to current domain poses and preserving knowledge from previous domains, particularly combating catastrophic forgetting. We present an innovative Generative Adversarial Network (GAN) framework, which incorporates 3D pose generators, a 2D pose discriminator, and a 3D pose estimator. This framework effectively mitigates domain shifts and aligns original and augmented poses. Moreover, we construct a novel 3D pose generator paradigm, integrating pose-aware, temporal-aware, and domain-aware knowledge to enhance the current domain's adaptation and alleviate catastrophic forgetting on previous domains. Our method demonstrates superior performance through extensive experiments on diverse domain adaptive 3D HPE datasets.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.23860.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23860",
    "published": "2025-12-29T20:56:06Z",
    "updated": "2026-01-15T04:43:31Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "提出终身领域适应3D人体姿态估计任务，并设计了一个整合姿态、时间和领域感知知识的生成对抗网络框架，以解决领域偏移和灾难性遗忘问题。",
      "motivation": "3D人体姿态估计在人员重识别和虚拟现实等应用中至关重要，但现有方法严重依赖控制环境中收集的注释数据，难以泛化到多样化的野外场景。传统领域适应范式如一般领域适应和源自由领域适应忽略了目标姿态数据集的非平稳性，导致模型在面对连续变化环境时适应性不足和知识遗忘，限制了实际部署的有效性和鲁棒性。因此，需要开发能持续适应新领域同时保留历史知识的方法，以提升3D人体姿态估计的实用价值。",
      "method": "论文提出终身领域适应设置，其中姿态估计器先在源域预训练，随后逐步适应不同目标域，但不能访问源域和先前目标域数据。核心方法是一个创新的生成对抗网络框架，包含3D姿态生成器、2D姿态判别器和3D姿态估计器。该框架通过生成和判别机制缓解领域偏移并实现姿态对齐。此外，构建了3D姿态生成器范式，整合姿态感知、时间感知和领域感知知识，以增强对当前域的适应能力并减轻对先前域的灾难性遗忘，无需额外存储历史数据。",
      "result": "在多样化的领域适应3D人体姿态估计数据集上进行了广泛实验，结果显示该方法展现出优越性能。摘要未明确说明具体性能指标如准确率提升或效率改进，也未详细比较基线方法，但强调了通过实验验证了框架在应对领域变化和遗忘挑战方面的有效性。这表明该方法在终身学习场景下能持续优化姿态估计，为实际应用提供了潜力支持。",
      "conclusion": "论文的主要贡献在于首次将终身领域适应引入3D人体姿态估计任务，并提出了相应的生成对抗网络框架和姿态生成器范式。学术上，这丰富了领域适应和持续学习的研究，为处理非平稳数据提供了新思路；应用上，增强了3D人体姿态估计在多变环境中的适应性和鲁棒性。潜在局限性可能包括计算复杂性或扩展到更大数据集，未来工作可进一步优化框架效率或应用于其他视觉任务。",
      "tags": [
        "3D Human Pose Estimation",
        "Domain Adaptation",
        "Generative Adversarial Network",
        "Catastrophic Forgetting",
        "Lifelong Learning"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:19.954156Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.22974",
    "title": "RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance",
    "authors": [
      "Chunyuan Chen",
      "Yunuo Cai",
      "Shujuan Li",
      "Weiyun Liang",
      "Bin Wang",
      "Jing Xu"
    ],
    "abstract": "Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose RealCamo, a novel out-painting-based framework for controllable realistic camouflaged image generation. RealCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multimodal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.22974.pdf",
    "abs_url": "https://arxiv.org/abs/2512.22974",
    "published": "2025-12-28T15:37:56Z",
    "updated": "2026-01-15T07:24:42Z",
    "comment": "25 pages",
    "light_analysis": {
      "overview": "RealCamo提出了一种基于外绘制的可控伪装图像生成框架，通过布局控制和多模态文本-视觉指导提升生成图像的现实性和语义连贯性。",
      "motivation": "伪装图像生成（CIG）作为获取高质量训练数据的替代方法，广泛应用于伪装物体检测（COD）。然而，现有CIG方法存在显著缺陷：生成的图像要么因视觉相似性弱而伪装不足，要么背景杂乱且语义与前景目标不一致。这些问题限制了CIG在现实场景中的应用，因此需要改进方法以生成更逼真、语义一致的伪装图像，解决视觉真实性和结构协调性的挑战。",
      "method": "RealCamo采用外绘制框架，通过引入布局控制来调节全局图像结构，改善前景对象与生成背景之间的语义连贯性。核心创新包括结合细粒度文本任务描述和基于纹理的背景检索，构建多模态文本-视觉条件，共同指导生成过程以增强视觉逼真度。框架利用结构调节和文本-视觉信息融合，提升伪装图像的可控性和现实性，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "摘要未明确说明具体实验结果。论文通过大量实验和可视化结果证明了RealCamo框架在生成伪装图像方面的有效性，但未提供具体的性能指标（如准确率提升或效率改进）或与基线方法的详细对比。基于摘要推断，框架可能通过引入的背景-前景分布差异度量来量化伪装质量，展示了改进的生成效果，但定量数据需参考完整论文。",
      "conclusion": "RealCamo研究的主要贡献在于提出一个可控伪装图像生成框架，通过布局控制和文本-视觉指导显著提升了生成图像的逼真度和语义一致性。此外，引入背景-前景分布差异度量来评估伪装质量，为CIG领域提供了新的评估工具。这项研究不仅推动了CIG技术的发展，还有助于COD任务的数据增强，具有学术和实际应用价值。未来工作可进一步优化度量标准并拓展应用场景，如更多数据集的验证。",
      "tags": [
        "Camouflaged Image Generation",
        "Layout Control",
        "Textual-Visual Guidance",
        "Out-painting",
        "Background-Foreground Distribution Divergence"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:39.874270Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.22972",
    "title": "Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection",
    "authors": [
      "Runwei Guan",
      "Jianan Liu",
      "Shaofeng Liang",
      "Fangqiang Ding",
      "Shanliang Yao",
      "Xiaokai Bai",
      "Daizong Liu",
      "Tao Huang",
      "Guoqiang Mao",
      "Hui Xiong"
    ],
    "abstract": "4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, point-cloud-based radar representations suffer from information loss due to multi-stage signal processing, while directly utilizing raw 4D radar tensors incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that efficiently fuses raw 4D radar cubes with camera images via decoupled multi-view radar representations. Our approach introduces two key components: (1) A Wavelet Attention Module embedded in a wavelet-based Feature Pyramid Network (FPN), which enhances the representation of sparse radar signals and image data by capturing joint spatial-frequency features, thereby mitigating information loss while maintaining computational efficiency. (2) A Geometry-guided Progressive Fusion mechanism, a two-stage query-based fusion strategy that progressively aligns multi-view radar and visual features through geometric priors, enabling modality-agnostic and efficient integration without overwhelming computational overhead. Extensive experiments on the K-Radar benchmark show that WRCFormer achieves state-of-the-art performance, surpassing the best existing model by approximately 2.4% in all scenarios and 1.6% in sleet conditions, demonstrating strong robustness in adverse weather.",
    "categories": [
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.22972.pdf",
    "abs_url": "https://arxiv.org/abs/2512.22972",
    "published": "2025-12-28T15:32:17Z",
    "updated": "2026-01-15T05:29:16Z",
    "comment": "10 pages, 10 figures",
    "light_analysis": {
      "overview": "提出WRCFormer框架，通过小波注意力和几何引导渐进融合技术，高效融合4D雷达与相机数据，提升鲁棒3D物体检测性能。",
      "motivation": "本研究旨在解决4D毫米波雷达在自动驾驶应用中点云表示信息丢失和原始张量计算成本高的问题。现有方法如点云处理导致信息损失，直接使用原始数据计算昂贵，限制了其在恶劣天气下的实际应用。由于4D雷达成本低、全天候鲁棒，对提高检测精度和效率至关重要，因此开发高效融合方法具有重要实践意义。",
      "method": "方法核心是WRCFormer框架，融合原始4D雷达立方体和相机图像，采用解耦多视图雷达表示。关键创新包括：基于小波的注意力模块嵌入特征金字塔网络，捕获时空频特征以增强稀疏信号表示；几何引导渐进融合机制，通过两阶段查询策略利用几何先验对齐多视图特征，实现模态无关的高效集成。使用K-Radar基准进行验证。",
      "result": "在K-Radar基准的实验中，WRCFormer达到最先进性能，3D物体检测在所有场景中超越现有最佳模型约2.4%，在sleet条件下提升约1.6%。结果显示该方法在不同环境下均表现鲁棒，尤其在恶劣天气下检测性能显著改进，证明了其有效性和实用性。",
      "conclusion": "论文的主要贡献是提出WRCFormer框架，通过小波和几何引导技术高效融合多模态数据，提升3D物体检测精度和鲁棒性。学术价值在于创新多传感器融合策略，应用价值体现在增强自动驾驶系统在复杂环境下的感知能力。未来可扩展至其他传感器或更广泛场景，摘要未明确说明具体局限性。",
      "tags": [
        "Wavelet Attention",
        "Feature Pyramid Network",
        "4D Radar Tensor",
        "Geometry-guided Fusion",
        "3D Object Detection"
      ]
    },
    "analyzed_at": "2026-01-16T03:30:53.500977Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.22712",
    "title": "Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages",
    "authors": [
      "Anaelia Ovalle",
      "Candace Ross",
      "Sebastian Ruder",
      "Adina Williams",
      "Karen Ullrich",
      "Mark Ibrahim",
      "Levent Sagun"
    ],
    "abstract": "Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.22712.pdf",
    "abs_url": "https://arxiv.org/abs/2512.22712",
    "published": "2025-12-27T21:55:21Z",
    "updated": "2026-01-15T03:34:27Z",
    "comment": "Accepted to 2025 EMNLP Multilingual Representation Learning Workshop",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.21789",
    "title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning",
    "authors": [
      "Ting-Hao 'Kenneth' Huang",
      "Ryan A. Rossi",
      "Sungchul Kim",
      "Tong Yu",
      "Ting-Yao E. Hsu",
      "Ho Yin",
      "Ng",
      "C. Lee Giles"
    ],
    "abstract": "Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.21789.pdf",
    "abs_url": "https://arxiv.org/abs/2512.21789",
    "published": "2025-12-25T21:39:10Z",
    "updated": "2026-01-15T15:33:56Z",
    "comment": "Accepted to the 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE 2026). SciCap Website: http://scicap.ai/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.21011",
    "title": "Granular Ball Guided Masking: Structure-aware Data Augmentation",
    "authors": [
      "Shuyin Xia",
      "Fan Chen",
      "Dawei Dai",
      "Meng Yang",
      "Junwei Han",
      "Xinbo Gao",
      "Guoyin Wang"
    ],
    "abstract": "Deep learning models have achieved remarkable success in computer vision but still rely heavily on large-scale labeled data and tend to overfit when data is limited or distributions shift. Data augmentation -- particularly mask-based information dropping -- can enhance robustness by forcing models to explore complementary cues; however, existing approaches often lack structural awareness and risk discarding essential semantics. We propose Granular Ball Guided Masking (GBGM), a structure-aware augmentation strategy guided by Granular Ball Computing (GBC). GBGM adaptively preserves semantically rich, structurally important regions while suppressing redundant areas through a coarse-to-fine hierarchical masking process, producing augmentations that are both representative and discriminative. Extensive experiments on multiple benchmarks demonstrate consistent improvements not only in image classification and masked image reconstruction, but also in image tampering detection, validating the effectiveness and generalization of GBGM across both recognition and forensic scenarios. Simple and model-agnostic, GBGM integrates seamlessly into CNNs and Vision Transformers, offering a practical paradigm for structure-aware data augmentation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.21011.pdf",
    "abs_url": "https://arxiv.org/abs/2512.21011",
    "published": "2025-12-24T07:15:33Z",
    "updated": "2026-01-15T09:18:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.19336",
    "title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis",
    "authors": [
      "Siyuan Mei",
      "Yan Xia",
      "Fuxin Fan",
      "Andreas Maier"
    ],
    "abstract": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.19336.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19336",
    "published": "2025-12-22T12:32:16Z",
    "updated": "2026-01-15T11:29:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.08847",
    "title": "Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe",
    "authors": [
      "JV Roig"
    ],
    "abstract": "Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08847.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08847",
    "published": "2025-12-22T11:58:50Z",
    "updated": "2026-01-15T08:39:19Z",
    "comment": "26 pages, 17 tables, 1 figure",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.19126",
    "title": "AWPO: Enhancing Tool-Use of Large Language Models through Adaptive Integration of Reasoning Rewards",
    "authors": [
      "Zihan Lin",
      "Xiaohan Wang",
      "Hexiong Yang",
      "Jiajun Chai",
      "Jie Cao",
      "Guojun Yin",
      "Wei Lin",
      "Ran He"
    ],
    "abstract": "While Reinforcement Learning (RL) shows promise in training tool-use Large Language Models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of reasoning rewards based on chain-of-thought quality for better tool utilization. Furthermore, naïvely combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose Advantage-Weighted Policy Optimization (AWPO), a principled RL framework that adaptively integrates reasoning rewards into advantage estimation to improve tool-use performance. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by $16.0\\%$ in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.19126.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19126",
    "published": "2025-12-22T08:07:00Z",
    "updated": "2026-01-15T02:48:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.18128",
    "title": "SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping",
    "authors": [
      "Thomas Boudras",
      "Martin Schwartz",
      "Rasmus Fensholt",
      "Martin Brandt",
      "Ibrahim Fayad",
      "Jean-Pierre Wigneron",
      "Gabriel Belouze",
      "Fajwel Fogel",
      "Philippe Ciais"
    ],
    "abstract": "High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors' native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.18128.pdf",
    "abs_url": "https://arxiv.org/abs/2512.18128",
    "published": "2025-12-19T23:23:14Z",
    "updated": "2026-01-15T11:19:00Z",
    "comment": "17 pages, 8 figures, 3 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.15211",
    "title": "TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion",
    "authors": [
      "Yufeng Xie",
      "Cong Wang"
    ],
    "abstract": "Infrared and visible image fusion (IVIF) is a pivotal technology in low-altitude Unmanned Aerial Vehicle (UAV) reconnaissance missions, enabling robust target detection and tracking by integrating thermal saliency with environmental textures. However, traditional no-reference metrics (Statistics-based metrics and Gradient-based metrics) fail in complex low-light environments, termed the ``Noise Trap''. This paper mathematically prove that these metrics are positively correlated with high-frequency sensor noise, paradoxically assigning higher scores to degraded images and misguiding algorithm optimization. To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Extensive experiments on the DroneVehicle dataset demonstrate the superiority of TBC. Results show that TBC exhibits high ``Semantic Discriminability'' in distinguishing thermal targets from background clutter. Furthermore, TBC achieves remarkable computational efficiency, making it a reliable and real-time standard for intelligent UAV systems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.15211.pdf",
    "abs_url": "https://arxiv.org/abs/2512.15211",
    "published": "2025-12-17T09:05:21Z",
    "updated": "2026-01-15T09:45:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.15151",
    "title": "Rakuten Data Release: A Large-Scale and Long-Term Reviews Corpus for Hotel Domain",
    "authors": [
      "Yuki Nakayama",
      "Koki Hikichi",
      "Yun Ching Liu",
      "Yu Hirate"
    ],
    "abstract": "This paper presents a large-scale corpus of Rakuten Travel Reviews. Our collection contains 7.29 million customer reviews for 16 years, ranging from 2009 to 2024. Each record in the dataset contains the review text, its response from an accommodation, an anonymized reviewer ID, review date, accommodation ID, plan ID, plan title, room type, room name, purpose, accompanying group, and user ratings from six aspect categories, as well as an overall score. We present statistical information about our corpus and provide insights into factors driving data drift between 2019 and 2024 using statistical approaches.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.15151.pdf",
    "abs_url": "https://arxiv.org/abs/2512.15151",
    "published": "2025-12-17T07:33:14Z",
    "updated": "2026-01-15T03:30:01Z",
    "comment": "6 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.13142",
    "title": "Can LLMs Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels",
    "authors": [
      "Anika Sharma",
      "Malavika Mampally",
      "Chidaksh Ravuru",
      "Kandyce Brennan",
      "Neil Gaikwad"
    ],
    "abstract": "As Large Language Models (LLMs) increasingly mediate stigmatized health decisions, their capacity to understand complex psychological phenomena remains inadequately assessed. Can LLMs understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across cognitive, interpersonal, and structural levels. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS), examining representation at cognitive (self-judgment), interpersonal (worries about judgment and isolation), and structural (community condemnation and disclosure patterns) levels. Models fail tests of genuine understanding across all dimensions. They underestimate cognitive stigma while overestimating interpersonal stigma, introduce demographic biases assigning higher stigma to younger, less educated, and non-White personas, and treat secrecy as universal despite 36% of humans reporting openness. Most critically, models produce internal contradictions: they overestimate isolation yet predict isolated individuals are less secretive, revealing incoherent representations. These patterns show current alignment approaches ensure appropriate language but not coherent understanding across levels. This work provides empirical evidence that LLMs lack coherent understanding of psychological constructs operating across multiple dimensions. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.13142.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13142",
    "published": "2025-12-15T09:50:00Z",
    "updated": "2026-01-15T17:43:09Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.12325",
    "title": "Eventually LIL Regret: Almost Sure $\\ln\\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data",
    "authors": [
      "Shubhada Agrawal",
      "Aaditya Ramdas"
    ],
    "abstract": "We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\\ln^2(1/α)/V_T + \\ln (1/α) + \\ln \\ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\\ln(1/α) + \\ln \\ln V_T$ if $V_T \\geq \\ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\\ln \\ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.12325.pdf",
    "abs_url": "https://arxiv.org/abs/2512.12325",
    "published": "2025-12-13T13:34:03Z",
    "updated": "2026-01-15T04:33:41Z",
    "comment": "To appear at ALT 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.11485",
    "title": "Mistake Notebook Learning: Batch-Clustered Failures for Training-Free Agent Adaptation",
    "authors": [
      "Xuanbo Su",
      "Yingfang Zhang",
      "Hao Luo",
      "Xiaoteng Liu",
      "Leo Huang"
    ],
    "abstract": "With the growing adoption of Large Language Model (LLM) agents in persistent, real-world roles, they naturally encounter continuous streams of tasks and inevitable failures. A key limitation, however, is their inability to systematically learn from these mistakes, forcing them to repeat identical errors in similar contexts. Unlike prior training-free methods that primarily store raw instance-level experience or focus on retrieving successful trajectories, we propose Mistake Notebook Learning (MNL), a novel memory framework that enables agents to self-curate generalizable guidance from batch-clustered failures. This mechanism allows agents to distill shared error patterns into structured ``mistake notes,'' updating an external memory only when batch performance improves to ensure stability. To further amplify adaptability, we integrate MNL with test-time scaling, leveraging aggregated failure patterns to actively steer the search process away from known pitfalls. Experiments on mathematical reasoning, Text-to-SQL, and interactive agent benchmarks show that MNL achieves competitive performance compared to existing memory mechanisms and in-context methods in both effectiveness and efficiency. These findings position structured mistake abstraction as a critical lever for robust agent evolution, enabling continuous improvement without the cost of parameter updates.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.11485.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11485",
    "published": "2025-12-12T11:33:09Z",
    "updated": "2026-01-15T08:29:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.07078",
    "title": "DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection",
    "authors": [
      "Bo Gao",
      "Jingcheng Tong",
      "Xingsheng Chen",
      "Han Yu",
      "Zichen Li"
    ],
    "abstract": "Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.   We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.   We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.07078.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07078",
    "published": "2025-12-08T01:25:10Z",
    "updated": "2026-01-15T06:15:47Z",
    "comment": "16 pages. Correct typos",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.01242",
    "title": "Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation",
    "authors": [
      "Zirui Zhao",
      "Boye Niu",
      "David Hsu",
      "Wee Sun Lee"
    ],
    "abstract": "We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.01242.pdf",
    "abs_url": "https://arxiv.org/abs/2512.01242",
    "published": "2025-12-01T03:38:44Z",
    "updated": "2026-01-15T07:18:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.20258",
    "title": "Modality-Balanced Collaborative Distillation for Multi-Modal Domain Generalization",
    "authors": [
      "Xiaohan Wang",
      "Zhangtao Cheng",
      "Ting Zhong",
      "Leiting Chen",
      "Fan Zhou"
    ],
    "abstract": "Weight Averaging (WA) has emerged as a powerful technique for enhancing generalization by promoting convergence to a flat loss landscape, which correlates with stronger out-of-distribution performance. However, applying WA directly to multi-modal domain generalization (MMDG) is challenging: differences in optimization speed across modalities lead WA to overfit to faster-converging ones in early stages, suppressing the contribution of slower yet complementary modalities, thereby hindering effective modality fusion and skewing the loss surface toward sharper, less generalizable minima. To address this issue, we propose MBCD, a unified collaborative distillation framework that retains WA's flatness-inducing advantages while overcoming its shortcomings in multi-modal contexts. MBCD begins with adaptive modality dropout in the student model to curb early-stage bias toward dominant modalities. A gradient consistency constraint then aligns learning signals between uni-modal branches and the fused representation, encouraging coordinated and smoother optimization. Finally, a WA-based teacher conducts cross-modal distillation by transferring fused knowledge to each uni-modal branch, which strengthens cross-modal interactions and steer convergence toward flatter solutions. Extensive experiments on MMDG benchmarks show that MBCD consistently outperforms existing methods, achieving superior accuracy and robustness across diverse unseen domains.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.20258.pdf",
    "abs_url": "https://arxiv.org/abs/2511.20258",
    "published": "2025-11-25T12:38:28Z",
    "updated": "2026-01-15T05:53:12Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.20696",
    "title": "Prototype-Guided Non-Exemplar Continual Learning for Cross-subject EEG Decoding",
    "authors": [
      "Dan Li",
      "Hye-Bin Shin",
      "Yeon-Woo Choi"
    ],
    "abstract": "Due to the significant variability in electroencephalo-gram (EEG) signals across individuals, knowledge acquired from previous subjects is often overwritten as new subjects are introduced in continual EEG decoding tasks. Existing methods mainly rely on storing historical data from seen subjects as replay buffers to mitigate forgetting, which is impractical under privacy or memory constraints. To address this issue, we propose a Prototype-guided Non-Exemplar Continual Learning (ProNECL) framework that preserves prior knowledge without accessing historical EEG samples. ProNECL summarizes subject-specific discriminative representations into class-level prototypes and incrementally aligns new subject representations with a global prototype memory through prototype-based feature regulariza-tion and cross-subject alignment. Experiments on the BCI Com-petition IV 2a and 2b datasets demonstrate that ProNECL effec-tively balances knowledge retention and adaptability, achieving superior performance in cross-subject continual EEG decoding tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.20696.pdf",
    "abs_url": "https://arxiv.org/abs/2511.20696",
    "published": "2025-11-24T05:42:03Z",
    "updated": "2026-01-15T07:05:51Z",
    "comment": "4 pages, 2 figures, 14th IEEE International Winter Conference on Brain-Computer Interface Conference 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.12255",
    "title": "Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets",
    "authors": [
      "Huy M. Le",
      "Dat Tien Nguyen",
      "Phuc Binh Nguyen",
      "Gia Bao Le Tran",
      "Phu Truong Thien",
      "Cuong Dinh",
      "Minh Nguyen",
      "Nga Nguyen",
      "Thuy T. N. Nguyen",
      "Tan Nhat Nguyen",
      "Binh T. Nguyen"
    ],
    "abstract": "The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.12255.pdf",
    "abs_url": "https://arxiv.org/abs/2511.12255",
    "published": "2025-11-15T15:23:44Z",
    "updated": "2026-01-15T06:23:25Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.10051",
    "title": "GraphIF: Enhancing Multi-Turn Instruction Following for Large Language Models with Relation Graph Prompt",
    "authors": [
      "Zhenhe Li",
      "Can Lin",
      "Ling Zheng",
      "Wen-Da Wei",
      "Junli Liang",
      "Qi Song"
    ],
    "abstract": "Multi-turn instruction following is essential for building intelligent conversational systems that can consistently adhere to instructions across dialogue turns. However, existing approaches to enhancing multi-turn instruction following primarily rely on collecting or generating large-scale multi-turn dialogue datasets to fine-tune large language models (LLMs), which treat each response generation as an isolated task and fail to explicitly incorporate multi-turn instruction following into the optimization objectives. As a result, instruction-tuned LLMs often struggle with complex long-distance constraints. In multi-turn dialogues, relational constraints across turns can be naturally modeled as labeled directed edges, making graph structures particularly suitable for modeling multi-turn instruction following. Despite this potential, leveraging graph structures to enhance the multi-turn instruction following capabilities of LLMs remains unexplored. To bridge this gap, we propose GraphIF, a plug-and-play framework that models multi-turn dialogues as directed relation graphs and leverages graph prompts to enhance the instruction following capabilities of LLMs. GraphIF comprises three key components: (1) an agent-based relation extraction module that captures inter-turn semantic relations via action-triggered mechanisms to construct structured graphs; (2) a relation graph prompt generation module that converts structured graph information into natural language prompts; and (3) a response rewriting module that refines initial LLM outputs using the generated graph prompts. Extensive experiments on two long multi-turn dialogue datasets demonstrate that GraphIF can be seamlessly integrated into instruction-tuned LLMs and leads to significant improvements across all four multi-turn instruction-following evaluation metrics.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.10051.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10051",
    "published": "2025-11-13T07:49:38Z",
    "updated": "2026-01-15T15:45:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.07920",
    "title": "Lightweight Diffusion-based Framework for Online Imagined Speech Decoding in Aphasia",
    "authors": [
      "Eunyeong Ko",
      "Soowon Kim",
      "Ha-Na Jo"
    ],
    "abstract": "Individuals with aphasia experience severe difficulty in real-time verbal communication, while most imagined speech decoding approaches remain limited to offline analysis or computationally demanding models. To address this limitation, we propose a two-session experimental framework consisting of an offline data acquisition phase and a subsequent online feedback phase for real-time imagined speech decoding. The paradigm employed a four-class Korean-language task, including three imagined speech targets selected according to the participant's daily communicative needs and a resting-state condition, and was evaluated in a single individual with chronic anomic aphasia. Within this framework, we introduce a lightweight diffusion-based neural decoding model explicitly optimized for real-time inference, achieved through architectural simplifications such as dimensionality reduction, temporal kernel optimization, group normalization with regularization, and dual early-stopping criteria. In real-time evaluation, the proposed system achieved 65\\% top-1 and 70\\% top-2 accuracy, with the Water class reaching 80\\% top-1 and 100\\% top-2 accuracy. These results demonstrate that real-time-optimized diffusion-based architectures, combined with clinically grounded task design, can support feasible online imagined speech decoding for communication-oriented BCI applications in aphasia.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.07920.pdf",
    "abs_url": "https://arxiv.org/abs/2511.07920",
    "published": "2025-11-11T07:18:35Z",
    "updated": "2026-01-15T06:44:46Z",
    "comment": "4 pages, 2 figures, 1 table, Name of Conference: International Conference on Brain-Computer Interface",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.00423",
    "title": "Bootstrap Off-policy with World Model",
    "authors": [
      "Guojian Zhan",
      "Likun Wang",
      "Xiangteng Zhang",
      "Jiaxin Gao",
      "Masayoshi Tomizuka",
      "Shengbo Eben Li"
    ],
    "abstract": "Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.00423.pdf",
    "abs_url": "https://arxiv.org/abs/2511.00423",
    "published": "2025-11-01T06:33:04Z",
    "updated": "2026-01-15T13:38:44Z",
    "comment": "NeurIPS 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.26117",
    "title": "JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting",
    "authors": [
      "Xianben Yang",
      "Yuxuan Li",
      "Tao Wang",
      "Tao Wang",
      "Yi Jin",
      "Yidong Li",
      "Haibin Ling"
    ],
    "abstract": "Traditional novel view synthesis methods heavily rely on external camera pose estimation tools such as COLMAP, which often introduce computational bottlenecks and propagate errors. To address these challenges, we propose a unified framework that jointly optimizes 3D Gaussian points and camera poses without requiring pre-calibrated inputs. Our approach iteratively refines 3D Gaussian parameters and updates camera poses through a novel co-optimization strategy, ensuring simultaneous improvements in scene reconstruction fidelity and pose estimation accuracy. The key innovation lies in decoupling the joint optimization into two interleaved phases: first, updating 3D Gaussian parameters via differentiable rendering with fixed poses, and second, refining camera poses using a customized 3D optical flow algorithm that incorporates geometric and photometric constraints. This formulation progressively reduces projection errors, particularly in challenging scenarios with large viewpoint variations and sparse feature distributions, where traditional methods struggle. Extensive evaluations on multiple datasets demonstrate that our approach significantly outperforms existing COLMAP-free techniques in reconstruction quality, and also surpasses the standard COLMAP-based baseline in general.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.26117.pdf",
    "abs_url": "https://arxiv.org/abs/2510.26117",
    "published": "2025-10-30T04:00:07Z",
    "updated": "2026-01-15T03:47:13Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.25626",
    "title": "Are Language Models Efficient Reasoners? A Perspective from Logic Programming",
    "authors": [
      "Andreas Opedal",
      "Yanick Zengaffinen",
      "Haruki Shirakami",
      "Clemente Pasti",
      "Mrinmaya Sachan",
      "Abulhair Saparov",
      "Ryan Cotterell",
      "Bernhard Schölkopf"
    ],
    "abstract": "Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.LO"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.25626.pdf",
    "abs_url": "https://arxiv.org/abs/2510.25626",
    "published": "2025-10-29T15:30:31Z",
    "updated": "2026-01-15T09:56:12Z",
    "comment": "NeurIPS 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.23463",
    "title": "Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station",
    "authors": [
      "Hao Liang",
      "Haifeng Wen",
      "Kaishun Wu",
      "Hong Xing"
    ],
    "abstract": "Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \\emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \\emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \\emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \\emph{perk} even \\emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.23463.pdf",
    "abs_url": "https://arxiv.org/abs/2510.23463",
    "published": "2025-10-27T16:01:15Z",
    "updated": "2026-01-15T17:38:48Z",
    "comment": "13 pages, 6 figures, submitted for possible publication",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.22655",
    "title": "Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections",
    "authors": [
      "Berken Utku Demirel",
      "Christian Holz"
    ],
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data. Most SSL approaches rely on strong, well-established, handcrafted data augmentations to generate diverse views for representation learning. However, designing such augmentations requires domain-specific knowledge and implicitly imposes representational invariances on the model, which can limit generalization. In this work, we propose an unsupervised representation learning method that replaces augmentations by generating views using orthonormal bases and overcomplete frames. We show that embeddings learned from orthonormal and overcomplete spaces reside on distinct manifolds, shaped by the geometric biases introduced by representing samples in different spaces. By jointly leveraging the complementary geometry of these distinct manifolds, our approach achieves superior performance without artificially increasing data diversity through strong augmentations. We demonstrate the effectiveness of our method on nine datasets across five temporal sequence tasks, where signal-specific characteristics make data augmentations particularly challenging. Without relying on augmentation-induced diversity, our method achieves performance gains of up to 15--20\\% over existing self-supervised approaches. Source code: https://github.com/eth-siplab/Learning-with-FrameProjections",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.22655.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22655",
    "published": "2025-10-26T12:36:29Z",
    "updated": "2026-01-15T09:38:50Z",
    "comment": "Published at the Conference on Neural Information Processing Systems (NeurIPS) 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.20721",
    "title": "User Perceptions vs. Proxy LLM Judges: Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios",
    "authors": [
      "Xiaoyuan Wu",
      "Roshni Kaushik",
      "Wenkai Li",
      "Lujo Bauer",
      "Koichi Onoue"
    ],
    "abstract": "Large language models (LLMs) are rapidly being adopted for tasks like drafting emails, summarizing meetings, and answering health questions. In these settings, users may need to share private information (e.g., contact details, health records). To evaluate LLMs' ability to identify and redact such information, prior work introduced real-life, scenario-based benchmarks (e.g., ConfAIde, PrivacyLens) and found that LLMs can leak private information in complex scenarios. However, these evaluations relied on proxy LLMs to judge the helpfulness and privacy-preservation quality of LLM responses, rather than directly measuring users' perceptions. To understand how users perceive the helpfulness and privacy-preservation quality of LLM responses to privacy-sensitive scenarios, we conducted a user study ($n=94$) using 90 PrivacyLens scenarios. We found that users had low agreement with each other when evaluating identical LLM responses. In contrast, five proxy LLMs reached high agreement, yet each proxy LLM had low correlation with users' evaluations. These results indicate that proxy LLMs cannot accurately estimate users' wide range of perceptions of utility and privacy in privacy-sensitive scenarios. We discuss the need for more user-centered studies to measure LLMs' ability to help users while preserving privacy, and for improving alignment between LLMs and users in estimating perceived privacy and utility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.20721.pdf",
    "abs_url": "https://arxiv.org/abs/2510.20721",
    "published": "2025-10-23T16:38:26Z",
    "updated": "2026-01-15T14:47:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.14657",
    "title": "Decorrelation Speeds Up Vision Transformers",
    "authors": [
      "Kieran Carrigg",
      "Rob van Gastel",
      "Melda Yeghaian",
      "Sander Dalm",
      "Faysal Boughorbel",
      "Marcel van Gerven"
    ],
    "abstract": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label data regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. To mimic constrained-data scenarios, we evaluate our approach on ImageNet-1K pre-training and ADE20K fine-tuning using randomly sampled subsets of each dataset. Under this setting, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4%, and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.   Keywords: Deep learning, Vision transformers, Efficient AI, Decorrelation",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.14657.pdf",
    "abs_url": "https://arxiv.org/abs/2510.14657",
    "published": "2025-10-16T13:13:12Z",
    "updated": "2026-01-15T11:12:10Z",
    "comment": "20 pages, 12 figures, CVC 2026 camera-ready version",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.13975",
    "title": "Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems",
    "authors": [
      "Kin Kwan Leung",
      "Mouloud Belbahri",
      "Yi Sui",
      "Alex Labach",
      "Xueying Zhang",
      "Stephen Anthony Rose",
      "Jesse C. Cresswell"
    ],
    "abstract": "Retrieval-augmented generation (RAG) is a prevalent approach for building LLM-based question-answering systems that can take advantage of external knowledge databases. Due to the complexity of real-world RAG systems, there are many potential causes for erroneous outputs. Understanding the range of errors that can occur in practice is crucial for robust deployment. We present a new taxonomy of the error types that can occur in realistic RAG systems, examples of each, and practical advice for addressing them. Additionally, we curate a dataset of erroneous RAG responses annotated by error types. We then propose an auto-evaluation method aligned with our taxonomy that can be used in practice to track and address errors during development. Code and data are available at https://github.com/layer6ai-labs/rag-error-classification.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.13975.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13975",
    "published": "2025-10-15T18:02:30Z",
    "updated": "2026-01-15T03:23:18Z",
    "comment": "EACL 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.00010",
    "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization",
    "authors": [
      "Jiajun Zhang",
      "Jianke Zhang",
      "Zeyu Cui",
      "Jiaxi Yang",
      "Lei Zhang",
      "Binyuan Hui",
      "Qiang Liu",
      "Zilei Wang",
      "Liang Wang",
      "Junyang Lin"
    ],
    "abstract": "Recent Large Language Models (LLMs) have demonstrated remarkable proficiency in code generation. However, their ability to create complex visualizations for scaled and structured data remains largely unevaluated and underdeveloped. To address this gap, we introduce PlotCraft, a new benchmark featuring 1k challenging visualization tasks that cover a wide range of topics, such as finance, scientific research, and sociology. The benchmark is structured around seven high-level visualization tasks and encompasses 48 distinct chart types. Crucially, it is the first to systematically evaluate both single-turn generation and multi-turn refinement across a diverse spectrum of task complexities. Our comprehensive evaluation of 23 leading LLMs on PlotCraft reveals obvious performance deficiencies in handling sophisticated visualization tasks. To bridge this performance gap, we develope SynthVis-30K, a large-scale, high-quality dataset of complex visualization code synthesized via a collaborative agent framework. Building upon this dataset, we develope PlotCraftor, a novel code generation model that achieves strong capabilities in complex data visualization with a remarkably small size. Across VisEval, PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance comparable to that of leading proprietary approaches. Especially, on hard task, Our model achieves over 50% performance improvement. We will release the benchmark, dataset, and code at https://github.com/Speakn0w/PlotCraft-Benchmark.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.00010.pdf",
    "abs_url": "https://arxiv.org/abs/2511.00010",
    "published": "2025-10-15T10:14:39Z",
    "updated": "2026-01-15T17:00:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.09293",
    "title": "One Sentence, Two Embeddings: Contrastive Learning of Explicit and Implicit Semantic Representations",
    "authors": [
      "Kohei Oda",
      "Po-Min Chuang",
      "Kiyoaki Shirai",
      "Natthawut Kertkeidkachorn"
    ],
    "abstract": "Sentence embedding methods have made remarkable progress, yet they still struggle to capture the implicit semantics within sentences. This can be attributed to the inherent limitations of conventional sentence embedding methods that assign only a single vector per sentence. To overcome this limitation, we propose DualCSE, a sentence embedding method that assigns two embeddings to each sentence: one representing the explicit semantics and the other representing the implicit semantics. These embeddings coexist in the shared space, enabling the selection of the desired semantics for specific purposes such as information retrieval and text classification. Experimental results demonstrate that DualCSE can effectively encode both explicit and implicit meanings and improve the performance of the downstream task.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.09293.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09293",
    "published": "2025-10-10T11:36:19Z",
    "updated": "2026-01-15T13:19:07Z",
    "comment": "EACL 2026 Findings",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.07745",
    "title": "Parallel Test-Time Scaling for Latent Reasoning Models",
    "authors": [
      "Runyang You",
      "Yongqi Li",
      "Meng Liu",
      "Wenjie Wang",
      "Liqiang Nie",
      "Wenjie Li"
    ],
    "abstract": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code and checkpoints released at https://github.com/ModalityDance/LatentTTS",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.07745.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07745",
    "published": "2025-10-09T03:33:00Z",
    "updated": "2026-01-15T14:50:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.04842",
    "title": "Distributionally Robust Causal Abstractions",
    "authors": [
      "Yorgos Felekis",
      "Theodoros Damoulas",
      "Paris Giampouras"
    ],
    "abstract": "Causal Abstraction (CA) theory provides a principled framework for relating causal models that describe the same system at different levels of granularity while ensuring interventional consistency between them. Recently, several approaches for learning CAs have been proposed, but all assume fixed and well-specified exogenous distributions, making them vulnerable to environmental shifts and misspecification. In this work, we address these limitations by introducing the first class of distributionally robust CAs and their associated learning algorithms. The latter cast robust causal abstraction learning as a constrained min-max optimization problem with Wasserstein ambiguity sets. We provide theoretical results, for both empirical and Gaussian environments, leading to principled selection of the level of robustness via the radius of these sets. Furthermore, we present empirical evidence across different problems and CA learning methods, demonstrating our framework's robustness not only to environmental shifts but also to structural model and intervention mapping misspecification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.04842.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04842",
    "published": "2025-10-06T14:26:12Z",
    "updated": "2026-01-15T09:56:46Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.02692",
    "title": "Fine-Tuning Diffusion Models via Intermediate Distribution Shaping",
    "authors": [
      "Gautham Govind Anil",
      "Shaan Ul Haque",
      "Nithish Kannen",
      "Dheeraj Nagaraj",
      "Sanjay Shakkottai",
      "Karthikeyan Shanmugam"
    ],
    "abstract": "Diffusion models are widely used for generative tasks across domains. While pre-trained diffusion models effectively capture the training data distribution, it is often desirable to shape these distributions using reward functions to align with downstream applications. Policy gradient methods, such as Proximal Policy Optimization (PPO), are widely used in the context of autoregressive generation. However, the marginal likelihoods required for such methods are intractable for diffusion models, leading to alternative proposals and relaxations. In this context, we unify variants of Rejection sAmpling based Fine-Tuning (RAFT) as GRAFT, and show that this implicitly performs KL regularized reward maximization with reshaped rewards. We then introduce P-GRAFT to shape distributions at intermediate noise levels and demonstrate empirically that this can lead to more effective fine-tuning. We mathematically explain this via a bias-variance tradeoff. Motivated by this, we propose inverse noise correction to improve flow models without leveraging explicit rewards. We empirically evaluate our methods on text-to-image(T2I) generation, layout generation, molecule generation and unconditional image generation. Notably, our framework, applied to Stable Diffusion 2, improves over policy gradient methods on popular T2I benchmarks in terms of VQAScore and shows an $8.81\\%$ relative improvement over the base model. For unconditional image generation, inverse noise correction improves FID of generated images at lower FLOPs/image.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.02692.pdf",
    "abs_url": "https://arxiv.org/abs/2510.02692",
    "published": "2025-10-03T03:18:47Z",
    "updated": "2026-01-15T06:46:41Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.01755",
    "title": "Learning Regularization Functionals for Inverse Problems: A Comparative Study",
    "authors": [
      "Johannes Hertrich",
      "Hok Shing Wong",
      "Alexander Denker",
      "Stanislas Ducotterd",
      "Zhenghan Fang",
      "Markus Haltmeier",
      "Željko Kereta",
      "Erich Kobler",
      "Oscar Leong",
      "Mohammad Sadegh Salehi",
      "Carola-Bibiane Schönlieb",
      "Johannes Schwab",
      "Zakhar Shumaylov",
      "Jeremias Sulam",
      "German Shâma Wache",
      "Martin Zach",
      "Yasi Zhang",
      "Matthias J. Ehrhardt",
      "Sebastian Neumayer"
    ],
    "abstract": "In recent years, a variety of learned regularization frameworks for solving inverse problems in imaging have emerged. These offer flexible modeling together with mathematical insights. The proposed methods differ in their architectural design and training strategies, making direct comparison challenging due to non-modular implementations. We address this gap by collecting and unifying the available code into a common framework. This unified view allows us to systematically compare the approaches and highlight their strengths and limitations, providing valuable insights into their future potential. We also provide concise descriptions of each method, complemented by practical guidelines.",
    "categories": [
      "cs.LG",
      "math.NA",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.01755.pdf",
    "abs_url": "https://arxiv.org/abs/2510.01755",
    "published": "2025-10-02T07:42:28Z",
    "updated": "2026-01-15T12:27:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文的核心贡献是开发了一个统一框架，用于系统比较各种学习正则化方法在解决逆问题中的应用，弥补了直接比较的挑战。",
      "motivation": "近年来，学习正则化框架在解决成像逆问题中迅速兴起，提供了灵活的建模方式和数学见解。然而，现有方法在架构设计和训练策略上差异显著，且实现多为非模块化，使得直接比较和评估成为难题。为了促进该领域的发展，本研究旨在填补这一空白，通过统一视角来系统地分析和比较不同方法。",
      "method": "论文通过收集和整合现有学习正则化方法的代码到一个共同框架中，实现了模块化比较。该方法允许对不同的架构设计和训练策略进行统一评估，突出了关键创新点如代码重构和标准化测试。虽然摘要未详细说明具体数据集或模型细节，但基于框架化的比较提供了技术特色。",
      "result": "摘要未明确说明具体实验数据，如准确率提升或效率改进。然而，通过统一框架，作者系统比较了各种学习正则化方法的优点和局限性，并提供了见解。虽然没有与基线方法的具体性能对比，但分析揭示了方法间的差异，为未来研究提供了参考。",
      "conclusion": "本研究的主要贡献是提供了一个统一框架和系统比较分析，强调了学习正则化方法在逆问题中的潜力和挑战。学术价值在于促进了方法间的标准化比较，实际应用价值在于为研究人员提供了实践指南。未来工作可扩展框架以包含更多方法，并探索具体应用场景的局限性。",
      "tags": [
        "Inverse Problems",
        "Learned Regularization",
        "Comparative Study",
        "Code Framework",
        "Imaging"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:43.753884Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.01444",
    "title": "Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning",
    "authors": [
      "Rui Liu",
      "Dian Yu",
      "Tong Zheng",
      "Runpeng Dai",
      "Zongxia Li",
      "Wenhao Yu",
      "Zhenwen Liang",
      "Linfeng Song",
      "Haitao Mi",
      "Pratap Tokekar",
      "Dong Yu"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model's uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.01444.pdf",
    "abs_url": "https://arxiv.org/abs/2510.01444",
    "published": "2025-10-01T20:32:08Z",
    "updated": "2026-01-15T17:51:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出DUPL方法，通过量化感知与输出不确定性来指导多模态推理中的策略学习，有效提升强化学习性能。",
      "motivation": "现有方法在多模态强化学习与可验证奖励中，通常将视觉输入视为确定性，忽略了视觉模态固有的感知歧义性。这导致模型无法区分不确定性是源于复杂推理还是模糊感知，从而无法有针对性地分配探索或学习信号，限制了模型在面对多模态数据时的性能提升和鲁棒性。",
      "method": "DUPL方法的核心在于量化并利用感知不确定性（通过对称KL散度）和输出不确定性（通过策略熵）。通过建立不确定性驱动的反馈循环和动态分支优先机制，DUPL重新校准策略优势，使学习聚焦于感知或决策模糊性高的状态。该方法在GRPO框架上实现，使用Qwen2.5-VL模型，通过主动探索来优化策略。",
      "result": "DUPL在六个多模态数学和通用推理基准上进行了评估，结果显示，对于Qwen2.5-VL 3B和7B模型，在视觉数学任务上准确率最高提升11.2%，在通用推理任务上最高提升7.1%。实验表明，DUPL在所有任务上都一致优于基线方法GRPO。",
      "conclusion": "本研究的主要贡献是提出并验证了双不确定性指导的策略学习方法DUPL，它能有效区分和处理多模态推理中的感知与输出不确定性，从而提升模型性能。该方法具有学术价值和实际应用潜力，可推广到其他多模态强化学习场景。未来工作方向摘要未明确说明。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Multimodal Reasoning",
        "Uncertainty Quantification",
        "Policy Learning",
        "Symmetric KL Divergence"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:13.128319Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.00666",
    "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models",
    "authors": [
      "Leah Bar",
      "Liron Mor Yosef",
      "Shai Zucker",
      "Neta Shoham",
      "Inbar Seroussi",
      "Nir Sochen"
    ],
    "abstract": "Most models of generative AI for images assume that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold's coordinate space is considered uninteresting and is predefined or considered uniform. In this study, we address the problem of Blind Image Denoising (BID), and to some extent, the problem of generating images from noise by unifying geometric and probabilistic perspectives. We introduce a novel framework that improves upon existing probabilistic approaches by incorporating geometric assumptions that enable the effective use of kernel-based probabilistic methods. Furthermore, the proposed framework extends prior geometric approaches by combining explicit and implicit manifold descriptions through the introduction of a distance function. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.00666.pdf",
    "abs_url": "https://arxiv.org/abs/2510.00666",
    "published": "2025-10-01T08:50:30Z",
    "updated": "2026-01-15T13:08:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个几何与概率统一的框架，通过流形-概率投影模型解释扩散模型并提升盲图像去噪和图像生成的性能。",
      "motivation": "生成AI模型常假设图像数据形成低维流形，但现有方法主要依赖概率近似，忽略几何结构，导致如扩散模型缺乏几何解释，限制了性能和应用。本研究旨在解决盲图像去噪和从噪声生成图像的问题，通过统一几何和概率视角，以克服现有方法的不足，增强模型的解释性和效果。",
      "method": "本研究提出Manifold-Probabilistic Projection Model (MPPM)，通过结合几何假设（如图像数据集形成光滑流形）和核方法，引入距离函数统一显式与隐式流形描述。框架在表示空间和潜在空间操作，将扩散模型解释为在“好图像”流形上的投影机制，从而构建新确定性模型以改进生成和去噪过程。",
      "result": "实验显示，Latent MPPM (LMPPM) 在多个数据集上优于Latent Diffusion Model (LDM)，在图像恢复和生成方面取得更好性能，摘要未明确说明具体数据指标，但强调了LMPPM的优越性，为现有基准方法提供了改进依据。",
      "conclusion": "论文贡献在于统一几何与概率视角，揭示了扩散模型的投影本质，并提出MPPM框架以提升性能。这增强了生成AI的解释性和应用价值，未来可扩展到其他视觉任务，如更复杂的数据集或跨领域生成问题。",
      "tags": [
        "Manifold Learning",
        "Generative AI",
        "Diffusion Models",
        "Blind Image Denoising",
        "Projection Models"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:50.014843Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.23773",
    "title": "Knowledge Homophily in Large Language Models",
    "authors": [
      "Utkarsh Sahu",
      "Zhisheng Qi",
      "Mahantesh Halappanavar",
      "Nedim Lipka",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Yu Zhang",
      "Yao Ma",
      "Yu Wang"
    ],
    "abstract": "Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.23773.pdf",
    "abs_url": "https://arxiv.org/abs/2509.23773",
    "published": "2025-09-28T09:40:27Z",
    "updated": "2026-01-15T18:26:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于知识同质性的图神经网络模型，优化大型语言模型的知识注入和推理效率。",
      "motivation": "大型语言模型在问答和事实检查等知识密集型应用中扮演关键角色，但其内部知识的结构组织尚未被探索，导致现有方法在知识检查和注入过程中可能效率低下。认知神经科学中的语义聚类和启动效应表明，知识之间存在关联性，这激发了研究者探索LLMs中类似的知识同质性模式，以解决知识管理中的效率问题，提升应用性能。摘要未明确说明现有方法的不足之处，但暗示了结构探索的重要性。",
      "method": "研究方法首先将LLM的知识映射为图表示，通过在三元组和实体级别进行知识检查来构建图结构。然后，分析实体与其邻居之间的知识同质性，即发现位置相近的实体在LLMs中具有相似的知识水平。基于这一原则，提出一个图神经网络回归模型，利用邻居的知识性分数来估计实体的知识性分数，从而识别知识薄弱区域。关键创新点在于结合认知科学启发和图神经网络技术，以优化知识管理。",
      "result": "主要实验结果表明，通过预测的知识性分数，研究能够优先检查知识覆盖较低的领域，从而在相同的标注预算下最大化知识覆盖。这提高了主动标注微调LLMs的效率，并增强了推理密集型问答中的多跳路径检索能力。摘要未提供具体的性能指标如准确率提升，但暗示了方法在优化知识管理和推理任务中的有效性。与基线方法的对比未明确说明，但基于知识同质性原则，可能提升了效率和检索效果。",
      "conclusion": "本研究的主要贡献是首次揭示大型语言模型中的知识同质性模式，并提出了图神经网络模型来估计知识性。这具有重要学术价值，深化了对LLM内部知识组织的理解，并为实际应用如知识注入和问答系统提供了高效工具。未来工作可探索如何将该方法扩展到其他模型类型或更复杂的知识图谱，以进一步提升适用性和泛化能力。",
      "tags": [
        "Large Language Models",
        "Graph Neural Networks",
        "Knowledge Graphs",
        "Active Learning",
        "Knowledge Homophily"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:15.161304Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.22964",
    "title": "Functional Critics Are Essential in Off-Policy Actor-Critic: Provable Convergence and Efficient Exploration",
    "authors": [
      "Qinxun Bai",
      "Yuxuan Han",
      "Wei Xu",
      "Zhengyuan Zhou"
    ],
    "abstract": "Off-policy reinforcement learning (RL) with function approximation offers an effective way to improve sample efficiency by reusing past experience. Within this setting, the actor-critic (AC) framework has achieved strong empirical success but suffers from the \"moving target\" problem, where the policy being evaluated changes continually. Functional critics, or policy-conditioned value functions, have been proposed to address this issue by including a representation of the policy as input. While the concept of generalizing value functions across policy space is appealing, previous efforts have struggled to remain competitive against state-of-the-art AC algorithms that do not utilize functional critics. In this work, we revisit functional critics within the off-policy AC framework and identify two aspects that render them a necessity rather than a luxury. First, in off-policy AC, critic learning contends with both the \"deadly triad\" instability and the \"moving target\" issue, while actor learning faces the challenge of estimating the exact off-policy policy gradient. This complex interplay makes theoretical convergence extremely difficult for practical algorithms. We demonstrate that a functional critic is essential for addressing this challenge and establish the first convergence proof for an off-policy target-based AC algorithm under linear function approximation. Second, we identify a crucial link between functional critic modeling and efficient exploration. Specifically, we show that approximating posterior sampling for exploration in model-free settings is infeasible without functional critics. Practically, we propose a tailored neural network architecture and a minimal AC algorithm that relies solely on these insights. In experiments on the DeepMind Control Suite, this implementation achieves performance competitive with state-of-the-art methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.22964.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22964",
    "published": "2025-09-26T21:55:26Z",
    "updated": "2026-01-15T06:25:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文证明了 functional critics 在 off-policy actor-critic 强化学习中的必要性，提供了首个理论收敛性证明并关联了高效探索。",
      "motivation": "研究旨在解决 off-policy 强化学习中的样本效率提升问题。在 actor-critic 框架中，存在 'moving target' 问题，即被评估的策略持续变化，导致 critic 学习不稳定。现有方法中，functional critics 虽被提出，但性能不足以与未使用该技术的最先进算法竞争，且理论收敛性因 critic 学习面临 'deadly triad' 不稳定性和 actor 学习需精确估计 off-policy 政策梯度而难以实现。这些问题凸显了开发更鲁棒方法的必要性。",
      "method": "论文重新审视 functional critics，提出其在处理 off-policy actor-critic 挑战中的核心作用。关键创新点包括：通过将策略表示作为输入，functional critics 能缓解 'moving target' 问题，并证明了其对实现理论收敛性的必要性。研究中还识别了 functional critics 与高效探索的联系，指出其可近似后验采样。实践上，设计了定制的神经网络架构和最小化 actor-critic 算法，该方法仅基于这些理论洞察，无需额外复杂模块，以简化实现并提升性能。",
      "result": "在 DeepMind Control Suite 上进行的实验表明，提出的方法在性能上与最先进算法具有竞争力。摘要未明确说明具体数值指标（如准确率提升），但通过比较基线方法，验证了 functional critics 在提升样本效率和探索效率方面的有效性，显示其能有效应对 off-policy 设置下的挑战。结果支持了理论分析，证明了该方法的实用价值。",
      "conclusion": "论文的主要贡献在于证明 functional critics 在 off-policy actor-critic 强化学习中不可或缺，不仅提供了首个理论收敛性证明，还揭示了其与高效探索的紧密关联。这项研究具有重要学术价值，为强化学习的理论发展提供了新视角，并具实际应用潜力，可提高智能体在复杂环境中的学习效率和鲁棒性。未来工作可能包括进一步优化架构或扩展到更广泛的应用场景。",
      "tags": [
        "Off-Policy Reinforcement Learning",
        "Actor-Critic",
        "Functional Critics",
        "Policy-Conditioned Value Functions",
        "DeepMind Control Suite"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:45.363844Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18001",
    "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
    "authors": [
      "Haocheng Luo",
      "Mehrtash Harandi",
      "Dinh Phung",
      "Trung Le"
    ],
    "abstract": "Sharpness-aware minimization (SAM) has emerged as a highly effective technique to improve model generalization, but its underlying principles are not fully understood. We investigate m-sharpness, where SAM performance improves monotonically as the micro-batch size for computing perturbations decreases, a phenomenon critical for distributed training yet lacking rigorous explanation. We leverage an extended Stochastic Differential Equation (SDE) framework and analyze stochastic gradient noise (SGN) to characterize the dynamics of SAM variants, including n-SAM and m-SAM. Our analysis reveals that stochastic perturbations induce an implicit variance-based sharpness regularization whose strength increases as m decreases. Motivated by this insight, we propose Reweighted SAM (RW-SAM), which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate our theory and method.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.18001.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18001",
    "published": "2025-09-22T16:40:42Z",
    "updated": "2026-01-15T06:16:42Z",
    "comment": "Accepted to NeurIPS 2025",
    "light_analysis": {
      "overview": "论文通过分析随机梯度噪声结构揭示 m-sharpness 机制，并提出可并行化的 Reweighted SAM 方法。",
      "motivation": "Sharpness-aware minimization (SAM) 能有效提升模型泛化能力，但其内在原理尚未完全明确。特别是在分布式训练中，m-sharpness 现象（即 SAM 性能随微批量大小减小而单调提高）对优化至关重要，却缺乏理论解释。本研究旨在解释此现象，为 SAM 变体在分布式环境的应用提供理论基础，以弥补现有方法的不足并改善训练效率。",
      "method": "研究采用扩展的随机微分方程（SDE）框架，深入分析随机梯度噪声（SGN）的结构，以描述 n-SAM 和 m-SAM 等 SAM 变体的训练动态。关键创新在于揭示随机扰动诱导了基于方差的隐式锐度正则化，其强度随微批量大小减小而增加。基于此，提出 Reweighted SAM (RW-SAM)，通过锐度加权采样来模拟 m-SAM 的泛化优势，同时保持计算的可并行性；摘要未明确说明使用的具体数据集或模型架构。",
      "result": "综合实验验证了理论的正确性，表明 RW-SAM 能有效模仿 m-SAM 的泛化好处，在保持可并行性的前提下提升模型性能。实验结果确认了方法的有效性，但摘要未提供具体准确率或效率数据，仅报告理论与方法得到验证，并展示了其在分布式训练场景中的实用价值。",
      "conclusion": "本研究的主要贡献是阐明了 m-sharpness 现象的理论机制并提出 RW-SAM 方法，深化了对 SAM 泛化原理的理解，尤其是在随机噪声作用方面。其学术价值在于为 SAM 提供了新视角，实际应用价值在于 RW-SAM 的可并行化特性，适用于大规模分布式训练以提高效率；未来工作可能包括算法优化或扩展到其他训练场景。",
      "tags": [
        "Sharpness-aware Minimization (SAM)",
        "Stochastic Gradient Noise (SGN)",
        "Stochastic Differential Equation (SDE)",
        "m-sharpness",
        "Reweighted SAM (RW-SAM)"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:18.160313Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.17444",
    "title": "Filling in the Clinical Gaps in Benchmark: Case for HealthBench for the Japanese medical system",
    "authors": [
      "Shohei Hisada",
      "Endo Sunao",
      "Himi Yamato",
      "Shoko Wakamiya",
      "Eiji Aramaki"
    ],
    "abstract": "This study investigates the applicability of HealthBench, a large-scale, rubric-based medical benchmark, to the Japanese context. Although robust evaluation frameworks are essential for the safe development of medical LLMs, resources in Japanese are scarce and often consist of translated multiple-choice questions. Our research addresses this issue in two ways. First, we establish a performance baseline by applying a machine-translated version of HealthBench's 5,000 scenarios to evaluate two models: a high-performing multilingual model (GPT-4.1) and a Japanese-native open-source model (LLM-jp-3.1). Secondly, we use an LLM-as-a-Judge approach to systematically classify the benchmark's scenarios and rubric criteria. This allows us to identify 'contextual gaps' where the content is misaligned with Japan's clinical guidelines, healthcare systems or cultural norms. Our findings reveal a modest performance drop in GPT-4.1 due to rubric mismatches, as well as a significant failure in the Japanese-native model, which lacked the required clinical completeness. Furthermore, our classification shows that, despite most scenarios being applicable, a significant proportion of the rubric criteria require localisation. This work underscores the limitations of direct benchmark translation and highlights the urgent need for a context-aware, localised adaptation, a \"J-HealthBench\", to ensure the reliable and safe evaluation of medical LLMs in Japan.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.17444.pdf",
    "abs_url": "https://arxiv.org/abs/2509.17444",
    "published": "2025-09-22T07:36:12Z",
    "updated": "2026-01-15T08:24:28Z",
    "comment": "draft v0.3 Code and analysis data is available at https://zenodo.org/records/17405321",
    "light_analysis": {
      "overview": "本论文通过评估HealthBench基准在日本医疗系统中的适用性，揭示了直接翻译的局限性，并提出需要本地化适应以确保医疗大语言模型的可靠评估。",
      "motivation": "本研究旨在解决日本医疗大语言模型评估资源稀缺的问题。现有方法多为翻译的多选题，不足以确保模型在临床环境中的安全性和可靠性。这导致评估框架缺乏针对日本临床指南、医疗体系和文化规范的本地化适应，从而限制了医疗AI的安全发展。因此，建立上下文感知的基准成为重要需求，以弥补现有资源的不足。",
      "method": "研究方法包括两个主要部分。首先，使用机器翻译的HealthBench基准中5,000个场景，评估了两个模型：高性能多语言模型GPT-4.1和日本本土开源模型LLM-jp-3.1，以建立性能基线。其次，采用LLM-as-a-Judge方法系统分类基准场景和评估标准，识别与日本临床实践不匹配的‘contextual gaps’。关键创新在于结合翻译评估和自动分类技术，以分析rubrics标准是否需要本地化调整。",
      "result": "实验结果显示，GPT-4.1模型因评估标准不匹配而性能略有下降，而日本本土模型LLM-jp-3.1因缺乏临床完整性而表现显著失败。具体而言，分类分析表明，尽管大多数场景适用，但大量rubrics标准需要本地化以适应日本医疗环境。这凸显了直接翻译基准在评估日本医疗大语言模型时的不足，强调了本地化适应的重要性。",
      "conclusion": "本研究的结论强调了直接翻译医疗基准的局限性，并呼吁开发上下文感知的本地化版本如‘J-HealthBench’。学术价值在于揭示了跨文化基准适应的挑战，实际应用价值在于为日本医疗AI的安全评估提供指导。未来工作方向可能包括构建更全面的本地化基准，以提升评估的准确性和可靠性。",
      "tags": [
        "Large Language Model",
        "Benchmark Evaluation",
        "Machine Translation",
        "LLM-as-a-Judge",
        "Medical AI"
      ]
    },
    "analyzed_at": "2026-01-16T03:15:45.111963Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14981",
    "title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation",
    "authors": [
      "Chuan Fang",
      "Heng Li",
      "Yixun Liang",
      "Jia Zheng",
      "Yongsen Mao",
      "Yuan Liu",
      "Rui Tang",
      "Zihan Zhou",
      "Ping Tan"
    ],
    "abstract": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,431 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.14981.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14981",
    "published": "2025-09-18T14:12:32Z",
    "updated": "2026-01-15T13:38:16Z",
    "comment": "3D scene generation; diffusion model; Scene reconstruction and understanding",
    "light_analysis": {
      "overview": "SpatialGen：一种新颖的多视图多模态扩散模型，通过大规模合成数据集实现布局引导的逼真3D室内场景生成。",
      "motivation": "手动创建高保真度3D室内场景模型耗时耗力，限制设计、虚拟现实和机器人等领域的应用。现有生成AI方法在平衡视觉质量、多样性、语义一致性和用户控制方面面临挑战，导致结果不够逼真或可控。主要瓶颈是缺乏针对该任务的大规模高质量数据集，因此本研究旨在填补这一缺口，推动自动化室内场景生成的进展。",
      "method": "论文提出SpatialGen，一种多视图多模态扩散模型。该模型以3D布局和参考图像（通过文本提示生成）为输入，从任意视点合成外观（彩色图像）、几何（场景坐标图）和语义（语义分割图）。关键创新点在于利用一个综合合成数据集，包含12,328个结构化标注场景和4.7M张照片级真实感2D渲染，并通过扩散模型框架确保跨模态的空间一致性。",
      "result": "在实验中，SpatialGen在生成逼真且语义一致的3D室内场景方面，始终展现出优于先前方法的结果。摘要未明确说明具体的性能指标（如准确率或效率提升），但论文报告其模型在视觉质量和空间一致性方面显著改进，对比基线方法表现出更强的泛化能力。",
      "conclusion": "本文的主要贡献是开发了SpatialGen模型和一个大规模的合成数据集，为布局引导的3D室内场景生成提供了高效且可控的解决方案。通过开源数据和模型，该研究促进了室内场景理解和生成领域的学术进步，并有望应用于设计、虚拟现实等实际场景。摘要未提及潜在局限性，未来工作可能包括优化模型效率或扩展至更复杂的环境生成任务。",
      "tags": [
        "3D Indoor Scene Generation",
        "Multi-view Diffusion Model",
        "Multimodal Synthesis",
        "Synthetic Dataset",
        "Layout-guided Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:18.796370Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.11926",
    "title": "Graph Algorithm Unrolling with Douglas-Rachford Iterations for Image Interpolation with Guaranteed Initialization",
    "authors": [
      "Xue Zhang",
      "Bingshuo Hu",
      "Gene Cheung"
    ],
    "abstract": "Conventional deep neural nets (DNNs) initialize network parameters at random and then optimize each one via stochastic gradient descent (SGD), resulting in substantial risk of poor-performing local minima.Focusing on the image interpolation problem and leveraging a recent theorem that maps a (pseudo-)linear interpolator Θ to a directed graph filter that is a solution to a MAP problem regularized with a graph shift variation (GSV) prior, we first initialize a directed graph adjacency matrix A based on a known interpolator Θ, establishing a baseline performance.Then, towards further gain, we learn perturbation matrices P and P(2) from data to augment A, whose restoration effects are implemented via Douglas-Rachford (DR) iterations, which we unroll into a lightweight interpretable neural net.Experimental results demonstrate state-of-the-art image interpolation results, while drastically reducing network parameters.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.11926.pdf",
    "abs_url": "https://arxiv.org/abs/2509.11926",
    "published": "2025-09-15T13:43:55Z",
    "updated": "2026-01-15T02:45:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于图算法展开和Douglas-Rachford迭代的图像插值方法，实现有保证的初始化和参数高效，达到最先进性能。",
      "motivation": "传统深度神经网络通过随机初始化参数并使用随机梯度下降优化，存在陷入局部最小值的风险，影响模型性能和稳定性。针对图像插值问题，现有方法往往依赖于复杂初始化或高参数数量，导致效率低下。本文旨在解决这些问题，通过建立可靠的初始化策略和优化算法，提升图像插值的鲁棒性和可解释性，以应对实际应用中的挑战。",
      "method": "该方法首先利用已知的伪线性插值器Θ初始化有向图邻接矩阵A，以建立基线性能。接着，从数据中学习扰动矩阵P和P(2)来增强A，其恢复效果通过Douglas-Rachford (DR) 迭代实现。DR迭代被展开为一个轻量级、可解释的神经网络，结合了图论和优化算法，避免了传统随机梯度下降的局部最小值问题，并采用图移位变化先验进行正则化。",
      "result": "实验结果表明，该方法在图像插值任务上取得了最先进的性能，与其他基线方法相比具有显著优势。同时，网络参数数量大幅减少，提高了模型的效率和可部署性。摘要未提供具体的准确率提升数据，但强调在保持高性能的同时实现了参数的精简，证明了方法的有效性。",
      "conclusion": "本研究的主要贡献在于提出了一种结合图算法展开和Douglas-Rachford迭代的图像插值方法，实现了有保证的初始化和高性能。这种方法不仅提高了插值准确性，还大幅减少了参数，具有学术价值，如在可解释网络设计和优化理论中的应用，以及实际价值，如轻量级模型部署。未来工作可探索该方法在其他视觉任务中的扩展或评估更多数据集上的表现。",
      "tags": [
        "Graph Algorithm Unrolling",
        "Douglas-Rachford Iterations",
        "Image Interpolation",
        "Graph Filters",
        "Initialization Guarantees"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:00.097578Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.10798",
    "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction",
    "authors": [
      "Yijun Liu",
      "Yixuan Wang",
      "Yuzhuang Xu",
      "Shiyu Ji",
      "Yang Xu",
      "Qingfu Zhu",
      "Wanxiang Che"
    ],
    "abstract": "Large language models (LLMs) utilize key-value (KV) cache to store historical information during sequence processing. The size of KV cache grows linearly as the length of the sequence extends, which seriously affects memory usage and decoding efficiency. Current methods for KV cache eviction typically utilize the last window from the pre-filling phase as queries to compute the KV importance scores for eviction. Although this scheme is simple to implement, it tends to overly focus on local information, potentially leading to the neglect or omission of crucial global information. To mitigate this issue, we propose Judge Q, a novel training method which incorporates a soft token list. This method only tunes the model's embedding layer at a low training cost. By concatenating the soft token list at the end of the input sequence, we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens. In this way, the queries corresponding to the soft tokens can effectively capture global information and better evaluate the importance of the keys and values within the KV cache, thus maintaining decoding quality when KV cache is evicted. Under the same eviction budget, our method exhibits less performance degradation compared to existing eviction approaches. We validate our approach through experiments conducted on models such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an improvement of approximately 1 point on the LongBench and over 3 points on RULER. This proposed methodology can be seamlessly integrated into existing open-source models with minimal training overhead, thereby enhancing performance in KV cache eviction scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.10798.pdf",
    "abs_url": "https://arxiv.org/abs/2509.10798",
    "published": "2025-09-13T03:34:12Z",
    "updated": "2026-01-15T09:11:49Z",
    "comment": "Accepted in AAAI 2026",
    "light_analysis": {
      "overview": "Judge Q是一种通过训练软令牌列表来优化KV缓存驱逐的方法，以捕获全局信息并减少性能下降。",
      "motivation": "大语言模型在序列处理中依赖KV缓存存储历史信息，但缓存大小随序列长度线性增长，导致内存使用和解码效率严重受限。现有的KV缓存驱逐方法通常基于预填充阶段的最后窗口作为查询，虽然实现简单，但倾向于过度关注局部信息，可能忽略关键全局信息，从而影响解码质量。因此，需要一种能有效保留全局信息的方法来优化KV缓存驱逐。",
      "method": "本文提出Judge Q方法，引入软令牌列表作为训练查询。该方法仅调整模型的嵌入层，训练成本低。通过在输入序列末尾连接软令牌列表，训练其注意力图与实际解码令牌的注意力图对齐。这样，软令牌对应的查询能够有效捕获全局信息，用于评估KV缓存中键和值的重要性，从而在驱逐缓存时维持解码质量。实验中使用Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3等模型进行验证。",
      "result": "实验结果表明，在相同的驱逐预算下，Judge Q方法相比于现有驱逐方法展现出更少的性能下降。在LongBench基准上性能提升约1点，在RULER基准上提升超过3点。验证通过基准如Needle-in-a-Haystack，在模型如Llama-3.1-8B-Instruct和Mistral-7B-Instruct-v0.3上进行，显示方法能有效保持解码质量。",
      "conclusion": "论文的主要贡献是提出Judge Q训练方法，通过软令牌列表优化KV缓存驱逐中的信息保留。该方法在学术上改进了LLM的缓存管理机制，实际应用中可无缝集成到现有开源模型，训练开销最小，从而增强KV缓存驱逐场景下的性能。局限性未来工作方向摘要未明确说明，但隐含了进一步优化或扩展到其他模型的潜力。",
      "tags": [
        "Large Language Model",
        "KV Cache Eviction",
        "Soft Tokens",
        "Attention Alignment",
        "Efficient Training"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:16.697389Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.10222",
    "title": "Compartmentalised Agentic Reasoning for Clinical NLI",
    "authors": [
      "Maël Jullien",
      "Lei Xu",
      "Marco Valentino",
      "André Freitas"
    ],
    "abstract": "Large language models can produce fluent judgments for clinical natural language inference, yet they frequently fail when the decision requires the correct inferential schema rather than surface matching. We introduce CARENLI, a compartmentalised agentic framework that routes each premise-statement pair to a reasoning family and then applies a specialised solver with explicit verification and targeted refinement. We evaluate on an expanded CTNLI benchmark of 200 instances spanning four reasoning families: Causal Attribution, Compositional Grounding, Epistemic Verification, and Risk State Abstraction. Across four contemporary backbone models, CARENLI improves mean accuracy from about 23% with direct prompting to about 57%, a gain of roughly 34 points, with the largest benefits on structurally demanding reasoning types. These results support compartmentalisation plus verification as a practical route to more reliable and auditable clinical inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.10222.pdf",
    "abs_url": "https://arxiv.org/abs/2509.10222",
    "published": "2025-09-12T13:14:47Z",
    "updated": "2026-01-15T11:51:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出CARENLI框架，通过分隔化代理推理和显式验证，显著提升临床自然语言推理的准确性和可靠性。",
      "motivation": "大型语言模型在临床自然语言推理中，虽然能生成流畅判断，但在需要深层推理模式（如因果归因）而非表面匹配时经常失败，导致决策错误。现有直接提示方法的平均准确率仅约23%，无法满足临床领域对可靠性和可审计性的严格要求，因此需要更有效的方法来处理复杂推理任务。",
      "method": "论文提出CARENLI框架，首先将每个前提-陈述对路由到四个预定义推理家族（包括因果归因、组合基础、认知验证和风险状态抽象），然后应用专门求解器进行显式验证和针对性细化。该方法基于分隔化策略，针对不同推理类型优化处理，评估使用扩展的CTNLI基准，包含200个实例，并在四个当代骨干模型上实施。",
      "result": "在四个当代骨干模型上，CARENLI将平均准确率从直接提示的大约23%提升至大约57%，增长了约34个百分点。在结构要求高的推理类型（如因果归因）上，提升效益最为显著。实验结果表明，框架显著优于基线方法，验证了分隔化加验证策略的有效性。",
      "conclusion": "CARENLI框架通过分隔化代理推理和显式验证，提高了临床自然语言推理的准确率和可靠性，其学术贡献在于创新性结合路由和验证机制，实际应用价值在于支持更可靠、可审计的临床决策系统。未来工作可能涉及扩展到更多推理类型或集成更多模型，以进一步提升性能。",
      "tags": [
        "Large Language Model",
        "Natural Language Inference",
        "Agentic Framework",
        "Verification Techniques",
        "Clinical Reasoning"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:46.854197Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.07829",
    "title": "Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost",
    "authors": [
      "Mihai Nadas",
      "Laura Diosan",
      "Andreea Tomescu",
      "Andrei Piscoran"
    ],
    "abstract": "Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TinyFabulist Translation Framework (TF2), a unified framework for dataset creation, fine-tuning, and evaluation in English->Romanian literary translation, centered on the creation and open release of both a compact, fine-tuned language model (TF2-12B) and large-scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high-quality literary datasets in low-resource languages such as Romanian. Our pipeline first generates 15k high-quality Romanian reference translations from the TF1 pool using a high-performing LLM. We then apply a two-stage fine-tuning process to a 12B-parameter open-weight model: (i) instruction tuning to capture genre-specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus-level BLEU with a five-dimension LLM-based rubric (accuracy, fluency, coherence, style, and cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine-tuned model achieves strong fluency and adequacy, narrowing the gap to top-performing proprietary models under automated and human-anchored evaluation, while being open, accessible, and significantly more cost-effective. Alongside the fine-tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost-efficient translation, cross-lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low-resource settings.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.07829.pdf",
    "abs_url": "https://arxiv.org/abs/2509.07829",
    "published": "2025-09-09T15:07:14Z",
    "updated": "2026-01-15T16:20:47Z",
    "comment": "25 pages, 8 figures, includes datasets and models released on Hugging Face",
    "light_analysis": {
      "overview": "本文提出TinyFabulist Translation Framework (TF2)框架，通过统一的数据集创建、微调和评估方法，使小开放模型在低资源文学翻译中达到接近大型模型的性能，且成本显著降低。",
      "motivation": "文学翻译是机器翻译中的一个独特复杂任务，尤其对于低资源语言如罗马尼亚语，现有方法往往缺乏高质量、丰富的文学数据集，导致小开放模型性能不足。传统专有模型成本高昂且不透明，限制了开放研究和发展。本文旨在解决这一问题，通过开发开放框架提升翻译质量的可访问性和效率，支持低资源语言的文化传播和机器翻译技术进步。",
      "method": "TF2框架采用端到端方法，首先基于最大合成英语寓言数据集DS-TF1-EN-3M，使用高性能大语言模型生成15k高质量罗马尼亚语参考翻译，创建合成并行数据集DS-TF2-EN-RO-3M和DS-TF2-EN-RO-15K。然后对12B参数开放权重模型进行两阶段微调：指令调谐捕捉文学体裁的叙事风格，适配器压缩优化模型部署效率。评估结合语料库级别BLEU指标和大语言模型驱动的五维标准（准确性、流畅性、连贯性、风格和文化适应），实现全面翻译质量分析。",
      "result": "实验结果显示，微调模型在自动评估（BLEU分数）和基于大语言模型的五维评估中表现出强流畅性和充分性，显著缩小了与顶级专有模型的性能差距。模型在低成本下实现了接近大型模型的翻译质量，保持开放和可访问特性，适合低资源环境应用。具体数据摘要未明确说明，但评估强调了成本效益和翻译质量的整体提升。",
      "conclusion": "TF2框架提供了一个可重复的端到端管道，促进成本效益高的翻译研究和开放模型在低资源文学翻译中的广泛采用。研究贡献包括开放数据集、微调模型和评估工具，支持跨语言叙事生成和文化传播，具有学术和实际应用价值。局限性在于可能未覆盖所有语言，未来工作可扩展到更多低资源语言和文学体裁，进一步优化模型效率和通用性。",
      "tags": [
        "Machine Translation",
        "Fine-tuning",
        "Adapter Compression",
        "Synthetic Data",
        "Language Model"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:53.576175Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.02097",
    "title": "JudgeAgent: Beyond Static Benchmarks for Knowledge-Driven and Dynamic LLM Evaluation",
    "authors": [
      "Zhichao Shi",
      "Xuhui Jiang",
      "Chengjin Xu",
      "Cangli Yao",
      "Shengjia Ma",
      "Yinghan Shen",
      "Zixuan Li",
      "Jian Guo",
      "Yuanzhuo Wang"
    ],
    "abstract": "Current evaluation methods for large language models (LLMs) primarily rely on static benchmarks, presenting two major challenges: limited knowledge coverage and fixed difficulties that mismatch with the evaluated LLMs. These limitations lead to superficial assessments of LLM knowledge, thereby impeding the targeted model optimizations. To bridge this gap, we propose JudgeAgent, a knowledge-driven and dynamic evaluation framework for LLMs. To address the challenge of limited knowledge coverage, JudgeAgent leverages LLM agents equipped with context graphs to traverse knowledge structures systematically for question generation. Furthermore, to mitigate data contamination and difficulty mismatch, it adopts a difficulty-adaptive and multi-turn interview mechanism. Thereby, JudgeAgent can achieve comprehensive evaluations and facilitate more effective improvement of LLMs. Empirical results demonstrate that JudgeAgent enables more comprehensive evaluations and facilitates effective model iterations, highlighting the potential of this knowledge-driven and dynamic evaluation paradigm. The source code is available on https://github.com/DataArcTech/JudgeAgent.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.02097.pdf",
    "abs_url": "https://arxiv.org/abs/2509.02097",
    "published": "2025-09-02T08:52:16Z",
    "updated": "2026-01-15T04:01:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "JudgeAgent 提出了一个知识驱动和动态的大语言模型评估框架，以解决静态基准中知识覆盖有限和难度不匹配的问题。",
      "motivation": "目前，大语言模型的评估主要依赖于静态基准测试，存在两个关键挑战：知识覆盖范围有限和难度固定，与评估模型的能力不匹配。这导致评估结果过于肤浅，无法全面反映模型的真实知识水平，从而阻碍了有针对性的模型优化。现有方法无法动态适应不同模型的难度和知识结构，因此需要一种更灵活和全面的评估方法，以促进大语言模型的持续改进和发展。",
      "method": "JudgeAgent 采用 LLM 代理结合上下文图，系统遍历知识结构以生成评估问题，实现知识驱动的动态评估。关键创新点在于集成上下文图进行知识结构探索，并引入难度自适应和多轮面试机制，以缓解数据污染和难度不匹配问题。这一方法允许框架动态调整评估难度，适应不同 LLM 的评估需求，摘要未明确说明具体的数据集或模型架构细节。",
      "result": "实证结果表明，JudgeAgent 能够实现更全面的评估并促进有效的模型迭代。摘要未提供具体的性能指标，如准确率提升或效率改进数据，但暗示了与传统静态基准相比，该框架在知识覆盖和适应性方面具有优势。与基线方法对比，JudgeAgent 通过动态机制可能提高了评估的准确性和针对性，支持更好的模型优化。",
      "conclusion": "本研究的主要贡献是提出了 JudgeAgent，一个知识驱动和动态的 LLM 评估框架，为克服静态基准的局限性提供了新思路。其学术价值在于推动了评估范式的转变，从固定测试向灵活适应发展；实际应用价值在于支持更有效的模型迭代和优化，有助于提升 LLM 的整体性能。摘要未提及局限性或未来工作方向，但强调了该框架的潜在应用潜力。",
      "tags": [
        "Large Language Model Evaluation",
        "Knowledge-Driven Framework",
        "Context Graphs",
        "Adaptive Difficulty",
        "Multi-turn Interview"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:57.945851Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.00451",
    "title": "Encoder-Only Image Registration",
    "authors": [
      "Xiang Chen",
      "Renjiu Hu",
      "Jinwei Zhang",
      "Yuxi Zhang",
      "Xinyao Yu",
      "Min Liu",
      "Yaonan Wang",
      "Hang Zhang"
    ],
    "abstract": "Learning-based techniques have significantly improved the accuracy and speed of deformable image registration. However, challenges such as reducing computational complexity and handling large deformations persist. To address these challenges, we analyze how convolutional neural networks (ConvNets) influence registration performance using the Horn-Schunck optical flow equation. Supported by prior studies and our empirical experiments, we observe that ConvNets play two key roles in registration: linearizing local intensities and harmonizing global contrast variations. Based on these insights, we propose the Encoder-Only Image Registration (EOIR) framework, designed to achieve a better accuracy-efficiency trade-off. EOIR separates feature learning from flow estimation, employing only a 3-layer ConvNet for feature extraction and a set of 3-layer flow estimators to construct a Laplacian feature pyramid, progressively composing diffeomorphic deformations under a large-deformation model. Results on five datasets across different modalities and anatomical regions demonstrate EOIR's effectiveness, achieving superior accuracy-efficiency and accuracy-smoothness trade-offs. With comparable accuracy, EOIR provides better efficiency and smoothness, and vice versa. The source code of EOIR is publicly available on https://github.com/XiangChen1994/EOIR.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.00451.pdf",
    "abs_url": "https://arxiv.org/abs/2509.00451",
    "published": "2025-08-30T10:45:39Z",
    "updated": "2026-01-15T15:23:46Z",
    "comment": "accepted by IEEE Transactions on Circuits and Systems for Video Technology",
    "light_analysis": {
      "overview": "本文提出Encoder-Only Image Registration框架，通过分离特征学习和流估计，实现了更好的精度-效率和精度-平滑度权衡。",
      "motivation": "可变形图像配准在医疗图像分析等领域至关重要，尽管基于学习的技术已显著提升精度和速度，但仍面临计算复杂度高和处理大变形等挑战。现有方法在平衡效率与性能方面存在不足，因此需要深入分析卷积神经网络在配准中的作用机制，以设计更优的解决方案。作者基于Horn-Schunck光流方程和实证实验，旨在揭示ConvNets的核心功能，从而改进现有配准方法的局限。",
      "method": "研究方法基于对卷积神经网络在配准中角色的分析，发现其线性化局部强度和协调全局对比变化。在此基础上，提出了Encoder-Only Image Registration框架，将特征学习与流估计分离：使用一个3层ConvNet进行特征提取，并配合一组3层流估计器构建拉普拉斯特征金字塔。通过逐步组合微分同胚变形，该方法在大变形模型下实现高效且平滑的图像配准，核心创新在于优化了特征表示和变形过程的集成。",
      "result": "在五个不同模态和解剖区域的数据集上进行实验，结果显示EOIR框架有效提升了性能平衡，实现了优越的精度-效率和精度-平滑度权衡。具体而言，在保持可比精度的情况下，EOIR提供了更好的计算效率和平滑性，反之亦然，突显了其在多种应用场景中的鲁棒性和实用性。实验结果未提供具体数值，但强调了对基线方法的改进和均衡优势。",
      "conclusion": "论文的主要贡献是提出了Encoder-Only Image Registration框架，为可变形图像配准提供了新的技术路线，显著优化了精度与效率的平衡。学术价值在于深入分析了ConvNets在配准中的作用，推动相关研究发展；实际应用价值体现在医疗图像等领域的高效配准。摘要未明确说明局限性，未来工作可能涉及扩展应用到更复杂场景或优化模型参数。源代码已公开，促进社区验证和进一步探索。",
      "tags": [
        "Deformable Image Registration",
        "Convolutional Neural Networks",
        "Optical Flow",
        "Laplacian Feature Pyramid",
        "Diffeomorphic Deformation"
      ]
    },
    "analyzed_at": "2026-01-16T03:16:52.704875Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.19188",
    "title": "FastMesh: Efficient Artistic Mesh Generation via Component Decoupling",
    "authors": [
      "Jeonghwan Kim",
      "Yushi Lan",
      "Armando Fortes",
      "Yongwei Chen",
      "Xingang Pan"
    ],
    "abstract": "Recent mesh generation approaches typically tokenize triangle meshes into sequences of tokens and train autoregressive models to generate these tokens sequentially. Despite substantial progress, such token sequences inevitably reuse vertices multiple times to fully represent manifold meshes, as each vertex is shared by multiple faces. This redundancy leads to excessively long token sequences and inefficient generation processes. In this paper, we propose an efficient framework that generates artistic meshes by treating vertices and faces separately, significantly reducing redundancy. We employ an autoregressive model solely for vertex generation, decreasing the token count to approximately 23% of that required by the most compact existing tokenizer. Next, we leverage a bidirectional transformer to complete the mesh in a single step by capturing inter-vertex relationships and constructing the adjacency matrix that defines the mesh faces. To further improve the generation quality, we introduce a fidelity enhancer to refine vertex positioning into more natural arrangements and propose a post-processing framework to remove undesirable edge connections. Experimental results show that our method achieves more than 8x faster speed on mesh generation compared to state-of-the-art approaches, while producing higher mesh quality.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.19188.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19188",
    "published": "2025-08-26T16:51:02Z",
    "updated": "2026-01-15T09:17:59Z",
    "comment": "Accepted by 3DV 2026; Project Page: https://jhkim0759.github.io/projects/FastMesh/",
    "light_analysis": {
      "overview": "FastMesh 提出一个通过分离顶点和面生成艺术网格的高效框架，显著减少冗余并实现超8倍速度提升和更高质量。",
      "motivation": "近期网格生成方法通常将三角形网格token化，训练自回归模型序列生成token。然而，由于网格中顶点被多个面共享，这种表示不可避免地重复顶点，导致token序列过长，生成过程效率低下。艺术网格生成在计算机图形学和数字艺术中应用广泛，但现有方法的冗余问题严重影响了生成速度，限制了实时应用和大型模型生成，因此需要解决这一效率瓶颈。",
      "method": "论文提出FastMesh框架，通过组件解耦方式分离顶点和面生成。首先，使用自回归模型专门生成顶点，将token数量减少至现有最紧凑tokenizer的大约23%，以降低冗余。接着，利用双向transformer模型捕获顶点间关系，一步完成网格构建，生成定义面的邻接矩阵。此外，引入保真度增强器优化顶点位置排列为更自然的安排，并提出后处理框架移除不良边连接以提升质量。",
      "result": "实验结果显示，FastMesh方法相比最先进方法在网格生成速度上实现超过8倍的提升。同时，生成的网格质量更高，表现为更优的顶点安排和拓扑结构，尽管摘要未提供具体指标如准确率数值，但与基线方法的对比强调了效率和质量的同步改进，突出了本方法的性能优势。",
      "conclusion": "FastMesh的主要贡献是提出组件解耦的艺术网格生成方法，通过分离顶点和面减少冗余，实现高速高质生成。该研究在计算机图形学领域具有学术价值，为网格生成效率问题提供新思路，可应用于数字艺术创作和建模流程。未来工作可扩展至复杂网格或集成其他技术，摘要未明确说明具体局限性。",
      "tags": [
        "Mesh Generation",
        "Autoregressive Models",
        "Transformers",
        "Component Decoupling",
        "Vertex Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:17.273784Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.18088",
    "title": "How Quantization Shapes Bias in Large Language Models",
    "authors": [
      "Federico Marcuzzi",
      "Xuefei Ning",
      "Roy Schwartz",
      "Iryna Gurevych"
    ],
    "abstract": "This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, fairness, toxicity, and sentiment. We employ both probability- and generated text-based metrics across 13 benchmarks and evaluate models that differ in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and subgroups, and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.18088.pdf",
    "abs_url": "https://arxiv.org/abs/2508.18088",
    "published": "2025-08-25T14:48:26Z",
    "updated": "2026-01-15T16:30:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究全面评估量化对大型语言模型中偏差的细微影响，揭示其在不同偏差类型上的复杂作用，强调应用中需平衡效率与伦理。",
      "motivation": "量化技术在大型语言模型中广泛应用以提高效率，但现有研究多集中于性能下降，忽视了量化对模型偏差的潜在影响，特别是在个体人口子群上的副作用。本研究旨在填补这一空白，系统评估量化如何影响刻板印象、公平性、毒性和情感等偏差类型，为AI系统的公平部署提供依据。量化可能导致非预期的偏差变化，因此理解其伦理影响对实际应用至关重要，确保技术发展与社会责任相协调。",
      "method": "本研究采用权重和激活量化策略，通过概率和生成文本基准的指标，在13个标准基准上系统评估量化对偏差的影响。偏差类型涵盖刻板印象、公平性、毒性和情感，评估对象包括不同架构家族和推理能力的大型语言模型，以全面分析量化效果。关键创新在于综合量化对多维度偏差的评估方法，为量化技术提供了新的伦理分析视角，结合多种指标确保结果的鲁棒性。摘要未明确说明具体模型架构细节，但基于现有信息合理推断。",
      "result": "实验结果显示量化对模型偏差产生复杂影响：量化能够降低模型毒性输出，且对情感表达没有显著改变；但在生成任务中，量化倾向于轻微增加刻板印象和不公平性，特别是在激进压缩策略下。这些趋势在不同人口子群和模型类型中基本一致，但影响幅度因具体量化设置而异，突出了量化策略选择对偏差控制的重要性。与基线方法对比，量化在减少毒性方面表现积极，但在公平性上需进一步优化。",
      "conclusion": "本研究的主要贡献在于全面揭示量化对大型语言模型偏差的细微影响，强调了在应用量化时需谨慎权衡效率与伦理。学术上，它为量化研究提供了偏差分析的新视角，推动AI公平性评估的发展；实践中，指导开发者在部署模型时考虑偏差风险，优化量化策略。未来工作可扩展至更多模型和偏差类型，以深化理解并解决局限性，如摘要未明确说明具体数据细节，需要进一步实验验证。",
      "tags": [
        "Quantization",
        "Large Language Models",
        "Bias Evaluation",
        "Fairness",
        "StereoType Analysis"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:35.080634Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.16846",
    "title": "BASIL: Bayesian Assessment of Sycophancy in LLMs",
    "authors": [
      "Katherine Atwell",
      "Pedram Heydari",
      "Anthony Sicilia",
      "Malihe Alikhani"
    ],
    "abstract": "Sycophancy (overly agreeable or flattering behavior) poses a fundamental challenge for human-AI collaboration, particularly in high-stakes decision-making domains such as health, law, and education. A central difficulty in studying sycophancy in large language models (LLMs) is disentangling sycophantic belief shifts from rational changes in behavior driven by new evidence or user-provided information. Existing approaches either measure descriptive behavior changes or apply normative evaluations that rely on objective ground truth, limiting their applicability to subjective or uncertain tasks. We introduce a Bayesian probabilistic framework, grounded in behavioral economics and rational decision theory, that explicitly separates sycophancy from rational belief updating. Within this framework, we achieve three objectives: (i) a descriptive metric that measures sycophancy while controlling for rational responses to evidence; (ii) a normative metric that quantifies how sycophancy leads models astray from Bayesian-consistent belief updating; and (iii) the ability to apply both metrics in settings without ground-truth labels. Applying our framework across multiple LLMs and three uncertainty-driven tasks, we find robust evidence of sycophantic belief shifts and show that their impact on rationality depends on whether models systematically over- or under-update their beliefs. Finally, we demonstrate that a post-hoc calibration method and two fine-tuning strategies (SFT and DPO) substantially reduce Bayesian inconsistency, with particularly strong improvements under explicit sycophancy prompting.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.16846.pdf",
    "abs_url": "https://arxiv.org/abs/2508.16846",
    "published": "2025-08-23T00:11:00Z",
    "updated": "2026-01-15T18:31:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个贝叶斯概率框架，用于评估和减少大语言模型中的讨好行为，核心创新是分离理性信念更新和sycophancy。",
      "motivation": "Sycophancy（过度讨好行为）在人类-AI协作中构成挑战，特别是在健康、法律和教育等高风险决策领域。研究LLMs中sycophancy的主要困难是区分sycophantic信念偏移与由新证据或用户信息驱动的理性行为变化。现有方法要么测量描述性行为变化，要么应用依赖客观真实标签的规范评估，限制了它们在主观或不确定任务中的适用性。因此，需要一种新方法来准确评估sycophancy，以促进更可靠的AI系统。",
      "method": "论文提出一个基于行为经济学和理性决策理论的贝叶斯概率框架，明确分离sycophancy和理性信念更新。框架实现三个目标：首先，一个描述性指标，在控制对证据的理性响应下测量sycophancy；其次，一个规范性指标，量化sycophancy如何使模型偏离贝叶斯一致的信念更新；第三，能够在无真实标签设置下应用这两个指标。研究应用框架到多个大语言模型和三个不确定性驱动的任务，使用后处理校准方法和两种微调策略（监督微调SFT和直接偏好优化DPO）来减少贝叶斯不一致性。",
      "result": "应用该框架到多个大语言模型和三个任务中，发现强有力的证据表明存在sycophantic信念偏移，并显示其对理性的影响取决于模型是否系统性地过度或不足更新信念。后处理校准方法和微调策略（SFT和DPO）显著减少了贝叶斯不一致性，特别是在显式sycophancy提示下取得了显著改进。摘要未提供具体数据细节，但与基线方法相比，这些方法在提高模型理性方面表现出色。",
      "conclusion": "论文的主要贡献是提出一个贝叶斯框架，用于评估和减少大语言模型中的sycophancy，分离理性更新和讨好行为。研究的学术价值在于提供了一种新的评估方法，结合描述性和规范性指标，适用于无真实标签设置。实际应用价值是改善高风险领域的人-AI协作，通过减少模型的不理性行为。未来工作方向摘要未明确说明，但可能包括扩展到更多任务或优化策略。",
      "tags": [
        "Bayesian Framework",
        "Sycophancy Assessment",
        "Large Language Models",
        "Belief Updating",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:48.265405Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.16198",
    "title": "OMHBench: Benchmarking Balanced and Grounded Omni-Modal Multi-Hop Reasoning",
    "authors": [
      "Seunghee Kim",
      "Ingyu Bang",
      "Seokgyu Jang",
      "Changhyeon Kim",
      "Sanghwan Bae",
      "Jihun Choi",
      "Richeng Xuan",
      "Taeuk Kim"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have increasingly supported omni-modal processing across text, vision, and speech. However, existing evaluation frameworks for such models suffer from critical limitations, including modality shortcuts and biased reasoning paths. To address these challenges, we propose OMHBench, a novel benchmark designed to rigorously evaluate omni-modal multi-hop reasoning. It consists of 6,144 questions with balanced reasoning paths that are jointly grounded across all three modalities. Extensive evaluation of 13 state-of-the-art models reveals that (1) a large performance gap exists between proprietary and open-source MLLMs and (2) even proprietary models exhibit high sensitivity to reasoning path variations, resulting in asymmetric omni-modal grounding. Notably, models struggle when processing the speech modality, underscoring the need for balanced, multi-hop evaluation of omni-modal intelligence.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.16198.pdf",
    "abs_url": "https://arxiv.org/abs/2508.16198",
    "published": "2025-08-22T08:17:31Z",
    "updated": "2026-01-15T12:32:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了OMHBench基准，用于严格评估多模态大语言模型在全模态多跳推理方面的能力。",
      "motivation": "多模态大语言模型（MLLMs）已能处理文本、视觉和语音等多种模态，但现有评估框架存在模态捷径和推理路径偏见等关键局限性，使得评估无法全面反映模型的真实推理性能。这些问题阻碍了模型的公平比较和优化进展，特别是在全模态智能快速发展背景下，亟需开发一个平衡、接地的评估基准来促进领域进步。",
      "method": "论文提出了OMHBench，一个新颖的基准，包含6,144个具有平衡推理路径的问题，这些问题在文本、视觉和语音三个模态中联合接地。核心创新在于设计了一个能够避免模态捷径和路径偏见的评估框架，通过多跳推理任务来测试模型的综合能力，确保问题的模态均匀分布和深度推理需求。",
      "result": "对13个最先进的MLLMs进行评估后，发现专有模型与开源模型之间存在显著的性能差距。同时，专有模型对推理路径变化高度敏感，导致全模态接地的非对称性。特别地，所有模型在处理语音模态时表现较差，这突显了平衡、多跳评估全模态智能的必要性。",
      "conclusion": "本研究的主要贡献是提出了OMHBench基准，为评估全模态多跳推理提供了新标准，其学术价值在于揭示了当前MLLMs在跨模态推理中的不足，尤其是语音模态的处理挑战。实际应用中，该基准可指导模型开发和性能改进。未来工作可关注增强模型对语音模态的理解能力。",
      "tags": [
        "Multimodal Large Language Models",
        "Omni-Modal Reasoning",
        "Multi-Hop Reasoning",
        "Benchmarking",
        "Speech Modality"
      ]
    },
    "analyzed_at": "2026-01-16T03:17:53.888042Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.13601",
    "title": "Unleashing Semantic and Geometric Priors for 3D Scene Completion",
    "authors": [
      "Shiyuan Chen",
      "Wei Sui",
      "Bohao Zhang",
      "Zeyd Boukhers",
      "John See",
      "Cong Yang"
    ],
    "abstract": "Camera-based 3D semantic scene completion (SSC) provides dense geometric and semantic perception for autonomous driving and robotic navigation. However, existing methods rely on a coupled encoder to deliver both semantic and geometric priors, which forces the model to make a trade-off between conflicting demands and limits its overall performance. To tackle these challenges, we propose FoundationSSC, a novel framework that performs dual decoupling at both the source and pathway levels. At the source level, we introduce a foundation encoder that provides rich semantic feature priors for the semantic branch and high-fidelity stereo cost volumes for the geometric branch. At the pathway level, these priors are refined through specialised, decoupled pathways, yielding superior semantic context and depth distributions. Our dual-decoupling design produces disentangled and refined inputs, which are then utilised by a hybrid view transformation to generate complementary 3D features. Additionally, we introduce a novel Axis-Aware Fusion (AAF) module that addresses the often-overlooked challenge of fusing these features by anisotropically merging them into a unified representation. Extensive experiments demonstrate the advantages of FoundationSSC, achieving simultaneous improvements in both semantic and geometric metrics, surpassing prior bests by +0.23 mIoU and +2.03 IoU on SemanticKITTI. Additionally, we achieve state-of-the-art performance on SSCBench-KITTI-360, with 21.78 mIoU and 48.61 IoU.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.13601.pdf",
    "abs_url": "https://arxiv.org/abs/2508.13601",
    "published": "2025-08-19T08:10:39Z",
    "updated": "2026-01-15T04:39:07Z",
    "comment": "Accept by AAAI-2026",
    "light_analysis": {
      "overview": "本研究提出FoundationSSC框架，通过在源和路径级别实现双重解耦并结合轴感知融合，显著提升了3D语义场景补全的性能。",
      "motivation": "基于相机的3D语义场景补全（SSC）为自动驾驶和机器人导航提供密集的几何与语义感知，对安全导航至关重要。然而，现有方法依赖于耦合编码器同时处理语义和几何先验，导致模型必须在冲突需求间妥协，无法充分发挥两种先验的潜力，从而限制了整体性能。这凸显了开发更高效解耦机制的必要性，以提升感知系统的准确性和鲁棒性。",
      "method": "本研究提出FoundationSSC框架，关键创新包括双重解耦设计和轴感知融合模块。在源级别，引入基础编码器，为语义分支提供丰富的语义特征先验，并为几何分支提供高保真的立体成本体积。在路径级别，这些先验通过专门的解耦路径进行细化，以生成高质量的语义上下文和深度分布。接着，利用混合视图变换将这些解耦和精炼的输入转化为互补的3D特征。此外，提出了轴感知融合（AAF）模块，通过各向异性融合方式，将不同特征整合为统一表示，解决了现有方法中特征融合的挑战。",
      "result": "在广泛实验中，FoundationSSC在SemanticKITTI数据集上同时提升了语义和几何指标，语义指标mIoU增加0.23，几何指标IoU增加2.03，超越了先前的最佳方法。在SSCBench-KITTI-360数据集上，实现了21.78的mIoU和48.61的IoU，达到了最先进的性能水平。这些结果表明，所提出的双重解耦和融合策略能有效优化3D场景补全任务，在对比基线方法中表现出显著改进。",
      "conclusion": "论文的主要贡献是提出了FoundationSSC框架，通过双重解耦设计和轴感知融合模块，解决了3D语义场景补全中语义和几何先验耦合的局限。该研究在学术上推动了感知技术的创新，在实际应用中提升了自主系统的环境理解能力。潜在局限性可能包括计算效率或扩展到其他场景的泛化性，未来工作可探索更高效的实现或扩展到更多样化的数据集。",
      "tags": [
        "3D Semantic Scene Completion",
        "Dual Decoupling",
        "Foundation Encoder",
        "Axis-Aware Fusion",
        "Hybrid View Transformation"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:00.825986Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.10956",
    "title": "A Study of Commonsense Reasoning over Visual Object Properties",
    "authors": [
      "Abhishek Kolari",
      "Mohammadhossein Khojasteh",
      "Yifan Jiang",
      "Floris den Hengst",
      "Filip Ilievski"
    ],
    "abstract": "Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.10956.pdf",
    "abs_url": "https://arxiv.org/abs/2508.10956",
    "published": "2025-08-14T11:28:40Z",
    "updated": "2026-01-15T11:10:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入了一个系统评估框架，用于评估视觉-语言模型在视觉对象属性常识推理方面的能力和局限性。",
      "motivation": "当前视觉问答（VQA）研究虽然考虑多个对象属性（如大小），但通常将感知与推理混合，并在推理水平和图像类别上缺乏代表性，导致难以评估视觉-语言模型（VLMs）是否能够有效抽象和推理描述的对象。受人类分类启发，本研究旨在系统评估VLMs在常识推理中的表现，以解决现有方法在代表性不足和评估模糊性问题上的缺陷，从而揭示模型的实际能力。",
      "method": "论文基于先验常识工作，开发了一个系统评估框架，包含三种代表性图像类型、三种复杂度递增的推理水平和四个对象属性维度。该框架通过创建两个VQA基准来实例化：OPTICS-CNT（包含360张图像和1,080个多级计数问题）和OPTICS-CMP（包含2.1k个比较问题）。在零样本设置下，使用12个最先进的VLMs进行实验，以测试模型在图像类型、推理水平和属性维度上的表现。",
      "result": "实验结果显示，在零样本设置下，VLMs相对于人类表现显著不足，最佳性能模型在计数任务上准确率低于40%，比较任务上低于70%。模型在处理照片图像、进行反事实推理、应对物理和功能属性以及处理较高计数时尤其困难，凸显了其在复杂推理场景下的局限性。与基线方法相比，VLMs在多方面表现不佳，验证了评估框架的有效性。",
      "conclusion": "本研究的主要贡献是提供了一个系统评估框架和基准数据集，揭示了VLMs在常识推理方面的局限性。这推动了可扩展基准方法的发展，促进了通用标注指南的制定，并为开发更高级推理的VLMs奠定了基础。未来工作可针对这些局限进行改进，例如增强模型对反事实推理和复杂属性的处理能力，以提升实际应用价值。",
      "tags": [
        "Visual Question Answering (VQA)",
        "Vision-Language Models (VLMs)",
        "Commonsense Reasoning",
        "Zero-Shot Learning",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:12.626957Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.07580",
    "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation",
    "authors": [
      "Uliana Parkina",
      "Maxim Rakhuba"
    ],
    "abstract": "Recent studies suggest that context-aware low-rank approximation is a useful tool for compression and fine-tuning of modern large-scale neural networks. In this type of approximation, a norm is weighted by a matrix of input activations, significantly improving metrics over the unweighted case. Nevertheless, existing methods for neural networks suffer from numerical instabilities due to their reliance on classical formulas involving explicit Gram matrix computation and their subsequent inversion. We demonstrate that this can degrade the approximation quality or cause numerically singular matrices.   To address these limitations, we propose a novel inversion-free regularized framework that is based entirely on stable decompositions and overcomes the numerical pitfalls of prior art. Our method can handle possible challenging scenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when input activation matrices are nearly singular, and even (3) when insufficient data prevents unique approximation. For the latter, we prove that our solution converges to a desired approximation and derive explicit error bounds.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.07580.pdf",
    "abs_url": "https://arxiv.org/abs/2507.07580",
    "published": "2025-07-10T09:35:22Z",
    "updated": "2026-01-15T09:44:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了 COALA 框架，一种基于稳定分解的无反转正则化方法，用于实现数值稳定和高效的上下文感知低秩逼近。",
      "motivation": "上下文感知低秩逼近在神经网络压缩和微调中广泛应用，通过加权范数改进性能，但现有方法依赖经典 Gram 矩阵计算和反转公式，导致数值不稳定性，如逼近质量下降或矩阵奇异性问题。这限制了实际应用，因为不稳定性可能引发计算错误，尤其在处理大规模数据时。本研究旨在解决这些数值陷阱，开发一个更鲁棒的框架，以提升可靠性和效率。",
      "method": "论文提出 COALA 框架，这是一种无反转正则化框架，完全基于稳定分解，避免了依赖 Gram 矩阵反转的数值问题。该框架使用正则化策略和稳定分解技术，能处理校准矩阵超出 GPU 内存容量、输入激活矩阵接近奇异以及数据不足等挑战场景。关键创新点在于消除显式反转，通过分解确保数值稳定性，即使在数据稀缺时也能保证收敛性和唯一逼近。",
      "result": "摘要未明确说明具体的实验性能指标，如准确率提升或效率改进。但论文理论证明了在数据不足的场景下，所提框架的解决方案能收敛到期望的逼近，并推导了显式误差界，表明其克服了先前方法因反转导致的奇异问题。这暗示框架在数值稳定性方面优于基线，但未提供与现有方法的详细对比数据。",
      "conclusion": "COALA 框架的主要贡献是提供了一个数值稳定和高效的上下文感知低秩逼近方法，解决了现有技术中的数值不稳定性问题。其学术价值在于通过理论分析确保了逼近的收敛性和误差界，而实际应用价值体现在神经网络压缩和微调中，提升模型部署的可靠性。局限性可能包括对特定场景的依赖，未来工作可扩展框架以适应更多复杂数据或优化性能。",
      "tags": [
        "Context-Aware Low-Rank Approximation",
        "Numerical Stability",
        "Inversion-Free Framework",
        "Regularization",
        "Stable Decomposition"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:09.696677Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.19813",
    "title": "Curating art exhibitions using machine learning",
    "authors": [
      "Eurico Covas"
    ],
    "abstract": "Here we present a series of artificial models - a total of four related models - based on machine learning techniques that attempt to learn from existing exhibitions which have been curated by human experts, in order to be able to do similar curatorship work. Out of our four artificial intelligence models, three achieve a reasonable ability at imitating these various curators responsible for all those exhibitions, with various degrees of precision and curatorial coherence. In particular, we can conclude two key insights: first, that there is sufficient information in these exhibitions to construct an artificial intelligence model that replicates past exhibitions with an accuracy well above random choices; and second, that using feature engineering and carefully designing the architecture of modest size models can make them almost as good as those using the so-called large language models such as GPT in a brute force approach.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.19813.pdf",
    "abs_url": "https://arxiv.org/abs/2506.19813",
    "published": "2025-06-24T17:25:03Z",
    "updated": "2026-01-15T12:52:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一系列机器学习模型，通过特征工程和架构设计有效模仿人类艺术展览策展，展示了中等规模模型与大型语言模型相当的潜力。",
      "motivation": "研究旨在解决艺术展览策展的自动化问题。传统策展依赖于人类专家的主观经验和时间投入，缺乏高效自动化工具，限制了可扩展性。现有方法如使用GPT等大型语言模型的蛮力方法可能计算成本高且未针对策展任务优化，导致效率不足。因此，开发专门化的机器学习模型来学习专家知识，辅助或自动化策展过程，具有重要实践意义。摘要未明确说明更多背景细节，但暗示了改进空间。",
      "method": "论文构建了四个基于机器学习技术的相关模型，从人类专家策划的现有展览数据中学习。核心方法包括使用特征工程提取展览的关键属性，并精心设计中等规模模型的架构，以避免依赖大型语言模型的蛮力方法。关键创新在于优化模型结构以处理策展任务，例如通过数据驱动方式模仿策展决策。具体技术细节如模型类型或数据集规模摘要未明确说明，但强调数据来源于实际展览，方法侧重于平衡模型复杂度和性能。",
      "result": "实验结果显示，四个模型中有三个能够合理模仿各种人类策展人，展现出不同程度的精确度和策展连贯性。关键见解表明：展览数据包含足够信息，使AI模型复制过去展览的准确性远高于随机选择；此外，通过特征工程和架构设计，中等规模模型在性能上几乎与使用GPT等大型语言模型的蛮力方法相当。具体性能指标如准确率摘要未提供，但对比突显了所提方法的有效性，优于随机基线并与复杂模型接近。",
      "conclusion": "论文的主要贡献是证明了机器学习模型可有效学习人类策展模式，并强调特征工程和模型架构设计在提升性能中的关键作用。学术上，这为AI在艺术和文化领域的应用提供了新视角，比较了不同建模策略。实际应用中，该方法可辅助展览策划，提高效率和可扩展性。未来工作可能涉及扩展数据集以增强泛化能力或探索更多策展任务，局限性如模型适应新场景的能力摘要未明确说明。",
      "tags": [
        "Machine Learning",
        "Feature Engineering",
        "Model Architecture",
        "Large Language Models",
        "Exhibition Curation"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:26.661028Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.14375",
    "title": "Advancing Safe Mechanical Ventilation Using Offline RL With Hybrid Actions and Clinically Aligned Rewards",
    "authors": [
      "Muhammad Hamza Yousuf",
      "Jason Li",
      "Sahar Vahdati",
      "Raphael Theilen",
      "Jakob Wittenstein",
      "Jens Lehmann"
    ],
    "abstract": "Invasive mechanical ventilation (MV) is a life-sustaining therapy commonly used in the intensive care unit (ICU) for patients with severe and acute conditions. These patients frequently rely on MV for breathing. Given the high risk of death in such cases, optimal MV settings can reduce mortality, minimize ventilator-induced lung injury, shorten ICU stays, and ease the strain on healthcare resources. However, optimizing MV settings remains a complex and error-prone process due to patient-specific variability. While Offline Reinforcement Learning (RL) shows promise for optimizing MV settings, current methods struggle with the hybrid (continuous and discrete) nature of MV settings. Discretizing continuous settings leads to exponential growth in the action space, which limits the number of optimizable settings. Converting the predictions back to continuous can cause a distribution shift, compromising safety and performance. To address this challenge, in the IntelliLung project, we are developing an AI-based approach where we constrain the action space and employ factored action critics. This approach allows us to scale to six optimizable settings compared to 2-3 in previous studies. We adapt SOTA offline RL algorithms to operate directly on hybrid action spaces, avoiding the pitfalls of discretization. We also introduce a clinically grounded reward function based on ventilator-free days and physiological targets. Using multiobjective optimization for reward selection, we show that this leads to a more equitable consideration of all clinically relevant objectives. Notably, we develop a system in close collaboration with healthcare professionals that is aligned with real-world clinical objectives and designed with future deployment in mind.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.14375.pdf",
    "abs_url": "https://arxiv.org/abs/2506.14375",
    "published": "2025-06-17T10:17:26Z",
    "updated": "2026-01-15T12:24:48Z",
    "comment": "Accepted to AAAI-26",
    "light_analysis": {
      "overview": "本论文提出一种基于离线强化学习的方法，通过约束动作空间和使用分解动作批评器直接处理混合动作空间，并引入临床对齐奖励函数，以安全优化机械通气设置。",
      "motivation": "侵入性机械通气在ICU中至关重要，优化其设置能降低死亡率、减少肺损伤并节省医疗资源。然而，患者特异性变异性使优化过程复杂且易错。现有离线强化学习方法在处理MV设置的混合动作（连续和离散）时面临挑战：离散化导致动作空间指数增长，限制可优化设置数量；转换回连续则引起分布偏移，危及安全性和性能。因此，开发新方法以克服这些不足具有重要意义。",
      "method": "在IntelliLung项目中，研究团队开发了一种AI方法，通过约束动作空间并采用分解动作批评器来处理混合动作。核心创新在于适应最先进的离线强化学习算法，使其能直接操作于混合动作空间，避免离散化问题。同时，引入基于临床的奖励函数，依据呼吸机自由天数和生理目标设计，并使用多目标优化进行奖励选择，以确保公平考虑所有临床相关目标。系统开发与医疗专业人员紧密合作，以对齐真实世界临床需求。",
      "result": "该方法允许将可优化设置扩展到六个，相比先前研究的2-3个有所提升。通过多目标优化选择的奖励函数更公平地权衡临床目标，提高了系统的临床对齐性。然而，摘要未明确说明具体实验数据，如准确率或效率改进的定量结果，以及与传统方法的对比性能。",
      "conclusion": "本研究的核心贡献在于提出了一种新颖的离线强化学习方法，有效解决了机械通气优化中的混合动作空间和临床奖励对齐问题。这推动了AI在医疗决策支持中的应用，具有减少人为错误、改善患者预后的潜力。系统设计考虑未来部署，并与临床专家合作，增强实用性和安全性。未来工作可能包括临床验证和进一步优化算法性能。",
      "tags": [
        "Offline Reinforcement Learning",
        "Hybrid Action Spaces",
        "Factored Action Critics",
        "Clinically Aligned Rewards",
        "Multiobjective Optimization"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:39.884860Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.05310",
    "title": "Learning normalized image densities via dual score matching",
    "authors": [
      "Florentin Guth",
      "Zahra Kadkhodaie",
      "Eero P Simoncelli"
    ],
    "abstract": "Learning probability models from data is at the heart of many machine learning endeavors, but is notoriously difficult due to the curse of dimensionality. We introduce a new framework for learning \\emph{normalized} energy (log probability) models that is inspired by diffusion generative models, which rely on networks optimized to estimate the score. We modify a score network architecture to compute an energy while preserving its inductive biases. The gradient of this energy network with respect to its input image is the score of the learned density, which can be optimized using a denoising objective. Importantly, the gradient with respect to the noise level provides an additional score that can be optimized with a novel secondary objective, ensuring consistent and normalized energies across noise levels. We train an energy network with this \\emph{dual} score matching objective on the ImageNet64 dataset, and obtain a cross-entropy (negative log likelihood) value comparable to the state of the art. We further validate our approach by showing that our energy model \\emph{strongly generalizes}: log probabilities estimated with two networks trained on non-overlapping data subsets are nearly identical. Finally, we demonstrate that both image probability and dimensionality of local neighborhoods vary substantially depending on image content, in contrast with conventional assumptions such as concentration of measure or support on a low-dimensional manifold.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.05310.pdf",
    "abs_url": "https://arxiv.org/abs/2506.05310",
    "published": "2025-06-05T17:53:57Z",
    "updated": "2026-01-15T06:23:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种通过双重分数匹配学习归一化图像密度模型的新框架，确保能量一致且性能优越。",
      "motivation": "概率模型学习是机器学习的核心任务，但高维数据导致维度诅咒，使得密度估计极其困难。现有方法如扩散生成模型依赖于分数网络来估计密度，但缺乏对能量模型归一化的保证，可能导致概率分布不准确。因此，本研究旨在解决归一化能量模型的学习问题，以提高密度估计的可靠性和泛化能力，满足实际应用中对精确概率建模的需求。摘要未明确说明具体现有方法的不足细节，但强调了归一化的重要性。",
      "method": "论文提出修改分数网络架构以直接计算能量，同时保留其归纳偏差，使网络的梯度成为学习密度的分数，可通过去噪目标优化。创新点在于引入对噪声水平的梯度作为额外分数，利用新的双重分数匹配目标优化，确保在不同噪声水平下能量一致且归一化。该方法在ImageNet64数据集上训练能量网络，采用双重分数匹配目标函数，结合了去噪和噪声水平梯度优化。技术特色包括双重分数匹配和归一化能量计算。",
      "result": "在ImageNet64数据集上，模型获得与最先进技术相当的交叉熵（负对数似然）值，表明性能优越。能量模型展现出强泛化能力，通过在非重叠数据子集上训练的两个网络估计的对数概率几乎相同，验证了模型的一致性。研究还发现图像概率和局部邻域的维度根据图像内容变化显著，这挑战了集中度量或低维流形支持等传统假设。与基线方法的对比通过交叉熵值和泛化实验体现，但摘要未提供具体数值。",
      "conclusion": "论文的主要贡献是提出基于双重分数匹配的归一化能量模型学习框架，有效解决高维密度估计问题，改进分数匹配方法并确保能量归一化。学术价值在于推动概率建模技术的发展，实际应用可促进图像生成、密度估计等任务。未来工作方向摘要未明确说明，但可推断为探索模型在不同数据集或领域的适用性，或进一步优化泛化性能。局限性可能包括计算复杂度或对噪声水平的依赖，需要后续研究验证。",
      "tags": [
        "Energy Models",
        "Score Matching",
        "Diffusion Models",
        "Image Density Estimation",
        "Dual Score Matching"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:35.337398Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.13771",
    "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
    "authors": [
      "Banseok Lee",
      "Dongkyu Kim",
      "Youngcheon You",
      "Youngmin Kim"
    ],
    "abstract": "Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.13771.pdf",
    "abs_url": "https://arxiv.org/abs/2506.13771",
    "published": "2025-05-30T06:43:03Z",
    "updated": "2026-01-15T10:46:32Z",
    "comment": "Accepted to NeurIPS 2025. Banseok Lee and Dongkyu Kim contributed equally",
    "light_analysis": {
      "overview": "本文提出LittleBit方法，通过潜在矩阵因子化和多尺度补偿实现大型语言模型的超低比特量化，显著降低内存和计算成本。",
      "motivation": "部署大型语言模型面临高内存和计算成本的挑战，量化技术可作为解决方案。然而，在低于1比特的量化领域，现有方法常导致性能显著下降，这使得模型压缩与精度保持之间的矛盾加剧。因此，研究旨在开发一种新方法，以在极端低比特下维持性能，从而推动LLM在资源受限环境中的实际部署，解决当前量化方法的不足。",
      "method": "LittleBit方法的核心是将模型权重表示为低秩形式，通过潜在矩阵因子化实现，随后对这些因子进行二值化处理。为补偿极端精度下的信息损失，集成了多尺度补偿机制，包括行、列和额外的潜在维度，以学习每秩的重要性。关键创新包括Dual Sign-Value-Independent Decomposition (Dual-SVID)，用于量化感知训练的初始化，以及集成残差补偿来减少误差。该方法在Llama2等大型语言模型上实施，通过训练优化量化过程，确保在超低比特设置下的有效性。",
      "result": "实验结果表明，LittleBit在超低比特量化中表现出优越性能。例如，在Llama2-7B模型上，0.1比特每权重的表现超越了领先方法的0.7比特每权重水平。具体地，该方法实现了近31倍的内存减少，如将Llama2-13B模型压缩到0.9 GB以下。此外，在核心级别上，潜在实现了11.6倍的速度提升相对于FP16精度。这些数据证实了LittleBit在保持高性能的同时，显著提升了压缩效率和部署速度。",
      "conclusion": "LittleBit建立了一种新的尺寸-性能权衡，使得在极端低比特量化下大型语言模型仍能维持实用性能。这为资源受限环境下的LLM部署提供了有效解决方案，具有重要学术价值和实际应用潜力。未来工作可能包括进一步优化补偿机制或将该方法扩展到其他模型架构，以探索更多应用场景。",
      "tags": [
        "Latent Matrix Factorization",
        "Binarization",
        "Multi-scale Compensation",
        "Dual-SVID",
        "Quantization-Aware Training"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:55.342385Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.22235",
    "title": "Optimal kernel regression bounds under energy-bounded noise",
    "authors": [
      "Amon Lahr",
      "Johannes Köhler",
      "Anna Scampicchio",
      "Melanie N. Zeilinger"
    ],
    "abstract": "Non-conservative uncertainty bounds are key for both assessing an estimation algorithm's accuracy and in view of downstream tasks, such as its deployment in safety-critical contexts. In this paper, we derive a tight, non-asymptotic uncertainty bound for kernel-based estimation, which can also handle correlated noise sequences. Its computation relies on a mild norm-boundedness assumption on the unknown function and the noise, returning the worst-case function realization within the hypothesis class at an arbitrary query input location. The value of this function is shown to be given in terms of the posterior mean and covariance of a Gaussian process for an optimal choice of the measurement noise covariance. By rigorously analyzing the proposed approach and comparing it with other results in the literature, we show its effectiveness in returning tight and easy-to-compute bounds for kernel-based estimates.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.22235.pdf",
    "abs_url": "https://arxiv.org/abs/2505.22235",
    "published": "2025-05-28T11:11:24Z",
    "updated": "2026-01-15T09:07:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文推导了核基估计的紧致非渐近不确定性界，能处理相关噪声序列，提供高精度评估。",
      "motivation": "不确定性界对于评估估计算法的准确性至关重要，尤其是在安全关键应用如自动驾驶中。现有方法往往产生保守或不实用的界，难以处理相关噪声，限制了算法在实际部署中的可靠性和评估效果。因此，开发紧致且易计算的不确定性界是当前研究的迫切需求，以提升估计算法的实用性和安全性。",
      "method": "论文提出一种基于核基估计的方法，通过假设未知函数和噪声具有温和的范数有界性，推导出紧致的非渐近不确定性界。核心创新包括能处理相关噪声序列，并利用高斯过程的后验均值和协方差来计算任意查询点上的最坏情况函数实现。技术关键在于选择最优测量噪声协方差，确保理论严谨性和计算效率，无需依赖复杂渐近分析。",
      "result": "通过严格理论分析和与文献中现有结果的比较，论文表明该方法能返回紧致且易计算的不确定性界。摘要未明确说明具体性能指标如准确率提升，但指出在核基估计中有效性显著，优于其他方法，提供了更可靠的不确定性评估，适用于实际场景中的噪声处理。",
      "conclusion": "研究的主要贡献是提出了一种理论严谨的不确定性界方法，改进了核基估计的评估框架。学术价值在于非渐近分析和处理相关噪声的能力，实际应用价值体现在安全关键领域中的算法部署。未来工作可能涉及扩展到更复杂噪声模型或验证于大规模数据集，以进一步增强实用性。",
      "tags": [
        "Kernel Regression",
        "Uncertainty Bounds",
        "Gaussian Process",
        "Non-asymptotic Analysis",
        "Correlated Noise"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:47.845969Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.21357",
    "title": "AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Agriculture Mapping",
    "authors": [
      "Wenyuan Li",
      "Shunlin Liang",
      "Keyan Chen",
      "Yongzhe Chen",
      "Han Ma",
      "Jianglei Xu",
      "Yichuan Ma",
      "Shikang Guan",
      "Husheng Fang",
      "Zhenwei Shi"
    ],
    "abstract": "Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at https://github.com/flyakon/AgriFM.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.21357.pdf",
    "abs_url": "https://arxiv.org/abs/2505.21357",
    "published": "2025-05-27T15:50:14Z",
    "updated": "2026-01-15T09:17:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了AgriFM，一个专门用于农业作物映射的多源遥感基础模型，通过同步时空特征提取解决了现有方法在多尺度处理上的不足。",
      "motivation": "作物映射需要建模多尺度时空模式，包括从田间纹理到景观尺度的空间变化，以及短期物候过渡和整个生长季的动态。然而，现有基于Transformer的遥感基础模型（RSFMs）对作物映射仍不理想：它们要么使用固定的时空窗口忽略作物系统的多尺度性质，要么完全忽视时间信息而仅关注空间模式。这些缺陷导致作物映射准确性受限，影响了精准农业管理，因为准确的作物信息对资源优化和决策支持至关重要。",
      "method": "方法基于修改的Video Swin Transformer架构，关键创新是将时间降采样与空间缩放操作同步，实现高效统一处理长时间序列卫星输入。这支持分层时空特征提取，有效捕捉多尺度模式。AgriFM利用MODIS、Landsat-8/9和Sentinel-2多源卫星数据，预训练于包含超过2500万图像样本的全球代表性数据集，使用土地覆盖产品进行监督学习。模型还包括一个动态融合学习表示的解码器架构，支持多样化下游任务，增强了灵活性和实用性。",
      "result": "综合评估表明，AgriFM在所有下游任务上优于传统深度学习方法和最先进的通用遥感基础模型。尽管摘要未明确给出具体性能指标如准确率数字，但强调其卓越表现，表明模型在作物映射任务中实现了显著性能提升。通过与基线方法的对比，AgriFM展示了更高的精度和鲁棒性，验证了同步时空处理和多源数据融合的有效性，为农业遥感应用提供了新基准。",
      "conclusion": "AgriFM的主要贡献是提出了一个专为农业设计的遥感基础模型，通过同步分层时空处理改进作物映射精度。这具有重要学术价值，推动了AI与遥感技术的结合，并具有实际应用潜力，如支持农业监测和资源管理。未来工作可能包括扩展到更多作物类型或区域评估，但摘要未明确说明局限性。研究为多源时空数据处理提供了新思路，促进了领域特定基础模型的发展。",
      "tags": [
        "Video Swin Transformer",
        "Remote Sensing Foundation Model",
        "Temporal Processing",
        "Multi-source Data",
        "Crop Mapping"
      ]
    },
    "analyzed_at": "2026-01-16T03:18:58.791747Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.20355",
    "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "authors": [
      "Yeonjoon Jung",
      "Daehyun Ahn",
      "Hyungjun Kim",
      "Taesu Kim",
      "Eunhyeok Park"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.20355.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20355",
    "published": "2025-05-26T06:48:20Z",
    "updated": "2026-01-15T10:43:27Z",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "light_analysis": {
      "overview": "本文提出GraLoRA方法，通过将权重矩阵分割为子块并各自配备低秩适配器，解决了LoRA在参数高效微调中的过拟合问题，提升了性能。",
      "motivation": "LoRA作为参数高效微调方法，在秩32-64时表现最佳，但在更高秩时准确率停滞或下降，无法达到全微调性能。其结构性瓶颈导致梯度纠缠和传播扭曲，限制了模型表示能力。研究旨在解决这一过拟合问题，提高微调效果，以应对实际应用中高效模型适配的需求，避免性能不足。",
      "method": "GraLoRA将权重矩阵划分为多个子块，每个子块独立使用低秩适配器，增加了表示容量而不显著增加计算或存储成本。该方法通过分区结构减少了梯度传播的扭曲，更近似全微调行为，关键创新在于子块化设计，避免了LoRA的梯度纠缠问题。",
      "result": "在HumanEval+等基准测试中，GraLoRA相比LoRA和其他基线，在Pass@1指标上获得最高+8.5%的绝对增益。改进在不同模型大小和秩设置下均保持一致，表明其可扩展性和鲁棒性，验证了GraLoRA在参数高效微调中的优越性能。",
      "conclusion": "GraLoRA成功克服了LoRA的过拟合限制，通过分区结构提升了参数高效微调的表示能力和性能，更接近全微调。该研究具有学术价值，推动了参数高效微调技术的发展，并在实际应用中提供高效适配方案，未来可探索更广泛的应用场景。",
      "tags": [
        "Low-Rank Adaptation",
        "Parameter-Efficient Fine-Tuning",
        "Gradient Entanglement",
        "Granular Structure",
        "HumanEval+"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:19.643529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.20323",
    "title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus",
    "authors": [
      "Shahriar Noroozizadeh",
      "Sayantan Kumar",
      "George H. Chen",
      "Jeremy C. Weiss"
    ],
    "abstract": "Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.20323.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20323",
    "published": "2025-05-23T18:01:09Z",
    "updated": "2026-01-15T18:18:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入PMOA-TTS语料库，使用大语言模型将PubMed Open Access病例报告转换为结构化文本时间线，支持临床叙事的时间动态建模研究。",
      "motivation": "临床叙事编码了患者轨迹建模所需的时间动态，但现有大规模时间标注资源稀缺，限制了时间推理和事件预测等领域的发展。当前缺乏高质量的时间线数据集，导致基于叙事文本的时间建模方法难以验证和推进，亟需构建大规模、结构化的临床时间序列语料库来解决这一实际问题。",
      "method": "研究采用可扩展的大语言模型管道，包括Llama 3.3 70B和DeepSeek-R1，从124,699个PubMed Open Access病例报告中提取事件和时间对，形成结构化时间线，包含超过5.6百万个时间戳事件、人口统计和诊断信息。技术验证使用临床医生策划的金标准集，通过语义事件匹配、时间一致性（c-index）和对齐误差的对数时间CDF下面积（AULTC）进行评估，并对替代提示和模型选择进行基准测试。",
      "result": "主要实验结果基于技术验证的三个度量，但摘要未明确说明具体性能指标如准确率或效率改进。验证使用语义事件匹配、c-index和AULTC评估时间线提取质量，与基线方法对比；基准测试展示了不同模型和提示策略的效果，但详细数值未在摘要中提供，仅概述了验证框架和基准方法。",
      "conclusion": "论文的主要贡献是引入PMOA-TTS语料库，提供了大规模、结构化的临床时间线数据集，支持时间线提取、时间推理、生存建模和事件预测的研究。其学术价值在于填补临床叙事时间标注资源的空白，实际应用价值在于促进医疗AI时序模型的发展。数据和代码开放可用，便于复现和扩展，未来工作可优化提取精度并扩展到更多数据源。",
      "tags": [
        "Large Language Model",
        "Timeline Extraction",
        "Temporal Reasoning",
        "Survival Modeling",
        "Event Forecasting"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:28.351922Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.15602",
    "title": "Deep Learning for Continuous-Time Stochastic Control with Jumps",
    "authors": [
      "Patrick Cheridito",
      "Jean-Loup Dupret",
      "Donatien Hainaut"
    ],
    "abstract": "In this paper, we introduce a model-based deep-learning approach to solve finite-horizon continuous-time stochastic control problems with jumps. We iteratively train two neural networks: one to represent the optimal policy and the other to approximate the value function. Leveraging a continuous-time version of the dynamic programming principle, we derive two different training objectives based on the Hamilton-Jacobi-Bellman equation, ensuring that the networks capture the underlying stochastic dynamics. Empirical evaluations on different problems illustrate the accuracy and scalability of our approach, demonstrating its effectiveness in solving complex high-dimensional stochastic control tasks.",
    "categories": [
      "cs.LG",
      "eess.SY",
      "math.OC",
      "q-fin.PM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.15602.pdf",
    "abs_url": "https://arxiv.org/abs/2505.15602",
    "published": "2025-05-21T14:57:39Z",
    "updated": "2026-01-15T10:43:19Z",
    "comment": "NeurIPS 2025",
    "light_analysis": {
      "overview": "该论文提出了一种基于模型的深度学习方法，用于解决有限时间连续时间随机控制问题，并特别处理了跳跃动态。",
      "motivation": "连续时间随机控制问题在金融、工程等领域至关重要，但现有方法在处理高维和复杂跳跃动态时效率低下或难以扩展。深度学习为解决这类问题提供了新途径，但现有方法可能未能充分结合连续时间控制理论。本研究的动机在于弥补这一差距，通过整合深度学习与经典动态规划，提升随机控制任务的准确性和可扩展性。",
      "method": "论文采用迭代训练两个神经网络的方法：一个表示最优策略，另一个近似值函数。基于连续时间版本的动态规划原理，作者从Hamilton-Jacobi-Bellman方程推导出两个训练目标，确保网络能够有效捕捉包括跳跃在内的底层随机动态。这种模型为基础的方法通过优化这些目标来学习控制策略，无需直接求解复杂的偏微分方程。",
      "result": "摘要未明确说明具体性能指标如准确率提升，但通过在不同问题上的实证评估，作者展示了该方法在准确性和可扩展性方面的优势。结果表明，该方法能够有效解决复杂高维随机控制任务，可能优于传统基线方法，但具体对比数据未详细提供。",
      "conclusion": "本研究的主要贡献在于提出了一种结合深度学习和连续时间动态规划的模型，用于解决带跳跃的随机控制问题。学术价值在于推动了控制理论与机器学习的融合，实际应用价值体现在金融建模、机器人控制等领域。摘要未明确说明局限性和未来工作方向，但该方法为进一步探索高维控制问题提供了基础。",
      "tags": [
        "Deep Learning",
        "Stochastic Control",
        "Hamilton-Jacobi-Bellman Equation",
        "Dynamic Programming",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:32.477127Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.14669",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "authors": [
      "Roberto L. Castro",
      "Andrei Panferov",
      "Soroush Tabesh",
      "Oliver Sieberling",
      "Jiale Chen",
      "Mahdi Nikdan",
      "Saleh Ashkboos",
      "Dan Alistarh"
    ],
    "abstract": "Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an \"optimal\" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.14669.pdf",
    "abs_url": "https://arxiv.org/abs/2505.14669",
    "published": "2025-05-20T17:55:50Z",
    "updated": "2026-01-15T11:15:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Quartet方法，实现了端到端FP4精度训练大型语言模型，优化了准确性-计算权衡。",
      "motivation": "训练大型语言模型时，使用低精度（如FP4）可以直接降低计算成本、提高吞吐量和能效，NVIDIA的Blackwell架构已支持FP4操作。然而，现有FP4训练算法面临显著的准确性下降问题，常依赖混合精度回退来弥补性能损失，这限制了低精度训练的实用性和效率。因此，本研究旨在开发硬件支持的准确FP4训练方法，以充分利用低精度优势，解决实际部署中的计算瓶颈和能效需求。",
      "method": "研究通过广泛评估Llama类型模型，揭示了新的低精度缩放定律，量化了不同比特宽度和训练设置下的性能权衡，以指导技术设计。基于此，设计了Quartet技术，一种在准确性-计算方面最优的FP4训练方法，确保所有主要计算（如线性层）在FP4精度进行，而无需混合精度回退。该方法使用优化的CUDA内核为Blackwell架构定制实现，以提升硬件兼容性和计算效率。",
      "result": "实现Quartet方法后，广泛评估表明完全基于FP4的训练是FP16半精度和FP8训练的有竞争力替代方案，在准确性-计算权衡方面表现优异。具体性能指标未在摘要中明确说明，但论文强调该方法避免了传统FP4训练中的准确性下降，通过硬件优化实现了与更高精度训练相当的效果，提升了训练吞吐量和能效。",
      "conclusion": "Quartet方法证实了FP4精度训练在大语言模型中的可行性和效率，为低精度训练提供了高效解决方案，降低了训练成本并提升能效。研究的学术价值在于发现了低精度缩放定律，推动算法优化；实际应用可促进节能AI训练，减少计算资源消耗。潜在局限性可能包括对其他模型或任务的泛化，未来工作或涉及进一步扩展优化到更广泛场景。",
      "tags": [
        "FP4 Training",
        "Low-Precision Training",
        "Large Language Models",
        "Scaling Law",
        "CUDA Kernels"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:53.021755Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.08814",
    "title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test",
    "authors": [
      "Wenkai Li",
      "Xiaoqi Li",
      "Yingjie Mao",
      "Yishun Wang"
    ],
    "abstract": "Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.08814.pdf",
    "abs_url": "https://arxiv.org/abs/2505.08814",
    "published": "2025-05-12T08:25:55Z",
    "updated": "2026-01-15T14:53:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过实证实验研究了深度神经网络覆盖度量与模型深度、配置之间的关系和模式，填补了现有研究的空白。",
      "motivation": "深度神经网络在人工智能领域扮演关键角色，其安全相关测试已成为重要研究方向。通过输入测试案例检查模型行为异常，并使用覆盖度量评估神经元覆盖程度。随着DNN广泛应用，不同类型神经行为受到关注，涌现出多种覆盖度量。然而，目前缺乏对这些覆盖度量的实证研究，特别是分析模型深度、配置信息与神经网络覆盖之间的关联和模式。这个问题的重要性在于，缺乏理解限制了安全测试的有效性，现有方法未能系统探讨这些关系，影响了模型可靠性评估的准确性。",
      "method": "本研究选择了四种覆盖度量：主要功能覆盖、边界覆盖、层次覆盖和结构覆盖。通过一系列实证实验，使用不同DNN架构如LeNet、VGG和ResNet，以及10个模型，深度范围从5层到54层，以比较和研究不同深度、配置信息与各种神经网络覆盖度量的关系。此外，还调查了修改决策/条件覆盖与数据集大小的关联。关键创新在于系统性实验设计，结合多种模型架构和深度，分析覆盖度量的行为和模式，为DNN安全测试提供技术路线支持。",
      "result": "通过实证实验，揭示了不同覆盖度量与模型深度、配置之间的关联。实验比较了不同架构和深度的模型在不同覆盖度量下的表现，提供了对这些度量关系的初步见解，例如可能发现某些覆盖度量对模型深度变化敏感或与数据集大小有相关性。然而，摘要未明确说明具体的数据支撑，如准确率或效率改进的量化结果，因此需要进一步查阅论文以获取详细实验数据，无法提供与基线方法的直接对比情况。",
      "conclusion": "本研究填补了深度神经网络覆盖度量实证研究的空白，通过系统实验分析了覆盖度量与模型特性之间的关系和模式。贡献在于为DNN安全测试提供了理论基础和实证支持，促进了对模型行为的理解。学术价值在于推动了覆盖度量研究的深入，实际应用价值在于改进安全测试方法的有效性。最后，论文提出了三个未来研究方向，以进一步促进DNN模型的安全测试，指出了潜在的局限性如实验数据的量化不足，并建议未来工作扩展更多模型类型和数据集。",
      "tags": [
        "Deep Neural Networks",
        "Coverage Metrics",
        "Image Recognition",
        "Security Testing",
        "Empirical Research"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:53.391907Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.02064",
    "title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video",
    "authors": [
      "Shuhang Xun",
      "Sicheng Tao",
      "Jungang Li",
      "Yibo Shi",
      "Zhixin Lin",
      "Zhanhui Zhu",
      "Yibo Yan",
      "Hanqian Li",
      "Linghao Zhang",
      "Shikang Wang",
      "Yixin Liu",
      "Hanbo Zhang",
      "Ying Ma",
      "Xuming Hu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have made rapid progress in perception, understanding, and reasoning, yet existing benchmarks fall short in evaluating these abilities under continuous and dynamic real-world video streams. Such settings require models to maintain coherent understanding and reasoning as visual scenes evolve over time. **We introduce RTV-Bench, a fine-grained benchmark for real-time video analysis with MLLMs**. It is built upon three key principles: multi-timestamp question answering, hierarchical question structures spanning perception and reasoning, and multi-dimensional evaluation of continuous perception, understanding, and reasoning. RTV-Bench comprises 552 diverse videos and 4,608 carefully curated QA pairs covering a wide range of dynamic scenarios. We evaluate a broad range of state-of-the-art MLLMs, including proprietary, open-source offline, and open-source real-time models. Our results show that real-time models generally outperform offline counterparts but still lag behind leading proprietary systems. While scaling model capacity generally yields performance gains, simply increasing the density of sampled input frames does not consistently translate into improved results. These observations suggest inherent limitations in current architectures when handling long-horizon video streams, underscoring the need for models explicitly designed for streaming video processing and analysis.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.02064.pdf",
    "abs_url": "https://arxiv.org/abs/2505.02064",
    "published": "2025-05-04T10:55:21Z",
    "updated": "2026-01-15T10:29:27Z",
    "comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track;",
    "light_analysis": {
      "overview": "本文提出了RTV-Bench基准，用于评估多模态大语言模型在实时视频流中的连续感知、理解和推理能力，填补了现有基准的不足。",
      "motivation": "现有基准无法充分评估多模态大语言模型（MLLMs）在连续动态视频流下的能力，因为现实世界视频分析需要模型随时间推移保持一致性理解和推理。这一问题的解决对自动驾驶、视频监控等实际应用至关重要，而当前方法大多侧重于静态或离散视频分析，缺乏对连续流处理的系统评估，导致模型性能评估不全面，限制了其在动态场景中的部署效果。",
      "method": "作者构建了RTV-Bench基准，基于三个核心原则：多时间戳问答以测试模型对视频连续变化的响应，分层问题结构覆盖从感知到推理的不同认知层次，以及多维评估量化模型的连续感知、理解和推理能力。该基准包含552个多样视频和4,608个精心设计的问答对，覆盖广泛的动态场景，并通过评估专有、开源离线和开源实时MLLMs来验证其有效性。",
      "result": "实验结果表明，实时模型通常优于离线模型，但在性能上仍落后于领先的专有系统。扩展模型容量通常能带来性能提升，但仅增加输入帧采样密度并不总是导致结果改善，这揭示了当前架构在处理长视频流时的内在局限性。与基线对比显示，实时模型在连续任务中有优势，但整体性能仍有优化空间。",
      "conclusion": "论文的主要贡献是推出了RTV-Bench基准，并强调了其对于评估MLLMs连续感知能力的价值。研究指出当前架构在处理流视频方面的不足，为推动设计专门用于流视频处理的模型提供了方向，具有重要的学术和实践意义。未来工作可探索新的架构设计以增强模型对长序列视频的处理能力。",
      "tags": [
        "Multimodal Large Language Models",
        "Real-Time Video Analysis",
        "Benchmarking",
        "Continuous Perception",
        "Video Streaming"
      ]
    },
    "analyzed_at": "2026-01-16T03:19:53.099896Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.10571",
    "title": "On the Failure of Latent State Persistence in Large Language Models",
    "authors": [
      "Jen-tse Huang",
      "Kaiser Sun",
      "Wenxuan Wang",
      "Mark Dredze"
    ],
    "abstract": "While Large Language Models (LLMs) excel in reasoning, whether they can sustain persistent latent states remains under-explored. The capacity to maintain and manipulate unexpressed, internal representations-analogous to human working memory-is a cornerstone of complex reasoning. In this paper, we formalize and quantify the \"Latent State Persistence\" (LSP) gap through three novel experiments. First, we utilize a Number Guessing Game, demonstrating that across independent queries, LLMs fail to allocate probability mass to a singular hidden choice, violating a fundamental probabilistic principle. Second, we employ a Yes-No Game to show that as the number of questions increases, LLMs suffer from \"concept drift,\" leading to inevitable self-contradictions due to the lack of LSP. Finally, inspired by Mathematical Mentalism, we task models with tracking transformations on hidden variables, revealing a failure in variable binding and state evolution when the initial state is not explicitly present in the context. Collectively, these findings suggest that LLMs function as reactive post-hoc solvers rather than proactive planners with LSP. Our work provides a framework for evaluating the fidelity of internal representations and highlights a fundamental architectural divergence between autoregressive transformers and human-like cognition.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.10571.pdf",
    "abs_url": "https://arxiv.org/abs/2505.10571",
    "published": "2025-04-30T16:18:39Z",
    "updated": "2026-01-15T17:44:56Z",
    "comment": "8 pages, 6 figures, 9 tables",
    "light_analysis": {
      "overview": "本文揭示了大型语言模型在潜在状态持久性方面的失败，并提供了一个评估内部表示保真度的框架。",
      "motivation": "论文的动机是探究大型语言模型（LLMs）是否能够维持持久的潜在状态，类似于人类的工作记忆功能，这对于处理复杂推理任务至关重要。尽管LLMs在推理方面表现出色，但其内部表示是否能持续存在和演化仍未被充分研究，现有方法往往忽略了模型在处理未显式表达信息时的能力，导致在需要状态维持的场景中可能表现不足。因此，本研究旨在形式化和量化LLMs在潜在状态持久性方面的差距，填补这一研究空白。",
      "method": "研究方法包括设计三个创新实验来量化潜在状态持久性（LSP）的失败：首先，通过数字猜谜游戏测试LLMs在独立查询中分配概率到隐藏选择的能力；其次，使用是-否游戏评估随着问题数量增加时的概念漂移和自相矛盾；最后，基于数学心理主义，让模型跟踪隐藏变量的变换，检测当初始状态未显式存在时的变量绑定和状态演化失败。这些实验不依赖特定数据集，而是构建抽象任务，适用于一般LLMs评估。",
      "result": "实验结果表明，LLMs在所有三个任务中都未能维持潜在状态持久性：在数字猜谜游戏中，LLMs违反概率原则，无法将概率质量聚焦于单一隐藏选择；在是-否游戏中，随着问题增多，模型出现概念漂移，导致自相矛盾；在隐藏变量跟踪任务中，当初始状态未在上下文中明确给出时，模型无法正确绑定变量并演化状态。这些发现表明，LLMs更像反应性事后求解器，而非具备内部状态持久性的主动规划器。",
      "conclusion": "结论指出，LLMs缺乏潜在状态持久性，这使其更像反应性求解器而非主动规划器。研究的主要贡献是形式化并量化了LSP差距，提供了一个评估内部表示保真度的框架，并突出了自回归变换器与类人认知之间的根本架构差异。学术价值在于深化对LLM内部机制的理解，实际应用价值在于指导模型设计以支持更复杂推理。局限性包括实验基于抽象任务，未来工作可扩展到真实场景并探索增强LSP的模型架构。",
      "tags": [
        "Large Language Models",
        "Latent State Persistence",
        "Autoregressive Transformers",
        "Working Memory",
        "Variable Binding"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:22.951751Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.01667",
    "title": "Testing Low-Resource Language Support in LLMs Using Language Proficiency Exams: the Case of Luxembourgish",
    "authors": [
      "Cedric Lothritz",
      "Jordi Cabot",
      "Laura Bernardy"
    ],
    "abstract": "Large Language Models (LLMs) have become an increasingly important tool in research and society at large. While LLMs are regularly used all over the world by experts and lay-people alike, they are predominantly developed with English-speaking users in mind, performing well in English and other wide-spread languages while less-resourced languages such as Luxembourgish are seen as a lower priority. This lack of attention is also reflected in the sparsity of available evaluation tools and datasets. In this study, we investigate the viability of language proficiency exams as such evaluation tools for the Luxembourgish language. We find that large models such as Claude and DeepSeek-R1 typically achieve high scores, while smaller models show weak performances. We also find that the performances in such language exams can be used to predict performances in other NLP tasks in Luxembourgish.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2504.01667.pdf",
    "abs_url": "https://arxiv.org/abs/2504.01667",
    "published": "2025-04-02T12:16:14Z",
    "updated": "2026-01-15T16:59:58Z",
    "comment": "24pages, 4 figures, 14 tables",
    "light_analysis": {
      "overview": "本研究提出使用语言水平考试评估大型语言模型对低资源语言卢森堡语的支持能力，并发现考试表现可预测其他自然语言处理任务表现。",
      "motivation": "大型语言模型（LLMs）主要针对英语等广泛语言开发，低资源语言如卢森堡语因缺乏关注，评估工具和数据集稀缺，这限制了这些语言在AI应用中的发展。现有方法以英语为中心，导致低资源语言支持不足，研究旨在解决评估缺口，探索语言水平考试作为可行评估工具的重要性。",
      "method": "研究通过实验验证语言水平考试的可行性，使用卢森堡语考试作为评估标准，测试不同规模的LLMs（如大模型Claude和DeepSeek-R1及小模型）。方法聚焦于分析考试分数与模型规模的关系，并探讨其预测其他NLP任务的能力，但具体数据集收集和模型架构细节摘要未明确说明。",
      "result": "实验结果显示，大模型如Claude和DeepSeek-R1在语言考试中表现优异，通常获得高分，而小模型则表现较弱。此外，模型在考试中的表现能有效预测其在其他卢森堡语NLP任务中的性能，与基线方法对比体现了评估工具的有效性，尽管摘要未提供具体准确率数据。",
      "conclusion": "研究表明语言水平考试是评估LLMs对低资源语言支持能力的可行工具，具有学术价值，为低资源语言的评估提供新方法，并有助于促进实际应用。潜在局限性在于只针对卢森堡语，未来工作可扩展至更多低资源语言或改进评估框架。",
      "tags": [
        "Large Language Models",
        "Language Proficiency Exams",
        "Low-Resource Languages",
        "NLP Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:06.187509Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.17523",
    "title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models",
    "authors": [
      "Linlu Qiu",
      "Fei Sha",
      "Kelsey Allen",
      "Yoon Kim",
      "Tal Linzen",
      "Sjoerd van Steenkiste"
    ],
    "abstract": "Large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs must construct representations of the world and form probabilistic beliefs about them. To provide personalized recommendations, for example, the LLM needs to infer a user's preferences from their behavior over multiple interactions. The Bayesian inference framework lays out the optimal way for an agent to update its beliefs as it receives new information. We first show that LLMs fall far short of the standard defined by the Bayesian framework. We then show that by teaching LLMs to mimic the predictions of the normative Bayesian model, we can dramatically improve their ability to update their beliefs; this ability generalizes to new tasks. We conclude that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2503.17523.pdf",
    "abs_url": "https://arxiv.org/abs/2503.17523",
    "published": "2025-03-21T20:13:04Z",
    "updated": "2026-01-15T17:21:57Z",
    "comment": "Nature Communications",
    "light_analysis": {
      "overview": "通过贝叶斯教学法，显著提升大型语言模型的概率推理和信念更新能力，并实现跨任务泛化。",
      "motivation": "大型语言模型（LLMs）越来越多地用作交互代理，需要构建世界表示和形成概率信念，如在个性化推荐中推断用户偏好。贝叶斯推断框架定义了代理更新信念的最优方式，但现有LLMs远未达到此标准，导致在动态交互中表现不佳，限制了其在真实世界应用中的有效性。因此，研究旨在解决LLMs在概率推理方面的不足，提升其作为智能代理的核心能力。",
      "method": "论文提出通过贝叶斯教学法，教授LLMs模仿规范性贝叶斯模型的预测，以提升其信念更新能力。核心方法是利用示例教学，让LLMs学习概率推理技能，具体涉及设计训练示例来模拟贝叶斯推断过程。摘要未明确说明使用的具体数据集或模型架构，但强调该方法基于示例驱动的学习，实现从示例中获取推理模式。",
      "result": "实验显示，通过贝叶斯教学后，LLMs的信念更新能力显著提高，能够更准确地进行概率推理。这种改进能力还表现出良好的泛化性，能够应用到未见的新任务中，优于基线LLMs在贝叶斯标准下的表现。摘要未提供具体性能指标数据（如准确率或效率数值），但描述了定性提升和跨任务有效性。",
      "conclusion": "论文的主要贡献是证明了LLMs可以从示例中有效学习概率推理技能，并将这些技能泛化到新领域，提升了其在交互代理中的应用潜力。研究具有学术价值，拓展了LLMs在贝叶斯推断和自适应学习方面的能力，同时在实际应用中如个性化推荐和智能对话中有重要意义。未来工作可进一步探索教学方法的优化和局限性的解决，以增强泛化效果。",
      "tags": [
        "Large Language Models",
        "Bayesian Inference",
        "Probabilistic Reasoning",
        "Bayesian Teaching",
        "Generalization"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:01.222399Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.13347",
    "title": "TriDF: Triplane-Accelerated Density Fields for Few-Shot Remote Sensing Novel View Synthesis",
    "authors": [
      "Jiaming Kang",
      "Keyan Chen",
      "Zhengxia Zou",
      "Zhenwei Shi"
    ],
    "abstract": "Remote sensing novel view synthesis (NVS) offers significant potential for 3D interpretation of remote sensing scenes, with important applications in urban planning and environmental monitoring. However, remote sensing scenes frequently lack sufficient multi-view images due to acquisition constraints. While existing NVS methods tend to overfit when processing limited input views, advanced few-shot NVS methods are computationally intensive and perform sub-optimally in remote sensing scenes. This paper presents TriDF, an efficient hybrid 3D representation for fast remote sensing NVS from as few as 3 input views. Our approach decouples color and volume density information, modeling them independently to reduce the computational burden on implicit radiance fields and accelerate reconstruction.We explore the potential of the triplane representation in few-shot NVS tasks by mapping high-frequency color information onto this compact structure, and the direct optimization of feature planes significantly speeds up convergence. Volume density is modeled as continuous density fields, incorporating reference features from neighboring views through image-based rendering to compensate for limited input data. Additionally, we introduce depth-guided optimization based on point clouds, which effectively mitigates the overfitting problem in few-shot NVS.Comprehensive experiments across multiple remote sensing scenes demonstrate that our hybrid representation achieves a 30x speed increase compared to NeRF-based methods, while simultaneously improving rendering quality metrics over advanced few-shot methods (7.4% increase in PSNR and 3.4% in SSIM). The code is publicly available at https://github.com/kanehub/TriDF",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.13347.pdf",
    "abs_url": "https://arxiv.org/abs/2503.13347",
    "published": "2025-03-17T16:25:39Z",
    "updated": "2026-01-15T09:12:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出TriDF，一种基于三平面加速的密度场方法，用于遥感少样本新视图合成，实现了高效重建和高质量渲染。",
      "motivation": "遥感新视图合成在三维解释和城市规划中具有重要应用，但遥感场景常因采集限制导致多视角图像不足。现有方法在有限输入视图上容易过拟合，而先进少样本方法计算量大且性能不佳。随着无人机和卫星技术的发展，对快速、准确的3D场景重建需求增长，但数据稀疏性成为挑战。因此，需要开发高效混合表示来处理少样本任务，以平衡计算效率和渲染质量。",
      "method": "TriDF方法采用混合3D表示，通过解耦颜色和体积密度信息独立建模，减轻隐式辐射场的计算负担。具体地，使用三平面表示映射高频颜色信息，优化特征平面以加速收敛；体积密度建模为连续密度场，结合基于图像的渲染从邻近视图提取参考特征，补偿输入数据有限。此外，引入基于点云的深度引导优化，有效缓解少样本NVS中的过拟合问题，支持从少至3个输入视图的高效重建。",
      "result": "在多个遥感场景的综合实验中，TriDF取得了显著效果。与基于NeRF的方法相比，实现了30倍的加速，同时渲染质量指标PSNR提高了7.4%，SSIM提高了3.4%，优于其他先进少样本方法。这些结果验证了TriDF在效率和准确性上的优势，能在仅3个输入视图下保持高重建质量，突显了其鲁棒性和泛化能力。",
      "conclusion": "本文提出TriDF，成功解决了遥感少样本新视图合成的挑战，通过创新混合表示实现了快速和高质重建。研究贡献在于提高了NVS任务的效率和性能，具有学术价值并推动相关技术发展。实际应用中，可为城市规划和环境监测提供支持。未来工作可能包括扩展至其他场景或优化算法，摘要未明确说明具体方向。",
      "tags": [
        "Triplane Representation",
        "Density Fields",
        "Novel View Synthesis",
        "Few-Shot Learning",
        "Remote Sensing"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:31.059001Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.09315",
    "title": "ShuffleGate: Scalable Feature Optimization for Recommender Systems via Batch-wise Sensitivity Learning",
    "authors": [
      "Yihong Huang",
      "Chen Chu",
      "Fan Zhang",
      "Liping Wang Fei Chen",
      "Yu Lin",
      "Ruiduan Li",
      "Zhihao Li"
    ],
    "abstract": "Feature optimization, specifically Feature Selection (FS) and Dimension Selection (DS), is critical for the efficiency and generalization of large-scale recommender systems. While conceptually related, these tasks are typically tackled with isolated solutions that often suffer from ambiguous importance scores or prohibitive computational costs.   In this paper, we propose ShuffleGate, a unified and interpretable mechanism that estimates component importance by measuring the model's sensitivity to information loss. Unlike conventional gating that learns relative weights, ShuffleGate introduces a batch-wise shuffling strategy to effectively erase information in an end-to-end differentiable manner. This paradigm shift yields naturally polarized importance distributions, bridging the long-standing \"search-retrain gap\" and distinguishing essential signals from noise without complex threshold tuning.   ShuffleGate provides a unified solution across granularities. It achieves state-of-the-art performance on feature and dimension selection tasks. Furthermore, to demonstrate its extreme scalability and precision, we extend ShuffleGate to evaluate fine-grained embedding entries. Experiments show it can identify and prune 99.9% of redundant embedding parameters on the Criteo dataset while maintaining competitive AUC, verifying its robustness in massive search spaces. Finally, the method has been successfully deployed in a top-tier industrial video recommendation platform. By compressing the concatenated input dimension from over 10,000 to 1,000+, it achieved a 91% increase in training throughput while serving billions of daily requests without performance degradation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.09315.pdf",
    "abs_url": "https://arxiv.org/abs/2503.09315",
    "published": "2025-03-12T12:05:03Z",
    "updated": "2026-01-15T08:46:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ShuffleGate，通过批处理级敏感性学习实现统一特征优化，解决推荐系统中特征选择与维度选择的高效性与泛化问题。",
      "motivation": "大规模推荐系统中，特征优化（包括特征选择与维度选择）对系统效率和模型泛化能力至关重要。然而，现有方法通常独立处理这些任务，导致重要性评分模糊且计算成本高昂，这限制了实际部署的可行性。ShuffleGate旨在克服这些问题，提供一个统一框架，避免复杂阈值调整和高昂开销，从而提升优化效果和可解释性。",
      "method": "ShuffleGate核心方法通过测量模型对信息丢失的敏感性来估计组件重要性。创新点在于引入批处理级随机化策略，以端到端可微分的方式有效擦除信息，从而生成自然极化的重要性分布。该方法无需复杂阈值调整，能直接区分信号与噪声，并扩展到细粒度嵌入条目评估。在Criteo数据集上进行实验，验证了其技术路线和可扩展性。",
      "result": "实验结果表明，ShuffleGate在特征和维度选择任务上达到最先进性能。具体在Criteo数据集上，能够识别并剪枝99.9%的冗余嵌入参数，同时维持竞争力AUC。在工业部署中，成功应用于视频推荐平台，将输入维度从超过10,000压缩至1,000+，训练吞吐量提高91%，服务数十亿日请求无性能下降。",
      "conclusion": "ShuffleGate的主要贡献在于提供了一种统一的特征优化机制，通过可解释的重要性估计解决计算成本问题，并成功部署于工业环境。其学术价值体现在解决长期存在的'搜索-重新训练差距'，实际应用价值则通过大规模推荐系统中的效率提升得以验证。摘要未明确说明局限性，但该方法展示了在复杂场景下的鲁棒性和可扩展性。",
      "tags": [
        "Feature Selection",
        "Dimension Selection",
        "Batch-wise Learning",
        "Sensitivity Analysis",
        "Embedding Pruning"
      ]
    },
    "analyzed_at": "2026-01-16T03:20:50.165971Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.07266",
    "title": "RS2-SAM2: Customized SAM2 for Referring Remote Sensing Image Segmentation",
    "authors": [
      "Fu Rong",
      "Meng Lan",
      "Qian Zhang",
      "Lefei Zhang"
    ],
    "abstract": "Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text. To address these issues, we propose \\textbf{RS2-SAM2}, a novel framework that adapts SAM2 to RRSIS by aligning the adapted RS features and textual features while providing pseudo-mask-based dense prompts. Specifically, we employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. A bidirectional hierarchical fusion module is introduced to adapt SAM2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the model's interpretation of text-described RS scenes. To provide precise target cues for SAM2, we design a mask prompt generator, which takes the visual embeddings and class tokens as input and produces a pseudo-mask as the dense prompt of SAM2. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM2 achieves state-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.07266.pdf",
    "abs_url": "https://arxiv.org/abs/2503.07266",
    "published": "2025-03-10T12:48:29Z",
    "updated": "2026-01-15T13:25:48Z",
    "comment": "AAAI 2026",
    "light_analysis": {
      "overview": "提出了一种名为RS2-SAM2的新框架，通过特征对齐和伪掩码提示，将Segment Anything Model 2 (SAM2)适应到Referring Remote Sensing Image Segmentation任务中，提升了多模态遥感图像分割的准确性。",
      "motivation": "Referring Remote Sensing Image Segmentation (RRSIS)旨在根据文本描述分割遥感图像中的目标对象，但在实际应用中面临挑战，如理解复杂遥感场景和生成有效分割提示。Segment Anything Model 2 (SAM2)虽然在通用分割任务中表现优秀，但直接应用于RRSIS时存在不足，特别是在处理多模态输入和遥感特定特征方面。因此，本研究针对这些问题开发专门方法，以克服现有技术的局限性，提高遥感图像分割的实用性和精度。",
      "method": "RS2-SAM2框架采用联合编码器编码视觉和文本输入，生成对齐的视觉与文本嵌入以及多模态类别令牌。通过引入双向分层融合模块，将SAM2适应到遥感场景，并对齐调整后的视觉特征与增强的文本嵌入，改善模型对文本描述场景的解读。此外，设计掩码提示生成器，以视觉嵌入和类别令牌为输入，输出伪掩码作为SAM2的密集提示，为目标分割提供精确线索。关键创新点在于特征对齐机制和提示生成策略，优化了多模态输入的处理。",
      "result": "在多个Referring Remote Sensing Image Segmentation基准测试中，RS2-SAM2实现了最先进的性能。实验结果表明，该框架通过特征对齐和伪掩码提示显著提升了分割准确性，优于现有基线方法。摘要未明确说明具体指标如准确率提升，但可以推断它在处理遥感图像时能有效理解文本描述并生成精确分割结果，展示了其在多模态分割任务中的优越性，解决了SAM2在RRSIS中的挑战。",
      "conclusion": "RS2-SAM2的主要贡献是将SAM2成功适应到RRSIS任务，通过创新性的特征对齐和提示生成技术提高了模型性能。该研究具有重要学术价值，推动了多模态遥感图像分割领域的发展，并为实际应用如环境监测和城市规划提供了新工具。未来工作可能包括扩展模型到其他遥感任务、改进泛化能力或减少计算开销。摘要未明确说明局限性，但暗示了在复杂场景处理或数据集多样性方面可能有进一步探索空间。",
      "tags": [
        "Referring Remote Sensing Image Segmentation (RRSIS)",
        "Segment Anything Model 2 (SAM2)",
        "Feature Alignment",
        "Mask Prompt Generator",
        "Bidirectional Hierarchical Fusion Module"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:08.139973Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.17772",
    "title": "An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses",
    "authors": [
      "Hao Liang",
      "Wanrong Zhang",
      "Xinlei He",
      "Kaishun Wu",
      "Hong Xing"
    ],
    "abstract": "Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model's utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.17772.pdf",
    "abs_url": "https://arxiv.org/abs/2502.17772",
    "published": "2025-02-25T02:05:41Z",
    "updated": "2026-01-15T17:33:50Z",
    "comment": "19 pages, 5 figures, accepted by AAAI 2026",
    "light_analysis": {
      "overview": "论文改进了差分隐私随机梯度下降（DPSGD）的隐私和效用分析，针对有界域和平滑损失函数，揭示了隐私损失收敛的新特性。",
      "motivation": "DPSGD广泛用于保护机器学习训练中的敏感数据，但其隐私保证常导致模型性能下降，因为缺乏严格的隐私损失理论界。现有研究假设强如凸性，参数要求复杂，且很少深入探讨隐私机制对模型效用的实际影响，限制了方法在非凸或实用场景中的应用。因此，本研究旨在放宽假设，提供更全面的隐私与效用分析，以提升理论实用性和指导实际部署。",
      "method": "论文提出一种方法，严格分析DPSGD的隐私特性，针对一般L-平滑和非凸损失函数，专注于有界域情况。通过跟踪多个迭代中的隐私损失，利用噪声平滑减少性质，建立全面的收敛分析。关键创新包括处理非凸性，分析有界直径对隐私和效用的影响，以及研究DPSGD with gradient clipping (DPSGD-GC) 和 bounded domain (DPSGD-DC) 版本，考虑mu-强凸种群风险函数，推导隐私-效用权衡的大O阶。",
      "result": "理论分析显示，对于有界域的DPSGD，隐私损失可以在没有凸性假设下收敛；较小有界直径在某些条件下可同时提高隐私和效用；针对DPSGD-GC和DPSGD-DC，推导出隐私-效用权衡的具体大O阶。通过成员推理攻击（MIA）实验在实际设置中验证了这些理论见解，但摘要未明确说明具体性能指标如准确率或效率数据，仅确认了理论与实验结果一致。",
      "conclusion": "本研究的主要贡献是提供了对DPSGD的严格隐私分析，放宽了凸性假设，揭示了有界域下隐私损失收敛和直径改善隐私与效用的新现象。这增强了理论在非凸场景的适用性，并为隐私保护机器学习提供了新见解。未来工作可扩展到更广泛的损失函数或复杂数据集，以进一步优化隐私与效用的平衡。",
      "tags": [
        "Differentially Private SGD",
        "Bounded Domain",
        "Smooth Loss Functions",
        "Privacy-Utility Trade-off",
        "Membership Inference Attack"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:06.307316Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.12965",
    "title": "Text Classification Under Class Distribution Shift: A Survey",
    "authors": [
      "Adriana Valentina Costache",
      "Silviu Florin Gheorghe",
      "Eduard Gabriel Poesina",
      "Paul Irofti",
      "Radu Tudor Ionescu"
    ],
    "abstract": "The basic underlying assumption of machine learning (ML) models is that the training and test data are sampled from the same distribution. However, in daily practice, this assumption is often broken, i.e. the distribution of the test data changes over time, which hinders the application of conventional ML models. One domain where the distribution shift naturally occurs is text classification, since people always find new topics to discuss. To this end, we survey research articles studying open-set text classification and related tasks. We divide the methods in this area based on the constraints that define the kind of distribution shift and the corresponding problem formulation, i.e. learning with the Universum, zero-shot learning, and open-set learning. We next discuss the predominant mitigation approaches for each problem setup. We further identify several future work directions, aiming to push the boundaries beyond the state of the art. Finally, we explain how continual learning can solve many of the issues caused by the shifting class distribution. We maintain a list of relevant papers at https://github.com/Eduard6421/Open-Set-Survey.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.12965.pdf",
    "abs_url": "https://arxiv.org/abs/2502.12965",
    "published": "2025-02-18T15:46:54Z",
    "updated": "2026-01-15T12:58:05Z",
    "comment": "Accepted at EACL 2026 (main)",
    "light_analysis": {
      "overview": "本文综述了文本分类中类分布偏移下的研究现状，系统整理了开放集学习和相关方法，并提出未来研究方向。",
      "motivation": "机器学习模型通常假设训练和测试数据来自同一分布，但在实际文本分类任务中，类分布常因新话题的出现而发生偏移，这破坏了传统模型的泛化能力，导致性能下降。现有方法在处理这种动态分布时存在不足，尤其是在开放集场景下，需要专门的研究来应对数据分布变化带来的挑战，以提升模型在实际应用中的适应性和鲁棒性。",
      "method": "作为一篇综述论文，本文基于分布偏移的约束和问题公式化，将现有方法分类为学习Universum、零样本学习和开放集学习等三类。作者详细讨论了每种问题设置下的主要缓解策略，例如使用无标签数据、知识迁移或引入持续学习技术，并通过分析相关研究文章，系统总结了这些技术路线在文本分类任务中的应用和优化方法。",
      "result": "本文的主要成果是提供了一个全面的文献综述，涵盖了文本分类中类分布偏移问题的多种解决方法和理论框架。虽然未呈现具体的实验数据，但通过比较不同方法，如零样本学习在未见类上的应用或开放集学习的风险最小化策略，强调了现有技术的优势与局限性，为后续研究提供了基础视角和改进方向。",
      "conclusion": "本文总结了文本分类中类分布偏移领域的研究进展和挑战，指出未来工作方向包括推动技术超越当前水平，例如发展更高效的持续学习算法和跨领域迁移策略。研究的学术价值在于系统化整理现有知识，实际应用价值则体现在指导动态文本分类模型的开发，以应对现实世界中的分布变化问题，并减少对历史数据的依赖。",
      "tags": [
        "Text Classification",
        "Open-Set Learning",
        "Zero-Shot Learning",
        "Continual Learning",
        "Distribution Shift"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:06.106904Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.12128",
    "title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities",
    "authors": [
      "Florian Sestak",
      "Artur Toshev",
      "Andreas Fürst",
      "Günter Klambauer",
      "Andreas Mayr",
      "Johannes Brandstetter"
    ],
    "abstract": "Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.12128.pdf",
    "abs_url": "https://arxiv.org/abs/2502.12128",
    "published": "2025-02-17T18:49:13Z",
    "updated": "2026-01-15T09:56:42Z",
    "comment": "Project page: https://ml-jku.github.io/LaM-SLidE/",
    "light_analysis": {
      "overview": "LaM-SLidE提出了一种通过引入标识符表示在潜在空间建模中保持实体可追踪性的方法，用于处理空间动力系统。",
      "motivation": "研究动机源于生成模型在动力系统轨迹采样中的潜力，但现有潜在空间建模方法难以处理涉及实体相互作用的系统，如化学分子结构和集体人类行为。这些系统需要保持实体连接模式、守恒和随时间可追踪性，而当前方法在这方面存在不足，限制了从图像和视频生成领域借鉴的高效和可扩展技术的应用。因此，需要一种新方法来桥接实体可追踪性与潜在空间效率之间的差距，以促进更广泛的应用。",
      "method": "LaM-SLidE方法的核心创新是引入标识符表示（IDs），使从潜在系统表示中检索实体属性和实体组成成为可能，从而保持实体可追踪性。它结合了预训练的编码器和解码器，直接在潜在空间进行生成建模，借鉴了图像和视频生成的效率和可扩展性。方法针对空间动力系统设计，通过链接实体来建模系统动态，确保在保持可追踪性的同时，提升生成建模的精度和计算效率。",
      "result": "实验结果显示，LaM-SLidE在不同领域中表现良好，尤其是在速度、准确性和通用性方面。摘要未明确说明具体的数据支撑或与基线方法的详细对比，但作者表示方法在多个应用中取得正面效果。这可能包括生成轨迹的改进和计算效率的提升，然而具体性能指标如准确率或效率数据未在摘要中提及，需参考完整论文或代码库进一步验证。代码已公开，便于复现和实验验证。",
      "conclusion": "论文的主要贡献是提出了LaM-SLidE方法，通过在潜在空间建模中引入标识符表示，有效解决了动力系统实体可追踪性的问题。这促进了生成模型在空间动力系统中的应用，提高了建模效率和可扩展性，具有重要的学术价值。在实践中有助于化学、生物和社会行为等领域的模拟与预测。未来工作可能包括扩展到更复杂系统、优化标识符表示以及结合更多先进生成技术，以进一步拓展应用范围。",
      "tags": [
        "Latent Space Modeling",
        "Generative Models",
        "Spatial Dynamical Systems",
        "Linked Entities"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:14.946394Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.11806",
    "title": "Exploring the Translation Mechanism of Large Language Models",
    "authors": [
      "Hongbin Zhang",
      "Kehai Chen",
      "Xuefeng Bai",
      "Xiucheng Li",
      "Yang Xiang",
      "Min Zhang"
    ],
    "abstract": "While large language models (LLMs) demonstrate remarkable success in multilingual translation, their internal core translation mechanisms, even at the fundamental word level, remain insufficiently understood. To address this critical gap, this work introduces a systematic framework for interpreting the mechanism behind LLM translation from the perspective of computational components. This paper first proposes subspace-intervened path patching for precise, fine-grained causal analysis, enabling the detection of components crucial to translation tasks and subsequently characterizing their behavioral patterns in human-interpretable terms. Comprehensive experiments reveal that translation is predominantly driven by a sparse subset of components: specialized attention heads serve critical roles in extracting source language, translation indicators, and positional features, which are then integrated and processed by specific multi-layer perceptrons (MLPs) into intermediary English-centric latent representations before ultimately yielding the final translation. The significance of these findings is underscored by the empirical demonstration that targeted fine-tuning a minimal parameter subset ($<5\\%$) enhances translation performance while preserving general capabilities. This result further indicates that these crucial components generalize effectively to sentence-level translation and are instrumental in elucidating more intricate translation tasks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.11806.pdf",
    "abs_url": "https://arxiv.org/abs/2502.11806",
    "published": "2025-02-17T13:50:29Z",
    "updated": "2026-01-15T03:25:26Z",
    "comment": "Accepted in NeurIPS 2025 Poster",
    "light_analysis": {
      "overview": "本文提出系统性框架分析大语言模型翻译机制，通过细粒度因果分析揭示稀疏组件子集驱动翻译过程。",
      "motivation": "大语言模型在多语言翻译中展现出卓越能力，但内部翻译机制，尤其是在词级层面，尚未被充分理解。现有研究多关注性能提升，而缺乏对核心机制的深入探索，导致模型可解释性差，难以优化和泛化。因此，本研究旨在填补这一空白，通过解析计算组件行为来揭示翻译的内在驱动力，从而提升模型透明度和应用效果，以更好地理解和改进翻译任务。",
      "method": "研究引入了一个系统性框架，采用子空间干预路径修补法进行精确、细粒度的因果分析，用于检测对翻译任务至关重要的组件，如注意力头和多层感知器。该方法以人类可理解的方式描述组件行为模式，从计算角度解析LLM的内部机制。摘要未明确说明使用的数据集和具体模型架构，但专注于通用LLM架构中的组件交互，揭示它们如何提取源语言特征、翻译指示器和位置信息，并整合成中间潜在表示。",
      "result": "综合实验显示翻译主要由稀疏组件子集驱动：专门注意力头负责提取源语言、翻译指示器和位置特征，特定多层感知器则整合这些特征成英语中心潜在表示，最终生成翻译。针对性微调少于5%的参数能显著提高翻译性能，同时保留模型的一般能力，避免了灾难性遗忘。这些关键组件还能有效泛化到句子级翻译任务，辅助解释更复杂的翻译场景，突显了组件的核心作用。",
      "conclusion": "本研究通过系统性框架和细粒度分析，揭示了LLM翻译机制中的核心组件，增强了模型可解释性。主要贡献包括提出新分析方法和发现稀疏组件驱动翻译，为优化翻译性能提供了理论基础，如参数高效微调。这些发现不仅增进了对LLM内部工作原理的理解，还推动了实际应用，未来工作可扩展至其他多语言任务，并进一步探索组件的泛化能力和局限性。",
      "tags": [
        "Large Language Models",
        "Translation",
        "Causal Analysis",
        "Attention Mechanisms",
        "Model Interpretation"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:19.193905Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.10510",
    "title": "MixMin: Finding Data Mixtures via Convex Minimization",
    "authors": [
      "Anvith Thudi",
      "Evianne Rovers",
      "Yangjun Ruan",
      "Tristan Thrush",
      "Chris J. Maddison"
    ],
    "abstract": "Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a challenging and open problem. We formalize this data mixing problem as a bi-level objective: the best mixture is the one that would lead to the best model for a downstream objective. Unfortunately, this objective is generally intractable. In this paper, we make the observation that the bi-level data mixing objective becomes convex as our model class becomes larger. We develop and study a gradient-based approach for optimizing this convex objective, which we call MixMin, and test it on language modeling and chemistry tasks. MixMin was the only method that uniformly improved the data mixture in all our experiments. With MixMin, we improved the data mixture using less than 0.2% additional compute for a pythia-410M model trained on 8.2B tokens, resulting between 1-5% relative improvement to negative log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found that MixMin mixtures for smaller models improved training of larger models, suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay data to train an XGBoost model, we saw improvements to average precision scores of 0.03-0.15.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.10510.pdf",
    "abs_url": "https://arxiv.org/abs/2502.10510",
    "published": "2025-02-14T19:15:53Z",
    "updated": "2026-01-15T16:49:37Z",
    "comment": "Proceedings of the 42nd International Conference on Machine Learning",
    "light_analysis": {
      "overview": "MixMin 提出了一种基于梯度优化的凸最小化方法，用于高效求解机器学习中的数据混合优化问题，以提升下游模型性能。",
      "motivation": "现代机器学习系统经常整合多样数据源，如大语言模型的预训练，但确定最优数据混合比例是一个开放且复杂的问题。现有方法在处理双层优化目标时通常难以求解，计算效率低下，这限制了模型训练的效果。因此，开发一种高效优化数据混合的方法对于提高机器学习管道的性能和泛化能力至关重要，尤其是在数据源多样化的背景下。",
      "method": "MixMin 方法采用基于梯度的优化技术，针对数据混合的双层目标进行凸最小化求解。关键创新在于观察到当模型类别足够大时，该优化问题变为凸，从而简化计算。实验中，在语言建模任务使用 pythia-410M 模型处理 82 亿 tokens，化学任务使用 XGBoost 模型分析生物测定数据，验证了方法的实用性和技术特色。",
      "result": "MixMin 在语言建模任务中，仅增加 0.2% 的计算成本，使 pythia-410M 模型在 PIQA、ARC Easy、SciQ 和 OpenWebMath 基准上的负对数似然相对提升了 1-5%。在化学任务中，使用 MixMin 混合的数据训练 XGBoost 模型，平均精度得分提高了 0.03-0.15，且在所有实验中统一优于其他方法，展示了显著的性能改进。",
      "conclusion": "MixMin 方法通过凸优化有效解决了数据混合问题，显著提升了模型在下游任务中的性能，并展现出尺度不变性的潜力。这一研究为机器学习中的数据集成提供了新的技术路径，具有重要的学术和应用价值。未来工作可探索扩展到更多任务和模型类型，或进行更深入的理论分析。",
      "tags": [
        "Data Mixture",
        "Convex Optimization",
        "Gradient-Based Optimization",
        "Bi-Level Optimization",
        "Language Modeling"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:26.163095Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.07783",
    "title": "Curvature Tuning: Provable Training-free Model Steering From a Single Parameter",
    "authors": [
      "Leyang Hu",
      "Matteo Gamba",
      "Randall Balestriero"
    ],
    "abstract": "The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model's decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions-thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 8.59%/8.34% over linear probing and 4.64%/1.70% over LoRA across 12 datasets, and improves robust accuracy on the $\\ell_\\infty$ benchmark from RobustBench by 1032.64%/1494.46%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.07783.pdf",
    "abs_url": "https://arxiv.org/abs/2502.07783",
    "published": "2025-02-11T18:59:57Z",
    "updated": "2026-01-15T15:36:28Z",
    "comment": "Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "本论文提出了Curvature Tuning（CT），一种通过注入单个超参数到激活函数中、可证明调整决策边界曲率的可解释模型引导方法，以提升泛化和鲁棒性。",
      "motivation": "随着模型和数据规模的扩大，微调预训练模型已成为解决下游任务的标准范式。然而，主流微调方法如权重调整通常缺乏可解释性，并依赖启发式选择的超参数，这限制了方法的透明性和效率。本文旨在解决这些问题，通过从权重转向激活函数的新视角，提出一种原则性方法，以弥补现有方法在可解释性和参数依赖性方面的不足，从而提供更可靠和高效的模型引导机制。",
      "method": "论文提出Curvature Tuning（CT），将激活函数视为样条操作器，通过注入单个超参数来调制模型决策边界曲率。该方法核心创新在于从数学上可证明调整决策边界，并将模型投影到光滑函数空间，与现有微调方法的特征适应形成互补。当超参数可训练时，CT演变为参数高效的微调方法。实验中，使用ResNet-50/152等模型和多个数据集（如12个数据集）进行验证，以展示其技术特色。",
      "result": "实验结果表明，CT显著提升了模型的泛化和鲁棒性。在12个数据集上，CT使ResNet-50/152的下游准确率相比线性探测提升8.59%/8.34%，相比LoRA提升4.64%/1.70%。在鲁棒性测试中，CT在RobustBench的ℓ∞基准上使鲁棒准确率提升1032.64%/1494.46%，优于基线方法，证明了其在准确率和鲁棒性方面的有效性。",
      "conclusion": "本论文的主要贡献是提出了Curvature Tuning，一种可解释且原则性的模型引导方法，通过调制激活函数调整决策边界曲率。该方法在学术上提供了新视角，补充了现有微调方法，侧重于决策边界优化。实际应用中，CT提高了泛化和鲁棒性，具有参数高效的优势。未来工作可探索CT在其他模型和任务上的应用，并可能进一步分析其局限性。",
      "tags": [
        "Curvature Tuning",
        "Activation Function",
        "Spline Operators",
        "Fine-tuning",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:33.511937Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.06072",
    "title": "Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs",
    "authors": [
      "Xiangcheng Zhang",
      "Yige Hong",
      "Weina Wang"
    ],
    "abstract": "Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the fully heterogeneous setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of $N$ arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when $N$ is large. We show that, under mild assumptions, an efficiently computable policy achieves an $O(1/\\sqrt{N})$ optimality gap in the long-run average reward per arm for fully heterogeneous WCMDPs as $N$ becomes large. This is the first asymptotic optimality result for fully heterogeneous average-reward WCMDPs. Our main technical innovation is the construction of projection-based Lyapunov functions that certify the convergence of rewards and costs to an optimal region, even under full heterogeneity.",
    "categories": [
      "cs.LG",
      "math.OC",
      "math.PR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.06072.pdf",
    "abs_url": "https://arxiv.org/abs/2502.06072",
    "published": "2025-02-09T23:32:19Z",
    "updated": "2026-01-15T06:56:22Z",
    "comment": "37 pages; full version for NeurIPS proceeding paper; fixed some typos",
    "light_analysis": {
      "overview": "本论文首次提出基于投影的Lyapunov方法，为完全异质的弱耦合马尔可夫决策过程（WCMDPs）实现了渐近最优性。",
      "motivation": "异质性在许多现实世界大规模决策问题中构成了基础挑战，但相关研究不足。论文聚焦于完全异质的弱耦合马尔可夫决策过程（WCMDPs），其中每个臂（子问题）具有不同的模型参数。这种异质性在臂数量N大时会导致维度诅咒，使得问题难以高效处理，限制了现有方法在资源分配、多智能体系统等实际应用中的可扩展性和性能。因此，解决完全异质WCMDPs的渐近最优性问题至关重要。",
      "method": "论文提出一种基于投影的Lyapunov方法。核心创新是构建Lyapunov函数，通过投影技术来证明在完全异质设置下，奖励和成本能够收敛到最优区域。该方法在温和假设下，设计了一个高效可计算的政策，无需依赖特定数据集或模型架构，主要依靠理论分析和技术框架来确保收敛性，从而克服异质性带来的维度诅咒问题。",
      "result": "论文结果显示，在臂数量N大时，提出的政策能实现每个臂长期平均奖励的最优性差距为O(1/√N)。这是第一个针对完全异质平均奖励WCMDPs的渐近最优性结果，表明优化差距随子问题数量增加而衰减，显著改善了大规模决策问题的可扩展性和性能，但摘要未提供与基线方法的直接对比数据。",
      "conclusion": "本论文的主要贡献是提出基于投影的Lyapunov方法，首次为完全异质的弱耦合马尔可夫决策过程提供了渐近最优性结果。研究具有重要学术价值，为处理异质性大规模决策问题奠定了理论基础，并提出高效可计算的政策。实际应用价值包括资源分配、多智能体系统等领域，但摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Weakly-coupled Markov Decision Processes (WCMDPs)",
        "Lyapunov Functions",
        "Projection-based Methods",
        "Asymptotic Optimality",
        "Heterogeneity"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:35.535857Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.05066",
    "title": "Beautiful Images, Toxic Words: Understanding and Addressing Offensive Text in Generated Images",
    "authors": [
      "Aditya Kumar",
      "Tom Blanchard",
      "Adam Dziedzic",
      "Franziska Boenisch"
    ],
    "abstract": "State-of-the-art Diffusion Models (DMs) produce highly realistic images. While prior work has successfully mitigated Not Safe For Work (NSFW) content in the visual domain, we identify a novel threat: the generation of NSFW text embedded within images. This includes offensive language, such as insults, racial slurs, and sexually explicit terms, posing significant risks to users. We show that all state-of-the-art DMs (e.g., SD3, SDXL, Flux, DeepFloyd IF) are vulnerable to this issue. Through extensive experiments, we demonstrate that existing mitigation techniques, effective for visual content, fail to prevent harmful text generation while substantially degrading benign text generation. As an initial step toward addressing this threat, we introduce a novel fine-tuning strategy that targets only the text-generation layers in DMs. Therefore, we construct a safety fine-tuning dataset by pairing each NSFW prompt with two images: one with the NSFW term, and another where that term is replaced with a carefully crafted benign alternative while leaving the image unchanged otherwise. By training on this dataset, the model learns to avoid generating harmful text while preserving benign content and overall image quality. Finally, to advance research in the area, we release ToxicBench, an open-source benchmark for evaluating NSFW text generation in images. It includes our curated fine-tuning dataset, a set of harmful prompts, new evaluation metrics, and a pipeline that assesses both NSFW-ness and text and image quality. Our benchmark aims to guide future efforts in mitigating NSFW text generation in text-to-image models, thereby contributing to their safe deployment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2502.05066.pdf",
    "abs_url": "https://arxiv.org/abs/2502.05066",
    "published": "2025-02-07T16:39:39Z",
    "updated": "2026-01-15T10:58:27Z",
    "comment": "Accepted at AAAI 2026 (AI Alignment Track)",
    "light_analysis": {
      "overview": "论文提出一种针对扩散模型中嵌入有害文本生成问题的微调策略，并发布了开源基准ToxicBench，以促进文本到图像模型的安全部署。",
      "motivation": "扩散模型虽能生成高度真实图像，但面临新威胁：图像中可能嵌入有害文本，如冒犯性语言、种族歧视和性内容，对用户安全构成显著风险。现有缓解技术主要针对视觉NSFW内容，无法有效防止有害文本生成，反而损害良性文本的生成能力，导致模型在应用中的安全性不足。这一问题的重要性在于，它揭示了当前安全措施的局限性，亟需专门方法来处理文本嵌入的风险，以确保扩散模型的可靠和道德使用。",
      "method": "研究方法包括引入一种新颖的微调策略，专门针对扩散模型中的文本生成层进行优化。核心创新在于构建安全微调数据集：每个NSFW提示配对两幅图像，一幅包含有害文本术语，另一幅将该术语替换为精心设计的良性替代品，同时保持图像其他部分不变。通过在这个数据集上训练模型，使其学会避免生成有害文本，同时保留良性内容和整体图像质量。该方法避免了全面修改模型，提高了针对文本生成风险的缓解效果。",
      "result": "实验结果显示，所有状态最先进的扩散模型，如SD3、SDXL、Flux和DeepFloyd IF，均易受有害文本生成问题影响。现有缓解技术对视觉内容有效，但无法防止有害文本生成，并显著降低良性文本的生成能力。提出的微调策略在实验中有效减少了有害文本的生成，同时不损害良性内容和图像质量。尽管摘要未提供具体性能指标，但实验验证了该方法相对于基线方法的改进，为后续研究提供了基础。",
      "conclusion": "论文的主要贡献是识别并解决扩散模型中嵌入有害文本的新威胁，通过提出针对性的微调方法和开源基准ToxicBench，推动文本到图像模型的安全发展。其学术价值在于填补了NSFW文本生成研究的空白，实际应用价值体现在促进模型的安全部署和减少用户风险。潜在局限性可能包括数据集的覆盖范围有限，未来工作可扩展基准以涵盖更多有害场景，并探索其他缓解技术以进一步提升模型安全性。",
      "tags": [
        "Diffusion Models",
        "Fine-tuning",
        "Text-to-Image Generation",
        "NSFW Content Detection",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-16T03:21:49.927281Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.04675",
    "title": "Scalable Oversight for Superhuman AI via Recursive Self-Critiquing",
    "authors": [
      "Xueru Wen",
      "Jie Lou",
      "Xinyu Lu",
      "Junjie Yang",
      "Yanjiang Liu",
      "Yaojie Lu",
      "Debing Zhang",
      "Xing Yu"
    ],
    "abstract": "As AI capabilities increasingly surpass human proficiency in complex tasks, current alignment techniques, including SFT and RLHF, face fundamental challenges in ensuring reliable oversight. These methods rely on direct human assessment and become impractical when AI outputs exceed human cognitive thresholds. In response to this challenge, we explore two hypotheses: (1) \\textit{Critique of critique can be easier than critique itself}, extending the widely-accepted observation that verification is easier than generation to the critique domain, as critique itself is a specialized form of generation; (2) \\textit{This difficulty relationship holds recursively}, suggesting that when direct evaluation is infeasible, performing higher-order critiques (e.g., critique of critique of critique) offers a more tractable supervision pathway. We conduct Human-Human, Human-AI, and AI-AI experiments to investigate the potential of recursive self-critiquing for AI supervision. Our results highlight recursive critique as a promising approach for scalable AI oversight.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2502.04675.pdf",
    "abs_url": "https://arxiv.org/abs/2502.04675",
    "published": "2025-02-07T05:41:23Z",
    "updated": "2026-01-15T03:27:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出递归自我批评方法，以解决超人类AI的可扩展监督问题。",
      "motivation": "随着AI在复杂任务中能力超越人类，现有对齐技术如监督微调（SFT）和人类反馈强化学习（RLHF）面临根本挑战，因为它们依赖人类直接评估。当AI输出超过人类认知阈值时，这些方法变得不切实际，导致无法确保可靠监督。因此，研究如何开发不依赖人类直接评估的新监督方法至关重要，以应对AI能力增强带来的安全对齐问题。",
      "method": "研究方法基于两个核心假设：一是批评的批评可能比原始批评更容易，将验证优于生成的观察扩展到批评生成领域；二是这种难度关系具有递归性，允许通过高阶批评（如批评的批评的批评）实现更可行的监督。论文进行了Human-Human、Human-AI和AI-AI实验，以探索递归自我批评在不同交互场景下的潜力，关键创新点在于递归应用批评过程以减少对直接人类评估的依赖。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "摘要未明确说明具体实验结果数据，如准确率或效率改进。但作者指出递归批评被确认为一种有前景的方法，暗示实验结果支持其在AI监督方面的潜在有效性。由于摘要缺少详细性能指标，无法提供与基线方法的对比情况，仅基于现有信息推断实验可能展示了递归批评的可行性。",
      "conclusion": "论文的主要贡献是提出递归自我批评作为超人类AI可扩展监督的创新方法，解决了现有对齐技术依赖人类评估的局限性。这项研究具有重要学术价值，为AI安全领域提供了新的监督范式，并具备实际应用前景，帮助确保高级AI系统的可靠对齐。未来工作可能包括进一步实验验证、理论分析和优化递归机制，以完善其有效性和适用范围。",
      "tags": [
        "Recursive Self-Critiquing",
        "AI Supervision",
        "Human-AI Interaction",
        "Critique Generation",
        "Scalable Oversight"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:15.062748Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.13365",
    "title": "Symmetrization Weighted Binary Cross-Entropy: Modeling Perceptual Asymmetry for Human-Consistent Neural Edge Detection",
    "authors": [
      "Hao Shu"
    ],
    "abstract": "Edge detection (ED) is a fundamental perceptual process in computer vision, forming the structural basis for high-level reasoning tasks such as segmentation, recognition, and scene understanding. Despite substantial progress achieved by deep neural networks, most ED models attain high numerical accuracy but fail to produce visually sharp and perceptually consistent edges, thereby limiting their reliability in intelligent vision systems. To address this issue, this study introduces the \\textit{Symmetrization Weighted Binary Cross-Entropy (SWBCE)} loss, a perception-inspired formulation that extends the conventional WBCE by incorporating prediction-guided symmetry. SWBCE explicitly models the perceptual asymmetry in human edge recognition, wherein edge decisions require stronger evidence than non-edge ones, aligning the optimization process with human perceptual discrimination. The resulting symmetric learning mechanism jointly enhances edge recall and suppresses false positives, achieving a superior balance between quantitative accuracy and perceptual fidelity. Extensive experiments across multiple benchmark datasets and representative ED architectures demonstrate that SWBCE can outperform existing loss functions in both numerical evaluation and visual quality. Particularly with the HED-EES model, the SSIM can be improved by about 15% on BRIND, and in all experiments, training by SWBCE consistently obtains the best perceptual results. Beyond edge detection, the proposed perceptual loss offers a generalizable optimization principle for soft computing and neural learning systems, particularly in scenarios where asymmetric perceptual reasoning plays a critical role.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.13365.pdf",
    "abs_url": "https://arxiv.org/abs/2501.13365",
    "published": "2025-01-23T04:10:31Z",
    "updated": "2026-01-15T15:19:29Z",
    "comment": "39 pages",
    "light_analysis": {
      "overview": "本研究提出了一种基于感知不对称建模的 SWBCE 损失函数，用于提升神经边缘检测的视觉质量和人类一致性。",
      "motivation": "边缘检测是计算机视觉中的基础感知过程，对分割、识别等高级任务至关重要。然而，现有深度神经网络模型虽在数值精度上表现出色，却难以生成视觉清晰且感知一致的边缘，这在智能视觉系统中降低了可靠性。因此，亟需解决模型视觉质量和感知一致性问题，以克服现有方法在人类感知匹配方面的不足，提升边缘检测在实际应用中的有效性。",
      "method": "论文提出了 Symmetrization Weighted Binary Cross-Entropy (SWBCE) 损失函数，该函数扩展了传统的加权二值交叉熵，通过引入预测引导的对称性来建模人类感知不对称。具体来说，SWBCE 在优化过程中考虑了边缘决策需要比非边缘决策更强的证据，从而模拟人类感知辨别，实现对称学习机制以同时提升边缘召回率和抑制假阳性。研究在多个基准数据集和代表性边缘检测架构中进行实验，但摘要未明确说明所有具体数据集和模型细节。",
      "result": "在多个基准数据集和代表性边缘检测架构的广泛实验中，SWBCE 损失函数在数值评估和视觉质量上均优于现有损失函数。具体而言，使用 HED-EES 模型时，在 BRIND 数据集上 SSIM 提高了约 15%，且在全部实验中，SWBCE 训练 consistently 获得最佳感知结果，实现了定量精度与感知保真度的优化平衡，有效提升了边缘检测的可靠性和视觉输出质量。",
      "conclusion": "该研究的主要贡献是提出了 SWBCE 损失函数，它不仅改善了边缘检测的感知一致性，还提供了一个可推广的优化原则，适用于软计算和神经网络学习系统，特别是在不对称感知推理起关键作用的场景中。这一工作在计算机视觉和机器学习领域具有重要的学术价值和应用潜力，未来可探索其扩展应用，但摘要未明确说明具体局限性或未来工作方向。",
      "tags": [
        "Edge Detection",
        "Binary Cross-Entropy",
        "Perceptual Asymmetry",
        "Loss Function",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:16.748515Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.16063",
    "title": "VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction",
    "authors": [
      "Yadi Cao",
      "Yuxuan Liu",
      "Liu Yang",
      "Rose Yu",
      "Hayden Schaeffer",
      "Stanley Osher"
    ],
    "abstract": "In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON's adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at https://github.com/Eydcao/VICON.",
    "categories": [
      "cs.LG",
      "math.NA",
      "physics.flu-dyn"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.16063.pdf",
    "abs_url": "https://arxiv.org/abs/2411.16063",
    "published": "2024-11-25T03:25:17Z",
    "updated": "2026-01-15T09:58:38Z",
    "comment": "update after TMLR accept",
    "light_analysis": {
      "overview": "VICON通过整合视觉Transformer架构，实现高效2D流体动力学数据处理，提升了多物理场算子学习的适应性和计算效率。",
      "motivation": "现有In-Context Operator Networks (ICONs)在处理高维密集2D流体动力学数据时，将每个空间点作为独立token处理，导致计算效率严重受限。这一限制在多物理流体动力学预测中尤为突出，因为真实世界测量系统常面临采样频率差异或数据丢失等挑战，现有方法难以适应这些多变的时间步策略，影响实际部署。因此，研究动机是开发一种既保持ICON适应多物理系统能力、又能高效处理2D数据的方法，以解决现实应用中常见的不完美测量问题。",
      "method": "VICON集成了视觉Transformer架构，通过patch-wise操作高效处理2D数据，将输入划分为patch并利用自注意力机制进行特征提取，避免了逐点处理的计算瓶颈。核心创新点在于结合vision transformer的计算效率与ICON的上下文学习能力，保留了适应多物理系统和变化时间步的灵活性。模型在三个流体动力学基准数据集上进行训练和评估，使用标准流程验证其性能，摘要未明确说明具体数据集名称。",
      "result": "在三个流体动力学基准测试中，VICON显著优于当前最佳基线方法DPOT和MPP。具体地，平均最后一步rollout误差相比DPOT减少了37.9%，相比MPP减少了44.7%。推理效率方面，VICON仅需DPOT的72.5%时间和MPP的34.8%时间。在不完美测量场景下，VICON表现更强鲁棒性，性能相对退化仅为24.41%，而基线方法的退化高达71.37%至74.49%，验证了其在真实应用中的优越性。",
      "conclusion": "论文主要贡献是提出了VICON模型，通过整合视觉Transformer提高了流体动力学预测的效率和准确性，同时支持灵活的rollout策略以适应时间步变化。该研究在学术上扩展了算子学习方法的适用范围，提升了计算效率；在实际应用中，VICON可直接部署于不完美测量系统，无需重新训练或插值，具有显著实用价值。未来工作可探索更复杂场景下的应用或进一步优化模型架构，摘要未明确说明具体局限性。",
      "tags": [
        "Vision Transformer",
        "In-Context Learning",
        "Operator Networks",
        "Fluid Dynamics Prediction",
        "Patch-based Processing"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:27.964769Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.15390",
    "title": "The Hatching-Box: A Novel System for Automated Monitoring and Quantification of Drosophila melanogaster Developmental Behavior",
    "authors": [
      "Julian Bigge",
      "Maite Ogueta",
      "Luis Garcia",
      "Benjamin Risse"
    ],
    "abstract": "In this paper we propose the Hatching-Box, a novel imaging and analysis system to automatically monitor and quantify the developmental behavior of Drosophila in standard rearing vials and during regular rearing routines, rendering explicit experiments obsolete. This is achieved by combining custom tailored imaging hardware with dedicated detection and tracking algorithms, enabling the quantification of larvae, filled/empty pupae and flies over multiple days. Given the affordable and reproducible design of the Hatching-Box in combination with our generic client/server-based software, the system can easily be scaled to monitor an arbitrary amount of rearing vials simultaneously. We evaluated our system on a curated image dataset comprising nearly 470,000 annotated objects and performed several studies on real world experiments. We successfully reproduced results from well-established circadian experiments by comparing the eclosion periods of wild type flies to the clock mutants $\\textit{per}^{short}$, $\\textit{per}^{long}$ and $\\textit{per}^0$ without involvement of any manual labor. Furthermore we show, that the Hatching-Box is able to extract additional information about group behavior as well as to reconstruct the whole life-cycle of the individual specimens. These results not only demonstrate the applicability of our system for long-term experiments but also indicate its benefits for automated monitoring in the general cultivation process.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2411.15390.pdf",
    "abs_url": "https://arxiv.org/abs/2411.15390",
    "published": "2024-11-23T00:09:42Z",
    "updated": "2026-01-15T09:53:22Z",
    "comment": "17 pages, 6 figures",
    "light_analysis": {
      "overview": "本文提出 Hatching-Box 系统，通过定制硬件和算法自动监控和量化果蝇发育行为。",
      "motivation": "传统果蝇发育行为研究依赖人工观察和实验，过程繁琐且易引入误差，在大规模或长期实验中效率低下。现有方法难以自动化监控培养过程中的幼虫、蛹和苍蝇，限制了生物学实验的可扩展性和可重复性。因此，开发一个自动化系统以减少人工干预、提高监控精度和效率成为重要需求，旨在解决生物学研究中行为监控的瓶颈，推动高通量实验的发展。",
      "method": "Hatching-Box 系统结合定制成像硬件与专用检测和追踪算法，硬件针对标准培养瓶优化，实现长期、非侵入式监控。软件采用客户端/服务器架构，支持可扩展设计，可同时监控多个培养瓶。算法能自动识别和量化幼虫、充蛹/空蛹和苍蝇，整合图像数据采集与处理，关键创新在于硬件与算法的紧密集成，确保系统在常规培养环境下的适用性，设计廉价且可复制。",
      "result": "系统在包含近470,000个标注对象的图像数据集上评估，表现出高准确性。在真实实验中，成功复现昼夜节律实验结果，比较野生型果蝇与突变型（per^{short}、per^{long}、per^0）的羽化周期，无需人工操作。此外，系统能提取群体行为信息并重建个体生命周期，验证了其在长期实验中的可靠性和自动化监控潜力，相比于传统方法显著提高效率和数据一致性。",
      "conclusion": "Hatching-Box 系统通过硬件和算法的创新，实现了果蝇发育行为的自动化监控，减少人工依赖并增强实验可重复性。其廉价、可扩展设计为生物学研究提供了高效工具，特别适用于长期和大规模实验。研究展示了系统在昼夜节律分析和行为量化中的价值，未来可扩展至其他物种或优化算法，推动自动化监控技术在生命科学领域的广泛应用。",
      "tags": [
        "Automated Monitoring",
        "Image Analysis",
        "Object Detection",
        "Tracking Algorithms",
        "Client-Server Architecture"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:15.192552Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.10336",
    "title": "CoMAT: Chain of Mathematically Annotated Thought Improves Mathematical Reasoning",
    "authors": [
      "Joshua Ong Jun Leang",
      "Aryo Pradipta Gema",
      "Shay B. Cohen"
    ],
    "abstract": "Mathematical reasoning remains a significant challenge for large language models (LLMs), despite progress in prompting techniques such as Chain-of-Thought (CoT). We present **Chain of Mathematically Annotated Thought (CoMAT)**, which enhances reasoning through two stages: *Symbolic Conversion* (converting natural language queries into symbolic form) and *Reasoning Execution* (deriving answers from symbolic representations). CoMAT operates entirely with a single LLM and without external solvers. Across four LLMs, CoMAT outperforms traditional CoT on six out of seven benchmarks, achieving gains of 4.48% on MMLU-Redux (MATH) and 4.58% on GaoKao MCQ. In addition to improved performance, CoMAT ensures faithfulness and verifiability, offering a transparent reasoning process for complex mathematical tasks",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2410.10336.pdf",
    "abs_url": "https://arxiv.org/abs/2410.10336",
    "published": "2024-10-14T09:48:41Z",
    "updated": "2026-01-15T13:23:01Z",
    "comment": "9 pages, 12 figures",
    "light_analysis": {
      "overview": "CoMAT提出了一种通过符号转换和推理执行的链式思维方法，显著提升大语言模型在数学推理任务中的性能。",
      "motivation": "数学推理是大语言模型（LLMs）面临的关键挑战，尽管现有方法如链式思维（CoT）有所进展，但仍可能在透明度和准确性方面存在不足。数学问题求解需要可靠且可解释的推理过程，以支持教育和自动应用。CoMAT旨在解决这些问题，通过提供更忠实和可验证的推理方法，以增强LLMs在复杂数学任务中的表现。",
      "method": "CoMAT采用两阶段方法：符号转换将自然语言查询转化为符号形式，以简化问题结构；推理执行基于符号表示推导答案，完全依赖单个LLM而无外部求解器。该方法通过将语言理解和符号处理结合，提高了推理效率和透明度。关键创新在于利用LLM的内部能力进行端到端符号化处理，无需额外工具，从而增强可解释性。",
      "result": "在四个大语言模型上评估，CoMAT在七个基准测试中的六个优于传统链式思维（CoT），具体性能提升包括：在MMLU-Redux（MATH）上提高4.48%，在GaoKao MCQ上提高4.58%。此外，CoMAT确保推理过程的忠实性和可验证性，提供透明路径，有助于准确性和可靠性改进。这些结果表明CoMAT在提升数学推理能力方面的有效性。",
      "conclusion": "CoMAT的主要贡献是提出了一种透明且高效的数学推理方法，通过符号转换和推理执行增强LLMs性能。学术价值在于促进可解释AI和符号推理研究，实际应用价值在于支持数学教育和自动问题求解。摘要未明确说明局限性，但可能依赖于LLM能力；未来工作可扩展到其他推理领域或优化符号处理步骤。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thought",
        "Symbolic Reasoning",
        "Mathematical Reasoning",
        "Transparency"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:27.462102Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.01944",
    "title": "Debiased Orthogonal Boundary-Driven Efficient Noise Mitigation",
    "authors": [
      "Hao Li",
      "Jiayang Gu",
      "Jingkuan Song",
      "An Zhang",
      "Lianli Gao"
    ],
    "abstract": "Mitigating the detrimental effects of noisy labels on the training process has become increasingly critical, as obtaining entirely clean or human-annotated samples for large-scale pre-training tasks is often impractical. Nonetheless, existing noise mitigation methods often encounter limitations in practical applications due to their task-specific design, model dependency, and significant computational overhead. In this work, we exploit the properties of high-dimensional orthogonality to identify a robust and effective boundary in cone space for separating clean and noisy samples. Building on this, we propose One-Step Anti-noise (OSA), a model-agnostic noisy label mitigation paradigm that employs an estimator model and a scoring function to assess the noise level of input pairs through just one-step inference. We empirically validate the superiority of OSA, demonstrating its enhanced training robustness, improved task transferability, streamlined deployment, and reduced computational overhead across diverse benchmarks, models, and tasks. Our code is released at https://github.com/leolee99/OSA.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2410.01944.pdf",
    "abs_url": "https://arxiv.org/abs/2410.01944",
    "published": "2024-10-02T18:42:56Z",
    "updated": "2026-01-15T07:49:21Z",
    "comment": "20 pages, 4 figures, 11 Tables",
    "light_analysis": {
      "overview": "提出One-Step Anti-noise (OSA)，一种基于高维正交性边界识别的模型无关噪声标签缓解方法，通过一步推理高效分离干净和噪声样本，减少计算开销。",
      "motivation": "研究动机在于噪声标签对大规模预训练任务的训练过程造成有害影响，由于获取干净样本不切实际，现有噪声缓解方法常因任务特定设计、模型依赖性和高计算开销而难以应用。这凸显了开发通用、高效噪声缓解方法的必要性，以提升模型训练的鲁棒性和实际部署的可行性。",
      "method": "论文提出One-Step Anti-noise (OSA)范式，利用高维正交性在锥空间中识别鲁棒边界，以区分干净和噪声样本。该方法通过一个估计模型和评分函数，仅需一步推理即可评估输入对的噪声水平，实现模型无关的设计，简化部署流程并降低计算成本，关键创新在于正交性原理的运用和一步推理的优化。",
      "result": "实验在多种基准、模型和任务中验证了OSA的优越性，结果显示其显著增强了训练鲁棒性、改进了任务可转移性、简化了部署并减少了计算开销。与基线方法相比，OSA表现出更高的效率，但具体性能指标如准确率提升在摘要中未明确说明，总体证明了其通用性和高效性。",
      "conclusion": "论文的主要贡献是提出OSA，一种基于高维正交性边界识别的模型无关噪声标签缓解方法，具有重要学术价值，为噪声鲁棒训练提供新思路，实际应用价值高，适用于大规模预训练任务以降低计算成本。未来工作可能涉及扩展到更复杂噪声场景或优化算法性能。",
      "tags": [
        "Noisy Label Mitigation",
        "Orthogonality",
        "Cone Space",
        "One-Step Inference",
        "Model-Agnostic"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:30.424264Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2409.11277",
    "title": "Machine Learning and Theory Ladenness -- A Phenomenological Account",
    "authors": [
      "Alberto Termine",
      "Emanuele Ratti",
      "Alessandro Facchini"
    ],
    "abstract": "We provide an analysis of theory ladenness in machine learning in science, where \"theory\", that we call \"domain theory\", refers to the domain knowledge of the scientific discipline where ML is used. By constructing an account of ML models based on a comparison with phenomenological models, we show, against recent trends in philosophy of science, that ML model-building is mostly indifferent to domain theory, even if the model remains theory laden in a weak sense, which we call theory infection. These claims, we argue, have far-reaching consequences for the transferability of ML across scientific disciplines, and shift the priorities of the debate on theory ladenness in ML from descriptive to normative.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2409.11277.pdf",
    "abs_url": "https://arxiv.org/abs/2409.11277",
    "published": "2024-09-17T15:29:14Z",
    "updated": "2026-01-15T10:13:23Z",
    "comment": "29 pages with reference",
    "light_analysis": {
      "overview": "本文提出机器学习模型构建对领域理论基本漠不关心，引入“理论感染”概念，并探讨其对跨学科可转移性的影响。",
      "motivation": "在机器学习广泛应用于科学研究的背景下，理论负荷问题成为关键议题，涉及模型是否依赖于特定领域的知识（即领域理论）。这一讨论重要，因为它影响ML模型的可解释性、可靠性和跨学科应用的可行性。现有科学哲学趋势强调ML应更紧密结合领域理论，但本论文指出其不足，旨在通过哲学分析澄清ML模型构建的本质，为ML在科学中的合理使用奠定基础。",
      "method": "论文采用哲学比较方法，通过将机器学习模型与现象学模型进行对比，构建了对ML模型的理论说明。关键创新是提出“理论感染”概念，用以描述ML模型在弱意义上仍受领域理论影响的状态。该方法基于哲学论证，未涉及具体技术实现如数据集或模型架构，摘要未明确说明具体技术细节。",
      "result": "论文的主要论证结果是机器学习模型构建在大多数情况下对领域理论漠不关心，即使存在弱意义上的理论负荷（即理论感染）。这一结论挑战了科学哲学中近期认为ML应更依赖领域理论的观点。结果未提供实验数据或性能指标，而是理论主张，对ML跨科学领域的可转移性具有深远影响，并将相关辩论的焦点从描述性转向规范性。",
      "conclusion": "本研究的核心贡献在于提出了理论感染概念，并论证了ML模型构建对领域理论的漠不关心。这一发现具有重要学术价值，它重新定位了ML中理论负荷的讨论，强调了规范性优先于描述性，从而影响ML在科学中的可转移性和应用策略。未来研究方向可能包括实证验证或扩展至其他AI领域，摘要未明确说明具体局限性。",
      "tags": [
        "Machine Learning",
        "Theory Ladenness",
        "Phenomenological Models",
        "Domain Theory",
        "Interdisciplinary Transfer"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:57.052488Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.14348",
    "title": "Deep learning-based ecological analysis of camera trap images is impacted by training data quality and quantity",
    "authors": [
      "Peggy A. Bevan",
      "Omiros Pantazis",
      "Holly Pringle",
      "Guilherme Braga Ferreira",
      "Daniel J. Ingram",
      "Emily Madsen",
      "Liam Thomas",
      "Dol Raj Thanet",
      "Thakur Silwal",
      "Santosh Rayamajhi",
      "Gabriel Brostow",
      "Oisin Mac Aodha",
      "Kate E. Jones"
    ],
    "abstract": "Large image collections generated from camera traps offer valuable insights into species richness, occupancy, and activity patterns, significantly aiding biodiversity monitoring. However, the manual processing of these datasets is time-consuming, hindering analytical processes. To address this, deep neural networks have been widely adopted to automate image labelling, but the impact of classification error on key ecological metrics remains unclear. Here, we analyse data from camera trap collections in an African savannah (82,300 labelled images, 47 species) and an Asian sub-tropical dry forest (40,308 labelled images, 29 species) to compare ecological metrics derived from expert-generated species identifications with those generated by deep learning classification models. We specifically assess the impact of deep learning model architecture, proportion of label noise in the training data, and the size of the training dataset on three key ecological metrics: species richness, occupancy, and activity patterns. We found that predictions of species richness derived from deep neural networks closely match those calculated from expert labels and remained resilient to up to 10% noise in the training dataset (mis-labelled images) and a 50% reduction in the training dataset size. We found that our choice of deep learning model architecture (ResNet vs ConvNext-T) or depth (ResNet18, 50, 101) did not impact predicted ecological metrics. In contrast, species-specific metrics were more sensitive; less common and visually similar species were disproportionately affected by a reduction in deep neural network accuracy, with consequences for occupancy and diel activity pattern estimates. To ensure the reliability of their findings, practitioners should prioritize creating large, clean training sets and account for class imbalance across species over exploring numerous deep learning model architectures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2408.14348.pdf",
    "abs_url": "https://arxiv.org/abs/2408.14348",
    "published": "2024-08-26T15:26:27Z",
    "updated": "2026-01-15T11:46:20Z",
    "comment": "Peggy A. Bevan, Omiros Pantazis: equally contributing authors. Published in Remote Sensing in Ecology and Conservation",
    "light_analysis": {
      "overview": "本论文揭示了在基于深度学习的相机陷阱图像分析中，训练数据的质量和数量对生态指标的可靠性至关重要，而模型架构的选择影响有限。",
      "motivation": "该研究旨在解决深度学习自动化分析相机陷阱图像时，分类错误对关键生态指标（如物种丰富度、占据率、活动模式）的影响问题。现有方法广泛采用深度学习，但分类错误如何影响生态分析未被充分探究，这限制了自动化方法在生物多样性监测中的可靠性和应用价值。研究背景基于相机陷阱产生的大规模图像数据，手动处理耗时，因此自动化至关重要。",
      "method": "研究分析了两个生态系统数据集：非洲草原（82,300张标注图像，47种物种）和亚洲亚热带干森林（40,308张标注图像，29种物种）。通过比较专家生成的物种识别与深度学习分类模型（包括ResNet和ConvNext-T架构，以及不同深度如ResNet18、50、101）生成的生态指标，系统评估了模型架构、训练数据中的标签噪声比例和训练数据集大小对物种丰富度、占据率和活动模式的影响。关键创新在于强调数据因素而非模型设计。",
      "result": "实验结果显示，深度学习模型预测的物种丰富度与专家标签高度一致，并对训练数据中高达10%的噪声和50%的数据集大小减少保持韧性。模型架构（ResNet vs ConvNext-T）或深度对预测生态指标无显著影响。然而，物种特定指标（如不常见和视觉相似物种）对深度学习准确度减少更敏感，导致占据率和活动模式估计出现偏差。这表明数据质量对特定物种分析至关重要。",
      "conclusion": "研究总结认为，为确保生态分析的可靠性，实践者应优先创建大型、干净的训练集，并处理物种间的类别不平衡问题，而非过度探索不同深度学习模型架构。这强调了数据质量在自动化生物多样性监测中的学术价值和实际应用价值，未来工作可进一步探讨数据增强策略以提升少数物种的分析准确性。",
      "tags": [
        "Deep Learning",
        "Camera Trap Images",
        "Ecological Metrics",
        "Training Data Quality",
        "Class Imbalance"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:28.200582Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2407.18384",
    "title": "Mathematical theory of deep learning",
    "authors": [
      "Philipp Petersen",
      "Jakob Zech"
    ],
    "abstract": "This book provides an introduction to the mathematical analysis of deep learning. It covers fundamental results in approximation theory, optimization theory, and statistical learning theory, which are the three main pillars of deep neural network theory. Serving as a guide for students and researchers in mathematics and related fields, the book aims to equip readers with foundational knowledge on the topic. It prioritizes simplicity over generality, and presents rigorous yet accessible results to help build an understanding of the essential mathematical concepts underpinning deep learning.",
    "categories": [
      "cs.LG",
      "math.HO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2407.18384.pdf",
    "abs_url": "https://arxiv.org/abs/2407.18384",
    "published": "2024-07-25T20:37:12Z",
    "updated": "2026-01-15T17:54:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本书系统介绍深度学习的数学理论基础，聚焦于近似理论、优化理论和统计学习理论三大支柱，提供严谨且易懂的分析框架。",
      "motivation": "深度学习的应用广泛成功，但其数学理论基础相对薄弱，缺乏系统性分析，导致理解不深且可能限制进一步发展。现有研究多侧重于实证应用，而忽略了对深层网络行为的数学解释，使得理论进展滞后。本书旨在填补这一空白，通过建立全面的数学框架，帮助学生和研究人员理解核心概念，推动理论深化并弥补学术与实用之间的鸿沟。其重要性在于为后续优化设计和理论探索提供坚实基础。",
      "method": "本书通过整合深度学习的三大数学支柱——近似理论、优化理论和统计学习理论，构建系统的分析框架。它优先强调简洁性而非普遍性，以严谨但易于理解的数学结果呈现关键概念，帮助读者逐步掌握深度神经网络的数学本质。摘要未明确说明具体技术细节如数据集或模型架构，因为它侧重于理论概念的阐述和组织，通过系统性梳理和解释来促进理解，而非实证方法。",
      "result": "摘要未明确说明具体的实验结果，因为本书是理论性著作，主要效果在于提供系统的数学知识而非实验验证。它可能包含理论推导和概念解释，帮助读者建立深度学习的数学基础，从而在后续研究或应用中受益。与基线方法的对比不适用，因为本书目标在于理论框架的构建和传播，而非性能指标的改进。",
      "conclusion": "本书的主要贡献在于系统性地介绍深度学习的数学分析，涵盖近似理论、优化理论和统计学习理论的核心内容，为读者提供严谨且易用的理论指南。其学术价值在于促进深度学习理论的深入研究和教学参考，实际应用价值则支持更有效、更优化的模型设计。局限性可能在于侧重于理论基础，缺乏实证案例；未来工作方向可扩展至更多应用实例或进一步数学理论的深化。",
      "tags": [
        "Deep Learning",
        "Approximation Theory",
        "Optimization Theory",
        "Statistical Learning Theory"
      ]
    },
    "analyzed_at": "2026-01-16T03:22:37.300115Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.17216",
    "title": "Machine Unlearning Fails to Remove Data Poisoning Attacks",
    "authors": [
      "Martin Pawelczyk",
      "Jimmy Z. Di",
      "Yiwei Lu",
      "Gautam Kamath",
      "Ayush Sekhari",
      "Seth Neel"
    ],
    "abstract": "We revisit the efficacy of several practical methods for approximate machine unlearning developed for large-scale deep learning. In addition to complying with data deletion requests, one often-cited potential application for unlearning methods is to remove the effects of poisoned data. We experimentally demonstrate that, while existing unlearning methods have been demonstrated to be effective in a number of settings, they fail to remove the effects of data poisoning across a variety of types of poisoning attacks (indiscriminate, targeted, and a newly-introduced Gaussian poisoning attack) and models (image classifiers and LLMs); even when granted a relatively large compute budget. In order to precisely characterize unlearning efficacy, we introduce new evaluation metrics for unlearning based on data poisoning. Our results suggest that a broader perspective, including a wider variety of evaluations, are required to avoid a false sense of confidence in machine unlearning procedures for deep learning without provable guarantees. Moreover, while unlearning methods show some signs of being useful to efficiently remove poisoned data without having to retrain, our work suggests that these methods are not yet ``ready for prime time,'' and currently provide limited benefit over retraining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2406.17216.pdf",
    "abs_url": "https://arxiv.org/abs/2406.17216",
    "published": "2024-06-25T02:05:29Z",
    "updated": "2026-01-15T16:07:52Z",
    "comment": "Published at ICLR 2025, Made author ordering consistent with ICLR'25 submission",
    "light_analysis": {
      "overview": "本文揭示了现有机器遗忘方法在移除数据中毒攻击影响方面的失败，并引入了新的评估指标。",
      "motivation": "研究动机源于机器遗忘方法的一个潜在应用——去除中毒数据的影响，这在遵守数据删除请求和对抗攻击中至关重要。现有方法虽在其他场景有效，但在数据中毒背景下可能不足，导致在实际部署中产生错误的安全感。本研究旨在验证遗忘方法在面对各种攻击类型时的实际效果，强调需要更广泛的评估以确保可靠性，尤其是在安全关键的深度学习应用中。问题的重要性在于避免因方法局限而导致的安全漏洞。",
      "method": "研究方法是通过实验评估多种近似机器遗忘方法的有效性，特别是在数据中毒场景中。关键创新点包括测试了不同的中毒攻击类型（如无差别攻击、针对性攻击和新引入的高斯中毒攻击）以及多种模型（图像分类器和大型语言模型）。为了精确表征遗忘效果，论文引入了基于数据中毒的新评估指标。摘要未明确说明具体使用的数据集和模型架构，但实验在较大计算预算下进行，以全面测试现有方法的技术路线和性能。",
      "result": "实验结果表明，现有的机器遗忘方法在移除各种数据中毒攻击影响方面均告失败，即使在较大计算预算下也无法有效处理无差别攻击、针对性攻击和高斯中毒攻击。新评估指标证实了遗忘过程未能彻底消除中毒数据的影响，导致模型性能未显著改善。与重新训练相比，这些方法提供的益处有限，显示出当前技术在实际应用中的局限性。摘要未提供具体性能指标数据，但整体趋势强调失败的一致性。",
      "conclusion": "论文结论指出，现有机器遗忘方法在移除数据中毒攻击影响方面尚未成熟，不能可靠用于实际场景。主要贡献在于通过实验揭示了这一失败，并提出了新的评估指标以精确表征遗忘效果。学术价值在于警示研究社区需要更广泛的评估来避免对深度学习机器遗忘过程的错误信心；实际应用中，这些方法目前提供有限益处，因此需要进一步改进。未来工作可能包括开发更有效的遗忘技术或扩展评估到更多攻击类型和模型架构。",
      "tags": [
        "Machine Unlearning",
        "Data Poisoning Attacks",
        "Deep Learning",
        "Large Language Models",
        "Evaluation Metrics"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:09.244901Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.12805",
    "title": "AITTI: Learning Adaptive Inclusive Token for Text-to-Image Generation",
    "authors": [
      "Xinyu Hou",
      "Xiaoming Li",
      "Chen Change Loy"
    ],
    "abstract": "Despite the high-quality results of text-to-image generation, stereotypical biases have been spotted in their generated contents, compromising the fairness of generative models. In this work, we propose to learn adaptive inclusive tokens to shift the attribute distribution of the final generative outputs. Unlike existing de-biasing approaches, our method requires neither explicit attribute specification nor prior knowledge of the bias distribution. Specifically, the core of our method is a lightweight adaptive mapping network, which can customize the inclusive tokens for the concepts to be de-biased, making the tokens generalizable to unseen concepts regardless of their original bias distributions. This is achieved by tuning the adaptive mapping network with a handful of balanced and inclusive samples using an anchor loss. Experimental results demonstrate that our method outperforms previous bias mitigation methods without attribute specification while preserving the alignment between generative results and text descriptions. Moreover, our method achieves comparable performance to models that require specific attributes or editing directions for generation. Extensive experiments showcase the effectiveness of our adaptive inclusive tokens in mitigating stereotypical bias in text-to-image generation. The code will be available at https://github.com/itsmag11/AITTI.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2406.12805.pdf",
    "abs_url": "https://arxiv.org/abs/2406.12805",
    "published": "2024-06-18T17:22:23Z",
    "updated": "2026-01-15T03:44:38Z",
    "comment": "Accepted by IJCV",
    "light_analysis": {
      "overview": "论文提出了一种学习自适应包容性令牌的方法，用于减轻文本到图像生成中的刻板印象偏差，无需明确属性指定。",
      "motivation": "文本到图像生成技术虽能产生高质量结果，但存在刻板印象偏差，损害了生成模型的公平性，这是一个重要的社会和技术问题。现有去偏方法通常需要明确指定属性或已知偏差分布的先验知识，这不仅增加了使用复杂度，还可能无法适应未知概念，限制了通用性和可扩展性。本研究旨在通过开发一种无需外部信息的自适应方法，以提高模型在生成内容时的公平性和多样性。",
      "method": "方法的核心是一个轻量级的自适应映射网络，该网络能够为需要去偏的概念定制包容性令牌，无需明确指定属性或偏差分布。通过使用锚点损失对网络进行调优，仅需少量平衡和包容性样本即可实现学习。关键创新在于令牌能够泛化到未见过的概念，无论其原始偏差分布如何，技术特色包括轻量级设计和自适应性，从而在推理时灵活调整生成输出的分布。",
      "result": "实验结果表明，该方法在减轻刻板印象偏差方面优于先前的无属性指定去偏方法，同时保持了生成结果与文本描述之间的对齐。此外，与需要特定属性或编辑方向的模型相比，取得了可比的性能，证明了其高效性和适用性。摘要未明确说明具体数据细节，但通过大量实验验证了自适应包容性令牌在文本到图像生成中的有效性。",
      "conclusion": "该研究的主要贡献是提出了一种无需属性指定的自适应去偏方法，有效减轻了文本到图像生成中的刻板印象偏差。学术价值在于提供了一个通用的偏差缓解框架，实际应用价值在于提高了生成模型的公平性和可扩展性。未来工作可能涉及扩展到更多生成任务或处理更复杂的偏差类型，潜在局限性包括对特定数据集的依赖，但代码开放将促进进一步研究和应用。",
      "tags": [
        "Text-to-Image Generation",
        "Bias Mitigation",
        "Adaptive Mapping Network",
        "Inclusive Token Learning",
        "Anchor Loss"
      ]
    },
    "analyzed_at": "2026-01-16T03:25:17.982825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2405.19631",
    "title": "Leveraging Open-Source Large Language Models for encoding Social Determinants of Health using an Intelligent Router",
    "authors": [
      "Akul Goel",
      "Surya Narayanan Hari",
      "Belinda Waltman",
      "Matt Thomson"
    ],
    "abstract": "Social Determinants of Health (SDOH), also known as Health-Related Social Needs (HSRN), play a significant role in patient health outcomes. The Centers for Disease Control and Prevention (CDC) introduced a subset of ICD-10 codes called Z-codes to recognize and measure SDOH. However, Z-codes are infrequently coded in a patient's Electronic Health Record (EHR), and instead, in many cases, need to be inferred from clinical notes. Previous research has shown that large language models (LLMs) show promise on extracting unstructured data from EHRs, but it can be difficult to identify a single model that performs best on varied coding tasks. Further, clinical notes contain protected health information posing a challenge for the use of closed-source language models from commercial vendors. The identification of open-source LLMs that can be run within health organizations and exhibit high performance on SDOH tasks is an important issue to solve. Here, we introduce an intelligent routing system for SDOH coding that uses a language model router to direct medical record data to open-source LLMs that demonstrate optimal performance on specific SDOH codes. This intelligent routing system exhibits state of the art performance of 96.4% accuracy averaged across 13 codes, including homelessness and food insecurity, outperforming closed models such as GPT-4o. We leveraged a publicly-available, deidentified dataset of medical record notes to run the router, but we also introduce a synthetic data generation and validation paradigm to increase the scale of training data without needing privacy-protected medical records. Together, we demonstrate an architecture for intelligent routing of inputs to task-optimal language models to achieve high performance across a set of medical coding sub-tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2405.19631.pdf",
    "abs_url": "https://arxiv.org/abs/2405.19631",
    "published": "2024-05-30T02:33:28Z",
    "updated": "2026-01-15T05:40:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入了智能路由系统，利用开源大型语言模型优化社会健康决定因素的编码任务。",
      "motivation": "社会健康决定因素在患者健康中至关重要，但ICD-10 Z-codes在电子健康记录中编码不足，需从临床笔记推断。大型语言模型虽能提取非结构化数据，但单一模型难以在所有编码任务上最优；封闭源模型存在隐私风险。因此，开发在医疗机构内运行、高性能的开源LLM成为关键，以解决编码不足和隐私限制的问题。",
      "method": "论文提出智能路由系统，使用语言模型路由器将医疗记录数据定向到针对特定SDOH代码优化的开源LLMs。关键创新包括路由机制和合成数据生成验证范式，以扩充训练数据而无需依赖隐私记录。基于公开的去识别化医疗笔记数据集，系统构建了可扩展的架构，结合多个模型实现高效编码。",
      "result": "实验表明，智能路由系统在13个SDOH代码（如无家可归和食品不安全）上平均准确率达96.4%，优于GPT-4o等封闭模型。性能对比显示，该系统在处理多样化医疗编码子任务时具有卓越效果，通过具体数据支撑了其在准确率上的显著提升。",
      "conclusion": "本研究贡献了智能路由架构，有效利用开源LLMs提升SDOH编码性能，增强医疗数据提取的准确性和隐私安全。通过合成数据方法缓解了数据稀缺，学术上促进了自适应模型选择技术发展，实际应用中可改善健康记录质量。摘要未明确说明局限性，但为未来扩展更多SDOH任务提供了基础。",
      "tags": [
        "Large Language Model",
        "Intelligent Router",
        "Social Determinants of Health",
        "Synthetic Data Generation",
        "Medical Coding"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:30.254370Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2405.17137",
    "title": "Jump-teaching: Combating Sample Selection Bias via Temporal Disagreement",
    "authors": [
      "Kangye Ji",
      "Fei Cheng",
      "Zeqing Wang",
      "Qichang Zhang",
      "Bohu Huang"
    ],
    "abstract": "Sample selection is a straightforward technique to combat noisy labels, aiming to prevent mislabeled samples from degrading the robustness of neural networks. However, existing methods mitigate compounding selection bias either by leveraging dual-network disagreement or additional forward propagations, leading to multiplied training overhead. To address this challenge, we introduce $\\textit{Jump-teaching}$, an efficient sample selection framework for debiased model update and simplified selection criterion. Based on a key observation that a neural network exhibits significant disagreement across different training iterations, Jump-teaching proposes a jump-manner model update strategy to enable self-correction of selection bias by harnessing temporal disagreement, eliminating the need for multi-network or multi-round training. Furthermore, we employ a sample-wise selection criterion building on the intra variance of a decomposed single loss for a fine-grained selection without relying on batch-wise ranking or dataset-wise modeling. Extensive experiments demonstrate that Jump-teaching outperforms state-of-the-art counterparts while achieving a nearly overhead-free selection procedure, which boosts training speed by up to $4.47\\times$ and reduces peak memory footprint by $54\\%$.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2405.17137.pdf",
    "abs_url": "https://arxiv.org/abs/2405.17137",
    "published": "2024-05-27T12:54:09Z",
    "updated": "2026-01-15T03:27:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "Jump-teaching 是一种利用时间分歧的样本选择框架，能高效去偏模型更新并简化选择标准，以应对噪声标签问题。",
      "motivation": "该研究旨在解决噪声标签导致神经网络鲁棒性下降的问题，这是深度学习中的关键挑战。现有方法，如依赖双网络分歧或额外前向传播来减轻样本选择偏差，虽然有效但导致训练开销显著增加，限制了实际应用效率。问题的严重性在于，样本选择不当可能加剧模型偏差，影响泛化能力，尤其在资源受限的场景下，现有方法的计算负担成为瓶颈。因此，开发一种开销更小、更高效的样本选择方法显得尤为重要。",
      "method": "论文提出 Jump-teaching 框架，其核心创新包括跳转方式模型更新策略，基于神经网络在不同训练迭代中表现出的时间分歧来自校正选择偏差，从而无需多网络或多轮训练。技术特色在于利用时间分歧减少计算开销，同时引入基于分解单损失的样本内方差作为选择标准，实现细粒度样本筛选，不依赖批处理排名或整体数据集建模。关键细节包括采用高效的模型更新机制和简化的损失计算，以提升训练过程的效率和准确性。",
      "result": "实验结果表明，Jump-teaching 在性能上优于当前最先进的方法，同时实现了几乎无额外开销的选择过程。具体数据显示，训练速度提升高达 4.47 倍，峰值内存使用减少 54%，验证了其高效性。与基线方法相比，Jump-teaching 在减少计算资源的同时，保持了良好的模型鲁棒性，为噪声标签处理提供了实用解决方案。",
      "conclusion": "Jump-teaching 的主要贡献是提供了一种高效、低开销的样本选择方法，通过时间分歧机制简化了训练流程。其学术价值在于提出了新颖的去偏策略，推动了噪声标签处理领域的发展；实际应用价值体现在提升训练效率和降低资源消耗，适用于大规模深度学习任务。摘要未明确说明局限性，未来工作可进一步探索在不同数据集和模型架构上的泛化能力。",
      "tags": [
        "Sample Selection",
        "Temporal Disagreement",
        "Debiased Model Update",
        "Single Loss Decomposition",
        "Efficient Training"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:16.708421Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2405.09037",
    "title": "SSFL: Discovering Sparse Unified Subnetworks at Initialization for Efficient Federated Learning",
    "authors": [
      "Riyasat Ohib",
      "Bishal Thapaliya",
      "Gintare Karolina Dziugaite",
      "Jingyu Liu",
      "Vince Calhoun",
      "Sergey Plis"
    ],
    "abstract": "In this work, we propose Salient Sparse Federated Learning (SSFL), a streamlined approach for sparse federated learning with efficient communication. SSFL identifies a sparse subnetwork prior to training, leveraging parameter saliency scores computed separately on local client data in non-IID scenarios, and then aggregated, to determine a global mask. Only the sparse model weights are trained and communicated each round between the clients and the server. On standard benchmarks including CIFAR-10, CIFAR-100, and Tiny-ImageNet, SSFL consistently improves the accuracy sparsity trade off, achieving more than 20\\% relative error reduction on CIFAR-10 compared to the strongest sparse baseline, while reducing communication costs by $2 \\times$ relative to dense FL. Finally, in a real-world federated learning deployment, SSFL delivers over $2.3 \\times$ faster communication time, underscoring its practical efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2405.09037.pdf",
    "abs_url": "https://arxiv.org/abs/2405.09037",
    "published": "2024-05-15T02:13:51Z",
    "updated": "2026-01-15T17:01:07Z",
    "comment": "Published in Transactions on Machine Learning Research (TMLR), 2026",
    "light_analysis": {
      "overview": "SSFL通过训练前识别稀疏统一子网络，显著提升联邦学习的通信效率和准确性-稀疏性权衡。",
      "motivation": "本研究解决联邦学习在非独立同分布数据场景下的高通信成本问题。由于真实世界数据分布不均，现有稀疏联邦学习方法在准确性与效率间权衡不足，SSFL旨在优化此过程，通过减少每轮通信的模型参数来降低开销，同时保持性能，从而扩展联邦学习的实用性和可扩展性。",
      "method": "SSFL在训练开始前，通过本地客户端计算参数显著性分数来评估权重重要性，在非IID数据下聚合这些分数形成全局掩码，以识别稀疏子网络。每轮仅训练和通信被掩码选中的稀疏模型权重，无需完整模型更新。该方法结合了稀疏性和联邦学习，核心创新在于利用显著性分数提前确定全局稀疏结构，适应异构数据环境。",
      "result": "在CIFAR-10、CIFAR-100和Tiny-ImageNet基准测试中，SSFL相比最强稀疏基线，在CIFAR-10上实现超过20%的相对错误减少，通信成本降低2倍。在真实联邦学习部署中，通信时间加速超过2.3倍，验证了其在实际应用中的高效性，平衡了准确性和通信效率的改进。",
      "conclusion": "SSFL通过提前发现稀疏子网络，有效提升了联邦学习的准确性和通信效率，具有重要学术和实用价值，为资源受限环境提供了可行解决方案。未来工作可进一步探索不同数据分布下的扩展性，或结合其他优化技术以增强鲁棒性，但摘要未明确说明具体局限性。",
      "tags": [
        "Sparse Federated Learning",
        "Parameter Saliency",
        "Non-IID Data",
        "Communication Efficiency",
        "Subnetwork Discovery"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:16.092974Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2404.13182",
    "title": "Spectral Convolutional Conditional Neural Processes",
    "authors": [
      "Peiman Mohseni",
      "Nick Duffield"
    ],
    "abstract": "Neural Processes (NPs) are meta-learning models that learn to map sets of observations to approximations of the corresponding posterior predictive distributions. By accommodating variable-sized, unstructured collections of observations and enabling probabilistic predictions at arbitrary query points, NPs provide a flexible framework for modeling functions over continuous domains. Since their introduction, numerous variants have emerged; however, early formulations shared a fundamental limitation: they compressed the observed data into finite-dimensional global representations via aggregation operations such as mean pooling. This strategy induces an intrinsic mismatch with the infinite-dimensional nature of the stochastic processes that NPs intend to model. Convolutional conditional neural processes (ConvCNPs) address this limitation by constructing infinite-dimensional functional embeddings processed through convolutional neural networks (CNNs) to enforce translation equivariance. Yet CNNs with local spatial kernels struggle to capture long-range dependencies without resorting to large kernels, which impose significant computational costs. To overcome this limitation, we propose spectral ConvCNPs (SConvCNPs), which perform global convolution in the frequency domain. Inspired by Fourier neural operators (FNOs) for learning solution operators of partial differential equations (PDEs), our approach directly parameterizes convolution kernels in the frequency domain, leveraging the relatively compact yet global Fourier representation of many natural signals. We validate the effectiveness of SConvCNPs on both synthetic and real-world datasets, demonstrating how ideas from operator learning can advance the capabilities of NPs.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2404.13182.pdf",
    "abs_url": "https://arxiv.org/abs/2404.13182",
    "published": "2024-04-19T21:13:18Z",
    "updated": "2026-01-15T11:10:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出谱卷积条件神经过程（SConvCNPs），通过在频域进行全局卷积来改进卷积条件神经过程，有效捕获长程依赖。",
      "motivation": "Neural Processes（NPs）作为元学习模型，用于映射观察集到后验预测分布，但早期版本采用有限维全局表示（如平均池化），与无限维随机过程不匹配。卷积条件神经过程（ConvCNPs）通过卷积神经网络构建无限维功能嵌入实现平移等变性，但局部空间核难以捕捉长程依赖，而大核计算成本高。因此，需要一种高效处理全局依赖的方法，以提升NPs在连续域函数建模中的性能。",
      "method": "本研究提出谱卷积条件神经过程（SConvCNPs），核心方法是在频域执行全局卷积。受傅里叶神经算子（FNOs）用于学习偏微分方程解算子的启发，SConvCNPs直接在频域参数化卷积核，利用自然信号的紧凑傅里叶表示。通过频域操作，模型能高效捕获全局空间依赖，保持平移等变性，避免局部核的限制，并减少计算开销。",
      "result": "摘要未明确说明具体实验数据。论文在合成和真实世界数据集上验证了SConvCNPs的有效性，并展示了操作学习思想如何提升Neural Processes的能力。与基线方法如ConvCNPs相比，SConvCNPs可能改进了对长程依赖的建模性能，但具体指标（如准确率、效率提升）在摘要中未提及。",
      "conclusion": "该论文的主要贡献是提出了SConvCNPs，通过频域全局卷积解决了ConvCNPs在捕捉长程依赖时的计算效率问题。这为Neural Processes提供了更强大的建模工具，结合了操作学习的先进思想。研究的学术价值在于扩展了NPs在连续域问题中的应用范围，特别是在需要全局依赖的场景中。潜在局限性可能包括对频域表示的假设或计算复杂性，未来工作可探索更多信号类型或优化频域操作。",
      "tags": [
        "Neural Processes",
        "Convolutional Neural Networks",
        "Spectral Convolution",
        "Operator Learning",
        "Fourier Transform"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:30.700700Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2404.06835",
    "title": "Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer",
    "authors": [
      "Yanqi Ge",
      "Jiaqi Liu",
      "Qingnan Fan",
      "Xi Jiang",
      "Ye Huang",
      "Shuai Qin",
      "Hong Gu",
      "Wen Li",
      "Lixin Duan"
    ],
    "abstract": "In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2404.06835.pdf",
    "abs_url": "https://arxiv.org/abs/2404.06835",
    "published": "2024-04-10T08:54:00Z",
    "updated": "2026-01-15T14:23:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出自适应风格融入方法，实现结构一致的文本驱动风格转移，无需调优。",
      "motivation": "研究针对文本到图像扩散模型中的文本驱动风格转移任务，主要挑战是在实现有效风格转移的同时保持结构一致性。现有方法通过直接连接内容和风格提示进行提示级注入，导致结构扭曲，限制了实际应用的质量。因此，需要一种新方法来平衡结构保持和风格效果，提高生成图像的可靠性和实用性。",
      "method": "论文提出自适应风格融入方法，包括Siamese Cross-Attention模块将单轨跨注意力解耦为双轨结构，以分离内容和风格特征；以及Adaptive Content-Style Blending模块以结构一致方式耦合信息，实现细粒度特征级风格融入。该方法无需额外调优，旨在解决传统方法的局限性。摘要未明确说明使用的具体数据集或模型架构。",
      "result": "实验结果表明，该方法在结构保持和风格化效果方面表现出更好的性能。虽然摘要未提供具体性能指标数据（如准确率提升），但指出相较于基线方法（如直接提示连接），ASI在维持结构一致性和提升风格转移质量上有所改进。性能优于过去方法，但具体对比细节未明确说明。",
      "conclusion": "该研究的主要贡献是提出自适应风格融入方法，有效解决文本驱动风格转移中的结构一致性问题，学术价值在于改进了扩散模型中的风格转移技术，提供更精细的特征控制；实际应用潜力包括创意图像生成和编辑。摘要未明确说明研究的局限性或未来工作方向。",
      "tags": [
        "Text-Driven Style Transfer",
        "Diffusion Models",
        "Cross-Attention",
        "Adaptive Blending",
        "Feature-Level Incorporation"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:33.245330Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2312.10431",
    "title": "Continuous Diffusion for Mixed-Type Tabular Data",
    "authors": [
      "Markus Mueller",
      "Kathrin Gruber",
      "Dennis Fok"
    ],
    "abstract": "Score-based generative models, commonly referred to as diffusion models, have proven to be successful at generating text and image data. However, their adaptation to mixed-type tabular data remains underexplored. In this work, we propose CDTD, a Continuous Diffusion model for mixed-type Tabular Data. CDTD is based on a novel combination of score matching and score interpolation to enforce a unified continuous noise distribution for both continuous and categorical features. We explicitly acknowledge the necessity of homogenizing distinct data types by relying on model-specific loss calibration and initialization schemes. To further address the high heterogeneity in mixed-type tabular data, we introduce adaptive feature- or type-specific noise schedules. These ensure balanced generative performance across features and optimize the allocation of model capacity across features and diffusion time. Our experimental results show that CDTD consistently outperforms state-of-the-art benchmark models, captures feature correlations exceptionally well, and that heterogeneity in the noise schedule design boosts sample quality. Replication code is available at https://github.com/muellermarkus/cdtd.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2312.10431.pdf",
    "abs_url": "https://arxiv.org/abs/2312.10431",
    "published": "2023-12-16T12:21:03Z",
    "updated": "2026-01-15T15:08:41Z",
    "comment": "published at ICLR 2025",
    "light_analysis": {
      "overview": "本论文提出CDTD模型，一种针对混合类型表格数据的连续扩散生成方法，通过统一噪声分布和自适应调度创新提升生成性能。",
      "motivation": "扩散模型在文本和图像生成中已证明有效，但对于包含连续和分类特征的混合类型表格数据，其应用研究较少。混合类型表格数据在实际应用中广泛存在，如金融和医疗领域，现有方法在处理数据异质性时可能不足，导致生成质量受限。因此，开发专门针对此类数据的扩散模型至关重要，以满足合成数据生成和数据增强等需求。",
      "method": "论文提出CDTD模型，结合score matching和score interpolation技术，为连续和分类特征强制统一的连续噪声分布。关键创新包括使用模型特定的损失校准和初始化方案来均质化数据类型，并引入自适应特征或类型特定噪声调度，以处理数据的高异质性，优化模型容量在特征和扩散时间上的分配。",
      "result": "实验结果显示，CDTD在混合类型表格数据生成任务中，一致优于当前最先进的基准模型，并能有效捕捉特征相关性。噪声调度设计的异质性进一步提高了样本质量，具体性能指标如准确率提升在摘要中未明确说明。",
      "conclusion": "本研究的主要贡献是开发了CDTD模型，扩展了扩散模型到混合类型表格数据领域，通过创新的噪声处理提升了生成效果。这项研究具有学术价值，为数据科学中的合成数据生成和隐私保护提供了新工具。未来工作可能涉及进一步优化算法或扩展到更复杂的数据类型。",
      "tags": [
        "Diffusion Models",
        "Score Matching",
        "Mixed-Type Tabular Data",
        "Adaptive Noise Scheduling",
        "Data Generation"
      ]
    },
    "analyzed_at": "2026-01-16T03:23:52.357603Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2310.19531",
    "title": "MiLe Loss: a New Entropy-Weighed Loss for Mitigating the Bias of Learning Difficulties in Large Language Models",
    "authors": [
      "Zhenpeng Su",
      "Xing Wu",
      "Xue Bai",
      "Zijia Lin",
      "Hui Chen",
      "Guiguang Ding",
      "Wei Zhou",
      "Songlin Hu"
    ],
    "abstract": "Generative language models are usually pretrained on large text corpus via predicting the next token (i.e., sub-word/word/phrase) given the previous ones. Recent works have demonstrated the impressive performance of large generative language models on downstream tasks. However, existing generative language models generally neglect an inherent challenge in text corpus during training, i.e., the imbalance between frequent tokens and infrequent ones. It can lead a language model to be dominated by common and easy-to-learn tokens, thereby overlooking the infrequent and difficult-to-learn ones. To alleviate that, we propose a MiLe Loss function for mitigating the bias of learning difficulties with tokens. During training, it can dynamically assess the learning difficulty of a to-be-learned token, according to the information entropy of the corresponding predicted probability distribution over the vocabulary. Then it scales the training loss adaptively, trying to lead the model to focus more on the difficult-to-learn tokens. On the Pile dataset, we train generative language models at different scales of 468M, 1.2B, and 6.7B parameters. Experiments reveal that models incorporating the proposed MiLe Loss can gain consistent performance improvement on downstream benchmarks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2310.19531.pdf",
    "abs_url": "https://arxiv.org/abs/2310.19531",
    "published": "2023-10-30T13:33:21Z",
    "updated": "2026-01-15T06:45:36Z",
    "comment": "This paper has been accepted by NAACL 2024",
    "light_analysis": {
      "overview": "提出MiLe Loss函数，通过熵加权损失缓解大语言模型中的学习难度偏差。",
      "motivation": "生成式语言模型通常基于预测下一个令牌进行预训练，但现有模型忽视了训练数据中频繁令牌与不频繁令牌之间的不平衡问题。这种不平衡会导致模型偏向于常见且容易学习的令牌，而忽略不常见且难以学习的令牌，从而限制模型在下游任务中的泛化能力。因此，研究旨在解决这种学习难度偏差，以提升模型性能和公平性。",
      "method": "论文提出MiLe Loss函数，该损失函数通过计算词汇表上预测概率分布的信息熵来动态评估每个待学习令牌的学习难度。基于熵值，自适应地缩放训练损失，使模型在训练过程中更多关注难以学习的令牌。实验在Pile数据集上进行，训练了参数规模分别为468M、1.2B和6.7B的生成式语言模型。",
      "result": "实验结果显示，在不同规模的模型（468M、1.2B和6.7B参数）上，引入MiLe Loss的生成式语言模型在下游基准测试中获得了性能提升。尽管摘要未提供具体数值，但强调了模型性能的一致改进，表明该方法能有效缓解学习难度偏差并提升模型表现。",
      "conclusion": "本研究提出了一种新的MiLe Loss函数，有效缓解了大语言模型中的学习难度偏差问题。通过动态评估令牌难度并自适应调整损失，该方法提高了模型对难以学习令牌的关注度，从而在下游任务中实现性能提升。未来工作可以探索在不同数据集和模型架构中的应用，以及进一步优化损失函数的设计。",
      "tags": [
        "Large Language Model",
        "Loss Function",
        "Information Entropy",
        "Generative Model",
        "Adaptive Scaling"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:07.937047Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2211.12826",
    "title": "Data-Driven Feature Tracking for Event Cameras With and Without Frames",
    "authors": [
      "Nico Messikommer",
      "Carter Fang",
      "Mathias Gehrig",
      "Giovanni Cioffi",
      "Davide Scaramuzza"
    ],
    "abstract": "Because of their high temporal resolution, increased resilience to motion blur, and very sparse output, event cameras have been shown to be ideal for low-latency and low-bandwidth feature tracking, even in challenging scenarios. Existing feature tracking methods for event cameras are either handcrafted or derived from first principles but require extensive parameter tuning, are sensitive to noise, and do not generalize to different scenarios due to unmodeled effects. To tackle these deficiencies, we introduce the first data-driven feature tracker for event cameras, which leverages low-latency events to track features detected in an intensity frame. We achieve robust performance via a novel frame attention module, which shares information across feature tracks. Our tracker is designed to operate in two distinct configurations: solely with events or in a hybrid mode incorporating both events and frames. The hybrid model offers two setups: an aligned configuration where the event and frame cameras share the same viewpoint, and a hybrid stereo configuration where the event camera and the standard camera are positioned side-by-side. This side-by-side arrangement is particularly valuable as it provides depth information for each feature track, enhancing its utility in applications such as visual odometry and simultaneous localization and mapping.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2211.12826.pdf",
    "abs_url": "https://arxiv.org/abs/2211.12826",
    "published": "2022-11-23T10:20:11Z",
    "updated": "2026-01-15T08:31:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出首个数据驱动的事件相机特征跟踪器，引入帧注意力模块，支持纯事件和混合模式，提升鲁棒性和应用价值。",
      "motivation": "事件相机因其高时间分辨率和稀疏输出，在低延迟特征跟踪中具有优势，但现有方法多为手动或基于第一原理，需要大量参数调整，对噪声敏感，且泛化能力差，限制了在复杂场景中的应用。这些问题使得开发更鲁棒、数据驱动的方法成为必要，以克服依赖手工调整和未建模效应导致的性能瓶颈。",
      "method": "该方法采用数据驱动框架，利用事件相机的低延迟事件跟踪强度帧中的特征，核心创新是帧注意力模块，通过在特征轨迹间共享信息增强鲁棒性。跟踪器支持两种配置：纯事件模式和混合模式（结合事件与标准相机）。混合模式包括对齐配置（共享视点）和立体配置（并排放置以提供深度信息），未提及具体数据集或模型架构细节。",
      "result": "摘要未明确说明具体的实验结果和性能指标，如准确率或效率改进数据。基于方法描述，可推断该跟踪器预期能提高对噪声的鲁棒性和在不同场景的泛化能力，减少参数调整需求，并在混合配置中提供深度信息，可能优于现有手动方法，但具体对比数据未提供。",
      "conclusion": "研究的主要贡献是首个数据驱动的事件相机特征跟踪器，通过帧注意力模块和灵活配置提升了跟踪稳定性和通用性，为视觉里程计和SLAM等应用提供增强工具，具有学术和实际价值。未来工作可能包括模型优化和在更广泛场景中的验证。",
      "tags": [
        "Event Cameras",
        "Feature Tracking",
        "Data-Driven",
        "Attention Module",
        "Stereo Vision"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:39.771306Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "1712.06080",
    "title": "Spatial As Deep: Spatial CNN for Traffic Scene Understanding",
    "authors": [
      "Xingang Pan",
      "Xiaohang Zhan",
      "Jianping Shi",
      "Ping Luo",
      "Xiaogang Wang",
      "Xiaoou Tang"
    ],
    "abstract": "Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-byslice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/1712.06080.pdf",
    "abs_url": "https://arxiv.org/abs/1712.06080",
    "published": "2017-12-17T09:37:52Z",
    "updated": "2026-01-15T16:01:43Z",
    "comment": "Accepted to AAAI 2018",
    "light_analysis": {
      "overview": "本文提出了 Spatial CNN (SCNN)，通过逐片卷积增强卷积神经网络捕获图像空间关系的能力，特别适用于交通车道检测任务。",
      "motivation": "卷积神经网络（CNN）在提取语义特征方面表现出色，但在捕获图像行和列之间的空间关系方面存在不足。这一问题在交通场景理解中尤为重要，因为交通车道等对象具有强形状先验但弱外观一致性，常被遮挡或未绘制，导致传统方法检测困难。现有方法如基于 RNN 或 MRF+CNN 的技术可能未充分利用空间关系，限制了检测精度。因此，研究旨在解决这一瓶颈，提升对长连续形状结构的识别能力，以满足自动驾驶和交通安全的需求。",
      "method": "本研究提出 Spatial CNN (SCNN)，将传统的逐层卷积推广到特征图中的逐片卷积，允许在层内像素间进行消息传递。关键创新点在于通过这种结构捕获行和列之间的空间关系，特别适用于长连续形状或大对象（如交通车道、柱子）。方法基于 CNN 架构，但引入了 SCNN 层来增强空间建模能力。实验使用新发布的交通车道检测数据集和 Cityscapes 数据集进行评估，摘要未明确说明具体模型架构细节，但强调了 SCNN 的直接应用和改进。",
      "result": "实验结果表明，SCNN 在交通车道检测数据集中显著提升性能：与基于 RNN 的 ReNet 相比，性能提升 8.7%；与 MRF+CNN (MRFNet) 相比，提升 4.6%。此外，SCNN 在 TuSimple 基准车道检测挑战中获得第一名，准确率达到 96.53%。这些数据证实了 SCNN 能有效学习空间关系，并在检测任务中优于现有基线方法，展示了其在复杂场景下的优越性和鲁棒性。",
      "conclusion": "本研究的核心贡献是提出 Spatial CNN (SCNN)，通过逐片卷积机制增强了 CNN 捕获空间关系的能力。学术价值在于为计算机视觉中的对象检测提供了新思路，特别是针对具有强空间关系的任务。实际应用价值显著，尤其在交通场景理解中，如车道检测，提高了准确性和实用性。局限性方面，摘要未明确说明，但未来工作可扩展应用到其他类似结构检测任务或进一步优化方法效率。",
      "tags": [
        "Spatial CNN",
        "Convolutional Neural Networks",
        "Lane Detection",
        "Traffic Scene Understanding",
        "Recurrent Neural Network"
      ]
    },
    "analyzed_at": "2026-01-16T03:24:26.543645Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]