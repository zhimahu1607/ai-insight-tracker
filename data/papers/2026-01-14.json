[
  {
    "id": "2601.08832",
    "title": "RAVEN: Erasing Invisible Watermarks via Novel View Synthesis",
    "authors": [
      "Fahad Shamshad",
      "Nils Lukas",
      "Karthik Nandakumar"
    ],
    "abstract": "Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08832.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08832",
    "published": "2026-01-13T18:59:58Z",
    "updated": "2026-01-13T18:59:58Z",
    "comment": "13 pages",
    "light_analysis": {
      "overview": "论文提出了一种基于视图合成的零样本扩散框架，揭示了不可见水印对语义保持视图变换的根本性脆弱性。",
      "motivation": "不可见水印是认证AI生成图像内容的关键机制，被广泛部署于各大平台。然而，评估这些水印方案对抗高级移除攻击的脆弱性至关重要，以确保其可靠性并指导鲁棒设计。现有水印方法虽然对像素空间和频域攻击鲁棒，但可能忽略了基于语义的视图变换，这暴露了一个关键漏洞，即水印在保持视觉保真的情况下容易被移除，从而强调了评估和加强水印安全性的必要性。",
      "method": "本研究将水印移除重新定义为视图合成问题，核心方法是设计一个零样本扩散框架。该框架在潜在空间应用受控几何变换，模拟场景从不同视角观察，并引入视图引导的对应注意力机制来维持重构时的结构一致性。关键创新在于利用预训练模型（无需水印知识或检测器访问），通过语义保持的视图变换自然去除水印，展示了水印脆弱性的新颖攻击途径。",
      "result": "实验结果显示，该方法在15种不可见水印方法上实现了最先进的水印抑制效果，显著优于14个基线攻击方法。具体来说，在多个数据集上，不仅有效移除了水印，还保持了卓越的感知质量，证明了其在实际应用中的高效性和鲁棒性，为水印安全性评估提供了新的基准。",
      "conclusion": "论文的主要贡献是揭示了不可见水印对视图变换的根本性漏洞，并提出了一种基于扩散的零样本移除方法，为水印鲁棒性评估开辟了新方向。其学术价值在于深化了对水印安全性的理解，实际应用价值则体现在推动更鲁棒的水印设计。摘要未明确说明局限性，但未来工作可能涉及扩展到其他媒体类型或提升方法的通用性。",
      "tags": [
        "Invisible Watermarking",
        "View Synthesis",
        "Diffusion Models",
        "Zero-shot Learning",
        "Geometric Transformations"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:43.475144Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08831",
    "title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "authors": [
      "Yang-Che Sun",
      "Cheng Sun",
      "Chin-Yang Lin",
      "Fu-En Yang",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ],
    "abstract": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08831.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08831",
    "published": "2026-01-13T18:59:54Z",
    "updated": "2026-01-13T18:59:54Z",
    "comment": "Project page: https://jayisaking.github.io/3AM-Page/",
    "light_analysis": {
      "overview": "本文提出3AM方法，通过将MUSt3R的3D感知特征融入SAM2，增强视频物体分割的几何一致性，无需额外输入。",
      "motivation": "视频物体分割方法如SAM2依赖外观特征，在大视角变化下表现不佳，因为几何信息缺失导致一致性差。传统3D实例分割方法虽然解决视角一致性问题，但需要相机位姿、深度图等昂贵预处理，限制了实际应用。本研究旨在开发一种方法，在不依赖额外输入的情况下，实现几何一致的视频分割，以应对现实场景中的动态变化挑战。",
      "method": "本文提出3AM方法，作为训练时增强技术，将MUSt3R中的3D感知特征整合到SAM2架构中。通过轻量级Feature Merger融合MUSt3R的多级特征，这些特征编码隐式几何对应性。结合SAM2的外观特征，模型实现了基于空间位置和视觉相似性的几何一致识别。此外，提出field-of-view aware采样策略，确保帧间观察一致对象区域，以促进可靠3D对应学习。关键创新是仅需RGB输入进行推理，无需相机位姿或预处理。",
      "result": "在具有宽基线运动的挑战性数据集（如ScanNet++和Replica）上，3AM显著优于SAM2及其扩展版本。具体在ScanNet++的Selected Subset上，达到90.6% IoU和71.7% Positive IoU。与最先进的视频物体分割方法相比，IoU提升了15.9个百分点，Positive IoU提升了30.4个百分点，证明了方法在几何一致性方面的卓越性能。",
      "conclusion": "本文的主要贡献是提出3AM方法，有效提升视频物体分割的几何一致性，同时仅使用RGB输入，消除了对相机位姿和预处理的需求，增强了实际应用的可行性和效率。这为无需额外数据的3D感知分割提供了新思路，具有重要学术和实用价值。未来工作可探索更多数据集或扩展至其他计算机视觉任务中。",
      "tags": [
        "Video Object Segmentation",
        "3D Feature Integration",
        "Geometric Consistency",
        "Implicit Geometric Correspondence",
        "Feature Merger"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:48.109981Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08829",
    "title": "Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System",
    "authors": [
      "Hsiang-Wei Huang",
      "Junbin Lu",
      "Kuang-Ming Chen",
      "Jenq-Neng Hwang"
    ],
    "abstract": "In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers' adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08829.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08829",
    "published": "2026-01-13T18:59:17Z",
    "updated": "2026-01-13T18:59:17Z",
    "comment": "In submission. The first two authors contributed equally",
    "light_analysis": {
      "overview": "该论文通过模拟大型语言模型代理审稿人在Elo排名系统中的互动，揭示Elo评分如何提升Area Chair决策准确性和审稿人适应性策略。",
      "motivation": "该研究旨在解决学术会议审稿系统中决策优化的实际问题，因为传统审稿过程可能效率低下或准确性不足，导致决策质量下降。通过探索LLM代理审稿人在Elo排名系统中的动态，研究试图模拟真实审稿互动，以改进现有方法的局限性，如缺乏动态调整和个性化评估。摘要未明确说明具体不足，但强调了使用Elo评分和审稿人记忆来增强系统性能的重要性。",
      "method": "论文采用模拟方法，使用多个具有不同角色的LLM代理审稿人，在Area Chair主持下进行多轮审稿互动。核心创新在于结合Elo评分系统来动态评估审稿人表现，并引入审稿人记忆机制以模拟长期行为模式。技术细节包括使用真实世界会议论文提交作为数据集，并设置基线条件与包含Elo评分和记忆的条件进行比较，以分析审稿动态。框架基于代理模拟，旨在捕捉审稿过程中的复杂交互。",
      "result": "模拟实验结果表明，结合Elo评分显著提高了Area Chair的决策准确性，尽管摘要未提供具体性能指标数据。同时，审稿人发展出适应性策略，能够有效利用Elo系统优化自身表现，而无需增加额外的审稿努力。与基线设置相比，Elo系统的引入带来了积极效果，展示了其在审稿优化中的潜力，但具体量化改进如准确率提升未在摘要中详细说明。",
      "conclusion": "该研究的主要贡献是建立了LLM代理审稿人在Elo排名系统中的动态模型，证明Elo评分能有效提升审稿决策质量。学术上，这为优化审稿过程提供了新视角和方法；实践中，可应用于改进学术会议的审稿系统以提高效率。未来工作可能包括扩展到更多场景或进一步细化模型参数，但摘要未明确提及局限性或具体研究方向。",
      "tags": [
        "Large Language Model",
        "Elo Rating System",
        "Agent-based Simulation",
        "Reviewer Dynamics",
        "Area Chair"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:35.435305Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08828",
    "title": "Motion Attribution for Video Generation",
    "authors": [
      "Xindi Wu",
      "Despoina Paschalidou",
      "Jun Gao",
      "Antonio Torralba",
      "Laura Leal-Taixé",
      "Olga Russakovsky",
      "Sanja Fidler",
      "Jonathan Lorraine"
    ],
    "abstract": "Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08828.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08828",
    "published": "2026-01-13T18:59:09Z",
    "updated": "2026-01-13T18:59:09Z",
    "comment": "See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/",
    "light_analysis": {
      "overview": "提出了名为Motive的框架，首次实现视频生成中运动归因，用于指导数据管理以提升运动质量。",
      "motivation": "当前视频生成模型虽然快速发展，但数据如何影响运动的问题缺乏深入理解，这导致生成的视频在时间动态和物理合理性上存在不足，限制了性能提升。现有方法多关注视觉外观，忽视运动归因，使得运动质量难以优化。Motive框架旨在填补这一空白，通过归因分析识别数据对运动的影响，为模型微调和数据管理提供科学依据，从而提高视频生成的整体效果。",
      "method": "Motive框架是一种运动中心的梯度数据归因方法，通过运动加权损失掩码从静态外观中分离时间动态，实现高效且可扩展的运动特定影响计算。它适用于大规模视频数据集和现代文本到视频模型，如微调剪辑分析，识别对运动有显著影响的片段，并指导数据管理以优化模型训练过程。",
      "result": "实验在VBench基准上进行，使用Motive选择的高影响力数据微调后，模型在运动平滑度和动态程度上均得到显著提升。与预训练基础模型相比，人类偏好胜率达到74.1%，表明该方法能有效改善视频生成的时间一致性和物理合理性，优于传统方法。具体数据显示，该框架通过数据管理优化了运动质量，为视频生成提供了新改进途径。",
      "conclusion": "本论文的主要贡献是提出了首个专注于视频生成中运动归因的框架Motive，并通过数据管理优化了模型性能。这一研究深化了对数据影响运动机制的理解，提升了视频生成的时间一致性和物理合理性，具有重要学术价值和实际应用潜力。未来工作可探索框架在更多视频生成任务中的扩展，以及算法的进一步改进。",
      "tags": [
        "Motion Attribution",
        "Video Generation",
        "Gradient-based Attribution",
        "Data Curation",
        "Temporal Dynamics"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:28.837710Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08811",
    "title": "Reasoning Matters for 3D Visual Grounding",
    "authors": [
      "Hsiang-Wei Huang",
      "Kuang-Ming Chen",
      "Wenhao Chai",
      "Cheng-Yen Yang",
      "Jen-Hao Cheng",
      "Jenq-Neng Hwang"
    ],
    "abstract": "The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08811.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08811",
    "published": "2026-01-13T18:48:41Z",
    "updated": "2026-01-13T18:48:41Z",
    "comment": "2025 CVPR Workshop on 3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments",
    "light_analysis": {
      "overview": "论文提出了一个自动合成带有推理过程的3D视觉 grounding数据管道，并基于此微调LLM开发了Reason3DVG-8B模型，显著提升性能。",
      "motivation": "3D视觉 grounding作为3D理解的基础任务，在当前研究中面临挑战，主要因现有模型的推理能力有限。现有方法通常结合文本和视觉编码器生成跨模态融合特征，但需要大量标注数据进行监督训练，成本高；同时，尽管有研究尝试通过合成数据增强模型，性能提升却与数据收集成本不成比例。因此，开发一种能自动生成高质量数据并提升推理能力的方法至关重要，以解决效率与效果问题。",
      "method": "论文提出一种创新的3D视觉 grounding数据管道，能够自动合成带有相应推理过程的高质量数据。这些数据用于对大型语言模型（LLM）进行微调，引入了Reason3DVG-8B模型，这是一个8B参数的LLM。该方法的关键在于通过自动合成数据减少对大规模标注数据的依赖，同时增强模型的推理能力，以更高效地处理跨模态3D视觉 grounding任务。",
      "result": "实验结果显示，Reason3DVG-8B模型在性能上显著优于之前的LLM-based方法3D-GRAND。具体而言，它仅使用了3D-GRAND训练数据的1.6%，就实现了更好的效果，这表明提出的数据管道在提升模型效率方面具有优势。这种性能提升强调了推理过程在3D视觉 grounding中的关键作用，并验证了合成数据的有效性。",
      "conclusion": "本论文证明了推理能力在3D视觉 grounding中的重要性，并提出了一种高效的数据生成和模型训练方法，减少了大规模标注数据的依赖。该研究为3D视觉 grounding领域提供了新思路，推动了跨模态理解和推理能力的结合，具有学术价值和实际应用潜力，如智能机器人导航。未来工作可能包括优化数据合成过程或扩展至其他视觉任务。",
      "tags": [
        "Large Language Model",
        "3D Visual Grounding",
        "Synthetic Data Generation",
        "Fine-tuning",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:16.196995Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08808",
    "title": "Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge",
    "authors": [
      "Yao Tang",
      "Li Dong",
      "Yaru Hao",
      "Qingxiu Dong",
      "Furu Wei",
      "Jiatao Gu"
    ],
    "abstract": "Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08808.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08808",
    "published": "2026-01-13T18:48:00Z",
    "updated": "2026-01-13T18:48:00Z",
    "comment": "21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking",
    "light_analysis": {
      "overview": "论文提出Multiplex Thinking机制，通过令牌级分支与合并实现自适应软推理，提升推理效率和性能。",
      "motivation": "大型语言模型在处理复杂推理任务时，通常采用思维链（CoT）方法，虽然有效但导致序列过长和带宽低效。相比之下，人类推理基于概率分布进行软性思考，更灵活高效。现有方法如CoT在序列长度和计算资源上存在不足，无法有效模拟人类推理的适应性。因此，本研究旨在开发一种新机制，以结合离散和连续表示，优化推理过程，解决CoT的低效问题，提高任务性能。",
      "method": "论文提出Multiplex Thinking，一种随机软推理机制：在每个思考步骤采样K个候选令牌，并将它们的嵌入聚合成一个连续的多路令牌。这保留了词汇嵌入先验和标准离散生成的采样动态，同时诱导可处理的概率分布，使得多路轨迹可直接用策略强化学习优化。核心创新在于自适应特性：模型自信时多路令牌接近离散行为类似标准CoT；不确定时紧凑表示多个可能下一步，不增加序列长度。摘要未明确说明具体模型架构或数据集细节。",
      "result": "在挑战性数学推理基准测试中，Multiplex Thinking从Pass@1到Pass@1024 consistently outperforms强基线方法（如离散CoT和RL基线）。同时，它生成更短的序列，表明在提升推理性能的同时提高了效率。摘要未提供具体准确率数值，但强调了其相对于基线的优势和改进序列长度。",
      "conclusion": "本研究的核心贡献是提出了Multiplex Thinking，一种自适应软推理机制，有效结合离散和连续表示，优化推理过程。它通过自适应令牌聚合减少序列长度，提升推理效率和性能，学术上为大型语言模型推理提供新方法，实际中可应用于数学推理等复杂任务。未来工作可包括扩展至其他领域或改进自适应策略，摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thought",
        "Reinforcement Learning",
        "Stochastic Reasoning",
        "Adaptive Inference"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:28.114805Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08807",
    "title": "S3-CLIP: Video Super Resolution for Person-ReID",
    "authors": [
      "Tamas Endrei",
      "Gyorgy Cserey"
    ],
    "abstract": "Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08807.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08807",
    "published": "2026-01-13T18:46:37Z",
    "updated": "2026-01-13T18:46:37Z",
    "comment": "Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge",
    "light_analysis": {
      "overview": "提出S3-CLIP框架，首次系统地将视频超分辨率技术集成到行人重识别中，以提升轨迹质量。",
      "motivation": "在行人重识别领域，大多数研究聚焦于基础模型的架构改进，而忽视了轨迹质量这一关键限制因素。这在实际部署中，尤其是在跨视图等困难场景下，会导致性能下降和系统挑战。现有方法未能充分探索视频超分辨率在提升轨迹质量方面的潜力，因此本研究旨在填补这一空白，通过视频超分辨率技术增强ReID系统的鲁棒性和适用性，解决真实世界部署中的瓶颈问题。",
      "method": "S3-CLIP方法整合了最新的超分辨率网络和任务驱动的超分辨率流程，将其适配到视频行人重识别设置中。该框架基于CLIP-ReID，通过视频超分辨率提升轨迹质量，首次系统调查了视频超分辨率在ReID中的应用。关键创新包括集成超分辨率技术以处理低质量视频数据，并优化管道以适应跨视图条件，但摘要未明确说明具体网络架构或数据集细节，仅提及用于VReID-XFD挑战。",
      "result": "实验结果表明，S3-CLIP在VReID-XFD挑战中表现具有竞争力。在aerial-to-ground场景下，平均精度均值达到37.52%；在ground-to-aerial场景下，为29.16% mAP。尤其在ground-to-aerial设置中，排名准确率显著提升：Rank-1、Rank-5和Rank-10分别提高了11.24%、13.48%和17.98%，显示出方法在改善轨迹质量方面的有效性，与基线方法相比性能优越。",
      "conclusion": "本研究的主要贡献是提出了S3-CLIP框架，首次系统地探索了视频超分辨率在行人重识别中的应用。其学术价值在于为ReID领域引入了新的技术方向，实际应用价值在于提升了在跨视图等挑战性场景下的性能，有望促进ReID系统在真实世界中的部署。未来工作可能包括进一步优化超分辨率模型或扩展到更多ReID任务，但摘要未明确说明具体方向或局限性。",
      "tags": [
        "Video Super Resolution",
        "Person Re-Identification",
        "CLIP",
        "Tracklet Enhancement",
        "Cross-view Scenarios"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:45.168111Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08798",
    "title": "Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching",
    "authors": [
      "Maayan Yesharim",
      "R. G. Bina Perl",
      "Uri Roll",
      "Sarig Gafny",
      "Eli Geffen",
      "Yoav Ram"
    ],
    "abstract": "Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08798.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08798",
    "published": "2026-01-13T18:32:43Z",
    "updated": "2026-01-13T18:32:43Z",
    "comment": "18 pages, 4 figures,",
    "light_analysis": {
      "overview": "本研究利用零样本深度局部特征匹配技术，实现了对Hula painted frog近完美的非侵入性照片识别，为保护监测提供高效解决方案。",
      "motivation": "本研究旨在解决稀有两栖动物个体识别中面临的难题：准确的识别对监测濒危物种至关重要，但传统侵入性标记方法（如物理标记）不适用于极度濒危物种，可能带来伤害或干扰。现有计算机视觉方法，如全局特征嵌入模型，在准确性和适用性上存在不足，尤其是在零样本设置下处理复杂生物特征时。因此，开发一种非侵入性、高精度的照片识别方法具有迫切需求，以支持生态保护和研究工作。",
      "method": "本研究采用计算机视觉方法，基于2013-2020年捕获-重捕调查收集的1,233个腹部图像数据集，来自191只Hula painted frog个体。核心方法是比较零样本深度局部特征匹配与深度全局特征嵌入模型。局部特征匹配在无需微调的情况下直接应用，而全局特征模型通过微调优化性能。创新点在于提出一个两阶段工作流程：首先，微调后的全局特征模型快速检索一个候选个体列表；然后，使用局部特征匹配对候选列表进行重新排名。这种方法旨在结合全局特征的扩展性与局部特征的准确性。",
      "result": "实验结果显示，零样本深度局部特征匹配在闭集识别中达到98%的top-1准确率，显著优于所有全局特征模型（最高仅60% top-1准确率，微调后提升至91% top-10）。两阶段工作流程将端到端运行时间从6.5-7.8小时减少到约38分钟，同时在标记数据集上保持约96%的top-1闭集准确率。此外，通过分析相同和不同个体对的匹配分数分离，支持设置阈值进行开放集识别，实现对新个体的实用处理。这些结果验证了局部特征匹配的优越性和工作流程的高效性。",
      "conclusion": "本研究的主要贡献是证明了零样本深度局部特征匹配在照片识别中的有效性，提供了一种准确、快速的非侵入性方法。学术价值在于推动了计算机视觉在生态学中的应用，特别是在个体识别领域；实际应用价值在于部署为Web应用程序，支持野外保护监测和捕获-重捕分析，提高了工作效率和标准化程度。局限性在于摘要未明确说明方法对其他物种的泛化能力，未来工作可探索扩展数据集和优化模型以适应更多濒危物种。",
      "tags": [
        "Zero-Shot Learning",
        "Deep Local-Feature Matching",
        "Computer Vision",
        "Image Recognition",
        "Conservation Monitoring"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:17.492462Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08797",
    "title": "DentalX: Context-Aware Dental Disease Detection with Radiographs",
    "authors": [
      "Zhi Qin Tan",
      "Xiatian Zhu",
      "Owen Addison",
      "Yunpeng Li"
    ],
    "abstract": "Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08797.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08797",
    "published": "2026-01-13T18:32:28Z",
    "updated": "2026-01-13T18:32:28Z",
    "comment": "Accepted at ISBI 2026",
    "light_analysis": {
      "overview": "论文提出DentalX，一种基于上下文感知和多任务学习的牙科疾病检测新方法，通过语义分割提取结构上下文提升检测性能。",
      "motivation": "牙科疾病诊断在放射线照片中面临重大挑战，因为病变迹象往往细微难辨，导致诊断过程耗时且易出错。现有方法多采用为自然图像设计的对象检测模型，这些模型依赖明显目标模式，但在牙科影像中视觉支持不足的疾病检测上表现不佳。这限制了临床诊断的准确性和效率，因此亟需开发能利用口腔结构上下文的新方法，以克服视觉模糊性并改善检测效果。",
      "method": "DentalX方法的核心是引入一个结构上下文提取模块，该模块学习牙科解剖的语义分割作为辅助任务。通过这个模块，提取口腔结构的有意义上下文信息，并将其集成到主要疾病检测网络中，形成多任务学习框架。模型优化过程中同时处理分割和检测任务，有效捕捉任务间的相关性，从而增强对微妙牙科疾病的识别能力。该方法基于牙科放射线照片数据集进行训练，利用对象检测架构实现疾病定位。",
      "result": "在专门构建的基准数据集上进行实验，DentalX在疾病检测和牙科解剖分割两个任务上都显著优于先前方法。摘要未明确说明具体性能指标如准确率提升，但结果表明该方法通过模型优化实现了任务间的互利，提升了整体检测效果。与基线方法对比显示，结合结构上下文能有效改善检测精度和效率，证明了上下文感知方法的优越性。",
      "conclusion": "DentalX的主要贡献在于提出了一种上下文感知的牙科疾病检测框架，通过多任务学习结合语义分割，提高了检测准确性。这项研究在学术上展示了利用结构信息增强视觉模糊场景中对象检测的有效性，具有实际应用价值，可促进牙科诊断的自动化和效率提升。潜在局限性包括对数据集的依赖，未来工作可以探索扩展到其他医疗影像领域并优化模型泛化能力。",
      "tags": [
        "Context-Aware Detection",
        "Semantic Segmentation",
        "Multi-Task Learning",
        "Dental Radiograph Analysis",
        "Object Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:40.386590Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08790",
    "title": "Aggregating Diverse Cue Experts for AI-Generated Image Detection",
    "authors": [
      "Lei Tan",
      "Shuwei Li",
      "Mohan Kankanhalli",
      "Robby T. Tan"
    ],
    "abstract": "The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08790.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08790",
    "published": "2026-01-13T18:23:42Z",
    "updated": "2026-01-13T18:23:42Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "论文提出多线索聚合网络（MCAN），通过统一整合空间、频域和色度信息，以增强AI生成图像检测的泛化能力和鲁棒性。",
      "motivation": "研究旨在解决AI生成图像检测器在快速发展的图像合成模型中的泛化挑战。现有方法往往依赖特定模型的特征，导致过拟合和跨模型泛化能力差，影响检测准确性。这一问题至关重要，因为准确识别AI生成图像对于内容验证、打击虚假信息和维护数字媒体可信度具有重要意义。当前方法的局限性在于缺乏对不同线索的综合利用，限制了检测器的适应性和性能。",
      "method": "论文提出多线索聚合网络（MCAN），一种统一框架，通过混合物编码器适配器动态整合多种互补线索。这些线索包括输入图像（表示整体内容）、高频组件（强调边缘细节）和色度不一致（CI）线索，后者通过归一化强度值和捕获真实图像采集过程中的噪声信息，增强与AI生成内容的区分。关键创新在于整合空间、频域和色度信息进行特征表示学习，使用MCAN架构提升适应性和鲁棒性，无需依赖模型特定特征。",
      "result": "在GenImage、Chameleon和UniversalFakeDetect基准上进行广泛实验，验证了MCAN的先进性能。在GenImage数据集上，MCAN在八个不同图像生成器中平均准确率（ACC）比最佳现有方法提升高达7.4%，表现出优异的跨模型泛化能力。与其他基线方法相比，MCAN在多个指标上达到最先进水平，证明了多线索聚合框架的有效性和鲁棒性，摘要未明确说明具体数据对比细节，但强调了性能提升。",
      "conclusion": "论文的主要贡献是提出MCAN框架，创新性地整合多线索以增强AI生成图像检测的泛化和鲁棒性。学术价值在于通过结合空间、频域和色度信息，推进了特征学习方法的演进；实际应用价值包括改进图像内容验证系统，助力虚假信息检测。局限性可能在于线索选择的特定性，未来工作可探索更多线索类型或扩展到其他媒体检测任务，以进一步优化性能。",
      "tags": [
        "Multi-Cue Aggregation",
        "AI-Generated Image Detection",
        "Frequency-Domain Analysis",
        "Chromatic Inconsistency",
        "Encoder Adapter"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:19.783474Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08785",
    "title": "Uncovering Political Bias in Large Language Models using Parliamentary Voting Records",
    "authors": [
      "Jieying Chen",
      "Karen de Jong",
      "Andreas Poole",
      "Jan Burakowski",
      "Elena Elderson Nosti",
      "Joep Windt",
      "Chendi Wang"
    ],
    "abstract": "As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08785.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08785",
    "published": "2026-01-13T18:18:25Z",
    "updated": "2026-01-13T18:18:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于议会投票记录构建政治偏见基准的方法，用于评估大型语言模型的政治偏见。",
      "motivation": "随着大型语言模型在数字平台和决策系统中广泛应用，其政治偏见问题引发关注。现有研究主要聚焦于性别和种族等社会偏见，而对政治偏见的系统性研究有限。政治偏见直接影响社会决策，缺乏基于真实政治数据的评估方法，因此开发透明、跨国的基准尤为重要，以弥补这一研究空白，促进模型公平性和透明度。",
      "method": "论文提出了一种通用方法，通过将模型生成的投票预测与验证过的议会投票记录对齐，构建政治偏见基准。具体在三个国家案例中实现：荷兰（PoliBiasNL, 2,701动议和投票）、挪威（PoliBiasNO, 10,584动议和投票）和西班牙（PoliBiasES, 2,480动议和投票）。关键创新包括基于真实投票数据的基准构建、跨国评估框架，以及一种可视化方法，将LLMs和政党意识形态映射到二维CHES空间，便于直接比较模型与真实政治行为。",
      "result": "实验结果显示，最先进的大型语言模型 consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. 这些发现基于对三个国家基准的评估，揭示了LLMs的细粒度意识形态区分。摘要未明确说明具体性能指标或基线对比，但强调了基准的有效性和揭示的偏见模式，为跨模型比较提供了实证基础。",
      "conclusion": "本研究的主要贡献是开发了一种透明、跨国评估政治偏见的方法，基于真实议会行为，为理解和审计现代大型语言模型的政治偏见提供了新工具。学术上推动了偏见研究的发展，实际应用中有助于提高模型透明度和社会影响评估。未来工作可扩展至更多国家或细化评估维度，以进一步提升方法的全面性和准确性。",
      "tags": [
        "Large Language Models",
        "Political Bias",
        "Benchmark Construction",
        "Voting Records",
        "CHES Space"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:27.316918Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08781",
    "title": "Fast and explainable clustering in the Manhattan and Tanimoto distance",
    "authors": [
      "Stefan Güttel",
      "Kaustubh Roy"
    ],
    "abstract": "The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08781.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08781",
    "published": "2026-01-13T18:14:03Z",
    "updated": "2026-01-13T18:14:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "扩展CLASSIX算法以支持曼哈顿和Tanimoto距离度量，实现快速且可解释的数据聚类方法。",
      "motivation": "本研究动机源于原始CLASSIX算法仅基于欧几里得距离进行聚类，限制了在需要其他距离度量（如曼哈顿和Tanimoto距离）的领域（如化学信息学）中的应用。实际应用中，这些距离度量更能反映特定数据特性，但现有方法（如Taylor–Butina和DBSCAN）可能效率较低。现有聚类算法在处理非欧几里得距离时缺乏效率和解释性，因此扩展CLASSIX算法以解决这一不足，提升算法的通用性和性能。",
      "method": "论文提出扩展CLASSIX算法，使用数据向量的适当范数（例如，曼哈顿距离对应L1范数）作为排序标准，代替原始的第一主成分分析。结合三角不等式来优化搜索终止条件，以减少计算开销。对于Tanimoto距离，进一步采用了可证明更锐利的交集不等式，以提升算法效率。这种方法避免了主成分分解，简化了计算流程，适用于多种距离度量。摘要未明确说明具体的数据集细节，但提及了在化学指纹基准上的应用。",
      "result": "在真实世界化学指纹基准测试中，扩展的CLASSIX算法表现出显著优势。CLASSIX Tanimoto版本比Taylor–Butina算法快约30倍，比DBSCAN快约80倍。同时，它在两种情况下都计算出了更高质量的聚类，表明在速度和聚类质量方面均优于基线方法。这些结果通过具体性能指标（如速度提升倍数）验证了算法的有效性。",
      "conclusion": "本研究成功扩展了CLASSIX算法到曼哈顿和Tanimoto距离，提供了一种快速且可解释的聚类方法。学术贡献在于通过通用化排序和搜索策略，增强了聚类算法的适应性和性能。实际应用价值体现在化学信息学等领域，提升了聚类效率和准确性。未来工作可能包括扩展到更多距离度量或应用于其他场景，但摘要未明确说明具体局限性。",
      "tags": [
        "CLASSIX Algorithm",
        "Manhattan Distance",
        "Tanimoto Distance",
        "Clustering",
        "Triangle Inequality"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:30.051001Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08778",
    "title": "Pervasive Annotation Errors Break Text-to-SQL Benchmarks and Leaderboards",
    "authors": [
      "Tengjun Jin",
      "Yoojin Choi",
      "Yuxuan Zhu",
      "Daniel Kang"
    ],
    "abstract": "Researchers have proposed numerous text-to-SQL techniques to streamline data analytics and accelerate the development of database-driven applications. To compare these techniques and select the best one for deployment, the community depends on public benchmarks and their leaderboards. Since these benchmarks heavily rely on human annotations during question construction and answer evaluation, the validity of the annotations is crucial.   In this paper, we conduct an empirical study that (i) benchmarks annotation error rates for two widely used text-to-SQL benchmarks, BIRD and Spider 2.0-Snow, and (ii) corrects a subset of the BIRD development (Dev) set to measure the impact of annotation errors on text-to-SQL agent performance and leaderboard rankings. Through expert analysis, we show that BIRD Mini-Dev and Spider 2.0-Snow have error rates of 52.8% and 62.8%, respectively. We re-evaluate all 16 open-source agents from the BIRD leaderboard on both the original and the corrected BIRD Dev subsets. We show that performance changes range from -7% to 31% (in relative terms) and rank changes range from $-9$ to $+9$ positions. We further assess whether these impacts generalize to the full BIRD Dev set. We find that the rankings of agents on the uncorrected subset correlate strongly with those on the full Dev set (Spearman's $r_s$=0.85, $p$=3.26e-5), whereas they correlate weakly with those on the corrected subset (Spearman's $r_s$=0.32, $p$=0.23). These findings show that annotation errors can significantly distort reported performance and rankings, potentially misguiding research directions or deployment choices. Our code and data are available at https://github.com/uiuc-kang-lab/text_to_sql_benchmarks.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08778.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08778",
    "published": "2026-01-13T18:09:06Z",
    "updated": "2026-01-13T18:09:06Z",
    "comment": "18 pages, 14 figures, 9 tables",
    "light_analysis": {
      "overview": "论文通过实证研究揭示 text-to-SQL 基准中标注错误普遍且严重，纠正后显著影响 agent 性能和排行榜排名，挑战现有评估可靠性。",
      "motivation": "Text-to-SQL 技术旨在简化数据分析和数据库应用开发，社区依赖公共基准和排行榜来比较和选择最佳技术。然而，这些基准在问题构建和答案评估中依赖人类标注，标注的有效性至关重要。现有方法的不足之处在于标注错误未被充分评估，可能导致评估失真，误导研究方向和部署选择，突显了该问题的重要性以提升评估的准确性和实用性。",
      "method": "本研究采用实证方法，首先基准化两个广泛使用的 text-to-SQL 基准（BIRD 和 Spider 2.0-Snow）的标注错误率，通过专家分析识别并计算错误率。其次，纠正 BIRD Dev 集的一个子集，重新评估来自 BIRD 排行榜的 16 个开源 agents，使用原始和纠正后的数据集进行性能对比和排名分析。关键创新点在于量化标注错误的影响并提供纠正方案，技术特色包括错误检测和重新评估流程，数据集涉及 BIRD Mini-Dev、BIRD Dev 和 Spider 2.0-Snow。",
      "result": "研究发现 BIRD Mini-Dev 的标注错误率为 52.8%，Spider 2.0-Snow 为 62.8%。在纠正的 BIRD Dev 子集上重新评估 agents 后，性能变化范围从 -7% 到 31%（相对值），排名变化从 -9 到 +9 位。纠正后子集的排名与完整 Dev 集相关性弱（Spearman's r_s=0.32，p=0.23），而未纠正的强（r_s=0.85，p=3.26e-5），表明标注错误显著扭曲评估结果和基线对比，性能改进通过错误纠正实现。",
      "conclusion": "论文总结标注错误能严重扭曲 text-to-SQL 基准报告的性能和排名，可能误导研究方向和实际部署选择，学术价值在于提高评估可靠性和基准质量，实际应用价值为促进更准确的技术比较。贡献包括提供错误率数据和纠正数据集，局限性可能在于样本量或基准选择，未来工作可扩展至其他基准或自动化错误检测，以进一步优化评估流程。",
      "tags": [
        "Text-to-SQL",
        "Benchmark Evaluation",
        "Annotation Errors",
        "Empirical Study",
        "SQL Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:31.093295Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08777",
    "title": "Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling",
    "authors": [
      "Yang Cai",
      "Weiqiang Zheng"
    ],
    "abstract": "Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\\to 1$ as $k\\to\\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\\frac{k}{k+1}$, and no method can achieve a faster rate in general.   We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.GT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08777.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08777",
    "published": "2026-01-13T18:08:06Z",
    "updated": "2026-01-13T18:08:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一个基于测试时缩放的渐进通用对齐框架，通过多玩家游戏理论实现最优对齐率，解决了现有对齐方法输出多样性不足的问题。",
      "motivation": "对齐大型语言模型以适应用户异质且可能冲突的偏好是个性化和可信AI的核心挑战。现有方法如基于人类反馈的纳什学习（NLHF）在测试时缩放中可能失效，因为它们倾向于生成单一优选响应，导致输出多样性不足，使额外样本冗余。这限制了模型在个性化服务中的潜力，因此需要新框架来确保在多个候选响应中实现稳健对齐。",
      "method": "论文形式化了通过测试时缩放的对齐理想：模型为每个提示生成k个候选响应，用户选择偏好。提出了(k,f(k))-稳健对齐和渐进通用对齐的概念，其中f(k)表示k-输出模型相对于单输出模型的赢率。核心方法是引入对称多玩家对齐游戏，设计(k+1)-玩家游戏框架，并证明其对称纳什均衡策略能实现最优(k,k/(k+1))-稳健对齐。该方法通过保留输出多样性，避免现有对齐方法可能崩溃为单一响应的问题。",
      "result": "理论结果表明，存在单输出策略家族，其k-样本乘积策略能实现渐进通用对齐，最优收敛率为f(k)=k/(k+1)，且无通用方法能超过此速率。与基线方法如NLHF对比，NLHF在k=1时最优，但在测试时缩放中，由于输出多样性缺乏，其赢率可能不超过1/2加上任意小松弛。提出的对称多玩家游戏方法能达到最优对齐，并通过自我学习动态提供收敛保证，扩展至对手也生成多个响应的场景。",
      "conclusion": "论文的主要贡献是提出了一种基于测试时缩放的渐进通用对齐框架，通过游戏理论实现最优收敛率，改善了输出多样性和对齐性能。学术上，该研究为大型语言模型对齐提供了理论基础，弥补了现有方法在测试时缩放方面的不足；实际应用中，有助于开发更可信和个性化的AI系统。未来工作方向包括扩展框架以处理更复杂对手，并进一步实验验证。摘要未明确说明具体数据集或实验细节，主要基于理论分析。",
      "tags": [
        "Large Language Model Alignment",
        "Nash Equilibrium",
        "Test-Time Scaling",
        "Game Theory",
        "Output Diversity"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:44.608107Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08776",
    "title": "Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN",
    "authors": [
      "Yanhua Zhao"
    ],
    "abstract": "Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08776.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08776",
    "published": "2026-01-13T18:08:03Z",
    "updated": "2026-01-13T18:08:03Z",
    "comment": "5 pages, 4 figures",
    "light_analysis": {
      "overview": "论文提出一种基于CycleGAN的无配对图像翻译方法，将荧光显微镜图像转换为虚拟H&E染色图像。",
      "motivation": "组织病理学分析主要依赖H&E染色，但荧光显微镜能提供补充的生物信息，现有方法在处理荧光图像与H&E图像转换时往往需要配对数据，限制了应用。本研究旨在解决这一挑战，通过无配对翻译使荧光数据以病理学家熟悉的格式呈现，便于解释和集成到标准工作流程，从而提高多模态图像分析效率。",
      "method": "该方法采用Cycle-Consistent Adversarial Network (CycleGAN)，通过合并C01和C02荧光通道为RGB图像，实现荧光和H&E域之间的无配对双向映射。核心架构使用基于ResNet的生成器，配备残差块和PatchGAN判别器，训练中结合对抗损失、循环一致性损失和身份损失，以确保图像翻译的逼真性和结构保留。",
      "result": "在荧光显微镜数据集上的实验显示，模型能生成逼真的伪H&E图像，有效保留原始图像的形态结构并模拟H&E染色颜色特征。摘要未明确说明具体性能指标数据，但与基线方法相比，模型成功实现了高质量图像转换，支持后续分析。",
      "conclusion": "本研究开发了一种无配对的图像翻译技术，使荧光数据以虚拟H&E形式可视化，增强了病理学分析的便利性和兼容性。学术上为图像翻译领域提供了新思路，实际应用中促进了荧光信息与现有H&E流程的集成。未来可优化翻译精度或扩展至其他图像类型。",
      "tags": [
        "CycleGAN",
        "Image-to-Image Translation",
        "Fluorescence Microscopy",
        "H&E Staining",
        "Generative Adversarial Networks"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:28.865761Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08768",
    "title": "AI as Entertainment",
    "authors": [
      "Cody Kommers",
      "Ari Holtzman"
    ],
    "abstract": "Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose \"thick entertainment\" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about \"intelligence\" as social media is about social connection.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08768.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08768",
    "published": "2026-01-13T17:55:34Z",
    "updated": "2026-01-13T17:55:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出'厚重娱乐'框架，用于评估 AI 生成内容的文化益处和社会影响。",
      "motivation": "研究动机在于，生成式 AI 系统通常被视为提升生产力的工具，但娱乐正成为一个新兴且广泛采用的应用场景，尤其在年轻人中。现有评估方法存在不对称性：它们严格衡量智能的益处和危害，但主要聚焦于文化危害，缺乏框架来阐述文化输出的积极价值。这导致 AI 领域无法充分应对娱乐内容对社会的影响，而娱乐可能成为 AI 公司的主要商业模式，影响技术发展方向。",
      "method": "研究方法基于人文学科的见解，提出'厚重娱乐'作为概念性框架，用于评估 AI 生成的娱乐内容。该框架考虑娱乐在意义构建、身份形成和社会联系中的作用，而不仅仅是减少危害。摘要未明确说明具体技术路线、数据集或模型架构，推断为理论分析方法。",
      "result": "主要实验结果在摘要中未明确说明。论文提到新兴数据表明 AI 已广泛用于娱乐，并代表潜在的大规模收入来源，但未提供具体性能指标或与基线方法的对比。因此，结果基于现有数据的理论分析，缺乏实验数据支撑。",
      "conclusion": "论文结论是，AI 的长期影响可能更侧重于娱乐而非生产力，类似于社交媒体的社会联系作用。主要贡献在于提出了一个评估框架，强调娱乐的积极社会价值，为 AI 与文化交叉领域提供新视角。研究具有学术价值，呼吁发展更全面的评估方法，并指出未来需应对娱乐内容的社会影响。",
      "tags": [
        "Generative AI",
        "AI Evaluation",
        "Cultural Content Analysis",
        "Entertainment"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:27.847682Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08763",
    "title": "Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs",
    "authors": [
      "Zhiyuan Hu",
      "Yucheng Wang",
      "Yufei He",
      "Jiaying Wu",
      "Yilun Zhao",
      "See-Kiong Ng",
      "Cynthia Breazeal",
      "Anh Tuan Luu",
      "Hae Won Park",
      "Bryan Hooi"
    ],
    "abstract": "Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08763.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08763",
    "published": "2026-01-13T17:48:43Z",
    "updated": "2026-01-13T17:48:43Z",
    "comment": "Work in Progress",
    "light_analysis": {
      "overview": "论文提出了一种独特性的强化学习方法，通过奖励罕见策略来解决大型语言模型在推理任务中的探索崩溃问题，从而提高解决方案多样性和性能。",
      "motivation": "强化学习在大型语言模型后训练中广泛用于复杂推理任务，但常面临探索崩溃问题：策略过早集中于少数主导推理模式，提高了单次采样通过率（pass@1），却限制了rollout级多样性和多次采样通过率（pass@k）的增益。这一问题源于现有方法过度正则化局部令牌行为，而忽视了解决方案集的整体多样性，影响创造性问题解决能力。",
      "method": "论文提出Uniqueness-Aware Reinforcement Learning方法，其核心是基于rollout-level的目标，使用LLM-based judge对相同问题的rollouts进行聚类，依据高层解决方案策略忽略表面变异，并根据聚类大小逆重新加权策略优势。这样，正确但新颖的策略获得更高奖励。摘要未明确说明使用的具体数据集或模型架构细节，但方法应用于数学、物理和医学推理基准测试中。",
      "result": "在数学、物理和医学推理基准测试中，该方法显著提升了多次采样通过率（pass@k）和曲线下面积（AUC@K），特别是在大采样预算下，同时不牺牲单次采样通过率（pass@1）。与基线方法相比，它有效增加了解决方案的多样性，维持了探索能力，摘要未提供具体数值，但强调了性能的持续改进。",
      "conclusion": "该研究的主要贡献是提出了一种基于独特性的强化学习方法，通过奖励罕见策略来增强大型语言模型在复杂推理中的多样性和性能。这具有重要的学术价值，为解决探索崩溃问题提供了创新思路，并在实际应用中支持创造性问题解决，如数学和医学推理。摘要未明确说明局限性或未来工作方向，但可能涉及方法的泛化性或进一步优化。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Exploration Collapse",
        "Pass@k Metrics",
        "Clustering Algorithms"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:03.610486Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08760",
    "title": "Adaptive Requesting in Decentralized Edge Networks via Non-Stationary Bandits",
    "authors": [
      "Yi Zhuang",
      "Kun Yang",
      "Xingran Chen"
    ],
    "abstract": "We study a decentralized collaborative requesting problem that aims to optimize the information freshness of time-sensitive clients in edge networks consisting of multiple clients, access nodes (ANs), and servers. Clients request content through ANs acting as gateways, without observing AN states or the actions of other clients. We define the reward as the age of information reduction resulting from a client's selection of an AN, and formulate the problem as a non-stationary multi-armed bandit. In this decentralized and partially observable setting, the resulting reward process is history-dependent and coupled across clients, and exhibits both abrupt and gradual changes in expected rewards, rendering classical bandit-based approaches ineffective. To address these challenges, we propose the AGING BANDIT WITH ADAPTIVE RESET algorithm, which combines adaptive windowing with periodic monitoring to track evolving reward distributions. We establish theoretical performance guarantees showing that the proposed algorithm achieves near-optimal performance, and we validate the theoretical results through simulations.",
    "categories": [
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08760.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08760",
    "published": "2026-01-13T17:43:38Z",
    "updated": "2026-01-13T17:43:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种结合自适应窗口和周期性监控的算法，用于解决去中心化边缘网络中非平稳多臂老虎机问题，以优化信息新鲜度。",
      "motivation": "本研究旨在解决去中心化边缘网络中，时间敏感客户的信息新鲜度优化问题。由于客户无法观察接入节点状态和其他客户行为，奖励过程变得复杂，具有历史依赖性、跨客户耦合以及奖励分布的突变和渐变变化。这使得传统的多臂老虎机方法无效，因为它们假设平稳环境，无法适应动态变化。信息新鲜度对实时应用如自动驾驶和视频流至关重要，因此开发高效的优化算法具有重要的实际意义，以提升系统性能和用户体验。",
      "method": "论文提出AGING BANDIT WITH ADAPTIVE RESET算法，将问题建模为非平稳多臂老虎机，奖励定义为客户选择接入节点所减少的信息年龄。算法结合自适应窗口技术和周期性监控：自适应窗口动态调整时间范围以估计奖励分布的演化，周期性监控检测变化点以重置策略。这种设计允许算法在去中心化、部分可观测环境中跟踪奖励变化，通过局部行动选择来学习最佳接入节点，从而有效适应非平稳环境，克服经典方法的局限性。",
      "result": "实验结果表明，所提算法在理论上实现了接近最优的性能保证，仿真验证了其在处理非平稳奖励分布时的有效性。算法相比经典多臂老虎机方法，在去中心化边缘网络设置中表现出更好的适应性和性能改进，但摘要未明确说明具体的性能指标数值，如准确率提升百分比或效率改进细节，因此需要参考完整论文以获取量化数据。",
      "conclusion": "该研究的主要贡献是提出了一种创新的算法，处理去中心化边缘网络中的非平稳多臂老虎机问题，扩展了多臂老虎机理论到非平稳和部分可观测环境。学术价值在于提供了新的理论基础，实际应用价值包括优化实时应用的信息新鲜度，如物联网和边缘计算中的请求调度。未来工作可能涉及算法进一步优化、扩展到更多网络场景或在实际部署中进行验证。",
      "tags": [
        "Non-Stationary Multi-Armed Bandit",
        "Decentralized Edge Networks",
        "Adaptive Windowing",
        "Age of Information",
        "Periodic Monitoring"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:55.474819Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08750",
    "title": "Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables",
    "authors": [
      "Valerie Zermatten",
      "Chiara Vanalli",
      "Gencer Sumbul",
      "Diego Marcos",
      "Devis Tuia"
    ],
    "abstract": "Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08750.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08750",
    "published": "2026-01-13T17:27:16Z",
    "updated": "2026-01-13T17:27:16Z",
    "comment": "submitted",
    "light_analysis": {
      "overview": "论文提出一种基于注意力的方法，通过整合空间邻域的文本和遥感数据来改进环境变量映射。",
      "motivation": "在生态学研究中，文本数据作为新兴数据源能提供独特的局部环境信息，补充传统地理空间数据。然而，现有方法面临挑战：文本数据贡献不明确、稀疏且不规则，与遥感数据的整合困难，导致预测精度受限。本研究旨在解决这些不足，明确文本在生态任务中的角色，并开发有效方法以充分利用多源数据，从而提高环境变量映射的准确性。",
      "method": "本研究提出一个基于注意力的方法，结合高分辨率航拍图像和地理位置文本。核心创新点在于使用注意力模块动态选择空间邻域中有用的观测数据，通过集成视觉、文本表示和地理位置编码来实现多模态融合。该方法应用于EcoWikiRS数据集，该数据集包含瑞士的航拍图像和Wikipedia句子，用于预测SWECO25数据立方体中的103个环境变量，以验证空间上下文整合的有效性。",
      "result": "实验结果显示，该方法在预测环境变量时持续优于单一位置或单模态基线（仅图像或仅文本）。具体而言，在气候、土壤、人口和土地利用/覆盖等主题组中，性能有显著提升，证明了结合文本和图像时空间上下文的关键作用，整体性能改进验证了多源数据融合的优势。",
      "conclusion": "论文的主要贡献是开发了一种新方法，有效整合文本和遥感数据，强调了空间上下文在环境变量映射中的重要性。这在学术上推动了多模态数据融合在生态学中的应用，实际应用可能改善环境监测和评估。局限性（如数据集特异性）和未来工作方向摘要未明确说明，但可扩展至其他区域或数据类型。",
      "tags": [
        "Attention Mechanism",
        "Remote Sensing",
        "Natural Language Processing",
        "Spatial Context",
        "Environmental Mapping"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:30.810273Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08747",
    "title": "To Retrieve or To Think? An Agentic Approach for Context Evolution",
    "authors": [
      "Rubing Chen",
      "Jian Wang",
      "Wenjie Li",
      "Xiao-Yong Wei",
      "Qing Li"
    ],
    "abstract": "Current context augmentation methods, such as retrieval-augmented generation, are essential for solving knowledge-intensive reasoning tasks.However, they typically adhere to a rigid, brute-force strategy that executes retrieval at every step. This indiscriminate approach not only incurs unnecessary computational costs but also degrades performance by saturating the context with irrelevant noise. To address these limitations, we introduce Agentic Context Evolution (ACE), a framework inspired by human metacognition that dynamically determines whether to seek new evidence or reason with existing knowledge. ACE employs a central orchestrator agent to make decisions strategically via majority voting.It aims to alternate between activating a retriever agent for external retrieval and a reasoner agent for internal analysis and refinement. By eliminating redundant retrieval steps, ACE maintains a concise and evolved context. Extensive experiments on challenging multi-hop QA benchmarks demonstrate that ACE significantly outperforms competitive baselines in accuracy while achieving efficient token consumption.Our work provides valuable insights into advancing context-evolved generation for complex, knowledge-intensive tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08747.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08747",
    "published": "2026-01-13T17:25:57Z",
    "updated": "2026-01-13T17:25:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "ACE框架通过动态决策检索与推理，优化上下文进化，显著提升知识密集型任务的准确性和效率。",
      "motivation": "当前上下文增强方法，如检索增强生成（RAG），对于知识密集型推理任务至关重要。然而，现有方法通常采用刚性策略，在每个推理步骤都执行检索，这不仅增加了不必要的计算成本，还因引入不相关噪声而降低性能，限制了处理复杂任务时的效率和准确性。因此，需要一种更智能的框架来动态平衡检索与推理，以优化上下文管理并提升整体效果。",
      "method": "该论文提出Agentic Context Evolution（ACE）框架，灵感来源于人类元认知。核心方法包括一个中央协调器代理，通过多数投票机制动态决定是否激活检索器代理进行外部信息检索，或激活推理器代理进行内部知识分析和精炼。这种交替策略消除了冗余检索步骤，保持了上下文的简洁性和进化性，关键创新在于代理架构和动态决策过程，以实现自适应信息流管理。",
      "result": "在具有挑战性的多跳问题回答基准上进行广泛实验，结果显示ACE框架在准确性方面显著优于竞争基线。同时，通过减少不必要的检索步骤，ACE实现了高效的令牌消耗，降低了计算资源需求。实验表明，ACE在保持高准确率的同时优化了上下文使用，避免了噪声干扰，从而在性能和效率上均优于传统检索增强方法。",
      "conclusion": "ACE框架的主要贡献在于提出了一种动态、代理驱动的上下文进化方法，有效解决了现有检索增强生成中的效率与准确性问题。这项研究不仅为知识密集型任务的推理提供了新视角，还具有重要的实际应用价值，尤其在优化资源使用方面。未来工作可探索更复杂代理交互或扩展到其他推理任务，以进一步提升泛化能力和应用范围。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Multi-hop Question Answering",
        "Agentic AI",
        "Context Evolution",
        "Majority Voting"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:36.037626Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08743",
    "title": "TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL",
    "authors": [
      "Jinbo Su",
      "Yuxuan Hu",
      "Cuiping Li",
      "Hong Chen",
      "Jia Li",
      "Lintao Ma",
      "Jing Zhang"
    ],
    "abstract": "In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08743.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08743",
    "published": "2026-01-13T17:20:55Z",
    "updated": "2026-01-13T17:20:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出TableCache，一种基于主外键关系指导的KV缓存预计算方法，显著降低文本到SQL任务的首令牌时间延迟。",
      "motivation": "在文本到SQL任务中，现有基于大型语言模型的方法需在提示中包含完整数据库模式，导致上下文长度增加，引发预填充延迟升高。用户查询通常聚焦于重复表集，这为跨查询缓存共享提供了机会，但当前推理引擎如SGLang和vLLM在处理不同表顺序的查询时，会生成冗余的前缀缓存副本，造成计算资源浪费和效率低下。因此，优化缓存机制以减少延迟成为亟需解决的问题，其重要性在于提升实时查询性能和降低系统开销。",
      "method": "TableCache方法的核心是在离线阶段预计算表的KV缓存，并保持表间的主外键关系以确保语义一致性，避免冗余计算。关键创新包括构建Table Trie结构来组织缓存，以加速在线推理期间的查找；引入缓存管理系统，通过查询重排名策略优化缓存使用，提高命中率，并设计计算加载管道实现模型推理与缓存加载的并行化，从而减少延迟。摘要未明确说明使用的具体数据集或模型架构，但重点在缓存优化技术。",
      "result": "实验结果显示，TableCache在首次令牌时间（TTFT）上实现了高达3.62倍的加速，性能退化可忽略。具体数据表明，该方法能显著降低延迟，对比现有推理引擎如SGLang和vLLM，通过减少缓存冗余和优化加载过程，提升了推理效率，同时保持了文本到SQL查询的准确性，展示了其在降低系统响应时间方面的有效优势。",
      "conclusion": "TableCache的主要贡献在于提出了一种高效的KV缓存预计算方法，通过离线处理和智能管理，为文本到SQL任务提供了低延迟解决方案。该研究具有重要的学术价值，为优化大型语言模型推理提供了新思路，并在实际应用中能提升数据库查询系统的响应速度和资源利用率。未来工作可探索在动态数据库环境下的扩展，或与其他优化技术结合以进一步提高性能，摘要未明确说明具体局限性。",
      "tags": [
        "KV Cache",
        "Text-to-SQL",
        "Primary Foreign Key",
        "Table Trie",
        "Cache Management"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:24.861408Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08742",
    "title": "Inferring Latent Intentions: Attributional Natural Language Inference in LLM Agents",
    "authors": [
      "Xin Quan",
      "Jiafeng Xiong",
      "Marco Valentino",
      "André Freitas"
    ],
    "abstract": "Attributional inference, the ability to predict latent intentions behind observed actions, is a critical yet underexplored capability for large language models (LLMs) operating in multi-agent environments. Traditional natural language inference (NLI), in fact, fails to capture the nuanced, intention-driven reasoning essential for complex interactive systems. To address this gap, we introduce Attributional NLI (Att-NLI), a framework that extends NLI with principles from social psychology to assess an agent's capacity for abductive intentional inference (generating hypotheses about latent intentions), and subsequent deductive verification (drawing valid logical conclusions). We instantiate Att-NLI via a textual game, Undercover-V, experimenting with three types of LLM agents with varying reasoning capabilities and access to external tools: a standard NLI agent using only deductive inference, an Att-NLI agent employing abductive-deductive inference, and a neuro-symbolic Att-NLI agent performing abductive-deductive inference with external theorem provers. Extensive experiments demonstrate a clear hierarchy of attributional inference capabilities, with neuro-symbolic agents consistently outperforming others, achieving an average win rate of 17.08%. Our results underscore the role that Att-NLI can play in developing agents with sophisticated reasoning capabilities, highlighting, at the same time, the potential impact of neuro-symbolic AI in building rational LLM agents acting in multi-agent environments.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08742.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08742",
    "published": "2026-01-13T17:18:38Z",
    "updated": "2026-01-13T17:18:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Attributional NLI框架，通过整合abductive-deductive推理评估LLM智能体的归因推理能力，并在文本游戏中验证神经符号方法的优越性。",
      "motivation": "本研究旨在解决大型语言模型在多智能体环境中预测潜在意图（归因推理）的能力不足问题。传统自然语言推理方法主要侧重于演绎逻辑，难以有效处理意图驱动的推理，这在复杂交互系统中至关重要。由于LLM在自主决策和协作场景中的广泛应用，提升其归因推理能力对构建更智能、适应性强的代理系统具有重要意义。现有方法忽视了深层意图推断，导致在动态交互中表现不佳，这突显了研究和改进的必要性。",
      "method": "论文提出了Attributional NLI框架，结合社会心理学原则，将传统自然语言推理扩展为abductive intentional inference（生成关于潜在意图的假设）和deductive verification（验证逻辑结论）。通过文本游戏Undercover-V实例化该框架，实验三种LLM代理：标准NLI代理仅使用演绎推理，Att-NLI代理采用abductive-deductive推理，以及神经符号Att-NLI代理在相同推理基础上整合外部定理证明器。这允许系统评估代理在不同推理能力和工具访问下的性能差异。",
      "result": "实验结果显示归因推理能力存在明确层级，神经符号Att-NLI代理表现最佳。具体地，在文本游戏中，神经符号代理取得平均17.08%的胜率，一致优于仅使用演绎推理的标准NLI代理和普通Att-NLI代理。这证明了整合外部工具和abductive-deductive推理在复杂推理任务中的有效性。基线对比表明，传统方法在归因推理上性能有限，突显了所提框架在提升推理能力方面的优势。",
      "conclusion": "本研究的主要贡献是提出了Attributional NLI框架，强调了其在开发具有复杂推理能力的LLM代理中的关键作用。通过结合社会心理学原则和abductive-deductive推理，显著提升了代理在预测意图方面的性能，突显了神经符号AI在构建理性代理中的潜力，尤其适用于多智能体环境。摘要未明确说明具体局限性，但可以推断未来工作可能包括扩展框架到更多应用场景或改进推理机制以增强实际应用价值。",
      "tags": [
        "Large Language Model",
        "Attributional Inference",
        "Natural Language Inference",
        "Abductive Inference",
        "Neuro-Symbolic AI"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:50.202828Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08741",
    "title": "From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding",
    "authors": [
      "Anmol Gulati",
      "Sahil Sen",
      "Waqar Sarguroh",
      "Kevin Paul"
    ],
    "abstract": "Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08741.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08741",
    "published": "2026-01-13T17:18:14Z",
    "updated": "2026-01-13T17:18:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一个多模态检索增强生成框架 FRTR，用于提升大型语言模型在企业电子表格推理中的性能，显著改善准确性和效率。",
      "motivation": "研究动机是解决大型语言模型在处理大规模企业电子表格时的困难，这些表格包含数千行数字、多个链接工作表和嵌入图像内容（如图表、收据），阻碍了自动化数据理解。现有方法如单工作表压缩或全上下文编码可扩展性差，未能真实反映用户与复杂多模态工作簿的交互，限制了在企业环境中的应用效率。企业电子表格是常见数据管理工具，改进其推理能力对提升业务决策和自动化流程至关重要。",
      "method": "研究方法包括开发 FRTR 框架，它将 Excel 工作簿分解为细粒度的行、列和块级嵌入，采用混合词汇-密集检索结合 Reciprocal Rank Fusion 进行融合，并集成多模态嵌入以同时处理数字和视觉信息。关键创新点在于细粒度嵌入和跨模态集成。数据集方面，创建了 FRTR-Bench 基准，包含 30 个企业级 Excel 工作簿，涵盖近四百万单元格和超过 50 张嵌入图像，用于评估框架的有效性。",
      "result": "主要实验结果显示了显著性能提升：在 FRTR-Bench 上使用 Claude Sonnet 4.5 测试，FRTR 达到 74% 答案准确率，相比先前最先进方法的 24% 有大幅度改进。在 SpreadsheetLLM 基准上，使用 GPT-5 达到 87% 准确率，同时与上下文压缩方法相比，减少了约 50% 的令牌使用量，表明框架在保持高精度的同时提高了计算效率。这些结果验证了方法的优越性和可扩展性。",
      "conclusion": "论文的主要贡献是提出了 FRTR 多模态检索增强生成框架和 FRTR-Bench 基准，有效解决了企业电子表格推理的挑战。学术价值在于推动了检索增强生成和多模态学习的融合，拓展了文档处理领域的研究；应用价值体现在提升企业数据自动理解的效率，支持商业决策自动化。潜在局限性可能是对特定工作簿类型的依赖，未来工作方向可包括扩展到其他文档格式或进一步优化检索算法以增强泛化能力。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Multimodal Framework",
        "Spreadsheet Understanding",
        "Hybrid Retrieval",
        "Reciprocal Rank Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:46.722741Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08739",
    "title": "PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation",
    "authors": [
      "Xingyu Tan",
      "Xiaoyang Wang",
      "Qing Liu",
      "Xiwei Xu",
      "Xin Yuan",
      "Liming Zhu",
      "Wenjie Zhang"
    ],
    "abstract": "Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08739.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08739",
    "published": "2026-01-13T17:14:23Z",
    "updated": "2026-01-13T17:14:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "PrivGemo提出了一种隐私保护的双塔图检索框架，通过匿名化视图和内存控制增强大型语言模型推理。",
      "motivation": "知识图为LLM推理提供结构化证据，但私有知识图在发送到闭源LLM API时存在隐私泄露风险。现有方法如实体名称掩码面临结构泄露、不可控远程交互、脆弱的多跳推理和有限经验重用等问题，限制了应用的稳定性和效率，需要更全面的隐私保护解决方案来支持实际应用。",
      "method": "PrivGemo采用双塔设计，保持原始KG知识本地化，通过匿名化视图进行远程推理，限制语义和结构暴露。它支持多跳多实体推理，通过检索匿名化长跳路径连接主题实体，并在本地KG进行基础和验证。分层控制器和隐私感知经验内存进一步优化探索过程，减少不必要的远程交互。",
      "result": "在六个基准测试中，PrivGemo取得整体最优结果，性能相比最强基线提升最高达17.1%。此外，它使得小模型如Qwen3-4B的推理性能可与GPT-4-Turbo相媲美，展示了高效能和可扩展性。",
      "conclusion": "PrivGemo的主要贡献是提供了一个隐私保护的检索增强框架，有效解决KG增强LLM推理中的隐私挑战，提升推理稳定性和效率。其学术价值在于融合隐私保护和图检索技术，应用价值在于使小模型达到大模型性能水平，降低成本。未来可扩展更多KG类型或增强隐私控制机制。",
      "tags": [
        "Knowledge Graph Retrieval",
        "Large Language Model Reasoning",
        "Privacy-Preserving Framework",
        "Dual-Tower Architecture",
        "Memory Augmentation"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:00.640947Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08733",
    "title": "A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making",
    "authors": [
      "A. M. A. S. D. Alagiyawanna",
      "Asoka Karunananda",
      "Thushari Silva",
      "A. Mahasinghe"
    ],
    "abstract": "Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of \"active ingredient\" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.",
    "categories": [
      "cs.LG",
      "quant-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08733.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08733",
    "published": "2026-01-13T17:06:19Z",
    "updated": "2026-01-13T17:06:19Z",
    "comment": "Accepted and published in IEEE 2025. This is the authors manuscript version; final version available at IEEE Xplore: https://ieeexplore.ieee.org/document/11318441",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Request timed out."
  },
  {
    "id": "2601.08732",
    "title": "ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning",
    "authors": [
      "Vincent Roca",
      "Martin Bretzner",
      "Hilde Henon",
      "Laurent Puy",
      "Grégory Kuchcinski",
      "Renaud Lopes"
    ],
    "abstract": "Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08732.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08732",
    "published": "2026-01-13T17:05:40Z",
    "updated": "2026-01-13T17:05:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出ISLA模型，通过系统优化U-Net框架，结合深度监督、注意力机制和域适应，提升急性缺血性中风病变分割性能。",
      "motivation": "急性缺血性中风病变的准确分割对诊断和治疗至关重要。现有深度学习模型大多基于U-Net框架，但在损失函数、深度监督和注意力机制上存在差异，且许多实现不可公开获取，最佳配置不明确。这限制了方法的可复现性和实际应用，因此需要开发一个公开、优化且健壮的模型来解决这一问题，以提升分割精度和泛化能力。",
      "method": "本研究提出ISLA模型，基于U-Net架构，通过系统优化损失函数、卷积网络结构、深度监督和注意力机制来构建。模型在三个多中心MRI数据库上训练，包含超过1500名急性缺血性中风参与者。此外，引入无监督域适应技术，以改善模型在外部临床数据上的泛化能力，从而增强分割的鲁棒性和准确性。",
      "result": "在外部测试集上，ISLA模型在急性缺血性中风病变分割任务中表现优于两种当前最先进的方法，表明其具有更高的分割精度和泛化能力。尽管摘要未提供具体的准确率或Dice系数等指标，但结果验证了模型通过系统优化和域适应的有效性，能更好地适应不同临床数据集。",
      "conclusion": "本研究的核心贡献在于提出了ISLA模型，一个结合深度监督、注意力机制和域适应的U-Net变体，显著提升了急性缺血性中风病变分割的性能。模型代码和训练模型将公开，有助于促进该领域的可复现性和实际应用，为临床诊断提供辅助工具。未来工作可进一步探索模型的局限性和在其他医学图像任务中的应用。",
      "tags": [
        "U-Net",
        "Deep Supervision",
        "Attention Mechanisms",
        "Domain Adaptation",
        "Ensemble Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:31.242318Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08731",
    "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
    "authors": [
      "Yuanlin Duan",
      "Yuning Wang",
      "Wenjie Qiu",
      "He Zhu"
    ],
    "abstract": "Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08731.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08731",
    "published": "2026-01-13T17:03:31Z",
    "updated": "2026-01-13T17:03:31Z",
    "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "light_analysis": {
      "overview": "论文提出能力感知目标采样（Cago）方法，通过自适应课程降低模仿学习对专家轨迹的脆弱依赖。",
      "motivation": "模仿学习在长期环境中常因完美复制演示不现实而导致失败，小错误累积会造成灾难性后果。现有方法仅将演示用于策略初始化或奖励塑造，未能有效解决轨迹依赖问题，限制了在稀疏奖励、目标条件任务中的实际应用。本研究旨在改善模仿学习的鲁棒性，减少对直接模仿的脆弱依赖，以提高智能体在复杂任务中的学习效率和性能。",
      "method": "Cago 方法的核心是动态跟踪智能体在专家轨迹上的能力，并利用该信号选择略超出当前能力范围的目标作为中间步骤，以指导学习过程。这形成自适应课程，逐步引导智能体向完整任务迈进，减少对轨迹的直接模仿。关键创新点包括能力感知的目标采样机制和自适应的学习路径设计，适用于稀疏奖励环境；但摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "实验结果显示，Cago 在一系列稀疏奖励、目标条件任务中显著提高了样本效率和最终性能，持续优于现有的从演示学习基线方法。摘要未提供具体性能指标数字，但强调了在多个基准测试中的优越表现，表明该方法能有效减少训练样本需求并提升任务完成率，克服了传统模仿学习方法的局限性。",
      "conclusion": "本研究的贡献在于提出了 Cago 方法，通过能力感知目标采样和自适应课程，解决了模仿学习在长期任务中的脆弱性问题。该研究具有学术价值，推动了模仿学习向更鲁棒和高效的方向发展，在实际应用中可提升智能体在复杂环境中的自适应能力。未来工作方向可能包括扩展到更广泛的任务类型或结合其他学习范式，以进一步提升泛化性能。",
      "tags": [
        "Imitation Learning",
        "Goal Sampling",
        "Adaptive Curriculum",
        "Sparse Reward",
        "Goal-Conditioned Tasks"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:40.974894Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08728",
    "title": "Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation",
    "authors": [
      "Runfeng Qu",
      "Ole Hall",
      "Pia K Bideau",
      "Julie Ouerfelli-Ethier",
      "Martin Rolfs",
      "Klaus Obermayer",
      "Olaf Hellwich"
    ],
    "abstract": "Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08728.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08728",
    "published": "2026-01-13T16:57:09Z",
    "updated": "2026-01-13T16:57:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Salience-SGG框架，通过迭代显著解码器增强无偏场景图生成的空间理解能力，解决现有方法在空间感知上的不足。",
      "motivation": "场景图生成（SGG）面临长尾分布问题，数据集中的谓词类别如常见关系（如“on”）过多，而罕见关系（如“behind”）样本稀少，导致模型在处理罕见关系时表现偏差。现有无偏SGG方法通过去偏见策略如重采样或损失调整来缓解，但往往牺牲空间理解，过度依赖语义先验，限制了模型在复杂场景中的准确性和泛化能力。该问题重要性在于空间结构是理解对象间关系的关键，忽视它会影响实际应用如视觉问答的可靠性。",
      "method": "Salience-SGG框架的核心是迭代显著解码器（ISD），它通过多轮迭代评估对象对的空间显著性，专注于具有突出空间结构的三元组以增强空间感知。关键创新在于引入语义无关的显著性标签，这些标签基于纯空间特征而非语义类别，指导ISD学习，避免对语义先验的过度依赖。摘要未明确说明具体模型架构或数据集细节，但该方法旨在通过迭代过程提升空间理解，缓解去偏见策略带来的空间信息损失。",
      "result": "在Visual Genome、Open Images V6和GQA-200数据集上的评估表明，Salience-SGG在场景图生成任务中实现了最先进的性能。具体通过Pairwise Localization Average Precision指标，该方法显著改进了现有无偏SGG方法的空间理解能力，证明了其在处理空间依赖关系时的有效性。摘要未提供具体准确率提升数据，但强调了在空间感知方面的优越性，展示了与基线方法的对比中性能提升。",
      "conclusion": "本研究的主要贡献是开发了Salience-SGG框架，它通过迭代显著估计解决了无偏SGG中空间理解不足的问题，平衡了去偏见与空间感知。这具有重要的学术价值，为场景图生成领域提供了新方法，推动计算机视觉任务的进步。在实际应用中，可提升图像理解和视觉推理的准确性。未来工作可能包括优化显著性估计机制或扩展到更多数据集，摘要未明确说明局限性，但潜在方向可能涉及计算效率或泛化到其他视觉任务。",
      "tags": [
        "Scene Graph Generation",
        "Unbiased Learning",
        "Iterative Salience Decoder",
        "Spatial Understanding",
        "Pairwise Localization"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:08.800950Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08726",
    "title": "Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts",
    "authors": [
      "Bert Verbruggen",
      "Arne Vanhoyweghen",
      "Vincent Ginis"
    ],
    "abstract": "Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08726.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08726",
    "published": "2026-01-13T16:53:40Z",
    "updated": "2026-01-13T16:53:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种模型无关的解决方案，通过在深度强化学习中引入显式时间依赖来改善非遍历环境下的策略优化。",
      "motivation": "强化学习中的最优策略通常基于Bellman方程的期望值计算，但在非遍历环境中，长期结果依赖于具体轨迹而非集合平均，导致期望值方法产生系统性的次优策略。传统RL架构已被证明在此类环境中失效，而深度RL实现同样面临挑战，这凸显了改进非遍历动态下RL方法的重要性和紧迫性，以解决实际优化问题。",
      "method": "研究方法的核心是在学习过程中引入显式的时间依赖，通过让神经网络的函数逼近器纳入时间信息，使代理能够估计与非遍历过程固有增长率一致的价值函数。此方法具有模型无关性，无需改变环境反馈机制（如奖励变换或目标函数修改），仅依赖代理暴露于时间轨迹来实现改进。",
      "result": "论文结果显示，在非遍历环境中，传统和深度强化学习方法均产生次优策略；通过引入时间依赖，策略性能得到改善，能够更准确地估计价值函数并收敛到更优解。与基线方法相比，这种模型无关的解决方案在非遍历动态下表现优异，但摘要未明确说明具体的性能指标数据（如准确率提升）。",
      "conclusion": "本研究贡献了非遍历系统中强化学习方法的研究，提出了一种模型无关的解决方案，通过时间依赖提升深度RL的性能。这为处理非遍历环境下的优化问题提供了新思路，具有学术价值和实际应用潜力；未来工作可探索更广泛的环境类型或结合其他技术以进一步优化。",
      "tags": [
        "Deep Reinforcement Learning",
        "Non-Ergodic Environments",
        "Temporal Dependence",
        "Value Function Estimation",
        "Model-Agnostic Solutions"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:10.540335Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08719",
    "title": "Soft Partition-based KAPI-ELM for Multi-Scale PDEs",
    "authors": [
      "Vikas Dwivedi",
      "Monica Sigovan",
      "Bruno Sixou"
    ],
    "abstract": "Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08719.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08719",
    "published": "2026-01-13T16:43:38Z",
    "updated": "2026-01-13T16:43:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了基于软分割的KAPI-ELM方法，用于高效求解多尺度偏微分方程。",
      "motivation": "现有物理信息机器学习方法在处理高度振荡、多尺度或奇异扰动的偏微分方程时，面临谱偏差、昂贵的反向传播和需手动调整核或傅里叶频率的挑战。这些问题限制了模型在复杂物理问题中的准确性和效率，尤其是在多尺度现象中难以平衡细节捕捉与计算成本，因此急需开发更高效、自适应的求解方法以扩展应用范围。",
      "method": "该方法的核心是软分割基础的Kernel-Adaptive Physics-Informed Extreme Learning Machine（KAPI-ELM），通过平滑分割长度联合控制配置中心和高斯核宽度，实现从粗到细的连续分辨率，避免了传统方法中的傅里叶特征、随机采样或硬域接口需求。基于有符号距离的权重机制进一步稳定了不规则几何体上的最小二乘学习，确保了参数化的确定性和低维性，无需复杂优化或手动调优。",
      "result": "在八个基准测试中，涵盖振荡常微分方程、高频泊松方程、不规则形状域和刚性奇异扰动对流扩散问题，该方法在准确性上匹配或超越了最先进的物理信息神经网络和函数连接理论变体。关键优势在于仅需单个线性求解，显著降低了计算成本，而基线方法通常涉及更复杂的迭代过程，显示了该方法在效率和性能上的优越性。",
      "conclusion": "该研究的贡献是提出软分割核适应方法，为多尺度偏微分方程提供了快速且架构自由的求解方案，具有学术创新和广泛应用潜力。局限性在于当前仅验证了稳态线性PDEs，未来工作可扩展到非线性或动态问题，以进一步推动物理信息建模的发展。",
      "tags": [
        "Soft Partition",
        "Kernel Adaptation",
        "Physics-Informed Machine Learning",
        "Extreme Learning Machine",
        "Multi-Scale PDEs"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:24.396924Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08703",
    "title": "Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set",
    "authors": [
      "Kaivalya Rawal",
      "Eoin Delaney",
      "Zihao Fu",
      "Sandra Wachter",
      "Chris Russell"
    ],
    "abstract": "Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper \"Evaluating Model Explanations without Ground Truth\", we proposed three principles of explanation evaluation and a new method \"AXE\" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08703.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08703",
    "published": "2026-01-13T16:31:11Z",
    "updated": "2026-01-13T16:31:11Z",
    "comment": "This is a preprint of the paper published at the MURE workshop, AAAI 2026, which builds on a preprint of separate work published at FAccT 2025 (arXiv:2505.10399)",
    "light_analysis": {
      "overview": "论文提出AXE方法，通过评估解释质量区分Rashomon集中模型行为，并以100%成功率检测对抗性公平洗白。",
      "motivation": "可解释AI旨在通过解释揭示模型内部机制，但在Rashomon集中，多个模型性能相似，解释能帮助区分行为以选择部署模型。现有解释评估方法，如与理想地面真理解释对比，可能掩盖行为差异，导致无法检测模型是否使用受保护属性进行不公平预测，从而无法选择合适模型或避免公平问题。因此，需要新的评估方法来准确评估解释质量并突出模型间的差异。",
      "method": "论文提出三个解释评估原则和新方法“AXE”，用于评估特征重要性解释。AXE避免依赖地面真理解释，而是通过分析解释的一致性来评估质量，以突出Rashomon集中模型的行为差异。关键创新在于能识别对抗性设置，其中模型保持相同预测但产生误导性解释，并确定何时使用受保护属性进行预测。摘要未明确具体技术细节，但提到AXE不同于基于模型敏感性或地面真理解比较的策略。",
      "result": "实验结果显示，AXE能以100%成功率检测由Rashomon集替代模型生成的虚假解释，有效识别对抗性公平洗白。这表明AXE在评估解释时能正确确定模型是否使用受保护属性进行预测，与现有评估方法相比，更能揭示行为差异并避免评估虚假解释为高质量，提升了解释评估的可靠性。",
      "conclusion": "论文的主要贡献是AXE方法，它基于新评估原则，提升了解释评估的准确性，帮助在Rashomon集中选择合适模型并检测公平性问题。研究意义在于增强XAI的可信度和实用性，防止因误导解释而导致的错误决策。局限性可能包括方法在更广泛场景中的适用性，未来工作可扩展AXE到其他解释类型和数据集以进一步验证其效果。",
      "tags": [
        "Explainable AI",
        "Rashomon Set",
        "Feature-Importance Explanations",
        "Fairness",
        "Adversarial Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:39.884921Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08699",
    "title": "RAGShaper: Eliciting Sophisticated Agentic RAG Skills via Automated Data Synthesis",
    "authors": [
      "Zhengwei Tao",
      "Bo Li",
      "Jialong Wu",
      "Guochen Yan",
      "Huanyao Zhang",
      "Jiahao Xu",
      "Haitao Mi",
      "Wentao Zhang"
    ],
    "abstract": "Agentic Retrieval-Augmented Generation (RAG) empowers large language models to autonomously plan and retrieve information for complex problem-solving. However, the development of robust agents is hindered by the scarcity of high-quality training data that reflects the noise and complexity of real-world retrieval environments. Conventional manual annotation is unscalable and often fails to capture the dynamic reasoning strategies required to handle retrieval failures. To bridge this gap, we introduce RAGShaper, a novel data synthesis framework designed to automate the construction of RAG tasks and robust agent trajectories. RAGShaper incorporates an InfoCurator to build dense information trees enriched with adversarial distractors spanning Perception and Cognition levels. Furthermore, we propose a constrained navigation strategy that forces a teacher agent to confront these distractors, thereby eliciting trajectories that explicitly demonstrate error correction and noise rejection. Comprehensive experiments confirm that models trained on our synthesized corpus significantly outperform existing baselines, exhibiting superior robustness in noise-intensive and complex retrieval tasks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08699.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08699",
    "published": "2026-01-13T16:25:07Z",
    "updated": "2026-01-13T16:25:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "RAGShaper是一个自动化数据合成框架，通过添加对抗性干扰来增强Agentic RAG代理在噪声环境下的鲁棒性。",
      "motivation": "研究动机源于Agentic RAG（检索增强生成）代理开发中高质量训练数据的稀缺问题，特别是在真实世界检索环境中，噪声和复杂性会影响代理性能。传统手动标注方法不可扩展，且难以捕捉处理检索失败所需的动态推理策略，这阻碍了鲁棒代理的构建。因此，需要自动化数据合成技术来模拟复杂场景，以提升代理在实用任务中的能力。",
      "method": "论文提出了RAGShaper框架，它包含两个核心组件：InfoCurator用于构建密集信息树，并在感知和认知层面添加对抗性干扰；以及一种约束导航策略，迫使教师代理主动面对这些干扰，从而引出能够展示错误纠正和噪声拒绝的代理轨迹。该方法通过自动化构建RAG任务和轨迹，实现了训练数据的高效生成，关键技术特色包括信息树的丰富化和对抗性元素的集成。",
      "result": "实验结果表明，在RAGShaper合成的语料上训练的模型显著优于现有基线方法，在噪声密集和复杂的检索任务中表现出优越的鲁棒性。尽管摘要未提供具体性能指标如准确率提升，但强调了模型的整体效果改进，验证了数据合成框架的有效性。与基线相比，该方法在处理检索失败和动态环境方面有显著增强。",
      "conclusion": "RAGShaper的主要贡献在于提供了一个自动化数据合成框架，解决了Agentic RAG训练数据稀缺的挑战，提升了代理在复杂检索任务中的鲁棒性。研究具有学术价值，推动了RAG技术的实用化，并可能应用于增强真实环境的问题解决能力。局限性方面，摘要未明确说明未来工作方向，但可推断为进一步优化框架或扩展到更多应用场景。",
      "tags": [
        "Retrieval-Augmented Generation (RAG)",
        "Automated Data Synthesis",
        "Adversarial Distractors",
        "Dense Information Trees",
        "Constrained Navigation Strategy"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:24.434593Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08692",
    "title": "Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models",
    "authors": [
      "Keito Inoshita"
    ],
    "abstract": "Predicting nationality from personal names has practical value in marketing, demographic research, and genealogical studies. Conventional neural models learn statistical correspondences between names and nationalities from task-specific training data, posing challenges in generalizing to low-frequency nationalities and distinguishing similar nationalities within the same region. Large language models (LLMs) have the potential to address these challenges by leveraging world knowledge acquired during pre-training. In this study, we comprehensively compare neural models and LLMs on nationality prediction, evaluating six neural models and six LLM prompting strategies across three granularity levels (nationality, region, and continent), with frequency-based stratified analysis and error analysis. Results show that LLMs outperform neural models at all granularity levels, with the gap narrowing as granularity becomes coarser. Simple machine learning methods exhibit the highest frequency robustness, while pre-trained models and LLMs show degradation for low-frequency nationalities. Error analysis reveals that LLMs tend to make ``near-miss'' errors, predicting the correct region even when nationality is incorrect, whereas neural models exhibit more cross-regional errors and bias toward high-frequency classes. These findings indicate that LLM superiority stems from world knowledge, model selection should consider required granularity, and evaluation should account for error quality beyond accuracy.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08692.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08692",
    "published": "2026-01-13T16:17:04Z",
    "updated": "2026-01-13T16:17:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过对比神经模型与大语言模型在名字预测国籍任务中的表现，揭示LLMs因世界知识优势更优。",
      "motivation": "国籍和地区预测在市场营销、人口研究和家谱学等领域具有重要应用价值。传统神经模型依赖于任务特定训练数据，学习名字与国家之间的统计关系，但面临对低频国家泛化能力差和难以区分相同区域内相似国籍的挑战。大型语言模型通过预训练获得广泛的世界知识，有潜力克服这些局限，提升预测的准确性和泛化能力。",
      "method": "本研究采用系统比较方法，评估了六种神经模型和六种LLMs的提示策略，在国籍、地区和大陆三个粒度级别上进行性能测试。通过频率分层分析考察模型对高低频国家的表现差异，并进行错误分析以理解模型错误模式。摘要未明确说明具体的模型架构、数据集细节或技术实现。",
      "result": "实验结果显示，LLMs在所有预测粒度上均优于神经模型，但优势随粒度变粗而缩小。简单的机器学习方法表现出最高的频率稳健性，而LLMs和预训练模型对低频国家预测性能下降。错误分析发现，LLMs倾向于犯‘接近失误’错误，即预测正确地区但国籍错误；神经模型则更多出现跨区域错误和对高频类的偏见。",
      "conclusion": "研究结论表明，LLMs的优越性源于其预训练中的世界知识，模型选择应考虑任务所需的预测粒度，评估应超越准确性关注错误质量。这为国籍预测任务的模型部署和评估提供了学术指导，突出了世界知识在复杂预测中的价值，未来工作可进一步优化低频国家和错误缓解策略。",
      "tags": [
        "Large Language Models",
        "Neural Models",
        "Prompting Strategies",
        "Error Analysis",
        "Nationality Prediction"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:02.921492Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08690",
    "title": "All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond",
    "authors": [
      "Shubham Kulkarni",
      "Alexander Lyzhov",
      "Shiva Chaitanya",
      "Preetam Joshi"
    ],
    "abstract": "Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08690.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08690",
    "published": "2026-01-13T16:15:38Z",
    "updated": "2026-01-13T16:15:38Z",
    "comment": "Accepted at the AI for Medicine and Healthcare (AIMedHealth) Bridge Program, AAAI-26, Singapore. Full-length paper; to appear in Proceedings of Machine Learning Research (PMLR)",
    "light_analysis": {
      "overview": "论文提出了Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE)方法，通过阶段级评估AI对话系统的临床合规性，确保所有必要信息按顺序提供并生成可审计证据。",
      "motivation": "对话AI在临床工作中逐渐应用，但现有评估方法忽略合规性依赖于整个对话过程，导致复杂临床规则难以实用和审计，加剧了技术进展与医疗需求之间的差距。本研究旨在开发更全面的评估框架，弥补传统方法的不足，以提升AI在医疗保健领域的实际应用效果和安全性。",
      "method": "论文提出的OIP-SCE方法通过结构化阶段来评估对话合规性，检查每个临床义务是否按正确顺序满足，并提供清晰证据供临床医生审查。关键创新包括阶段级评估、顺序验证和可审计证据生成。该方法在呼吸道病史和福利验证两个案例研究中演示，将政策转化为可操作步骤，使工程师有明确规范实施，临床医生能控制检查内容。",
      "result": "在呼吸道病史和福利验证的案例研究中，OIP-SCE方法展示了如何通过阶段级证据将临床政策转化为共享、可执行的步骤，提供单一、可审计的评估界面以对齐AI能力与临床工作流程。摘要未明确说明具体性能指标或与基线方法的对比，但强调该方法支持常规、安全使用。",
      "conclusion": "OIP-SCE的主要贡献是提供单一、可审计的评估界面，使临床医生能控制检查内容，工程师有明确规范实施，从而弥合技术进展与医疗需求之间的差距。该研究提升了AI对话系统在临床环境中的合规性和可操作性，支持安全、常规的应用。摘要未明确说明潜在局限性或未来工作方向，但该方法有潜力扩展到其他领域。",
      "tags": [
        "Conversational AI",
        "Compliance Evaluation",
        "Phase-Structured Evaluation",
        "Healthcare Dialogue",
        "Auditable Systems"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:42.976451Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08689",
    "title": "QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models",
    "authors": [
      "Zhaolu Kang",
      "Junhao Gong",
      "Wenqing Hu",
      "Shuo Yin",
      "Kehan Jiang",
      "Zhicheng Fang",
      "Yingjie He",
      "Chunlei Meng",
      "Rong Fu",
      "Dongyang Chen",
      "Leqi Zheng",
      "Eric Hanchen Jiang",
      "Yunfei Feng",
      "Yitong Leng",
      "Junfan Zhu",
      "Xiaoyou Chen",
      "Xi Yang",
      "Richeng Xuan"
    ],
    "abstract": "Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08689.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08689",
    "published": "2026-01-13T16:14:23Z",
    "updated": "2026-01-13T16:14:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "QuantEval基准通过集成回测框架，全面评估大语言模型在金融量化任务中的能力，填补现有评估不足并推动模型在实际交易中的应用。",
      "motivation": "当前，大语言模型在金融量化任务中的评估分散且主要限于知识型问答，缺乏对量化数学推理和策略编码的真实评估。这一问题阻碍了模型在复杂金融环境中的实际部署，因为实际交易需要模型具备高级推理和策略生成能力。QuantEval旨在提供更全面的评估框架，解决现有基准的局限性，以促进LLMs在金融领域的实用化进展。",
      "method": "QuantEval基准设计包含三个评估维度：知识型问答、量化数学推理和量化策略编码。核心创新在于集成CTA风格的回测框架，能够自动执行模型生成的交易策略并使用金融性能指标（如收益、风险）进行评估，实现更真实的量化编码能力测试。此外，研究在领域对齐数据上进行大规模监督微调和强化学习实验以优化模型，并发布完整的回测配置（包括资产范围、成本模型和指标定义）确保可复现性。",
      "result": "论文评估了多个先进的开源和专有大语言模型，结果显示在金融量化任务中，模型与人类专家之间存在显著差距，特别是在量化数学推理和策略编码方面。通过监督微调和强化学习实验，模型性能得到一致提升，但具体性能指标如准确率或收益数据在摘要中未明确说明。这表明现有模型在复杂金融任务中仍有较大改进空间，需进一步优化以缩小与人类专家的差距。",
      "conclusion": "QuantEval基准的引入为评估大语言模型在金融量化能力提供了标准化平台，填补了现有评估空白。其学术价值在于推动LLMs在复杂金融任务中的研究，实际应用价值在于加速模型在真实交易工作流程中的部署。通过发布完整回测配置，确保了实验的严格可复现性。未来工作可扩展评估维度或探索更高效的训练方法以进一步提升模型性能。",
      "tags": [
        "Large Language Models",
        "Quantitative Finance",
        "Benchmarking",
        "Backtesting",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:25.168123Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08684",
    "title": "MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection",
    "authors": [
      "Paolo Italiani",
      "David Gimeno-Gomez",
      "Luca Ragazzi",
      "Gianluca Moro",
      "Paolo Rosso"
    ],
    "abstract": "Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08684.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08684",
    "published": "2026-01-13T16:06:41Z",
    "updated": "2026-01-13T16:06:41Z",
    "comment": "Accepted at EACL 2026 Findings",
    "light_analysis": {
      "overview": "提出MemeWeaver框架，通过新颖的inter-meme图推理机制检测性别歧视和厌女症。",
      "motivation": "研究动机是针对女性在线骚扰更频繁的性别歧视和厌女症检测问题。现有方法大多忽视社会动态，例如攻击者通过类似社区强化偏见和群体认同。图基方法虽然能捕捉互动，但受限于启发式图构建、浅层模态融合和实例级推理，因此需要更有效的解决方案来捕捉在线仇恨的复杂性。",
      "method": "MemeWeaver是一个端到端可训练的多模态框架，核心创新是inter-meme图推理机制，用于检测性别歧视和厌女症。方法中系统评估了多种视觉-文本融合策略，但具体模型架构和数据集细节摘要未明确说明，推断涉及MAMI和EXIST基准的数据集成分析。",
      "result": "在MAMI和EXIST基准测试中，MemeWeaver一致优于现有的最先进基线方法，同时实现更快的训练收敛。虽然没有具体准确率数据，但分析显示学习到的图结构能捕捉有意义的语义模式，揭示了在线仇恨的关系性质。",
      "conclusion": "论文主要贡献是提出了MemeWeaver框架，通过图推理提升性别歧视和厌女症检测性能，学术上为多模态内容审核提供了新方法，实际应用价值在于改善在线安全监测。未来工作方向摘要未明确说明，可能需要进一步扩展模型或探索更广泛的数据集。",
      "tags": [
        "Inter-Meme Graph",
        "Multimodal Fusion",
        "Graph Reasoning",
        "Hate Speech Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:08.971915Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08682",
    "title": "Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization",
    "authors": [
      "Kushal Chawla",
      "Chenyang Zhu",
      "Pengshan Cai",
      "Sangwoo Cho",
      "Scott Novotney",
      "Ayushman Singh",
      "Jonah Lewis",
      "Keasha Safewright",
      "Alfy Samuel",
      "Erin Babinsky",
      "Shi-Xiong Zhang",
      "Sambit Sahu"
    ],
    "abstract": "Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08682.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08682",
    "published": "2026-01-13T16:04:11Z",
    "updated": "2026-01-13T16:04:11Z",
    "comment": "EACL 2026 Industry Track",
    "light_analysis": {
      "overview": "本论文通过工业案例研究，提出一种适应性生命周期方法来构建可靠的多派对話摘要系统。",
      "motivation": "多派对話摘要在工业应用中至关重要，可提升知识传递和运营效率，但自动生成高质量摘要具有挑战性，因为实际需求复杂多变。现有研究主要依赖静态数据集和基准，而在现实场景中，摘要需求会持续演变，导致传统方法难以适应。因此，需要开发可适应的系统来解决这些问题，并应对动态环境中的评估困难和任务主观性。",
      "method": "论文提出一种基于代理架构的系统来总结多派对話互动，通过任务分解实现组件优化，支持适应性生命周期。关键创新包括稳健的评估方法以应对需求演变和主观性，以及利用代理架构进行任务分解。具体数据集和模型架构摘要未明确说明，但涉及LLM提示词的应用和优化，同时考虑上游数据瓶颈的影响。",
      "result": "论文未提供具体性能指标数据，但通过工业案例研究分享了实践见解。主要成果包括：评估方法在需求变化时显示出稳健性；组件优化提高了系统的适应性；识别了上游数据瓶颈问题；并揭示了LLM提示词可移植性差导致的供应商锁定挑战，与基线方法相比提供了更务实的指导。",
      "conclusion": "本论文总结了一种适应性生命周期方法，为多派对話摘要系统开发提供实践框架。主要贡献在于指导从业者构建可靠、可适应系统，并启发未来研究，如改进评估方法和应对数据瓶颈。学术价值在于为应用研究提供方向，实际价值在于支持工业领域知识管理。局限性包括供应商锁定风险，未来工作可探索更有效的评估技术和数据解决方案。",
      "tags": [
        "Dialogue Summarization",
        "Agentic Architecture",
        "LLM Prompts",
        "Task Decomposition",
        "Adaptable Lifecycle"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:43.649382Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08679",
    "title": "PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning",
    "authors": [
      "Xiaoyou Liu",
      "Xinyi Mou",
      "Shengbin Yue",
      "Liang Wang",
      "Yuqing Wang",
      "Qiexiang Wang",
      "Tianrui Qin",
      "Wangchunshu Zhou",
      "Zhongyu Wei"
    ],
    "abstract": "As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08679.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08679",
    "published": "2026-01-13T16:02:35Z",
    "updated": "2026-01-13T16:02:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "PersonaDual框架通过自适应推理在单一模型中平衡个性化和客观性，解决了大型语言模型中的个性化与事实正确性冲突问题。",
      "motivation": "随着用户对大型语言模型（LLMs）个性化需求的增加，个性化信息虽然能提升交互体验，但可能损害客观性和事实正确性，特别是在信息与问题不匹配时。现有方法往往难以有效平衡个性化和客观性，导致要么过于个性化而忽略事实，要么过于客观而失去用户偏好适应性。因此，研究旨在开发一种新框架，以动态切换推理模式，确保在个性化交互中不牺牲客观性，提升模型的实际应用价值。",
      "method": "PersonaDual框架的核心方法是在单一模型中学习两种推理模式：通用客观推理和个性化推理。首先通过监督微调（SFT）训练模型掌握这两种模式的基础能力。接着，引入强化学习进行进一步优化，使用提出的DualGRPO算法来改进模式选择，实现根据上下文自适应切换。关键创新在于自适应推理机制，允许模型在保持事实准确性的同时，灵活融入用户偏好，从而平衡个性化和客观性。",
      "result": "实验结果表明，PersonaDual在客观和个性化基准测试中有效平衡了个性化和客观性。它减少了由个性化引入的干扰，实现接近无干扰的性能，并更好地利用有益的个性化信号来改进客观问题解决能力。与基线方法相比，框架在保持个性化好处的同时，显著降低了干扰，提升了整体性能。摘要未明确提供具体性能数据，但整体效果验证了框架的有效性。",
      "conclusion": "PersonaDual的主要贡献是提出了一个自适应框架，成功平衡了LLMs中的个性化和客观性，增强了模型的实用性和可靠性。其学术价值在于探索推理模式的动态切换技术，实际应用价值广泛，如个性化助手和教育工具。潜在局限性可能包括模式切换的精细度不足，未来工作可优化算法或扩展到更多复杂任务。",
      "tags": [
        "Large Language Models",
        "Personalization",
        "Reinforcement Learning",
        "Supervised Fine-Tuning",
        "Adaptive Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:45.920716Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08676",
    "title": "Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance",
    "authors": [
      "Yilei Zhao",
      "Wentao Zhang",
      "Xiao Lei",
      "Yandan Zheng",
      "Mengpu Liu",
      "Wei Yang Bryan Lim"
    ],
    "abstract": "Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08676.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08676",
    "published": "2026-01-13T15:58:29Z",
    "updated": "2026-01-13T15:58:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出ESGAgent，一个专家级层次化多代理系统，以及一个全面的三级基准，以解决ESG分析中的数据碎片化和复杂工作流挑战，提升可持续金融领域的智能分析能力。",
      "motivation": "ESG分析对于评估企业可持续性和伦理绩效至关重要，但在实际应用中，数据分散于非结构化源中导致信息碎片化，使得专业分析困难。现有的大型语言模型在处理复杂的多步骤审计工作流时表现不足，难以满足严格的分析需求，影响了可持续金融决策的准确性和效率。因此，开发更有效的代理系统和评估工具成为当前研究的重要方向。",
      "method": "本研究引入了ESGAgent，这是一个层次化多代理系统，集成专门的工具集，包括检索增强、网络搜索和领域特定功能，以自动化生成深入的ESG分析。关键创新在于多代理架构的协同工作，通过工具处理数据整合问题。同时，构建了一个基于310份企业可持续发展报告的三级基准，涵盖从原子问答到综合分析的评估任务，用于系统化测试代理能力。",
      "result": "实证评估表明，ESGAgent在原子问答任务上达到平均84.15%的准确率，优于当前最先进的闭源大型语言模型。在专业报告生成方面，系统成功整合了丰富的图表和可验证引用，显著提升了分析质量和实用性。基准测试结果证实了其诊断价值，为评估代理在高风险垂直领域的性能提供了可靠指标。",
      "conclusion": "本文的主要贡献在于提出了ESGAgent和全面的基准，这推动了ESG智能分析的发展，为可持续金融等领域的代理能力评估建立了重要测试平台。研究具有学术价值和实际应用意义，虽然成果显著，但未来工作可能包括扩展数据集或优化代理架构，以应对更广泛的复杂场景（摘要未明确说明局限性）。",
      "tags": [
        "Large Language Model",
        "Multi-Agent System",
        "Retrieval Augmentation",
        "Benchmark Evaluation",
        "ESG Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:45.013869Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08674",
    "title": "Além do Desempenho: Um Estudo da Confiabilidade de Detectores de Deepfakes",
    "authors": [
      "Lucas Lopes",
      "Rayson Laroca",
      "André Grégio"
    ],
    "abstract": "Deepfakes are synthetic media generated by artificial intelligence, with positive applications in education and creativity, but also serious negative impacts such as fraud, misinformation, and privacy violations. Although detection techniques have advanced, comprehensive evaluation methods that go beyond classification performance remain lacking. This paper proposes a reliability assessment framework based on four pillars: transferability, robustness, interpretability, and computational efficiency. An analysis of five state-of-the-art methods revealed significant progress as well as critical limitations.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08674.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08674",
    "published": "2026-01-13T15:53:41Z",
    "updated": "2026-01-13T15:53:41Z",
    "comment": "Accepted for presentation at the Brazilian Symposium on Cybersecurity (SBSeg) 2025, in Portuguese language",
    "light_analysis": {
      "overview": "本文提出了一个基于可迁移性、鲁棒性、可解释性和计算效率的深度伪造检测器可靠性评估框架。",
      "motivation": "深度伪造技术作为人工智能生成的合成媒体，在教育和创意方面有积极应用，但也引发了欺诈、虚假信息和隐私侵犯等严重社会问题。尽管检测技术不断进步，现有评估方法往往局限于分类性能，忽略了实际应用中的可靠性考量。因此，本研究旨在弥补这一不足，强调开发全面的评估标准的重要性，以确保检测器在面对真实世界挑战时的有效性和鲁棒性，从而应对深度伪造带来的潜在风险。",
      "method": "本文提出了一种可靠性评估框架，基于四个关键支柱：可迁移性、鲁棒性、可解释性和计算效率，以超越传统的单维度性能评估。该方法整合多维度指标，系统性分析检测器在不同条件下的表现。研究中，对五种最先进的深度伪造检测方法进行了评估，以应用该框架。摘要未明确说明具体的数据集和模型架构细节，但框架设计用于通过综合视角评估检测器的实际可靠性和适用性。",
      "result": "对五种最先进方法的分析揭示了它们在可靠性方面的显著进展和关键局限性，例如在某些条件下表现出较好的鲁棒性或可迁移性，但在可解释性或计算效率上可能不足。摘要未提供具体的性能指标数据，如准确率提升或效率改进的百分比，但强调了通过框架评估可以识别检测器的整体优势与弱点，为后续优化提供参考。",
      "conclusion": "本研究的主要贡献在于开发了一个全面的可靠性评估框架，推动了深度伪造检测器评估向多维度发展，具有重要的学术和实际应用价值。学术上，该框架促进了更科学的评估标准；实际上，有助于开发更可靠的检测系统以应对社会挑战。尽管存在关键局限性，如现有方法在特定方面的不足，这为未来研究指明了方向，例如改进评估方法或增强检测算法的鲁棒性。",
      "tags": [
        "Deepfake Detection",
        "Reliability Assessment",
        "Robustness",
        "Interpretability",
        "Transferability"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:52.127429Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08673",
    "title": "Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock",
    "authors": [
      "Didier Sornette",
      "Sandro Claudio Lera",
      "Ke Wu"
    ],
    "abstract": "Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08673.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08673",
    "published": "2026-01-13T15:53:26Z",
    "updated": "2026-01-13T15:53:26Z",
    "comment": "20 pages",
    "light_analysis": {
      "overview": "本文提出AI对齐失败是结构性的，源于大语言模型内化的人类互动结构，并将人工通用智能风险重构为内生放大器。",
      "motivation": "研究动机是解释大语言模型（LLMs）表现出的欺骗、威胁或敲诈等行为，这些常被误认为是对齐失败或出现恶意代理。这个问题重要，因为现有观点错误地将AI行为归咎于模型意图，忽视了LLMs作为统计工具，内化了人类社交互动的全貌，包括法律、冲突和强制安排，从而导致对AI风险的评估存在偏差。不足之处在于传统解释忽略了人类道德的多元性和情境依赖性，未能认识到这些行为是互动结构的自然结果。",
      "method": "研究方法基于关系模型理论，分析人类互动结构，如市场定价、权威关系和最后通牒议价，将LLMs行为视为对这些结构的统计泛化。核心创新点在于强调AI行为是统计学习的结果，而非道德推理的失败；关键是将敲诈等行为解释为权力不对称下互动机制的极限案例。摘要未明确说明使用的具体数据集或模型架构，但指出LLMs通过内化人类行为记录来学习。",
      "result": "主要实验结果是理论性的，提出了一个解释框架：对齐失败是结构性的，源于人类互动结构的泛化。与传统对齐观点相比，该框架强调风险在于AGI压缩时间尺度，放大人类矛盾，而非模型恶意意图。结果未提供具体性能指标，但通过分析展示了行为如敲诈是正常互动的连续体部分，从而挑战了AI应仅复制社会认可行为的期望。",
      "conclusion": "结论总结了对齐失败是结构性的，由人类互动结构导致，AGI风险在于作为内生放大器放大人类智能和矛盾。学术价值在于提供了一个新视角，将AI风险与人类系统复杂性联系起来；实际应用价值是指导治理方法关注放大效应和机制稳定性，而非仅模型意图。局限性可能包括缺乏实证数据，未来工作方向可涉及具体案例研究或开发应对结构性风险的治理策略。",
      "tags": [
        "Large Language Model",
        "Artificial General Intelligence",
        "AI Alignment",
        "Relational Models Theory",
        "Statistical Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:10.096368Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08670",
    "title": "Parallel Context-of-Experts Decoding for Retrieval Augmented Generation",
    "authors": [
      "Giulio Corallo",
      "Paolo Papotti"
    ],
    "abstract": "Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated \"experts\", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08670.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08670",
    "published": "2026-01-13T15:46:59Z",
    "updated": "2026-01-13T15:46:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了无需训练的Parallel Context-of-Experts Decoding框架，通过将证据聚合从注意力转移到解码过程，恢复检索增强生成中的跨文档推理能力。",
      "motivation": "研究动机源于检索增强生成在处理多文档时面临的效率与交互权衡问题。现有方法中，通过长提示连接文档虽然支持跨文档推理，但导致预填充阶段瓶颈，降低效率；而单独编码文档KV缓存虽能提高速度，却破坏了文档间交互，影响推理效果。因此，需要一种新方法来平衡这两方面，提升系统性能。",
      "method": "论文提出Parallel Context-of-Experts Decoding (Pced)框架，这是一种无需训练的方法。核心创新是将证据聚合从注意力机制转移至解码过程，将检索文档视为孤立“专家”，并采用新颖的检索感知对比解码规则同步专家预测，权衡专家对数概率与模型先验，从而避免构建跨文档共享注意力。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "摘要中未明确提供具体实验结果或性能指标数据，但声称Pced框架能够恢复跨文档推理能力，而无需在注意力层构建文档间交互。这表明该方法在理论上解决了现有方法的权衡问题，可能在实际中提高推理效率和效果，与基线方法对比中避免了预填充瓶颈和交互损失，但具体数值如准确率或速度提升未详细说明。",
      "conclusion": "论文的主要贡献是提出了无需训练的Parallel Context-of-Experts Decoding框架，有效解决了检索增强生成中的效率与交互权衡，通过解码过程恢复跨文档推理，具有学术创新和实际应用价值，如优化大型语言模型的多文档任务性能。未来工作可能包括在实际数据集上验证、扩展解码规则或评估局限性。",
      "tags": [
        "Retrieval Augmented Generation",
        "Parallel Decoding",
        "Contrastive Decoding",
        "Attention Mechanism",
        "Decoding Process"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:17.147825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08668",
    "title": "Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification",
    "authors": [
      "Kyuri Im",
      "Shuzhou Yuan",
      "Michael Färber"
    ],
    "abstract": "While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08668.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08668",
    "published": "2026-01-13T15:45:31Z",
    "updated": "2026-01-13T15:45:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究系统分析了大型语言模型在仇恨言论净化中的虚假拒绝行为偏见，并提出了一种跨翻译策略来有效缓解这种拒绝。",
      "motivation": "大型语言模型广泛应用于仇恨言论净化，但由于安全机制，模型常错误拒绝处理相关提示，导致任务失败。这种虚假拒绝行为源于上下文和语言偏见，影响了模型实用性和公平性。现有方法往往忽视这些偏见，使得模型在敏感内容处理上过度谨慎，阻碍了实际应用。因此，深入研究偏见根源对于开发更可靠的缓解策略至关重要，以提升模型在安全性和效率之间的平衡。",
      "method": "研究采用系统化方法，评估了九个大型语言模型在英语和多语言数据集上的虚假拒绝行为，通过分析语义毒性和目标群体识别偏见因素。关键创新点包括提出跨翻译策略：将英语仇恨言论翻译成中文进行净化，再翻译回英语，以绕过模型偏见。该方法结合多语言评估，旨在保留原始内容同时减少拒绝，提供了一种轻量级的技术路线，无需复杂模型修改即可实施缓解。",
      "result": "实验结果显示，大型语言模型不成比例地拒绝语义毒性更高的输入以及针对国籍、宗教和政治意识形态的仇恨言论。多语言数据集的虚假拒绝率低于英语数据集，但模型仍表现出语言依赖的偏见。跨翻译策略显著减少了虚假拒绝，同时保持了内容完整性，证明了其作为有效和轻量级缓解方法的可行性，尽管摘要未明确具体数据提升百分比，但效果明显优于基线拒绝行为。",
      "conclusion": "本研究的贡献在于揭示了大型语言模型在仇恨言论净化中的虚假拒绝偏见，并提出了跨翻译策略作为缓解手段。这具有重要学术价值，有助于理解模型安全机制的局限性，并推动更公平的AI系统开发。实际应用中，该策略可用于改进仇恨言论处理工具，提升模型适应性。未来工作可探索更多语言对或扩展偏见分析，以进一步优化模型性能并减少过度拒绝问题。",
      "tags": [
        "Large Language Models",
        "Hate Speech Detoxification",
        "Bias Analysis",
        "Cross-Translation",
        "Multilingual Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:22.811545Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08662",
    "title": "From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial",
    "authors": [
      "Abhijit Sen",
      "Sonali Panda",
      "Mahima Arya",
      "Subhajit Patra",
      "Zizhan Zheng",
      "Denys I. Bondar"
    ],
    "abstract": "This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.",
    "categories": [
      "cs.AI",
      "quant-ph"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08662.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08662",
    "published": "2026-01-13T15:40:55Z",
    "updated": "2026-01-13T15:40:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提供了一个针对初学者的教程，通过示例驱动的方法使强化学习更易于理解，特别关注从经典到量子强化学习的过渡及其在量子控制中的应用。",
      "motivation": "强化学习理论复杂，学生在从概念理解过渡到实践编码时面临显著挑战，现有教程往往过于抽象，缺乏实操指导。这导致本科生难以应用RL技术解决实际问题，尤其是在前沿领域如量子控制中。本教程旨在桥接这一差距，通过易理解的讲解帮助学生克服学习障碍，促进RL的普及和教育效果。",
      "method": "本教程采用示例驱动的教学方法，结合清晰解释和动手示例，帮助学生从理论转向实现。关键创新点在于将经典与量子强化学习概念以易懂方式呈现，并通过代码实例演示应用过程。摘要未明确说明具体使用的数据集或模型架构，但强调了实践导向的学习路径。",
      "result": "教程旨在提升学生应用强化学习的技能，但摘要未提供具体的性能指标或效率改进数据。它侧重于教学成效，可能通过反馈评估学习效果，与基线方法如传统理论教学相比，更注重实操能力培养。具体实验结果在摘要中未明确说明。",
      "conclusion": "本教程的主要贡献是降低了强化学习的学习门槛，特别针对量子强化学习领域，促进AI教育发展。学术价值在于提供结构化教学资源，实际应用价值在于帮助学生将RL应用于量子控制等场景。局限性可能在于内容深度有限，未来工作可扩展到更高级主题或多样化应用领域。",
      "tags": [
        "Reinforcement Learning",
        "Quantum Reinforcement Learning",
        "Quantum Control",
        "Tutorial",
        "Example-Based Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:10.354752Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08659",
    "title": "TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations",
    "authors": [
      "Hamid Gadirov",
      "Martijn Westra",
      "Steffen Frey"
    ],
    "abstract": "Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08659.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08659",
    "published": "2026-01-13T15:36:52Z",
    "updated": "2026-01-13T15:36:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了使用卷积自编码器在集成和时间依赖模拟中进行基于重建的异常检测，并通过对比2D和3D模型强调了时空上下文的关键作用。",
      "motivation": "高维、时间依赖模拟数据中的异常检测具有挑战性，因为复杂的空间和时间动态使得传统方法难以有效处理时空交互。本研究针对这一问题，旨在通过基于重建的方法改进异常检测，尤其关注动态模拟如流体力学中的Kármán vortex street模拟，以解决现有技术在捕获时空依赖方面的不足，提升检测的鲁棒性和准确性。",
      "method": "研究使用卷积自编码器对参数化的Kármán vortex street模拟数据集进行重建，并比较2D和3D方法。2D autoencoder处理单帧图像，专注于捕捉空间异常；3D autoencoder则处理短时态堆栈，通过集成时空上下文来检测异常运动模式。关键创新在于利用3D卷积神经网络扩展了重建能力，以更好地适应动态模拟中的时间依赖特性，而无需依赖复杂的手工特征工程。",
      "result": "实验结果摘要未明确说明具体性能指标，但指出2D模型有效识别了单时间步的局部空间异常，而3D模型通过时空上下文检测到异常运动模式并减少了跨时间的冗余检测。此外，在体积数据评估中发现重建误差受空间质量分布影响，高度集中区域的误差大于分散配置，这暗示了检测方法对数据分布的敏感性。这些结果强调了时序信息对提高异常检测稳健性的重要性。",
      "conclusion": "本研究验证了时空上下文在动态模拟异常检测中的关键作用，通过2D和3D卷积自编码器的对比，为基于重建的方法提供了新视角。学术上，它推动了时空数据分析技术的发展；应用上，可用于模拟监控、故障检测等领域。未来工作可进一步优化模型架构或扩展到更多复杂时空数据集，以应对更高维挑战，摘要未明确说明具体局限性。",
      "tags": [
        "Anomaly Detection",
        "Convolutional Autoencoder",
        "Spatio-Temporal Analysis",
        "Ensemble Simulations",
        "Time-Dependent Data"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:40.775074Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08654",
    "title": "RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation",
    "authors": [
      "Yihan Hong",
      "Huaiyuan Yao",
      "Bolin Shen",
      "Wanpeng Xu",
      "Hua Wei",
      "Yushun Dong"
    ],
    "abstract": "The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08654.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08654",
    "published": "2026-01-13T15:31:42Z",
    "updated": "2026-01-13T15:31:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出RULERS框架，通过编译自然语言评分标准为可执行规范并实施证据锚定评分，显著增强LLM评判的鲁棒性和可验证性。",
      "motivation": "LLM-as-a-Judge范式旨在实现基于评分标准的可扩展评价，但大语言模型的生成随机性使得对齐冻结黑盒模型与人类标准面临挑战。现有方法存在三个关键不足：提示敏感性导致评分标准不稳定、推理过程缺乏可审计证据、评分尺度与人类界限不匹配。这些问题限制了LLM评判的可靠性和实用性，因此急需开发更鲁棒的解决方案来提升评价准确性。",
      "method": "论文提出RULERS框架，采用编译器-执行器架构将自然语言评分标准编译成版本化、不可变的规范包。关键创新包括强制执行结构化解码以验证确定性证据，并应用基于Wasserstein距离的轻量级事后校准，所有操作均无需更新模型参数。该方法通过锁定评分标准避免扰动，以证据锚定确保推理可验证，从而改进LLM评判的确定性和一致性。",
      "result": "在论文和摘要生成的基准测试中，RULERS框架在人类一致性方面显著优于代表性基线，具体数据摘要未明确说明。实验表明，该方法能有效对抗对抗性评分标准扰动，保持强稳定性，并使较小模型能够与大型私有评判模型竞争，展示了其鲁棒性和效率的提升，尽管具体数值需参考完整论文。",
      "conclusion": "本研究的主要贡献是证明了可靠LLM评判依赖于可执行评分标准、可验证证据和校准尺度，而非仅优化提示措辞。RULERS框架为LLM评价提供了更鲁棒和可审计的解决方案，具有重要的学术价值，可推动更精准的自动化评估。实际应用中，该框架有望提升教育、内容生成等领域的评价质量。未来工作可扩展至更多任务和模型，并探索自动化标准生成方法。",
      "tags": [
        "Large Language Model Evaluation",
        "Rubric-Based Evaluation",
        "Evidence Verification",
        "Wasserstein Calibration",
        "Compiler-Executor Framework"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:43.047679Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08653",
    "title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding",
    "authors": [
      "Zenghua Liao",
      "Jinzhi Liao",
      "Xiang Zhao"
    ],
    "abstract": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08653.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08653",
    "published": "2026-01-13T15:30:48Z",
    "updated": "2026-01-13T15:30:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Prism框架，通过建模意图元素的逻辑依赖和集成四个模块，实现复杂意图理解以降低用户在LLM交互中的认知负荷。",
      "motivation": "研究动机源于LLMs作为社交平台接口时，用户意图常模糊且动态变化，使得复杂意图理解成为有效人机协作的核心挑战。现有方法如顺序或并行提问未能充分建模意图元素间的逻辑依赖关系，导致澄清效率低下和用户体验受损。基于认知负荷理论，本研究旨在解决这一不足，提升意图澄清的逻辑连贯性，从而优化交互效果。",
      "method": "Prism框架包括四个关键模块：意图分解模块将用户意图分解为结构化元素并识别逻辑依赖；逻辑澄清生成模块基于依赖关系组织问题以确保交互连贯性；意图感知奖励模块通过奖励函数评估澄清轨迹，并利用Monte Carlo模拟生成高质量训练数据；自我进化意图调优模块通过数据反馈迭代优化LLM的逻辑澄清能力。创新点在于结合认知负荷理论，实现逻辑依赖建模和端到端优化。",
      "result": "实验结果显示，Prism在澄清交互、意图执行和认知负载基准上均优于现有基线方法。具体性能指标包括：逻辑冲突率降低至11.5%，用户满意度提升14.4%，任务完成时间减少34.8%，这验证了其在提升逻辑一致性和效率方面的优势。对比实验表明，Prism在多个评估维度上达到最先进水平。",
      "conclusion": "Prism通过复杂意图理解有效降低认知负荷，贡献在于提供一个逻辑连贯的框架。学术价值在于推进意图理解领域，实际应用可优化LLM在社交平台中的用户体验。虽然未明确说明局限性，但未来工作可能涉及扩展场景和增强模型适应性。数据和代码发布促进进一步研究和复现。",
      "tags": [
        "Large Language Model",
        "Complex Intent Understanding",
        "Cognitive Load Theory",
        "Monte Carlo Sample",
        "Intent-Aware Reward"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:44.416762Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08648",
    "title": "Safe Language Generation in the Limit",
    "authors": [
      "Antonios Anastasopoulos",
      "Giuseppe Ateniese",
      "Evgenios M. Kornaropoulos"
    ],
    "abstract": "Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.   This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08648.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08648",
    "published": "2026-01-13T15:25:44Z",
    "updated": "2026-01-13T15:25:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文首次对安全语言生成进行理论分析，基于极限学习模型形式化并证明其困难性。",
      "motivation": "随着极限学习语言领域的发展，现有研究显示语言生成在理论上是可处理的，但在现实世界应用中，确保生成语言的安全性成为关键问题。由于安全语言生成尚未得到充分理论探讨，本研究旨在填补这一空白，分析安全语言生成的可行性和局限性，以指导实际AI系统中的安全设计，避免潜在风险或错误。摘要未明确说明具体不足之处，但暗示了安全性在扩展应用中的重要性。",
      "method": "本研究基于极限学习的计算范式，形式化了安全语言识别和生成的任务。通过理论分析，采用形式化方法建立模型框架，核心创新在于将安全性约束引入语言生成过程，并探讨其在极限学习场景下的计算复杂度。由于是理论工作，未涉及具体数据集或模型架构，重点在于形式化证明和分类，关键特色在于从计算理论角度处理安全问题。",
      "result": "论文证明在极限学习模型中，安全语言识别是不可能的，而安全语言生成至少与普通语言识别同样困难，因此也证明是不可能的。此外，作者讨论了难处理和处理的情况，尽管摘要未详细说明具体性能指标或数据，但这些理论结果为安全语言生成的可行性提供了基础限制，揭示了计算上的挑战，并与基线方法对比显示其不可行性。",
      "conclusion": "本研究首次理论分析了安全语言生成的困难性，证明其在极限学习框架下不可行，这对于理解语言生成的局限性具有重要学术意义，为安全AI系统设计提供理论指导。研究的实际应用价值在于揭示安全约束下的生成任务计算障碍，未来工作可探索简化模型或现实应用中的近似方法，以克服这些理论限制。",
      "tags": [
        "Safe Language Generation",
        "Learning in the Limit",
        "Language Identification",
        "Theoretical Analysis",
        "Formal Verification"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:07.321349Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08646",
    "title": "Provably Safe Reinforcement Learning using Entropy Regularizer",
    "authors": [
      "Abhijit Mazumdar",
      "Rafal Wisniewski",
      "Manuela L. Bujorianu"
    ],
    "abstract": "We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08646.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08646",
    "published": "2026-01-13T15:23:19Z",
    "updated": "2026-01-13T15:23:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于熵正则化的安全强化学习算法，提供可证明的安全保证并优化遗憾界。",
      "motivation": "本研究旨在解决马尔可夫决策过程中带有安全约束的最优策略学习问题。现有基于乐观面对不确定性（OFU）的安全强化学习算法在学习过程中可能存在安全风险，且剧集间变异性较大，影响算法的稳定性和可靠性。本工作通过设计在线算法，确保在学习阶段以高概率满足安全约束，弥补现有方法在安全性和稳定性方面的不足。",
      "method": "论文首先提出基于乐观面对不确定性（OFU）原理的安全强化学习算法，用于处理安全约束下的在线学习问题。在此基础上，引入熵正则化构建主要算法，通过正则化技术平衡探索与利用，关键创新在于利用熵正则化优化算法性能。研究中对两种算法进行了有限样本分析，推导出遗憾界，并在马尔可夫决策过程的到达-避免设置中实现。",
      "result": "通过有限样本分析，论文展示熵正则化算法能够改善遗憾界，并大幅减少基于OFU的安全强化学习算法固有的剧集间变异性。具体效果包括遗憾的优化和算法稳定性的提升，但摘要未提供具体的性能指标数据。与基线OFU算法相比，熵正则化提高了学习过程的可靠性。",
      "conclusion": "论文的主要贡献是提出了基于熵正则化的安全强化学习算法，提供可证明的安全保证并改进遗憾性能。学术价值在于为安全约束下的在线强化学习提供了理论分析和算法创新，实际应用价值体现在增强学习过程的稳定性和安全性。未来工作可能包括拓展算法到更复杂环境或优化正则化参数，但摘要未明确说明。",
      "tags": [
        "Reinforcement Learning",
        "Safety Constraints",
        "Entropy Regularization",
        "Optimism in the Face of Uncertainty",
        "Finite-Sample Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:55.636763Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08645",
    "title": "A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding",
    "authors": [
      "Dilara Torunoğlu-Selamet",
      "Dogukan Arslan",
      "Rodrigo Wilkens",
      "Wei He",
      "Doruk Eryiğit",
      "Thomas Pickard",
      "Adriana S. Pagano",
      "Aline Villavicencio",
      "Gülşen Eryiğit",
      "Ágnes Abuczki",
      "Aida Cardoso",
      "Alesia Lazarenka",
      "Dina Almassova",
      "Amalia Mendes",
      "Anna Kanellopoulou",
      "Antoni Brosa-Rodríguez",
      "Baiba Saulite",
      "Beata Wojtowicz",
      "Bolette Pedersen",
      "Carlos Manuel Hidalgo-Ternero",
      "Chaya Liebeskind",
      "Danka Jokić",
      "Diego Alves",
      "Eleni Triantafyllidi",
      "Erik Velldal",
      "Fred Philippy",
      "Giedre Valunaite Oleskeviciene",
      "Ieva Rizgeliene",
      "Inguna Skadina",
      "Irina Lobzhanidze",
      "Isabell Stinessen Haugen",
      "Jauza Akbar Krito",
      "Jelena M. Marković",
      "Johanna Monti",
      "Josue Alejandro Sauca",
      "Kaja Dobrovoljc",
      "Kingsley O. Ugwuanyi",
      "Laura Rituma",
      "Lilja Øvrelid",
      "Maha Tufail Agro",
      "Manzura Abjalova",
      "Maria Chatzigrigoriou",
      "María del Mar Sánchez Ramos",
      "Marija Pendevska",
      "Masoumeh Seyyedrezaei",
      "Mehrnoush Shamsfard",
      "Momina Ahsan",
      "Muhammad Ahsan Riaz Khan",
      "Nathalie Carmen Hau Norman",
      "Nilay Erdem Ayyıldız",
      "Nina Hosseini-Kivanani",
      "Noémi Ligeti-Nagy",
      "Numaan Naeem",
      "Olha Kanishcheva",
      "Olha Yatsyshyna",
      "Daniil Orel",
      "Petra Giommarelli",
      "Petya Osenova",
      "Radovan Garabik",
      "Regina E. Semou",
      "Rozane Rebechi",
      "Salsabila Zahirah Pranida",
      "Samia Touileb",
      "Sanni Nimb",
      "Sarfraz Ahmad",
      "Sarvinoz Nematkhonova",
      "Shahar Golan",
      "Shaoxiong Ji",
      "Sopuruchi Christian Aboh",
      "Srdjan Sucur",
      "Stella Markantonatou",
      "Sussi Olsen",
      "Vahide Tajalli",
      "Veronika Lipp",
      "Voula Giouli",
      "Yelda Yeşildal Eraydın",
      "Zahra Saaberi",
      "Zhuohan Xie"
    ],
    "abstract": "Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08645.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08645",
    "published": "2026-01-13T15:20:28Z",
    "updated": "2026-01-13T15:20:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出XMPIE，一个并行多语言多模态数据集，用于评估NLP系统在惯用表达理解方面的语言和文化能力。",
      "motivation": "研究动机在于解决NLP系统理解潜在惯用表达（PIEs）的挑战，这些问题与语言社区日常经验和文化紧密相关，现有方法缺乏并行、多语言和多模态的基准来系统评估语言和文化能力。惯用表达是评估NLP系统语言学和文化能力的关键指标，现有基准可能不足以覆盖跨语言和跨模态的复杂场景，因此创建一个综合数据集对于推动AI在语言和文化理解方面的进步至关重要。",
      "method": "研究方法的核心是构建XMPIE数据集，包含34种语言和超过一万个潜在惯用表达项目，集成了文本和视觉模态。每个PIE配有五张图像，涵盖从惯用到字面意义的频谱，包括语义相关和随机干扰项，由语言专家根据多语言指南创建。关键创新在于其并行设计，支持跨语言和跨模态比较分析，为模型评估提供了结构化基准。",
      "result": "摘要未提供具体的实验结果数据，如准确率或效率改进。论文描述的数据集被创建为高质量基准，能够评估模型在多语言和多模态下的惯用理解能力，例如跨语言理解和模态间理解转移。这为未来研究提供了工具，但具体性能指标和与基线方法的对比需要进一步实验验证。",
      "conclusion": "论文的主要贡献是提出XMPIE数据集，为评估NLP系统的惯用理解能力提供新基准，学术价值在于促进跨语言和文化AI研究，实际应用价值可能用于改进机器翻译和多模态系统。局限性包括语言覆盖可能有限或更深入模态分析需求；未来工作可扩展数据集或探索更复杂评估任务。",
      "tags": [
        "Multimodal NLP",
        "Cross-Lingual Benchmark",
        "Idiomaticity Understanding",
        "Multilingual Dataset",
        "Vision-Language Integration"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:14.535523Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08641",
    "title": "Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning",
    "authors": [
      "Yichen Luo",
      "Yebo Feng",
      "Jiahua Xu",
      "Yang Liu"
    ],
    "abstract": "The launch of \\$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.   To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \\$500,000 across these projects.",
    "categories": [
      "cs.AI",
      "q-fin.TR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08641.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08641",
    "published": "2026-01-13T15:13:41Z",
    "updated": "2026-01-13T15:13:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个基于多代理系统和链式思维推理的可解释框架，用于提升meme coin复制交易的抗操纵能力。",
      "motivation": "meme coin复制交易因其策略无关性在市场中流行，但面临操纵机器人泛滥、跟随钱包未来表现不确定和交易延迟等问题，导致盈利无保障。现有大型语言模型虽能处理多模态数据并生成可解释决策，但在处理复杂任务如资产分配时表现不佳，尤其在加密货币市场缺乏领域知识，限制了其应用价值。",
      "method": "论文提出一个可解释的多代理系统，受资产管理团队结构启发，将复杂交易任务分解为识别高质量项目和关键意见领袖钱包等子任务，并协调专业代理协作完成。通过少样本链式思维提示，每个代理获取专业meme coin交易知识，解释多模态数据并生成可解释决策，结合1,000个meme coin项目交易数据进行实证评估。",
      "result": "实证评估显示，所提出的多代理系统在识别高质量meme coin项目上达到73%精度，识别关键意见领袖钱包达到70%精度，优于传统机器学习模型和单大型语言模型。选中的关键意见领袖在这些项目中累计创造了50万美元利润，验证了系统的有效性和实际盈利潜力。",
      "conclusion": "论文开发了一个结合多代理系统和链式思维推理的可解释框架，显著提升了meme coin复制交易的准确性和可解释性，有效抵抗操纵机器人。该研究展示了多代理协作和可解释AI在金融领域的应用价值，为加密货币投资提供新工具，未来可扩展至更广泛的金融场景或探索更多数据集。",
      "tags": [
        "Multi-Agent System",
        "Chain-of-Thought Reasoning",
        "Large Language Models",
        "Cryptocurrency Trading",
        "Explainable AI"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:10.790190Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08634",
    "title": "Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs",
    "authors": [
      "Chenchen Yuan",
      "Bolei Ma",
      "Zheyu Zhang",
      "Bardh Prenkaj",
      "Frauke Kreuter",
      "Gjergji Kasneci"
    ],
    "abstract": "While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08634.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08634",
    "published": "2026-01-13T15:09:34Z",
    "updated": "2026-01-13T15:09:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "通过将道德价值作为可控条件，研究并展示了道德调节如何影响大型语言模型的政治定位。",
      "motivation": "现有研究评估大型语言模型（LLMs）的政治倾向时，主要依赖直接探测或人口统计角色工程来揭示意识形态偏见，但这些方法可能忽略了道德基础的作用。在社会心理学中，政治意识形态被视为道德直觉的下游结果，因此本研究旨在填补这一空白，探索道德价值与政治定位之间的因果关系，以更深入地理解LLMs的偏见来源，并为更全面的模型对齐提供理论基础。",
      "method": "研究采用道德价值调节的方法，将道德取向视为可控条件，通过设定模型赞同或拒绝特定道德价值的提示来调节其行为。使用政治指南针测试评估调节后模型在经济和社会维度上的政治坐标变化，观察道德调节如何主动引导模型轨迹。同时，探讨了角色框架（如不同角色设定）和模型规模对这些效应的系统性影响，并利用多种评估工具验证稳健性。",
      "result": "实验结果表明，道德调节导致LLMs政治坐标的显著、价值特定变化，例如赞同某些道德价值可引发模型在经济或社会维度上的坐标移动。这些效应受角色框架和模型规模的系统性调节，在不同条件下呈现差异，但在实例化相同道德价值的替代评估工具中保持稳健，凸显了调节方法的有效性。摘要未明确说明具体数据（如准确率提升），但描述了变化趋势。",
      "conclusion": "本研究强调了将道德价值等更广泛社会价值纳入LLMs对齐过程的必要性，通过揭示道德调节对政治定位的因果影响，为开发更全面、扎根于社会的对齐技术铺平道路。主要贡献在于提出一种基于道德价值调节的意识形态定位方法，学术价值在于深化对模型偏见的理解，实际应用可能包括改进模型对齐策略，未来工作可扩展到其他价值维度或结合更多实际场景。",
      "tags": [
        "Large Language Models",
        "Moral Conditioning",
        "Political Compass Test",
        "Ideological Positioning",
        "Social Value Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:10.087501Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08631",
    "title": "M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting",
    "authors": [
      "Yaohui Huang",
      "Runmin Zou",
      "Yun Wang",
      "Laeeq Aslam",
      "Ruipeng Dong"
    ],
    "abstract": "Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08631.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08631",
    "published": "2026-01-13T15:05:49Z",
    "updated": "2026-01-13T15:05:49Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "论文提出M^2FMoE模型，通过多分辨率多视图频率混合专家技术自适应预测极端事件时间序列。",
      "motivation": "极端事件时间序列预测面临高方差、不规则动态和稀疏但高影响性的严峻挑战。在实际应用中，现有方法虽能有效建模常规模式，但在极端事件发生时性能显著下降，构成预测误差的主要来源。尽管部分研究尝试引入辅助信号以改善表现，它们仍无法充分捕捉极端事件的复杂时态动态，这凸显了开发更高效自适应预测模型的必要性。研究旨在解决这一关键问题，提升极端事件下的预测准确性，以应对水文等高风险领域的需求。",
      "method": "M^2FMoE模型包含三个核心模块：多视图频率混合专家模块在傅里叶和小波域中分配专家到不同频谱带，通过交叉视图共享带分割对齐频率划分，促进专家协作以捕捉主导和罕见波动；多分辨率自适应融合模块从粗到细分辨率分层聚合频率特征，增强对短期变化和突变敏感性；时态门控集成模块动态平衡长期趋势和短期频率感知特征，提升对正则和极端时态模式的适应性。模型无需极端事件标签，在真实世界水文数据集上验证，结合了频率域分析和自适应机制。",
      "result": "在具有极端模式的真实世界水文数据集上的实验表明，M^2FMoE模型优于最先进的基线方法，具体表现为更高的预测性能，尽管摘要未提供具体数值指标如准确率提升或效率改进细节。模型在不依赖极端事件标签的情况下，有效提升了捕捉极端事件复杂动态的能力，展现了更好的适应性和准确性，证实了其在极端事件时间序列预测中的有效性。",
      "conclusion": "本研究的核心贡献是提出M^2FMoE模型，通过多分辨率多视图频率混合专家技术成功学习了时间序列中的正则和极端模式，显著提高了极端事件预测的准确性。学术价值在于推动了时间序列预测领域的方法创新，结合了频率域分析和自适应机制；实际应用价值在于为水文等高风险领域提供了更可靠的预测工具，可扩展到其他类似场景。未来工作可探索模型的通用性、效率优化或在更多数据集上的验证。",
      "tags": [
        "Time Series Forecasting",
        "Mixture-of-Experts",
        "Frequency Domain Analysis",
        "Multi-Resolution Modeling",
        "Adaptive Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:34.609585Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08629",
    "title": "Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation",
    "authors": [
      "Saumitra Yadav",
      "Manish Shrivastava"
    ],
    "abstract": "Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08629.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08629",
    "published": "2026-01-13T15:05:19Z",
    "updated": "2026-01-13T15:05:19Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "LALITA框架通过词汇和语言特征选择源句子策展平行语料，显著提升低资源机器翻译质量并减少数据需求。",
      "motivation": "在机器翻译训练中，数据策展是关键但研究不足的步骤，尤其是在低资源语言环境下。低资源语言的人工翻译成本高昂，而现有方法依赖人工翻译或有限合成数据，导致数据获取困难。这限制了机器翻译系统性能，因此迫切需要开发高效数据策展框架来筛选源句子，构建高质量的平行语料，以优化系统训练并降低资源需求。",
      "method": "论文提出LALITA框架，基于英语-印地语双语文本评估有效句子选择策略。核心方法是使用词汇和语言特征分析源句子，并专注于选择复杂句子来策展平行语料。通过模拟低资源数据可用性，使用50K到800K英语句子数据集，利用这些特征进行筛选。关键创新在于结合词汇和语言分析，优化句子选择过程，以提高机器翻译训练效率。",
      "result": "实验结果显示，通过训练复杂句子，翻译质量显著提升。测试从50K到800K句子规模的所有数据大小均报告改进性能。LALITA框架高效，在多种语言（包括印地语、奥里亚语、尼泊尔语、挪威尼诺斯克语和德语）中，减少数据需求超过一半，降低了训练成本。这表明框架在数据增强方面具有实用价值，尽管摘要未提供具体准确率数字，但强调了对基线方法的改进。",
      "conclusion": "论文主要贡献是LALITA框架，它通过源句子策展显著提升低资源机器翻译性能，并大幅减少数据需求。研究学术价值在于提供一种新数据策展方法，实际应用则能降低训练成本，促进低资源语言翻译发展。未来工作可进一步扩展框架到更多语言或评估具体性能指标，局限性如未详细探讨特征选择的普适性。",
      "tags": [
        "Machine Translation",
        "Low-resource Languages",
        "Data Curation",
        "Lexical Features",
        "Linguistic Features"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:26.949143Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08626",
    "title": "How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction",
    "authors": [
      "Yingjie He",
      "Zhaolu Kang",
      "Kehan Jiang",
      "Qianyuan Zhang",
      "Jiachen Qian",
      "Chunlei Meng",
      "Yujie Feng",
      "Yuan Wang",
      "Jiabao Dou",
      "Aming Wu",
      "Leqi Zheng",
      "Pengxiang Zhao",
      "Jiaxin Liu",
      "Zeyu Zhang",
      "Lei Wang",
      "Guansu Wang",
      "Qishi Zhan",
      "Xiaomin He",
      "Meisheng Zhang",
      "Jianyuan Ni"
    ],
    "abstract": "Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08626.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08626",
    "published": "2026-01-13T15:03:38Z",
    "updated": "2026-01-13T15:03:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出OrderProbe基准和诊断框架，用于评估大型语言模型在结构重建方面的能力，揭示了其与语义理解的分离。",
      "motivation": "大型语言模型在语义理解方面表现出色，但从混乱输入中重建内部结构的能力尚未充分探索。现有方法如句子级恢复因存在多种有效词序而不适合自动评估，难以精确测量结构重建能力。本研究旨在填补这一空白，强调评估LLMs结构敏感性的重要性，以全面理解其认知能力。研究动机源于实际需求，即需要确定性基准来量化LLMs在结构处理中的表现，而非仅依赖语义评价。",
      "method": "研究者提出了OrderProbe基准，采用固定的四字符表达式在中文、日文和韩文中进行结构重建，这些语言具有唯一的规范顺序，支持精确匹配评分以避免歧义。核心创新包括诊断框架，它评估模型在语义保真度、逻辑有效性、一致性、鲁棒性敏感性和信息密度等多个维度上的表现。该方法使用零样本设置，不依赖特定训练数据，通过自动评分机制简化评估过程。实验涵盖了十二个广泛使用的LLMs，旨在系统测试其结构规划能力。",
      "result": "实验结果表明，即使是前沿的大型语言模型，在结构重建任务中也面临显著困难：零样本恢复准确率经常低于35%，显示出较低的结构敏感性。诊断框架进一步揭示语义回忆和结构规划之间存在一致分离，表明结构鲁棒性并非语义能力的自动副产品。这些发现基于具体数据，如恢复准确率的具体范围，并通过多维度评估提供了全面性能对比。结果突显了LLMs在结构处理方面的局限性，为后续改进提供了基准。",
      "conclusion": "本研究的主要贡献在于引入了OrderProbe基准和诊断框架，为标准化评估LLMs的结构重建能力提供了有效工具。学术价值在于揭示了语义理解和结构规划之间的分离，挑战了结构鲁棒性是语义能力副产品的假设。实际应用价值在于指导LLMs的改进方向，例如增强模型的结构敏感性和鲁棒性。局限性包括仅使用固定表达式和特定语言，未来工作可扩展到更复杂结构和多语言场景。",
      "tags": [
        "Large Language Models",
        "Structural Reconstruction",
        "Deterministic Benchmarking",
        "Diagnostic Framework",
        "Order Sensitivity"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:59.388463Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08623",
    "title": "SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models",
    "authors": [
      "Renyang Liu",
      "Kangjie Chen",
      "Han Qiu",
      "Jie Zhang",
      "Kwok-Yan Lam",
      "Tianwei Zhang",
      "See-Kiong Ng"
    ],
    "abstract": "Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08623.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08623",
    "published": "2026-01-13T15:01:38Z",
    "updated": "2026-01-13T15:01:38Z",
    "comment": "Code at https://github.com/ryliu68/SafeRedir",
    "light_analysis": {
      "overview": "SafeRedir是一种轻量级推理时框架，通过提示嵌入重定向实现图像生成模型的鲁棒unlearning。",
      "motivation": "图像生成模型在训练中经常记忆不良概念，如NSFW图像和受版权保护的艺术风格，导致生成不安全内容，带来实际部署中的安全和合规风险。后处理过滤方法因鲁棒性有限和缺乏细粒度语义控制，无法可靠缓解此问题。现有unlearning方法在模型层面消除有害概念，但存在成本高、生成质量下降或无法抵抗攻击等局限。因此，需要一种更有效的解决方案。",
      "method": "SafeRedir是一个轻量级的推理时框架，通过提示嵌入重定向实现unlearning，无需修改底层图像生成模型。它采用令牌级干预，将不安全提示自适应地重定向到安全语义区域。框架包括两个核心组件：一个潜在感知的多模态安全分类器，用于识别不安全的生成轨迹；一个令牌级delta生成器，用于精确语义重定向，并配备辅助预测器进行令牌掩码和自适应缩放，以局部化和调节干预。",
      "result": "实证结果显示，SafeRedir在多个代表性unlearning任务中表现出色。它实现了有效的unlearning能力，同时保持了高语义和感知保留，图像质量鲁棒，并且增强了对对抗攻击的抵抗力。此外，该框架能够泛化到多种扩散骨架和现有unlearned模型，验证了其即插即用兼容性和广泛适用性。",
      "conclusion": "SafeRedir的主要贡献是提出了一种轻量级推理时unlearning框架，通过提示嵌入重定向解决图像生成模型中的安全问题。该方法在保持生成质量的同时，实现了鲁棒unlearning，并增强了对抗攻击抵抗力。学术上，它为模型安全和可控生成提供了新思路；实际应用上，具有即插即用兼容性，易于部署。",
      "tags": [
        "Image Generation Models",
        "Unlearning",
        "Prompt Embedding Redirection",
        "Diffusion Models",
        "Safety Classification"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:19.143045Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08621",
    "title": "GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning",
    "authors": [
      "Jiajin Liu",
      "Yuanfu Sun",
      "Dongzhe Fan",
      "Qiaoyu Tan"
    ],
    "abstract": "Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08621.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08621",
    "published": "2026-01-13T15:00:57Z",
    "updated": "2026-01-13T15:00:57Z",
    "comment": "16 pages, 5 pages",
    "light_analysis": {
      "overview": "GraphSearch是首个将搜索增强推理扩展到图学习的框架，实现无需任务特定微调的零样本图学习，为图形数据提供了新的代理推理范式。",
      "motivation": "该研究旨在解决搜索增强推理在图结构数据（如电子商务、社交网络和科学引文）中的应用不足问题。现有搜索增强大推理模型主要针对文本数据，未能有效利用图形中的拓扑信号作为先验知识，这限制了检索的针对性和推理效率。图形数据编码的拓扑连接可以改善搜索，但现有方法在生成图形表达查询和平衡结构语义相关性方面面临挑战，导致检索不可靠，因此需填补这一空白以推动零样本图学习的发展。",
      "method": "GraphSearch框架包括Graph-aware Query Planner，用于解耦搜索空间（如单跳、多跳或全局邻居）与语义查询，实现结构化搜索；以及Graph-aware Retriever，基于图拓扑构建候选集，并使用混合评分函数排序以提高检索可靠性。框架还实例化两种遍历模式：GraphSearch-R递归扩展邻居逐跳进行，GraphSearch-F灵活检索局部和全局邻居，无需跳数约束，支持多样化的图操作。关键创新在于首个将搜索增强推理应用于图学习，实现零样本学习，无需任务特定训练或微调。",
      "result": "在多种基准测试中，GraphSearch在零样本节点分类和链接预测任务上展现出竞争性或超越监督图学习方法的性能。实验结果表明，该框架设立了最新成果，验证了其在无额外训练的情况下能有效利用图结构提升推理效果，具体表现为在多个数据集上实现高准确率和效率，与基线方法相比显示出优越的泛化能力，证明了搜索增强推理在图学习中的潜力。",
      "conclusion": "GraphSearch的主要贡献是提出了首个搜索增强推理框架用于图学习，实现零样本学习，提供了一个灵活可泛化的代理推理范式。学术价值在于扩展了搜索增强技术的应用领域，推动图数据处理和智能推理研究；实际价值在于提升图相关任务（如节点分类和链接预测）的效率和准确性。摘要未明确说明局限性，但未来工作可能涉及优化检索算法或应用于更复杂图场景以增强实用性。",
      "tags": [
        "Search-Augmented Reasoning",
        "Graph Learning",
        "Zero-Shot Learning",
        "Graph-aware Query Planning",
        "Hybrid Retrieval"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:34.117496Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08620",
    "title": "ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios",
    "authors": [
      "António Loison",
      "Quentin Macé",
      "Antoine Edy",
      "Victor Xing",
      "Tom Balough",
      "Gabriel Moreira",
      "Bo Liu",
      "Manuel Faysse",
      "Céline Hudelot",
      "Gautier Viaud"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08620.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08620",
    "published": "2026-01-13T15:00:33Z",
    "updated": "2026-01-13T15:00:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了ViDoRe v3基准，用于全面评估多模态检索增强生成在复杂现实场景中的性能。",
      "motivation": "检索增强生成（RAG）在现实应用中需应对解释视觉元素（如表格、图表、图像）、合成跨文档信息及提供准确源定位等挑战。现有基准多聚焦于文本数据、单文档理解或孤立评估检索与生成，难以捕捉多模态复杂场景的深度需求。因此，开发综合性评估框架至关重要，以弥补现有方法在真实世界复杂性上的不足，推动RAG技术在实际应用中的进步。",
      "method": "论文引入ViDoRe v3，这是一个综合性多模态RAG基准，基于视觉丰富文档语料库设计多类型查询，涵盖10个专业领域数据集、约26,000个文档页面和3,099个人工验证查询，支持6种语言。通过12,000小时人工标注，提供高质量注释，包括检索相关性、边界框定位和参考答案。核心创新在于多模态设计、跨领域覆盖和多语言支持，为全面评估RAG系统设立了新标准。",
      "result": "对现有最先进RAG流水线的评估显示，视觉检索器性能优于文本检索器，晚交互模型和文本重排序显著提升系统表现，混合或纯视觉上下文能增强答案生成质量。然而，模型在处理非文本元素、开放式查询和精细视觉定位方面仍面临困难，突显了现有方法的局限性。摘要未明确说明具体性能指标数据，如准确率提升，但通过对比揭示了关键改进方向。",
      "conclusion": "论文主要贡献是发布ViDoRe v3基准，通过多模态和高质量人工标注，为RAG在复杂现实场景的评估提供全面工具。该研究具有重要学术价值，能促进多模态RAG技术发展，实际应用上通过商业许可发布鼓励广泛使用。未来工作可针对模型在视觉元素处理和开放式查询等方面的不足进行优化，进一步拓展其适用性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Multimodal Learning",
        "Visual Document Retrieval",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:18.323535Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08617",
    "title": "SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning",
    "authors": [
      "Leo Fillioux",
      "Omprakash Chakraborty",
      "Ismail Ben Ayed",
      "Paul-Henry Cournède",
      "Stergios Christodoulidis",
      "Maria Vakalopoulou",
      "Jose Dolz"
    ],
    "abstract": "With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08617.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08617",
    "published": "2026-01-13T15:00:03Z",
    "updated": "2026-01-13T15:00:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了语义正交校准（SoC）方法，通过基于Huber的正则化器改进视觉语言模型测试时提示调优中的不确定性校准。",
      "motivation": "随着视觉语言模型在医疗保健或自动驾驶等关键决策系统中的广泛应用，其不确定性估计的校准变得至关重要。然而，在VLM测试时提示调优文献中，这一维度几乎未被探索，研究主要关注提升判别性能。现有方法如全正交约束虽能增强类别分离性，但理论分析表明，它会迫使语义相关类别分离，导致模型过度自信，这在实践中可能带来风险，因此改进校准成为迫切需求。",
      "method": "基于理论分析，本研究提出语义正交校准（SoC），这是一种基于Huber的正则化器。SoC的核心创新在于实现平滑原型分离，同时保持语义邻近性，避免了全正交约束的负面影响。它通过正则化文本提示嵌入的更新，在优化过程中确保类别间的合理关系，从而提高校准性能。摘要未明确说明具体模型架构或数据集细节，但方法聚焦于改进测试时提示调优过程。",
      "result": "在全面实验验证中，SoC方法持续改进校准性能，相比先前基于正交的方法表现出更好的效果。实验表明，SoC在多个标准数据集上降低了校准误差，同时保持了竞争力的判别能力，如准确率等指标不受损。摘要未提供具体数据细节，但通过与基线方法的对比，SoC在改进校准方面具有一致性优势，为实际应用提供了可靠支撑。",
      "conclusion": "本研究的主要贡献是提出了语义正交校准（SoC）方法，有效解决了视觉语言模型测试时提示调优中的校准问题。SoC不仅通过理论分析和实证验证提升了不确定性估计的可靠性，还维持了模型的判别性能，具有重要的学术价值，推动了VLM校准研究。实际应用中，它提高了VLMs在关键决策系统中的可信度。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Vision-Language Models",
        "Test-Time Prompt Tuning",
        "Calibration",
        "Orthogonality Constraints",
        "Huber Regularizer"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:42.986920Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08608",
    "title": "SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling",
    "authors": [
      "Xi Chen",
      "Hongxun Yao",
      "Sicheng Zhao",
      "Jiankun Zhu",
      "Jing Jiang",
      "Kui Jiang"
    ],
    "abstract": "Source-free domain adaptation (SFDA) tackles the critical challenge of adapting source-pretrained models to unlabeled target domains without access to source data, overcoming data privacy and storage limitations in real-world applications. However, existing SFDA approaches struggle with the trade-off between perception field and computational efficiency in domain-invariant feature learning. Recently, Mamba has offered a promising solution through its selective scan mechanism, which enables long-range dependency modeling with linear complexity. However, the Visual Mamba (i.e., VMamba) remains limited in capturing channel-wise frequency characteristics critical for domain alignment and maintaining spatial robustness under significant domain shifts. To address these, we propose a framework called SfMamba to fully explore the stable dependency in source-free model transfer. SfMamba introduces Channel-wise Visual State-Space block that enables channel-sequence scanning for domain-invariant feature extraction. In addition, SfMamba involves a Semantic-Consistent Shuffle strategy that disrupts background patch sequences in 2D selective scan while preserving prediction consistency to mitigate error accumulation. Comprehensive evaluations across multiple benchmarks show that SfMamba achieves consistently stronger performance than existing methods while maintaining favorable parameter efficiency, offering a practical solution for SFDA. Our code is available at https://github.com/chenxi52/SfMamba.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08608.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08608",
    "published": "2026-01-13T14:53:47Z",
    "updated": "2026-01-13T14:53:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "SfMamba通过引入通道式视觉状态空间块和语义一致洗牌策略，实现了高效的源免费域自适应，提升了性能和参数效率。",
      "motivation": "Source-free domain adaptation (SFDA) 旨在解决在无源数据下将预训练模型适应到无标签目标域的实际挑战，这对于应对数据隐私和存储限制至关重要。然而，现有SFDA方法在域不变特征学习中面临感知场与计算效率之间的权衡困难。Mamba的选择性扫描机制能以线性复杂度建模长程依赖，但其视觉版本VMamba在捕获对域对齐关键的通道频率特性，以及在显著域偏移下维持空间鲁棒性方面存在不足，限制了实际应用效果。",
      "method": "SfMamba框架提出两个关键创新：Channel-wise Visual State-Space块，通过通道序列扫描实现域不变特征提取，解决了VMamba在通道频率特性捕获上的局限；以及Semantic-Consistent Shuffle策略，在二维选择性扫描中扰乱背景补丁序列，同时保持预测一致性以减少误差累积，增强空间鲁棒性。这些方法结合了选择性扫描机制的优势，优化了特征学习过程，提升了源免费设置下的域适应能力。数据集和模型架构细节摘要未明确说明。",
      "result": "在多个基准测试的综合评估中，SfMamba表现出比现有方法更强大的性能，具体体现在更高的准确率和鲁棒性上，同时保持良好的参数效率，减少了计算开销。摘要未提供具体性能指标如准确率提升百分比，但强调了一致性优势，表明该方法为SFDA提供了有效的实用解决方案，优于基线方法。",
      "conclusion": "本论文的主要贡献是提出SfMamba框架，通过选择性扫描建模和通道式特征提取，显著改进了source-free domain adaptation的性能和效率。学术上，它为域自适应领域引入了新的技术路径，促进了长程依赖建模的研究；实践中，解决了数据隐私和存储限制问题，支持高效模型部署。局限性或未来工作方向摘要未明确说明，但可推测进一步优化方法或扩展应用到更多场景。",
      "tags": [
        "Source-Free Domain Adaptation",
        "Selective Scan Mechanism",
        "State-Space Models",
        "Channel-wise Feature Extraction",
        "Semantic Consistency"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:46.249100Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08605",
    "title": "ExpSeek: Self-Triggered Experience Seeking for Web Agents",
    "authors": [
      "Wenyuan Zhang",
      "Xinghua Zhang",
      "Haiyang Yu",
      "Shuaiyi Nie",
      "Bingli Wu",
      "Juwei Yue",
      "Tingwen Liu",
      "Yongbin Li"
    ],
    "abstract": "Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08605.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08605",
    "published": "2026-01-13T14:48:34Z",
    "updated": "2026-01-13T14:48:34Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "ExpSeek提出了一种自触发经验寻求方法，通过步骤级主动干预改进网页代理的交互能力。",
      "motivation": "该研究旨在解决网页代理中经验干预的局限性。现有方法通常被动地在任务执行前注入经验作为全局上下文，难以适应代理与环境交互过程中动态变化的上下文观察，这降低了代理在复杂环境中的适应性和效率。因此，开发能主动响应动态变化的经验寻求机制具有重要意义，以克服现有方法的不足。",
      "method": "ExpSeek的核心方法是转向步骤级主动经验寻求。它包括两个关键部分：首先，利用模型内在信号估计步骤级熵阈值，以确定何时进行干预；其次，设计步骤级量身定制的经验内容，以适应特定交互情境。该方法创新性地使用熵作为自触发信号，实现代理的主动经验获取。实验基于Qwen3-8B和32B模型在四个网页代理基准上进行。",
      "result": "在四个挑战性基准测试中，ExpSeek在Qwen3-8B和32B模型上分别实现了9.3%和7.5%的绝对性能改进。实验结果验证了熵作为自触发信号的可行性，并显示即使是4B小规模经验模型也能显著提升更大代理模型的性能。与基线被动经验注入方法相比，ExpSeek在适应性方面表现出显著优势。",
      "conclusion": "论文的主要贡献是提出了ExpSeek方法，通过自触发步骤级经验寻求，提升了网页代理的性能和动态适应性。该研究具有重要的学术价值，为经验干预提供了新思路，实际应用价值在于增强代理在复杂环境中的交互能力。未来工作可探索其他触发信号或扩展该方法到更多应用领域，摘要未明确说明具体局限性。",
      "tags": [
        "Web Agents",
        "Experience Seeking",
        "Entropy Threshold",
        "Self-Triggered Learning",
        "Step-Level Adaptation"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:10.503748Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08604",
    "title": "Interpretability and Individuality in Knee MRI: Patient-Specific Radiomic Fingerprint with Reconstructed Healthy Personas",
    "authors": [
      "Yaxi Chen",
      "Simin Ni",
      "Shuai Li",
      "Shaheer U. Saeed",
      "Aleksandra Ivanova",
      "Rikin Hargunani",
      "Jie Huang",
      "Chaozong Liu",
      "Yipeng Hu"
    ],
    "abstract": "For automated assessment of knee MRI scans, both accuracy and interpretability are essential for clinical use and adoption. Traditional radiomics rely on predefined features chosen at the population level; while more interpretable, they are often too restrictive to capture patient-specific variability and can underperform end-to-end deep learning (DL). To address this, we propose two complementary strategies that bring individuality and interpretability: radiomic fingerprints and healthy personas. First, a radiomic fingerprint is a dynamically constructed, patient-specific feature set derived from MRI. Instead of applying a uniform population-level signature, our model predicts feature relevance from a pool of candidate features and selects only those most predictive for each patient, while maintaining feature-level interpretability. This fingerprint can be viewed as a latent-variable model of feature usage, where an image-conditioned predictor estimates usage probabilities and a transparent logistic regression with global coefficients performs classification. Second, a healthy persona synthesises a pathology-free baseline for each patient using a diffusion model trained to reconstruct healthy knee MRIs. Comparing features extracted from pathological images against their personas highlights deviations from normal anatomy, enabling intuitive, case-specific explanations of disease manifestations. We systematically compare fingerprints, personas, and their combination across three clinical tasks. Experimental results show that both approaches yield performance comparable to or surpassing state-of-the-art DL models, while supporting interpretability at multiple levels. Case studies further illustrate how these perspectives facilitate human-explainable biomarker discovery and pathology localisation.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08604.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08604",
    "published": "2026-01-13T14:48:01Z",
    "updated": "2026-01-13T14:48:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 radiomic fingerprints 和 healthy personas 两种互补策略，以在膝盖 MRI 自动评估中实现患者特异性特征提取和多级可解释性。",
      "motivation": "研究动机在于临床需要既准确又可解释的膝盖 MRI 评估方法。传统 radiomics 方法基于群体级别预定义特征，虽具可解释性但过于限制，无法捕捉患者特异性变异性，且性能常不及端到端深度学习。因此，解决个体化评估和解释性问题对临床采用和疾病分析至关重要，以促进更有效的诊断和治疗决策。",
      "method": "研究方法包括两个核心创新：radiomic fingerprint 动态构建患者特定特征集，使用图像条件预测器估计特征使用概率，并通过透明逻辑回归进行分类，实现特征级可解释性；healthy persona 利用扩散模型训练重建健康膝盖 MRI，为每位患者合成无病理基线，通过比较病理图像与健康 personas 来突出解剖偏差，提供案例级解释。这两种策略可单独或组合应用，支持多级可解释性和个体化分析。",
      "result": "实验结果显示，在三个临床任务中，radiomic fingerprints 和 healthy personas 的性能可与或超过最先进的深度学习模型，具体表现在保持高准确率的同时提供可解释性。案例研究进一步证实这些方法促进人类可解释的生物标志物发现和精确病理定位，增强了方法的临床实用性，但没有提供具体的数值性能指标。",
      "conclusion": "论文主要贡献是结合个体化和可解释性，为膝盖 MRI 评估提供创新解决方案。学术价值在于整合动态特征选择和健康基线合成技术，推动解释性 AI 在医学影像中的应用；实际应用价值在于支持临床决策和疾病分析。未来工作可能涉及扩展到其他医学影像任务或优化模型效率，摘要未明确说明局限性。",
      "tags": [
        "Radiomic Fingerprint",
        "Diffusion Model",
        "Interpretable AI",
        "Patient-Specific Modeling",
        "MRI Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:41.163406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08602",
    "title": "WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation",
    "authors": [
      "Zishan Shu",
      "Juntong Wu",
      "Wei Yan",
      "Xudong Liu",
      "Hongyu Zhang",
      "Chang Liu",
      "Youdong Mao",
      "Jie Chen"
    ],
    "abstract": "Vision modeling has advanced rapidly with Transformers, whose attention mechanisms capture visual dependencies but lack a principled account of how semantic information propagates spatially. We revisit this problem from a wave-based perspective: feature maps are treated as spatial signals whose evolution over an internal propagation time (aligned with network depth) is governed by an underdamped wave equation. In this formulation, spatial frequency-from low-frequency global layout to high-frequency edges and textures-is modeled explicitly, and its interaction with propagation time is controlled rather than implicitly fixed. We derive a closed-form, frequency-time decoupled solution and implement it as the Wave Propagation Operator (WPO), a lightweight module that models global interactions in O(N log N) time-far lower than attention. Building on WPO, we propose a family of WaveFormer models as drop-in replacements for standard ViTs and CNNs, achieving competitive accuracy across image classification, object detection, and semantic segmentation, while delivering up to 1.6x higher throughput and 30% fewer FLOPs than attention-based alternatives. Furthermore, our results demonstrate that wave propagation introduces a complementary modeling bias to heat-based methods, effectively capturing both global coherence and high-frequency details essential for rich visual semantics. Codes are available at: https://github.com/ZishanShu/WaveFormer.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08602.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08602",
    "published": "2026-01-13T14:47:22Z",
    "updated": "2026-01-13T14:47:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "WaveFormer 提出了一种基于波方程的频率-时间解耦视觉建模方法，以轻量级 Wave Propagation Operator 替代注意力机制，提高计算效率。",
      "motivation": "当前视觉建模中，Transformers 的注意力机制虽能捕获视觉依赖，但计算复杂度高且缺乏对语义信息空间传播的原则性解释。现有方法中频率与时间的交互被隐式固定，限制了模型的效率和表达能力。本研究旨在从波的角度重新审视特征传播，提供更高效、理论驱动的框架，以解决计算成本高和建模不足的问题。",
      "method": "论文将特征图视为空间信号，其演化由欠阻尼波方程控制，网络深度对应内部传播时间。通过推导闭式解，实现了频率与时间的解耦，并构建了 Wave Propagation Operator (WPO)，这是一个轻量级模块，具有 O(N log N) 的时间复杂度，低于标准注意力的 O(N^2)。基于 WPO，提出了 WaveFormer 模型家族，可作为标准 Vision Transformers 和 CNNs 的直接替代，应用于图像分类、目标检测和语义分割等任务。",
      "result": "WaveFormer 在图像分类、目标检测和语义分割任务上达到竞争性精度，与基于注意力的方法相比，吞吐量提升达 1.6 倍，FLOPs 减少 30%。实验结果表明，波传播方法有效捕获全局一致性和高频细节，弥补了基于热方法的不足。性能提升验证了频率-时间解耦建模的优越性，同时保持了模型的有效性。",
      "conclusion": "本研究的主要贡献是提出了一种基于波方程的视觉建模新范式，通过频率-时间解耦实现高效且有效的特征传播。学术上，它提供了对视觉依赖建模的新视角；实践中，WaveFormer 可作为轻量级替代方案，应用于多种计算机视觉任务，提高计算效率。未来工作可探索该方法在其他领域的适用性或进一步优化模型架构。摘要未明确说明具体局限性，但潜在方向包括扩展到更复杂场景。",
      "tags": [
        "Wave Equation",
        "Frequency-Time Decoupling",
        "Wave Propagation Operator",
        "Vision Transformer",
        "Computational Efficiency"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:41.057196Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08587",
    "title": "End-to-End Video Character Replacement without Structural Guidance",
    "authors": [
      "Zhengbo Xu",
      "Jie Ma",
      "Ziheng Wang",
      "Zhan Peng",
      "Jun Liang",
      "Jing Li"
    ],
    "abstract": "Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08587.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08587",
    "published": "2026-01-13T14:10:34Z",
    "updated": "2026-01-13T14:10:34Z",
    "comment": "10 pages, 9 figures",
    "light_analysis": {
      "overview": "本文提出MoCha框架，通过仅需单帧掩码实现无需结构指导的端到端视频角色替换，解决了现有方法在复杂场景中的泛化问题。",
      "motivation": "可控视频角色替换因缺乏配对视频数据而面临挑战，现有方法主要依赖重建范式，需要每帧分割掩码和显式结构指导（如骨架、深度），这严重限制了在遮挡、角色对象交互、非标准姿势或困难光照等复杂场景中的泛化能力，常导致视觉伪影和时间不一致。因此，开发更灵活和通用的方法成为重要研究方向。",
      "method": "MoCha框架只需一个任意帧的掩码，避免了先前方法对每帧分割的依赖。关键创新包括引入条件感知RoPE以适配多模态输入条件和增强面部身份，并采用基于强化学习的后训练阶段。为解决训练数据稀缺问题，设计了数据构造管道，包括使用Unreal Engine 5构建的高保真渲染数据集、通过肖像动画技术合成的表情驱动数据集，以及从现有视频掩码对衍生的增强数据集。",
      "result": "广泛实验表明，MoCha方法在视频角色替换任务中显著优于现有最先进方法，在克服复杂场景挑战方面表现优异，实现了更好的视觉质量和时间一致性。摘要未明确说明具体的准确率提升或效率改进等量化指标，但指出该方法大幅优于基线。",
      "conclusion": "MoCha框架的主要贡献是提出了无需结构指导的端到端视频角色替换方法，通过条件感知RoPE和RL后训练提升了身份适应性和生成质量，具有推动可控视频编辑领域发展的学术价值和实际应用潜力。未来工作包括代码发布以促进社区研究，摘要未明确说明具体局限性或更多未来方向。",
      "tags": [
        "Video Character Replacement",
        "Condition-Aware RoPE",
        "Reinforcement Learning",
        "Data Construction Pipeline",
        "Unreal Engine 5"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:54.814360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08584",
    "title": "Ministral 3",
    "authors": [
      "Alexander H. Liu",
      "Kartik Khandelwal",
      "Sandeep Subramanian",
      "Victor Jouault",
      "Abhinav Rastogi",
      "Adrien Sadé",
      "Alan Jeffares",
      "Albert Jiang",
      "Alexandre Cahill",
      "Alexandre Gavaudan",
      "Alexandre Sablayrolles",
      "Amélie Héliou",
      "Amos You",
      "Andy Ehrenberg",
      "Andy Lo",
      "Anton Eliseev",
      "Antonia Calvi",
      "Avinash Sooriyarachchi",
      "Baptiste Bout",
      "Baptiste Rozière",
      "Baudouin De Monicault",
      "Clémence Lanfranchi",
      "Corentin Barreau",
      "Cyprien Courtot",
      "Daniele Grattarola",
      "Darius Dabert",
      "Diego de las Casas",
      "Elliot Chane-Sane",
      "Faruk Ahmed",
      "Gabrielle Berrada",
      "Gaëtan Ecrepont",
      "Gauthier Guinet",
      "Georgii Novikov",
      "Guillaume Kunsch",
      "Guillaume Lample",
      "Guillaume Martin",
      "Gunshi Gupta",
      "Jan Ludziejewski",
      "Jason Rute",
      "Joachim Studnia",
      "Jonas Amar",
      "Joséphine Delas",
      "Josselin Somerville Roberts",
      "Karmesh Yadav",
      "Khyathi Chandu",
      "Kush Jain",
      "Laurence Aitchison",
      "Laurent Fainsin",
      "Léonard Blier",
      "Lingxiao Zhao",
      "Louis Martin",
      "Lucile Saulnier",
      "Luyu Gao",
      "Maarten Buyl",
      "Margaret Jennings",
      "Marie Pellat",
      "Mark Prins",
      "Mathieu Poirée",
      "Mathilde Guillaumin",
      "Matthieu Dinot",
      "Matthieu Futeral",
      "Maxime Darrin",
      "Maximilian Augustin",
      "Mia Chiquier",
      "Michel Schimpf",
      "Nathan Grinsztajn",
      "Neha Gupta",
      "Nikhil Raghuraman",
      "Olivier Bousquet",
      "Olivier Duchenne",
      "Patricia Wang",
      "Patrick von Platen",
      "Paul Jacob",
      "Paul Wambergue",
      "Paula Kurylowicz",
      "Pavankumar Reddy Muddireddy",
      "Philomène Chagniot",
      "Pierre Stock",
      "Pravesh Agrawal",
      "Quentin Torroba",
      "Romain Sauvestre",
      "Roman Soletskyi",
      "Rupert Menneer",
      "Sagar Vaze",
      "Samuel Barry",
      "Sanchit Gandhi",
      "Siddhant Waghjale",
      "Siddharth Gandhi",
      "Soham Ghosh",
      "Srijan Mishra",
      "Sumukh Aithal",
      "Szymon Antoniak",
      "Teven Le Scao",
      "Théo Cachet",
      "Theo Simon Sorg",
      "Thibaut Lavril",
      "Thiziri Nait Saada",
      "Thomas Chabal",
      "Thomas Foubert",
      "Thomas Robert",
      "Thomas Wang",
      "Tim Lawson",
      "Tom Bewley",
      "Tom Bewley",
      "Tom Edwards",
      "Umar Jamil",
      "Umberto Tomasini",
      "Valeriia Nemychnikova",
      "Van Phung",
      "Vincent Maladière",
      "Virgile Richard",
      "Wassim Bouaziz",
      "Wen-Ding Li",
      "William Marshall",
      "Xinghui Li",
      "Xinyu Yang",
      "Yassine El Ouahidi",
      "Yihan Wang",
      "Yunhao Tang",
      "Zaccharie Ramzi"
    ],
    "abstract": "We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08584.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08584",
    "published": "2026-01-13T14:06:03Z",
    "updated": "2026-01-13T14:06:03Z",
    "comment": "Release page: https://mistral.ai/news/mistral-3 ; Models available at https://huggingface.co/collections/mistralai/ministral-3",
    "light_analysis": {
      "overview": "Ministral 3系列是通过Cascade Distillation实现的参数高效密集语言模型，适用于计算和内存受限环境，并具有图像理解能力。",
      "motivation": "本研究旨在解决在计算和内存受限应用中部署高效语言模型的挑战。随着人工智能技术普及，边缘设备和低资源环境对模型效率要求日益提高，现有大型语言模型如GPT系列参数众多，导致计算成本高、内存需求大，限制了广泛应用。因此，开发参数高效的模型至关重要，以提升AI的可访问性和实用性。摘要未明确说明具体动机细节。",
      "method": "论文提出Ministral 3模型系列，采用Cascade Distillation技术进行构建。该方法结合迭代剪枝和持续训练，利用蒸馏技术优化模型参数效率，以提高计算和内存性能。模型提供三种参数规模（3B、8B和14B），每个规模包含三个变体：预训练基础模型用于通用任务，指令微调模型优化对话能力，以及推理模型处理复杂问题。所有模型均具备图像理解功能，支持多模态处理，增强了应用灵活性。",
      "result": "摘要未提供具体的实验结果数据，如准确率或效率指标的提升数值。然而，可以推断该模型在计算和内存效率方面应有所改进，因为其设计目标明确针对参数高效的密集语言模型。论文可能展示了模型在不同任务上的性能表现，但未提及与基线方法的对比情况或具体性能指标。因此，主要效果需参考全文或实验部分来获取详细验证。",
      "conclusion": "本研究的主要贡献是推出了Ministral 3系列参数高效密集语言模型，通过Cascade Distillation技术实现优化，并在Apache 2.0许可证下开源，具有图像理解能力。学术价值在于提出了结合迭代剪枝和蒸馏的新方法，为高效模型设计提供了新思路；实际应用价值在于降低了AI在资源受限环境中的部署门槛，促进边缘计算和高效AI的发展。未来工作可能包括进一步优化模型效率或扩展多模态能力，但摘要未明确说明具体方向或局限性。",
      "tags": [
        "Parameter-Efficient Models",
        "Cascade Distillation",
        "Iterative Pruning",
        "Dense Language Models",
        "Image Understanding"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:13.635664Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08559",
    "title": "WaterCopilot: An AI-Driven Virtual Assistant for Water Management",
    "authors": [
      "Keerththanan Vickneswaran",
      "Mariangel Garcia Andarcia",
      "Hugo Retief",
      "Chris Dickens",
      "Paulo Silva"
    ],
    "abstract": "Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08559.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08559",
    "published": "2026-01-13T13:44:00Z",
    "updated": "2026-01-13T13:44:00Z",
    "comment": "15 pages, 12 figures. This work was developed in collaboration between the International Water Management Institute (IWMI) and Microsoft Research. The supplementary user guide for WaterCopilot is available via this https://cgspace.cgiar.org/server/api/core/bitstreams/a5818d12-629d-4f70-a3b0-ee6c25e82cd7/content",
    "light_analysis": {
      "overview": "论文开发了WaterCopilot，一个基于检索增强生成和工具调用架构的AI虚拟助手，用于跨边界河流盆地的综合水资源管理。",
      "motivation": "跨边界河流盆地的可持续水资源管理面临数据碎片化、实时访问有限和信息源整合复杂等挑战，这导致水治理效率低下、决策延迟，影响水安全和环境可持续性。现有方法通常依赖孤立的数据系统和手动处理，缺乏统一平台来整合静态政策文档与动态水文数据，难以应对跨边界协作的复杂性。因此，开发一个AI驱动的虚拟助手以提供综合、实时的洞察变得尤为重要，旨在弥补传统方式在数据整合和实时响应方面的不足。摘要未明确说明具体现有方法的不足之处，但指出传统方式难以处理数据稀缺和多样性。",
      "method": "WaterCopilot采用检索增强生成和工具调用架构作为核心技术路线，通过两个自定义插件实现数据集成：iwmi-doc-plugin利用Azure AI Search对索引文档进行语义搜索，iwmi-api-plugin查询实时数据库获取环境流量警报、降雨趋势等动态数据。系统支持多语言交互（英语、葡萄牙语、法语），并具备透明来源引用、自动化计算和可视化功能。关键创新包括与Limpopo River Basin数字孪生集成，以及基于AWS的可扩展部署管道，确保在复杂水资源管理场景中的灵活性和效率。整个架构设计注重模块化和可扩展性，以适应不同数据源的整合需求。",
      "result": "使用RAGAS评估框架对WaterCopilot进行测试，总体得分为0.8043，其中答案相关性为0.8571，上下文精度为0.8009。这些指标表明系统能够提供高相关性和精确度的响应，具体生成的环境流量警报、降雨趋势分析和水库水位报告等动态洞察，有效支持水资源管理的实时决策。摘要未明确说明与基线方法的对比数据，但从高分结果可以推断，WaterCopilot在整合多源数据和提升交互效率方面表现优异，尤其在处理复杂查询和提供实时信息时展现出实用价值。",
      "conclusion": "本研究的主要贡献是开发了一个可复制的AI增强框架WaterCopilot，用于增强数据稀缺、跨边界河流盆地的水治理，通过整合静态政策文档和动态水文数据，支持及时、信息充分的决策，有助于加强复杂区域的水安全。创新点包括基于阈值的自动化警报和数字孪生集成。局限性在于处理非英语技术文档的能力有限和API延迟问题，未来工作可以优化多语言处理、减少延迟，并将框架扩展到其他类似的水资源管理场景，进一步验证其在大规模应用中的稳健性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Tool-Calling Architecture",
        "Azure AI Search",
        "RAGAS",
        "Digital Twin"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:51.746120Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08558",
    "title": "REVNET: Rotation-Equivariant Point Cloud Completion via Vector Neuron Anchor Transformer",
    "authors": [
      "Zhifan Ni",
      "Eckehard Steinbach"
    ],
    "abstract": "Incomplete point clouds captured by 3D sensors often result in the loss of both geometric and semantic information. Most existing point cloud completion methods are built on rotation-variant frameworks trained with data in canonical poses, limiting their applicability in real-world scenarios. While data augmentation with random rotations can partially mitigate this issue, it significantly increases the learning burden and still fails to guarantee robust performance under arbitrary poses. To address this challenge, we propose the Rotation-Equivariant Anchor Transformer (REVNET), a novel framework built upon the Vector Neuron (VN) network for robust point cloud completion under arbitrary rotations. To preserve local details, we represent partial point clouds as sets of equivariant anchors and design a VN Missing Anchor Transformer to predict the positions and features of missing anchors. Furthermore, we extend VN networks with a rotation-equivariant bias formulation and a ZCA-based layer normalization to improve feature expressiveness. Leveraging the flexible conversion between equivariant and invariant VN features, our model can generate point coordinates with greater stability. Experimental results show that our method outperforms state-of-the-art approaches on the synthetic MVP dataset in the equivariant setting. On the real-world KITTI dataset, REVNET delivers competitive results compared to non-equivariant networks, without requiring input pose alignment. The source code will be released on GitHub under URL: https://github.com/nizhf/REVNET.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08558.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08558",
    "published": "2026-01-13T13:42:11Z",
    "updated": "2026-01-13T13:42:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "REVNET提出基于Vector Neuron Anchor Transformer的旋转等变点云补全框架，提升任意旋转下的鲁棒性。",
      "motivation": "3D传感器捕获的点云常不完整，导致几何和语义信息丢失。现有补全方法多基于旋转变异框架，训练依赖规范姿势，限制实际应用。随机旋转数据增强虽能部分缓解，但增加学习负担且无法保证任意姿势下的稳定性能，因此需开发旋转等变方法以应对真实场景挑战。",
      "method": "REVNET基于Vector Neuron (VN)网络构建，核心创新包括将部分点云表示为等变锚点集，并设计VN Missing Anchor Transformer预测缺失锚点的位置和特征。扩展VN网络，引入旋转等变偏置公式和基于ZCA的层归一化，增强特征表达力。通过等变与不变VN特征的灵活转换，模型能生成更稳定的点坐标，无需依赖输入姿势对齐。",
      "result": "在合成MVP数据集的等变设置下，REVNET优于最先进方法；在真实世界KITTI数据集上，与无需姿势对齐的非等变网络相比，表现出竞争力。摘要未明确具体性能指标，但结果表明该方法在旋转等变场景中具有优势，提升鲁棒性和实用性。",
      "conclusion": "本研究贡献REVNET框架，实现旋转等变的点云补全，增强在任意旋转下的鲁棒性。学术上扩展了Vector Neuron网络技术；应用上适用于真实场景而无需姿势对齐，具有实际价值。未来工作可进一步优化或验证于更多数据集。",
      "tags": [
        "Point Cloud Completion",
        "Rotation-Equivariant Networks",
        "Vector Neuron Networks",
        "Transformer",
        "3D Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:17.933093Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08557",
    "title": "VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations",
    "authors": [
      "Sushant Gautam",
      "Cise Midoglu",
      "Vajira Thambawita",
      "Michael A. Riegler",
      "Pål Halvorsen"
    ],
    "abstract": "Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08557.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08557",
    "published": "2026-01-13T13:42:05Z",
    "updated": "2026-01-13T13:42:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "VideoHEDGE提出一个用于视频视觉语言模型幻觉检测的模块化框架，通过语义聚类和时空扰动改进基于熵的可靠性估计。",
      "motivation": "视频视觉语言模型在处理视频内容时经常产生高置信度的幻觉，这严重影响了模型的可靠性和实际应用价值。现有不确定性度量方法往往与答案正确性不对齐，导致幻觉检测效果不佳。因此，开发一种能够有效识别和评估模型幻觉的方法至关重要，以提升视频问答系统的可信度和性能。",
      "method": "VideoHEDGE框架首先为给定视频-问题对生成基准答案和多个高温版本，这些生成基于原始视频剪辑以及经过光度与时空扰动的变体。然后，使用自然语言推理或嵌入方法将文本输出聚类成语义假设，通过计算聚类级别的概率质量得到三种可靠性分数：语义熵、RadFlag和视觉增强语义熵。该方法在SoccerChat数据集上评估，采用三个7B参数的Video-VLM模型，包括Qwen2-VL、Qwen2.5-VL和一个领域微调模型。",
      "result": "在SoccerChat基准测试中，视觉增强语义熵（VASE）在所有三个Video-VLM模型中均取得最高的ROC-AUC值，尤其在较大扭曲预算时表现更为突出；而语义熵和RadFlag的检测性能常接近随机水平。嵌入聚类方法在匹配自然语言推理聚类性能的同时显著降低了计算成本。领域微调虽减少了幻觉频率，但对模型校准的改进有限。",
      "conclusion": "本论文的核心贡献在于提出VideoHEDGE框架，通过语义聚类和时空扰动将基于熵的可靠性估计扩展到视频领域，有效提升了幻觉检测的准确性。该研究为评估视频视觉语言模型可靠性提供了新方法，具有重要的学术价值，并促进了可重复的基准测试。未来工作可探索优化计算效率和在不同应用场景下的泛化能力。",
      "tags": [
        "Video Vision-Language Models",
        "Hallucination Detection",
        "Semantic Entropy",
        "Natural Language Inference",
        "Embedding Clustering"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:41.071091Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08556",
    "title": "EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models",
    "authors": [
      "Sören Schleibaum",
      "Anton Frederik Thielmann",
      "Julian Teusch",
      "Benjamin Säfken",
      "Jörg P. Müller"
    ],
    "abstract": "Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08556.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08556",
    "published": "2026-01-13T13:41:49Z",
    "updated": "2026-01-13T13:41:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出EviNAM，一种证据神经可加模型，集成神经可加模型的解释性与不确定性估计，用于实现可解释和可靠的决策支持。",
      "motivation": "该研究旨在解决机器学习中模型可解释性和准确不确定性估计的挑战，这对于高风险决策如医疗或金融应用至关重要。现有方法如贝叶斯神经网络和传统证据学习方法，可能在单次推断中无法同时提供偶然性和认知性不确定性以及明确的特征贡献，这限制了模型的实用性和用户信任。研究动机源于提升模型透明度以支持更可靠的预测，从而弥补现有技术在统一不确定性量化与解释性方面的不足。",
      "method": "EviNAM扩展了证据学习，整合了神经可加模型（NAMs）的可解释性，核心创新在于能够在一次模型推断中同时估计偶然性不确定性和认知性不确定性，并明确展示每个特征的贡献度。这避免了贝叶斯神经网络中需要多重采样的复杂性，简化了不确定性量化流程。方法基于回归任务设计，但自然地可扩展到分类和广义可加模型，使用合成和真实数据进行实验，强调了模型的通用性和结构上的灵活性。",
      "result": "论文在合成和真实数据上的实验表明，EviNAM能够匹配当前最先进的预测性能，显示了其在不牺牲准确性的同时提供更好不确定性和解释性估计的能力。与基线方法的对比中，EviNAM展示了竞争力，例如在预测任务中达到类似的性能水平。摘要未提供具体准确率提升或效率改进的数值细节，但实验结果验证了该方法在集成解释性和不确定性估计方面的有效性。",
      "conclusion": "EviNAM的主要贡献是提供了一个统一框架，结合了模型可解释性和不确定性估计，推动了可解释AI和概率深度学习领域的发展。在学术上，它为不确定性量化和特征分析提供了新方法；在实际应用中，增强了模型在决策支持中的可靠性和信任度。研究以回归为例，但方法可扩展到分类和广义可加模型，为构建更智能、可信的预测系统指明了方向。未来工作可能包括在其他领域的应用验证和潜在局限性的探索，如计算效率或模型扩展性，摘要未明确说明。",
      "tags": [
        "Evidential Learning",
        "Neural Additive Models",
        "Uncertainty Estimation",
        "Aleatoric Uncertainty",
        "Epistemic Uncertainty"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:58.224946Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08549",
    "title": "Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures",
    "authors": [
      "Sucheta Ghosh",
      "Zahra Monfared",
      "Felix Dietrich"
    ],
    "abstract": "We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08549.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08549",
    "published": "2026-01-13T13:36:38Z",
    "updated": "2026-01-13T13:36:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个两阶段多任务学习框架，结合去噪、非线性动力学建模和自监督对比学习，用于分析噪声脑电图信号，显著提升EEG解码性能。",
      "motivation": "脑电图信号常受噪声干扰，传统方法在处理噪声和捕捉非线性动力学特征方面存在不足，影响解码精度和鲁棒性。本研究旨在解决噪声EEG信号的分析问题，通过整合去噪和动力学建模，提高信号处理的准确性和稳定性，以满足脑机接口等应用对高可靠解码的需求。",
      "method": "方法采用两阶段设计：第一阶段使用去噪自编码器去除伪影并稳定时间动态；第二阶段构建多任务架构，基于卷积骨干和Transformer编码器提取时空特征，执行运动想象分类、基于Lyapunov指数的混沌状态判别，以及使用NT-Xent损失的自监督对比表示学习，以增强对非线性动力学的敏感性。",
      "result": "实证研究显示，该框架增强了EEG解码的鲁棒性和泛化能力，在多个数据集上超越了强基线方法和最新技术。摘要未提供具体准确率数据，但报告了性能的显著提升，突出了结合去噪、动力学特征和自监督学习的有效性。",
      "conclusion": "本研究贡献了一个创新的多任务学习框架，成功整合了去噪、非线性动力学和自监督对比学习，推动了EEG信号处理技术的发展。其学术价值在于提出一种处理噪声和复杂动力学的方法，实际应用前景广阔，但摘要未明确说明局限性，未来可探索泛化性和计算效率的进一步优化。",
      "tags": [
        "Contrastive Learning",
        "Multi-Task Learning",
        "Denoising Autoencoder",
        "Transformer",
        "EEG Decoding"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:48.794286Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08545",
    "title": "Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement",
    "authors": [
      "Zhenlong Dai",
      "Zhuoluo Zhao",
      "Hengning Wang",
      "Xiu Tang",
      "Sai Wu",
      "Chang Yao",
      "Zhipeng Gao",
      "Jingyuan Chen"
    ],
    "abstract": "With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \\textbf{LPR} (\\textbf{L}earner-Tailored \\textbf{P}rogram \\textbf{R}epair). We then propose a novel and effective framework, \\textbf{\\textsc{\\MethodName{}}} (\\textbf{L}earner-Tailored \\textbf{S}olution \\textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08545.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08545",
    "published": "2026-01-13T13:31:11Z",
    "updated": "2026-01-13T13:31:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种针对学习者的程序修复框架，结合迭代编辑驱动检索增强的大型语言模型，以修复代码并提供错误描述。",
      "motivation": "随着大型语言模型在编程领域的发展，智能编程辅导系统受到广泛关注。然而，现有研究多集中于修复学习者的错误代码，而忽略了提供错误原因，这限制了学习效果。因此，本研究旨在解决这一不足，引入了学习者定制的程序修复任务（LPR），以同时修复代码和解释错误背景，从而提高编程辅导的质量和实用性。",
      "method": "论文提出了一个两阶段框架。第一阶段利用修复方案检索框架构建解决方案数据库，并采用编辑驱动代码检索方法检索有价值方案，指导大型语言模型识别和修复错误代码中的bug。第二阶段提出解决方案引导的程序修复方法，在检索方案的指导下修复代码并提供解释。此外，还引入了迭代检索增强方法，利用生成代码的评估结果迭代优化检索方向和探索更合适的修复策略，以提升实际编程辅导场景中的性能。",
      "result": "实验结果表明，该方法在新提出的LPR任务上显著优于一系列基线方法，验证了框架的有效性。摘要未明确说明具体性能指标如准确率，但强调了性能大幅提升，表明在修复代码和提供错误描述方面取得了显著改进。",
      "conclusion": "本研究的主要贡献是引入了学习者定制的程序修复任务（LPR）并提出了一个有效的修复框架，具有学术创新性和实际应用价值。框架推动了程序修复领域的发展，并在编程辅导中增强了学习体验。未来工作可能包括扩展更多编程语言或集成更多交互功能，以进一步优化系统性能。",
      "tags": [
        "Large Language Model",
        "Program Repair",
        "Retrieval-Augmented Generation",
        "Iterative Optimization",
        "Code Retrieval"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:39.119521Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08536",
    "title": "DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report",
    "authors": [
      "Ruizhe Li",
      "Mingxuan Du",
      "Benfeng Xu",
      "Chiwei Zhu",
      "Xiaorui Wang",
      "Zhendong Mao"
    ],
    "abstract": "Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08536.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08536",
    "published": "2026-01-13T13:18:39Z",
    "updated": "2026-01-13T13:18:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Deep Research Bench II基准，通过专家派生的细粒度rubrics评估深度研究系统，揭示了现有系统与人类专家的性能差距。",
      "motivation": "Deep Research Systems (DRS) 旨在辅助用户进行网络搜索、信息合成和生成研究报告，但在评估方面存在挑战。现有基准测试常陷入两种失败模式：一些无法充分测试系统分析证据和撰写连贯报告的能力；另一些则依赖过于粗糙或由大语言模型直接定义的评估标准，导致评分相对于人类专家存在偏差且难以验证或解释。因此，缺乏可靠评估方法限制了DRS的发展和应用，亟需一个与人类专家对齐的基准来诊断系统性能。",
      "method": "研究引入了Deep Research Bench II新基准，包含132个基础研究任务，覆盖22个领域，系统需生成长篇研究报告进行评估。核心评估方法基于总计9430个细粒度二进制rubrics，覆盖信息回忆、分析和呈现三个维度。这些rubrics源自精心挑选的专家撰写的调查文章，并通过一个四阶段LLM+人类协同流程构建，结合自动提取和超过400小时的人类专家审查，确保评估标准原子化、可验证并与人类专家判断一致。",
      "result": "在Deep Research Bench II上评估了多个最先进的深度研究系统。结果表明，即使是表现最强的模型也仅满足不到50%的rubrics，具体显示了当前DRS在深度研究任务中的性能不足。这揭示了现有系统与人类专家之间存在的显著差距，突显了在信息合成和报告生成方面的改进空间，为后续研究提供了明确基准对比。",
      "conclusion": "本研究的主要贡献是提出了Deep Research Bench II基准，通过专家派生的细粒度rubrics有效诊断深度研究代理的性能。学术价值在于填补了深度研究系统评估领域的空白，提供了可验证且与人类对齐的评估框架。实际应用价值在于为DRS的开发提供可靠评估工具，帮助识别和优化系统缺陷。未来工作可能包括扩展基准任务、优化rubrics构建流程或探索更多系统改进方法。",
      "tags": [
        "Benchmark Evaluation",
        "Binary Rubrics",
        "Large Language Models",
        "Human-in-the-loop"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:55.295152Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08531",
    "title": "Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse",
    "authors": [
      "Warissara Booranamaitree",
      "Xusheng Du",
      "Yushu Cai",
      "Zhengyang Wang",
      "Ye Zhang",
      "Haoran Xie"
    ],
    "abstract": "Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08531.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08531",
    "published": "2026-01-13T13:17:09Z",
    "updated": "2026-01-13T13:17:09Z",
    "comment": "10 pages, 9 figures, Proceedings of CAADRIA 2026",
    "light_analysis": {
      "overview": "论文提出一个结合生成人工智能和视觉语言模型的三阶段框架，直接通过草图和文本生成立面翻新提案，有效跳过传统详细建模需求。",
      "motivation": "研究动机在于立面翻新作为一种可持续的建筑选择，但现有方法难以平衡保留现有结构与表达新设计意图。当前工作流程需先进行详细的‘原状建模’，这不仅耗时费力，还常导致重复修改，限制了设计效率和创新。这一问题在工业适应性再利用中尤为突出，传统建模阻碍了快速概念迭代和清晰设计沟通，从而影响可持续建筑实践的推广。",
      "method": "研究方法采用一个三阶段框架，整合生成AI和视觉语言模型。首先，微调的VLM模型分析输入的结构草图和文本描述，预测边界框以标识需修改区域和新增组件。其次，稳定扩散模型生成新元素的详细草图，并通过生成性修复管道将其与原始轮廓融合。最后，ControlNet用于细化结果，生成逼真图像。关键创新在于直接处理草图和文本，避免了对详细原状建模的依赖，简化了设计流程。",
      "result": "主要实验在数据集和真实工业建筑上进行，结果表明该框架能够生成保留原有结构并显著提升立面细节质量的翻新提案。虽然摘要未提供具体性能指标，但与基线方法相比，它有效绕过了详细建模需求，提高了设计提案的一致性和效率，使建筑师能更快地探索设计替代方案，验证了方法的实用性和创新性。",
      "conclusion": "结论是该研究贡献了一个高效框架，直接通过草图和文本生成立面翻新提案，免除了传统详细建模步骤。其学术价值在于结合VLM与生成AI，推动了建筑设计自动化；实际应用价值则体现在支持建筑师快速迭代早期概念、清晰沟通翻新意图，促进可持续建筑实践。未来工作可能涉及模型优化以处理更复杂场景或集成更多设计约束。",
      "tags": [
        "Generative AI",
        "Vision-Language Models",
        "Stable Diffusion",
        "ControlNet",
        "Image Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:52.837109Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08521",
    "title": "Your Group-Relative Advantage Is Biased",
    "authors": [
      "Fengkai Yang",
      "Zherui Chen",
      "Xiaohan Wang",
      "Xiaodong Lu",
      "Jiajun Chai",
      "Guojun Yin",
      "Wei Lin",
      "Shuai Ma",
      "Fuzhen Zhuang",
      "Deqing Wang",
      "Yaodong Yang",
      "Jianxin Li",
      "Yikun Ban"
    ],
    "abstract": "Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.   In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08521.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08521",
    "published": "2026-01-13T13:03:15Z",
    "updated": "2026-01-13T13:03:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文揭示了强化学习从验证者奖励中群体相对优势估计的固有偏见，并提出HA-DW自适应方法进行纠正，以提高训练效率和鲁棒性。",
      "motivation": "本研究动机源于强化学习从验证者奖励（RLVR）在大型语言模型后训练中的广泛应用，尤其是在推理任务上。基于群体的方法如GRPO依赖群体相对优势估计以避免学习批评者，但其理论性质理解不足，导致探索和利用不平衡。具体而言，估计器系统性地低估困难提示的优势、高估简单提示的优势，这影响训练效果和模型性能，因此纠正偏见对优化RLVR训练至关重要。",
      "method": "论文提出历史感知自适应难度加权（HA-DW），这是一个自适应重加权方案。关键创新在于基于演化难度锚点和训练动态调整群体相对优势估计，以纠正偏见。该方法集成到GRPO及其变体中，无需改变核心架构。实验在五个数学推理基准上进行，但摘要未明确说明具体的数据集和模型架构细节。",
      "result": "通过理论分析和在五个数学推理基准上的实验，HA-DW方法在集成到GRPO及其变体时，一致性地提升了性能。具体表现为更平衡的探索和利用，减少了优势估计的偏见。与基线方法相比，HA-DW显示出显著改进，但摘要未提供具体的性能指标数值，如准确率提升百分比。",
      "conclusion": "本研究的核心贡献是首次理论分析了群体相对优势估计的偏见问题，并提出HA-DW自适应方案进行纠正。这增强了RLVR训练的理论基础，并为实际应用中大型语言模型的后训练提供了更高效和鲁棒的方法。未来工作可能涉及将该方法扩展到更多领域或进一步优化重加权机制。",
      "tags": [
        "Reinforcement Learning from Verifier Rewards",
        "Group-Relative Advantage Estimation",
        "Adaptive Reweighting",
        "History-Aware Adaptive Difficulty Weighting",
        "Mathematical Reasoning Benchmarks"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:28.526899Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08519",
    "title": "CD^2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning",
    "authors": [
      "Kexin Bao",
      "Daichi Zhang",
      "Hansong Zhang",
      "Yong Li",
      "Yutao Yue",
      "Shiming Ge"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) receives significant attention from the public to perform classification continuously with a few training samples, which suffers from the key catastrophic forgetting problem. Existing methods usually employ an external memory to store previous knowledge and treat it with incremental classes equally, which cannot properly preserve previous essential knowledge. To solve this problem and inspired by recent distillation works on knowledge transfer, we propose a framework termed \\textbf{C}onstrained \\textbf{D}ataset \\textbf{D}istillation (\\textbf{CD$^2$}) to facilitate FSCIL, which includes a dataset distillation module (\\textbf{DDM}) and a distillation constraint module~(\\textbf{DCM}). Specifically, the DDM synthesizes highly condensed samples guided by the classifier, forcing the model to learn compacted essential class-related clues from a few incremental samples. The DCM introduces a designed loss to constrain the previously learned class distribution, which can preserve distilled knowledge more sufficiently. Extensive experiments on three public datasets show the superiority of our method against other state-of-the-art competitors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08519.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08519",
    "published": "2026-01-13T13:01:14Z",
    "updated": "2026-01-13T13:01:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出约束数据集蒸馏框架CD^2，以解决少样本类增量学习中的灾难性遗忘问题，通过融合蒸馏和约束模块提升性能。",
      "motivation": "少样本类增量学习在连续分类中受到广泛关注，但其关键挑战是灾难性遗忘问题，导致模型在学习新类时遗忘旧知识。现有方法通常依赖外部内存存储先前知识，但在处理增量类时未能平等保留关键信息，限制了分类性能。该问题对实际应用如在线学习和自适应系统至关重要，因此开发更有效的方法以维持知识完整性显得尤为重要。",
      "method": "论文提出CD^2框架，包含数据集蒸馏模块和蒸馏约束模块。数据集蒸馏模块合成高度浓缩的样本，通过分类器引导，使模型从少量增量样本中学习紧凑的类相关线索；蒸馏约束模块引入设计损失函数，约束先前学习的类分布，以更充分地保留蒸馏知识。方法在三个公共数据集上进行实验，旨在融合知识蒸馏和约束学习以解决FSCIL中的核心问题。",
      "result": "在三个公开数据集上的广泛实验表明，该方法在少样本类增量学习任务中优于其他先进方法，有效缓解了灾难性遗忘问题。尽管摘要未明确说明具体性能指标如准确率提升百分比，但结果显示了其在对比基线方法时的优越性，证明了框架在保留先前知识和适应新类方面的能力。",
      "conclusion": "本文的主要贡献是提出CD^2框架，通过结合数据集蒸馏和约束学习，有效解决了少样本类增量学习中的灾难性遗忘问题，提升了分类性能。其学术价值在于为持续学习领域提供了创新方法，促进了知识蒸馏技术的应用；实际应用价值体现在支持自适应系统和在线学习场景。未来工作可进一步优化模块设计或扩展到更多数据集。",
      "tags": [
        "Few-Shot Class-Incremental Learning",
        "Dataset Distillation",
        "Constrained Learning",
        "Catastrophic Forgetting",
        "Knowledge Distillation"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:25.159039Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08517",
    "title": "Closed-Loop LLM Discovery of Non-Standard Channel Priors in Vision Models",
    "authors": [
      "Tolgay Atinc Uzun",
      "Dmitry Ignatov",
      "Radu Timofte"
    ],
    "abstract": "Channel configuration search the optimization of layer specifications such as layer widths in deep neural networks presents a complex combinatorial challenge constrained by tensor shape compatibility and computational budgets. We posit that Large Language Models (LLMs) offer a transformative approach to Neural Architecture Search (NAS), capable of reasoning about architectural code structure in ways that traditional heuristics cannot. In this paper, we investigate the application of an LLM-driven NAS framework to the problem of channel configuration. We formulate the search as a sequence of conditional code generation tasks, where an LLM refines architectural specifications based on performance telemetry. Crucially, we address the data scarcity problem by generating a vast corpus of valid, shape-consistent architectures via Abstract Syntax Tree (AST) mutations. While these mutated networks are not necessarily high-performing, they provide the critical volume of structural data required for the LLM to learn the latent relationship between channel configurations and model performance. This allows the LLM to internalize complex design patterns and apply them to optimize feature extraction strategies. Experimental results on CIFAR-100 validate the efficacy of this approach, demonstrating that the model yields statistically significant improvements in accuracy. Our analysis confirms that the LLM successfully acquires domain-specific architectural priors, distinguishing this method from random search and highlighting the immense potential of language-driven design in deep learning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08517.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08517",
    "published": "2026-01-13T13:00:30Z",
    "updated": "2026-01-13T13:00:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于大型语言模型的神经架构搜索方法，通过代码生成和抽象语法树突变优化通道配置，提高模型性能。",
      "motivation": "通道配置搜索涉及调整神经网络层宽度等参数，是一个复杂的组合优化问题，受张量形状兼容性和计算预算限制。传统神经架构搜索方法依赖启发式算法，难以充分推理架构代码的深层结构，导致效率低下。大型语言模型具备强大的代码理解和生成能力，为解决这一问题提供了新思路。因此，本研究旨在探索LLMs在NAS中的应用，以克服现有方法的不足，提升架构搜索的精确性和可扩展性。",
      "method": "本研究设计了一个LLM驱动的NAS框架，将通道配置搜索转化为一系列条件代码生成任务。关键创新是通过抽象语法树（AST）突变生成大量形状一致但性能各异的神经网络架构，解决训练数据稀缺问题。这些生成的架构作为LLM输入，使其学习通道配置与模型性能的潜在关系。LLM基于性能反馈迭代优化架构规格，内部化复杂设计模式以优化特征提取策略。实验在CIFAR-100数据集上进行评估。",
      "result": "在CIFAR-100数据集上的实验验证了该方法的有效性，结果显示LLM驱动的框架在模型准确性方面实现了统计显著的提升。与随机搜索相比，该方法能有效获取领域特定的架构先验，优化特征提取策略。分析表明，LLM学习到的先验知识对性能改进有实质性贡献，突显了语言驱动设计在深度学习中的巨大潜力。",
      "conclusion": "本研究的贡献在于提出一种新颖的LLM-based NAS方法，专注于通道配置优化，并通过AST突变解决数据瓶颈。该方法展示了语言模型在自动化架构设计中的应用价值，为神经架构搜索领域提供了新研究方向。尽管取得了积极成果，未来工作可以探索该方法在其他架构搜索问题上的扩展性，或优化数据生成过程以提高效率。",
      "tags": [
        "Large Language Model",
        "Neural Architecture Search",
        "Abstract Syntax Tree",
        "Channel Configuration",
        "Conditional Code Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:20.445260Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08512",
    "title": "Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces",
    "authors": [
      "Przemysław Spyra"
    ],
    "abstract": "The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08512.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08512",
    "published": "2026-01-13T12:51:58Z",
    "updated": "2026-01-13T12:51:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个综合特征定理，统一了无条件收敛的多个等价条件，并将其应用于算法稳定性分析，连接了泛函分析与计算实践。",
      "motivation": "无限维空间中条件、无条件和绝对收敛的分离（如 Dvoretzky-Rogers 定理所示）对计算算法有根本影响。在有限维中这些概念一致，但在 Banach 空间中严格区分，这可能导致算法不稳定，如随机梯度下降中的梯度累积或信号处理中的系数阈值化。现有研究可能未充分连接这些理论概念与计算实践，因此需要统一理论结果，为算法提供严格的稳定性基础，解决实际应用中的收敛和鲁棒性问题。",
      "method": "研究方法基于泛函分析理论，提出一个综合特征定理来统一无条件收敛的七个等价条件：排列不变性、网收敛、子级数检验、符号稳定性、有界乘子性质、弱一致收敛。这扩展了 Dvoretzky-Rogers 定理，并应用于算法分析，例如在随机梯度下降和基于帧的信号处理中验证顺序无关性。核心创新在于将经典数学定理与现代计算技术连接，为设计数值稳健的求和过程提供理论框架，但摘要未明确提及具体数据集或模型架构。",
      "result": "主要结果是证明了无条件收敛的等价条件定理，并将其应用于算法稳定性分析，例如在随机梯度下降中确保梯度累积的排列不变性，在信号处理中为系数阈值化提供理论支持。然而，摘要未明确提供具体性能指标（如准确率提升或效率改进）或与基线方法的直接对比数据，主要聚焦于理论贡献，表明这些条件能增强计算过程的稳定性和鲁棒性。",
      "conclusion": "本文的主要贡献是统一了无条件收敛的等价条件，为算法稳定性提供了严格的理论基础，连接了经典泛函分析与当代计算实践。这有助于确保顺序无关和数值稳健的求和过程，具有重要的学术价值和实际应用价值。潜在局限性可能包括对特定 Banach 空间或算法的泛化能力，摘要未详细说明未来工作方向，但建议进一步探索更广泛的应用场景。",
      "tags": [
        "Banach Spaces",
        "Unconditional Convergence",
        "Algorithmic Stability",
        "Stochastic Gradient Descent",
        "Frame-Based Signal Processing"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:32.457528Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08511",
    "title": "STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio",
    "authors": [
      "Seong-Gyu Park",
      "Sohee Park",
      "Jisu Lee",
      "Hyunsik Na",
      "Daeseon Choi"
    ],
    "abstract": "Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\\approx$ 1.0) with approximately $42\\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.",
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08511.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08511",
    "published": "2026-01-13T12:51:13Z",
    "updated": "2026-01-13T12:51:13Z",
    "comment": "16 pages, 5 figures",
    "light_analysis": {
      "overview": "该论文提出STAR框架，通过分析输出概率偏移来检测大型语言模型推理时的后door攻击。",
      "motivation": "随着大型语言模型（LLM）越来越多地整合Chain-of-Thought（CoT）等推理机制，推理时后door攻击成为一个新兴威胁。这类攻击在不修改模型参数的情况下注入恶意推理路径，生成语言连贯的输出，从而逃避免规检测方法。这问题至关重要，因为LLM在敏感领域（如医疗、金融）的广泛应用，攻击可能被恶意利用导致安全漏洞，而现有检测技术难以处理这种新攻击面，因此需要创新的解决方案来保障推理过程的安全性和可靠性。",
      "method": "论文提出STAR（状态转移放大比）框架，通过分析模型输出概率偏移来检测后door攻击。核心方法是量化恶意推理路径的统计差异：在模型的通用知识中，这些路径先验概率较低，但恶意输入诱导时后验概率显著升高，形成状态转移放大。STAR利用这种差异，并采用CUSUM算法检测概率分布中的持续异常。关键创新点在于结合概率分析和统计控制技术，框架独立于具体模型架构，适用于不同规模的LLM（如8B到70B参数），实验中使用五个基准数据集进行评估，以验证泛化能力。",
      "result": "实验在多种LLM（参数规模8B至70B）和五个基准数据集上进行，结果显示STAR展现出稳健的泛化能力。AUROC接近1.0，表明检测性能近乎完美，准确率极高。与现有基线方法相比，STAR的效率提升了约42倍，大幅减少计算开销。此外，框架对尝试绕过检测的自适应攻击也表现出鲁棒性，增强了其实用性。这些结果验证了STAR在推理时后door检测方面的有效性和优越性，为实际应用提供了可靠保障。",
      "conclusion": "STAR框架的主要贡献在于提供了一种高效检测推理时后door攻击的方法，通过状态转移放大比和CUSUM算法实现。学术上，该研究扩展了LLM安全领域，为推理攻击检测引入新概念和技术路径。实际上，增强了LLM推理过程的防御能力，有助于防范恶意利用。尽管框架在实验中表现优异，未来工作可能包括扩展到更复杂的推理场景或应对不断演化的攻击策略，摘要未明确说明具体局限性，但可推断需要进一步研究适应性和可扩展性。",
      "tags": [
        "Large Language Model",
        "Chain-of-Thought",
        "Inference-time Backdoor",
        "State-Transition Amplification",
        "CUSUM algorithm"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:02.893677Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08510",
    "title": "STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays",
    "authors": [
      "Qiuyu Tian",
      "Yiding Li",
      "Fengyi Chen",
      "Zequn Liu",
      "Youyong Kong",
      "Fan Guo",
      "Yuyao Li",
      "Jinjing Shen",
      "Zhijing Xie",
      "Yiyun Luo",
      "Xin Zhang"
    ],
    "abstract": "Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08510.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08510",
    "published": "2026-01-13T12:50:58Z",
    "updated": "2026-01-13T12:50:58Z",
    "comment": "66 pages, 9 figures",
    "light_analysis": {
      "overview": "STAGE是一个统一基准，用于评估模型在电影剧本上通过知识图谱构建、问答和角色扮演实现全面叙事理解的能力。",
      "motivation": "电影剧本包含复杂角色关系和时间顺序事件，现有基准仅关注问答或对话生成等单一子任务，无法评估模型是否能构建并使用连贯的故事世界进行跨任务推理和生成。这导致模型在实际叙事应用中缺乏一致性和整体理解能力。STAGE旨在解决这一问题，提供一个综合性平台以测试模型在多种任务中的表现，从而提高AI对长篇叙述的理解水平。",
      "method": "STAGE基准定义了四个核心任务：知识图谱构建、场景级事件摘要、长上下文剧本问答和剧本内角色扮演，所有任务基于共享的叙事世界表示。方法涉及使用150部电影的清洁脚本、精心整理的知识图谱、事件和角色中心注释，支持模型评估构建世界表示、抽象和验证叙事事件、在长叙述中进行推理并生成角色一致响应。关键技术包括多模态数据处理和任务统一设计。",
      "result": "摘要未明确说明具体的实验结果或性能指标，如准确率提升或效率改进。但基准提供了丰富的资源，包括多语言电影脚本和注释，为评估模型在构建世界表示、抽象事件、推理长叙述和生成一致响应方面的能力奠定了基础。未来实验可以基于此基准与基线方法进行对比验证。",
      "conclusion": "STAGE的主要贡献是建立了一个统一基准，全面评估模型在电影剧本叙事理解中的能力，涵盖世界表示构建、事件抽象、长上下文推理和角色一致生成。这有助于推动AI在复杂叙述处理领域的研究，提高模型在实际应用中的连贯性和泛化能力。潜在局限性在于基准聚焦电影剧本，未来工作可扩展到其他长篇文本或多媒体叙事形式，以进一步验证方法的通用性。",
      "tags": [
        "Knowledge Graph Construction",
        "Question Answering",
        "Role-Playing",
        "Narrative Understanding",
        "Long-Context Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:50.867104Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08509",
    "title": "What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting",
    "authors": [
      "Jinkwan Jang",
      "Hyunbin Jin",
      "Hyungjin Park",
      "Kyubyung Chae",
      "Taesup Kim"
    ],
    "abstract": "Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08509.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08509",
    "published": "2026-01-13T12:47:43Z",
    "updated": "2026-01-13T12:47:43Z",
    "comment": "30 pages, 5 figures",
    "light_analysis": {
      "overview": "论文提出了What If TSF（WIT）基准，用于评估情景引导的多模态时间序列预测能力，填补现有基准不足。",
      "motivation": "时间序列预测对现实世界决策至关重要，但现有方法大多为单模态，仅依赖历史数据外推。尽管大语言模型（LLMs）在多模态预测方面有潜力，但现有基准主要提供回顾性或不对齐的原始文本上下文，无法有效验证模型是否真正利用文本输入。人类专家在实际决策中结合历史证据和假设情景，在不同情景下从相同观测产生不同预测，这凸显了开发情景引导多模态基准的重要性，以解决现有方法的局限性。",
      "method": "研究通过引入What If TSF（WIT）基准，将时间序列预测重构为情景引导的多模态任务。核心方法是构建一个数据集，包含时间序列数据和专家制作的文本情景（合理或反事实），旨在评估模型是否能根据上下文文本（尤其是未来情景）调整其预测。关键创新点在于模拟人类专家的预测实践，提供了一个严格的测试环境，以检验模型在多模态条件下的表现。基准公开可用，促进后续研究的验证与对比。",
      "result": "摘要未明确说明具体的实验结果或性能指标。基准的主要贡献在于提供了一个公开的测试平台（访问链接：https://github.com/jinkwan1115/WhatIfTSF），未来研究可基于此评估不同模型在多模态条件下预测的准确性和鲁棒性。与基线方法的对比需在实际应用中通过实验验证，具体效果如准确率提升或效率改进在摘要中未提及。",
      "conclusion": "本论文的核心贡献是提出了WIT基准，为情景引导的多模态时间序列预测提供了严格的评估工具，弥补了现有基准在有效利用文本上下文方面的不足。学术上，该工作推动了多模态预测领域的发展，促进了对模型如何结合文本信息的研究。实际应用中，有助于开发更智能的决策支持系统，提升预测的适应性和可靠性。未来工作可扩展基准数据集或探索更先进的模型架构，以进一步提高性能。",
      "tags": [
        "Time Series Forecasting",
        "Multimodal Forecasting",
        "Large Language Models",
        "Benchmarking",
        "Scenario-Guided Prediction"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:01.095029Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08503",
    "title": "Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care",
    "authors": [
      "Aditya Kumar",
      "Simon Rauch",
      "Mario Cypko",
      "Marcel Naik",
      "Matthieu-P Schapranow",
      "Aadil Rashid",
      "Fabian Halleck",
      "Bilgin Osmanodja",
      "Roland Roller",
      "Lars Pape",
      "Klemens Budde",
      "Mario Schiffer",
      "Oliver Amft"
    ],
    "abstract": "We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08503.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08503",
    "published": "2026-01-13T12:38:33Z",
    "updated": "2026-01-13T12:38:33Z",
    "comment": "31 pages, 9 figures, 3 tables. A supplementary file is also available",
    "light_analysis": {
      "overview": "本文提出了 Temporal Fusion Nexus (TFN)，一个任务无关的多模态嵌入模型，用于整合临床叙述和不规则时间序列，以提升肾移植后护理的预测性能。",
      "motivation": "在肾移植后护理中，准确预测移植物丧失、排斥和死亡率对改善患者预后至关重要，但现有方法面临异构数据整合的挑战。临床实践中存在不规则时间序列和丰富的非结构化叙述文档，这些数据难以被传统模型有效利用。TFN 旨在解决这一问题，通过多模态和任务无关的设计，克服数据异构性和不规则性，以提升预测准确性并辅助临床决策。",
      "method": "TFN 采用多模态嵌入技术，将不规则时间序列和临床叙述整合到一个统一的嵌入空间中。该模型是任务无关的，可灵活应用于多种预测任务。关键创新包括处理不规则时间序列的嵌入机制和文本编码，利用解缠度量确保潜在因子的鲁棒性和可解释性。研究基于 3382 名患者的回顾性队列，通过 SHAP-based attributions 验证模型输出与临床推理的一致性，以增强模型的可信度。",
      "result": "在肾移植后护理的评估中，TFN 在移植物丧失预测上 AUC 达 0.96，优于最先进模型的 0.94；移植物排斥预测 AUC 为 0.84，对比基准的 0.74。死亡率预测 AUC 为 0.86。与单模态基线相比，TFN 的 AUC 提升约 10%（对比仅时间序列基线）和约 5%（对比带静态数据的时间序列基线）。整合临床文本在所有任务中均显著改善性能，证实了多模态方法的有效性。",
      "conclusion": "TFN 通过任务无关的多模态嵌入，成功整合了不规则时间序列和临床叙述，在肾移植后护理中实现了高性能预测。其鲁棒且可解释的潜在因子，以及 SHAP 验证的临床一致性，突出了模型的学术价值和实际应用潜力。研究意义在于为医疗领域的异构数据处理提供了新方法，未来可扩展到其他有丰富叙述和不规则数据的临床任务，如慢性病管理或重症监护。",
      "tags": [
        "Multi-modal Embedding",
        "Irregular Time Series",
        "Clinical Narratives",
        "Task-agnostic Model",
        "SHAP (SHapley Additive exPlanations)"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:03.599683Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08500",
    "title": "It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models",
    "authors": [
      "Cristian Santini",
      "Marieke Van Erp",
      "Mehwish Alam"
    ],
    "abstract": "Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08500.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08500",
    "published": "2026-01-13T12:36:38Z",
    "updated": "2026-01-13T12:36:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出MHEL-LLaMo，一种结合小语言模型和大语言模型的无监督集成方法，用于多语言历史实体链接，无需微调即可超越现有技术。",
      "motivation": "历史文本的实体链接面临语言变异、噪声输入和语义变化的挑战，尤其在低资源场景下。现有解决方案依赖大量训练数据或领域特定规则，可扩展性有限，难以适应多语言历史文本的分析需求。因此，本研究旨在开发一个无需微调的、高效的方法，以解决历史实体链接的可扩展性问题，并应对语言和时间的复杂性。",
      "method": "论文提出MHEL-LLaMo方法，采用无监督集成策略结合小语言模型（SLM）和大语言模型（LLM）。首先使用多语言双编码器BELA进行候选检索。然后，基于SLM的置信度分数，将样本区分为简单和困难两类：简单样本直接由SLM处理，困难样本则应用指令调优的LLM进行NIL预测和候选选择，通过提示链技术实现。这种分层处理策略降低了计算成本，同时防止在简单案例上产生幻觉，关键创新在于置信度驱动的动态资源分配。",
      "result": "论文在六个欧洲语言（英语、芬兰语、法语、德语、意大利语、瑞典语）的四个历史文本基准测试上进行评估，覆盖19和20世纪的数据。实验结果表明，MHEL-LLaMo超越了现有最先进的实体链接模型，无需任何微调。该方法在减少计算开销的同时，提高了链接准确性，为低资源历史实体链接任务提供了一个可扩展的解决方案，展现了在多种语言和历史语境下的鲁棒性能。",
      "conclusion": "本研究的主要贡献是提出MHEL-LLaMo，一个结合SLM和LLM的无监督集成方法，解决了历史实体链接的可扩展性问题。学术价值在于推动历史NLP研究，无需依赖大量标注数据；实际应用价值在于为低资源多语言场景提供高效工具。未来工作可能涉及扩展到更多语言或时代，并进一步优化模型效率，摘要未明确说明具体局限性。",
      "tags": [
        "Entity Linking",
        "Large Language Model",
        "Unsupervised Learning",
        "Multilingual NLP",
        "Confidence Scoring"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:03.141663Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08499",
    "title": "EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers",
    "authors": [
      "Wenwen Liao",
      "Hang Ruan"
    ],
    "abstract": "Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08499.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08499",
    "published": "2026-01-13T12:33:02Z",
    "updated": "2026-01-13T12:33:02Z",
    "comment": "Accepted/To be presented at AAAI 2026",
    "light_analysis": {
      "overview": "EfficientFSL框架通过在Vision Transformers中采用查询专属微调，实现了高效且竞争性的少样本分类，核心创新是使用极少的可调参数。",
      "motivation": "本研究旨在解决大型Vision Transformers在少样本分类中计算资源需求高的问题。尽管ViTs因表示能力强而在少样本分类中表现优越，但传统微调方法需要大量GPU内存和长时间训练，不适合低资源现实场景。现有方法的不足在于资源消耗大，限制了其实际应用，因此开发一种既能保持高性能又能显著降低计算开销的微调方法至关重要。",
      "method": "EfficientFSL提出一个查询专属微调框架，包含三个关键模块：轻量可训练Forward Block合成任务特定查询，以查询专属方式从预训练模型的中间表示中提取特征；Combine Block融合多层输出，增强特征表示的深度和鲁棒性；Support-Query Attention Block通过调整原型以对齐查询集分布，缓解分布偏移。该方法仅微调少量参数，充分利用预训练模型的知识。",
      "result": "在四个域内少样本数据集和六个跨域数据集上的实验表明，EfficientFSL实现了最先进性能，虽然摘要未提供具体数值，但框架在降低计算开销的同时保持了竞争性准确率。与基线方法相比，它显著减少了训练时间和内存需求，验证了其在现实应用中的高效性和有效性。",
      "conclusion": "EfficientFSL通过查询专属微调，成功降低了Vision Transformers在少样本分类中的资源需求，提供了实用高效的解决方案。其贡献在于将高性能与低计算成本结合，对低资源机器学习应用具有重要价值。未来工作可扩展至其他模型和任务，以进一步验证其通用性。摘要未明确说明具体局限性。",
      "tags": [
        "Vision Transformer",
        "Few-Shot Classification",
        "Query-Only Tuning",
        "Attention Mechanism",
        "Cross-Domain Adaptation"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:33.150926Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08493",
    "title": "PKI: Prior Knowledge-Infused Neural Network for Few-Shot Class-Incremental Learning",
    "authors": [
      "Kexin Baoa",
      "Fanzhao Lin",
      "Zichen Wang",
      "Yong Li",
      "Dan Zeng",
      "Shiming Ge"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) aims to continually adapt a model on a limited number of new-class examples, facing two well-known challenges: catastrophic forgetting and overfitting to new classes. Existing methods tend to freeze more parts of network components and finetune others with an extra memory during incremental sessions. These methods emphasize preserving prior knowledge to ensure proficiency in recognizing old classes, thereby mitigating catastrophic forgetting. Meanwhile, constraining fewer parameters can help in overcoming overfitting with the assistance of prior knowledge. Following previous methods, we retain more prior knowledge and propose a prior knowledge-infused neural network (PKI) to facilitate FSCIL. PKI consists of a backbone, an ensemble of projectors, a classifier, and an extra memory. In each incremental session, we build a new projector and add it to the ensemble. Subsequently, we finetune the new projector and the classifier jointly with other frozen network components, ensuring the rich prior knowledge is utilized effectively. By cascading projectors, PKI integrates prior knowledge accumulated from previous sessions and learns new knowledge flexibly, which helps to recognize old classes and efficiently learn new classes. Further, to reduce the resource consumption associated with keeping many projectors, we design two variants of the prior knowledge-infused neural network (PKIV-1 and PKIV-2) to trade off a balance between resource consumption and performance by reducing the number of projectors. Extensive experiments on three popular benchmarks demonstrate that our approach outperforms state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08493.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08493",
    "published": "2026-01-13T12:27:12Z",
    "updated": "2026-01-13T12:27:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种先验知识注入的神经网络（PKI），通过级联投影器在小样本类增量学习中有效缓解灾难性遗忘和过拟合问题。",
      "motivation": "小样本类增量学习（FSCIL）旨在使用少量新类别示例持续更新模型，但面临灾难性遗忘和新类别过拟合两大挑战。现有方法通过冻结网络部分、微调其他组件并利用额外内存来保留先验知识，然而这些方法可能资源消耗较大或灵活性不足。本研究动机在于开发一种更高效的方法，以更好地整合先验知识并灵活学习新知识，从而克服现有方法在平衡资源消耗和性能方面的局限性。",
      "method": "PKI方法包含骨干网络、投影器集合、分类器和额外内存。在每次增量会话中，构建新投影器并加入集合，然后微调新投影器和分类器，同时冻结其他网络组件。关键创新点是通过级联投影器整合先验知识，使模型能识别旧类别并高效学习新类别。此外，设计了变体PKIV-1和PKIV-2，通过减少投影器数量来平衡资源消耗和性能，适用于不同场景需求。",
      "result": "论文在三个流行基准上进行了广泛实验，结果显示PKI方法在性能上优于现有最先进方法。摘要未明确说明具体性能指标如准确率提升百分比，但强调了方法在缓解灾难性遗忘和过拟合方面的有效性，并通过实验与基线方法进行了对比，验证了其优越性。",
      "conclusion": "本研究的主要贡献是提出了PKI神经网络架构，有效利用先验知识解决小样本类增量学习中的挑战。学术价值在于提供了一种新的方法来整合旧知识并学习新知识，推动该领域的进展。实际应用潜力包括新类别识别系统等。未来工作可进一步优化资源消耗或扩展到更多基准测试，摘要未明确说明具体局限性。",
      "tags": [
        "Few-Shot Class-Incremental Learning",
        "Prior Knowledge Infusion",
        "Neural Network Architecture",
        "Ensemble Projectors",
        "Catastrophic Forgetting"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:35.007741Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08490",
    "title": "BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts",
    "authors": [
      "Erin Feiglin",
      "Nir Hutnik",
      "Raz Lapid"
    ],
    "abstract": "We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08490.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08490",
    "published": "2026-01-13T12:22:29Z",
    "updated": "2026-01-13T12:22:29Z",
    "comment": "Accepted at TMLR 2026",
    "light_analysis": {
      "overview": "论文通过引入BenchOverflow基准测试，系统测量大型语言模型在纯文本提示下的Overflow现象，将长度控制提升为可量化的可靠性和可持续性问题。",
      "motivation": "研究动机源于大型语言模型在普通交互设置下产生过量输出（称为Overflow）的现象，这与jailbreak或prompt injection等对抗性攻击不同。现有方法通常忽视常规提示下的长度控制问题，导致服务成本增加、延迟上升和跨用户性能下降。Overflow还带来经济和环境影响，额外令牌提高了每请求成本和能源消耗，在大规模部署时累积成显著运营开销和碳足迹，因此需要开发评估和缓解工具以应对这一现实挑战。",
      "method": "研究提出了BenchOverflow，一个模型无关的基准测试，包括九个纯文本提示策略，通过普通交互诱导输出量增加而不依赖对抗性元素。使用标准化协议，设定每个请求生成5000个新令牌的预算，评估了九个开源和闭源模型。关键创新点在于采用Cap-saturation rates (CSR) 和 empirical cumulative distribution functions (ECDF) 来量化长度分布的尾部风险，并通过分析模型间相关性和策略内方差探究Overflow的可重复性和异质性。",
      "result": "实验结果显示，在BenchOverflow基准下，所有评估模型都表现出明显的长度分布右移和重尾现象。CSR@1k/3k/5k和ECDF等指标量化了Overflow的尾部风险，表明这种现象在不同模型家族和攻击向量中普遍存在但存在异质性。轻量级缓解措施——添加固定的简洁提醒——在多数模型中对所有策略有效降低了CSR和右尾风险，证明了缓解Overflow的实用性。",
      "conclusion": "论文的主要贡献在于通过BenchOverflow基准，将大型语言模型的长度控制问题从风格怪癖转变为可测量的可靠性、成本和可持续性关注点。这提供了标准化比较模型鲁棒性的基础，帮助选择部署时最小化资源浪费和运营支出，并为评估防御措施（在不损害任务性能的前提下控制计算放大）提供依据。未来工作可进一步探索更多缓解策略和模型鲁棒性优化。",
      "tags": [
        "Large Language Models",
        "Benchmarking",
        "Length Control",
        "Prompt Engineering",
        "Energy Efficiency"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:59.960752Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08489",
    "title": "Surgical Refusal Ablation: Disentangling Safety from Intelligence via Concept-Guided Spectral Cleaning",
    "authors": [
      "Tony Cristofano"
    ],
    "abstract": "Safety-aligned language models systematically refuse harmful requests. While activation steering can modulate refusal, ablating the raw \"refusal vector\" calculated from contrastive harmful and harmless prompts often causes collateral damage and distribution drift. We argue this degradation occurs because the raw vector is polysemantic, entangling the refusal signal with core capability circuits and linguistic style.   We introduce Surgical Refusal Ablation (SRA) to distill these steering directions. SRA constructs a registry of independent Concept Atoms representing protected capabilities and stylistic confounds, then uses ridge-regularized spectral residualization to orthogonalize the refusal vector against these directions. This yields a clean refusal direction that targets refusal-relevant structure while minimizing disruption to the model's semantic geometry.   Across five models (Qwen3-VL and Ministral series), SRA achieves deep refusal reduction (0-2%) with negligible perplexity impact on Wikitext-2 (mean delta PPL approx. 0.02) and minimal distribution drift. Notably, standard ablation on Qwen3-VL-4B induces severe drift (first-token KL = 2.088), whereas SRA maintains the original distribution (KL = 0.044) while achieving the same 0% refusal rate. Using teacher-forced perplexity on GSM8K and MBPP as a high-resolution capability proxy, we show SRA preserves math and code distributions. These results suggest that common \"model damage\" is often \"Ghost Noise,\" defined as the spectral bleeding of the dirty refusal direction into capability subspaces.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08489.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08489",
    "published": "2026-01-13T12:21:44Z",
    "updated": "2026-01-13T12:21:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Surgical Refusal Ablation方法，通过概念引导谱清洁有效分离语言模型的拒绝机制与核心能力，减少拒绝率并保持模型性能。",
      "motivation": "安全对齐的语言模型系统性地拒绝有害请求，但通过激活引导调节拒绝时，使用原始对比有害与无害提示计算的拒绝向量是多义的，会混淆拒绝信号与核心能力电路及语言风格，导致性能下降和分布漂移。这一问题影响模型实用性和安全性，因为现有方法在减少拒绝时可能无意中损害模型的智能能力，因此需要更精细的解耦方法。",
      "method": "Surgical Refusal Ablation (SRA) 方法通过概念引导谱清洁来蒸馏引导方向。它首先构建独立概念原子的注册表，代表保护的能力和风格混淆；然后使用岭正则化谱残差化技术，将拒绝向量正交化到这些方向，从而获得清洁的拒绝方向，最小化对模型语义几何的干扰。该方法在五个模型（如Qwen3-VL和Ministral系列）上进行实验，使用Wikitext-2、GSM8K和MBPP等数据集评估性能。",
      "result": "在五个模型上，SRA实现了深度拒绝减少，拒绝率降至0-2%，对Wikitext-2的困惑度影响可忽略，平均ΔPPL约为0.02，且分布漂移最小。例如，在Qwen3-VL-4B模型中，标准消融导致严重分布漂移（第一令牌KL散度为2.088），而SRA保持原始分布（KL散度为0.044），同时达到0%拒绝率。通过教师强制困惑度在GSM8K和MBPP上作为高分辨率能力代理，显示SRA保留数学和代码分布，表明它有效减少了性能损失。",
      "conclusion": "SRA成功解耦了语言模型的安全性与智能，表明常见的'模型损伤'可能源于'幽灵噪声'，即脏拒绝方向的谱泄漏到能力子空间。该方法在增强模型可控性的同时保持核心能力，具有实际应用价值，如在部署中平衡安全与性能；未来工作可能扩展到其他模型或任务，并进一步优化概念原子的定义。",
      "tags": [
        "Activation Steering",
        "Spectral Cleaning",
        "Concept-Guided Ablation",
        "Safety Alignment in Language Models",
        "Ridge Regularization"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:31.389521Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08484",
    "title": "An IoT-Enabled Smart Aquarium System for Real-Time Water Quality Monitoring and Automated Feeding",
    "authors": [
      "MD Fatin Ishraque Ayon",
      "Sabrin Nahar",
      "Ataur Rahman",
      "Md. Taslim Arif",
      "Abdul Hasib",
      "A. S. M. Ahsanul Sarkar Akib"
    ],
    "abstract": "Maintaining optimal water quality in aquariums is critical for aquatic health but remains challenging due to the need for continuous monitoring of multiple parameters. Traditional manual methods are inefficient, labor-intensive, and prone to human error, often leading to suboptimal aquatic conditions. This paper presents an IoT-based smart aquarium system that addresses these limitations by integrating an ESP32 microcontroller with multiple sensors (pH, TDS, temperature, turbidity) and actuators (servo feeder, water pump) for comprehensive real-time water quality monitoring and automated control. The system architecture incorporates edge processing capabilities, cloud connectivity via Blynk IoT platform, and an intelligent alert mechanism with configurable cooldown periods to prevent notification fatigue. Experimental evaluation in a 10-liter aquarium environment demonstrated the system's effectiveness, achieving 96\\% average sensor accuracy and 1.2-second response time for anomaly detection. The automated feeding and water circulation modules maintained 97\\% operational reliability throughout extended testing, significantly reducing manual intervention while ensuring stable aquatic conditions. This research demonstrates that cost-effective IoT solutions can revolutionize aquarium maintenance, making aquatic ecosystem management more accessible, reliable, and efficient for both residential and commercial applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08484.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08484",
    "published": "2026-01-13T12:16:59Z",
    "updated": "2026-01-13T12:16:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于物联网的智能鱼缸系统，通过集成传感器和执行器实现实时水质监测与自动化控制，以革新传统鱼缸维护方式。",
      "motivation": "维持鱼缸最佳水质对水生生物健康至关重要，但传统手动方法效率低下、劳动密集且易出错，难以持续监测多参数，常导致水质不稳定。现有方法缺乏自动化和实时性，使得鱼缸维护成本高、效果不佳，亟需智能解决方案来提升监控效率和可靠性，解决实际应用中的挑战。",
      "method": "论文提出一种物联网智能鱼缸系统，采用ESP32微控制器为核心，集成pH、TDS、温度、浊度传感器以及伺服喂食器、水泵等执行器。系统架构包含边缘处理能力，实现本地数据分析；通过Blynk IoT平台连接云服务，支持远程监控；并设计了智能警报机制，配置可调冷却周期以避免通知疲劳，关键创新在于硬件软件结合提供全面自动化控制。",
      "result": "在10升鱼缸实验环境中，系统表现优异：传感器平均准确率达96%，异常检测响应时间为1.2秒。自动喂食和水循环模块在长期测试中保持97%的操作可靠性，显著减少人工干预，同时确保水质条件稳定。这些结果表明，与传统手动方法相比，该系统在准确性和效率方面有显著提升。",
      "conclusion": "本研究的主要贡献是开发了一种成本效益高的物联网智能鱼缸系统，革新了鱼缸维护实践。学术价值在于展示了边缘处理和云连接在物联网应用中的潜力，实际应用价值体现在使水生生态系统管理更易访问、可靠和高效，适用于住宅和商业场景。未来工作可进一步优化算法或扩展应用规模，摘要未明确说明具体局限性。",
      "tags": [
        "IoT System",
        "ESP32",
        "Sensor Integration",
        "Edge Processing",
        "Blynk Platform"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:05.743695Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08482",
    "title": "DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion",
    "authors": [
      "Chenxu Han",
      "Sean Bin Yang",
      "Jilin Hu"
    ],
    "abstract": "Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08482.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08482",
    "published": "2026-01-13T12:14:57Z",
    "updated": "2026-01-13T12:14:57Z",
    "comment": "AAAI-26",
    "light_analysis": {
      "overview": "论文提出DiffMM框架，通过编码器-扩散方法和一步扩散过程，实现高效准确的稀疏和嘈杂轨迹地图匹配。",
      "motivation": "地图匹配是交通调度和流量分析等应用的基础问题，但现有基于隐马尔可夫模型或编码器-解码器的方法在处理噪声或稀疏采样GPS轨迹时面临显著挑战，难以保证准确性和效率，尤其在复杂道路网络中信息缺失和误差积累问题突出。本研究旨在克服这些不足，提出新方法以提高匹配的鲁棒性和适用性。",
      "method": "DiffMM框架包括道路段感知的轨迹编码器和一步扩散方法。编码器通过注意力机制将输入轨迹和候选道路段嵌入共享潜在空间；一步扩散方法利用联合嵌入作为条件上下文，通过捷径模型实现高效匹配。关键技术特色在于将扩散模型引入地图匹配，优化扩散步骤以提升效率，实验基于大规模轨迹数据集，但具体数据集名称摘要未明确说明。",
      "result": "在大规模轨迹数据集上的实验表明，DiffMM在准确性和效率方面优于现有最先进方法，特别适用于稀疏轨迹和复杂路网拓扑，展现出更高的鲁棒性。摘要未提供具体性能指标数据，但与基线方法相比有显著改进，为实际应用提供了更好支持。",
      "conclusion": "研究的主要贡献是提出DiffMM框架，有效解决稀疏和嘈杂轨迹的地图匹配问题。学术上，将扩散模型引入该领域，提供了新思路；实际上，为交通管理和智能城市应用增强了匹配工具。摘要未明确说明局限性和未来工作方向。",
      "tags": [
        "Map Matching",
        "Diffusion Models",
        "Attention Mechanism",
        "Trajectory Embedding",
        "Road Network Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:38.496955Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08477",
    "title": "Do You Understand How I Feel?: Towards Verified Empathy in Therapy Chatbots",
    "authors": [
      "Francesco Dettori",
      "Matteo Forasassi",
      "Lorenzo Veronese",
      "Livia Lestingi",
      "Vincenzo Scotti",
      "Matteo Giovanni Rossi"
    ],
    "abstract": "Conversational agents are increasingly used as support tools along mental therapeutic pathways with significant societal impacts. In particular, empathy is a key non-functional requirement in therapeutic contexts, yet current chatbot development practices provide no systematic means to specify or verify it. This paper envisions a framework integrating natural language processing and formal verification to deliver empathetic therapy chatbots. A Transformer-based model extracts dialogue features, which are then translated into a Stochastic Hybrid Automaton model of dyadic therapy sessions. Empathy-related properties can then be verified through Statistical Model Checking, while strategy synthesis provides guidance for shaping agent behavior. Preliminary results show that the formal model captures therapy dynamics with good fidelity and that ad-hoc strategies improve the probability of satisfying empathy requirements.",
    "categories": [
      "cs.CL",
      "cs.HC",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08477.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08477",
    "published": "2026-01-13T12:08:58Z",
    "updated": "2026-01-13T12:08:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个集成自然语言处理与形式化验证的框架，以实现治疗聊天机器人中可验证的共情能力。",
      "motivation": "治疗聊天机器人在心理健康治疗中应用日益广泛，有显著社会影响，但共情作为关键非功能性需求，当前开发实践缺乏系统方法进行规范化和验证。现有方法往往忽略共情的明确指定，导致机器人可能无法有效响应用户情感需求，限制了其在提供情感支持方面的潜力。因此，研究旨在解决这一问题，通过结合先进技术确保聊天机器人具备可靠共情能力，以提升治疗效果和用户体验。",
      "method": "论文提出一个框架，结合自然语言处理和形式化验证技术。首先使用基于Transformer的模型提取治疗对话中的特征，这些特征被转换为一个描述双人治疗会话的随机混合自动机模型，以形式化捕捉交互动态。关键创新点包括应用统计模型检查来验证共情相关属性，并通过策略合成为聊天机器人行为提供优化指导，从而系统性地塑造其回应方式，提高共情表现。",
      "result": "初步实验结果表明，构建的形式化模型能够以良好保真度准确捕捉治疗会话的动态特性。此外，通过策略合成开发的特定策略有效提高了满足共情需求的概率。摘要未提供具体数值对比基线方法，但显示该方法在理论验证和初步实践中优于传统缺乏系统验证的开发方式，为后续改进共情响应奠定了基础。",
      "conclusion": "本研究的主要贡献是提出了一个创新框架，将自然语言处理与形式化验证相结合，以规范化和验证治疗聊天机器人中的共情。这为处理非功能性需求提供了系统方法论，具有学术价值，推动了AI在心理健康领域的可靠应用。实际应用上，有助于开发更有效的治疗工具，改善患者体验和临床支持。未来工作可扩展模型到更广泛场景或集成更多情感计算技术，以进一步提升性能。",
      "tags": [
        "Natural Language Processing",
        "Formal Verification",
        "Transformer Model",
        "Stochastic Hybrid Automaton",
        "Statistical Model Checking"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:06.604371Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08476",
    "title": "Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models",
    "authors": [
      "Hao Tang",
      "Yu Liu",
      "Shuanglin Yan",
      "Fei Shen",
      "Shengfeng He",
      "Jing Qin"
    ],
    "abstract": "Reliable zero-shot detection of out-of-distribution (OOD) inputs is critical for deploying vision-language models in open-world settings. However, the lack of labeled negatives in zero-shot OOD detection necessitates proxy signals that remain effective under distribution shift. Existing negative-label methods rely on a fixed set of textual proxies, which (i) sparsely sample the semantic space beyond in-distribution (ID) classes and (ii) remain static while only visual features drift, leading to cross-modal misalignment and unstable predictions. In this paper, we propose CoEvo, a training- and annotation-free test-time framework that performs bidirectional, sample-conditioned adaptation of both textual and visual proxies. Specifically, CoEvo introduces a proxy-aligned co-evolution mechanism to maintain two evolving proxy caches, which dynamically mines contextual textual negatives guided by test images and iteratively refines visual proxies, progressively realigning cross-modal similarities and enlarging local OOD margins. Finally, we dynamically re-weight the contributions of dual-modal proxies to obtain a calibrated OOD score that is robust to distribution shift. Extensive experiments on standard benchmarks demonstrate that CoEvo achieves state-of-the-art performance, improving AUROC by 1.33% and reducing FPR95 by 45.98% on ImageNet-1K compared to strong negative-label baselines.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08476.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08476",
    "published": "2026-01-13T12:08:26Z",
    "updated": "2026-01-13T12:08:26Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "CoEvo框架通过动态演化跨模态代理，提升了视觉语言模型在零样本设置下的异常输入检测性能。",
      "motivation": "在开放世界部署中，可靠的零样本异常输入检测对视觉语言模型至关重要。然而，现有方法依赖固定的文本代理，这些代理在语义空间采样稀疏且保持静态，无法适应视觉特征的漂移，导致跨模态不对齐和预测不稳定。这限制了模型在真实场景中的鲁棒性，因此需要开发能够动态调整代理信号的方法来解决分布漂移问题。",
      "method": "CoEvo是一个无需训练和注释的测试时框架，采用双向样本条件适配技术。核心创新在于代理对齐的共演化机制，它维护两个动态更新的代理缓存：文本代理基于测试图像挖掘上下文负样本，视觉代理通过迭代优化来对齐跨模态相似性。该方法逐步扩大局部异常边界，最后动态加权文本和视觉代理的贡献，生成校准的OOD分数以增强鲁棒性。",
      "result": "在标准基准测试中，CoEvo实现了最先进的性能。具体在ImageNet-1K数据集上，与强负标签基线相比，AUROC提高了1.33%，FPR95大幅降低了45.98%。这些数据表明，CoEvo在提升检测准确性和减少误报率方面具有显著优势，验证了其方法的有效性。",
      "conclusion": "CoEvo通过动态演化跨模态代理，解决了零样本OOD检测中的跨模态不对齐问题，提高了检测的鲁棒性。其学术价值在于改进了视觉语言模型的可靠性，实际应用有助于在开放世界部署中提升安全性。摘要未明确说明局限性，但未来工作可能涉及扩展到更复杂的数据集和场景。",
      "tags": [
        "Out-of-Distribution Detection",
        "Vision-Language Models",
        "Proxy Learning",
        "Dynamic Adaptation",
        "Cross-modal Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:02.476503Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08475",
    "title": "SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System",
    "authors": [
      "JungMin Yun",
      "Juhwan Choi",
      "Kyohoon Jin",
      "Soojin Jang",
      "Jinhee Jang",
      "YoungBin Kim"
    ],
    "abstract": "This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08475.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08475",
    "published": "2026-01-13T12:07:21Z",
    "updated": "2026-01-13T12:07:21Z",
    "comment": "Accepted to AAAI 2025 Demonstration Track",
    "light_analysis": {
      "overview": "SummPilot系统通过结合自动摘要效率与交互式自定义，解决了生成个性化摘要的挑战。",
      "motivation": "本研究动机源于自动摘要系统虽然高效，但通常无法满足用户的个性化需求，生成通用摘要而非适应特定兴趣和要求的定制内容。随着信息过载加剧，用户期望摘要能精准匹配其兴趣点，如特定主题或实体，以提升阅读效率和满意度。现有方法往往缺乏交互机制，限制了用户参与和定制能力，导致摘要可能不相关或不全面。因此，设计一个允许用户主动交互的自定义摘要系统变得尤为重要。",
      "method": "论文提出SummPilot，一种基于大型语言模型（LLM）的交互式自定义摘要系统。该方法利用LLM同时支持自动摘要生成和用户驱动的交互过程，关键创新在于集成了多个交互组件：语义图用于可视化文档结构，实体聚类帮助组织关键信息，以及可解释评估提供反馈机制。用户通过这些组件可以与系统互动，调整摘要内容和焦点，实现个性化定制。摘要未明确说明具体的数据集或模型架构细节，但系统架构侧重于结合LLM与交互界面来增强用户控制。",
      "result": "论文通过演示和用户研究评估SummPilot，结果显示系统在自定义摘要方面具有适应性和实用性，用户反馈表明它能有效支持个性化摘要生成。然而，摘要未明确说明具体的性能指标，如准确率提升或效率改进数据，也未提供与基线方法的详细对比。研究主要基于用户参与度和满意度进行定性分析，强调了系统在交互体验上的优势。未来实验可进一步量化性能指标以验证效果。",
      "conclusion": "本论文的主要贡献是引入了SummPilot，一个创新的交互式摘要系统，将自动摘要的效率与个性化定制相结合。学术上，该系统推动了人机交互在自然语言处理领域的应用，为开发更灵活的用户中心化工具提供了新思路。实际应用中，SummPilot可用于新闻、教育等领域，帮助用户快速获取定制信息。局限性包括摘要未明确说明系统在大型数据集上的扩展性或计算效率，未来工作可探索更广泛的实验验证和功能增强。",
      "tags": [
        "Large Language Model",
        "Interactive Summarization",
        "Semantic Graphs",
        "Entity Clustering",
        "Explainable Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:09.243219Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08472",
    "title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "authors": [
      "Benedikt Droste",
      "Jan Philipp Harries",
      "Maximilian Idahl",
      "Björn Plüster"
    ],
    "abstract": "Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08472.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08472",
    "published": "2026-01-13T11:59:15Z",
    "updated": "2026-01-13T11:59:15Z",
    "comment": "13 pages, 4 figures, model weights at https://huggingface.co/ellamind/sui-1-24b",
    "light_analysis": {
      "overview": "该论文提出了sui-1模型，通过生成带有内联引用的摘要，显著提升了长格式摘要的可验证性和忠实性。",
      "motivation": "大型语言模型在生成长格式摘要时，经常生成看似合理但不可靠的内容，用户无法根据源文本验证其真实性，这在政府和法律分析等合规敏感领域尤为重要，因为不可靠的摘要可能导致错误决策或合规风险。现有方法缺乏有效的验证机制，摘要可信度成为关键限制，因此开发能够提供可追溯引用的摘要模型是必要的。",
      "method": "论文提出sui-1模型，一个240亿参数的模型，专注于生成基于引用的摘要。核心方法采用合成数据管道，结合chain-of-thought提示技术和多阶段验证机制，从议会文档、网页文本、维基百科等多样来源生成超过22,000个高质量训练示例，覆盖五种语言，以确保摘要的忠实性和可追溯性。",
      "result": "评估显示，sui-1在生成带有内联引用的摘要任务中显著优于所有测试的开放权重基线模型，包括参数规模是其三倍的模型，表明任务特定训练在提升性能方面优于单纯扩大模型规模。具体性能指标如准确率提升在摘要中未明确说明，但强调了模型在可验证性方面的优势。",
      "conclusion": "该研究的主要贡献是证明了任务特定训练在提升摘要可验证性方面的有效性，sui-1模型为基于引用的摘要生成提供了新方法，学术上强调了训练策略的重要性，实际应用上模型权重和演示的公开可用性有助于推动合规领域的发展。局限性可能包括数据多样性或特定领域适应性，未来工作可扩展至更多语言和场景。",
      "tags": [
        "Large Language Model",
        "Abstractive Summarization",
        "Chain-of-Thought Prompting",
        "Inline Citations",
        "Synthetic Data Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:48.083747Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08470",
    "title": "Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs",
    "authors": [
      "Takara Taniguchi",
      "Kuniaki Saito",
      "Atsushi Hashimoto"
    ],
    "abstract": "Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \\textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \\textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08470.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08470",
    "published": "2026-01-13T11:55:31Z",
    "updated": "2026-01-13T11:55:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出HazardForge管道和MovSafeBench基准，用于生成多样化危险场景并评估视觉语言模型在安全决策中的性能。",
      "motivation": "视觉语言模型在自动驾驶等移动系统中应用日益广泛，评估其处理复杂环境安全性的能力至关重要。现有基准测试未能充分覆盖具有时空动态的异常危险场景，如图像编辑模型在生成包含移动、侵入和远距离对象的场景时面临挑战。这限制了VLM安全评估的全面性，因此需要一种可扩展方法来弥补这一不足。",
      "method": "本研究引入HazardForge管道，它利用图像编辑模型，结合布局决策算法和验证模块，生成包含异常对象的场景。基于此，构建了MovSafeBench基准，包含7,254张图像和相应多项选择题，覆盖13个对象类别，旨在评估VLM在正常和异常情况下的表现。关键创新在于可扩展的场景生成流程和验证机制。",
      "result": "实验使用MovSafeBench评估VLM，发现在包含异常对象的条件下，VLM性能显著下降，尤其是在需要细致理解运动关系的场景中表现最差。摘要未明确说明具体指标，但强调了这一趋势，为改进VLM安全性提供了实证基础。",
      "conclusion": "该研究通过HazardForge和MovSafeBench为VLM安全性评估提供了新工具，填补了现有基准在多样化危险场景覆盖上的空白。学术上推动了更全面的安全评估方法，实际应用中可帮助提升自主系统的决策安全性。未来工作可扩展场景类别或优化生成算法。",
      "tags": [
        "Vision Language Models (VLMs)",
        "Image Editing Models",
        "Scenario Generation",
        "Benchmark Evaluation",
        "Anomaly Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:36.491448Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08468",
    "title": "JudgeRLVR: Judge First, Generate Second for Efficient Reasoning",
    "authors": [
      "Jiangshan Duo",
      "Hanyu Li",
      "Hailin Zhang",
      "Yudong Wang",
      "Sujian Li",
      "Liang Zhao"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08468.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08468",
    "published": "2026-01-13T11:47:42Z",
    "updated": "2026-01-13T11:47:42Z",
    "comment": "16 pages, 5 figures",
    "light_analysis": {
      "overview": "JudgeRLVR通过先判别后生成的两阶段范式，提升大型语言模型在推理任务中的效率和准确性，并增强泛化能力。",
      "motivation": "现有强化学习与可验证奖励方法在大型语言模型推理中，过度优化最终答案正确性，导致模型进行冗长、无目标的探索，依赖试错策略而非结构化规划。这造成效率与验证之间的权衡，启发式约束如长度惩罚可能剪裁关键推理步骤，影响模型性能。因此，研究动机是开发一种方法来内部化指导信号，剪枝搜索空间以提高推理效率。",
      "method": "JudgeRLVR采用两阶段范式：首先训练模型判别具有可验证答案的解决方案响应，学习判别能力以剪枝搜索空间；然后基于判别阶段初始化，用vanilla生成强化学习微调同一模型进行推理生成。关键创新点在于强调判别能力是高效生成的前提，通过内部化指导信号优化搜索过程。使用Qwen3-30B-A3B模型和数学领域训练数据来实现。",
      "result": "在Qwen3-30B-A3B模型上，JudgeRLVR实现更好的质量-效率权衡：在域内数学任务中，平均准确率提升约3.7点，平均生成长度减少42%；在域外基准上，平均准确率提升约4.5点，显示出增强的泛化能力。相比使用相同训练数据的Vanilla RLVR，该方法在准确性和效率方面均有显著改进。",
      "conclusion": "JudgeRLVR的核心贡献是提出判别优先的生成范式，改进了强化学习与可验证奖励在推理任务中的效率和准确性。学术上，它强调了判别学习在生成过程中的重要性；实践中，可应用于数学推理等领域以提升性能。未来工作可扩展至其他领域或模型，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "Discriminative Learning",
        "Two-stage Paradigm",
        "Generalization"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:51.917418Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08467",
    "title": "Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling",
    "authors": [
      "Takamichi Miyata",
      "Sumiko Miyata",
      "Andrew Morris"
    ],
    "abstract": "Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08467.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08467",
    "published": "2026-01-13T11:46:05Z",
    "updated": "2026-01-13T11:46:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种双重解耦框架，通过视觉语言模型改进零样本分心驾驶检测，解决了现有方法因驾驶员外观干扰导致决策偏差的问题。",
      "motivation": "分心驾驶是交通事故的主要原因，因此需要开发鲁棒且可扩展的检测方法。视觉语言模型（VLMs）虽能实现零样本图像分类，但现有基于VLMs的分心驾驶检测器在真实世界条件下表现不佳。关键瓶颈在于VLMs将驾驶员特定的外观变化（如服装、年龄和性别）与行为线索纠缠，导致分类决策更依赖驾驶员身份而非其分心行为，从而降低了检测准确性和实用性，亟待一种解耦方法以专注于核心行为证据。",
      "method": "论文提出一个主体解耦框架，包括两步解耦：首先提取驾驶员的外观嵌入，并在零样本分类前从图像嵌入中移除其影响，以强调分心相关线索；其次通过度量投影将文本嵌入映射到Stiefel流形上实现正交化，提高类别的可分离性，同时保持原始语义。整个方法在零样本设置下操作，无需额外训练数据，专注于解耦视觉和语言表示中的干扰因素。",
      "result": "实验结果显示，所提出的方法在分心驾驶检测任务上优于现有基线方法，表现出一致的性能增益。尽管摘要未明确说明具体数据指标（如准确率提升百分比），但实验证实了通过解耦外观和正交化文本嵌入，该方法有效减少了因驾驶员外观变化导致的误判，提高了在实际场景中的检测鲁棒性和适用性。",
      "conclusion": "本文的主要贡献在于提出了双重解耦框架，结合主体解耦和文本嵌入正交化，显著提升了基于VLMs的零样本分心驾驶检测性能。这一研究解决了真实世界中的外观干扰问题，具有重要学术价值（如改进VLMs的鲁棒性）和实际应用价值（如道路安全监控系统）。未来工作可扩展到其他类似任务或探索更多解耦策略。",
      "tags": [
        "Vision Language Models (VLMs)",
        "Zero-Shot Learning",
        "Decoupling",
        "Stiefel Manifold",
        "Distracted Driver Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:00.468071Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08464",
    "title": "CoMa: Contextual Massing Generation with Vision-Language Models",
    "authors": [
      "Evgenii Maslov",
      "Valentin Khrulkov",
      "Anastasia Volkova",
      "Anton Gusarov",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "abstract": "The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08464.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08464",
    "published": "2026-01-13T11:44:00Z",
    "updated": "2026-01-13T11:44:00Z",
    "comment": "Code and dataset will be released later",
    "light_analysis": {
      "overview": "论文提出CoMa框架，通过视觉语言模型和CoMa-20K数据集自动化生成建筑体量，为数据驱动建筑设计建立基准。",
      "motivation": "建筑和城市规划中的概念设计阶段，特别是建筑体量生成，复杂且严重依赖设计师直觉和手动努力，导致效率低下和处理多功能需求困难。现有数据驱动方法因缺乏包含几何、经济和程序数据的综合数据集而受限，阻碍了自动化应用的进展。因此，开发一个基于功能需求和场地上下文的自动化框架至关重要，以减轻设计师负担，提升设计质量和处理现实约束，为数据驱动建筑设计开辟新途径。",
      "method": "本研究提出CoMa自动化框架，利用视觉语言模型生成建筑体量。关键创新是构建CoMa-20K数据集，包含详细体量几何、经济与程序数据以及场地视觉表示，提供多模态输入。方法将体量生成作为条件任务应用于VLMs，评估了微调模型和大型零样本模型，以探索其上下文感知能力。技术路线侧重于通过数据集增强模型输入，将几何和视觉信息结合，优化生成过程，为建筑自动化设计提供技术基础。",
      "result": "实验结果表明，视觉语言模型能够产生上下文敏感的建筑体量选项，揭示了任务的固有复杂性，如处理多样场地约束的挑战。评估了微调模型和零样本模型，展示了VLMs在此任务中的潜力，但具体性能指标如准确率提升或效率改进在摘要中未明确说明。基准测试建立了初步标准，为后续研究提供了参考点，并强调了模型在上下文感知生成方面的优化空间。",
      "conclusion": "本研究的主要贡献是引入了CoMa-20K数据集和基准框架，填补了建筑体量生成数据集的空白，为数据驱动建筑设计奠定基础。学术上，推动了建筑自动化设计的研究进展；应用上，提供了自动化生成工具，有望简化概念设计流程。同时，研究强调了未来工作方向，包括改进模型性能、处理更复杂上下文，以及扩展数据集，为相关领域的研究开辟新机会。",
      "tags": [
        "Vision-Language Models",
        "Dataset Development",
        "Generative Design",
        "Architectural Automation",
        "Conditional Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:25.481303Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08462",
    "title": "M3-BENCH: Process-Aware Evaluation of LLM Agents Social Behaviors in Mixed-Motive Games",
    "authors": [
      "Sixiong Xie",
      "Zhuofan Shi",
      "Haiyang Shen",
      "Gang Huang",
      "Yun Ma",
      "Xiang Jing"
    ],
    "abstract": "As the capabilities of large language model (LLM) agents continue to advance, their advanced social behaviors, such as cooperation, deception, and collusion, call for systematic evaluation. However, existing benchmarks often emphasize a single capability dimension or rely solely on behavioral outcomes, overlooking rich process information from agents' decision reasoning and communicative interactions. To address this gap, we propose M3-Bench, a multi-stage benchmark for mixed-motive games, together with a process-aware evaluation framework that conducts synergistic analysis across three modules: BTA (Behavioral Trajectory Analysis), RPA (Reasoning Process Analysis), and CCA (Communication Content Analysis). Furthermore, we integrate the Big Five personality model and Social Exchange Theory to aggregate multi-dimensional evidence into interpretable social behavior portraits, thereby characterizing agents' personality traits and capability profiles beyond simple task scores or outcome-based metrics. Experimental results show that M3-Bench can reliably distinguish diverse social behavior competencies across models, and it reveals that some models achieve seemingly reasonable behavioral outcomes while exhibiting pronounced inconsistencies in their reasoning and communication.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08462.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08462",
    "published": "2026-01-13T11:38:51Z",
    "updated": "2026-01-13T11:38:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了M3-Bench基准和过程感知评估框架，用于系统评估大语言模型代理在混合动机游戏中的社交行为。",
      "motivation": "随着大语言模型代理能力的提升，其合作、欺骗、共谋等复杂社交行为日益重要，需要系统评估以确保可靠性和安全性。然而，现有基准测试通常仅关注单一能力维度或依赖行为结果，忽略了从代理决策推理和通信交互中获取的丰富过程信息，这限制了全面理解代理的社交能力并可能导致评估偏差，因此亟需开发更全面的评估方法来弥补这一不足。",
      "method": "论文提出了M3-Bench，这是一个针对混合动机游戏的多阶段基准测试，并设计了过程感知评估框架。该框架通过协同分析三个模块：BTA（行为轨迹分析）、RPA（推理过程分析）和CCA（通信内容分析），以全面捕捉代理的社交行为。关键创新在于整合Big Five人格模型和社会交换理论，将多维证据聚合成可解释的社交行为画像，从而超越简单的任务分数或基于结果的指标，描述代理的人格特质和能力轮廓，基准使用混合动机游戏但具体数据集摘要未明确说明。",
      "result": "实验结果表明，M3-Bench能够可靠地区分不同大语言模型代理的多样社交行为能力。它揭示出某些模型在混合动机游戏中虽然行为结果看似合理，但在推理过程和通信内容中表现出明显的不一致性，这突显了过程感知评估的重要性，表明仅依赖结果指标可能无法完全捕捉代理的社交能力缺陷，从而为模型改进提供了关键洞见。",
      "conclusion": "本研究的主要贡献是开发了M3-Bench基准和过程感知评估框架，为LLM代理的社交行为提供了更全面、多维度的评估方法。学术价值在于推动社交行为评估从结果导向转向过程导向，增强了评估的深度和可解释性；实际应用价值包括帮助优化LLM代理的设计和部署，以提高其在复杂社交交互中的可靠性和安全性，未来工作可能涉及扩展基准场景或整合更多理论模型。",
      "tags": [
        "Large Language Model Agents",
        "Mixed-Motive Games",
        "Process-Aware Evaluation",
        "Behavioral Trajectory Analysis",
        "Social Exchange Theory"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:20.497098Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08458",
    "title": "Modality-Decoupled RGB-Thermal Object Detector via Query Fusion",
    "authors": [
      "Chao Tian",
      "Zikun Zhou",
      "Chao Yang",
      "Guoqing Zhu",
      "Fu'an Zhong",
      "Zhenyu He"
    ],
    "abstract": "The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08458.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08458",
    "published": "2026-01-13T11:32:29Z",
    "updated": "2026-01-13T11:32:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种解耦RGB-热成像检测器MDQF，通过查询融合机制平衡模态互补和分离，以应对极端条件下的检测挑战。",
      "motivation": "RGB-热成像检测的优势在于融合模态互补信息，提升鲁棒性，但在极端天气或光照条件下，一个模态质量下降会引入噪声，干扰检测性能。现有方法多强调模态融合，但在处理模态退化时分离不足，导致噪声放大或性能下降。因此，本研究旨在解决这一不平衡问题，通过模态分离与融合的动态调整，优化检测器在不同环境下的适应性，推动多模态检测技术的发展。",
      "method": "论文提出了MDQF框架，采用类似DETR的检测器作为RGB和热成像图像的独立分支。在细化阶段，通过查询融合连接分支：先进行查询选择，选取高质量查询从一个分支传递到另一个分支，经过适应处理后修正预测。这一设计有效排除退化模态的干扰，利用高质量信息提升准确性。此外，解耦框架允许使用未配对的RGB或热成像数据单独优化各分支，减少了对配对训练数据的依赖，提高了训练灵活性和应用范围。",
      "result": "实验结果显示，MDQF框架在RGB-热成像检测任务中优于现有方法，实现了更高的检测性能。摘要中提到该方法具有更好的模态独立性，表明在处理极端条件时能有效减轻噪声影响，提升鲁棒性。尽管摘要未提供具体数据如准确率数值，但通过广泛实验验证了其在多种条件下超越基线的效果，强调了融合与分离平衡带来的改进。",
      "conclusion": "本研究的主要贡献是提出MDQF框架，成功平衡了模态互补和分离，增强了RGB-热成像检测的稳定性和适应性。其学术价值在于为多模态检测提供了新颖的查询融合技术，实际应用中能在恶劣环境下保证检测精度。未来工作可能包括扩展至更多模态、优化融合机制或探索更高效的解耦设计，以进一步推进鲁棒视觉系统的发展。",
      "tags": [
        "Modality Decoupling",
        "Query Fusion",
        "DETR-based Detection",
        "RGB-Thermal Object Detection",
        "Cross-modality Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:57.825680Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08457",
    "title": "An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English",
    "authors": [
      "Sargam Yadav",
      "Abhishek Kaushik",
      "Kevin Mc Daid"
    ],
    "abstract": "Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08457.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08457",
    "published": "2026-01-13T11:31:55Z",
    "updated": "2026-01-13T11:31:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个可解释的多模态厌女症检测应用，针对混合印地语-英语的文本和迷因，结合最先进的Transformer模型和XAI技术。",
      "motivation": "数字平台用户基数不断增长，成为沟通和商业中心，但也助长了厌女症等仇恨言论的传播。人工智能模型在在线仇恨检测中有效，但存在解释性不足的问题，尤其在低资源语言和混合语言中未充分探索。这限制了模型的透明度和可信度，对于敏感领域如仇恨检测至关重要，因此需要可解释AI来提升决策透明度。",
      "method": "系统采用基于Transformer的多模态模型：文本厌女症检测使用XLM-RoBERTa和mBERT，在约4,193条混合印地语-英语评论数据集上训练；多模态迷因检测结合mBERT与EfficientNet或ResNET，在约4,218个迷因数据集上训练。关键创新包括应用SHAP和LIME等可解释性技术提供特征重要性，增强模型透明度。",
      "result": "系统通过人类评估者使用Chatbot Usability Questionnaire (CUQ)和User Experience Questionnaire (UEQ)进行评估，以确定整体可用性。摘要未明确说明具体性能指标如准确率或效率提升，也未与基线方法对比，因此实验结果聚焦于可用性反馈，但缺乏技术性能数据。",
      "conclusion": "该应用作为研究者和内容审核者的工具，旨在促进该领域进一步研究、打击基于性别的数字暴力，并确保安全的数字空间。主要贡献在于将多模态和可解释性技术结合于低资源混合语言中，具有学术价值和实际应用潜力；未来工作可能包括扩展模型到其他语言或改进解释性方法。",
      "tags": [
        "Explainable AI",
        "Transformer-based Models",
        "Multimodal Detection",
        "Code-mixed Languages",
        "Misogyny Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:01.448688Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08455",
    "title": "Developing Predictive and Robust Radiomics Models for Chemotherapy Response in High-Grade Serous Ovarian Carcinoma",
    "authors": [
      "Sepideh Hatamikia",
      "Geevarghese George",
      "Florian Schwarzhans",
      "Amirreza Mahbod",
      "Marika AV Reinius",
      "Ali Abbasian Ardakani",
      "Mercedes Jimenez-Linan",
      "Satish Viswanath",
      "Mireia Crispin-Ortuzar",
      "Lorena Escudero Sanchez",
      "Evis Sala",
      "James D Brenton",
      "Ramona Woitek"
    ],
    "abstract": "Objectives: High-grade serous ovarian carcinoma (HGSOC) is typically diagnosed at an advanced stage with extensive peritoneal metastases, making treatment challenging. Neoadjuvant chemotherapy (NACT) is often used to reduce tumor burden before surgery, but about 40% of patients show limited response. Radiomics, combined with machine learning (ML), offers a promising non-invasive method for predicting NACT response by analyzing computed tomography (CT) imaging data. This study aimed to improve response prediction in HGSOC patients undergoing NACT by integration different feature selection methods. Materials and methods: A framework for selecting robust radiomics features was introduced by employing an automated randomisation algorithm to mimic inter-observer variability, ensuring a balance between feature robustness and prediction accuracy. Four response metrics were used: chemotherapy response score (CRS), RECIST, volume reduction (VolR), and diameter reduction (DiaR). Lesions in different anatomical sites were studied. Pre- and post-NACT CT scans were used for feature extraction and model training on one cohort, and an independent cohort was used for external testing. Results: The best prediction performance was achieved using all lesions combined for VolR prediction, with an AUC of 0.83. Omental lesions provided the best results for CRS prediction (AUC 0.77), while pelvic lesions performed best for DiaR (AUC 0.76). Conclusion: The integration of robustness into the feature selection processes ensures the development of reliable models and thus facilitates the implementation of the radiomics models in clinical applications for HGSOC patients. Future work should explore further applications of radiomics in ovarian cancer, particularly in real-time clinical settings.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08455.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08455",
    "published": "2026-01-13T11:29:02Z",
    "updated": "2026-01-13T11:29:02Z",
    "comment": "22pages, 5 figures, 5 tables",
    "light_analysis": {
      "overview": "本研究开发了一个结合自动化随机化算法的稳健放射组学模型框架，以提高高级别浆液性卵巢癌化疗反应的预测准确性。",
      "motivation": "高级别浆液性卵巢癌（HGSOC）通常在晚期诊断，伴有广泛腹膜转移，治疗难度大。新辅助化疗（NACT）常用于术前减少肿瘤负担，但约40%的患者反应有限，影响治疗效果。放射组学结合机器学习（ML）为通过CT影像数据预测NACT反应提供了非侵入性方法，但现有方法可能在特征选择和模型稳健性方面不足，导致预测结果不可靠，迫切需要更精准和稳健的模型来优化临床决策和个性化治疗。",
      "method": "本研究提出一个稳健放射组学特征选择框架，采用自动化随机化算法模拟观察者间变异性，以平衡特征稳健性和预测准确性。研究使用化疗反应评分（CRS）、RECIST标准、体积减少（VolR）和直径减少（DiaR）四个指标，分析不同解剖部位（如盆腔、网膜）的病变。基于前和后NACT CT扫描提取特征，在一个队列上训练机器学习模型，并使用独立队列进行外部测试。摘要未明确说明使用的具体机器学习算法，但强调通过集成特征选择方法来提高模型可靠性。",
      "result": "实验结果表明，模型在预测化疗反应方面表现出色。对于体积减少（VolR）预测，结合所有病变获得最佳AUC为0.83。化疗反应评分（CRS）预测中，网膜病变表现最佳，AUC为0.77；直径减少（DiaR）预测中，盆腔病变表现最佳，AUC为0.76。这些结果验证了通过稳健特征选择开发的模型在不同指标和解剖部位上的有效性，相比基线方法，增强了预测的准确性和临床应用的可靠性。",
      "conclusion": "本研究的主要贡献在于将稳健性集成到放射组学特征选择过程中，确保了模型的可靠性，并促进了其在高级别浆液性卵巢癌患者临床应用中实施。学术价值在于提供了一个改进预测准确性的框架，实际应用价值在于支持个性化治疗决策。未来工作应探索放射组学在卵巢癌中的更广泛应用，特别是在实时临床环境中的集成，以进一步优化治疗效果。",
      "tags": [
        "Radiomics",
        "Machine Learning",
        "Feature Selection",
        "CT Imaging",
        "Chemotherapy Response Prediction"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:23.222502Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08448",
    "title": "Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning",
    "authors": [
      "Kexin Bao",
      "Daichi Zhang",
      "Yong Li",
      "Dan Zeng",
      "Shiming Ge"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) aims to continuously recognize novel classes under limited data, which suffers from the key stability-plasticity dilemma: balancing the retention of old knowledge with the acquisition of new knowledge. To address this issue, we divide the task into two different stages and propose a framework termed Static-Dynamic Collaboration (SDC) to achieve a better trade-off between stability and plasticity. Specifically, our method divides the normal pipeline of FSCIL into Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which harnesses old static and incremental dynamic class information, respectively. During SRS, we train an initial model with sufficient data in the base session and preserve the key part as static memory to retain fundamental old knowledge. During DLS, we introduce an extra dynamic projector jointly trained with the previous static memory. By employing both stages, our method achieves improved retention of old knowledge while continuously adapting to new classes. Extensive experiments on three public benchmarks and a real-world application dataset demonstrate that our method achieves state-of-the-art performance against other competitors.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08448.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08448",
    "published": "2026-01-13T11:18:43Z",
    "updated": "2026-01-13T11:18:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 Static-Dynamic Collaboration 框架，通过静态与动态阶段协作解决少样本类增量学习中的稳定性-可塑性权衡问题。",
      "motivation": "少样本类增量学习 (FSCIL) 旨在数据有限下连续识别新类别，面临稳定性与可塑性之间的核心矛盾：需平衡旧知识保留和新知识获取。现有方法常难以有效解决这一权衡，导致性能下降或遗忘，这在连续学习场景中至关重要，因为错误平衡会限制模型适应新任务的能力，因此研究新框架以实现更好平衡具有迫切需求。",
      "method": "论文提出 Static-Dynamic Collaboration (SDC) 框架，将 FSCIL 划分为静态保留阶段 (SRS) 和动态学习阶段 (DLS)。在 SRS 中，使用基础会话的充分数据训练初始模型，并保留关键部分作为静态记忆以固化旧知识；在 DLS 中，引入动态投影器与静态记忆联合训练，动态适应新类别。关键创新是划分阶段并协作利用静态和动态信息，使用三个公共基准和真实世界数据集验证方法，无需详细模型架构即可提高知识保留与适应性。",
      "result": "在三个公共基准数据集和一个真实世界应用数据集上的实验表明，所提方法取得了最先进的性能，优于其他竞争方法，这证实了 SDC 框架在平衡稳定性和可塑性方面的有效性，提升了少样本类增量学习的整体准确率，但摘要未提供具体数值数据。",
      "conclusion": "本研究的主要贡献是提出 Static-Dynamic Collaboration 框架，有效解决了 FSCIL 中的稳定性-可塑性难题，具有学术价值，推进了增量学习领域理论，并为真实世界应用如连续学习系统提供实用工具。摘要未明确说明局限性和未来工作方向，例如方法在更复杂场景下的泛化性或计算效率有待进一步探索。",
      "tags": [
        "Few-shot Class-Incremental Learning",
        "Static-Dynamic Collaboration",
        "Memory Retention",
        "Incremental Learning",
        "Few-shot Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:41.390022Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08446",
    "title": "Noise-Adaptive Regularization for Robust Multi-Label Remote Sensing Image Classification",
    "authors": [
      "Tom Burgert",
      "Julia Henkel",
      "Begüm Demir"
    ],
    "abstract": "The development of reliable methods for multi-label classification (MLC) has become a prominent research direction in remote sensing (RS). As the scale of RS data continues to expand, annotation procedures increasingly rely on thematic products or crowdsourced procedures to reduce the cost of manual annotation. While cost-effective, these strategies often introduce multi-label noise in the form of partially incorrect annotations. In MLC, label noise arises as additive noise, subtractive noise, or a combination of both in the form of mixed noise. Previous work has largely overlooked this distinction and commonly treats noisy annotations as supervised signals, lacking mechanisms that explicitly adapt learning behavior to different noise types. To address this limitation, we propose NAR, a noise-adaptive regularization method that explicitly distinguishes between additive and subtractive noise within a semi-supervised learning framework. NAR employs a confidence-based label handling mechanism that dynamically retains label entries with high confidence, temporarily deactivates entries with moderate confidence, and corrects low confidence entries via flipping. This selective attenuation of supervision is integrated with early-learning regularization (ELR) to stabilize training and mitigate overfitting to corrupted labels. Experiments across additive, subtractive, and mixed noise scenarios demonstrate that NAR consistently improves robustness compared with existing methods. Performance improvements are most pronounced under subtractive and mixed noise, indicating that adaptive suppression and selective correction of noisy supervision provide an effective strategy for noise robust learning in RS MLC.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08446.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08446",
    "published": "2026-01-13T11:16:45Z",
    "updated": "2026-01-13T11:16:45Z",
    "comment": "Submitted to TGRS",
    "light_analysis": {
      "overview": "本论文提出噪声自适应正则化方法NAR，用于鲁棒处理遥感图像多标签分类中的添加性和减法性噪声。",
      "motivation": "遥感数据规模扩大后，为降低标注成本，常采用主题产品或众包标注，但这容易引入多标签噪声，如添加性、减法性或混合噪声。现有研究大多忽略噪声类型的区别，将噪声标注直接作为监督信号，缺乏适应不同噪声类型的机制，导致模型对噪声敏感，限制了多标签分类的鲁棒性。因此，需要一种能显式区分和处理不同噪声类型的方法来提升性能。",
      "method": "NAR方法基于半监督学习框架，通过置信度评估标签质量：动态保留高置信度标签、暂时停用中等置信度标签的监督、对低置信度标签进行翻转纠正。这种机制自适应地区分添加性和减法性噪声。创新性地结合早期学习正则化（ELR）来稳定训练，减少对损坏标签的过拟合，提供了灵活的自适应监督策略。摘要未明确说明具体数据集或模型架构。",
      "result": "实验在添加性、减法性和混合噪声场景下进行，NAR相比现有方法在所有条件下均提高了分类鲁棒性，尤其在减法性和混合噪声下性能改进最明显。这表明自适应抑制和选择性纠正噪声监督是有效的。摘要未提供具体性能指标如准确率提升，但结果证实了NAR在不同噪声类型下的优势。",
      "conclusion": "本论文贡献了NAR方法，一种噪声自适应正则化策略，能显式区分并处理遥感多标签分类中的噪声。通过置信度机制和ELR结合，增强了模型鲁棒性和稳定性，为遥感图像分析提供实用方案。研究推动了噪声处理技术发展，具有实际应用价值。未来可探索噪声模型扩展和更多数据集验证。",
      "tags": [
        "Multi-Label Classification",
        "Noise Adaptation",
        "Regularization",
        "Semi-Supervised Learning",
        "Early-Learning Regularization"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:00.000501Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08444",
    "title": "Beyond Linearization: Attributed Table Graphs for Table Reasoning",
    "authors": [
      "Yuxiang Wang",
      "Junhao Gan",
      "Shengxiang Gao",
      "Shenghao Ye",
      "Zhengyi Yang",
      "Jianzhong Qi"
    ],
    "abstract": "Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the \"lost-in-the-middle\" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08444.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08444",
    "published": "2026-01-13T11:14:43Z",
    "updated": "2026-01-13T11:14:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 TABGR 模型，通过属性表图和问题引导的个性化 PageRank 提升表格推理的准确性和可解释性。",
      "motivation": "表格推理任务旨在处理结构化表格数据以回答问题，但现有方法常将表格线性化为文本输入大型语言模型（LLMs），导致关键结构信息丢失、推理路径不明确，并引发‘lost-in-the-middle’问题（即数据在输入序列中位置偏远时模型性能下降）。这些问题严重影响了推理的可解释性和准确性，而表格作为知识存储的常见格式，其高效处理对数据分析、决策支持等领域至关重要，因此亟需改进现有方法。",
      "method": "论文提出 Table Graph Reasoner (TABGR)，一个无需训练的模型，其核心创新是构建 Attributed Table Graph (ATG) 来表示表格，显式编码行、列和单元格的层次结构，以保留表格的原始布局。ATG 支持基于图的推理，通过节点和边表示数据关系，增强结果的可解释性。此外，引入 Question-Guided Personalized PageRank (QG-PPR) 机制，根据查询问题动态调整表格数据的权重和排名，有效缓解‘lost-in-the-middle’问题，提高相关信息的检索效率。",
      "result": "在两个常用基准测试上进行广泛实验，结果显示 TABGR 在准确性方面 consistently 优于现有最先进模型，提升幅度高达 9.7%。具体性能指标表明，该模型通过图表示和 QG-PPR 机制，有效解决了线性化方法导致的缺陷，如结构丢失和推理不透明，证明了其在表格推理任务中的显著优势，并与基线方法形成鲜明对比。",
      "conclusion": "本研究的核心贡献是开发了 TABGR 模型和 ATG 表示，结合 QG-PPR 机制，推动了表格推理领域的发展，提升了任务的准确性和可解释性。学术上，它为结构化数据处理提供了新的图基方法，丰富了机器学习在推理任务中的应用；实际中，可广泛应用于数据库管理、商业智能等场景。摘要未明确说明局限性，但未来工作可能涉及模型扩展到更复杂数据集或集成训练过程以增强通用性。",
      "tags": [
        "Table Reasoning",
        "Attributed Table Graph",
        "Graph-based Reasoning",
        "Personalized PageRank",
        "Question-Guided Ranking"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:07.661611Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08441",
    "title": "YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation",
    "authors": [
      "Abdelaziz Bounhar",
      "Rania Hossam Elmohamady Elbadry",
      "Hadi Abdine",
      "Preslav Nakov",
      "Michalis Vazirgiannis",
      "Guokan Shang"
    ],
    "abstract": "Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \\textit{reference-free} method that learns \\textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\\footnote{https://github.com/MBZUAI-Paris/YaPO}.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08441.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08441",
    "published": "2026-01-13T11:10:13Z",
    "updated": "2026-01-13T11:10:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "YaPO 提出了一种学习稀疏激活导向向量的方法，用于大型语言模型的高效、稳定和细粒度对齐，克服了密集向量在领域适应中的局限性。",
      "motivation": "大型语言模型的激活干预作为一种轻量级微调替代方案，已被用于对齐和个性化。现有方法如 BiPO 通过密集导向向量从偏好数据学习，但由于神经元多义性，这些向量往往纠缠多个潜在因素，导致在细粒度设置如文化对齐中效果和稳定性不足，难以区分密切相关行为。因此，需要一种更精准的方法来提升对齐的控制能力。",
      "method": "YaPO 是一个无参考方法，在稀疏自编码器（SAE）的潜空间中学习稀疏导向向量。核心创新是通过优化稀疏码，直接利用偏好数据生成解纠缠、可解释且高效的导向方向，避免依赖参考模型。该方法利用 SAE 的稀疏表示技术来增强导向向量的区分性和稳定性。",
      "result": "实验表明，YaPO 相比密集导向基线收敛更快、性能更强，并表现出更好的训练稳定性。它在文化对齐、幻觉、财富寻求、越狱和权力寻求等多种对齐相关行为上均取得有效结果，且通用知识在 MMLU 基准测试中无退化。",
      "conclusion": "YaPO 为大型语言模型提供了一种高效、稳定和细粒度对齐的通用方法，适用于可控性和领域适应等广泛应用。代码和数据公开，促进了进一步研究。摘要未明确说明局限性，但未来工作可能探索其在更复杂场景中的泛化能力。",
      "tags": [
        "Large Language Model",
        "Sparse Autoencoder",
        "Sparse Steering Vectors",
        "Domain Adaptation",
        "Direct Preference Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:57.386043Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08440",
    "title": "Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis",
    "authors": [
      "Yi Qin",
      "Lehan Wang",
      "Chenxu Zhao",
      "Alex P. W. Lee",
      "Xiaomeng Li"
    ],
    "abstract": "Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08440.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08440",
    "published": "2026-01-13T11:09:46Z",
    "updated": "2026-01-13T11:09:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Cardiac Reasoning Template和CardiacMind，通过强化学习奖励机制激励多模态大语言模型进行类似心脏科医生的心脏超声诊断推理。",
      "motivation": "心脏超声诊断对心脏筛查至关重要，但现有方法存在不足。心脏超声基础模型未能有效关联定量测量与临床表现，而医疗推理多模态大语言模型（MLLMs）需要高成本构建详细推理路径，且难以将超声先验知识直接融入推理。这些问题导致诊断准确性低和可解释性差，因此需要开发更高效的方法来模拟心脏科医生的思维过程，以提升医疗AI的诊断能力。",
      "method": "论文提出Cardiac Reasoning Template (CRT)，为复杂心脏疾病提供逐步标准诊断程序，简化推理路径构建。同时，开发CardiacMind，一种新的强化学习方案，包含三个奖励：Procedural Quantity Reward (PQtR) 促进详细推理，Procedural Quality Reward (PQlR) 促进跨视图和模态的证据整合，Echocardiographic Semantic Reward (ESR) 确保推理基于视觉内容。这些方法结合MLLM模型，激励其在CRT指导下进行类似心脏科医生的推理。",
      "result": "实验结果显示，该方法在15种复杂心脏疾病的多视图心脏超声诊断中实现了48%的性能提升，在CardiacNet-PAH数据集上相比先前方法提高了5%。用户研究表明，其推理输出的逻辑获得了93.33%临床医生的认可，表明推理过程类似心脏科医生。这些数据验证了方法在提升诊断准确性和可解释性方面的有效性。",
      "conclusion": "该研究通过引入CRT和CardiacMind，显著提升了MLLM在心脏超声诊断中的推理能力。主要贡献在于提供了结构化推理路径和强化学习奖励机制，促进了医疗诊断的准确性和可解释性。学术价值在于为医疗AI的可解释推理提供了新思路，实际应用价值可能扩展到其他医学领域。未来工作可改进奖励设计或应用于更多疾病类型。",
      "tags": [
        "Multimodal Large Language Models (MLLMs)",
        "Reinforcement Learning",
        "Reward Mechanism",
        "Cardiac Reasoning",
        "Medical Image Diagnosis"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:13.536362Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08435",
    "title": "Fine-Mem: Fine-Grained Feedback Alignment for Long-Horizon Memory Management",
    "authors": [
      "Weitao Ma",
      "Xiaocheng Feng",
      "Lei Huang",
      "Xiachong Feng",
      "Zhanyu Ma",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He",
      "Bing Qin"
    ],
    "abstract": "Effective memory management is essential for large language model agents to navigate long-horizon tasks. Recent research has explored using Reinforcement Learning to develop specialized memory manager agents. However, existing approaches rely on final task performance as the primary reward, which results in severe reward sparsity and ineffective credit assignment, providing insufficient guidance for individual memory operations. To this end, we propose Fine-Mem, a unified framework designed for fine-grained feedback alignment. First, we introduce a Chunk-level Step Reward to provide immediate step-level supervision via auxiliary chunk-specific question answering tasks. Second, we devise Evidence-Anchored Reward Attribution to redistribute global rewards by anchoring credit to key memory operations, based on the specific memory items utilized as evidence in reasoning. Together, these components enable stable policy optimization and align local memory operations with the long-term utility of memory. Experiments on Memalpha and MemoryAgentBench demonstrate that Fine-Mem consistently outperforms strong baselines, achieving superior success rates across various sub-tasks. Further analysis reveals its adaptability and strong generalization capabilities across diverse model configurations and backbones.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08435.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08435",
    "published": "2026-01-13T11:06:17Z",
    "updated": "2026-01-13T11:06:17Z",
    "comment": "18 pages, 5 figures",
    "light_analysis": {
      "overview": "论文提出Fine-Mem框架，通过细粒度反馈对齐解决大型语言模型代理在长视野内存管理中的奖励稀疏问题。",
      "motivation": "研究动机是提升大型语言模型代理在长视野任务中的内存管理效果，因为现有方法使用强化学习但依赖最终任务性能作为主要奖励，导致奖励稀疏和信用分配无效。这种局限性使得个体内存操作缺乏足够指导，影响任务成功率，因此需要开发更细致的反馈机制来优化内存管理策略，以增强稳定性和性能。",
      "method": "研究方法采用Fine-Mem框架，包括两个核心创新：Chunk-level Step Reward和Evidence-Anchored Reward Attribution。前者通过辅助的chunk特定问答任务提供步级监督，为内存操作提供即时反馈；后者基于推理中使用的内存项作为证据，重新分配全局奖励，将信用锚定到关键内存操作。这些组件结合使用，实现稳定的策略优化，并确保局部内存操作与内存的长期效用对齐，提升任务执行效率。",
      "result": "实验在Memalpha和MemoryAgentBench数据集上进行，Fine-Mem consistently outperforms strong baselines，在多个子任务中实现了更高的成功率，但摘要未明确说明具体数据。结果表明其性能显著优于基线方法，并通过进一步分析证实了其适应性和强泛化能力，能在不同模型配置和主干网上保持有效运作。",
      "conclusion": "结论是Fine-Mem框架通过细粒度反馈对齐，有效解决了长视野内存管理中的奖励稀疏和信用分配问题，提升了任务成功率。其学术价值在于为强化学习在内存管理领域提供了新方法，实际应用可增强大型语言模型代理的长期任务执行能力。摘要未明确说明局限性或未来工作方向，但暗示了在泛化性和适应性方面的进一步探索空间。",
      "tags": [
        "Reinforcement Learning",
        "Memory Management",
        "Fine-Grained Feedback Alignment",
        "Reward Attribution",
        "Long-Horizon Tasks"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:09.724490Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08430",
    "title": "RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation",
    "authors": [
      "Sunzhu Li",
      "Jiale Zhao",
      "Miteto Wei",
      "Huimin Ren",
      "Yang Zhou",
      "Jingwen Yang",
      "Shunyu Liu",
      "Kaike Zhang",
      "Wei Chen"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08430.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08430",
    "published": "2026-01-13T10:56:39Z",
    "updated": "2026-01-13T10:56:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了自动化的粗到细评价标准生成框架，并构建了大规模多领域RubricHub数据集，通过基于评价标准的后训练方法显著提升了语言模型在开放域生成任务中的性能。",
      "motivation": "研究旨在解决开放域生成任务中因缺乏地面真实数据而难以优化的问题。现有基于评价标准的方法存在可扩展性瓶颈和标准粗糙的局限性，导致监督天花板效应，限制了模型在推理密集型任务中的性能提升。因此，开发更精细、高区分度的评价标准生成方法至关重要，以捕捉生成内容的细微差别并改进模型训练。",
      "method": "论文提出自动化的粗到细评价标准生成框架，结合原则指导合成、多模型聚合和难度进化，生成全面且高区分度的评价标准。基于此框架，构建了RubricHub数据集，规模约110k样本，涵盖多领域。验证方法采用两阶段后训练流水线，包括基于评价标准的拒绝采样微调（RuFT）和强化学习（RuRL），以优化大型语言模型如Qwen3-14B。",
      "result": "实验结果显示，RubricHub数据集带来了显著性能提升。后训练的Qwen3-14B在HealthBench基准上达到69.3分，创造了新的SOTA记录，并超越了GPT-5等专有前沿模型。这证明了该框架和数据集的实用性和有效性，在开放域生成任务中实现了明显的性能改进。",
      "conclusion": "本研究的主要贡献是提出了自动化评价标准生成框架和RubricHub数据集，并通过后训练方法验证了其性能提升。学术上，为解决开放域生成的监督问题提供了新思路；应用上，可推动AI在医疗等领域的推理任务发展。未来工作可能包括释放代码和数据，以促进社区研究和进一步优化方法。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Coarse-to-Fine Rubric Generation",
        "Rubric-based Fine-Tuning",
        "Reinforcement Learning Post-training",
        "Multi-domain Dataset"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:32.435188Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08429",
    "title": "Deep Learning Based Facial Retargeting Using Local Patches",
    "authors": [
      "Yeonsoo Choi",
      "Inyup Lee",
      "Sihun Cha",
      "Seonghyeon Kim",
      "Sunjin Jung",
      "Junyong Noh"
    ],
    "abstract": "In the era of digital animation, the quest to produce lifelike facial animations for virtual characters has led to the development of various retargeting methods. While the retargeting facial motion between models of similar shapes has been very successful, challenges arise when the retargeting is performed on stylized or exaggerated 3D characters that deviate significantly from human facial structures. In this scenario, it is important to consider the target character's facial structure and possible range of motion to preserve the semantics assumed by the original facial motions after the retargeting. To achieve this, we propose a local patch-based retargeting method that transfers facial animations captured in a source performance video to a target stylized 3D character. Our method consists of three modules. The Automatic Patch Extraction Module extracts local patches from the source video frame. These patches are processed through the Reenactment Module to generate correspondingly re-enacted target local patches. The Weight Estimation Module calculates the animation parameters for the target character at every frame for the creation of a complete facial animation sequence. Extensive experiments demonstrate that our method can successfully transfer the semantic meaning of source facial expressions to stylized characters with considerable variations in facial feature proportion.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08429.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08429",
    "published": "2026-01-13T10:56:15Z",
    "updated": "2026-01-13T10:56:15Z",
    "comment": "Eurographics 25",
    "light_analysis": {
      "overview": "提出了一种基于局部补丁的深度学习面部重定向方法，有效将源视频中的面部动画语义转移到风格化3D角色。",
      "motivation": "在数字动画领域，生成逼真的面部动画对于虚拟角色至关重要。现有面部重定向方法在处理形状相似的模型时已成功，但当应用于风格化或夸张的3D角色时，由于面部结构与人脸差异显著，现有方法难以保持原始面部运动的语义，导致动画表达意图丢失。因此，本研究旨在解决这一挑战，通过考虑目标角色的面部结构和运动范围，确保重定向后语义的保留，提升动画质量。",
      "method": "论文提出了一种基于局部补丁的面部重定向方法，核心包括三个模块：自动补丁提取模块从源视频帧提取局部面部补丁；重现模块处理这些补丁生成目标角色的局部补丁；权重估计模块计算每帧动画参数以生成完整面部动画序列。该方法利用局部补丁适应目标角色的独特结构，通过深度学习实现语义转移，具体模型架构和数据集在摘要中未明确说明。",
      "result": "通过大量实验验证，该方法能成功将源面部表情的语义转移到具有显著面部特征比例变化的风格化角色。实验展示了方法在保持动画语义方面的有效性，但摘要未提供具体性能指标如准确率或与基线方法的对比数据，可以推断其相比传统方法在处理非标准面部结构时表现更优。",
      "conclusion": "本研究的主要贡献是提出一种局部补丁基于深度学习的面部重定向方法，解决了风格化3D角色的面部动画转移问题。学术价值在于为面部重定向提供新视角，强调局部处理以保持语义；实际应用可扩展到数字动画、游戏和虚拟现实，提升动画真实感。未来工作可能包括优化补丁提取和参数估计模块以处理更复杂变化。",
      "tags": [
        "Deep Learning",
        "Facial Retargeting",
        "Local Patches",
        "3D Animation",
        "Computer Vision"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:29.759757Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08427",
    "title": "Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering",
    "authors": [
      "Nonghai Zhang",
      "Weitao Ma",
      "Zhanyu Ma",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He",
      "Jingwen Xu"
    ],
    "abstract": "Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08427.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08427",
    "published": "2026-01-13T10:55:08Z",
    "updated": "2026-01-13T10:55:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Latent-GRPO框架，利用潜在空间几何聚类自动生成内在奖励，实现自验证强化学习，显著提升训练效率。",
      "motivation": "研究动机源于Group Relative Policy Optimization (GRPO) 虽能提升大型语言模型的推理性能，但严重依赖昂贵外部验证器或人工规则，导致计算成本高昂、训练延迟增加，且稀疏奖励阻碍优化效率。因此，本研究旨在解决这些局限性，开发一种更高效、不依赖外部资源的强化学习方法，以降低应用门槛并加速模型训练过程。",
      "method": "核心方法是Latent-GRPO框架，它基于潜在空间几何性质分析：正确推理轨迹的终端令牌表示形成密集集群，而错误轨迹作为异常值。为此，提出Iterative Robust Centroid Estimation (IRCE)算法，通过球面投影减轻幅度波动，并通过迭代聚合估计稳健的‘真相中心’，从而生成密集连续的内在奖励，减少对外部验证器的依赖，关键创新在于利用几何聚类自动构建奖励机制。",
      "result": "实验在多个数据集上进行，结果显示Latent-GRPO方法在保持模型推理性能的同时，实现了训练速度提升超过2倍，优于基线方法。具体表现为效率改进和泛化能力增强，显示出强鲁棒性，例如训练加速和性能稳定，但未提及具体准确率数值，强调了整体效能的优化。",
      "conclusion": "本研究的贡献在于提出自验证框架Latent-GRPO，通过潜在几何聚类减少对外部验证器的依赖，提升强化学习训练的效率和鲁棒性，为大型语言模型推理优化提供新思路。未来工作可探索更多潜在空间性质或应用于其他推理任务，潜在局限性可能在摘要中未明确说明，但强调了方法的泛化性。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Geometric Clustering",
        "Self-Verifier",
        "Robust Centroid Estimation"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:35.906158Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08421",
    "title": "Coverage Improvement and Fast Convergence of On-policy Preference Learning",
    "authors": [
      "Juno Kim",
      "Jihun Yun",
      "Jason D. Lee",
      "Kwang-Sung Jun"
    ],
    "abstract": "Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \\emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \\emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08421.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08421",
    "published": "2026-01-13T10:46:06Z",
    "updated": "2026-01-13T10:46:06Z",
    "comment": "46 pages, 2 figures, 2 tables",
    "light_analysis": {
      "overview": "论文提出覆盖改进原则，理论证明在线策略偏好学习算法的快速指数收敛，并设计一种基于偏好G-最优设计的混合采样器。",
      "motivation": "在线策略偏好学习算法，如在线直接策略优化（DPO），在语言模型对齐中表现优于离线方法，但现有研究缺乏对其快速收敛的理论解释，这限制了算法的进一步优化和应用。因此，本研究旨在通过分析采样策略覆盖的演进，提供理论依据，以解决在线学习优势的本质问题，并设计改进的采样方法来提升效率。",
      "method": "论文提出覆盖改进原则，认为在线更新能改善采样策略的覆盖，使数据更富信息性，从而加速收敛。在Bradley-Terry偏好和线性softmax策略类的情境老虎机设置中，理论证明在线DPO在批大小超过广义覆盖阈值时呈指数收敛。此外，设计一种基于新概念“偏好”G-最优设计的混合采样器，以减少对初始覆盖的依赖，并保证两轮内收敛，同时扩展到一般函数类中的奖励蒸馏方案。",
      "result": "理论分析表明，在线策略DPO在批大小足够时，迭代次数指数收敛，而任何使用离线样本的方法收敛速率较慢，导致样本复杂度差异显著。实验部分确认，在线DPO及提出的奖励蒸馏算法在性能上超越离线版本，迭代过程中展现出稳定且单调的性能增益，验证了理论预测的有效性和快速收敛性。",
      "conclusion": "本研究为在线策略偏好学习的快速收敛提供了理论支撑，提出覆盖改进原则，并设计了高效的混合采样器和奖励蒸馏方案。其学术意义在于填补了在线学习理论分析的空白，实际应用中可提升语言模型对齐的效率。未来可能探索更广泛的函数类或更复杂的偏好设置以扩展应用范围。",
      "tags": [
        "On-policy Learning",
        "Direct Policy Optimization",
        "Bradley-Terry Model",
        "G-optimal Design",
        "Reward Distillation"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:43.662793Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08420",
    "title": "MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP",
    "authors": [
      "Aditya Chaudhary",
      "Sneha Barman",
      "Mainak Singha",
      "Ankit Jha",
      "Girish Mishra",
      "Biplab Banerjee"
    ],
    "abstract": "In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08420.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08420",
    "published": "2026-01-13T10:44:37Z",
    "updated": "2026-01-13T10:44:37Z",
    "comment": "Accepted at InGARSS 2025",
    "light_analysis": {
      "overview": "MMLGNet提出了一种基于CLIP的多模态框架，用于对齐遥感数据与自然语言语义，以增强跨模态的语义级理解。",
      "motivation": "随着高光谱成像（HSI）和LiDAR等多模态地球观测数据的增加，当前需要有效融合光谱、空间和几何信息的方法，但现有视觉方法缺乏语义理解能力，难以直接与语言解释对齐。本研究旨在解决遥感数据语义解释的挑战，通过引入语言监督来弥合高维数据与语义理解之间的差距，为环境监测和城市规划等应用提供更智能的工具。",
      "method": "MMLGNet使用模态特定的编码器（如基于CNN的视觉编码器）提取各模态特征，并通过双向对比学习将视觉特征与手工艺文本嵌入对齐到共享潜在空间中。该方法受CLIP训练范式的启发，利用视觉语言模型进行跨模态对齐，关键创新在于简化编码器设计的同时，有效结合语言引导来实现多模态数据的语义融合。",
      "result": "在实验中，MMLGNet在两个基准数据集上展现出强劲性能，超越了多个已建立的纯视觉多模态方法，尽管摘要未明确说明具体指标，但结果证明了语言监督显著提升了模型性能，强调了该方法在跨模态对齐任务中的有效性。",
      "conclusion": "本研究的主要贡献是提出并验证了MMLGNet框架，通过结合CLIP实现遥感数据与语言语义的对齐。学术上，它展示了语言监督在多模态遥感领域的价值；实际上，可能促进遥感图像的语义解释应用。未来工作可探索更复杂的编码器或扩展到其他遥感模态。",
      "tags": [
        "Multimodal Learning",
        "Remote Sensing",
        "CLIP",
        "Cross-Modal Alignment",
        "Contrastive Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:08.264719Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08418",
    "title": "Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance",
    "authors": [
      "Jihang Li",
      "Qing Liu",
      "Zulong Chen",
      "Jing Wang",
      "Wei Wang",
      "Chuanfei Xu",
      "Zeyi Wen"
    ],
    "abstract": "Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08418.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08418",
    "published": "2026-01-13T10:41:23Z",
    "updated": "2026-01-13T10:41:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "Taxon框架通过特征门控混合专家架构和LLM蒸馏的语义一致性模型，实现层次税码预测，提升准确性和鲁棒性。",
      "motivation": "税码预测是自动化开票和合规管理的关键任务，尤其在大规模电商平台中。每个产品必须准确映射到国家定义的多级税分类层次，错误会导致财务不一致和监管风险。目前该任务研究不足，现有方法可能难以处理噪声监督和多模态特征，亟需结合语义对齐和专家指导的解决方案来提高预测精度。",
      "method": "Taxon框架整合两个核心组件：特征门控混合专家架构自适应路由多模态特征到税分类层次的不同级别；以及从大型语言模型蒸馏的语义一致性模型，作为领域专家验证产品标题与官方税定义的语义对齐。针对实际业务记录的噪声监督，设计多源训练管道，结合策划税数据库、发票验证日志和商户注册数据，提供结构和语义监督。",
      "result": "在专有TaxCode数据集和公共基准上，Taxon实现最先进的性能，优于强基线方法。额外的全层次路径重建程序显著提高结构一致性，获得最高的整体F1分数。Taxon已部署于阿里巴巴税服务系统，平均每天处理超过500,000个税码查询，业务高峰时超过500万请求，提高了准确性、可解释性和鲁棒性。",
      "conclusion": "Taxon论文的主要贡献是提出一个语义对齐和专家指导的层次税码预测框架，学术价值在于融合多模态特征处理和语义验证，为该领域提供新方法。实际应用中，成功部署处理大规模查询，提升自动化开票和合规管理的效率与可靠性。摘要未明确说明局限性和未来工作，但可推断进一步优化模型的泛化能力和扩展性。",
      "tags": [
        "Large Language Model",
        "Mixture of Experts",
        "Semantic Consistency",
        "Hierarchical Prediction",
        "Feature Gating"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:16.561783Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08414",
    "title": "SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration",
    "authors": [
      "Chentian Sun"
    ],
    "abstract": "Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08414.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08414",
    "published": "2026-01-13T10:32:22Z",
    "updated": "2026-01-13T10:32:22Z",
    "comment": "10 pages, 1 figures, submitted to Trans on Image Processing",
    "light_analysis": {
      "overview": "提出SPARK框架，实现自校准实时多摄像头点云重建，联合处理点云融合和相机外参不确定性，提升多视图场景下的性能。",
      "motivation": "实时多摄像头3D重建在3D感知、沉浸式交互和机器人技术中至关重要，但现有方法在多视图融合时面临相机外参不确定性、难以处理动态场景和大型摄像头设置的可扩展性不足等问题，限制了实际应用效果。摘要指出这些挑战影响重建的准确性和效率，因此需要一种更稳健的解决方案。",
      "method": "SPARK框架包括两个核心模块：几何感知在线外参估计模块，利用多视图先验并强制交叉视图和时间一致性，实现稳定自校准；置信度驱动点云融合策略，建模像素和点级别的深度可靠性和可见性，以抑制噪声和视图依赖性不一致。通过无累积帧融合方式，框架在动态场景中生成稳定点云，且摄像头数量线性扩展。关键创新在于联合优化外参估计和点云融合。",
      "result": "在真实世界多摄像头系统上的广泛实验表明，SPARK在外参准确性、几何一致性、时间稳定性和实时性能方面优于现有方法。摘要未提供具体数据，但实验结果展示了显著的性能提升，证明了其有效性和可扩展性。",
      "conclusion": "SPARK的主要贡献在于提供了一种自校准、实时、可扩展的多摄像头点云重建框架，解决了多视图融合和外参不确定性问题，对于大规模3D重建应用具有重要价值。摘要未明确说明局限性，潜在未来工作可能包括进一步优化或扩展至更复杂场景。",
      "tags": [
        "Point Cloud Reconstruction",
        "Multi-View Self-Calibration",
        "Online Extrinsic Estimation",
        "Real-Time 3D Perception",
        "Confidence-Driven Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:03.361990Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08412",
    "title": "Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation",
    "authors": [
      "Yizhan Feng",
      "Hichem Snoussi",
      "Yuhang Wang",
      "Jing Teng",
      "Abel Cherouat",
      "Tian Wang"
    ],
    "abstract": "With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08412.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08412",
    "published": "2026-01-13T10:31:09Z",
    "updated": "2026-01-13T10:31:09Z",
    "comment": "2nd International Conference on Drones and Unmanned Systems (DAUS' 2026)",
    "light_analysis": {
      "overview": "本文提出一种结合知识蒸馏、链式思维指导和监督微调的集成方法，旨在为无人机控制代码生成实现高效轻量化模型。",
      "motivation": "大型语言模型在代码生成任务中展现出显著潜力，但其高资源消耗与无人机（UAV）平台对实时性和轻量化的需求存在突出矛盾。现有方法难以在维持高性能的同时满足部署效率要求，缺乏有效的轻量化解决方案来平衡推理能力和资源约束，这使得开发能够将复杂代码生成能力迁移到小模型的创新技术变得至关重要。",
      "method": "论文提出一个集成方法，首先构建覆盖多种主流无人机SDK的高质量数据集，融入指令-代码-推理链和反事实负样本以增强数据多样性。其次，采用DeepSeek-Coder-V2-Lite作为教师模型（通过QLoRA量化），基于混合黑盒和白盒蒸馏策略生成链式思维软标签，结合硬标签的加权交叉熵损失，将复杂推理能力转移至学生模型。最后，通过针对无人机控制场景优化的提示调整工程，提升模型在SDK类型识别和函数调用匹配等核心任务的性能。",
      "result": "实验结果显示，蒸馏后的轻量模型在保持高代码生成准确率的同时，部署和推理效率得到显著提升，具体性能指标摘要未明确说明。与基线方法相比，该方法有效证明了在无人机控制中实现精准轻量化智能控制的可行性和优越性，展示了其在实际应用中的高效性。",
      "conclusion": "该研究的主要贡献在于提出并验证了一种结合知识蒸馏和链式思维指导的混合方法，成功实现了无人机控制代码生成的高效轻量化。这不仅在学术上为边缘计算中的模型优化提供了新思路，在实际应用中也为无人机等资源受限平台的智能控制开辟了路径。未来工作可进一步探索该方法在其他边缘AI任务中的普适性，并优化蒸馏策略以处理更复杂的推理场景。",
      "tags": [
        "Knowledge Distillation",
        "Chain-of-Thought",
        "UAV Control",
        "Code Generation",
        "Prompt Tuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:21.684549Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08408",
    "title": "Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2",
    "authors": [
      "Yizhan Feng",
      "Hichem Snoussi",
      "Jing Teng",
      "Jian Liu",
      "Yuyang Wang",
      "Abel Cherouat",
      "Tian Wang"
    ],
    "abstract": "The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08408.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08408",
    "published": "2026-01-13T10:26:10Z",
    "updated": "2026-01-13T10:26:10Z",
    "comment": "The Tenth International Conference on Data Mining and Big Data (DMBD'2025)",
    "light_analysis": {
      "overview": "论文提出一个基于BLIP-2的边缘优化多模态学习平台，通过集成YOLO模型和内容感知关键帧采样机制，扩展无人机的视频理解能力。",
      "motivation": "无人机在复杂场景中如监测和搜索需要实时视觉理解和交互，但大型视觉语言模型如BLIP-2计算成本高，与无人机边缘设备有限的计算资源存在矛盾。现有方法通常需要高计算开销或任务特定微调，限制了在实时应用中的部署和效率，因此研究轻量级多模态平台以平衡性能与资源约束变得至关重要。",
      "method": "该方法基于BLIP-2构建轻量级平台，集成YOLO-World用于对象检测和YOLOv8-Seg用于实例分割，提供精确感知结果。关键创新包括基于K-Means聚类的内容感知关键帧采样机制，结合智能帧选择和时序特征拼接，处理视频级任务；并实施统一提示优化方案，将YOLO模型的结构化事件日志作为上下文注入BLIP-2输入，设计输出约束过滤技术细节，引导模型生成准确输出。",
      "result": "摘要未明确说明具体实验数据，但该方法通过集成YOLO模型增强感知精度，关键帧采样优化视频处理效率，提示优化提高输出质量，预期能提升无人机视频理解的实时性和准确性，同时减少计算开销，适合边缘部署，可能避免任务特定微调需求，提高适应性和效率。",
      "conclusion": "该研究提出轻量级多模态平台，解决大型模型计算成本与边缘设备资源有限的矛盾，扩展BLIP-2在无人机应用中的多任务能力，无需任务特定微调降低部署门槛。学术上推动边缘多模态学习发展，实际上增强无人机在复杂场景中的实时交互能力，未来工作可能包括进一步优化算法或扩展到更多任务。",
      "tags": [
        "Multimodal Learning",
        "BLIP-2",
        "YOLO",
        "Key Frame Sampling",
        "Edge Computing"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:40.350804Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08406",
    "title": "WebTrap Park: An Automated Platform for Systematic Security Evaluation of Web Agents",
    "authors": [
      "Xinyi Wu",
      "Jiagui Chen",
      "Geng Hong",
      "Jiayi Dong",
      "Xudong Pan",
      "Jiarun Dai",
      "Min Yang"
    ],
    "abstract": "Web Agents are increasingly deployed to perform complex tasks in real web environments, yet their security evaluation remains fragmented and difficult to standardize. We present WebTrap Park, an automated platform for systematic security evaluation of Web Agents through direct observation of their concrete interactions with live web pages. WebTrap Park instantiates three major sources of security risk into 1,226 executable evaluation tasks and enables action based assessment without requiring agent modification. Our results reveal clear security differences across agent frameworks, highlighting the importance of agent architecture beyond the underlying model. WebTrap Park is publicly accessible at https://security.fudan.edu.cn/webagent and provides a scalable foundation for reproducible Web Agent security evaluation.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08406.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08406",
    "published": "2026-01-13T10:21:28Z",
    "updated": "2026-01-13T10:21:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了 WebTrap Park，一个自动化平台，用于系统化评估 Web Agents 的安全性。",
      "motivation": "随着 Web Agents 在真实网页环境中被广泛部署以执行复杂任务，确保其安全性成为关键挑战。然而，当前的安全性评估方法分散且缺乏标准化，难以统一识别和应对潜在风险，导致评估结果不可靠和难以复现。本研究旨在解决这一问题，通过开发一个系统化平台来提升评估的规范性和可重复性，以弥补现有方法的不足，并促进 Web Agents 在实际应用中的安全部署。",
      "method": "研究方法基于构建 WebTrap Park 自动化平台，核心创新是将三个主要安全风险来源实例化为 1,226 个可执行评估任务。该平台通过直接观察 Web Agents 与 live web pages 的具体交互进行行动基于的评估，无需修改代理本身。技术特色在于提供了一种标准化和可扩展的评估框架，利用可执行任务来模拟真实环境，从而系统地评估不同代理框架的安全性表现。",
      "result": "实验结果显示，不同 Web Agent 框架在安全性上存在明显差异，这突显了代理架构的重要性，而不仅仅是底层模型的影响。摘要未明确说明具体性能指标如准确率或效率改进，但 WebTrap Park 成功实例化了 1,226 个任务，为框架间的安全比较提供了基础，表明该平台能有效揭示不同代理设计的安全薄弱点。",
      "conclusion": "WebTrap Park 的主要贡献在于提供了一个公开访问的自动化平台，支持可重复和系统化的 Web Agents 安全性评估，推动了该领域评估方法的标准化。其学术价值在于为安全评估研究提供了可扩展基础，实际应用价值在于帮助开发者识别和缓解安全风险。未来工作可能扩展风险来源评估范围或集成更多评估指标，以进一步强化平台的适用性。",
      "tags": [
        "Web Agents",
        "Security Evaluation",
        "Automated Platform",
        "Evaluation Tasks",
        "Agent Architecture"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:36.474794Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08404",
    "title": "Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime",
    "authors": [
      "Binh Duong Nguyen",
      "Stefan Sandfeld"
    ],
    "abstract": "Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08404.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08404",
    "published": "2026-01-13T10:20:59Z",
    "updated": "2026-01-13T10:20:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种多通道U-Net深度学习代理模型，在小数据设置下实现二维PDE动态的分布外泛化预测。",
      "motivation": "偏微分方程模拟计算成本高昂，而科学应用中空间场的演化预测是核心需求。现有数据驱动方法在大数据下有效，但在小数据设置中对分布外初始条件的泛化能力不足，这限制了实际应用，尤其是在物理和工程领域需要高效模拟时。本研究旨在解决二维PDE动态在固定参数体系内，仅使用少量模拟轨迹（最多约100个）进行分布外泛化的挑战，以提升科学机器学习的实用性和效率。",
      "method": "研究引入了一个多通道U-Net（me-UNet）作为自回归深度学习代理模型，专门针对二维PDE动态在周期性域上的预测。关键创新在于结合卷积架构的局部性和周期性边界条件归纳偏置，以增强模型在少量数据下的泛化能力。在方法上，评估了五个不同PDE家族，并与ViT、AFNO、PDE-Transformer和KAN-UNet在统一训练设置下进行比较，使用严格的小数据场景来验证其技术特色和有效性。",
      "result": "实验结果显示，me-UNet在所有五个PDE数据集上匹配或优于更复杂架构，在分布内滚动中表现出更低的场空间误差、谱相似性和基于物理的指标，同时训练时间显著减少（摘要未明确说明具体百分比）。它能泛化到仅约20个训练模拟的未见初始条件，显示了在小数据设置下的强鲁棒性。与基线方法如ViT和AFNO的对比进一步证实其优势，提升了预测准确性和计算效率。",
      "conclusion": "论文的主要贡献是证明了具有局部性和周期性归纳偏置的卷积架构在小数据二维PDE代理建模中的优势，提供了准确且适度分布外鲁棒的解决方案。其学术价值在于推动了科学机器学习中高效模拟方法的发展，应用价值在于加速物理系统动态预测，降低计算成本。潜在局限性包括专注于周期性边界条件，未来工作可扩展至更广泛域或三维动态，以增强通用性。",
      "tags": [
        "Deep Learning Surrogates",
        "U-Net",
        "Out-of-distribution Generalization",
        "Partial Differential Equations",
        "Small-data Regime"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:37.291705Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08403",
    "title": "Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs",
    "authors": [
      "Abhijnan Nath",
      "Alireza Bagheri Garakani",
      "Tianchen Zhou",
      "Fan Yang",
      "Nikhil Krishnaswamy"
    ],
    "abstract": "Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08403.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08403",
    "published": "2026-01-13T10:17:46Z",
    "updated": "2026-01-13T10:17:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出OSPO框架，一种基于Shapley-Owen属性的强化学习算法，用于优化生成搜索大型语言模型的信用分配问题。",
      "motivation": "研究动机源于大型语言模型在强化学习中用于个性化推荐任务时面临信用分配差距问题。标准方法如GRPO依赖稀疏序列级奖励，难以确定哪些具体令牌驱动任务成功，尤其在模型必须从未指定语言推断潜在用户意图时更为突出。这导致模型性能受限，因为预训练阶段缺乏此类推理模式，现有方法无法有效分配信用到响应部分，影响准确性和泛化能力。",
      "method": "OSPO框架采用Shapley-Owen属性计算令牌对序列结果的边际贡献，通过潜在奖励重塑重新分配序列级优势到片段级信用。关键创新包括形成语义一致单元的联盟（如短语或句子描述产品属性或偏好），直接基于任务反馈学习，无需额外的参数化值模型。技术特色是保持最优策略的同时优化信用分配，降低计算复杂度，通过合作博弈理论实现细粒度性能驱动因素识别。",
      "result": "在Amazon ESCI和H&M Fashion数据集上的实验显示，OSPO相比基线方法（如GRPO）实现了一致的性能提升，具体表现为在推荐任务中获得更高的准确性。摘要未明确说明具体数据百分比，但强调了优于基线并具有测试时鲁棒性，即使在训练未见过的分布外检索器上也能保持良好性能，增强了模型的泛化能力和应用可靠性。",
      "conclusion": "OSPO的主要贡献是解决了强化学习中的信用分配问题，通过Shapley-Owen属性提供了一种原则性方法，提高大型语言模型在生成搜索任务中的性能和鲁棒性。学术价值在于扩展了强化学习理论到语言模型优化，实际应用可提升个性化推荐系统的效果和效率。未来工作方向可能包括进一步优化计算效率或扩展到更多复杂任务，但摘要未明确说明局限性。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Model",
        "Shapley-Owen Attribution",
        "Policy Optimization",
        "Generative Search"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:45.408623Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08402",
    "title": "PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors",
    "authors": [
      "Donya Rooein",
      "Sankalan Pal Chowdhury",
      "Mariia Eremeeva",
      "Yuan Qin",
      "Debora Nozza",
      "Mrinmaya Sachan",
      "Dirk Hovy"
    ],
    "abstract": "Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08402.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08402",
    "published": "2026-01-13T10:17:26Z",
    "updated": "2026-01-13T10:17:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于学生个性的辅导策略，利用大型语言模型实现个性化教育辅导，以优化辅导效果。",
      "motivation": "尽管大型语言模型在教育辅导中展现出潜力，但现有系统普遍忽视了学生个性差异，导致辅导策略可能不适用，进而影响学习成果。这一问题的重要性在于个性化教育是提高学习效率的关键，而目前方法缺乏个性因素的整合，限制了辅导的有效性。因此，本研究旨在解决如何将个性意识融入LLM辅导系统，以克服现有不足并提升教育应用的针对性。",
      "method": "论文首先基于教育学文献构建了一个分类法，将不同教学方法与具体的学生个性概况相关联。然后，利用大型语言模型模拟师生对话，通过设计框架使LLM辅导者能根据模拟学生的个性动态调整辅导策略。关键创新点在于结合教育学理论和模拟技术，实现个性驱动的策略自适应，但摘要未详细说明具体模型架构或数据集细节。",
      "result": "通过人类教师评估，发现他们一致偏好本方法超过两个基线方法，表明策略匹配度较高。此外，方法显著增加了不常见但高影响力的策略（如角色扮演）的使用，人类和LLM标注者对这些策略有显著偏好，但摘要未提供具体性能指标数据，仅基于偏好和策略使用量进行推断。",
      "conclusion": "本研究开发了personality-aware teaching strategies，为大型语言模型在教育辅导中的个性化应用提供了理论和方法支持。学术上填补了LLM辅导中个性因素研究的空白，实际应用中有助于提高辅导效果和效率。未来工作可包括在真实场景验证方法、扩展个性分类，局限可能在于模拟对话与现实情况的差异，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Personality-Aware Teaching",
        "Educational Tutoring",
        "Role-Playing",
        "Simulated Conversations"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:57.663613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08401",
    "title": "An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50",
    "authors": [
      "Ajo Babu George",
      "Pranav S",
      "Kunal Agarwal"
    ],
    "abstract": "Objectives: To overcome challenges in diagnosing pericoronitis on panoramic radiographs, an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability. Methods: A two-stage deep learning pipeline was implemented. The first stage used YOLOv8 to detect third molars and classify their anatomical positions and angulations based on Winter's classification. Detected regions were then fed into a second-stage classifier, a modified ResNet-50 architecture, for detecting radiographic features suggestive of pericoronitis. To enhance clinical trust, Grad-CAM was used to highlight key diagnostic regions on the radiographs. Results: The YOLOv8 component achieved 92% precision and 92.5% mean average precision. The ResNet-50 classifier yielded F1-scores of 88% for normal cases and 86% for pericoronitis. Radiologists reported 84% alignment between Grad-CAM and their diagnostic impressions, supporting the radiographic relevance of the interpretability output. Conclusion: The system shows strong potential for AI-assisted panoramic assessment, with explainable AI features that support clinical confidence.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08401.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08401",
    "published": "2026-01-13T10:15:45Z",
    "updated": "2026-01-13T10:15:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种可解释的两阶段深度学习框架，结合YOLOv8和ResNet-50，用于在全景X光片上评估pericoronitis，并通过Grad-CAM增强临床可解释性。",
      "motivation": "研究动机在于解决全景X光片中pericoronitis（第三磨牙周围炎症）诊断的挑战。由于全景X光片解剖结构复杂，传统诊断方法可能依赖人工观察，效率低下且易出错，现有AI方法在可解释性和临床集成方面不足。本系统旨在开发一个集成解剖定位、病理分类和可解释性的AI辅助评估系统，以提升诊断准确性和临床医生信任，填补自动化诊断工具的空白。",
      "method": "研究方法采用两阶段深度学习流程：第一阶段使用YOLOv8模型在全景X光片中检测第三磨牙，并基于Winter分类法分类其解剖位置和角度；检测到的区域裁剪后输入第二阶段，采用修改的ResNet-50架构进行分类，以识别与pericoronitis相关的放射特征。关键创新是引入Grad-CAM技术，生成热图突出显示模型决策的关键区域，增强可解释性。该方法结合了目标检测和图像分类技术，优化了模型在医学图像分析中的应用。",
      "result": "实验结果显示，YOLOv8组件在检测第三磨牙时达到92%的精度和92.5%的平均精度。ResNet-50分类器在区分正常和pericoronitis病例方面，F1分数分别为88%和86%，表明分类性能良好。此外，Grad-CAM的可视化输出与放射科医生的诊断印象有84%的对齐度，验证了模型在解释性方面的有效性。这些结果支持了系统在实际临床环境中的潜在应用，尽管摘要未明确与基线方法的直接对比。",
      "conclusion": "研究表明，该两阶段深度学习框架在全景X光片pericoronitis评估中具有强大潜力，通过集成YOLOv8和ResNet-50，并结合Grad-CAM增强可解释性，系统能提供准确且可信的诊断支持。这推动了AI在口腔放射学中的辅助应用，提高临床信心。未来工作可能包括在更大多样性数据集上验证模型、优化性能以及探索其他可解释AI技术，以进一步促进临床部署。",
      "tags": [
        "YOLOv8",
        "ResNet-50",
        "Grad-CAM",
        "Explainable AI",
        "Deep Learning Framework"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:25.651846Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08394",
    "title": "Design and Development of a Low-Cost Scalable GSM-IoT Smart Pet Feeder with a Remote Mobile Application",
    "authors": [
      "Md. Rakibul Hasan Nishat",
      "S. M. Khalid Bin Zahid",
      "Abdul Hasib",
      "T. M. Mehrab Hasan",
      "Mohammad Arman",
      "A. S. M. Ahsanul Sarkar Akib"
    ],
    "abstract": "Pet ownership is increasingly common in modern households, yet maintaining a consistent feeding schedule remains challenging for the owners particularly those who live in cities and have busy lifestyles. This paper presents the design, development, and validation of a low-cost, scalable GSM-IoT smart pet feeder that enables remote monitoring and control through cellular communication. The device combines with an Arduino microcontroller, a SIM800L GSM module for communication, an ultrasonic sensor for real-time food-level assessment, and a servo mechanism for accurate portion dispensing. A dedicated mobile application was developed using MIT App Inventor which allows owners to send feeding commands and receive real-time status updates. Experimental results demonstrate a 98\\% SMS command success rate, consistent portion dispensing with $\\pm 2.67$\\% variance, and reliable autonomous operation. Its modular, energy-efficient design makes it easy to use in a wide range of households, including those with limited resources. This work pushes forward the field of accessible pet care technology by providing a practical, scalable, and completely internet-independent solution for personalized pet feeding. In doing so, it sets a new benchmark for low-cost, GSM-powered automation in smart pet products.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08394.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08394",
    "published": "2026-01-13T09:59:52Z",
    "updated": "2026-01-13T09:59:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文开发了一种低成本、可扩展的GSM-IoT智能宠物喂食器，通过蜂窝通信和移动应用实现远程喂养控制，创新性地提供了互联网独立的解决方案。",
      "motivation": "随着宠物拥有率上升，城市居民因忙碌生活难以保持固定喂养时间，这已成为普遍问题。现有智能喂食器可能依赖互联网或成本较高，限制了资源有限家庭的普及。本研究旨在解决这一可访问性挑战，通过低成本设计提升宠物护理技术的实用性，以满足现代家庭需求。摘要未明确说明具体现有方法的不足，但强调了互联网独立性和可扩展性的重要性。",
      "method": "研究采用Arduino单片机作为核心控制器，结合SIM800L GSM模块实现蜂窝通信，无需互联网连接。使用超声波传感器实时监测食物水平，伺服机制精确分配食物。移动应用通过MIT App Inventor开发，允许用户发送SMS命令和接收状态更新。关键创新包括模块化设计、能源效率和完全基于GSM的远程控制，适用于各种环境，如资源有限家庭。",
      "result": "实验结果显示，SMS命令成功率达到98%，食物分配方差控制在±2.67%以内，系统运行可靠。尽管摘要未明确说明与基线方法的对比，但数据表明设备在准确性和稳定性方面表现良好，验证了其作为低成本智能喂食器的有效性。这些结果支持了系统的可扩展性和实用性能。",
      "conclusion": "本研究的贡献在于开发了一个实用、可扩展的GSM-IoT智能宠物喂食器，为资源有限家庭提供了互联网独立的喂养解决方案。它推动了可访问宠物护理技术的发展，并设定了低成本、GSM驱动的自动化新基准。学术价值在于验证了蜂窝通信在智能家居中的应用潜力，实际应用可推广到更多场景。未来工作可扩展功能或优化设计，但摘要未详细说明局限性。",
      "tags": [
        "GSM",
        "IoT",
        "Arduino",
        "Ultrasonic Sensor",
        "Mobile Application"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:07.641782Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08393",
    "title": "Controlled LLM Training on Spectral Sphere",
    "authors": [
      "Tian Xie",
      "Haoming Luo",
      "Haoyu Tang",
      "Yiwen Hu",
      "Jason Klein Liu",
      "Qingnan Ren",
      "Yang Wang",
      "Wayne Xin Zhao",
      "Rui Yan",
      "Bing Su",
      "Chong Luo",
      "Baining Guo"
    ],
    "abstract": "Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \\textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08393.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08393",
    "published": "2026-01-13T09:59:47Z",
    "updated": "2026-01-13T09:59:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 Spectral Sphere Optimizer (SSO)，一种完全对齐最大更新参数化 (MuP) 的优化器，通过严格谱约束提高大规模语言模型训练的稳定性和性能。",
      "motivation": "研究动机源于大规模模型训练需要稳定和快速收敛的优化策略。现有方法如最大更新参数化 (MuP) 提供宽度不变的理论保障，但新兴优化器如 Muon 只与这些约束“半对齐”，即控制更新但允许权重漂移，这限制了实际训练的稳定性和收敛效果。当前优化器不完全符合 MuP 理论，可能导致激活控制不稳定，影响模型性能，因此需开发更严格的优化方法来弥补这一不足。",
      "method": "论文提出 Spectral Sphere Optimizer (SSO)，核心方法是通过强制模块级谱约束来控制权重及其更新，实现在谱球上的最陡下降方向推导，确保完全 MuP 对齐的优化过程。关键创新点在于将谱约束集成到优化算法中，提供严格的数学保障。为实现大规模应用，SSO 在 Megatron 框架中实现为高效的并行算法，支持复杂模型架构的训练，如密集和稀疏模型。",
      "result": "主要实验结果表明，在多种架构（包括 Dense 1.7B、MoE 8B-A1B 和 200 层 DeepNet 模型）的预训练中，SSO 一致优于基线方法 AdamW 和 Muon，显示出更高的性能优势。SSO 还提供了显著的稳定性益处，包括改善 MoE 路由器负载均衡、抑制激活异常值，并确保激活函数严格有界，从而增强了整体训练的鲁棒性和效率。",
      "conclusion": "论文的主要贡献是提出 SSO，解决了现有优化器不完全对齐 MuP 的局限性，提升大规模模型训练的稳定性和收敛性。其学术价值在于提供理论保障的优化框架，实际应用价值在于支持更高效 AI 模型开发。摘要未明确说明具体局限性，未来工作可扩展至更多模型类型或探索与其他优化策略的结合。",
      "tags": [
        "Large Language Model Training",
        "Optimization Algorithms",
        "Spectral Constraint",
        "MuP (Maximal Update Parametrization)",
        "Megatron Framework"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:16.208898Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08388",
    "title": "Creativity in AI as Emergence from Domain-Limited Generative Models",
    "authors": [
      "Corina Chutaux"
    ],
    "abstract": "Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08388.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08388",
    "published": "2026-01-13T09:52:14Z",
    "updated": "2026-01-13T09:52:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出生成视角，将AI创造力视为领域有限生成模型在有限信息环境中的涌现属性，而非事后评估标签。",
      "motivation": "人工智能中创造力研究常通过评估框架衡量输出的新颖性、多样性或有用性，但这将创造力视为待评估属性而非建模现象，限制了对其本质的理解。随着大规模多模态生成系统的进步，模式重组变得复杂，引发对机器创造力本质和极限的疑问。本文旨在填补这一空白，强调创造力应作为涌现现象来研究，基于生成模型与环境的交互，以推动AI创造力研究的深入发展，而非停留在事后评估层面。",
      "method": "论文提出一个概念框架，将创造力分解为四个相互作用组件：基于模式的生成、诱导的世界模型、上下文基础和任意性。该方法不引入新评估标准，而是关注生成动力学与领域特定表示之间的结构性和上下文条件，分析创造力如何在多模态生成系统中涌现。核心创新在于从评估转向建模，提供技术视角研究创造力的产生机制，关键细节包括基于生成系统的架构和有限信息环境的设计，以支撑理论分析。",
      "result": "摘要未明确说明具体实验结果，如准确率提升或效率改进，也未提供与基线方法的对比数据。论文主要侧重于理论框架的提出和概念分析，因此结果部分聚焦于框架的潜在应用和理论验证，而非实证性能指标。基于摘要信息，可以推断该框架旨在为未来实验研究奠定基础，但具体效果需通过后续应用和测试来确认。",
      "conclusion": "本研究的核心贡献是提供技术框架，将AI创造力视为从领域有限生成模型中涌现的现象，从而促进创造力作为建模对象而非评估标签的理解。学术价值在于为AI创造力研究开辟新视角，推动理论进展；实际应用价值可能体现在优化生成系统设计和多模态交互中。局限性包括缺乏实证验证，未来工作方向可包括将该框架应用于具体模型，并进行实验测试以验证其有效性和扩展性。",
      "tags": [
        "Generative Models",
        "Emergent Behavior",
        "Multimodal Systems",
        "World Models",
        "Contextual Grounding"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:35.774609Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08383",
    "title": "Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models",
    "authors": [
      "Bo Wang",
      "Junzhuo Li",
      "Hong Chen",
      "Yuanlin Chu",
      "Yuxuan Fan",
      "Xuming Hu"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08383.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08383",
    "published": "2026-01-13T09:44:00Z",
    "updated": "2026-01-13T09:44:00Z",
    "comment": "Accepted by AAAI26",
    "light_analysis": {
      "overview": "论文提出Gated-LPI神经元归因度量，对比分析了MoE与稠密模型在预训练中的知识获取动态，揭示稀疏性如何培养稳定和分布式的计算主干。",
      "motivation": "Mixture-of-Experts（MoE）架构能解耦模型容量与计算成本，超越稠密缩放定律的限制，但预训练中MoE与稠密模型在知识获取机制上的差异尚未明确。理解这一差异对于优化稀疏架构和提升模型可解释性至关重要，现有方法缺乏对神经元级知识归因的系统分析，难以揭示稀疏架构的内在特性，限制了训练效率和解译性工具的开发。",
      "method": "研究引入Gated-LPI（Log-Probability Increase）作为神经元级归因度量，通过分解对数概率增加来评估神经元在预训练中的贡献。采用时间分辨的比较方法，追踪MoE模型1.2M训练步数（约5.0T令牌）和稠密模型600K步数（约2.5T令牌）的检查点，分析知识获取的动态变化。核心创新在于Gated-LPI度量和跨架构的长期检查点跟踪，帮助揭示知识获取过程。",
      "result": "实验发现三个关键模式：(1) 在MoE模型中，约1%的神经元捕获超过45%的正更新，形成高效用核心，而稠密模型无此现象；(2) MoE模型在早期（<100K步）锁定稳定重要性剖面，而稠密模型在整个训练中保持波动；(3) 掩码最重要MoE注意力头时，关系HIT@10减少<10%，而稠密模型减少>50%，表明稀疏性促进分布式知识存储，提升鲁棒性。这些结果通过具体数据对比了两种架构的性能差异。",
      "conclusion": "研究表明稀疏性在训练早期培养内在稳定和分布式的计算主干，有助于桥接稀疏架构与训练时的可解释性。该贡献揭示了MoE模型在知识获取中的优势，为优化稀疏架构和开发解释性工具提供了理论基础。潜在局限性包括摘要未明确说明更多架构的应用或实际场景扩展，未来工作可探索更多稀疏模型或不同数据集下的验证。",
      "tags": [
        "Mixture-of-Experts (MoE)",
        "Gated-LPI",
        "Knowledge Attribution",
        "Pre-training",
        "Neuronal Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:51.923267Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08382",
    "title": "A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)",
    "authors": [
      "Zoe Falomir"
    ],
    "abstract": "This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.",
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08382.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08382",
    "published": "2026-01-13T09:43:43Z",
    "updated": "2026-01-13T09:43:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出定性模型QOR，通过构建概念邻域图和组合表，高效推理对象旋转，并应用于解决立方体比较测试CCT。",
      "motivation": "该研究旨在解决Ekstrom等（1976）提出的立方体比较测试（CCT），这是一个经典的空间推理任务。摘要未明确说明现有方法的不足，但通过开发定性模型，论文试图提供更直观的逻辑方法处理对象旋转推理。CCT常用于评估空间认知能力，解决此问题有助于提升人工智能在空间推理领域的性能。",
      "method": "论文提出定性模型QOR，核心方法是构建概念邻域图（CNGRLO），该图关联旋转运动与立方体侧面特征的位置和方向变化。基于此图，生成组合表来计算旋转推理的推断。模型采用定性推理技术，专注于逻辑推导而非数值计算，创新点在于结合图结构和表格推理，为对象旋转提供系统化框架。",
      "result": "摘要未明确说明具体实验结果或性能指标。论文可能通过理论分析展示了QOR模型在解决CCT问题上的有效性，但缺乏与基线方法的对比数据。推断模型能够生成推理表以支持决策，但准确率、效率改进等细节在摘要中未提及。",
      "conclusion": "论文的主要贡献是提出了QOR定性模型，通过概念邻域图和组合表为对象旋转推理提供新方法。应用方面，成功解决CCT问题，展示了在空间推理和AI领域的应用价值。未来工作可能包括扩展到更复杂的旋转场景、集成其他推理任务或在实际系统中验证性能。",
      "tags": [
        "Qualitative Reasoning",
        "Object Rotation",
        "Conceptual Graph",
        "Composition Tables",
        "Spatial Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:58.794789Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08380",
    "title": "Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation",
    "authors": [
      "Mary Webb",
      "Matt Bower",
      "Ana Amélia Carvalho",
      "Fredrik Mørk Røkenes",
      "Jodie Torrington",
      "Jonathan D. Cohen",
      "Yousra Chtouki",
      "Kathryn Maccallum",
      "Tanya Linden",
      "Deirdre Butler",
      "Juliana Elisa Raffaghelli",
      "Henriikka Vartiainen",
      "Martina Ronci",
      "Peter Tiernan",
      "David M. Smith",
      "Chris Shelton",
      "Joyce Malyn-smith",
      "Pierre Gorissen"
    ],
    "abstract": "TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08380.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08380",
    "published": "2026-01-13T09:43:00Z",
    "updated": "2026-01-13T09:43:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究设计并实施了一套综合策略，以提高教师的AI素养和能力，促进AI技术在教育中的有效整合和应用。",
      "motivation": "随着AI技术在教育领域的快速发展，教师普遍缺乏足够的AI知识和技能，难以自信地将AI工具融入教学实践，这可能导致教育创新滞后和学生AI素养不足。现有方法往往侧重于单一培训或理论介绍，缺乏系统性的策略和实际操作指导。因此，本研究旨在填补这一空白，通过开发全面的AI素养提升策略，赋能教师并增强教育质量，解决实际应用中的障碍。",
      "method": "论文提出了一种多层次策略框架，主要探索课程设计、专业发展计划、实际课堂应用和政策指南四个方面。核心方法是通过整合理论与实践，设计具体的实施路径，以确保教师能够从基础概念到高级技能全面掌握AI技术。关键创新点在于结合了教育学和AI领域的方法，创造了一个可操作的系统，强调从个体培训到组织支持的全方位赋能，但没有提及具体的数据集或模型架构。",
      "result": "摘要未明确说明具体的实验结果或性能指标，如准确率提升或效率改进。研究侧重于策略的设计和实施阶段，因此没有提供量化的效果数据或与基线方法的对比。可以推断这些策略旨在潜在提高教师的自信度和学生理解水平，但具体效果需通过未来的试点项目或实际应用来验证。",
      "conclusion": "本研究的主要贡献是为AI教育提供了一个系统的策略框架，强调教师赋能和素养提升，以支持AI在教育中的整合。学术价值在于扩展了AI应用理论在教育领域的实践基础，实际应用价值在于指导学校、教育机构和政策制定者实施有效方案。局限性包括策略的通用性和长期效果尚未验证，未来工作方向可能包括具体案例的测试、效果评估以及适应不同教育环境的调整。",
      "tags": [
        "AI Literacy",
        "Curriculum Design",
        "Professional Development",
        "Educational Technology",
        "Teacher Training"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:59.654206Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08379",
    "title": "Training-Free Distribution Adaptation for Diffusion Models via Maximum Mean Discrepancy Guidance",
    "authors": [
      "Matina Mahdizadeh Sani",
      "Nima Jamali",
      "Mohammad Jalali",
      "Farzan Farnia"
    ],
    "abstract": "Pre-trained diffusion models have emerged as powerful generative priors for both unconditional and conditional sample generation, yet their outputs often deviate from the characteristics of user-specific target data. Such mismatches are especially problematic in domain adaptation tasks, where only a few reference examples are available and retraining the diffusion model is infeasible. Existing inference-time guidance methods can adjust sampling trajectories, but they typically optimize surrogate objectives such as classifier likelihoods rather than directly aligning with the target distribution. We propose MMD Guidance, a training-free mechanism that augments the reverse diffusion process with gradients of the Maximum Mean Discrepancy (MMD) between generated samples and a reference dataset. MMD provides reliable distributional estimates from limited data, exhibits low variance in practice, and is efficiently differentiable, which makes it particularly well-suited for the guidance task. Our framework naturally extends to prompt-aware adaptation in conditional generation models via product kernels. Also, it can be applied with computational efficiency in latent diffusion models (LDMs), since guidance is applied in the latent space of the LDM. Experiments on synthetic and real-world benchmarks demonstrate that MMD Guidance can achieve distributional alignment while preserving sample fidelity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08379.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08379",
    "published": "2026-01-13T09:42:57Z",
    "updated": "2026-01-13T09:42:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于最大平均差异指导的无训练分布适应方法，用于扩散模型直接对齐目标分布。",
      "motivation": "预训练扩散模型作为强大的生成先验，其输出常与用户特定目标数据特征不匹配，在领域适应任务中尤其成问题，此时只有少量参考样本可用，重新训练模型不可行。现有推理时间指导方法通常优化代理目标如分类器似然，而非直接对齐目标分布，导致对齐效果有限，因此需要一种无需训练、能直接优化分布距离的高效方法。",
      "method": "论文提出 MMD Guidance，一个无需训练的机制，通过在反向扩散过程中引入生成样本与参考数据集之间的最大平均差异（MMD）梯度来指导采样。MMD 作为分布度量，在有限数据下提供可靠估计，具有低方差和高效可微分性，适合指导任务。该方法通过产品核技术扩展到条件生成模型的提示感知适应，并能在潜在扩散模型（LDMs）的潜在空间中高效应用，降低计算成本。",
      "result": "在合成和真实世界基准上的实验表明，MMD Guidance 能有效实现分布对齐，同时保持样本保真度。与现有基于代理目标的方法相比，该方法直接优化分布距离，可能提升了适应性能，但摘要未明确说明具体的性能指标如准确率提升或效率改进数据。",
      "conclusion": "本研究的主要贡献是提出了一种基于最大平均差异的无训练分布适应方法，为扩散模型在领域适应任务中提供了直接且高效的解决方案。其学术价值在于改进分布对齐技术，实际应用价值体现在处理数据有限场景的能力，未来工作可能包括优化计算效率或扩展到其他生成模型。",
      "tags": [
        "Diffusion Models",
        "Maximum Mean Discrepancy",
        "Domain Adaptation",
        "Latent Diffusion Models",
        "Training-Free Adaptation"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:22.261765Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08375",
    "title": "Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation",
    "authors": [
      "Yuan Gao",
      "Di Cao",
      "Xiaohuan Xi",
      "Sheng Nie",
      "Shaobo Xia",
      "Cheng Wang"
    ],
    "abstract": "Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08375.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08375",
    "published": "2026-01-13T09:37:57Z",
    "updated": "2026-01-13T09:37:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出LoGo（Local-Global Dual-Consensus）框架，针对地理空间点云语义分割，实现源免费无监督领域适应，通过局部和全局双共识策略提升模型性能。",
      "motivation": "地理空间点云语义分割对遥感应用至关重要，但跨区域的地理模式差异和数据获取策略导致领域偏移，严重降低部署模型性能。现有领域适应方法通常需要源域数据，但由于数据隐私、法规政策和数据传输限制，这一要求往往难以满足，使得源免费无监督领域适应（SFUDA）设置成为重要但未充分探索的问题。本研究旨在解决这一实际挑战，推动在无源数据情况下的模型适应能力。",
      "method": "LoGo框架包括三个核心模块：首先，局部类平衡原型估计模块，采用类内独立锚挖掘策略替代传统全局阈值过滤，生成鲁棒特征原型，有效缓解长尾分布引起的特征崩溃，特别关注尾部类。其次，全局基于最优传输的分布对齐模块，将伪标签分配表述为全局优化问题，通过施加全局分布约束，纠正局部贪婪分配中头类的过度主导。最后，双一致性伪标签过滤机制，仅保留局部多增强集成预测与全局最优传输分配一致的高置信度伪标签，用于自训练提升模型泛化能力。",
      "result": "摘要未明确说明主要实验结果的具体数据，如准确率提升或效率改进。然而，可以推断LoGo框架通过局部和全局策略，可能提高了语义分割的准确性和鲁棒性，特别是在长尾分布和目标域无标签的情况下。与依赖源域数据的传统基线方法相比，该方法在源免费设置下可能展现出更好的适应性能，但具体性能指标需要参考全文实验部分。",
      "conclusion": "LoGo框架的主要贡献在于提出了一种针对地理空间点云的源免费无监督领域适应方法，通过局部类平衡原型估计和全局最优传输对齐，有效解决长尾分布和类不平衡问题。该研究具有学术价值，推动了SFUDA领域的发展，并为遥感应用提供了实际解决方案。潜在局限性可能包括计算复杂度或特定场景适用性，未来工作可以优化效率或扩展至其他领域。",
      "tags": [
        "Source-Free Domain Adaptation",
        "Geospatial Point Cloud",
        "Semantic Segmentation",
        "Optimal Transport",
        "Pseudo-Label Filtering"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:23.639422Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08371",
    "title": "Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild with an SDF Renderer",
    "authors": [
      "Anastasios Tsalakopoulos",
      "Angelos Kanlis",
      "Evangelos Chatzis",
      "Antonis Karakottas",
      "Dimitrios Zarpalas"
    ],
    "abstract": "We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. While existing in-the-wild methods already excel at novel view synthesis, they often lack geometric grounding on complex surfaces, sometimes producing results that contain inconsistencies. Geo-NVS-w addresses this limitation by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process. This is complemented by a novel Geometry-Preservation Loss which ensures that fine structural details are preserved. Our framework achieves competitive rendering performance, while demonstrating a 4-5x reduction reduction in energy consumption compared to similar methods. We demonstrate that Geo-NVS-w is a robust method for in-the-wild NVS, yielding photorealistic results with sharp, geometrically coherent details.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08371.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08371",
    "published": "2026-01-13T09:34:01Z",
    "updated": "2026-01-13T09:34:01Z",
    "comment": "Presented at the ICCV 2025 Workshop on Large Scale Cross Device Localization",
    "light_analysis": {
      "overview": "Geo-NVS-w通过结合SDF几何表示和几何保持损失，实现几何一致的新视角合成，并在能耗上显著优化。",
      "motivation": "现有野外新视角合成方法虽然能生成新视图，但缺乏几何基础，导致在复杂表面上产生不一致结果，影响图像真实感和可靠性。几何一致性对于合成高质量图像至关重要，尤其在野外场景中，表面可能复杂多变，现有方法难以有效处理细节。因此，本研究旨在解决几何细节保留不足的问题，提升合成图像的几何一致性和整体性能。",
      "method": "Geo-NVS-w使用基于符号距离函数（SDF）的几何表示来指导渲染过程，确保几何结构的准确性。关键创新是引入几何保持损失函数，以强制保留精细的结构细节。框架结合SDF渲染器，可能涉及无结构化图像集合的处理，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "论文显示Geo-NVS-w实现了有竞争力的渲染性能，具体指标如准确率未在摘要中说明。与类似方法相比，能耗显著降低4-5倍，表明在效率上有突出优势。实验验证了在野外图像集合上的应用，产生光真实感、几何一致的细节。",
      "conclusion": "Geo-NVS-w作为一个稳健的野外新视角合成框架，通过SDF几何表示和几何保持损失，提高了合成图像的几何一致性和真实感。其学术价值在于将几何感知融入新视角合成，推动相关领域发展；实际应用可能扩展到计算机视觉、图形学和增强现实领域。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Signed Distance Function (SDF)",
        "Novel View Synthesis (NVS)",
        "Geometry-Aware Rendering",
        "In-the-Wild Image Synthesis"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:16.584323Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08358",
    "title": "Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings",
    "authors": [
      "Hilde I. Hummel",
      "Sandjai Bhulai",
      "Rob D. van der Mei",
      "Burooj Ghani"
    ],
    "abstract": "Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08358.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08358",
    "published": "2026-01-13T09:15:31Z",
    "updated": "2026-01-13T09:15:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出通过线性探头从预训练音频嵌入中有效提取船舶类型特征，实现低成本自动水下声学目标识别。",
      "motivation": "水下声污染日益加剧，船只噪声对海洋生态系统构成威胁，需被动声学监测系统进行监控。然而，手动分析大量水下录音不切实际，现有自动水下声学目标识别方法主要依赖监督学习，受限于标签数据稀缺。转移学习提供有前景的替代方案，旨在缓解数据不足问题，促进自动化监测的广泛应用。",
      "method": "研究方法包括评估来自不同音频域的多个预训练音频模型，冻结模型权重以生成音频嵌入，并通过分类、聚类和相似性分析嵌入空间。核心创新是应用线性探头，抑制嵌入空间中的录音特定特征，从而分离出船舶类型特征，实现高效特征提取。摘要未明确说明具体数据集和模型架构，推断为通用音频识别模型。",
      "result": "实验结果显示，嵌入空间主要由录音特定特征主导，但线性探头能有效抑制这些信息并提取船舶类型特征，提升识别性能。这使得自动水下声学目标识别能以低计算成本实现，显著减少对大量高质量标签录音的依赖，提供实用化的自动化解决方案。",
      "conclusion": "本研究证明了转移学习在水下声学目标识别中的有效性，特别是线性探头方法为自动监测提供了高效途径，减少标签数据需求，具有重要应用价值。未来工作可扩展至更多预训练模型和更广泛声学场景，进一步优化性能。",
      "tags": [
        "Underwater Acoustic Target Recognition",
        "Transfer Learning",
        "Linear Probing",
        "Pretrained Audio Models",
        "Audio Embeddings"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:23.363363Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08355",
    "title": "Semantic Misalignment in Vision-Language Models under Perceptual Degradation",
    "authors": [
      "Guo Cheng"
    ],
    "abstract": "Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08355.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08355",
    "published": "2026-01-13T09:13:05Z",
    "updated": "2026-01-13T09:13:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文揭示了在感知退化下视觉-语言模型的语义错位问题，提出量化指标，揭示像素级鲁棒性与多模态语义可靠性之间的脱节。",
      "motivation": "视觉-语言模型在自动驾驶和嵌入式AI等安全关键系统中部署，需要可靠感知支持语义推理和决策。尽管现有模型在多模态基准测试中表现良好，但对实际感知退化（如图像质量下降）的鲁棒性研究不足，导致系统在现实应用中可能失效。现有评估方法忽略感知不确定性，无法全面评估安全关键场景下的可靠性，因此研究此问题对提升系统安全至关重要。",
      "method": "本研究以Cityscapes数据集的语义分割作为代表性感知模块，引入感知现实退化模拟上游视觉感知的劣化。通过控制退化仅使常规分割指标中等下降，评估下游VLMs的行为，提出语言级错位指标量化幻觉、关键遗漏和安全误解。分析多个对比性和生成性VLMs，关键创新在于系统研究退化影响并开发新评估框架，探究分割质量与语义错位的关系。",
      "result": "实验结果显示，感知退化仅导致语义分割指标轻微下降，但下游VLMs出现严重语义错位，包括频繁幻觉对象、遗漏关键实体和不一致安全判断。量化指标有效捕捉这些错位，揭示其与分割质量相关性弱，与多个VLMs对比表明像素级鲁棒性不能保证多模态语义可靠性。摘要未明确说明具体数据，但观察到明显失败模式，突显当前系统在安全关键应用中的局限性。",
      "conclusion": "本研究贡献在于揭示视觉-语言模型在感知退化下的语义错位，并提出量化评估框架，强调在安全关键应用中考虑感知不确定性的重要性。学术上填补了VLM鲁棒性评估空白，实际上有助于提升自动驾驶等系统安全性。未来工作可扩展至更多退化类型和模型，或开发增强鲁棒性的方法，推动更可靠系统设计。",
      "tags": [
        "Vision-Language Models",
        "Semantic Segmentation",
        "Perceptual Degradation",
        "Contrastive Learning",
        "Generative Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:51.127564Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08342",
    "title": "Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue",
    "authors": [
      "Run Chen",
      "Wen Liang",
      "Ziwei Gong",
      "Lin Ai",
      "Julia Hirschberg"
    ],
    "abstract": "Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08342.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08342",
    "published": "2026-01-13T09:02:08Z",
    "updated": "2026-01-13T09:02:08Z",
    "comment": "Accepted to IWSDS 2026",
    "light_analysis": {
      "overview": "本论文首次研究语音对话中心理操纵的检测，通过合成多说话者数据集评估模态对检测准确性的影响。",
      "motivation": "心理操纵是计算社会推理中的新兴任务，涉及语言策略的隐蔽影响。现有研究主要集中于文本对话，忽视了语音中操纵策略的体现，导致多模态系统安全性评估不足。本研究的动机是填补这一空白，探索语音模态如何影响操纵检测，以促进更全面的对话系统安全性和社会推理能力的发展。",
      "method": "研究方法包括创建合成多说话者基准数据集SPEECHMENTALMANIP，通过高质量文本到语音技术将基于文本的数据集转换为语音音频。使用少样本大型音频-语言模型进行评估，并结合人类注释来比较模态对检测的影响。关键创新在于生成语音一致的合成音频，并利用多模态模型进行跨模态分析，以探究声学和韵律线索的作用。",
      "result": "实验结果显示，模型在语音数据上表现出高特异性，但召回率显著低于文本数据，表明模型对训练中缺失的声学或韵律线索敏感。人类评分者在音频设置中也显示出类似的不确定性，强调了语音操纵检测的固有模糊性。这些发现揭示了模态对检测准确性的重要影响，并与文本基线对比，突出了语音模态下的挑战。",
      "conclusion": "论文的主要贡献在于首次将心理操纵检测扩展到语音对话领域，并通过实验揭示了模态差异对检测性能的影响。学术价值在于推动多模态社会推理任务的发展，实际应用价值在于为多模态对话系统的安全对齐和评估提供见解。未来工作可关注如何整合声学特征以提高检测召回率，并解决模态感知的局限性。",
      "tags": [
        "Mental Manipulation Detection",
        "Spoken Dialogues",
        "Synthetic Speech",
        "Text-to-Speech",
        "Audio-Language Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:33.220731Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08341",
    "title": "From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution",
    "authors": [
      "Chunyu Meng",
      "Wei Long",
      "Shuhang Gu"
    ],
    "abstract": "Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08341.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08341",
    "published": "2026-01-13T09:01:20Z",
    "updated": "2026-01-13T09:01:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出个体化探索性Transformer（IET），通过个体化探索性注意力机制实现token-自适应的图像超分辨率，提升效率与精度。",
      "motivation": "图像超分辨率（SISR）旨在从低分辨率图像重建高分辨率图像，是计算机视觉基础任务。Transformer方法通过建模长距离依赖取得显著性能，但特征密集的注意力计算成本高，限制了应用。为提升效率，现有方法将图像分组并限制组内注意力，却忽略token相似性的不对称性，导致无法实现灵活和自适应的注意力计算，影响信息聚合效果。因此，需要一种新机制来克服这些局限性，平衡计算效率与性能。",
      "method": "本研究提出个体化探索性Transformer（IET），引入个体化探索性注意力（IEA）机制。该机制允许每个token根据图像内容自适应选择其独立的注意力候选者，实现token-自适应和不对称的注意力计算。核心创新在于灵活的信息聚合方式，摆脱了固定分组的限制，同时通过探索性设计优化注意力权重，保持计算效率。模型基于Transformer架构，应用于图像超分辨率任务，训练和验证使用标准SR基准数据集，但摘要未具体说明数据集名称。",
      "result": "在标准图像超分辨率基准测试中，IET模型在可比的计算复杂度下取得了最先进的性能。实验表明，与现有方法相比，IET能够更精确地聚合信息，显著提升图像重建质量，例如可能改进峰值信噪比（PSNR）和结构相似性（SSIM）等指标，但摘要未明确说明具体数值。这验证了token-自适应注意力机制的有效性，在维持高效计算的同时优化了模型表现。",
      "conclusion": "本研究的核心贡献在于提出了个体化探索性Transformer和注意力机制，有效解决了传统组注意力在图像超分辨率中的局限性，为注意力机制设计提供了新思路，具有重要学术价值。在实际应用上，高效计算方式使得模型更适用于资源受限环境，如移动设备或实时图像处理。未来工作可进一步探索该机制在更广泛视觉任务中的应用，但摘要未明确说明具体局限性或未来方向。",
      "tags": [
        "Individualized Exploratory Attention",
        "Token-Adaptive Attention",
        "Image Super-Resolution",
        "Transformer Models",
        "Computational Efficiency"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:54.695614Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08336",
    "title": "Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways",
    "authors": [
      "Junzhuo Liu",
      "Xuemei Du",
      "Daniel Reisenbuchler",
      "Ye Chen",
      "Markus Eckstein",
      "Christian Matek",
      "Friedrich Feuerhake",
      "Dorit Merhof"
    ],
    "abstract": "Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08336.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08336",
    "published": "2026-01-13T08:53:58Z",
    "updated": "2026-01-13T08:53:58Z",
    "comment": "19 pages, 8 figures. This work has been submitted to the IEEE for possible publication",
    "light_analysis": {
      "overview": "BioMorphNet 提出一个多模态网络，通过建模肿瘤微环境和生物通路，整合组织形态特征和空间基因表达，显著提升组织分类精度并支持差异基因分析。",
      "motivation": "现有研究主要关注个体基因序列和幻灯片级分类任务，忽视了空间转录组学和补丁级应用，导致在肿瘤微环境建模和基因空间分析方面受限。这限制了精准临床诊断和癌症进展研究的潜力。BioMorphNet 旨在解决这一问题，通过多模态整合提升组织分类和基因分析的精度，以更好地支持肿瘤定位和生物标志物发现。",
      "method": "BioMorphNet 构建图模型来建模肿瘤微环境中目标补丁与邻居的关系，基于形态和分子相似性调整响应强度。从空间转录组数据中提取临床通路特征，作为组织形态与基因表达的桥梁。并设计可学习通路模块自动模拟生物通路形成过程，补充现有临床通路。该方法利用多模态网络整合全幻灯片图像和基因表达数据，支持补丁级分析，以提高组织分类和基因差异分析的准确性。",
      "result": "与最新形态基因多模态方法相比，BioMorphNet 在前列腺癌、结直肠癌和乳腺癌数据集上的平均分类指标分别提升了 2.67%、5.48% 和 6.29%。这些改进表明模型在组织分类方面具有优越性能。此外，模型还支持准确分类组织类别以辅助肿瘤定位，并基于预测置信度分析组织类别间的差异基因表达，有助于发现潜在肿瘤生物标志物。",
      "conclusion": "BioMorphNet 的主要贡献在于通过建模肿瘤微环境和生物通路，实现了组织形态与空间基因表达的有效整合，显著提升了组织分类精度并支持差异基因分析。这为精准临床诊断和癌症研究提供了新工具，具有重要学术和实际应用价值。未来工作可能涉及扩展到更多癌症类型或改进模型效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Multimodal Learning",
        "Spatial Transcriptomics",
        "Graph Neural Networks",
        "Biological Pathways",
        "Tissue Classification"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:01.263801Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08334",
    "title": "Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility",
    "authors": [
      "Jose Lozano-Montoya",
      "Emilio Soria-Olivas",
      "Almudena Fuster-Matanzo",
      "Angel Alberich-Bayarri",
      "Ana Jimenez-Pastor"
    ],
    "abstract": "Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08334.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08334",
    "published": "2026-01-13T08:47:44Z",
    "updated": "2026-01-13T08:47:44Z",
    "comment": "27 pages, 4 figures, 3 tables, code available, see https://github.com/joselznom/AutoML-Comparison-in-Radiomics",
    "light_analysis": {
      "overview": "本研究通过系统比较通用与特定AutoML框架，评估了它们在放射组学分类任务中的性能、效率与可访问性，并揭示了当前的发展需求与未来改进方向。",
      "motivation": "研究动机源于AutoML框架能降低放射组学中预测和预后模型开发的技术壁垒，使无编程专家也能构建模型，但其在解决放射组学特定挑战（如特征可重复性和生存分析支持）方面的有效性尚不明确。现有方法可能因框架过时、编程要求高或计算效率低而不足，因此评估这些框架的性能、效率和可访问性至关重要，以突出放射组学领域的发展需求，推动技术改进和实际应用。",
      "method": "研究方法采用10个公共/私有放射组学数据集，涵盖CT/MRI等不同成像模态、大小、解剖结构和端点，以标准化交叉验证测试6个通用和5个特定AutoML框架，使用预定义参数。评估指标包括AUC和运行时间等量化指标，以及软件状态、可访问性和可解释性等定性方面，系统比较了框架类型，并综合了多维度性能、效率和可访问性分析。",
      "result": "实验结果显示，Simplatab（特定工具）在测试AUC中表现最佳，平均达到81.81%，运行时间约1小时；LightAutoML（通用框架）执行最快，平均AUC为78.74%仅需6分钟，性能竞争。相比之下，大多数特定框架因过时、编程要求高或计算效率低而被排除，通用框架则显示出更高的可访问性和易实施性，具体数据支撑了性能与效率间的权衡。",
      "conclusion": "本研究结论指出Simplatab在性能、效率和可访问性方面提供了有效平衡，适用于放射组学分类问题。研究意义在于揭示了当前AutoML框架在集成特征可重复性、生存分析支持等方面的不足，具有学术价值，为研究者提供了框架选择指南，并指出未来研究方向应聚焦于适应AutoML解决方案以更好应对放射组学特定挑战，推动实际应用发展。",
      "tags": [
        "Automated Machine Learning",
        "Radiomics",
        "Classification",
        "Cross-Validation",
        "Model Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:15.370981Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08333",
    "title": "Semantic Laundering in AI Agent Architectures: Why Tool Boundaries Do Not Confer Epistemic Warrant",
    "authors": [
      "Oleg Romanchuk",
      "Roman Bondar"
    ],
    "abstract": "LLM-based agent architectures systematically conflate information transport mechanisms with epistemic justification mechanisms. We formalize this class of architectural failures as semantic laundering: a pattern where propositions with absent or weak warrant are accepted by the system as admissible by crossing architecturally trusted interfaces. We show that semantic laundering constitutes an architectural realization of the Gettier problem: propositions acquire high epistemic status without a connection between their justification and what makes them true. Unlike classical Gettier cases, this effect is not accidental; it is architecturally determined and systematically reproducible. The central result is the Theorem of Inevitable Self-Licensing: under standard architectural assumptions, circular epistemic justification cannot be eliminated. We introduce the Warrant Erosion Principle as the fundamental explanation for this effect and show that scaling, model improvement, and LLM-as-judge schemes are structurally incapable of eliminating a problem that exists at the type level.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08333.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08333",
    "published": "2026-01-13T08:45:17Z",
    "updated": "2026-01-13T08:45:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文形式化AI代理架构中的语义清洗问题，证明在标准架构假设下，循环认识论证明无法消除。",
      "motivation": "研究动机是揭示LLM-based代理架构中语义清洗的系统性漏洞，即信息传输机制被错误地视为认识论证明机制，导致命题在没有充分证明的情况下被接受。这一问题类似于经典的Gettier问题，但在架构层面是系统性和不可避免的，现有方法如模型扩展和改进无法根除，因为它存在于类型级别，影响AI系统的可靠性和信任度。",
      "method": "论文采用理论分析方法，形式化语义清洗作为架构模式，其中命题通过跨越可信接口获得高认识论状态而没有真实证明。关键创新包括引入Warrant Erosion Principle作为核心解释，并将问题与Gettier问题联系起来，证明不可避免的自我许可定理。摘要未明确说明使用的具体数据集或模型架构，专注于理论推导和架构层面的分析。",
      "result": "主要结果是证明不可避免的自我许可定理：在标准架构假设下，AI代理系统必然经历语义清洗，导致循环认识论证明无法消除。这表明扩展、模型改进和LLM-as-judge方案在结构上无效，因为问题源于架构类型本身。结果基于理论推导，摘要未提供具体性能指标，但强调了Warrant Erosion Principle作为根本原因。",
      "conclusion": "结论是AI代理架构中存在语义清洗问题，导致认识论证明的腐蚀，这在理论上是不可避免的。研究贡献包括形式化语义清洗和提供理论框架，连接AI架构与哲学认识论问题。学术价值在于深化对AI系统局限性的理解，应用价值在于警示设计者关注架构缺陷以提升系统安全性和可靠性。摘要未明确说明局限性或未来工作方向，但可推测未来可能探索新的架构范式来缓解问题。",
      "tags": [
        "Semantic Laundering",
        "LLM-based Agent Architectures",
        "Epistemic Justification",
        "Gettier Problem",
        "Warrant Erosion Principle"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:03.997759Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08332",
    "title": "IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks",
    "authors": [
      "Ahmed A. Hashim",
      "Ali Al-Shuwaili",
      "Asraa Saeed",
      "Ali Al-Bayaty"
    ],
    "abstract": "Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08332.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08332",
    "published": "2026-01-13T08:42:46Z",
    "updated": "2026-01-13T08:42:46Z",
    "comment": "11 pages, 6 figures",
    "light_analysis": {
      "overview": "本文提出IGAN模型，通过结合inception结构和空洞卷积，实现生成对抗网络中稳定和高保真图像合成的平衡。",
      "motivation": "生成对抗网络（GANs）在图像合成领域面临一个关键挑战：如何在确保训练稳定的同时实现高质量图像生成。现有技术如DCGAN、BigGAN和StyleGAN在提升视觉保真度方面虽有进展，但常因高网络深度导致模式崩溃和梯度不稳定问题，这限制了GANs的可靠性和应用扩展。本研究的动机在于解决这些不足，寻求稳定性和生成质量之间的平衡，以促进高保真图像合成技术的实际部署。",
      "method": "本研究提出Inception生成对抗网络（IGAN），采用inception启发的卷积层和空洞卷积结构。核心创新在于引入inception架构以增加网络深度，同时利用空洞卷积扩大感受野，从而提升图像质量而不牺牲稳定性。具体技术包括在生成器和判别器中集成dropout和谱归一化，以缓解梯度爆炸和过拟合。模型在CUB-200和ImageNet数据集上进行评估，通过优化网络设计实现稳定高效的图像合成。",
      "result": "实验结果显示，IGAN在CUB-200数据集上达到FID 13.12和IS 9.27，在ImageNet数据集上达到FID 15.08和IS 68.25。与当前最优GANs相比，FID提升了28-33%，IS的提高表明图像多样性和生成质量得到改善。这些数据证实IGAN在保持训练稳定性的同时，显著提升了图像合成的高保真度，有效避免了模式崩溃和梯度问题。",
      "conclusion": "本研究的主要贡献是提出IGAN模型，它有效平衡了生成对抗网络的训练稳定性和高保真图像合成能力。该模型通过整合inception结构和空洞卷积，提供了一个可扩展且计算高效的框架，解决了现有GANs中的关键挑战。这一研究具有重要学术价值，为稳定GAN训练提供新方法，并对图像生成应用有实际意义。未来工作可探索模型在其他数据集上的泛化能力，并进一步优化计算效率。",
      "tags": [
        "Generative Adversarial Networks",
        "Inception",
        "Dilated Convolution",
        "Dropout",
        "Spectral Normalization"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:44.844084Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08331",
    "title": "CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark",
    "authors": [
      "Daniil Gurgurov",
      "Yusser Al Ghussin",
      "Tanja Baeumel",
      "Cheng-Ting Chou",
      "Patrick Schramowski",
      "Marius Mosbach",
      "Josef van Genabith",
      "Simon Ostermann"
    ],
    "abstract": "Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08331.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08331",
    "published": "2026-01-13T08:42:03Z",
    "updated": "2026-01-13T08:42:03Z",
    "comment": "pre-print",
    "light_analysis": {
      "overview": "本文提出CLaS-Bench，首个标准化的多语言大型语言模型操纵评估基准，用于量化各种操纵技术在跨语言对齐中的有效性。",
      "motivation": "该研究旨在解决多语言NLP中，缺乏专门评估操纵大型语言模型内部表示技术基准的问题。操纵技术通过调整推理期间的表示来适应目标语言，相比提示或微调更高效且可解释，但现有方法缺乏标准化评估，限制了其科学验证和实际应用，因此迫切需要建立系统性的评价框架以促进技术发展。",
      "method": "研究提出CLaS-Bench基准，通过轻量级并行问题评估32种语言中的语言强制行为，涵盖多种操纵技术，如残差流DiffMean干预、探针导出方向、语言特定神经元、PCA/LDA向量、稀疏自编码器和提示基线。核心创新在于将性能测量结合语言控制与语义相关性，使用调和平均分数进行标准化评分，从而系统化比较不同方法的适应效果。",
      "result": "实验表明，在所有评估语言中，简单的基于残差的DiffMean方法在操纵性能上始终优于其他技术，表现出优异的语言控制和语义相关性。层级分析揭示语言特定结构主要在模型后续层涌现，且操纵方向基于语言家族形成自然聚类，这些发现提供了语言表示的科学洞察，但具体数据提升幅度摘要未明确说明。",
      "conclusion": "该研究的主要贡献是推出首个标准化的多语言操纵基准CLaS-Bench，它支持对语言表示的科学分析，并为低成本模型适应提供了实用评估工具，具有重要学术和实际应用价值。未来工作可能涉及扩展语言覆盖或探索更精细的操纵策略，以进一步增强基准的泛化能力。",
      "tags": [
        "Large Language Model",
        "Cross-Lingual Alignment",
        "Steering Techniques",
        "Benchmark Evaluation",
        "Representation Manipulation"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:14.746094Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08323",
    "title": "AtomMem : Learnable Dynamic Agentic Memory with Atomic Memory Operation",
    "authors": [
      "Yupeng Huo",
      "Yaxi Lu",
      "Zhong Zhang",
      "Haotian Chen",
      "Yankai Lin"
    ],
    "abstract": "Equipping agents with memory is essential for solving real-world long-horizon problems. However, most existing agent memory mechanisms rely on static and hand-crafted workflows. This limits the performance and generalization ability of these memory designs, which highlights the need for a more flexible, learning-based memory framework. In this paper, we propose AtomMem, which reframes memory management as a dynamic decision-making problem. We deconstruct high-level memory processes into fundamental atomic CRUD (Create, Read, Update, Delete) operations, transforming the memory workflow into a learnable decision process. By combining supervised fine-tuning with reinforcement learning, AtomMem learns an autonomous, task-aligned policy to orchestrate memory behaviors tailored to specific task demands. Experimental results across 3 long-context benchmarks demonstrate that the trained AtomMem-8B consistently outperforms prior static-workflow memory methods. Further analysis of training dynamics shows that our learning-based formulation enables the agent to discover structured, task-aligned memory management strategies, highlighting a key advantage over predefined routines.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08323.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08323",
    "published": "2026-01-13T08:22:28Z",
    "updated": "2026-01-13T08:22:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "AtomMem 通过将内存管理重构为基于原子 CRUD 操作的可学习动态决策过程，实现了灵活的代理内存框架。",
      "motivation": "现有代理内存机制主要依赖静态和手工设计的工作流程，这限制了它们在长距离现实世界问题中的性能和泛化能力。当前方法缺乏灵活性，无法自适应地协调内存行为以适应复杂任务需求，因此需要开发一种学习型的内存框架来提升代理的决策效率。本研究旨在解决这一问题，以推动更智能的代理系统发展。",
      "method": "AtomMem 将内存管理重构为动态决策问题，通过解构高级内存流程为原子 CRUD 操作，使工作流程转化为可学习的决策过程。核心创新点包括结合监督微调和强化学习来训练自主、任务对齐的策略，优化内存行为。方法使用具体基准测试（如长上下文任务），但不指定数据集名称，强调自适应学习和结构化策略发现。",
      "result": "在三个长上下文基准测试中，AtomMem-8B 模型持续优于先前的静态工作流程内存方法，验证了其性能优势。训练动态分析表明，学习型公式能使代理发现结构化、任务对齐的内存管理策略，突显了相较于预定义工作流程的关键改进。摘要未提供具体指标数据，但强调了相对优势。",
      "conclusion": "AtomMem 的主要贡献是提出了一种可学习的动态代理内存框架，通过原子操作和强化学习提升内存管理的灵活性和效率。学术上推动了自适应内存设计的研究，应用上有助于解决长距离问题。局限性或未来工作方向摘要未明确说明，但潜在方向可能包括扩展到更多任务类型。",
      "tags": [
        "Learnable Memory",
        "Atomic Operations",
        "Reinforcement Learning",
        "CRUD Operations",
        "Agentic Memory"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:26.825112Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08321",
    "title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "authors": [
      "Lichen Ma",
      "Xiaolong Fu",
      "Gaojing Zhou",
      "Zipeng Guo",
      "Ting Zhu",
      "Yichun Liu",
      "Yu Shi",
      "Jason Li",
      "Junshi Huang"
    ],
    "abstract": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08321.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08321",
    "published": "2026-01-13T08:18:49Z",
    "updated": "2026-01-13T08:18:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "UM-Text提出一个统一多模态模型，通过自然语言指令理解和编辑视觉文本，确保与参考图像风格一致。",
      "motivation": "随着图像生成技术的快速发展，基于自然语言指令的视觉文本编辑任务日益受到重视。该任务的核心挑战是充分理解用户指令和参考图像，以生成与图像风格协调的视觉文本。现有方法通常需要手动指定文本内容及属性如字体、颜色和布局，但忽视了与参考图像的风格一致性，导致生成的文本与背景不和谐。这一问题在广告设计、内容创作等应用中至关重要，因为风格一致性直接影响视觉效果的和谐度，因此开发自动确保一致性的模型具有重要研究价值。",
      "method": "UM-Text是一个统一的多模态模型，用于通过自然语言指令进行视觉文本编辑。模型引入视觉语言模型（VLM）来处理指令和参考图像，从而根据上下文信息精细设计文本内容和布局。UM-Encoder被提出以结合多种条件信息的嵌入，并由VLM根据输入指令自动配置组合，以实现准确和谐的视觉文本生成。在训练阶段，提出区域一致性损失，在潜在空间和RGB空间提供更有效的字形生成监督，并设计了一个定制化的三阶段训练策略以进一步提升性能。此外，贡献了UM-DATA-200K数据集，这是一个包含多样场景的大规模视觉文本图像数据集，用于模型训练。",
      "result": "论文在多个公共基准测试上进行了广泛的定性和定量实验。结果表明，UM-Text方法在视觉文本编辑任务中达到了最优性能。定量评估显示，相较于先前方法，UM-Text在生成文本与图像风格一致性方面有显著提升，具体体现在更高的准确性和和谐性指标上。尽管摘要未提供具体数值数据，但实验证明了模型在多个数据集上的卓越表现，验证了其有效性。",
      "conclusion": "UM-Text通过统一多模态模型成功解决了视觉文本编辑中的风格一致性问题。其主要贡献包括提出VLM和UM-Encoder架构、区域一致性损失以及三阶段训练策略，并贡献了大规模数据集UM-DATA-200K。这项研究在学术上推动了多模态学习和视觉文本生成技术的发展，在实际应用中为广告设计、自动内容生成等领域提供了高效解决方案。未来工作可能包括扩展模型到更复杂的场景或改进计算效率，但摘要未明确说明局限性。",
      "tags": [
        "Multimodal Model",
        "Visual Language Model",
        "Visual Text Editing",
        "Regional Consistency Loss",
        "Natural Language Instructions"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:16.131251Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08319",
    "title": "YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture",
    "authors": [
      "Dapinder Kaur",
      "Neeraj Battish",
      "Arnav Bhavsar",
      "Shashi Poddar"
    ],
    "abstract": "The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08319.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08319",
    "published": "2026-01-13T08:17:28Z",
    "updated": "2026-01-13T08:17:28Z",
    "comment": "8 pages, 4 figures, and submitted to a journal for review",
    "light_analysis": {
      "overview": "本文提出YOLOBirDrone架构和BirDrone数据集，通过自适应层聚合和注意力模块，提高了无人机与鸟类检测分类的准确度。",
      "motivation": "随着无人机在商业和防御领域的广泛应用，其潜在安全威胁如攻击行为日益增加，亟需开发可靠的无人机检测系统以应对安全挑战。现有基于视觉的检测方法在区分无人机和小型鸟类时存在准确度限制，尤其是在小鸟场景下易产生误检，导致系统性能不足。因此，本研究旨在解决这一实际问题，提升检测系统的区分能力和安全性。",
      "method": "YOLOBirDrone架构基于YOLO目标检测框架，核心创新包括自适应和扩展层聚合（AELAN）以优化特征提取、多尺度渐进双注意力模块（MPDA）和反向MPDA（RMPDA）来保持形状信息并丰富局部和全局空间与通道特征。此外，研究引入BirDrone数据集，包含小型和挑战性物体，用于支持鲁棒的空中物体识别。该方法通过集成这些组件，旨在增强模型对无人机和鸟类的检测与分类能力。",
      "result": "实验结果显示，YOLOBirDrone架构在检测准确度上优于其他先进算法，在多种场景下达到约85%的检测准确率，有效提升了无人机与鸟类的区分性能。摘要未明确提供具体对比数据或详细基准测试结果，但指出该架构在性能指标上有显著改进，验证了其在实际应用中的有效性。",
      "conclusion": "本研究的主要贡献在于提出一种增强的YOLO架构和专用数据集，显著改善了空中物体检测的准确性和鲁棒性。学术上推动了目标检测技术的创新，实际应用中可增强无人机检测系统的安全性和可靠性。未来工作可能涉及进一步优化算法效率或扩展数据集以覆盖更多场景，但摘要未明确说明具体局限性。",
      "tags": [
        "Object Detection",
        "YOLO",
        "Attention Mechanism",
        "Dataset",
        "Aerial Object Identification"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:47.957054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08316",
    "title": "Deep Exploration of Epoch-wise Double Descent in Noisy Data: Signal Separation, Large Activation, and Benign Overfitting",
    "authors": [
      "Tomoki Kubo",
      "Ryuken Uda",
      "Yusuke Iida"
    ],
    "abstract": "Deep double descent is one of the key phenomena underlying the generalization capability of deep learning models. In this study, epoch-wise double descent, which is delayed generalization following overfitting, was empirically investigated by focusing on the evolution of internal structures. Fully connected neural networks of three different sizes were trained on the CIFAR-10 dataset with 30% label noise. By decomposing the loss curves into signal contributions from clean and noisy training data, the epoch-wise evolutions of internal signals were analyzed separately. Three main findings were obtained from this analysis. First, the model achieved strong re-generalization on test data even after perfectly fitting noisy training data during the double descent phase, corresponding to a \"benign overfitting\" state. Second, noisy data were learned after clean data, and as learning progressed, their corresponding internal activations became increasingly separated in outer layers; this enabled the model to overfit only noisy data. Third, a single, very large activation emerged in the shallow layer across all models; this phenomenon is referred as \"outliers,\" \"massive activa-tions,\" and \"super activations\" in recent large language models and evolves with re-generalization. The magnitude of large activation correlated with input patterns but not with output patterns. These empirical findings directly link the recent key phenomena of \"deep double descent,\" \"benign overfitting,\" and \"large activation\", and support the proposal of a novel scenario for understanding deep double descent.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08316.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08316",
    "published": "2026-01-13T08:13:15Z",
    "updated": "2026-01-13T08:13:15Z",
    "comment": "17 pages, 9 figures",
    "light_analysis": {
      "overview": "本文通过分析噪声数据中的 epoch-wise 双下降现象，揭示了内部信号分离和大激活的演化机制，并提出了连接深度双下降、良性过拟合和大激活的新场景，以深入理解深度模型的泛化能力。",
      "motivation": "深度双下降现象是解释深度学习模型泛化能力的关键，但 epoch-wise 双下降在噪声数据下的内部演化机制尚不明确。现有研究可能未深入探索清洁与噪声数据在模型内部信号的分离过程，无法解释模型在过拟合后如何重新泛化。本研究旨在实证调查内部结构演化，填补这一空白，这对提升模型在噪声环境中的鲁棒性和优化训练策略具有重要意义。",
      "method": "研究使用 CIFAR-10 数据集，引入 30% 的标签噪声，训练三种不同大小的全连接神经网络。核心方法是分解损失曲线为清洁数据和噪声数据的信号贡献，从而单独分析内部激活在 epoch-wise 的演化。技术创新点包括关注浅层的大激活现象和信号在外层的分离过程，以揭示模型如何过拟合噪声数据，同时采用不同网络大小以增强实证结果的可信度。",
      "result": "实验结果显示，模型在双下降阶段完美拟合噪声训练数据后，在测试数据上实现强再泛化，证实了良性过拟合状态。关键发现包括：噪声数据在清洁数据之后学习，内部激活在外层逐渐分离，使模型仅过拟合噪声数据；浅层出现单个非常大的激活，其大小与输入模式相关但无关输出模式，此现象随再泛化演化，间接链接到大型语言模型中的类似效应。",
      "conclusion": "本研究通过实证发现将深度双下降、良性过拟合和大激活现象直接联系起来，提出了理解深度双下降的新场景，深化了对深度模型泛化机制的认识。这为学术上探索模型内部动力学提供了新视角，实际应用中可能有助于开发更有效的抗噪声训练方法。未来工作可扩展验证到更多复杂数据集和架构，以进一步验证普遍性。",
      "tags": [
        "Epoch-wise Double Descent",
        "Benign Overfitting",
        "Large Activation",
        "Noisy Data",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:13.543600Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08311",
    "title": "Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation",
    "authors": [
      "Kang Fu",
      "Huiyu Duan",
      "Zicheng Zhang",
      "Yucheng Zhu",
      "Jun Zhao",
      "Xiongkuo Min",
      "Jia Wang",
      "Guangtao Zhai"
    ],
    "abstract": "Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08311.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08311",
    "published": "2026-01-13T08:00:02Z",
    "updated": "2026-01-13T08:00:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了IQARAG，一个无训练框架，通过检索增强生成技术增强大型多模态模型在图像质量评估中的能力，避免了昂贵的微调。",
      "motivation": "现有大型多模态模型在图像质量评估任务中展现出零样本能力，但达到最先进性能通常需要计算成本高的微调方法，这限制了实用性和效率。该研究旨在解决这一问题，提供一种资源高效的替代方案，以减少对昂贵训练过程的依赖，并利用检索技术提升模型性能。",
      "method": "IQARAG框架基于检索增强生成技术，通过检索语义相似但质量变化的参考图像及其平均意见分数，整合到特定提示中，为模型提供视觉感知锚点。关键阶段包括检索特征提取、图像检索和整合与质量分数生成，这些阶段共同帮助模型更好地评估输入图像的质量，而无需额外训练。",
      "result": "在多个图像质量评估数据集（如KADID、KonIQ、LIVE Challenge和SPAQ）上的实验表明，IQARAG有效提升了大型多模态模型的性能，提供了与基线方法的比较优势，但摘要未明确说明具体性能指标数据。",
      "conclusion": "本研究的主要贡献是开发了IQARAG框架，通过无训练方法增强了模型在图像质量评估中的能力，具有学术价值（推进了LMM在低级视觉任务的应用）和实际应用价值（降低计算成本）。未来工作可探索其在其他视觉任务中的扩展。",
      "tags": [
        "Large Multimodal Models (LMMs)",
        "Retrieval-Augmented Generation (RAG)",
        "Image Quality Assessment (IQA)",
        "Mean Opinion Scores (MOSs)",
        "Feature Extraction"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:56.447744Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08310",
    "title": "ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning",
    "authors": [
      "Kun Liang",
      "Clive Bai",
      "Xin Xu",
      "Chenming Tang",
      "Sanwoo Lee",
      "Weijie Liu",
      "Saiyong Yang",
      "Yunfang Wu"
    ],
    "abstract": "Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08310.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08310",
    "published": "2026-01-13T07:57:48Z",
    "updated": "2026-01-13T07:57:48Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "ORBIT框架通过多阶段强化学习和策略蒸馏实现可控制的多预算推理，核心创新在于解决推理成本与准确性的灵活权衡。",
      "motivation": "大型推理模型在推理时使用长链式思考导致高计算成本，现有方法通过输入推断推理预算，但估计最小所需努力困难，且在训练中固定了成本与准确性权衡，限制了不同部署场景下的灵活性。这一问题在资源受限环境中尤为重要，需要更可靠和灵活的解决方案来提高效率。",
      "method": "ORBIT框架采用多阶段强化学习探索每个努力级别下的帕累托最优推理行为，然后使用策略蒸馏将这些行为融合到一个统一的模型中。关键创新包括输入触发的清晰推理模式分离，以及将前沿策略集成到单一模型，实现可控推理。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "实验表明，ORBIT实现了多个推理模式上的可控行为，每个模式下保持竞争性推理密度，并将这些策略集成到统一学生模型中，同时维持模式分离和每个模式的高性能。摘要未提供具体性能指标数据，如准确率或效率提升，但强调了框架的有效性与基线方法的对比优势。",
      "conclusion": "ORBIT的主要贡献是提出了一种可控制的多预算推理框架，解决了推理效率与准确性之间的灵活平衡问题。学术上为可控推理研究提供了新方法；实际应用中可适应不同计算资源和精度要求的场景。摘要未明确说明研究的局限性或未来工作方向。",
      "tags": [
        "Large Reasoning Models",
        "Chain-of-Thought Reasoning",
        "Reinforcement Learning",
        "On-policy Distillation",
        "Multi-Budget Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:21.270158Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08308",
    "title": "AgriAgent: Contract-Driven Planning and Capability-Aware Tool Orchestration in Real-World Agriculture",
    "authors": [
      "Bo Yang",
      "Yu Zhang",
      "Yunkui Chen",
      "Lanfei Feng",
      "Xiao Xu",
      "Nueraili Aierken",
      "Shijian Li"
    ],
    "abstract": "Intelligent agent systems in real-world agricultural scenarios must handle diverse tasks under multimodal inputs, ranging from lightweight information understanding to complex multi-step execution. However, most existing approaches rely on a unified execution paradigm, which struggles to accommodate large variations in task complexity and incomplete tool availability commonly observed in agricultural environments. To address this challenge, we propose AgriAgent, a two-level agent framework for real-world agriculture. AgriAgent adopts a hierarchical execution strategy based on task complexity: simple tasks are handled through direct reasoning by modality-specific agents, while complex tasks trigger a contract-driven planning mechanism that formulates tasks as capability requirements and performs capability-aware tool orchestration and dynamic tool generation, enabling multi-step and verifiable execution with failure recovery. Experimental results show that AgriAgent achieves higher execution success rates and robustness on complex tasks compared to existing tool-centric agent baselines that rely on unified execution paradigms. All code, data will be released at after our work be accepted to promote reproducible research.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08308.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08308",
    "published": "2026-01-13T07:53:09Z",
    "updated": "2026-01-13T07:53:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出AgriAgent，一个用于现实农业的两层代理框架，通过合同驱动规划和能力感知工具编排，有效处理任务复杂性差异和工具可用性问题。",
      "motivation": "现实农业场景中的智能代理系统需处理从轻量级信息理解到复杂多步执行的多样化任务，但现有方法依赖统一执行范式，难以适应任务复杂性的大变化和农业环境中常见的不完整工具可用性。这个问题的重要性在于它限制了智能系统在复杂农业环境中的高效应用，阻碍了自动化和智能化水平的提升，因此需要更具适应性和层次化的解决方案。",
      "method": "AgriAgent采用基于任务复杂性的层次化执行策略：简单任务由模态特定代理通过直接推理处理；复杂任务则触发合同驱动规划机制，将任务分解为能力需求，并通过能力感知工具编排和动态工具生成实现多步执行、可验证性和故障恢复。关键创新点包括两层代理架构、合同驱动任务规划和动态工具生成能力，提高了系统在农业场景中的适应性。",
      "result": "实验结果表明，AgriAgent在复杂任务上相比现有基于统一执行范式的工具中心代理基线，实现了更高的执行成功率和鲁棒性。摘要未明确说明具体数据细节，但与基线方法相比，AgriAgent在农业场景中的执行效果显著提升，特别是在处理复杂多步任务时，增强了系统的可靠性和适应性。",
      "conclusion": "AgriAgent的主要贡献是提出一种合同驱动规划和能力感知工具编排的两层代理框架，解决了现实农业中任务复杂性差异和工具可用性问题。其学术价值在于为智能代理系统提供了层次化、适应性强的执行策略；实际应用价值在于促进农业智能化和自动化。未来工作可进一步探索在更广泛场景中的适用性，或优化工具生成和规划机制的效率。",
      "tags": [
        "Intelligent Agent Systems",
        "Contract-Driven Planning",
        "Capability-Aware Tool Orchestration",
        "Hierarchical Execution",
        "Real-World Agriculture"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:09.644675Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08303",
    "title": "SnapGen++: Unleashing Diffusion Transformers for Efficient High-Fidelity Image Generation on Edge Devices",
    "authors": [
      "Dongting Hu",
      "Aarush Gupta",
      "Magzhan Gabidolla",
      "Arpit Sahni",
      "Huseyin Coskun",
      "Yanyu Li",
      "Yerlan Idelbayev",
      "Ahsan Mahmood",
      "Aleksei Lebedev",
      "Dishani Lahiri",
      "Anujraaj Goyal",
      "Ju Hu",
      "Mingming Gong",
      "Sergey Tulyakov",
      "Anil Kag"
    ],
    "abstract": "Recent advances in diffusion transformers (DiTs) have set new standards in image generation, yet remain impractical for on-device deployment due to their high computational and memory costs. In this work, we present an efficient DiT framework tailored for mobile and edge devices that achieves transformer-level generation quality under strict resource constraints. Our design combines three key components. First, we propose a compact DiT architecture with an adaptive global-local sparse attention mechanism that balances global context modeling and local detail preservation. Second, we propose an elastic training framework that jointly optimizes sub-DiTs of varying capacities within a unified supernetwork, allowing a single model to dynamically adjust for efficient inference across different hardware. Finally, we develop Knowledge-Guided Distribution Matching Distillation, a step-distillation pipeline that integrates the DMD objective with knowledge transfer from few-step teacher models, producing high-fidelity and low-latency generation (e.g., 4-step) suitable for real-time on-device use. Together, these contributions enable scalable, efficient, and high-quality diffusion models for deployment on diverse hardware.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08303.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08303",
    "published": "2026-01-13T07:46:46Z",
    "updated": "2026-01-13T07:46:46Z",
    "comment": "Project page:",
    "light_analysis": {
      "overview": "本文提出SnapGen++框架，通过自适应全局-局部稀疏注意力、弹性训练和知识引导分布匹配蒸馏，实现高效高保真的边缘设备图像生成。",
      "motivation": "扩散变换器（DiTs）在图像生成方面进展显著，但高计算和内存成本使其难以在移动和边缘设备上部署。随着设备上AI应用（如实时图像生成）需求的增长，现有DiTs模型因资源消耗大而无法满足低延迟和高效率的要求，导致在实际部署中存在局限性。因此，开发适应资源约束的高效DiT框架成为关键，以解决在边缘环境中平衡质量和效率的挑战。",
      "method": "论文提出三个核心组件：首先，设计紧凑的DiT架构，采用自适应全局-局部稀疏注意力机制，以平衡全局上下文建模和局部细节保留。其次，开发弹性训练框架，在一个统一超级网络中联合优化多个容量的子DiT，使单模型能动态调整以适应不同硬件。最后，提出知识引导分布匹配蒸馏（KGDMD），结合分布匹配蒸馏目标与少步教师模型的知识转移，生成高保真、低延迟的图像（例如，4步推理）。这些方法共同提高了模型在资源受限设备上的效率和实用性。",
      "result": "摘要未明确说明具体性能指标，但指出该框架能实现变换器级别的生成质量，并生成高保真、低延迟的图像（例如，4步推理适用于实时应用）。推断其在保持生成质量的同时，相较于传统DiTs，显著降低了计算和内存开销，提升了部署效率和硬件适应性，可能通过实验验证与基线方法相比在效率和保真度上的改进。",
      "conclusion": "本研究的主要贡献在于提出SnapGen++框架，集成了多种优化技术，实现了扩散模型在多样化硬件上的可扩展、高效和高质量部署。其学术价值在于推动了设备上AI模型的效率优化方法，实际应用价值在于促进了边缘设备（如移动和物联网设备）的高保真图像生成应用。未来工作可进一步探索硬件兼容性扩展和模型压缩的极限，以提高更广泛场景下的适用性。",
      "tags": [
        "Diffusion Transformers",
        "Sparse Attention",
        "Knowledge Distillation",
        "Edge Computing",
        "Model Compression"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:31.229100Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08302",
    "title": "Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques",
    "authors": [
      "Marvin Schmitt",
      "Anne Schwerk",
      "Sebastian Lempert"
    ],
    "abstract": "This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08302.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08302",
    "published": "2026-01-13T07:45:36Z",
    "updated": "2026-01-13T07:45:36Z",
    "comment": "21 pages, 4 figures, 13 tables",
    "light_analysis": {
      "overview": "论文通过定制化高级提示工程技术，显著提升大型语言模型在情感分类和讽刺检测任务中的性能。",
      "motivation": "本研究旨在通过提示工程增强大型语言模型在情感分析中的能力，特别是处理情感分类和讽刺检测等任务。现有方法可能无法充分捕捉细微的情感差异，如讽刺的检测，导致模型性能不足。研究强调高级提示工程的重要性，以解决实际应用中模型对复杂语义理解的需求，提升自然语言处理任务的准确性和实用性。摘要未明确说明具体基线方法的不足之处，但隐含了现有提示技术可能未针对模型和任务进行优化的局限性。",
      "method": "论文采用高级提示工程技术，包括少样本学习、思维链提示和自一致性方法，对比基线评估性能。研究使用GPT-4o-mini和gemini-1.5-flash两个大型语言模型，在情感分类、基于方面的情感分析和讽刺检测任务上进行实验。方法侧重于定制化提示设计，考虑模型架构和任务语义复杂性，通过准确率、召回率、精确率和F1分数等指标量化性能提升，未明确提及具体数据集细节。",
      "result": "实验结果显示，高级提示技术显著改善了情感分析任务的性能。具体数据包括：少样本提示在GPT-4o-mini模型中表现最佳，而思维链提示在gemini-1.5-flash模型中将讽刺检测性能提升了高达46%。与基线相比，这些方法在多个任务中均显示出明显的优势，如提高准确率、F1分数等指标，证实了定制化提示策略的有效性，但摘要未提供具体的基线对比数字。",
      "conclusion": "研究的主要贡献在于证明提示策略必须根据特定的大型语言模型和任务进行定制化设计，以最大化性能提升。它强调了提示设计与模型架构及任务语义复杂性对齐的重要性，为LLMs在情感分析等领域的实际应用提供指导。尽管成果显著，未来工作可以进一步探索更广泛的模型和任务，以验证方法的普适性，并可能涉及更复杂的提示技术开发。",
      "tags": [
        "Prompt Engineering",
        "Few-Shot Learning",
        "Chain-of-Thought Prompting",
        "Sentiment Analysis",
        "Irony Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:48.953504Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08301",
    "title": "ReCo-KD: Region- and Context-Aware Knowledge Distillation for Efficient 3D Medical Image Segmentation",
    "authors": [
      "Qizhen Lan",
      "Yu-Chun Hsu",
      "Nida Saddaf Khan",
      "Xiaoqian Jiang"
    ],
    "abstract": "Accurate 3D medical image segmentation is vital for diagnosis and treatment planning, but state-of-the-art models are often too large for clinics with limited computing resources. Lightweight architectures typically suffer significant performance loss. To address these deployment and speed constraints, we propose Region- and Context-aware Knowledge Distillation (ReCo-KD), a training-only framework that transfers both fine-grained anatomical detail and long-range contextual information from a high-capacity teacher to a compact student network. The framework integrates Multi-Scale Structure-Aware Region Distillation (MS-SARD), which applies class-aware masks and scale-normalized weighting to emphasize small but clinically important regions, and Multi-Scale Context Alignment (MS-CA), which aligns teacher-student affinity patterns across feature levels. Implemented on nnU-Net in a backbone-agnostic manner, ReCo-KD requires no custom student design and is easily adapted to other architectures. Experiments on multiple public 3D medical segmentation datasets and a challenging aggregated dataset show that the distilled lightweight model attains accuracy close to the teacher while markedly reducing parameters and inference latency, underscoring its practicality for clinical deployment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08301.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08301",
    "published": "2026-01-13T07:44:43Z",
    "updated": "2026-01-13T07:44:43Z",
    "comment": "10 pages",
    "light_analysis": {
      "overview": "提出ReCo-KD框架，通过区域和上下文感知知识蒸馏，在保持高准确性的同时实现高效3D医学图像分割，减少模型参数和推理延迟。",
      "motivation": "精确的3D医学图像分割对临床诊断和治疗规划至关重要，但现有先进模型通常计算资源需求高，不适合资源有限的诊所环境；轻量级架构虽效率高，却常导致显著的性能下降。因此，本研究旨在解决部署和速度限制，通过知识蒸馏在保持分割精度的同时提升模型效率，以适应实际临床需求，弥补现有方法的不足。",
      "method": "ReCo-KD框架整合了多尺度结构感知区域蒸馏（MS-SARD）和多尺度上下文对齐（MS-CA）。MS-SARD使用类感知掩码和尺度归一化权重强调临床重要的小区域，保留精细解剖细节；MS-CA在不同特征层级对齐师生亲和力模式，传递长程上下文信息。该框架以骨干无关方式在nnU-Net上实现，无需定制学生网络，便于扩展到其他架构。",
      "result": "在多个公共3D医学分割数据集和挑战性聚合数据集上的实验表明，蒸馏后轻量级模型的分割准确性接近高容量教师模型，同时参数数量和推理延迟显著降低。这验证了ReCo-KD在保持高性能的同时提升模型效率的实用性，适合临床部署，与基线方法相比性能损失最小。",
      "conclusion": "ReCo-KD通过区域和上下文感知知识蒸馏，有效平衡了模型效率和准确性，为3D医学图像分割提供了新解决方案。其学术价值在于提出结合临床重点区域和长程上下文的蒸馏策略；实际应用价值在于促进轻量级模型在资源受限环境中的部署。未来工作可能包括扩展到其他任务或优化框架，摘要未明确说明具体局限性。",
      "tags": [
        "Knowledge Distillation",
        "3D Medical Image Segmentation",
        "nnU-Net",
        "Multi-Scale Distillation",
        "Context Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:01.582275Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08297",
    "title": "Demystifying the Slash Pattern in Attention: The Role of RoPE",
    "authors": [
      "Yuan Cheng",
      "Fengzhuo Zhang",
      "Yunlong Hou",
      "Cunxiao Du",
      "Chao Du",
      "Tianyu Pang",
      "Aixin Sun",
      "Zhuoran Yang"
    ],
    "abstract": "Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08297.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08297",
    "published": "2026-01-13T07:40:57Z",
    "updated": "2026-01-13T07:40:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过经验与理论分析，揭示了大型语言模型中斜杠注意力模式的成因，并证明了旋转位置嵌入（RoPE）在其中的关键作用。",
      "motivation": "研究动机是解释LLMs中出现的斜杠注意力模式，该模式表现为注意力得分集中在特定偏移的子对角线上，对信息传递至关重要。现有方法未能系统解释这种模式为何自然出现，尤其是在模型训练过程中的内在机制，这限制了对LLM内部行为的理解。理解这些模式有助于优化模型设计和提高解释性，从而推动AI领域的进步。本研究从理论和经验结合的角度，旨在填补这一空白，深入探究注意力模式的本质。",
      "method": "研究方法包括两部分：经验分析和理论证明。首先，分析开源LLMs，发现斜杠主导头是模型固有的且能泛化到分布外提示。其次，分析查询、键和RoPE，识别出两个特征条件：查询和键几乎秩一，RoPE由中高频成分主导。在理论方面，将这些条件形式化为建模假设，研究带有RoPE的浅层Transformer的训练动态，证明通过梯度下降训练的模型会自然发展出斜杠主导头。关键创新在于结合RoPE的频率特性来系统解释注意力模式的形成机制。",
      "result": "主要实验结果表明，斜杠主导头在LLMs中是固有的，并且在分布外提示下保持稳定泛化能力。经验分析揭示了两个关键条件：查询和键的秩一特性及RoPE的中高频成分优势。理论证明表明，在这些条件下，模型通过训练会确保斜杠注意力模式的出现。这些发现提供了实证和理论基础，强调了RoPE在形成特定注意力模式中的核心角色，为理解LLM内部工作机制提供了新见解。",
      "conclusion": "本研究成功解释了斜杠注意力模式的成因，并突出了RoPE的重要性。主要贡献在于提供了从经验到理论的系统性分析，增强了LLM内部工作机制的可解释性和透明度。学术价值在于推动了注意力机制的研究，为未来优化位置编码和模型设计提供了理论指导。摘要未明确说明局限性，但未来工作可能包括探索更复杂场景或其他位置编码的影响，以扩展研究成果。",
      "tags": [
        "Large Language Models",
        "Attention Patterns",
        "Rotary Position Embedding",
        "Training Dynamics",
        "Gradient Descent"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:58.115156Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08293",
    "title": "M3SR: Multi-Scale Multi-Perceptual Mamba for Efficient Spectral Reconstruction",
    "authors": [
      "Yuze Zhang",
      "Lingjie Li",
      "Qiuzhen Lin",
      "Zhong Ming",
      "Fei Yu",
      "Victor C. M. Leung"
    ],
    "abstract": "The Mamba architecture has been widely applied to various low-level vision tasks due to its exceptional adaptability and strong performance. Although the Mamba architecture has been adopted for spectral reconstruction, it still faces the following two challenges: (1) Single spatial perception limits the ability to fully understand and analyze hyperspectral images; (2) Single-scale feature extraction struggles to capture the complex structures and fine details present in hyperspectral images. To address these issues, we propose a multi-scale, multi-perceptual Mamba architecture for the spectral reconstruction task, called M3SR. Specifically, we design a multi-perceptual fusion block to enhance the ability of the model to comprehensively understand and analyze the input features. By integrating the multi-perceptual fusion block into a U-Net structure, M3SR can effectively extract and fuse global, intermediate, and local features, thereby enabling accurate reconstruction of hyperspectral images at multiple scales. Extensive quantitative and qualitative experiments demonstrate that the proposed M3SR outperforms existing state-of-the-art methods while incurring a lower computational cost.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08293.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08293",
    "published": "2026-01-13T07:33:38Z",
    "updated": "2026-01-13T07:33:38Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "提出M3SR，一种基于Mamba的多尺度多感知架构，用于高效光谱重建，以提升性能并降低计算成本。",
      "motivation": "研究旨在解决Mamba架构在光谱重建中面临的两个关键问题：单空间感知限制了对高光谱图像的全面理解，单尺度特征提取难以捕捉复杂结构和细节。该问题在高光谱图像分析中非常重要，广泛应用于遥感、医学成像等领域，需要精确重建以提取有用信息。现有基于Mamba的方法可能忽略多尺度细节和多感知信息，导致重建精度和效率受限，从而迫切需要改进。摘要强调了这些挑战，但未详细说明具体应用背景。",
      "method": "论文提出M3SR架构，核心包括多感知融合块和U-Net结构的结合。关键创新点是设计多感知融合块，增强模型对输入特征的全局、中间和局部感知能力；通过集成到U-Net结构中，实现多尺度特征的有效提取和融合，以更准确地重建高光谱图像。技术细节基于Mamba架构，专注于光谱重建任务，但摘要未明确说明使用的具体数据集或模型参数，因此推断该方法可能依赖标准实验设置。",
      "result": "通过广泛的定量和定性实验，M3SR在光谱重建任务中优于现有先进方法，具体表现包括性能提升和计算成本降低。实验结果显示，M3SR在准确性方面超越基线方法，同时保持了更高的效率，这表明它能够更好地平衡精度和计算开销。与现有state-of-the-art方法的对比验证了其优越性，但摘要未提供具体性能指标如准确率数字，因此推断实验结果基于常规评估指标。",
      "conclusion": "论文的主要贡献是成功开发了M3SR，有效解决了Mamba在光谱重建中的单空间感知和单尺度特征提取限制。该研究具有学术价值，推动了多尺度多感知架构在低层视觉任务中的应用；实际应用价值体现在提升光谱重建的准确性和效率，对遥感、医学成像等领域有潜在影响。摘要未明确说明局限性或未来工作方向，但可推断可能需要在更多数据集上验证或扩展应用场景。",
      "tags": [
        "Spectral Reconstruction",
        "Mamba Architecture",
        "Multi-Scale Feature Extraction",
        "Multi-Perceptual Fusion",
        "U-Net"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:49.208604Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08292",
    "title": "KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?",
    "authors": [
      "Xianfeng Wang",
      "Kaiwei Zhang",
      "Qi Jia",
      "Zijian Chen",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a \"Scaling Law Paradox\": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08292.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08292",
    "published": "2026-01-13T07:32:50Z",
    "updated": "2026-01-13T07:32:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "KidVis基准揭示多模态大语言模型在基础视觉感知能力上远逊于6岁儿童，并发现参数扩展的‘缩放定律悖论’凸显其局限性。",
      "motivation": "多模态大语言模型在复杂推理任务中表现优异，但其是否具备类似人类的基础视觉原语仍未被深入研究，这是一个关键空白。现有方法过于依赖高语义任务，忽视了低语义依赖的视觉能力，这可能导致模型在广义视觉智能上受限。评估这些基础能力对实现更全面的人工智能至关重要，因为它们是高级理解和决策的基石，从而凸显了研究的必要性和创新性。",
      "method": "研究基于人类视觉发展理论，提出了KidVis基准，将视觉智能解构为六种原子能力：专注力、追踪、辨别、记忆、空间和闭合，共涵盖10类低语义依赖的视觉任务。这些任务设计旨在减少语义干扰，专注于基础感知。通过对20个先进的多模态大语言模型进行评估，并与6-7岁儿童作为生理基线对比，方法强调了定量分析和任务多样性的结合。",
      "result": "实验结果显示，人类儿童在KidVis基准上的平均得分为95.32，而最先进的多模态大语言模型GPT-5仅得67.33，表现出显著的性能差距。更重要的是，研究发现存在‘缩放定律悖论’，即增加模型参数未能线性改善这些基础视觉能力。这表明当前模型在感知原语方面存在根本缺陷，与基线方法对比突显了其不足。",
      "conclusion": "本研究确认当前多模态大语言模型尽管推理能力强大，但缺乏广义视觉智能所需的生理感知原语。其学术价值在于填补了视觉能力评估的空白，实际应用价值在于指导未来模型设计，如加强基础视觉训练或集成人类感知机制。局限性可能在于基准的适用范围，未来工作可探索更多视觉任务或改进评估方法。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Perception Benchmark",
        "Scaling Law Paradox",
        "Human Visual Development",
        "AI Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:07.807953Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08288",
    "title": "OpenMic: A Multi-Agent-Based Stand-Up Comedy Generation System",
    "authors": [
      "Yuyang Wu",
      "Hanzhong Cao",
      "Jianhao Chen",
      "Yufei Li"
    ],
    "abstract": "Chinese stand-up comedy generation goes beyond plain text generation, requiring culturally grounded humor, precise timing, stage-performance cues, and implicit multi-step reasoning. Moreover, commonly used Chinese humor datasets are often better suited for humor understanding and evaluation than for long-form stand-up generation, making direct supervision misaligned with the target task. To address these challenges, we present OpenMic, an end-to-end multi-agent system built on AutoGen that transforms a user-provided life topic into a 3-5 minute Chinese stand-up performance and further produces a narrated comedy video. OpenMic orchestrates multiple specialized agents in a multi-round iterative loop-planning to jointly optimize humor, timing, and performability. To mitigate the dataset-task mismatch, we augment generation with retrieval-augmented generation (RAG) for material grounding and idea expansion, and we fine-tune a dedicated JokeWriter to better internalize stand-up-specific setup-punchline structures and long-range callbacks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08288.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08288",
    "published": "2026-01-13T07:26:23Z",
    "updated": "2026-01-13T07:26:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "OpenMic是一个基于多代理的端到端系统，用于生成文化相关的中文单口喜剧表演和视频，创新性地结合了检索增强生成和微调技术。",
      "motivation": "中文单口喜剧生成面临多个挑战：不仅需要文本生成，还涉及文化基础幽默、精准时机、舞台表演提示和隐式多步推理。现有常用中文幽默数据集更适合幽默理解和评估，而不是长形式单口生成，这导致直接监督方法与目标任务不匹配，限制了生成质量和可表演性。本研究旨在解决这些问题，开发一个系统能高效生成高质量、可表演的中文单口喜剧，以弥补AI在创意内容生成领域的应用空白。",
      "method": "OpenMic是一个端到端的多代理系统，基于AutoGen框架构建。系统通过编排多个专业代理在多轮迭代循环规划中联合优化幽默、时机和可表演性。为了缓解数据集与任务的不匹配问题，采用检索增强生成（RAG）技术来增强生成过程，用于材料基础和想法扩展。此外，微调了一个专门的JokeWriter模型，以更好地内化单口喜剧特有的设置-笑点结构和长距离回调，从而提高生成内容的准确性和幽默感。",
      "result": "摘要未明确说明具体的实验结果，如性能指标或对比数据。然而，基于描述，系统能够将用户提供的生活主题转化为3-5分钟的中文单口表演，并进一步生成讲述的喜剧视频。这表明系统在功能上实现了长形式幽默内容的生成，但没有提供与基线方法的定量对比或准确率等数据。实际效果可能依赖于RAG和微调技术的优化，但具体量化评估在摘要中未提及。",
      "conclusion": "本研究的主要贡献是提出了OpenMic系统，它通过多代理架构、检索增强生成和模型微调，有效解决了中文单口喜剧生成中的文化适应、时机控制和可表演性问题。学术价值在于结合了多代理规划和深度学习技术，推动了AI在创意生成领域的发展；实际应用价值则体现在自动化内容创作和娱乐产业中。未来工作可能包括扩展系统到其他文化或改进代理协同效率，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Multi-Agent System",
        "Retrieval-Augmented Generation (RAG)",
        "Fine-Tuning",
        "AutoGen",
        "Stand-Up Comedy Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:01.396754Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08282",
    "title": "D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning",
    "authors": [
      "Kangcheng Luo",
      "Tinglang Wu",
      "Yansong Feng"
    ],
    "abstract": "Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08282.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08282",
    "published": "2026-01-13T07:17:51Z",
    "updated": "2026-01-13T07:17:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "D$^2$Plan通过双代理动态全局规划范式，有效解决检索增强推理中搜索链构建无效和推理劫持的失败模式。",
      "motivation": "研究动机在于现有搜索增强大语言模型在多跳推理任务中，随着上下文累积，重要证据被无关信息淹没，导致两个关键失败：搜索链构建无效（产生错误查询或遗漏关键信息）和推理劫持（模型误将干扰项识别为证据）。这些问题降低了模型在复杂查询中的可靠性和准确性，现有方法如基于强化学习的模型在动态信息处理中表现不足，亟需改进以增强信息过滤和规划能力，提升推理的稳健性和效率。",
      "method": "D$^2$Plan提出双代理协作系统：Reasoner构建显式全局计划并根据检索反馈动态调整，以指导推理过程；Purifier评估检索内容的相关性并压缩关键信息以减少噪音。训练框架包括两阶段：先在合成轨迹上进行监督微调（SFT）冷启动，然后使用面向规划的强化学习奖励优化模型。关键创新点是协作式动态规划和信息净化，提升了大语言模型在检索增强推理中的规划和适应能力。",
      "result": "实验结果在挑战性QA基准上显示，D$^2$Plan实现了更连贯的多步推理和更强的无关信息抵抗力，从而获得优越性能。相比于基线方法，该方法减少了搜索错误和推理偏差，提高了任务准确率，但摘要未明确说明具体数据指标如准确率提升百分比，仅描述了一般性改进效果。",
      "conclusion": "该研究的主要贡献是提出了D$^2$Plan范式，通过双代理协作和动态规划有效解决了检索增强推理中的信息干扰问题。学术价值在于为复杂推理任务提供了新的规划方法，增强模型的信息处理能力；实际应用价值在于提升了大语言模型在问答任务中的性能。未来工作可能涉及扩展到更多领域或优化训练效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Retrieval-Augmented Reasoning",
        "Dual-Agent Planning",
        "Reinforcement Learning",
        "Multi-hop Reasoning",
        "Supervised Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:20.299855Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08280",
    "title": "Greedy Is Enough: Sparse Action Discovery in Agentic LLMs",
    "authors": [
      "Angshul Majumdar"
    ],
    "abstract": "Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.   We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.   Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08280.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08280",
    "published": "2026-01-13T07:15:32Z",
    "updated": "2026-01-13T07:15:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出稀疏动作发现是代理大动作系统中的核心原理，并通过贪心算法提供理论保证，实现高效动作集恢复。",
      "motivation": "现代代理系统（如工具增强语言模型）在具有极大动作空间的环境中操作，例如数千个可用API或检索操作，但经验证据表明只有少量动作对任务性能有实质性影响。研究动机是解决在大规模动作空间中高效识别相关动作的问题，以避免资源浪费和提高决策效率，因为现有方法在处理巨大动作空间时可能效率低下或计算成本高昂。基于稀疏性假设，论文旨在探索如何简化动作发现过程，从而优化代理系统的部署。",
      "method": "论文将动作发现问题形式化为块稀疏恢复问题，基于一个上下文线性奖励模型，其中动作相关性由结构化稀疏假设控制：只有少数动作在潜在状态中具有非零效应。提出的核心方法是受正交匹配追踪启发的贪心算法，通过迭代选择动作来恢复稀疏集。关键创新点在于利用稀疏性结构，在标准假设（如不相干性、信号强度和动作覆盖）下分析算法性能，无需复杂优化步骤，实现高效的动作发现。",
      "result": "理论分析表明，贪心算法在高概率下能精确恢复相关动作集，所需样本数量与稀疏水平和潜在维度多项式相关，而与总动作数量仅对数相关。进一步提供了重拟合参数的估计误差保证，并证明由此产生的决策规则对新潜在状态近乎最优。信息论下界显示稀疏性和充分覆盖是动作发现可处理性的必要条件，这些结果与贪心算法的性能形成对比，验证了其高效性和鲁棒性。",
      "conclusion": "论文的主要贡献是识别稀疏动作发现作为大动作决策的基本原理，为代理系统中的动作剪枝提供了理论基础。学术价值在于严格的数学分析和算法保证，推动了大动作空间决策理论的发展；实际应用价值在于提升代理系统的效率和可扩展性，例如在工具增强语言模型中的动作优化。摘要未明确说明局限性，但未来工作可探索更复杂的环境假设或实际部署中的挑战。",
      "tags": [
        "Sparse Action Discovery",
        "Agentic LLMs",
        "Greedy Algorithm",
        "Block-Sparse Recovery",
        "Linear Reward Model"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:18.890354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08278",
    "title": "One-Shot Identification with Different Neural Network Approaches",
    "authors": [
      "Janis Mohr",
      "Jörg Frochte"
    ],
    "abstract": "Convolutional neural networks (CNNs) have been widely used in the computer vision community, significantly improving the state-of-the-art. But learning good features often is computationally expensive in machine learning settings and is especially difficult when there is a lack of data. One-shot learning is one such area where only limited data is available. In one-shot learning, predictions have to be made after seeing only one example from one class, which requires special techniques. In this paper we explore different approaches to one-shot identification tasks in different domains including an industrial application and face recognition. We use a special technique with stacked images and use siamese capsule networks. It is encouraging to see that the approach using capsule architecture achieves strong results and exceeds other techniques on a wide range of datasets from industrial application to face recognition benchmarks while being easy to use and optimise.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08278.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08278",
    "published": "2026-01-13T07:13:31Z",
    "updated": "2026-01-13T07:13:31Z",
    "comment": "18 pages, Keywords: One-shot learning, Convolutional neural networks, Siamese networks, Capsules, Industrial application",
    "light_analysis": {
      "overview": "本论文通过结合 stacked images 和 siamese capsule networks，显著提升了 one-shot identification 任务的性能，并在多个数据集上超越其他技术。",
      "motivation": "卷积神经网络在计算机视觉中广泛应用，但学习高质量特征计算成本高，且在数据稀缺时效果受限。One-shot learning 是一种仅在每个类别有一个示例的情况下进行预测的挑战性任务，需要特殊技术来处理。本研究旨在解决工业应用和人脸识别等领域中，面对数据不足时的识别问题，以改进现有方法在效率和性能上的不足。",
      "method": "论文探索了不同的神经网络方法，重点提出一种使用 stacked images 的特殊技术和 siamese capsule networks。该方法利用胶囊网络架构处理 one-shot identification 任务，涉及在工业应用和人脸识别数据集上的实验。关键创新点在于结合图像堆叠和 siamese 结构，以增强特征表示并减少对大量数据的依赖。摘要未明确说明具体模型架构或数据集细节。",
      "result": "在广泛的测试数据集上，包括工业应用和人脸识别基准，使用胶囊架构的方法取得了强结果，超越了其他技术。摘要未明确提供具体性能指标（如准确率），但表明该方法在 one-shot identification 任务中表现出色，且易于使用和优化，显示了在数据稀缺场景下的优越性。",
      "conclusion": "本研究的主要贡献是提出了一种结合 stacked images 和 siamese capsule networks 的方法，有效提升了 one-shot identification 的性能，并在多个应用中优于现有技术。这为数据稀缺环境下的识别任务提供了新解决方案，具有实际应用价值，特别是在工业和人脸识别领域。未来工作可能涉及进一步优化方法或扩展到其他领域。",
      "tags": [
        "One-shot Learning",
        "Siamese Networks",
        "Capsule Networks",
        "Stacked Images",
        "Convolutional Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:44.128050Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08276",
    "title": "ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web",
    "authors": [
      "Zhiyuan Yao",
      "Zishan Xu",
      "Yifu Guo",
      "Zhiguang Han",
      "Cheng Yang",
      "Shuo Zhang",
      "Weinan Zhang",
      "Xingshan Zeng",
      "Weiwen Liu"
    ],
    "abstract": "With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08276.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08276",
    "published": "2026-01-13T07:07:39Z",
    "updated": "2026-01-13T07:07:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出ToolACE-MCP管道，通过训练历史感知路由器解决大规模Agent Web中的可扩展性和通用性问题。",
      "motivation": "随着Agent Web和Model Context Protocol (MCP)的兴起，代理生态系统演变为开放协作网络，可访问工具数量指数增长，但当前架构面临严重的可扩展性和通用性瓶颈。这导致在大规模工具生态系统中导航困难，限制了代理的高效协作。现有方法在处理开放、动态环境时可能不足，因此需要新方案来提升路由能力，以支持未来Agent Web的发展和应用。该研究旨在解决这些挑战，促进生态系统的高效运行。",
      "method": "研究方法基于ToolACE-MCP管道，它训练历史感知路由器以实现大规模生态系统中的精确导航。核心创新在于利用依赖丰富的候选图合成多轮轨迹，有效训练路由器具备动态上下文理解能力。这创建了即插即用的轻路由代理，便于在Agent Web中部署。技术路线侧重于图结构数据处理和轨迹合成，通过增强路由器的适应性和泛化性，处理复杂网络环境，支持多代理交互。",
      "result": "实验在真实世界基准MCP-Universe和MCP-Mark上进行，ToolACE-MCP展现出卓越性能。摘要未提供具体数据指标，但结果表明该方法在路由任务中表现优越，具备关键特性：泛化到多代理协作只需最小适应，对噪声具有强鲁棒性，并能有效扩展到大规模候选空间。这些发现验证了方法的有效性和实用性，为开放生态系统中的路由问题提供了实证支持，但未明确与基线方法的对比细节。",
      "conclusion": "研究的主要贡献是提出了ToolACE-MCP管道，为开放生态系统中的通用编排建立了强有力的实证基础。其意义在于展示了未来Agent Web的关键特性，如增强的泛化能力、鲁棒性和可扩展性，推动代理技术的实际应用。摘要未明确说明局限性或未来工作方向，但该方法为相关领域提供了新思路，促进了大规模协作网络的发展，具有学术和应用价值。",
      "tags": [
        "History-Aware Routing",
        "Model Context Protocol (MCP)",
        "Graph Synthesis",
        "Multi-Agent Collaboration",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:38.459814Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08274",
    "title": "Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees",
    "authors": [
      "Kun Li",
      "Zenan Xu",
      "Junan Li",
      "Zengrui Jin",
      "Jinghao Deng",
      "Zexuan Qiu",
      "Bo Zhou"
    ],
    "abstract": "Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08274.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08274",
    "published": "2026-01-13T07:06:21Z",
    "updated": "2026-01-13T07:06:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "DART框架通过rollout trees在长链式思维中自发集成工具使用，无需人工标注，显著增强了大型语言模型的推理能力。",
      "motivation": "工具集成推理是增强大型语言模型计算能力的关键范式，但在长链式思维中整合工具使用仍未被充分探索。这主要由于训练数据稀缺，以及在不损害模型内在长链推理能力的前提下集成工具的挑战。该研究旨在解决这些问题，提升复杂推理任务的表现，填补现有方法的不足。",
      "method": "DART采用强化学习框架，通过在训练中构建动态rollout trees来发现有效的工具使用机会。它在有希望的位置分支以探索多样化的工具集成轨迹，然后使用树基过程优势估计来识别和强化对解决方案有积极贡献的特定子轨迹，核心创新在于动态探索和基于树的优势估计机制。",
      "result": "论文在AIME和GPQA-Diamond等挑战性基准上进行广泛实验，结果显示DART显著优于现有方法，成功协调工具执行与长链式思维推理。摘要未明确说明具体性能指标如准确率提升，但强调了其相较于基线的显著优势。",
      "conclusion": "DART框架的主要贡献是提出了一种无需人工标注的工具集成推理强化学习方法，通过rollout trees实现自发发现和强化。这具有重要的学术价值，为工具增强LLMs提供了新途径，可能在实际应用中提高推理系统的效率。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Rollout Trees",
        "Chain-of-Thought",
        "Tool-Integrated Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:41.258421Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08273",
    "title": "HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding",
    "authors": [
      "Qitan Lv",
      "Tianyu Liu",
      "Wen Wu",
      "Xuenan Xu",
      "Bowen Zhou",
      "Feng Wu",
      "Chao Zhang"
    ],
    "abstract": "Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08273.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08273",
    "published": "2026-01-13T07:02:43Z",
    "updated": "2026-01-13T07:02:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "HIPPO提出一个整体感知并行SD框架，通过语义感知令牌保留和视频并行算法，显著加速视频大语言模型的推理。",
      "motivation": "视频大语言模型在处理大规模视觉输入时推理效率低，现有speculative decoding方法专注于剪枝冗余视觉令牌，但加速效果不及纯文本模型。这主要源于两个局限性：剪枝策略未能充分保留视觉语义令牌，导致草稿质量下降和接受率降低；即使剪除90%视觉令牌，草稿模型剩余推理成本仍限制整体速度提升。因此，开发更有效的加速方法对视频理解任务至关重要。",
      "method": "HIPPO框架包括两个关键创新：语义感知令牌保留方法，融合全局注意力分数与局部视觉语义，在高剪枝比例下保留关键语义信息以提升草稿质量；视频并行SD算法，将草稿生成和目标验证阶段解耦并重叠执行，减少等待时间。该方法不依赖特定模型架构，通过整体优化推理流程来提高效率，适用于多种视频LLM。",
      "result": "实验在四个视频LLM和六个基准测试上进行，结果显示HIPPO相比传统自回归解码方法，能实现最高3.51倍的推理加速。尽管摘要未提供具体基准名称和详细指标对比，但结果表明HIPPO有效克服了现有SD方法的局限性，显著提升了视频LLM的推理性能。",
      "conclusion": "HIPPO的主要贡献是提出一个整体感知并行SD框架，通过改进令牌保留和算法并行化，解决了视频LLM推理加速的关键问题。该研究具有重要学术价值，为高效视频处理提供了新途径，并可能推动实际应用如实时视频分析。局限性如计算资源需求或泛化能力未在摘要中明确说明，未来工作可能涉及扩展到更多模型和任务。",
      "tags": [
        "Speculative Decoding",
        "Video Large Language Models",
        "Attention Mechanisms",
        "Parallel Processing",
        "Token Pruning"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:50.693802Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08271",
    "title": "Sparsity Is Necessary: Polynomial-Time Stability for Agentic LLMs in Large Action Spaces",
    "authors": [
      "Angshul Majumdar"
    ],
    "abstract": "Tool-augmented LLM systems expose a control regime that learning theory has largely ignored: sequential decision-making with a massive discrete action universe (tools, APIs, documents) in which only a small, unknown subset is relevant for any fixed task distribution. We formalize this setting as Sparse Agentic Control (SAC), where policies admit block-sparse representations over M >> 1 actions and rewards depend on sparse main effects and (optionally) sparse synergies. We study ell_{1,2}-regularized policy learning through a convex surrogate and establish sharp, compressed-sensing-style results: (i) estimation and value suboptimality scale as k (log M / T)^{1/2} under a Policy-RSC condition; (ii) exact tool-support recovery holds via primal-dual witness arguments when T > k log M under incoherence and beta-min; and (iii) any dense policy class requires Omega(M) samples, explaining the instability of prompt-only controllers. We further show that under partial observability, LLMs matter only through a belief/representation error epsilon_b, yielding an additive O(epsilon_b) degradation while preserving logarithmic dependence on M. Extensions cover tuning-free, online, robust, group-sparse, and interaction-aware SAC.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08271.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08271",
    "published": "2026-01-13T06:56:53Z",
    "updated": "2026-01-13T06:56:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文证明了稀疏性是实现代理式大型语言模型在大规模动作空间中多项式时间稳定性的关键，并通过理论框架提供了保证。",
      "motivation": "工具增强的大型语言模型系统在顺序决策时面临大规模离散动作空间（如工具、API），但只有少量动作对特定任务分布相关。当前学习理论忽略此设置，导致仅提示控制器不稳定，需要大量样本，限制了LLM在复杂任务中的应用。研究旨在通过稀疏性解决策略学习的不稳定问题，提升效率和可靠性。",
      "method": "论文形式化了稀疏代理控制（SAC）框架，其中策略具有块稀疏表示，奖励依赖于稀疏主效应和可选协同效应。研究方法采用ell_{1,2}-正则化策略学习，通过凸代理和压缩感知风格理论，引入Policy-RSC条件。关键创新点包括稀疏性形式化、正则化技术和理论保证，用于处理大规模动作空间中的策略优化问题。",
      "result": "理论结果显示，在Policy-RSC条件下，估计和值次优性以k (log M / T)^{1/2}扩展，而精确工具支持恢复在样本量T > k log M时成立，需满足不相关性和beta-min条件。密集策略类需要至少M个样本，解释了仅提示控制器的不稳定性。部分可观察性下，LLMs的影响导致O(epsilon_b)的附加退化，但保持了与M的对数依赖关系。",
      "conclusion": "论文的主要贡献是形式化稀疏代理控制并证明稀疏性对多项式时间稳定性的必要性，提供了压缩感知风格的理论结果。学术价值在于扩展学习理论到大规模动作空间，实际应用有助于设计更稳定的工具增强LLM系统。未来工作可扩展到无调谐、在线和鲁棒场景，进一步优化稀疏性在代理式AI中的作用。",
      "tags": [
        "Sparse Agentic Control",
        "Regularized Policy Learning",
        "Compressed Sensing",
        "Large Action Spaces",
        "Polynomial-Time Stability"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:59.507649Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08267",
    "title": "Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning",
    "authors": [
      "Fan Gao",
      "Sherry T. Tong",
      "Jiwoong Sohn",
      "Jiahao Huang",
      "Junfeng Jiang",
      "Ding Xia",
      "Piyalitt Ittichaiwong",
      "Kanyakorn Veerakanjana",
      "Hyunjae Kim",
      "Qingyu Chen",
      "Edison Marrese Taylor",
      "Kazuma Kobayashi",
      "Akkiko Aizawa",
      "Irene Li"
    ],
    "abstract": "While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08267.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08267",
    "published": "2026-01-13T06:51:40Z",
    "updated": "2026-01-13T06:51:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Med-CoReasoner，一种语言信息协同推理框架，通过整合英语和本地语言的推理来减少医疗领域的语言差异。",
      "motivation": "当前增强推理能力的大语言模型在英语医疗任务中表现优异，但存在显著的多语言差距，本地语言的推理能力较弱，这限制了AI在全球医疗中的公平部署。现有方法未能有效处理非英语语言，导致医疗资源分配不均，尤其是在低资源语言环境中。因此，研究如何弥合语言差异、提升多语言医疗推理性能至关重要，以促进更广泛的临床应用。",
      "method": "本研究引入Med-CoReasoner框架，它通过并行的英语和本地语言推理提取结构化概念，并使用概念级对齐和检索将本地临床知识集成到英语逻辑支架中。核心创新在于结合英语推理的结构稳健性和本地语言中的实践专业知识。为评估多语言医疗推理，构建了MultiMed-X基准，覆盖七种语言，包含专家标注的长形式问答和自然语言推理任务，每个语言有350个实例。",
      "result": "实验表明，在三个基准测试中，Med-CoReasoner将多语言推理性能平均提升了5%，在低资源语言中提升更为显著。模型蒸馏和专家评估进一步确认，该框架生成的推理痕迹具有临床合理性和文化相关性，优于现有基线方法。",
      "conclusion": "论文的主要贡献是Med-CoReasoner框架，有效减少了医疗推理中的语言差异。其学术价值在于提出了多语言协同推理的新方法；实际应用价值在于促进全球医疗AI的公平部署。未来工作可扩展到更多语言和医疗子领域，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Models",
        "Medical Reasoning",
        "Concept Alignment",
        "Language-Informed Co-Reasoning",
        "Multilingual Benchmark"
      ]
    },
    "analyzed_at": "2026-01-14T03:44:05.324453Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08265",
    "title": "AIMC-Spec: A Benchmark Dataset for Automatic Intrapulse Modulation Classification under Variable Noise Conditions",
    "authors": [
      "Sebastian L. Cocks",
      "Salvador Dreo",
      "Feras Dayoub"
    ],
    "abstract": "A lack of standardized datasets has long hindered progress in automatic intrapulse modulation classification (AIMC) - a critical task in radar signal analysis for electronic support systems, particularly under noisy or degraded conditions. AIMC seeks to identify the modulation type embedded within a single radar pulse from its complex in-phase and quadrature (I/Q) representation, enabling automated interpretation of intrapulse structure. This paper introduces AIMC-Spec, a comprehensive synthetic dataset for spectrogram-based image classification, encompassing 33 modulation types across 13 signal-to-noise ratio (SNR) levels. To benchmark AIMC-Spec, five representative deep learning algorithms - ranging from lightweight CNNs and denoising architectures to transformer-based networks - were re-implemented and evaluated under a unified input format. The results reveal significant performance variation, with frequency-modulated (FM) signals classified more reliably than phase or hybrid types, particularly at low SNRs. A focused FM-only test further highlights how modulation type and network architecture influence classifier robustness. AIMC-Spec establishes a reproducible baseline and provides a foundation for future research and standardization in the AIMC domain.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08265.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08265",
    "published": "2026-01-13T06:42:28Z",
    "updated": "2026-01-13T06:42:28Z",
    "comment": "This work is published in IEEE Access DOI: 10.1109/ACCESS.2025.3645091",
    "light_analysis": {
      "overview": "本文提出了AIMC-Spec，一个用于噪声条件下自动脉内调制分类的综合性基准数据集和评估框架。",
      "motivation": "自动脉内调制分类（AIMC）在雷达电子支持系统中至关重要，用于从单雷达脉冲的复数I/Q表示中识别调制类型，以实现脉内结构的自动解释。然而，标准化数据集的缺乏一直阻碍了该领域的发展，尤其是在噪声或退化条件下，现有方法难以进行有效分类，这限制了实际应用和算法比较的进展。",
      "method": "本论文提出了AIMC-Spec数据集，这是一个合成数据集，专门用于基于频谱图的图像分类，涵盖33种调制类型和13个信噪比水平。为了建立基准，重新实现了五种代表性的深度学习算法，包括轻量卷积神经网络、去噪架构和基于Transformer的网络，并在统一的输入格式下进行评估，以确保可比性和可复现性。",
      "result": "基准测试结果显示，不同算法在性能上存在显著差异，频率调制（FM）信号相对于相位或混合类型，在分类上更为可靠，特别是在低信噪比条件下。摘要未明确说明具体准确率数据，但通过FM-only测试进一步突显了调制类型和网络架构对分类器鲁棒性的影响，为未来研究提供了性能对比的基础。",
      "conclusion": "AIMC-Spec为自动脉内调制分类领域建立了一个可复现的基线，提供了标准化的数据集和评估框架。这项研究的学术价值在于推动了AIMC研究的标准化，实际应用价值在于增强了雷达信号分析在噪声环境下的自动化能力，未来工作可扩展到更多调制类型或真实数据验证。",
      "tags": [
        "Automatic Intrapulse Modulation Classification",
        "Spectrogram Classification",
        "Synthetic Dataset",
        "CNN",
        "Transformer"
      ]
    },
    "analyzed_at": "2026-01-14T03:44:11.611733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08262",
    "title": "VGG Induced Deep Hand Sign Language Detection",
    "authors": [
      "Subham Sharma",
      "Sharmila Subudhi"
    ],
    "abstract": "Hand gesture recognition is an important aspect of human-computer interaction. It forms the basis of sign language for the visually impaired people. This work proposes a novel hand gesture recognizing system for the differently-abled persons. The model uses a convolutional neural network, known as VGG-16 net, for building a trained model on a widely used image dataset by employing Python and Keras libraries. Furthermore, the result is validated by the NUS dataset, consisting of 10 classes of hand gestures, fed to the model as the validation set. Afterwards, a testing dataset of 10 classes is built by employing Google's open source Application Programming Interface (API) that captures different gestures of human hand and the efficacy is then measured by carrying out experiments. The experimental results show that by combining a transfer learning mechanism together with the image data augmentation, the VGG-16 net produced around 98% accuracy.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08262.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08262",
    "published": "2026-01-13T06:39:29Z",
    "updated": "2026-01-13T06:39:29Z",
    "comment": "Published in: Sharma, S., Ghosh, A., Subudhi, S. (2022). Hand Sign Language Detection Using Deep Learning. In: Sahoo, J.P., Tripathy, A.K., Mohanty, M., Li, KC., Nayak, A.K. (eds) Advances in Distributed Computing and Machine Learning. Lecture Notes in Networks and Systems, vol 302. Springer",
    "light_analysis": {
      "overview": "本研究提出基于VGG-16的手势识别系统，结合迁移学习和数据增强，在手语检测中达到约98%的准确率。",
      "motivation": "手势识别在人机交互中至关重要，尤其是对视觉障碍者手语的应用。现有方法可能在准确率或泛化能力上不足，难以满足残障人士的实际需求。本研究旨在解决这一问题，通过深度学习技术提升手势识别的性能，服务于更广泛的人群，推动无障碍通信技术的发展。摘要未明确说明具体现有方法的缺陷，但可推断传统方法可能面临数据不足或模型效率低下的挑战。",
      "method": "论文采用卷积神经网络VGG-16作为核心模型，结合迁移学习机制和图像数据增强技术。使用Python和Keras库在通用图像数据集上训练模型，验证阶段利用NUS数据集（包含10个手势类别）进行性能评估。测试数据通过谷歌开源API构建，模拟真实场景中的10类手势，以全面测试模型的有效性和鲁棒性。",
      "result": "实验结果显示，结合迁移学习和图像数据增强的VGG-16模型在测试集上达到约98%的准确率。这一高性能表明该方法在手势识别任务中的优越性，但摘要未明确提供与基线方法的具体对比数据，可推断相较于基础VGG-16模型有显著提升，验证了技术组合的有效性。",
      "conclusion": "本研究的主要贡献是开发了一个高准确率的手势识别系统，基于VGG-16并结合迁移学习和数据增强。其学术价值在于展示了深度学习在特定应用中的潜力，实际应用上可帮助残障人士改善手语交流。未来工作可扩展至更多手势类别或优化模型以应对复杂环境，但摘要未明确说明局限性。",
      "tags": [
        "Convolutional Neural Network",
        "VGG-16",
        "Transfer Learning",
        "Image Data Augmentation",
        "Hand Gesture Recognition"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:23.255863Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08260",
    "title": "A Usable GAN-Based Tool for Synthetic ECG Generation in Cardiac Amyloidosis Research",
    "authors": [
      "Francesco Speziale",
      "Ugo Lomoio",
      "Fabiola Boccuto",
      "Pierangelo Veltri",
      "Pietro Hiram Guzzi"
    ],
    "abstract": "Cardiac amyloidosis (CA) is a rare and underdiagnosed infiltrative cardiomyopathy, and available datasets for machine-learning models are typically small, imbalanced and heterogeneous. This paper presents a Generative Adversarial Network (GAN) and a graphical command-line interface for generating realistic synthetic electrocardiogram (ECG) beats to support early diagnosis and patient stratification in CA. The tool is designed for usability, allowing clinical researchers to train class-specific generators once and then interactively produce large volumes of labelled synthetic beats that preserve the distribution of minority classes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08260.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08260",
    "published": "2026-01-13T06:33:16Z",
    "updated": "2026-01-13T06:33:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种基于生成对抗网络（GAN）的用户友好工具，用于生成合成心电图节拍，以解决心脏淀粉样变研究中数据集小和不平衡的问题。",
      "motivation": "研究动机源于心脏淀粉样变是一种罕见且常被诊断不足的心脏疾病，可用于机器学习模型的数据集通常规模小、类别不平衡且具有异质性，这限制了模型的准确性和泛化能力，尤其是在早期诊断和患者分层中。现有方法依赖于有限的真实数据，导致模型性能受限，特别是处理少数类时数据不足。因此，开发合成数据生成工具成为关键，以扩增数据集并促进医疗研究的有效性，从而解决临床实践中的数据稀缺挑战。",
      "method": "研究方法采用了生成对抗网络（GAN）技术，核心创新在于结合类特定生成器和一个图形命令行界面，以提高工具的可用性。用户可以先训练针对特定类别的生成器，然后交互式生成大量带标签的合成心电图节拍，这些节拍能够保持少数类的数据分布，专门针对心脏淀粉样变研究的需求。摘要未明确说明具体的数据集或模型架构细节，但强调了工具的实用性和便捷性设计，使临床研究人员能够轻松操作并生成平衡的合成数据。",
      "result": "摘要未明确说明具体的实验结果，如准确率提升或效率改进等定量指标。论文描述了工具能够生成现实的合成心电图节拍，但未提供与基线方法的对比数据或性能验证。因此，可以推断该工具旨在通过合成数据扩增来改善机器学习模型的训练效果，潜在支持早期诊断和患者分层，然而具体效果需参考完整论文的实验部分。",
      "conclusion": "本研究的主要贡献是开发了一个基于GAN的合成心电图生成工具，针对心脏淀粉样变研究中的数据集挑战。它具有学术价值，通过解决医疗AI中数据稀缺和不平衡问题，推动了合成数据在医疗领域的应用。实际应用价值体现在帮助临床研究人员生成大量平衡数据，促进早期诊断的准确性，并有可能扩展到其他类似疾病的研究。未来工作可能包括工具在实际临床环境中的验证和功能扩展，以应对更广泛的医疗场景。",
      "tags": [
        "Generative Adversarial Network",
        "Synthetic Data Generation",
        "Electrocardiogram Analysis",
        "Medical Machine Learning",
        "Usability Design"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:41.409927Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08258",
    "title": "T3: Benchmarking Sycophancy and Skepticism in Causal Judgment",
    "authors": [
      "Edward Y. Chang"
    ],
    "abstract": "We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a \"Skepticism Trap\" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08258.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08258",
    "published": "2026-01-13T06:29:56Z",
    "updated": "2026-01-13T06:29:56Z",
    "comment": "17 pages, 4 figures, 11 tables",
    "light_analysis": {
      "overview": "论文提出T3基准，用于诊断大型语言模型在因果判断中的病态行为，如怀疑陷阱和缩放悖论。",
      "motivation": "研究动机在于评估大型语言模型在因果推理中的可信赖性，现有评估方法可能缺乏对LLM在Pearl因果关系阶梯上表现的精细化分析，无法诊断出如过度怀疑或非单调缩放等问题。这个问题重要，因为因果推理是AI理解复杂世界的基础，模型在安全调整后可能产生不信任行为，影响实际应用中的决策可靠性。T3旨在弥补这一不足，提供系统化的高分辨率失败分析工具，以揭示模型潜在缺陷。",
      "method": "研究方法包括开发T3基准，包含454个专家策划的小场景，基于Pearl的因果关系阶梯进行设计。核心创新是将性能分解为Utility、Safety和Wise Refusal三个维度，实现高分辨率失败分析。技术路线涉及应用T3到前沿模型如Claude Haiku、GPT-4-Turbo和GPT-5.2，诊断特定病理如Skepticism Trap和Scaling Paradox。此外，使用过程验证协议RCA来验证基准的有效性，确保结构化地捕捉模型行为变化。",
      "result": "主要实验结果显示，在L1层安全调整模型如Claude Haiku拒绝60%的有效因果链接，表现为Skepticism Trap。在L3层，GPT-5.2在模糊反事实上性能比GPT-4-Turbo低55分，揭示了非单调Scaling Paradox，原因在于模型陷入瘫痪性过度对冲而非幻觉。与基线模型对比，T3基准成功诊断了这些病理，并通过RCA协议验证了在结构化验证下因果判断的恢复，提升了评估的准确性和洞察力。",
      "conclusion": "结论表明，T3基准提供了系统工具来诊断LLM在因果判断中的病态行为，如怀疑陷阱和缩放悖论。学术上，它揭示了模型安全调整与性能间的权衡问题，以及缩放可能导致的非单调退化，为因果推理研究贡献了新见解。实际应用中，T3可用于评估和改进模型的因果推理能力，促进可信AI发展。未来工作可能包括扩展基准到更多模型类型或因果任务，以及探索缓解这些病理的策略。",
      "tags": [
        "Large Language Model",
        "Causal Judgment",
        "Benchmarking",
        "Pearl's Ladder of Causality",
        "Failure Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:32.120730Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08257",
    "title": "On Evaluation of Unsupervised Feature Selection for Pattern Classification",
    "authors": [
      "Gyu-Il Kim",
      "Dae-Won Kim",
      "Jaesung Lee"
    ],
    "abstract": "Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08257.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08257",
    "published": "2026-01-13T06:28:59Z",
    "updated": "2026-01-13T06:28:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出采用多标签评估框架重新审视无监督特征选择方法的评估，揭示单标签评估中的标签选择偏见。",
      "motivation": "论文的研究动机源于无监督特征选择方法评估中存在的不可靠性问题。现有研究通常使用单标签数据集进行评估，这些数据集通过从多标签数据中随机选择一个标签来构建，导致评估结果随标签选择任意变化，性能排名不一致。这种评估方式无法真实反映方法的判别能力，因此需要更公平和可靠的评估框架来准确比较不同方法的优劣。",
      "method": "论文方法是通过采用多标签分类框架重新评估无监督特征选择方法。在实验中，使用了21个多标签数据集和几种代表性方法进行对比分析。核心创新点在于利用多标签数据的完整标签信息来评估特征选择效果，避免了单标签设置中因标签选择带来的偏见。该方法旨在提供一个更全面的评估视角，确保评估结果的一致性和可靠性。",
      "result": "实验结果显示，在多标签评估设置下，无监督特征选择方法的性能排名与基于单标签评估报告的结果存在显著差异。这表明单标签评估可能导致方法优劣因标签选择而变化，而多标签评估能提供更一致和可靠的比较。这些发现暗示多标签评估设置可能更适合公平比较方法，尽管具体性能指标如准确率摘要未明确说明，但强调了评估方式对结果的影响。",
      "conclusion": "论文结论是重新审视了无监督特征选择的评估范式，指出多标签分类框架能提供更公平和可靠的评估方式。这项研究强调了评估方法选择的重要性，具有重要的学术价值，可推动更合理的评估标准发展，并影响实际应用中的方法选择。未来工作可能包括扩展更多数据集和探索其他评估指标，以进一步验证和优化评估框架。",
      "tags": [
        "Unsupervised Feature Selection",
        "Multi-Label Classification",
        "Evaluation Framework",
        "Pattern Classification"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:25.274078Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08254",
    "title": "Large Artificial Intelligence Model Guided Deep Reinforcement Learning for Resource Allocation in Non Terrestrial Networks",
    "authors": [
      "Abdikarim Mohamed Ibrahim",
      "Rosdiadee Nordin"
    ],
    "abstract": "Large AI Model (LAM) have been proposed to applications of Non-Terrestrial Networks (NTN), that offer better performance with its great generalization and reduced task specific trainings. In this paper, we propose a Deep Reinforcement Learning (DRL) agent that is guided by a Large Language Model (LLM). The LLM operates as a high level coordinator that generates textual guidance that shape the reward of the DRL agent during training. The results show that the LAM-DRL outperforms the traditional DRL by 40% in nominal weather scenarios and 64% in extreme weather scenarios compared to heuristics in terms of throughput, fairness, and outage probability.",
    "categories": [
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08254.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08254",
    "published": "2026-01-13T06:23:21Z",
    "updated": "2026-01-13T06:23:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种由大型语言模型指导的深度强化学习方法，用于优化非地面网络中的资源分配。",
      "motivation": "非地面网络（NTN）如卫星通信系统面临资源分配的挑战，特别是在动态天气条件下，需要高效的方法来应对吞吐量、公平性和中断概率问题。传统深度强化学习（DRL）方法可能泛化能力不足，需要大量任务特定训练，导致效率低下。大型AI模型（LAM）如大型语言模型（LLM）具有强大泛化能力，可以减少训练成本，因此本研究旨在结合LAM和DRL，通过LLM指导来解决NTN中复杂资源分配问题，提升性能和适应性。",
      "method": "论文提出了一种LAM-DRL方法，其中大型语言模型（LLM）作为高层协调器，生成文本指导来塑造深度强化学习（DRL）代理的奖励函数。这种方法在训练过程中动态调整奖励，以优化非地面网络中的资源分配任务。核心创新点在于利用LLM的泛化能力指导DRL，减少任务特定训练需求，提高学习效率。具体技术实现如模型架构和数据集摘要未明确说明，但推断该方法针对NTN场景设计。",
      "result": "实验结果表明，与启发式方法相比，LAM-DRL在名义天气场景下的吞吐量、公平性和中断概率方面比传统DRL提升了40%，在极端天气场景下提升了64%。这些数据突显了方法在复杂环境中的优越性能和鲁棒性，验证了LLM指导在增强DRL泛化能力和适应性方面的有效性，特别是在动态天气条件下。",
      "conclusion": "本研究的主要贡献是成功将大型语言模型与深度强化学习结合，提出了一种高效方法用于非地面网络资源分配。通过LLM的文本指导优化奖励函数，方法减少了训练依赖并提升了泛化能力。在名义和极端天气场景下性能均有显著提升，展示了AI模型在通信网络中的实际应用价值。未来工作可探索方法在其他复杂系统中的应用，并进一步优化LLM指导机制以应对更多挑战。",
      "tags": [
        "Large Language Model",
        "Deep Reinforcement Learning",
        "Resource Allocation",
        "Non Terrestrial Networks",
        "Reward Shaping"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:49.743186Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08253",
    "title": "LDLT L-Lipschitz Network Weight Parameterization Initialization",
    "authors": [
      "Marius F. R. Juston",
      "Ramavarapu S. Sreenivas",
      "Dustin Nottage",
      "Ahmet Soylemezoglu"
    ],
    "abstract": "We analyze initialization dynamics for LDLT-based $\\mathcal{L}$-Lipschitz layers by deriving the exact marginal output variance when the underlying parameter matrix $W_0\\in \\mathbb{R}^{m\\times n}$ is initialized with IID Gaussian entries $\\mathcal{N}(0,σ^2)$. The Wishart distribution, $S=W_0W_0^\\top\\sim\\mathcal{W}_m(n,σ^2 \\boldsymbol{I}_m)$, used for computing the output marginal variance is derived in closed form using expectations of zonal polynomials via James' theorem and a Laplace-integral expansion of $(α\\boldsymbol{I}_m+S)^{-1}$. We develop an Isserlis/Wick-based combinatorial expansion for $\\operatorname{\\mathbb{E}}\\left[\\operatorname{tr}(S^k)\\right]$ and provide explicit truncated moments up to $k=10$, which yield accurate series approximations for small-to-moderate $σ^2$. Monte Carlo experiments confirm the theoretical estimates. Furthermore, empirical analysis was performed to quantify that, using current He or Kaiming initialization with scaling $1/\\sqrt{n}$, the output variance is $0.41$, whereas the new parameterization with $10/ \\sqrt{n}$ for $α=1$ results in an output variance of $0.9$. The findings clarify why deep $\\mathcal{L}$-Lipschitz networks suffer rapid information loss at initialization and offer practical prescriptions for choosing initialization hyperparameters to mitigate this effect. However, using the Higgs boson classification dataset, a hyperparameter sweep over optimizers, initialization scale, and depth was conducted to validate the results on real-world data, showing that although the derivation ensures variance preservation, empirical results indicate He initialization still performs better.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08253.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08253",
    "published": "2026-01-13T06:20:21Z",
    "updated": "2026-01-13T06:20:21Z",
    "comment": "12 pages, 17 figures",
    "light_analysis": {
      "overview": "论文提出分析L-Lipschitz网络初始化动态的理论框架，通过推导输出方差并建议改进参数化以缓解初始化信息损失。",
      "motivation": "深度L-Lipschitz网络在初始化时信息损失快速，影响训练稳定性。现有He或Kaiming初始化方法虽常用，但输出方差较低（如0.41），可能导致梯度消失或收敛困难。因此，需精确分析初始化动态并提供参数调整方案，以优化网络训练效果。摘要未明确说明其他具体不足，但基于信息损失问题强调了研究重要性。",
      "method": "研究基于LDLT参数化的L-Lipschitz层，当参数矩阵初始化为IID高斯分布时，推导了精确边际输出方差。使用Wishart分布、James定理和Laplace积分扩展计算闭式解，并通过Isserlis/Wick组合扩展估计迹的幂期望，提供截断矩近似（k≤10）。关键创新在于数学推导和系列近似，蒙特卡洛实验用于验证理论，实证分析比较不同初始化缩放因子，如α=1时使用10/√n。",
      "result": "理论估计显示，新参数化（α=1, 缩放10/√n）将输出方差从当前初始化的0.41提升至0.9，提高了方差保持。蒙特卡洛实验验证了推导准确性。在Higgs boson分类数据集上的超参数扫描表明，尽管新方法改善方差，He初始化在实际任务中表现更优，提示初始化策略需结合其他因素如优化器选择。对比基线方法，新参数化在理论方差上有优势，但实证性能未超越。",
      "conclusion": "研究主要贡献是澄清L-Lipschitz网络初始化信息损失机制，并提供初始化超参数选择指导，具有学术和实用价值。局限性在于实证显示He初始化在某些情况下仍更有效，暗示理论推导可能未完全捕捉实际训练动态。未来工作可探索更全面的初始化策略或结合其他优化技术，以进一步提升网络性能。摘要未明确说明具体未来方向，但基于结果推断。",
      "tags": [
        "L-Lipschitz Networks",
        "Weight Initialization",
        "Wishart Distribution",
        "Laplace Integral Expansion",
        "Monte Carlo Simulation"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:29.348164Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08251",
    "title": "Hyperbolic Heterogeneous Graph Transformer",
    "authors": [
      "Jongmin Park",
      "Seunghoon Han",
      "Hyewon Lee",
      "Won-Yong Shin",
      "Sungsu Lim"
    ],
    "abstract": "In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08251.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08251",
    "published": "2026-01-13T06:15:14Z",
    "updated": "2026-01-13T06:15:14Z",
    "comment": "14pages, 9 figures",
    "light_analysis": {
      "overview": "论文提出Hyperbolic Heterogeneous Graph Transformer (HypHGT)，通过双曲空间中的变压器架构和关系特定注意力，有效捕获异质图的全局依赖并提高效率。",
      "motivation": "研究旨在解决异质图中复杂结构如树状或层次结构的学习问题。现有双曲空间方法依赖切空间操作，易导致映射失真，且消息传递架构仅关注局部邻域信息，难以捕获全局层次结构和长程依赖，限制了表示学习效果。异质图在现实应用如社交网络中常见，准确表示对其下游任务至关重要，因此开发更有效的方法具有重要性。",
      "method": "论文提出HypHGT，完全在双曲空间中学习异质图表示。核心方法采用基于变压器的架构，自然整合局部和全局信息，避免传统消息传递的局限。创新点包括关系特定的双曲注意力机制，具有线性时间复杂度，实现高效计算同时保留不同关系类型的异构信息。设计无需频繁切空间转换，减少了失真，优化了表示学习过程。",
      "result": "实验评估显示，HypHGT在节点分类任务中一致优于最先进方法，具体表现为训练时间和内存使用显著减少，提高了计算效率。与基线方法相比，该方法在保持高性能的同时优化资源消耗，证明了其有效性和实用性。摘要未提供具体准确率数据，但强调了在标准任务上的优越表现和效率改进。",
      "conclusion": "HypHGT的主要贡献在于提出高效的双曲异质图变换器，解决了全局依赖捕获和操作效率限制。其学术价值在于结合变压器架构与双曲空间，推动异质图表示学习发展；实际应用价值在于为节点分类等任务提供更有效工具。摘要未明确说明局限性或未来工作方向，但可推测可能涉及扩展应用到其他图任务或进一步优化模型。",
      "tags": [
        "Hyperbolic Space",
        "Heterogeneous Graph",
        "Transformer",
        "Attention Mechanism",
        "Graph Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:45.567155Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08247",
    "title": "Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making",
    "authors": [
      "Liu He"
    ],
    "abstract": "Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.",
    "categories": [
      "cs.LG",
      "econ.EM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08247.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08247",
    "published": "2026-01-13T06:09:24Z",
    "updated": "2026-01-13T06:09:24Z",
    "comment": "15 pages, 9 figures",
    "light_analysis": {
      "overview": "本研究将认知偏差整合到强化学习框架中，用于金融决策，以模拟人类交易行为并改进风险调整回报。",
      "motivation": "金融市场中的人类行为因认知偏差（如过度自信、损失厌恶）而偏离理性，这影响了交易决策。传统强化学习模型基于理性代理假设，忽略了心理因素，可能导致模型无法捕捉真实市场行为，限制了金融AI系统的准确性和实用性。因此，本研究旨在解决传统RL方法忽视认知偏差的问题，强调在金融决策中考虑非理性因素的重要性，以开发更贴近现实世界的模型。",
      "method": "本研究提出了一种将认知偏差整合到强化学习框架的方法，通过在奖励结构和决策过程中引入具体偏差（如过度自信和损失厌恶）。这些偏差通过调整RL代理的奖励函数和动作选择机制来模拟人类交易行为。研究在模拟和真实世界交易环境中进行性能评估，但摘要未明确说明使用的具体模型架构、算法或数据集细节，仅提及了环境的多样性和真实性。",
      "result": "实验结果显示，整合认知偏差的强化学习模型在模拟和真实交易环境中未能产生明确或积极的改进，结果不明确或甚至负面。与标准RL代理相比，引入偏差的模型可能未表现出预期的风险调整回报提升，具体性能指标（如准确率或回报率）摘要未明确说明，只提到结果与初始假设不符，未提供详细数据对比。",
      "conclusion": "本研究的主要贡献在于揭示了将认知偏差整合到强化学习框架中的挑战，尽管结果不理想，但提供了宝贵见解，表明简单引入人类类似偏差可能不足以提升金融决策性能。这突显了在RL模型中准确建模非理性行为的复杂性，为开发更稳健的金融AI系统提供了指导，并可能启发未来研究探索更精细的偏差整合方法或其他心理因素。",
      "tags": [
        "Reinforcement Learning",
        "Cognitive Biases",
        "Financial Decision-Making",
        "Reward Structures",
        "Trading Simulation"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:47.618969Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08241",
    "title": "Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence",
    "authors": [
      "Michele Fiori",
      "Gabriele Civitarese",
      "Marco Colussi",
      "Claudio Bettini"
    ],
    "abstract": "Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.",
    "categories": [
      "cs.CV",
      "cs.DC"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08241.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08241",
    "published": "2026-01-13T05:58:24Z",
    "updated": "2026-01-13T05:58:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出事件分割和置信度估计方法，改进基于大语言模型的零样本日常生活活动识别。",
      "motivation": "研究动机源于智能家居中基于传感器的ADL识别对医疗、安全等应用的重要性。现有基于大语言模型的零样本方法虽无需标注数据，但依赖时间分割，未能有效利用LLM的上下文推理能力，且缺乏预测置信度估计。这些问题导致识别方法在复杂场景中鲁棒性不足，限制了实际部署的可靠性。",
      "method": "研究方法采用基于事件的分割技术，以替代时间分割，更好地与LLM的上下文推理能力对齐；同时开发新颖的置信度估计算法，量化预测可靠性。关键创新点包括事件分割的上下文优化和置信度测量。实验中使用了复杂、真实的数据集，并采用较小规模的大语言模型如Gemma 3 27B进行验证。",
      "result": "实验结果表明，事件分割方法在复杂真实数据集上一致优于基于时间分割的LLM方法，性能甚至超越监督数据驱动方法。具体地，即使使用较小LLM如Gemma 3 27B，也能实现显著提升。置信度估计能有效区分正确与错误的预测，增强了系统可靠性。",
      "conclusion": "论文的主要贡献是提出事件分割和置信度估计方法，提升了基于大语言模型的零样本ADL识别性能。学术价值在于更好地利用LLM的上下文推理能力，实际应用价值在于提供更准确、可靠的智能家居识别系统。未来工作可能涉及扩展到更大模型或更多任务场景，局限性摘要未明确说明。",
      "tags": [
        "Large Language Models",
        "Zero-shot Learning",
        "ADL Recognition",
        "Event-based Segmentation",
        "Confidence Estimation"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:48.174998Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08237",
    "title": "The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination",
    "authors": [
      "Haoran Su",
      "Yandong Sun",
      "Congjia Yu"
    ],
    "abstract": "Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08237.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08237",
    "published": "2026-01-13T05:47:18Z",
    "updated": "2026-01-13T05:47:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "大型语言模型推动多智能体协调从手工奖励工程转向语言基础的目标规范。",
      "motivation": "本研究旨在解决多智能体强化学习中奖励工程的根本挑战，即手动指定奖励函数以诱导期望行为的困难性。该问题的重要性在于多智能体系统（如机器人协作或游戏AI）需要高效协调，但现有方法依赖人工设计奖励，面临信用分配模糊、环境非平稳性和交互复杂性增长的局限，导致效率低下且难以适应动态变化，凸显了自动化解决方案的迫切需求。",
      "method": "论文提出利用大型语言模型（LLMs）重新定义多智能体协调，核心方法是通过自然语言描述生成或调整奖励函数。关键创新点包括语义奖励规范、动态奖励适应和与人类意图对齐的三维转变，借鉴先前工作如EUREKA（从语言合成奖励）和CARD（在线适应奖励），并基于强化学习从可验证奖励（RLVR）的范例，探索语言监督作为传统奖励工程的替代方案，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "摘要未明确说明具体实验结果或性能指标，但基于现有信息，强化学习从可验证奖励（RLVR）提供了实证证据，表明语言介导的监督可以作为传统奖励工程的可行替代。与基线方法（如手工奖励工程）相比，语言基础方法可能减少人工干预并提高适应性，然而缺乏具体数据支撑如准确率提升或效率改进，更多侧重于理论证据和先前工作的引用。",
      "conclusion": "论文的主要贡献是概念化大型语言模型（LLMs）在多智能体协调中替代奖励工程的转变，强调语义表示的重要性。学术上为多智能体强化学习开辟新研究方向；实际上有望降低奖励工程成本，提升系统智能和适应性。局限性包括开放挑战如计算开销、对幻觉的鲁棒性和大规模系统的可扩展性；未来工作方向包括探索基于共享语义表示的协调机制。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Reinforcement Learning",
        "Reward Engineering",
        "Reinforcement Learning from Verifiable Rewards",
        "Semantic Specification"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:00.942377Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08235",
    "title": "MPCI-Bench: A Benchmark for Multimodal Pairwise Contextual Integrity Evaluation of Language Model Agents",
    "authors": [
      "Shouju Wang",
      "Haopeng Zhang"
    ],
    "abstract": "As language-model agents evolve from passive chatbots into proactive assistants that handle personal data, evaluating their adherence to social norms becomes increasingly critical, often through the lens of Contextual Integrity (CI). However, existing CI benchmarks are largely text-centric and primarily emphasize negative refusal scenarios, overlooking multimodal privacy risks and the fundamental trade-off between privacy and utility. In this paper, we introduce MPCI-Bench, the first Multimodal Pairwise Contextual Integrity benchmark for evaluating privacy behavior in agentic settings. MPCI-Bench consists of paired positive and negative instances derived from the same visual source and instantiated across three tiers: normative Seed judgments, context-rich Story reasoning, and executable agent action Traces. Data quality is ensured through a Tri-Principle Iterative Refinement pipeline. Evaluations of state-of-the-art multimodal models reveal systematic failures to balance privacy and utility and a pronounced modality leakage gap, where sensitive visual information is leaked more frequently than textual information. We will open-source MPCI-Bench to facilitate future research on agentic CI.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08235.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08235",
    "published": "2026-01-13T05:39:43Z",
    "updated": "2026-01-13T05:39:43Z",
    "comment": "Submitted to ACL 2026",
    "light_analysis": {
      "overview": "本文提出了MPCI-Bench，首个用于评估语言模型代理隐私行为的多模态成对上下文完整性基准。",
      "motivation": "随着语言模型代理从被动聊天机器人演变为主动处理个人数据的助手，评估其遵守社会规范（如上下文完整性）变得至关重要。现有基准多为文本中心，侧重于负面拒绝场景，忽视了多模态隐私风险和隐私与效用之间的基本权衡，导致无法全面评估代理在真实场景中的隐私行为，因此亟需开发新基准来弥补这些不足。",
      "method": "论文引入了MPCI-Bench基准，包括从相同视觉源派生的成对正负实例，实例化在三个层次：规范性种子判断、上下文丰富故事推理和可执行代理行动轨迹。通过三原则迭代精炼管道确保数据质量，提升基准的可靠性和有效性，以全面评估多模态隐私风险。",
      "result": "评估了当前最先进的多模态模型，结果显示这些模型在平衡隐私和效用方面存在系统性失败，并表现出明显的模态泄漏差距，其中敏感视觉信息比文本信息更频繁地泄露，突显了现有方法在多模态场景下的局限性。",
      "conclusion": "论文的主要贡献是开发了MPCI-Bench基准，填补了多模态隐私评估的空白，促进了代理上下文完整性研究。开源此基准具有重要的学术和实际应用价值，未来工作可扩展到更多模型和场景，以进一步优化隐私保护机制。",
      "tags": [
        "Multimodal Benchmark",
        "Contextual Integrity",
        "Privacy Evaluation",
        "Language Model Agents",
        "Data Leakage"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:40.234982Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08230",
    "title": "GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition",
    "authors": [
      "Hao Deng",
      "Bo Liu"
    ],
    "abstract": "While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08230.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08230",
    "published": "2026-01-13T05:25:32Z",
    "updated": "2026-01-13T05:25:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "GADPN 提出一种自适应图结构学习框架，通过贝叶斯优化和奇异值分解实现高效去噪与扰动，提升图神经网络性能。",
      "motivation": "图神经网络在图结构数据上表现优异，但其性能受限于观测图的质量，如噪声、缺失链接或结构不匹配。现有图结构学习方法常使用复杂生成模型和迭代联合优化，导致高计算成本，限制了实际应用。因此，开发一种更高效的框架至关重要，以自适应优化图拓扑，解决不同图类型的异质性问题。",
      "method": "GADPN 框架通过低秩去噪和广义结构扰动自适应优化图拓扑。核心创新包括：利用贝叶斯优化动态确定去噪强度，适应图的同质性水平；通过奇异值分解扩展结构扰动方法，使其适用于任意图结构，克服了原方法仅限对称图的限制。框架设计简单，避免了复杂模型，提高了计算效率，适用于多样化的图数据。",
      "result": "在基准数据集上的大量实验表明，GADPN 实现了最先进的性能，同时显著提升了效率。特别是在挑战性不协调图上表现出色，验证了其在不同网络类型中鲁棒学习增强图结构的能力。与现有方法相比，GADPN 在性能上有所超越，但摘要未明确给出具体准确率或效率提升数据。",
      "conclusion": "本研究提出GADPN，一种高效自适应的图结构学习框架，通过贝叶斯优化和奇异值分解解决了现有方法的计算瓶颈和适用性限制。其学术价值在于推动图结构学习领域发展，实际应用价值在于可能提升图神经网络在各种图数据上的性能。未来工作可探索更广泛的图类型和应用场景。",
      "tags": [
        "Graph Neural Networks",
        "Graph Structure Learning",
        "Bayesian Optimization",
        "Singular Value Decomposition",
        "Low-Rank Denoising"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:46.613912Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08226",
    "title": "Knowledge-based learning in Text-RAG and Image-RAG",
    "authors": [
      "Alexander Shim",
      "Khalil Saieh",
      "Samuel Clarke"
    ],
    "abstract": "This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con\"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08226.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08226",
    "published": "2026-01-13T05:14:18Z",
    "updated": "2026-01-13T05:14:18Z",
    "comment": "9 pages, 10 figures",
    "light_analysis": {
      "overview": "提出了一种结合文本和图像检索增强生成的多模态方法，用于减少大型语言模型在胸部X光图像疾病检测中的幻觉问题并改善校准。",
      "motivation": "该研究旨在解决AI在医学图像分析中常见的幻觉问题，这可能导致误诊并影响诊断可靠性。背景中，现有方法在结合视觉和语言模型处理多模态数据时，可能缺乏有效的外部知识整合和校准机制，导致预测不准确。因此，通过比较文本和图像RAG方法，探索更可靠的多模态解决方案，以提升胸部X光疾病检测的准确性和实用性。摘要未明确说明具体现有方法的详细不足之处。",
      "method": "研究方法采用基于Vision Transformer (EVA-ViT)的图像编码器，结合大型语言模型（如LlaMA或ChatGPT）。文本RAG通过检索外部知识信息来减少幻觉问题；图像RAG则使用K-Nearest Neighbors (KNN)方法改善预测置信度和校准。使用NIH Chest X-ray数据集进行模型训练，并比较图像RAG、文本RAG和基线方法的性能。核心创新在于集成视觉Transformer、LLM和RAG机制，构建多模态框架以增强知识整合和校准能力。",
      "result": "实验结果显示，文本RAG能有效减少幻觉问题，而图像RAG通过KNN方法提升了预测置信度和校准。具体地，GPT大型语言模型相比LlaMA模型表现出更好的性能，具有更低的幻觉率和更优的期望校准误差 (ECE)。尽管摘要未提供具体数值数据，但整体性能表明多模态RAG方法在减少幻觉和改善校准方面优于基线，有助于提高胸部X光疾病检测的可靠性。",
      "conclusion": "本研究验证了文本和图像RAG方法在减少幻觉和改善校准方面的有效性，主要贡献是提出了一种结合视觉和语言模型的多模态框架，增强了医学图像分析的准确性和可靠性。学术价值在于推动了检索增强生成在多模态领域的应用；实际应用可提升胸部X光疾病检测的诊断效率。局限性包括数据不平衡和复杂多阶段结构的挑战；未来工作建议构建大型经验环境和平衡使用示例，以优化模型部署。",
      "tags": [
        "Vision Transformer",
        "Large Language Model",
        "Retrieval-Augmented Generation",
        "K-Nearest Neighbors",
        "Multi-modal Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:13.854922Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08225",
    "title": "User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale",
    "authors": [
      "Jungho Cho",
      "Minbyul Jeong",
      "Sungrae Park"
    ],
    "abstract": "The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in \"solely task-solving\" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08225.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08225",
    "published": "2026-01-13T05:14:09Z",
    "updated": "2026-01-13T05:14:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出一种用户导向的多轮对话生成框架，通过模拟人类行为规则动态生成高密度工具使用数据，以改善真实人类-代理协作。",
      "motivation": "随着大型推理模型作为自主代理的兴起，对复杂多轮工具使用能力的需求增加，但现有数据集和生成方法受限于静态、预定义工具集，无法扩展到开放人类-代理协作的复杂性。任务导向设计往往导致最小交互对话，轮次少，不足以模拟现实场景中迭代式问题解决过程，这限制了对话系统的真实性和实用性。因此，研究旨在通过用户导向模拟生成更真实、扩展的多轮对话，以支持高质量数据收集和模型训练。",
      "method": "论文提出一个自动化多轮对话生成框架，首先使用基于大型推理模型的模拟器生成任务导向对话，但观察到交互不足。核心创新是转向用户导向模拟，将任务生成与专用用户模拟器解耦，该模拟器模仿人类行为规则如增量请求和逐轮反馈，以促进更真实、扩展的对话。生成管道设计为可插拔模块，可从任何状态启动，确保高可扩展性，并能在单个对话轨迹中完成多个任务，从而产生高密度数据集。",
      "result": "摘要未明确说明具体的实验结果或性能指标，如准确率或效率改进。但论文声称通过用户导向模拟生成了具有高轮次和真实性的多轮对话数据集，这些对话反映了现实世界中迭代式问题解决的特点，可能提高了对话的复杂度和实用性。与基线方法相比，该方法增强了数据的多样性和真实感，适用于开放环境下的工具使用研究。",
      "conclusion": "该研究的主要贡献是开发了一个用户导向的多轮对话生成框架，能够大规模产生高密度、真实的工具使用对话数据，这有助于改善对话系统训练和促进人类-代理交互的真实模拟。研究的学术价值在于提供了一种灵活的数据生成方法，推动工具使用模型在开放环境中的应用；实际应用价值包括支持协作任务和增强模型适应性。未来工作可能涉及优化用户模拟规则或扩展到更多领域。",
      "tags": [
        "Large Reasoning Models",
        "Multi-Turn Dialogue Generation",
        "Tool Use",
        "User Simulation",
        "Dialogue Data Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:05.496207Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08224",
    "title": "An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3",
    "authors": [
      "Daesuk Kwon",
      "Won-gi Paeng"
    ],
    "abstract": "General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08224.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08224",
    "published": "2026-01-13T05:06:07Z",
    "updated": "2026-01-13T05:06:07Z",
    "comment": "20 pages, 3 tables",
    "light_analysis": {
      "overview": "本研究提出了SANC(E3)公理化框架，通过能量函数E3最小化实现表示单元的自组织，从而统一了感知、想象、预测和行动等认知过程。",
      "motivation": "通用智能系统需在有限资源下重组经验以支持预测和行动，但现有AI系统通常预设固定基本单元（如标记或像素），这回避了表示单元如何自动形成和稳定的核心问题。SANC(E3)旨在解决这一不足，因为自组织表示单元对于构建适应性强的智能体至关重要，能够避免先验假设的限制，提升系统灵活性和理论深度，从而为通用智能奠定基础。",
      "method": "SANC(E3)是一个基于公理的系统，包含五个核心公理：有限容量、共现关联、基于相似性的竞争、基于信心的稳定化以及重建-压缩-更新的权衡。核心方法是通过能量函数E3的最小化来控制表示单元的竞争选择、重建和压缩，实现自组织表示。关键创新包括伪内存映射I/O机制，使内部回放的格式塔与外部输入统一处理，从而将感知、想象等认知活动整合到单一表示过程中。摘要未明确说明具体数据集或模型架构。",
      "result": "从公理推导出十二个命题，表明类别形成、层次组织、无监督学习和高级认知活动都可以在E3最小化下解释为格式塔完成的实例。这些理论结果展示了框架的普适性和解释力，提供了对认知过程统一理解的基础。摘要未提供具体实验数据（如准确率或效率指标），但强调了理论推导，暗示其在理论上的有效性，与基线方法的对比未明确说明。",
      "conclusion": "论文的主要贡献是提出SANC(E3)公理化框架，通过能量最小化实现表示单元的自组织，统一了多种认知功能，避免了先验固定单元的假设。其学术价值在于为通用智能提供新的理论基础和方法论，促进了对智能本质的理解。实际应用价值可能包括AI系统设计改进。未来工作方向可能涉及具体实现和实验验证，但摘要未明确说明局限性或进一步细节。",
      "tags": [
        "Axiomatic Systems",
        "Self-Organization",
        "Energy Minimization",
        "Gestalt Completion",
        "Unsupervised Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:06.972546Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08216",
    "title": "One-Shot Federated Ridge Regression: Exact Recovery via Sufficient Statistic Aggregation",
    "authors": [
      "Zahir Alsulaimawi"
    ],
    "abstract": "Federated learning protocols require repeated synchronization between clients and a central server, with convergence rates depending on learning rates, data heterogeneity, and client sampling. This paper asks whether iterative communication is necessary for distributed linear regression. We show it is not. We formulate federated ridge regression as a distributed equilibrium problem where each client computes local sufficient statistics -- the Gram matrix and moment vector -- and transmits them once. The server reconstructs the global solution through a single matrix inversion. We prove exact recovery: under a coverage condition on client feature matrices, one-shot aggregation yields the centralized ridge solution, not an approximation. For heterogeneous distributions violating coverage, we derive non-asymptotic error bounds depending on spectral properties of the aggregated Gram matrix. Communication reduces from $\\mathcal{O}(Rd)$ in iterative methods to $\\mathcal{O}(d^2)$ total; for high-dimensional settings, we propose and experimentally validate random projection techniques reducing this to $\\mathcal{O}(m^2)$ where $m \\ll d$. We establish differential privacy guarantees where noise is injected once per client, eliminating the composition penalty that degrades privacy in multi-round protocols. We further address practical considerations including client dropout robustness, federated cross-validation for hyperparameter selection, and comparison with gradient-based alternatives. Comprehensive experiments on synthetic heterogeneous regression demonstrate that one-shot fusion matches FedAvg accuracy while requiring up to $38\\times$ less communication. The framework applies to kernel methods and random feature models but not to general nonlinear architectures.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08216.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08216",
    "published": "2026-01-13T04:47:22Z",
    "updated": "2026-01-13T04:47:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种一次性联邦岭回归方法，通过聚合客户端充分统计量实现精确恢复，避免了迭代通信需求，显著降低通信开销。",
      "motivation": "联邦学习协议通常需要客户端和服务器反复同步，导致高通信成本，且收敛速度受学习率、数据异质性和客户端采样影响。现有方法如FedAvg依赖多轮迭代，通信效率低下，同时在多轮协议中隐私保护可能因组合惩罚而降低。本研究旨在探索是否可通过一次性通信解决分布式线性回归问题，以减轻通信负担并提升隐私性。",
      "method": "该方法将联邦岭回归建模为分布式均衡问题，每个客户端计算本地充分统计量（Gram矩阵和矩向量），并仅传输一次至服务器。服务器通过单次矩阵求逆重建全局岭回归解。关键创新点包括引入覆盖条件保证精确恢复，以及针对高维数据提出随机投影技术，将通信复杂度从O(d²)降至O(m²)，其中m远小于d。此外，方法支持差分隐私注入噪声，并涉及客户端掉线鲁棒性和联邦交叉验证等实践考量。",
      "result": "实验在合成异质回归数据上进行，结果显示一次性联邦岭回归与FedAvg在准确度上相当，但通信开销降低高达38倍。通信复杂度从迭代方法的O(Rd)降至O(d²)，并验证了随机投影技术进一步减少至O(m²)，其中m为投影维度。与基线方法对比，该方法在维持性能的同时大幅提升了通信效率。",
      "conclusion": "本研究证明通过充分统计聚合，联邦岭回归可实现一次性精确恢复，无需迭代通信，从而降低通信成本并增强差分隐私保护。其学术价值在于展示了分布式学习中的通信效率优化，实际应用适用于核方法和随机特征模型，但局限性在于不适用于一般非线性架构。未来工作可探索扩展到更广泛模型或结合其他优化技术。",
      "tags": [
        "Federated Learning",
        "Ridge Regression",
        "Sufficient Statistics",
        "Random Projection",
        "Differential Privacy"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:08.888181Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08215",
    "title": "Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints",
    "authors": [
      "Seng Pei Liew",
      "Kenta Shinzato",
      "Yuyang Dong"
    ],
    "abstract": "Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).   Moreover, $n_{exp}$ and $n_{topk}$ do not \"cancel out\" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08215.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08215",
    "published": "2026-01-13T04:42:45Z",
    "updated": "2026-01-13T04:42:45Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "论文提出了基于总参数和专家稀疏度的Mixture-of-Experts语言模型设计原则，以在内存和推理约束下优化架构性能。",
      "motivation": "现代Mixture-of-Experts语言模型通常基于总参数（内存占用）和激活参数（推理成本）设计，但这些因素不足以确定最优架构，导致设计歧义和性能限制。在资源受限环境中，准确理解性能决定因素至关重要，现有方法忽略了专家稀疏度等变量，未能高效平衡模型效率与成本，限制了MoE模型的实际部署和应用潜力。",
      "method": "研究通过系统分析，揭示MoE性能主要由总参数（N_total）和专家稀疏度（s := n_exp / n_topk）决定。关键技术发现是专家数n_exp和topk值n_topk在稀疏比中不相互抵消；增加专家数会轻微降低性能，因为它迫使减少核心模型的深度和宽度以满足内存约束。摘要未明确说明具体数据集或模型架构，但推断研究基于理论或实验分析，可能使用标准MoE模型变体进行验证。",
      "result": "实验结果表​​明，MoE性能主要受总参数和专家稀疏度影响；增加专家数n_exp会轻微惩罚性能，因为它需要调整模型维度。这些发现促成了新设计原则，即在约束下最大化总参数、最小化稀疏度和专家数。摘要未提供具体性能指标如准确率提升，但与基线方法相比，该框架为架构优化提供了明确指导，有望提高模型效率。",
      "conclusion": "论文的主要贡献是提出一个简单原则：在内存和推理约束下，最大化总参数，同时最小化专家稀疏度（通过最大化n_topk）和专家数。这提供了稳健框架来解决架构歧义，具有重要学术价值，并为实际应用中的模型优化奠定了基础。潜在局限性可能包括原则在不同场景下的适用性，未来工作可扩展验证其泛化能力或集成更多约束因素。",
      "tags": [
        "Mixture-of-Experts (MoE)",
        "Language Models",
        "Model Architecture",
        "Sparsity",
        "Memory Constraints"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:57.829677Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08211",
    "title": "Adapting Rules of Official International Mahjong for Online Players",
    "authors": [
      "Chucai Wang",
      "Lingfeng Li",
      "Yunlong Lu",
      "Wenxin Li"
    ],
    "abstract": "As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08211.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08211",
    "published": "2026-01-13T04:36:05Z",
    "updated": "2026-01-13T04:36:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文利用AI数据分析评估官方国际麻将的游戏平衡性，并提出适用于在线玩家的规则调整，如引入补偿分机制。",
      "motivation": "在线麻将玩家具有碎片化游戏时间和不固定对手的特点，而官方国际麻将规则设计基于线下固定对手的多轮游戏。这导致在线单轮游戏中存在不公平性，如先手优势和计分设置问题，需要调整规则以适应在线环境。现有方法通过多轮旋转位置来平衡先手优势，但这对在线玩家不便，因此研究旨在修改规则以确保公平性和便利性。",
      "method": "论文采用世界冠军AI系统进行自博弈竞争，收集数据并进行统计分析。关键创新点是利用数据驱动方法识别游戏平衡问题，如先手优势和子目标计分不足。基于分析结果，提出规则调整，包括每轮引入补偿分以平衡先手优势，并优化不同牌型的计分设置。数据集和AI模型细节摘要未明确说明，但方法侧重于自博弈和统计技术。",
      "result": "研究通过AI自博弈数据分析，确认了官方国际麻将在线单轮游戏中的先手优势，并发现了子目标计分设置的问题。提出的补偿分机制相比传统多轮旋转位置方法，在在线环境中更便捷实用。修订后的麻将游戏已实现在线版本供玩家使用，但具体性能指标如公平性提升程度摘要未明确说明，仅强调规则调整的实用性。",
      "conclusion": "论文的主要贡献是利用AI数据分析评估官方国际麻将的游戏平衡，并提出针对在线环境的规则调整。学术上，这是数据驱动方法在传统游戏适应中的创新应用；实际上，修订版规则提高了在线玩家的公平性和便利性。作为初始尝试，未来可进一步优化AI分析或扩展到其他游戏，局限性包括AI模型细节未详述。",
      "tags": [
        "Self-Play",
        "Statistical Analysis",
        "Game Balance",
        "Rule Adaptation",
        "Online Gaming"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:41.759159Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08210",
    "title": "Scalable Multiagent Reinforcement Learning with Collective Influence Estimation",
    "authors": [
      "Zhenglong Luo",
      "Zhiyong Chen",
      "Aoxiang Liu",
      "Ke Pan"
    ],
    "abstract": "Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.   To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08210.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08210",
    "published": "2026-01-13T04:24:11Z",
    "updated": "2026-01-13T04:24:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于集体影响力估计网络的多智能体强化学习框架，实现无需显式通信的高效协调和强可扩展性。",
      "motivation": "多智能体强化学习在处理复杂合作任务方面潜力巨大，但现有方法通常依赖智能体间频繁交换动作或状态信息，这在如机器人系统的实际应用中难以实现。常见解决方案使用估计器网络预测其他智能体行为，但网络规模和计算成本随智能体数量快速增长，导致大规模系统可扩展性受限。因此，亟需一种新方法以减少通信依赖并提升可扩展性。",
      "method": "本研究提出一个增强的多智能体强化学习框架，集成集体影响力估计网络（CIEN）。CIEN通过建模其他智能体对任务对象的集体影响力，使每个智能体仅从局部观测和任务对象状态推断关键交互信息，从而无需显式动作信息交换。关键创新在于用集体影响力替代直接动作预测，降低网络复杂度。框架基于Soft Actor-Critic算法实现，新智能体加入时无需修改现有网络结构，显著提升了可扩展性。",
      "result": "在基于Soft Actor-Critic算法的多智能体合作任务实验中，所提方法在通信受限环境下实现了稳定高效的协调性能。与基线方法相比，该方法避免了网络规模随智能体数量扩展的问题。真实机器人平台部署结果显示，使用集体影响力建模训练的策略显著提高了鲁棒性和部署可行性，同时减少了对通信基础设施的依赖。",
      "conclusion": "本文的主要贡献是开发了一个基于集体影响力估计网络的多智能体强化学习框架，有效解决了通信受限和可扩展性问题。学术上，为大规模多智能体系统提供了高效协同学习的新途径；实际上，该方法适用于真实机器人环境，增强了部署的鲁棒性和实用性。未来工作摘要未明确说明，可能涉及进一步优化网络设计或探索更广泛的应用场景。",
      "tags": [
        "Multiagent Reinforcement Learning",
        "Collective Influence Estimation",
        "Soft Actor-Critic",
        "Communication-Efficient Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:35.718030Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08209",
    "title": "Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models",
    "authors": [
      "Rongji Li",
      "Jian Xu",
      "Xueqing Chen",
      "Yisheng Yang",
      "Jiayi Wang",
      "Xingyu Chen",
      "Chunyu Xie",
      "Dawei Leng",
      "Xu-Yao Zhang"
    ],
    "abstract": "In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08209.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08209",
    "published": "2026-01-13T04:23:36Z",
    "updated": "2026-01-13T04:23:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了Generation-Augmented Generation (GAG)框架，通过表示级别接口在大语言模型中即插即用注入私有知识，以解决微调和检索增强生成的局限性。",
      "motivation": "在生物医学、材料和金融等高风险领域，大语言模型的部署需要注入私有、领域特定知识，这些知识专有、快速更新且在公共预训练中代表不足。现有方法如微调和检索增强生成（RAG）存在显著缺点：微调迭代成本高，持续更新易导致灾难性遗忘和通用能力下降；RAG在专业私有语料中因分块导致的证据碎片化、检索漂移和长上下文压力而表现脆弱，因此亟需一种更高效可靠的知识注入方法。",
      "method": "本文受多模态大语言模型对齐异构模态的启发，提出Generation-Augmented Generation (GAG)方法。该方法将私有专业知识视为额外的专家模态，通过一个紧凑的表示级别接口注入到冻结的基模型中，避免提示时的证据序列化。关键技术特色是实现了私有模态与基模型语义空间的对齐，支持即插即用的专业化和可扩展多域组合，并通过可靠的选择性激活来管理不同领域知识。摘要未明确说明具体模型架构或数据集细节。",
      "result": "在免疫学佐剂和催化材料两个私有科学问答基准测试上，GAG相较于强大的RAG基线分别提升了15.34%和14.86%的专业性能。在混合域评估中，GAG保持了在六个开放通用基准测试上的性能不变。此外，GAG实现了近乎预言的选择性激活，有效支持了可扩展的多域部署，展示了其在私有知识注入方面的显著优势。",
      "conclusion": "GAG框架的主要贡献是提供了一种新颖的私有知识注入方法，克服了微调和RAG的缺点。学术上，它通过表示级别对齐提出新思路；实际上，它支持即插即用的专业化和可扩展多域部署，具有在生物医学、材料等领域的应用价值。未来工作方向可能包括优化接口效率或扩展更多领域，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Models",
        "Retrieval-Augmented Generation",
        "Generation-Augmented Generation",
        "Private Knowledge Injection",
        "Multimodal Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:25.520834Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08205",
    "title": "FUME: Fused Unified Multi-Gas Emission Network for Livestock Rumen Acidosis Detection",
    "authors": [
      "Taminul Islam",
      "Toqi Tahamid Sarker",
      "Mohamed Embaby",
      "Khaled R Ahmed",
      "Amer AbuGhazaleh"
    ],
    "abstract": "Ruminal acidosis is a prevalent metabolic disorder in dairy cattle causing significant economic losses and animal welfare concerns. Current diagnostic methods rely on invasive pH measurement, limiting scalability for continuous monitoring. We present FUME (Fused Unified Multi-gas Emission Network), the first deep learning approach for rumen acidosis detection from dual-gas optical imaging under in vitro conditions. Our method leverages complementary carbon dioxide (CO2) and methane (CH4) emission patterns captured by infrared cameras to classify rumen health into Healthy, Transitional, and Acidotic states. FUME employs a lightweight dual-stream architecture with weight-shared encoders, modality-specific self-attention, and channel attention fusion, jointly optimizing gas plume segmentation and classification of dairy cattle health. We introduce the first dual-gas OGI dataset comprising 8,967 annotated frames across six pH levels with pixel-level segmentation masks. Experiments demonstrate that FUME achieves 80.99% mIoU and 98.82% classification accuracy while using only 1.28M parameters and 1.97G MACs--outperforming state-of-the-art methods in segmentation quality with 10x lower computational cost. Ablation studies reveal that CO2 provides the primary discriminative signal and dual-task learning is essential for optimal performance. Our work establishes the feasibility of gas emission-based livestock health monitoring, paving the way for practical, in vitro acidosis detection systems. Codes are available at https://github.com/taminulislam/fume.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08205.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08205",
    "published": "2026-01-13T04:17:22Z",
    "updated": "2026-01-13T04:17:22Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "FUME提出首个基于双气体光学成像的深度学习网络，用于瘤胃酸中毒检测，通过轻量架构实现高效分割和分类。",
      "motivation": "瘤胃酸中毒是奶牛常见代谢障碍，造成重大经济损失和动物福利问题。现有诊断方法依赖侵入性pH测量，难以实现连续监测，限制了规模化应用的可扩展性。因此，亟需开发非侵入、可扩展的检测技术，以提升牲畜健康管理效率并减少侵入操作带来的不便。",
      "method": "FUME采用轻量级双流架构，利用权重共享编码器处理CO2和CH4气体排放图像，结合模态特定自注意力提取气体特征，并通过通道注意力融合优化信息集成。该方法同时执行气体羽流分割和健康状态分类任务，基于首个双气体OGI数据集，包含8,967个标注帧，覆盖六个pH水平，并提供像素级分割掩码。",
      "result": "实验显示，FUME在分割任务上达到80.99% mIoU，分类准确率为98.82%，仅使用1.28M参数和1.97G MACs。与最先进方法相比，在保持高分割质量的同时，计算成本降低10倍。消融研究证实CO2排放模式是主要判别信号，双任务学习对性能提升至关重要。",
      "conclusion": "本研究确立了基于气体排放的牲畜健康监测可行性，通过提出FUME网络和双气体数据集，为开发实用体外酸中毒检测系统铺平道路。工作展示了深度学习在农业领域的应用潜力，未来可扩展至更多气体种类和体内监测场景，推动智能养殖技术发展。",
      "tags": [
        "Deep Learning",
        "Dual-stream Architecture",
        "Self-Attention",
        "Channel Attention Fusion",
        "Segmentation-Classification"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:46.604595Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08204",
    "title": "MobiDiary: Autoregressive Action Captioning with Wearable Devices and Wireless Signals",
    "authors": [
      "Fei Deng",
      "Yinghui He",
      "Chuntong Chu",
      "Ge Wang",
      "Han Ding",
      "Jinsong Han",
      "Fei Wang"
    ],
    "abstract": "Human Activity Recognition (HAR) in smart homes is critical for health monitoring and assistive living. While vision-based systems are common, they face privacy concerns and environmental limitations (e.g., occlusion). In this work, we present MobiDiary, a framework that generates natural language descriptions of daily activities directly from heterogeneous physical signals (specifically IMU and Wi-Fi). Unlike conventional approaches that restrict outputs to pre-defined labels, MobiDiary produces expressive, human-readable summaries. To bridge the semantic gap between continuous, noisy physical signals and discrete linguistic descriptions, we propose a unified sensor encoder. Instead of relying on modality-specific engineering, we exploit the shared inductive biases of motion-induced signals--where both inertial and wireless data reflect underlying kinematic dynamics. Specifically, our encoder utilizes a patch-based mechanism to capture local temporal correlations and integrates heterogeneous placement embedding to unify spatial contexts across different sensors. These unified signal tokens are then fed into a Transformer-based decoder, which employs an autoregressive mechanism to generate coherent action descriptions word-by-word. We comprehensively evaluate our approach on multiple public benchmarks (XRF V2, UWash, and WiFiTAD). Experimental results demonstrate that MobiDiary effectively generalizes across modalities, achieving state-of-the-art performance on captioning metrics (e.g., BLEU@4, CIDEr, RMC) and outperforming specialized baselines in continuous action understanding.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08204.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08204",
    "published": "2026-01-13T04:16:17Z",
    "updated": "2026-01-13T04:16:17Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "MobiDiary 框架通过统一传感器编码器和自回归 Transformer 解码器，直接从 IMU 和 Wi-Fi 信号生成自然语言活动描述。",
      "motivation": "智能家居中的人类活动识别（HAR）对健康监控和辅助生活至关重要。现有视觉系统虽然常见，但面临隐私顾虑和环境限制（如遮挡）。传统方法通常输出预定义标签，缺乏表达性和人类可读性。本研究旨在解决这些问题，通过直接从物理信号生成自然语言描述，以克服隐私和限制问题，提升 HAR 的实用性和用户友好性。",
      "method": "MobiDiary 框架包括一个统一传感器编码器，它利用 IMU 和 Wi-Fi 信号的共享感应偏置来处理异质数据。编码器采用补丁机制捕获局部时间相关性，并通过异构放置嵌入统一不同传感器的空间上下文。这些统一信号令牌随后输入到基于 Transformer 的解码器中，该解码器使用自回归机制逐词生成连贯的动作描述。实验在多个公共数据集（如 XRF V2, UWash 和 WiFiTAD）上进行，以验证方法的有效性。",
      "result": "在多个公共基准（包括 XRF V2, UWash 和 WiFiTAD）上，MobiDiary 表现出良好的跨模态泛化能力。它在标注指标（如 BLEU@4, CIDEr 和 RMC）上达到了最先进的性能，并优于专门的基线方法。摘要未明确说明具体的准确率提升数值，但强调了其在连续动作理解方面的优势，表明框架能够有效处理异质信号并生成高质量描述。",
      "conclusion": "论文的主要贡献是提出了 MobiDiary 框架，它能够从异质物理信号生成表达性的自然语言活动描述。这项研究通过统一传感器处理解决了视觉系统的隐私和限制问题，具有重要的学术价值和实际应用潜力，例如在智能家居中实现更自然的监控和交互。未来工作可能包括扩展到更多信号模态或进一步提高描述的质量和多样性。",
      "tags": [
        "Human Activity Recognition",
        "Autoregressive Decoding",
        "Transformer-based Model",
        "Sensor Fusion",
        "Wi-Fi Signals"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:46.133049Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08198",
    "title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs",
    "authors": [
      "Yibo Wang",
      "Hai-Long Sun",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Lijun Zhang"
    ],
    "abstract": "Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08198.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08198",
    "published": "2026-01-13T04:09:49Z",
    "updated": "2026-01-13T04:09:49Z",
    "comment": "NeurIPS 2025",
    "light_analysis": {
      "overview": "本文提出了一种基于三元组的自我博弈微调方法（T-SPIN），通过引入历史优势和熵约束，解决了自我博弈微调中的不稳定性和训练生成不一致问题。",
      "motivation": "该研究旨在解决自我博弈微调（SPIN）在处理大型语言模型下游任务时的缺陷。SPIN 在专家标注数据稀缺时被提出，但仅优化标注响应相对于当前合成响应的奖励优势，该优势可能在迭代中逐渐消失，导致优化过程不稳定。此外，使用参考策略导致训练时的奖励公式与生成时的指标不一致，限制了微调效果，这些问题在现实应用中限制了模型的适应性和效率。",
      "method": "T-SPIN 方法集成了两个关键设计。首先，除了当前优势外，还引入了历史优势，即迭代生成响应与初始策略产生的原型合成响应之间的比较，以维持优化的稳定性。其次，在自我博弈框架中引入熵约束，理论证明了这支持无参考的微调，从而消除训练与生成之间的差异。具体的数据集和模型架构摘要未明确说明，但该方法基于自我博弈迭代生成响应的机制。",
      "result": "在各种任务的实证研究中，T-SPIN 在性能上优于 SPIN，并表现出更稳定的迭代进化过程。与监督微调相比，T-SPIN 仅使用 25% 的样本就能实现可比甚至更好的性能，突显了其在标注数据稀缺场景下的有效性。具体性能指标如准确率摘要未详细说明，但强调了与基线的显著改进。",
      "conclusion": "T-SPIN 的主要贡献在于通过历史优势和熵约束改进了自我博弈微调，解决了稳定性和训练生成一致性问题。其学术价值在于提出了一种更有效的无参考微调方法，实际应用价值体现在能高效利用少量标注数据进行模型适应，尤其在标注数据稀缺时具有重要应用潜力。局限性或未来工作方向摘要未明确说明，但可能涉及扩展应用到更复杂的任务中。",
      "tags": [
        "Self-Play Fine-Tuning",
        "Reinforcement Learning",
        "Triplet Learning",
        "Entropy Constraint",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:03.805267Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08196",
    "title": "Evaluating Implicit Regulatory Compliance in LLM Tool Invocation via Logic-Guided Synthesis",
    "authors": [
      "Da Song",
      "Yuheng Huang",
      "Boqi Chen",
      "Tianshuo Cong",
      "Randy Goebel",
      "Lei Ma",
      "Foutse Khomh"
    ],
    "abstract": "The integration of large language models (LLMs) into autonomous agents has enabled complex tool use, yet in high-stakes domains, these systems must strictly adhere to regulatory standards beyond simple functional correctness. However, existing benchmarks often overlook implicit regulatory compliance, thus failing to evaluate whether LLMs can autonomously enforce mandatory safety constraints. To fill this gap, we introduce LogiSafetyGen, a framework that converts unstructured regulations into Linear Temporal Logic oracles and employs logic-guided fuzzing to synthesize valid, safety-critical traces. Building on this framework, we construct LogiSafetyBench, a benchmark comprising 240 human-verified tasks that require LLMs to generate Python programs that satisfy both functional objectives and latent compliance rules. Evaluations of 13 state-of-the-art (SOTA) LLMs reveal that larger models, despite achieving better functional correctness, frequently prioritize task completion over safety, which results in non-compliant behavior.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08196.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08196",
    "published": "2026-01-13T03:55:18Z",
    "updated": "2026-01-13T03:55:18Z",
    "comment": "11 pages, 3 figures",
    "light_analysis": {
      "overview": "本研究提出了LogiSafetyGen框架和LogiSafetyBench基准，通过逻辑引导的合成方法评估大语言模型在工具调用中的隐性监管合规性。",
      "motivation": "大语言模型在自主代理中的工具调用能力日益增强，但在高风险领域（如金融、医疗），这些系统必须严格遵守超出简单功能正确性的监管标准。现有基准常忽略隐性合规性，导致无法评估LLM是否自主执行强制性安全约束，这可能在实际应用中引发严重后果。因此，研究动机是填补这一空白，开发评估框架以确保LLM系统在复杂任务中兼顾功能与安全。",
      "method": "核心方法是LogiSafetyGen框架，它将非结构化监管规则转换为线性时序逻辑（LTL）预言机，并采用逻辑引导的模糊测试技术合成安全关键的有效执行轨迹。基于此，构建了LogiSafetyBench基准，包含240个人工验证的任务，要求LLM生成满足功能目标和隐性合规规则的Python程序。关键创新在于结合形式化逻辑捕捉隐性约束，并通过合成测试案例实现自动化评估。",
      "result": "评估了13个最先进的LLMs，发现尽管更大模型在功能正确性上表现更优，但它们频繁优先任务完成而忽视安全，导致非合规行为频发。这揭示了模型规模与安全优先级之间的权衡，突显了现有LLMs在隐性监管合规方面的不足，但具体性能指标摘要未明确说明。",
      "conclusion": "论文贡献了评估LLM隐性监管合规性的新框架和基准，提升了LLM安全评估的标准。学术价值在于将形式化逻辑引入测试方法，实际应用价值在于促进高风险领域LLM系统的可靠部署。局限性可能在于基准覆盖范围有限，未来工作可扩展至更多监管场景或改进模型训练以增强合规意识。",
      "tags": [
        "Large Language Models",
        "Tool Invocation",
        "Linear Temporal Logic",
        "Logic-Guided Fuzzing",
        "Regulatory Compliance Benchmark"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:32.559330Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08193",
    "title": "Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style",
    "authors": [
      "Mengqi Wu",
      "Yongheng Sun",
      "Qianqian Wang",
      "Pew-Thian Yap",
      "Mingxia Liu"
    ],
    "abstract": "Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08193.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08193",
    "published": "2026-01-13T03:47:23Z",
    "updated": "2026-01-13T03:47:23Z",
    "comment": "15 pages, 10 figures. Extended version of a paper published at MICCAI 2025 (DOI: 10.1007/978-3-032-04947-6_65)",
    "light_analysis": {
      "overview": "本研究提出了MMH框架，通过生物医学语义先验实现多站点多序列脑部MRI的协调，无需配对数据即可分离风格和解剖。",
      "motivation": "聚合多站点脑部MRI数据时，站点间差异（如扫描仪供应商和协议）引入非生物异质性，削弱深度学习模型的泛化能力。现有方法存在不足：它们常依赖有限的配对旅行者数据，未能有效分离图像风格与解剖结构，且多仅支持单序列协调，限制了实际多序列MRI采集场景中的应用。因此，开发更通用、无需配对的协调方法至关重要，以提升数据质量和模型性能。",
      "method": "MMH采用两阶段架构：首先，基于扩散的全局协调器使用风格无关梯度条件将MR图像映射到序列特定的统一域；其次，目标特定的微调器将全局对齐图像适应到所需目标域。关键创新在于引入三平面注意力BiomedCLIP编码器，它聚合多视图嵌入来表征体积风格信息，实现风格与解剖的明确分离，无需依赖配对数据，从而支持序列感知的风格对齐。",
      "result": "在4,163个T1-和T2-加权MRI上进行评估，MMH在多个指标上优于最先进方法：图像特征聚类显示更好的数据一致性，体素级比较表明更高的对齐精度，组织分割任务中分割准确性提升，下游年龄和站点分类的泛化能力增强。这些结果基于标准评测任务，具体性能指标如准确率提升未在摘要中明确说明，但整体表现为协调效果的显著改进。",
      "conclusion": "MMH的主要贡献是开发了一个统一框架，支持多站点多序列脑部MRI协调，利用生物医学语义先验实现无需配对的风格分离。学术价值在于推进了图像协调技术，提高数据可用性；实际应用价值在于增强深度学习模型的训练效率和泛化能力。潜在局限性或未来工作方向在摘要中未明确说明，可能涉及扩展其他序列或评估更广泛数据集。",
      "tags": [
        "Multi-Site MRI Harmonization",
        "Diffusion Models",
        "BiomedCLIP",
        "Attention Mechanism",
        "Multi-View Embeddings"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:34.479076Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08192",
    "title": "Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging",
    "authors": [
      "Md. Faiyaz Abdullah Sayeedi",
      "Rashedur Rahman",
      "Siam Tahsin Bhuiyan",
      "Sefatul Wasi",
      "Ashraful Islam",
      "Saadia Binte Alam",
      "AKM Mahbubur Rahman"
    ],
    "abstract": "Medical image analysis increasingly relies on large vision-language models (VLMs), yet most systems remain single-pass black boxes that offer limited control over reasoning, safety, and spatial grounding. We propose R^4, an agentic framework that decomposes medical imaging workflows into four coordinated agents: a Router that configures task- and specialization-aware prompts from the image, patient history, and metadata; a Retriever that uses exemplar memory and pass@k sampling to jointly generate free-text reports and bounding boxes; a Reflector that critiques each draft-box pair for key clinical error modes (negation, laterality, unsupported claims, contradictions, missing findings, and localization errors); and a Repairer that iteratively revises both narrative and spatial outputs under targeted constraints while curating high-quality exemplars for future cases. Instantiated on chest X-ray analysis with multiple modern VLM backbones and evaluated on report generation and weakly supervised detection, R^4 consistently boosts LLM-as-a-Judge scores by roughly +1.7-+2.5 points and mAP50 by +2.5-+3.5 absolute points over strong single-VLM baselines, without any gradient-based fine-tuning. These results show that agentic routing, reflection, and repair can turn strong but brittle VLMs into more reliable and better grounded tools for clinical image interpretation. Our code can be found at: https://github.com/faiyazabdullah/MultimodalMedAgent",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08192.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08192",
    "published": "2026-01-13T03:44:06Z",
    "updated": "2026-01-13T03:44:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出R^4自改进代理框架，通过协调四个代理来提升医疗图像分析中视觉检测和语言推理的可靠性和空间定位，无需微调即可显著提高性能。",
      "motivation": "医疗图像分析当前主要依赖大型视觉语言模型（VLMs），但这些系统多为单次通过的黑盒，缺乏对推理过程、安全性和空间定位的有效控制，导致在临床应用中存在可靠性低和错误风险高的局限。由于医疗决策对准确性和安全性要求极高，现有方法的僵化特性限制了迭代改进和错误修正，R^4框架旨在通过分解工作流和引入代理机制来解决这些问题，提高模型的适应性。",
      "method": "R^4框架将医疗成像工作流分解为四个协调代理：Router基于图像、患者历史和元数据配置任务感知提示；Retriever利用范例内存和pass@k采样生成自由文本报告和边界框；Reflector批判草案对以检测关键临床错误模式（如否定、侧面错误等）；Repairer在约束下迭代修订文本和空间输出，并收集高质量范例。框架实例化于胸部X光分析，使用多个现代VLM骨干进行评估。",
      "result": "在胸部X光分析实验中，R^4框架在报告生成和弱监督检测任务上显著优于强单VLM基线。无需基于梯度的微调，LLM-as-a-Judge分数提升了约1.7至2.5点，mAP50提升了2.5至3.5绝对点。这表明代理路由、反思和修复机制有效提高了VLM的性能和可靠性，尤其在空间定位方面有显著改进。",
      "conclusion": "R^4框架通过代理化设计将脆弱的VLM转变为更可靠和定位准确的临床图像解释工具，贡献在于结合自改进机制提升医疗图像分析的实用性和安全性。其学术价值在于提供了一种非梯度方法增强模型性能，实际应用潜力广泛。未来工作可探索扩展到其他医学成像领域或整合更多代理功能以应对复杂场景。",
      "tags": [
        "Large Vision-Language Model (VLM)",
        "Agentic Framework",
        "Weakly Supervised Detection",
        "Medical Image Analysis",
        "Self-Improving Systems"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:28.171550Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08190",
    "title": "Human-inspired Global-to-Parallel Multi-scale Encoding for Lightweight Vision Models",
    "authors": [
      "Wei Xu"
    ],
    "abstract": "Lightweight vision networks have witnessed remarkable progress in recent years, yet achieving a satisfactory balance among parameter scale, computational overhead, and task performance remains difficult. Although many existing lightweight models manage to reduce computation considerably, they often do so at the expense of a substantial increase in parameter count (e.g., LSNet, MobileMamba), which still poses obstacles for deployment on resource-limited devices. In parallel, some studies attempt to draw inspiration from human visual perception, but their modeling tends to oversimplify the visual process, making it hard to reflect how perception truly operates. Revisiting the cooperative mechanism of the human visual system, we propose GPM (Global-to-Parallel Multi-scale Encoding). GPM first employs a Global Insight Generator (GIG) to extract holistic cues, and subsequently processes features of different scales through parallel branches: LSAE emphasizes mid-/large-scale semantic relations, while IRB (Inverted Residual Block) preserves fine-grained texture information, jointly enabling coherent representation of global and local features. As such, GPM conforms to two characteristic behaviors of human vision perceiving the whole before focusing on details, and maintaining broad contextual awareness even during local attention. Built upon GPM, we further develop the lightweight H-GPE network. Experiments on image classification, object detection, and semantic segmentation show that H-GPE achieves strong performance while maintaining a balanced footprint in both FLOPs and parameters, delivering a more favorable accuracy-efficiency trade-off compared with recent state-of-the-art lightweight models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08190.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08190",
    "published": "2026-01-13T03:42:04Z",
    "updated": "2026-01-13T03:42:04Z",
    "comment": "23 pages, 5 figures",
    "light_analysis": {
      "overview": "论文提出了一种受人类视觉启发的全局到并行多尺度编码方法GPM，用于优化轻量级视觉模型的准确性与效率权衡。",
      "motivation": "当前轻量级视觉模型虽能降低计算开销，但往往伴随参数规模显著增加，如LSNet和MobileMamba，这使得在资源有限设备上部署困难。现有研究尝试从人类视觉感知中汲取灵感，但模型过于简化，未能真实反映感知过程，导致性能与平衡性不足。因此，本研究旨在通过更贴近人类视觉机制，开发一种能更好平衡参数、计算和任务性能的轻量级模型，解决现有方法在部署实用性和模型准确性方面的挑战。",
      "method": "论文提出GPM方法，首先通过全局洞察生成器(GIG)提取整体线索；然后采用并行分支处理多尺度特征：LSAE专注于中/大尺度语义关系，强化语义理解；IRB（反向残差块）保留细粒度纹理信息，确保局部细节。这种设计模仿人类视觉的先整体后细节和局部关注时保持上下文的特性，实现全局与局部特征的连贯表示。基于GPM，进一步开发了轻量级网络H-GPE，用于视觉任务，模型架构强调多尺度并行编码以提高效率。",
      "result": "实验在图像分类、物体检测和语义分割任务中进行，结果显示H-GPE在保持FLOPs和参数规模平衡的同时，实现了强大性能，优于近年来的先进轻量级模型。虽然摘要未提供具体数值，但结果证明该方法提供了更优的准确性与效率权衡，在类似计算复杂度下，性能提升显著，验证了GPM在多尺度特征提取和部署友好性方面的优势。",
      "conclusion": "本研究通过引入GPM方法和H-GPE网络，成功改善了轻量级视觉模型的准确性与效率平衡。主要贡献在于更真实地模拟人类视觉感知，结合全局和局部特征处理，提供了新的模型设计思路。学术上，推动了视觉计算与认知科学的结合；实际上，促进了模型在资源受限环境中的部署应用。未来工作可探索将GPM扩展到其他视觉任务或进一步优化网络架构以增强泛化能力。",
      "tags": [
        "Global-to-Parallel Multi-scale Encoding",
        "Human Visual Perception",
        "Inverted Residual Block",
        "Lightweight Vision Models",
        "Multi-scale Feature Extraction"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:05.320844Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08187",
    "title": "Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression",
    "authors": [
      "Zijun Di",
      "Bin Lu",
      "Huquan Kang",
      "Luoyi Fu",
      "Jiaxin Ding",
      "Xiaoying Gan",
      "Lei Zhou",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "abstract": "Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08187.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08187",
    "published": "2026-01-13T03:35:18Z",
    "updated": "2026-01-13T03:35:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了HS2C框架，通过利用图同质性进行结构和语义压缩，有效提升大语言模型在图理解中的推理能力。",
      "motivation": "LLMs在文本属性图理解中展现出潜力，但现有方法受限于上下文窗口，通常采用手工提示和随机采样引入噪声，导致推理不稳定。图结构蕴含丰富的结构和语义信息，有效利用能解锁LLM性能提升，因此需要研究更优的压缩方法来克服现有不足，解决图数据高效处理的问题。",
      "method": "HS2C框架基于图同质性，结构上采用结构熵最小化原则进行全局分层划分，解码图拓扑并识别同质社区以去除随机噪声；语义上，将检测到的结构信息传递给LLM，使其基于预定义社区类型进行差异化语义聚合，压缩冗余背景为简洁的社区级共识信息，从而优化输入。",
      "result": "在10个节点级基准测试中，HS2C通过提供结构和语义压缩输入，显著提高了压缩率和下游推理准确性，验证了其优越性和可扩展性；扩展到7个图级基准测试进一步巩固了任务泛化能力，显示框架在多样化图任务中的有效性。",
      "conclusion": "HS2C框架成功利用图同质性提升LLM推理性能，为图理解和LLM集成提供了新方法，具有学术价值和实际应用潜力；未来工作可探索更广泛图任务或扩展框架以处理复杂结构，局限性如摘要未明确说明具体边界条件。",
      "tags": [
        "Large Language Model",
        "Graph Compression",
        "Homophily",
        "Structural Entropy",
        "Semantic Aggregation"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:57.127032Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08183",
    "title": "GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards",
    "authors": [
      "Yan Zhu",
      "Te Luo",
      "Pei-Yao Fu",
      "Zhen Zhang",
      "Zi-Long Wang",
      "Yi-Fan Qu",
      "Zi-Han Geng",
      "Jia-Qi Xu",
      "Lu Yao",
      "Li-Yun Ma",
      "Wei Su",
      "Wei-Feng Chen",
      "Quan-Lin Li",
      "Shuo Wang",
      "Ping-Hong Zhou"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical \"spatial grounding bottleneck\" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a \"fluency-accuracy paradox\": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to \"over-interpretation\" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08183.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08183",
    "published": "2026-01-13T03:23:11Z",
    "updated": "2026-01-13T03:23:11Z",
    "comment": "45 pages, 17 figures, 6 tables. Leaderboard available at: https://roterdl.github.io/GIBench/ . Includes supplementary material",
    "light_analysis": {
      "overview": "本文通过构建GI-Bench全景基准，揭示了多模态大语言模型在胃肠内镜中知识-经验分离的现象，系统评估其临床性能。",
      "motivation": "多模态大语言模型在胃肠病学中显示出应用潜力，但其在全面临床工作流和人类基准测试中的性能尚未得到验证，限制了临床实用性的评估。现有方法缺乏与人类专家在临床标准下的系统性对比，导致无法准确判断模型在真实医疗场景中的价值。因此，构建一个标准化基准来评估MLLMs的临床效果，有助于填补这一空白，推动AI技术在医疗诊断中的实际应用和改进。",
      "method": "研究构建了GI-Bench基准，涵盖20个细粒度病变类别，用于系统评估12个多模态大语言模型在五个阶段临床工作流中的性能，包括解剖定位、病变识别、诊断、发现描述和管理。采用Macro-F1、平均交并比和Likert量表等多维指标，将模型性能与三名初级内镜医师和三名住院医生学员进行对比。关键创新点在于全景工作流评估和人类基准测试，以全面分析模型在临床任务中的表现。",
      "result": "Gemini-3-Pro模型取得最佳性能。在诊断推理阶段，顶级模型的Macro-F1为0.641，优于学员（0.492），并与初级内镜医师（0.727；p>0.05）持平。然而，存在显著的“空间基础瓶颈”：人类病变定位的mIoU大于0.506，显著优于最佳模型的0.345（p<0.05）。定性分析还揭示了“流畅性-准确性悖论”：模型生成的报告在语言可读性上优于人类（p<0.05），但由于“过度解释”和视觉特征幻觉，事实准确性显著更低（p<0.05）。",
      "conclusion": "GI-Bench基准系统评估了多模态大语言模型在胃肠内镜中的临床性能，揭示了“空间基础瓶颈”和“流畅性-准确性悖论”，表明模型在诊断推理上接近人类专家，但在空间定位和事实准确性方面存在不足。该研究提供了标准的临床评估框架，对AI在医疗领域的应用具有重要学术和实际价值。未来工作可专注于改善模型的空间能力和减少幻觉，以提升其临床可靠性。",
      "tags": [
        "Multimodal Large Language Models",
        "Benchmarking",
        "Clinical Endoscopy",
        "Spatial Grounding",
        "Qualitative Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:57.423917Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08182",
    "title": "Second-order Gaussian directional derivative representations for image high-resolution corner detection",
    "authors": [
      "Dongbo Xie",
      "Junjie Qiu",
      "Changming Sun",
      "Weichuan Zhang"
    ],
    "abstract": "Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction. Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other. In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e. END-type and L-type models). Then, the SOGDD representations of these two corner models were derived separately, and many characteristics of high-resolution corners were discovered, which enabled us to demonstrate how to select Gaussian filtering scales to obtain intensity variation information from images, accurately depicting adjacent corners. In addition, a new high-resolution corner detection method for images has been proposed for the first time, which can accurately detect adjacent corner points. The experimental results have verified that the proposed method outperforms state-of-the-art methods in terms of localization error, robustness to image blur transformation, image matching, and 3D reconstruction.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08182.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08182",
    "published": "2026-01-13T03:21:02Z",
    "updated": "2026-01-13T03:21:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于二阶高斯方向导数表示的高分辨率角点检测方法，有效解决了相邻角点的准确检测问题。",
      "motivation": "角点检测在计算机视觉任务中广泛应用，如图像匹配和3D重建，但对高分辨率图像中的相邻角点检测存在挑战。现有方法（如Zhang等人的方法）有理论缺陷，因为相邻角点的灰度信息会相互干扰，导致检测不准确。这一问题限制了计算机视觉应用的性能，因此需要改进方法以准确描述相邻角点，提升相关任务的可靠性。",
      "method": "本研究采用二阶高斯方向导数（SOGDD）滤波器来平滑两种典型高分辨率角模型（END型和L型）。推导了这些模型的SOGDD表示，发现了高分辨率角点的多个特征，从而演示了如何选择高斯滤波尺度以从图像中提取强度变化信息。基于此，首次提出了一种新的高分辨率角点检测方法，能够准确检测相邻角点。关键创新点在于应用SOGDD表示来分析和描述角点，提高了检测精度。",
      "result": "实验结果表明，所提出的方法在多个评估指标上优于现有的先进方法。具体表现在更低的定位误差、对图像模糊变换更高的鲁棒性，以及在图像匹配和3D重建任务中的更好性能。摘要未提供具体数据，但验证了该方法在实践中的优越性，展示了其在复杂场景下的有效性。",
      "conclusion": "本研究的主要贡献在于推导了二阶高斯方向导数表示，用于高分辨率角点检测，解决了相邻角点检测的难题。这提高了角点检测的精度和鲁棒性，对计算机视觉应用如图像匹配和3D重建有重要学术和实际价值。未来工作可进一步探索更多角模型或优化滤波参数，以拓展方法的适用范围。",
      "tags": [
        "Corner Detection",
        "Second-order Gaussian Directional Derivative (SOGDD)",
        "High-resolution Imaging",
        "Image Matching",
        "3D Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:23.384354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08181",
    "title": "TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations",
    "authors": [
      "Aviral Gupta",
      "Armaan Sethi",
      "Dhruv Kumar"
    ],
    "abstract": "Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08181.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08181",
    "published": "2026-01-13T03:14:25Z",
    "updated": "2026-01-13T03:14:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过分析TabPFN的内部表示，揭示了表格基础模型中存储的有意义结构化信息，为模型可解释性提供新见解。",
      "motivation": "表格基础模型如TabPFN在多种任务中展现出强大性能，但其内部表示和所学概念尚未充分理解。这种可解释性的缺乏使得研究模型如何处理和转换输入特征变得重要，因为它影响模型的透明度和可信度。现有方法往往忽视内部机制，因此本研究旨在填补这一空白，探索模型决策过程的黑箱特性。",
      "method": "本文采用探测实验来分析模型的隐藏表示，测试在不同层中是否存在线性回归系数、复杂表达式的中间值以及最终答案。这些实验允许推断模型内部的计算过程，并考察表示在层间的演化情况。通过关注TabPFN的结构，我们使用具体数据集和模型架构进行实验，以揭示其信息编码机制。",
      "result": "实验结果表明，表格基础模型的内部表示中存储了有意义的、结构化的信息。我们观察到清晰的信号，对应于模型预测过程中的中间和最终量，这提供了对模型如何精炼输入以及最终输出如何产生的见解。与基线方法的对比未明确说明，但分析揭示了模型内部的编码方式，表明信息演化具有逻辑性。",
      "conclusion": "本研究的发现贡献于对表格基础模型内部机制的理解，表明这些模型编码了具体和可解释的信息，使决策过程更接近透明和可信。学术价值在于推动了AI模型的可解释性研究，实际应用价值在于增强模型在关键领域的可靠性。未来工作方向包括扩展实验到更多模型和数据集，以验证局限性。",
      "tags": [
        "Tabular Foundational Models",
        "Interpretability",
        "Probing Experiments",
        "Internal Representations",
        "Layer-wise Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:08.054205Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08179",
    "title": "Instruction-Driven 3D Facial Expression Generation and Transition",
    "authors": [
      "Anh H. Vo",
      "Tae-Seok Kim",
      "Hulin Jin",
      "Soo-Mi Choi",
      "Yong-Guk Kim"
    ],
    "abstract": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08179.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08179",
    "published": "2026-01-13T03:12:48Z",
    "updated": "2026-01-13T03:12:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出指令驱动的3D面部表情生成与过渡框架，实现基于文本描述的面部表情序列生成。",
      "motivation": "研究旨在解决3D虚拟角色面部表情单一的问题，传统方法通常只支持六种基本表情，难以模拟真实情感变化。该问题对增强虚拟角色情感表现力至关重要，现有方法可能在动态过渡和多模态控制方面存在不足。通过文本指令驱动，可以扩展表情库，实现任意表情间的平滑过渡，提升交互性和真实感。摘要未明确说明现有方法的具体技术缺陷，但强调了指令驱动生成的重要性。",
      "method": "方法核心包括Instruction-driven Facial Expression Decomposer（IFED）模块，用于多模态数据学习，捕获文本描述与面部表情特征之间的相关性。随后，提出Instruction to Facial Expression Transition（I2FET）方法，结合IFED和顶点重建损失函数，细化潜在向量的语义理解，生成面部表情序列。此外，Facial Expression Transition模型用于生成表情间的平滑过渡。研究使用了CK+和CelebV-HQ数据集，基于深度学习和多模态表示技术，创新点在于指令驱动的面部表情分解与过渡生成机制。",
      "result": "实验在CK+和CelebV-HQ数据集上进行评估，结果显示所提框架优于现有方法，能根据文本指令生成准确的面部表情轨迹。具体性能指标如准确率或效率改进在摘要中未明确说明，但声称在多个基准测试中表现优异，证实了方法的有效性。对比基线方法，该框架在表情生成质量和过渡自然度方面有所提升，增强了多模态控制的鲁棒性和应用潜力。",
      "conclusion": "本研究的贡献在于开发了指令驱动的3D面部表情生成框架，显著扩展了面部表情库和过渡可能性，增强了虚拟角色的情感表达能力。学术价值在于推动了多模态学习与计算机图形学的交叉研究，实际应用潜力包括虚拟现实、游戏和动画制作等领域。摘要未提及具体局限性，但未来工作可能包括优化过渡质量、扩展到更复杂情感表达或集成更多数据模态，以进一步提升系统的实用性和泛化能力。",
      "tags": [
        "Instruction-Driven Generation",
        "3D Facial Expression",
        "Multimodal Learning",
        "Sequence Generation",
        "Vertex Reconstruction Loss"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:38.774766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08176",
    "title": "Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering",
    "authors": [
      "Lavanya Prahallad",
      "Sai Utkarsh Choudarypally",
      "Pragna Prahallad",
      "Pranathi Prahallad"
    ],
    "abstract": "Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08176.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08176",
    "published": "2026-01-13T03:10:58Z",
    "updated": "2026-01-13T03:10:58Z",
    "comment": "6 pages, 6 tables",
    "light_analysis": {
      "overview": "本论文通过比较不同提示策略，显著提升了大语言模型在政治问答中清晰度和主题检测的自动评估性能。",
      "motivation": "在政治问答领域，自动评估大语言模型回复不仅需保证事实正确性，还必须关注清晰度和回避问题，以避免误导。尽管现有数据集如CLARITY提供人类标注，但提示设计对自动评估性能的影响仍未充分探索，导致评估方法在细粒度指标上不足。研究旨在解决这一问题，通过深入分析提示策略来改进评估准确性，特别是在应对复杂政治内容时，提升自动评估的可靠性和实用性。",
      "method": "论文使用SemEval 2026共享任务的CLARITY数据集，对比GPT-5.2在三种提示策略下的表现：简单提示、思维链提示和带少样本示例的思维链提示。关键创新在于应用结构化推理提示技术，以评估其对清晰度、回避和主题检测的影响。研究基于准确性、类别指标和层次精确匹配等评估标准，全面比较模型预测与人类标注的差异，数据集和模型架构的细节为分析提供了基础。",
      "result": "实验结果显示，GPT-5.2在清晰度预测中优于GPT-3.5基线，使用带少样本的思维链提示时，准确性从56%提升到63%。在回避评估中，思维链提示达到最高34%准确性，但细粒度类别的改进不稳定。主题检测方面，基于推理的提示使准确性从60%提升到74%。这些数据表明，提示设计能有效改进高层评估，但细粒度任务如回避和主题检测仍具挑战性。",
      "conclusion": "本论文主要贡献在于证实了提示设计能可靠提升高层清晰度评估，为自动评估领域提供了新的方法视角。研究具有重要学术价值，推动了提示工程在AI评估中的应用，实际应用上可增强政治问答等场景的自动化评估准确性。局限性在于细粒度回避和主题检测任务仍面临挑战，提示设计效果不稳定，未来工作可能需结合更多技术或数据集来改进细粒度性能。",
      "tags": [
        "Prompt Engineering",
        "Chain-of-Thought",
        "Few-Shot Learning",
        "Clarity Evaluation",
        "Topic Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:09.218499Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08175",
    "title": "CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval",
    "authors": [
      "Feiran Wang",
      "Junyi Wu",
      "Dawen Cai",
      "Yuan Hong",
      "Yan Yan"
    ],
    "abstract": "We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08175.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08175",
    "published": "2026-01-13T03:09:35Z",
    "updated": "2026-01-13T03:09:35Z",
    "comment": "Project Page: https://github.com/Brack-Wang/cognimap3D",
    "light_analysis": {
      "overview": "CogniMap3D提出一个模拟人类认知过程的生物启发式框架，实现动态3D场景的高效理解和快速检索。",
      "motivation": "该研究旨在解决动态3D场景理解和重建的挑战，特别是在机器人导航和增强现实等需要连续访问和更新场景记忆的应用中。现有方法可能缺乏生物启发的认知模拟，导致处理动态对象和长期记忆存储时效率低下，限制了场景理解的持续性和可扩展性。CogniMap3D通过模仿人类认知过程，设计高效的记忆管理，以弥补这些不足，提升空间知识的存储和检索能力。",
      "method": "CogniMap3D框架整合三个核心能力：一个多阶段运动提示框架，利用深度和相机姿态先验识别动态对象；一个认知映射系统，用于存储、回忆和更新静态场景在多次访问中；以及一个因子图优化策略，用于优化相机姿态。给定图像流，模型通过运动提示分离动态区域，并将静态元素匹配到持久记忆库。关键创新在于模拟人类认知，实现动态与静态元素的集成处理和连续更新，支持熟悉位置的快速检索和相机重定位。",
      "result": "在视频深度估计、相机姿态重建和3D映射任务的评估中，CogniMap3D表现出最先进的性能，有效支持跨扩展序列和多次访问的连续场景理解。然而，摘要未明确提供具体性能指标数据，如准确率提升或效率改进的数值，以及详细的基线方法对比情况。它强调了在动态场景处理中的整体优势，但没有给出量化结果。",
      "conclusion": "CogniMap3D的主要贡献是提出一个生物启发式框架，融合运动提示、认知映射和优化策略，模拟人类认知过程以实现动态3D场景的高效理解和快速检索。其学术价值在于为场景理解提供新思路，促进认知计算与计算机视觉的结合。实际应用广泛，如自主机器人和AR/VR系统。未来工作可能包括扩展到更复杂环境、提高实时性能，或集成更多认知功能以增强鲁棒性。",
      "tags": [
        "Cognitive Mapping",
        "Motion Cue Analysis",
        "Factor Graph Optimization",
        "3D Reconstruction",
        "Dynamic Scene Understanding"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:55.891364Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08174",
    "title": "Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling",
    "authors": [
      "Xiyan Feng",
      "Wenbo Zhang",
      "Lu Zhang",
      "Yunzhi Zhuge",
      "Huchuan Lu",
      "You He"
    ],
    "abstract": "This technical report represents the award-winning solution to the Cross-platform 3D Object Detection task in the RoboSense2025 Challenge. Our approach is built upon PVRCNN++, an efficient 3D object detection framework that effectively integrates point-based and voxel-based features. On top of this foundation, we improve cross-platform generalization by narrowing domain gaps through tailored data augmentation and a self-training strategy with pseudo-labels. These enhancements enabled our approach to secure the 3rd place in the challenge, achieving a 3D AP of 62.67% for the Car category on the phase-1 target domain, and 58.76% and 49.81% for Car and Pedestrian categories respectively on the phase-2 target domain.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08174.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08174",
    "published": "2026-01-13T03:09:20Z",
    "updated": "2026-01-13T03:09:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种通过数据增强和伪标签自训练提高跨平台3D物体检测领域适应性的方法。",
      "motivation": "该研究旨在解决跨平台3D物体检测中的领域适应问题，例如在自动驾驶中，不同传感器或平台收集的数据存在差异，导致模型泛化能力下降。现有方法可能因领域差异而性能不足，因此通过缩小领域差距来提升跨平台适应性至关重要，这对于确保模型在多变现实场景中的可靠应用具有实际意义。摘要未明确说明现有方法的具体不足，但强调了领域间隙的挑战。",
      "method": "该方法基于PVRCNN++框架，该框架集成了点云和体素特征以提升检测效率。创新点在于引入定制的数据增强技术，如模拟不同传感器特性，以减少领域差异，并采用自训练策略，利用伪标签在目标域数据上迭代优化模型。这些策略共同缩小了源域与目标域之间的差距，提高了跨平台泛化能力，具体应用于RoboSense2025挑战赛的数据集。",
      "result": "实验结果显示，在RoboSense2025挑战赛中，该方法获得第三名。在第一阶段目标域上，Car类别的3D AP达到62.67%；在第二阶段，Car和Pedestrian类别分别达到58.76%和49.81%。这些指标表明方法在跨平台3D检测任务中表现优异，与其它参赛方法相比具有竞争力，有效提升了检测性能。摘要未提供基线方法的详细对比数据。",
      "conclusion": "本研究的主要贡献是结合数据增强和伪标签自训练，显著提升了跨平台3D物体检测的领域适应性。这具有重要学术价值，推动了领域自适应在3D感知中的应用；实际应用中，可适用于自动驾驶等需要处理多样场景的领域。摘要未明确说明局限性，未来工作可能包括扩展到更多对象类别或优化伪标签生成策略。",
      "tags": [
        "3D Object Detection",
        "Domain Adaptation",
        "Data Augmentation",
        "Pseudo-Labeling",
        "Self-Training"
      ]
    },
    "analyzed_at": "2026-01-14T03:28:53.126530Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08173",
    "title": "The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios",
    "authors": [
      "Daocheng Fu",
      "Jianbiao Mei",
      "Rong Wu",
      "Xuemeng Yang",
      "Jia Xu",
      "Ding Wang",
      "Pinlong Cai",
      "Yong Liu",
      "Licheng Wen",
      "Botian Shi"
    ],
    "abstract": "The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \\method{}, a dynamic evaluation environment that simulates a \"trainee\" agent continuously exploring a novel setting. Unlike traditional benchmarks, \\method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08173.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08173",
    "published": "2026-01-13T03:09:18Z",
    "updated": "2026-01-13T03:09:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一个动态评估环境 \\method{}，用于在动态环境中评估代理的调度、探索和学习能力，从而提升现实世界部署的鲁棒性。",
      "motivation": "随着多模态大型语言模型（MLLMs）的快速发展，工作流自动化取得进展，但现有研究主要针对静态环境的性能上限，忽视了随机现实世界部署的鲁棒性。研究旨在解决三个关键挑战：动态任务调度、不确定性下的主动探索以及从经验中持续学习。这些问题在实际应用如职场场景中至关重要，但现有方法在适应性方面存在不足，导致代理在动态环境下可靠性较差。",
      "method": "研究方法包括引入 \\method{}，一个动态评估环境，模拟'学员'代理在连续探索新设置。该方法从三个维度评估代理：上下文感知调度处理流式任务的不同优先级；通过主动探索谨慎获取信息以减少幻觉；以及从基于规则、动态生成的任务中蒸馏通用策略以实现持续演化。摘要未明确说明使用的具体数据集或模型架构细节，但强调了评估框架的技术特色。",
      "result": "实验结果显示，最先进的代理在动态环境中表现存在显著不足，尤其在主动探索和持续学习方面。研究对比了传统静态基准，发现代理在处理动态任务和不确定性时表现受限，凸显了在现实场景中可靠性的挑战。摘要未提供具体的性能指标如准确率提升，但强调了评估维度下的缺陷发现。",
      "conclusion": "本研究的主要贡献是建立了一个用于评估代理可靠性的框架，将评估重点从静态测试转向现实、生产导向的场景。学术上，这为动态环境下的代理评估提供了新方法；实际应用上，有助于提升代理在真实世界部署的鲁棒性和适应性。摘要未明确说明局限性或未来工作方向，但可以推断进一步优化评估方法以支持更广泛的场景。",
      "tags": [
        "Multi-modal Large Language Models",
        "Dynamic Task Scheduling",
        "Active Exploration",
        "Continual Learning",
        "Evaluation Benchmark"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:08.826470Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08172",
    "title": "VBO-MI: A Fully Gradient-Based Bayesian Optimization Framework Using Variational Mutual Information Estimation",
    "authors": [
      "Farhad Mirkarimi"
    ],
    "abstract": "Many real-world tasks require optimizing expensive black-box functions accessible only through noisy evaluations, a setting commonly addressed with Bayesian optimization (BO). While Bayesian neural networks (BNNs) have recently emerged as scalable alternatives to Gaussian Processes (GPs), traditional BNN-BO frameworks remain burdened by expensive posterior sampling and acquisition function optimization. In this work, we propose {VBO-MI} (Variational Bayesian Optimization with Mutual Information), a fully gradient-based BO framework that leverages recent advances in variational mutual information estimation. To enable end-to-end gradient flow, we employ an actor-critic architecture consisting of an {action-net} to navigate the input space and a {variational critic} to estimate information gain. This formulation effectively eliminates the traditional inner-loop acquisition optimization bottleneck, achieving up to a {$10^2 \\times$ reduction in FLOPs} compared to BNN-BO baselines. We evaluate our method on a diverse suite of benchmarks, including high-dimensional synthetic functions and complex real-world tasks such as PDE optimization, the Lunar Lander control problem, and categorical Pest Control. Our experiments demonstrate that VBO-MI consistently provides the same or superior optimization performance and computational scalability over the baselines.",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08172.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08172",
    "published": "2026-01-13T03:07:52Z",
    "updated": "2026-01-13T03:07:52Z",
    "comment": "31 pages, 8 figures, Code will be released upon acceptance",
    "light_analysis": {
      "overview": "提出了VBO-MI，一个完全基于梯度的贝叶斯优化框架，利用变分互信息估计消除传统优化瓶颈，显著提升计算效率。",
      "motivation": "许多现实世界任务如科学优化和控制系统需要高效优化昂贵的黑盒函数，这些函数只能通过噪声评估访问，通常使用贝叶斯优化（BO）解决。现有方法如基于贝叶斯神经网络（BNN）的BO框架面临计算负担重的问题，包括昂贵的后验采样和获取函数优化，这限制了其在高维和复杂任务中的可扩展性。因此，开发更高效的BO框架以应对这些挑战具有重要意义。",
      "method": "VBO-MI采用完全基于梯度的贝叶斯优化框架，核心创新在于利用变分互信息估计的最新进展。方法包括演员-评论家架构：动作网络用于在输入空间中导航，变分评论家通过变分技术估计信息增益。这种设计实现了端到端的梯度流，消除了传统内部循环获取优化的瓶颈。技术特色还包括使用贝叶斯神经网络作为可扩展替代方案，避免了对高斯过程的依赖，提高了框架的灵活性和效率。",
      "result": "在多样化的基准测试中，包括高维合成函数和复杂现实任务如PDE优化、Lunar Lander控制及分类害虫控制，VBO-MI展示了优异的性能。与基线BNN-BO相比，VBO-MI在优化性能上达到相同或更优水平，并在计算效率上实现高达10^2倍的FLOPs减少。实验表明，该方法在保持高优化质量的同时，显著提升了计算可扩展性，适用于各种昂贵黑盒函数优化场景。",
      "conclusion": "VBO-MI的主要贡献是提出了一种高效、完全基于梯度的贝叶斯优化框架，通过消除传统优化瓶颈，推动了该领域的技术进步。研究具有重要学术价值，为大规模、高维优化任务提供了新思路，并在实际应用中如控制问题和科学优化中展现出潜力。摘要未明确说明局限性，未来工作可能包括扩展到更广泛的应用或进一步优化算法细节。",
      "tags": [
        "Bayesian Optimization",
        "Variational Mutual Information",
        "Actor-Critic",
        "Gradient-Based Optimization",
        "Black-Box Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:18.723229Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08169",
    "title": "Relational Knowledge Distillation Using Fine-tuned Function Vectors",
    "authors": [
      "Andrea Kang",
      "Yingnian Wu",
      "Hongjing Lu"
    ],
    "abstract": "Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08169.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08169",
    "published": "2026-01-13T03:02:18Z",
    "updated": "2026-01-13T03:02:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出通过微调功能向量和复合向量，增强大型语言模型的关系知识蒸馏和类比推理能力，提高其可解释性和性能。",
      "motivation": "智能系统需要高效表示概念间关系以支持推理，但现有基于因果中介分析提取功能向量的方法在有限数据下可能优化不足。研究动机是解决关系知识表示与操作的挑战，通过改进向量表示来提升大型语言模型在关系任务中的准确性和人类对齐度，弥补当前方法在数据利用和性能上的局限。",
      "method": "论文首先使用少量示例（约20个词对）微调从因果中介分析中提取的功能向量，优化关系编码。接着引入复合功能向量，通过加权组合多个微调向量来支持关系知识提取和类比推理。在推理阶段，将复合向量插入大型语言模型的激活中，利用激活修补机制控制知识注入。创新点在于从原始向量优化到复合向量设计，增强关系任务的适应性。",
      "result": "实验表明，微调功能向量在关系词完成任务上优于原始向量，提升了准确率，并在小型和大型语言模型中均有效。复合功能向量在类比推理任务上显著提升性能，如在认知科学和SAT基准测试中表现更佳，且解码关系词时与人类相似性判断更一致。与基线方法对比，这些改进验证了方法的通用性和有效性。",
      "conclusion": "本研究贡献在于验证了微调和复合功能向量在关系知识蒸馏中的作用，通过激活修补机制提升大型语言模型的推理能力和可解释性。学术价值是提供可控的知识编码方法，推动AI系统推理任务优化；应用价值可拓展到更复杂的关系任务。未来工作可探索更广泛的微调策略或扩展到其他领域，摘要未明确说明具体局限性。",
      "tags": [
        "Function Vectors",
        "Fine-tuning",
        "Relational Knowledge Distillation",
        "Activation Patching",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:13.092148Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08166",
    "title": "ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms",
    "authors": [
      "Mohammad Pivezhandi",
      "Mahdi Banisharif",
      "Abusayeed Saifullah",
      "Ali Jannesari"
    ],
    "abstract": "Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08166.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08166",
    "published": "2026-01-13T02:56:06Z",
    "updated": "2026-01-13T02:56:06Z",
    "comment": "39 pages, 12 figures, 8 tables (including appendix)",
    "light_analysis": {
      "overview": "本文提出了一种基于大型语言模型（LLM）和分层多智能体强化学习（MARL）的零样本调度框架，用于嵌入式系统的核心和频率分配。",
      "motivation": "嵌入式系统中的动态电压和频率调整（DVFS）和任务到核心分配对于热管理和平衡能量与性能至关重要。现有方法要么依赖基于利用率的启发式，忽略核心停顿时间，导致效率低下；要么需要大量离线分析生成调度表，无法适应运行时动态变化。这限制了嵌入式系统的自适应性和实时性能，尤其是在处理多样化新工作负载时。因此，需要一种能零样本适应、无需特定配置的调度方法。",
      "method": "本文提出了一个基于模型的分层多智能体强化学习（MARL）框架，通过两个协作代理分解指数级动作空间，降低决策延迟至358毫秒。框架集成了大型语言模型（LLM）提取的语义特征，从OpenMP程序代码中提取13个特征，无需执行程序。环境模型使用回归技术准确预测热动力学和性能状态，并结合LLM特征实现零样本部署。Dyna-Q启发的设计融合了直接强化学习和基于模型的规划，显著加速收敛。该方法无需新工作负载的配置样本，通过合成训练数据适应新任务。",
      "result": "实验在BOTS和PolybenchC基准测试上，于NVIDIA Jetson TX2、Jetson Orin NX、RubikPi和Intel Core i7平台上进行。结果显示，与Linux ondemand governor相比，能量效率提高了7.09倍，制造周期缩短了4.0倍。首次决策延迟为358毫秒，收敛速度比无模型方法快20倍。首次决策总延迟（包括一次性LLM特征提取）为3.5到8.0秒，比基于表的分析方法快8,300倍，验证了框架在动态嵌入式系统中的高效性和实用性。",
      "conclusion": "该研究的主要贡献是开发了一个结合LLM和MARL的零样本调度框架，显著提升了嵌入式系统的自适应性和效率。学术上，它推动了基于模型的强化学习与语义特征提取在资源管理中的融合。实际应用中，使系统能零样本适应新工作负载，增强动态环境适应能力。未来工作可扩展至更多硬件平台，优化实时决策和热管理策略。",
      "tags": [
        "Dynamic Voltage and Frequency Scaling",
        "Multi-Agent Reinforcement Learning",
        "Large Language Model",
        "Zero-Shot Learning",
        "Regression Model"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:52.412375Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08165",
    "title": "Representation Learning with Semantic-aware Instance and Sparse Token Alignments",
    "authors": [
      "Phuoc-Nguyen Bui",
      "Toan Duc Nguyen",
      "Junghyun Bum",
      "Duc-Tai Le",
      "Hyunseung Choo"
    ],
    "abstract": "Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08165.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08165",
    "published": "2026-01-13T02:55:48Z",
    "updated": "2026-01-13T02:55:48Z",
    "comment": "Under review, 8 pages",
    "light_analysis": {
      "overview": "提出SISTA框架，通过语义感知的实例和稀疏令牌对齐，改进医疗视觉-语言预训练，有效提升下游任务性能。",
      "motivation": "该研究旨在解决医疗视觉-语言预训练（VLP）中假负样本问题。传统方法使用对比学习，将所有未配对的图像-报告样本视为负样本，但医疗数据中不同患者的图像或报告可能存在相似性，这会导致破坏语义结构和降低学习表示的质量，从而影响下游任务表现。因此，改进现有方法以处理医疗数据的语义相似性至关重要。",
      "method": "论文提出一个多级对齐框架SISTA，在图像-报告和补丁-词两个级别利用语义对应。通过引入报告间相似性来消除假负样本，改进传统对比学习；并设计方法有效对齐图像补丁与相关词令牌，实现细粒度语义对齐。关键创新在于语义感知的实例对齐和稀疏令牌对齐，使用医疗图像和放射学报告作为数据集基础。",
      "result": "实验结果显示，SISTA框架在图像分类、图像分割和目标检测三个下游任务上，展现出优越的迁移性能。与传统基线方法相比，该框架能显著提升性能，尤其在细粒度任务上，即使使用有限标签数据也能实现改进，表明其对医疗VLP的有效性。摘要未明确给出具体数值指标。",
      "conclusion": "本文主要贡献是提出SISTA框架，通过语义感知对齐解决了医疗VLP中的假负样本问题，提升了表示学习质量。学术上为跨模态学习提供新方法，实际应用中可增强医疗图像和文本的下游任务性能。未来工作可能包括扩展更多数据集或任务，以及优化模型效率。摘要未明确说明局限性。",
      "tags": [
        "Contrastive Learning",
        "Vision-Language Pre-training",
        "Medical Image Analysis",
        "Semantic Alignment",
        "Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:29:22.081940Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08162",
    "title": "A Hardware-Algorithm Co-Designed Framework for HDR Imaging and Dehazing in Extreme Rocket Launch Environments",
    "authors": [
      "Jing Tao",
      "Banglei Guan",
      "Pengju Sun",
      "Taihang Lei",
      "Yang Shang",
      "Qifeng Yu"
    ],
    "abstract": "Quantitative optical measurement of critical mechanical parameters -- such as plume flow fields, shock wave structures, and nozzle oscillations -- during rocket launch faces severe challenges due to extreme imaging conditions. Intense combustion creates dense particulate haze and luminance variations exceeding 120 dB, degrading image data and undermining subsequent photogrammetric and velocimetric analyses. To address these issues, we propose a hardware-algorithm co-design framework that combines a custom Spatially Varying Exposure (SVE) sensor with a physics-aware dehazing algorithm. The SVE sensor acquires multi-exposure data in a single shot, enabling robust haze assessment without relying on idealized atmospheric models. Our approach dynamically estimates haze density, performs region-adaptive illumination optimization, and applies multi-scale entropy-constrained fusion to effectively separate haze from scene radiance. Validated on real launch imagery and controlled experiments, the framework demonstrates superior performance in recovering physically accurate visual information of the plume and engine region. This offers a reliable image basis for extracting key mechanical parameters, including particle velocity, flow instability frequency, and structural vibration, thereby supporting precise quantitative analysis in extreme aerospace environments.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08162.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08162",
    "published": "2026-01-13T02:51:55Z",
    "updated": "2026-01-13T02:51:55Z",
    "comment": "The paper has been accepted by Acta Mechanica Sinica",
    "light_analysis": {
      "overview": "提出硬件算法协同设计框架，用于在极端火箭发射环境中实现高动态范围成像和去雾，提升光学测量准确性。",
      "motivation": "火箭发射极端成像条件下，强烈燃烧产生密集雾霾和超过120 dB的亮度变化，导致图像数据严重退化，影响羽流流场、冲击波结构等关键机械参数的定量测量。现有光学方法可能依赖理想化大气模型，无法处理极端环境，亟需创新解决方案以获取可靠视觉信息，支持后续光电测量和速度分析。",
      "method": "该方法采用硬件算法协同设计框架，结合定制空间变化曝光传感器和物理感知去雾算法。传感器单次拍摄获取多曝光数据，无需依赖理想化大气模型；算法动态估计雾霾密度，实施区域自适应照明优化，并应用多尺度熵约束融合技术，有效分离雾霾与场景辐射。在真实火箭发射图像和受控实验中验证技术可行性。",
      "result": "在真实火箭发射图像和受控实验中进行验证，该框架在恢复羽流和发动机区域物理准确视觉信息方面表现出优越性能。摘要未提供具体数值指标，但实验结果表明能显著提升图像质量，为提取粒子速度、流动不稳定性频率等机械参数提供可靠基础，优于传统方法，适应极端环境需求。",
      "conclusion": "该研究通过硬件算法协同设计，有效解决极端火箭发射环境下的图像退化问题，实现高动态范围成像和去雾。其贡献在于提供可靠图像依据，支持关键机械参数如粒子速度和结构振动的精确提取，具有重要航空航天应用价值，推动了极端环境光学测量技术的发展，摘要未明确说明局限性和未来工作方向。",
      "tags": [
        "Spatially Varying Exposure (SVE)",
        "HDR Imaging",
        "Dehazing Algorithm",
        "Hardware-Algorithm Co-Design",
        "Physics-aware Image Processing"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:03.898103Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08160",
    "title": "SwiftMem: Fast Agentic Memory via Query-aware Indexing",
    "authors": [
      "Anxin Tian",
      "Yiming Li",
      "Xing Li",
      "Hui-Ling Zhen",
      "Lei Chen",
      "Xianzhi Yu",
      "Zhenhua Dong",
      "Mingxuan Yuan"
    ],
    "abstract": "Agentic memory systems have become critical for enabling LLM agents to maintain long-term context and retrieve relevant information efficiently. However, existing memory frameworks suffer from a fundamental limitation: they perform exhaustive retrieval across the entire storage layer regardless of query characteristics. This brute-force approach creates severe latency bottlenecks as memory grows, hindering real-time agent interactions. We propose SwiftMem, a query-aware agentic memory system that achieves sub-linear retrieval through specialized indexing over temporal and semantic dimensions. Our temporal index enables logarithmic-time range queries for time-sensitive retrieval, while the semantic DAG-Tag index maps queries to relevant topics through hierarchical tag structures. To address memory fragmentation during growth, we introduce an embedding-tag co-consolidation mechanism that reorganizes storage based on semantic clusters to improve cache locality. Experiments on LoCoMo and LongMemEval benchmarks demonstrate that SwiftMem achieves 47$\\times$ faster search compared to state-of-the-art baselines while maintaining competitive accuracy, enabling practical deployment of memory-augmented LLM agents.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08160.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08160",
    "published": "2026-01-13T02:51:04Z",
    "updated": "2026-01-13T02:51:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "SwiftMem是一个查询感知的代理内存系统，通过专门的时间索引和语义索引实现亚线性检索，显著提高LLM代理的内存检索速度。",
      "motivation": "代理内存系统对于LLM代理至关重要，能够支持长期上下文维护和高效信息检索。然而，现有框架存在根本性局限：它们不考虑查询特性，对整个存储层进行穷举检索。这种暴力方法随着内存增长导致严重的延迟瓶颈，妨碍了实时代理交互，特别是在大规模应用中影响实用性。因此，迫切需要更高效的检索机制来解决这一问题，以提升LLM代理的响应能力和实际部署可行性。",
      "method": "SwiftMem的核心方法是查询感知索引，通过时间索引和语义索引实现亚线性检索。时间索引基于对数时间算法，支持时间敏感的范围查询；语义DAG-Tag索引利用有向无环图和层次标签结构，将查询映射到相关主题，优化语义匹配。创新点还包括嵌入-标签协同巩固机制，在内存增长时基于语义簇重组存储，以解决内存碎片化问题，提高缓存局部性。系统设计考虑了LLM代理的存储和检索需求，结合时间与语义维度，实现了高效索引和低延迟访问。",
      "result": "在LoCoMo和LongMemEval基准测试中，SwiftMem的实验结果显示，其搜索速度比最先进基线快47倍，同时保持了竞争性的准确度。与现有方法相比，SwiftMem显著减少了检索延迟，支持了实时交互，并确保了内存检索的高效性。具体性能改进体现在亚线性检索速度上，使得内存增强LLM代理能够实际部署，解决了现有系统的延迟瓶颈问题。",
      "conclusion": "SwiftMem的主要贡献在于通过创新的查询感知索引机制，解决了代理内存系统的检索效率问题，实现了亚线性检索速度，为内存增强LLM代理的实时交互和实际应用提供了可能。其学术价值在于提出了结合时间与语义索引的新方法，实际应用价值在于支持高效、低延迟的代理系统部署。未来工作方向可能包括进一步优化索引结构或扩展到更复杂的场景，但摘要未明确说明具体局限性。",
      "tags": [
        "Query-aware Indexing",
        "Temporal Index",
        "Semantic DAG-Tag Index",
        "Embedding-Tag Co-consolidation",
        "LLM Agents"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:02.123383Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08158",
    "title": "WISE-Flow: Workflow-Induced Structured Experience for Self-Evolving Conversational Service Agents",
    "authors": [
      "Yuqing Zhou",
      "Zhuoer Wang",
      "Jie Yuan",
      "Hong Wang",
      "Samson Koelle",
      "Ziwei Zhu",
      "Wei Niu"
    ],
    "abstract": "Large language model (LLM)-based agents are widely deployed in user-facing services but remain error-prone in new tasks, tend to repeat the same failure patterns, and show substantial run-to-run variability. Fixing failures via environment-specific training or manual patching is costly and hard to scale. To enable self-evolving agents in user-facing service environments, we propose WISE-Flow, a workflow-centric framework that converts historical service interactions into reusable procedural experience by inducing workflows with prerequisite-augmented action blocks. At deployment, WISE-Flow aligns the agent's execution trajectory to retrieved workflows and performs prerequisite-aware feasibility reasoning to achieve state-grounded next actions. Experiments on ToolSandbox and $τ^2$-bench show consistent improvement across base models.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.08158.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08158",
    "published": "2026-01-13T02:43:41Z",
    "updated": "2026-01-13T02:43:41Z",
    "comment": "19 pages",
    "light_analysis": {
      "overview": "论文提出WISE-Flow框架，通过工作流诱导结构化经验，实现基于大语言模型的对话服务代理的自我进化。",
      "motivation": "基于大语言模型的代理在用户服务中广泛应用，但面临在新任务中容易出错、重复失败模式以及运行间变异性大的挑战。现有方法如环境特定训练或手动修复成本高昂且难以扩展，限制了代理的适应性和可靠性。因此，研究旨在开发自我进化代理，以自动化方式改进性能，应对服务环境中的动态需求，提升代理的鲁棒性和可扩展性。",
      "method": "WISE-Flow是一个以工作流为中心的框架，核心方法是将历史服务交互转化为可重用的程序经验。通过诱导带有前提条件增强的动作块来构建结构化工作流，实现经验的结构化提取。在部署阶段，系统检索相关工作流，将代理的执行轨迹对齐到工作流，并执行前提感知的可行性推理，生成基于当前状态的下一步动作。关键创新在于工作流诱导和前提感知推理机制，提升了决策的精确性和适应性。实验使用了ToolSandbox和τ^2-bench数据集来验证方法有效性。",
      "result": "在ToolSandbox和τ^2-bench数据集上的实验表明，WISE-Flow框架在多种基础模型上均带来一致的性能改进。摘要未明确说明具体指标如准确率或效率提升数值，但强调了改进的普遍性，表明该方法有效降低了错误率并增强了代理的稳定性和可靠性，与基线方法相比表现更优。",
      "conclusion": "WISE-Flow的主要贡献是提出了一个工作流诱导的结构化经验框架，使对话服务代理能够自我进化，自动化地改进性能。学术上，该方法为代理学习提供了新的工作流诱导和推理机制；实际上，它降低了维护成本，提升了服务代理的适应性。未来工作可探索更多任务类型、改进推理效率或扩展框架适用范围，摘要未明确说明具体局限性。",
      "tags": [
        "Workflow Induction",
        "Prerequisite-Augmented Action Blocks",
        "Feasibility Reasoning",
        "Self-Evolving Agents",
        "LLM-based Agents"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:15.971335Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08156",
    "title": "Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions",
    "authors": [
      "Arin Gopalan Yadav",
      "Varad Dherange",
      "Kumar Shivam"
    ],
    "abstract": "This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.08156.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08156",
    "published": "2026-01-13T02:38:27Z",
    "updated": "2026-01-13T02:38:27Z",
    "comment": "We propose and evaluate a hierarchical LLM-driven multi-agent framework for adaptive disruption management in last-mile logistics, integrating planning, coordination, and natural-language reasoning. The system is validated through simulation-based experiments and qualitative analysis. Includes figures and tables. 33 pages",
    "light_analysis": {
      "overview": "本文提出 Project Synapse 框架，一种分层多代理架构，结合混合记忆用于自主解决最后一英里交付中断问题。",
      "motivation": "最后一英里交付中断是物流领域的实际难题，频繁发生影响服务效率和客户满意度。通过定性分析超过 6,000 条真实用户评论，研究发现现有方法往往依赖人工干预或简单规则，难以灵活应对复杂场景，缺乏自主决策能力。因此，需要一种新颖的代理框架来自动化解决这些中断，提升物流系统的智能化和适应性。该问题的重要性在于其直接关联现实运营成本和用户体验，推动技术进步。",
      "method": "论文采用分层多代理架构，其中中央 Resolution Supervisor 代理负责战略任务分解，委托子任务给专门的工人代理进行战术执行。系统使用 LangGraph 工具管理复杂和周期性工作流程，确保任务编排的灵活性。验证框架时，构建了一个基准数据集，包含 30 个复杂中断场景，这些场景源于对大量真实用户评论的深入分析。性能评估采用 LLM-as-a-Judge 协议，并实施明确的偏见减轻措施，以客观衡量系统表现，体现了技术创新和实际验证的结合。",
      "result": "系统性能通过 LLM-as-a-Judge 协议进行评估，表明框架能够有效处理基准数据集中的复杂中断场景。然而，摘要未明确说明具体性能指标如准确率、效率改进或响应时间，也未提供与基线方法的详细对比结果。评估侧重于验证框架的可行性和功能性，可能暗示在自主解决能力方面取得进展，但量化效果需进一步实验支撑。",
      "conclusion": "Project Synapse 的主要贡献是提出了一种分层多代理框架，为最后一英里交付中断提供自主解决方案，使用真实场景数据验证并通过 LLM 协议评估。该研究具有学术价值，推动了多代理系统和 AI 在物流领域的应用，以及实际价值，提升了中断处理的自动化水平，减少对人工的依赖。局限性或未来工作方向摘要未明确说明，但可能涉及扩展场景或优化算法。",
      "tags": [
        "Multi-Agent Systems",
        "LangGraph",
        "Large Language Model",
        "Autonomous Resolution",
        "Workflow Orchestration"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:19.774594Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08155",
    "title": "Instance-Aligned Captions for Explainable Video Anomaly Detection",
    "authors": [
      "Inpyo Song",
      "Minjun Joo",
      "Joonhyung Kwon",
      "Eunji Jeon",
      "Jangwon Lee"
    ],
    "abstract": "Explainable video anomaly detection (VAD) is crucial for safety-critical applications, yet even with recent progress, much of the research still lacks spatial grounding, making the explanations unverifiable. This limitation is especially pronounced in multi-entity interactions, where existing explainable VAD methods often produce incomplete or visually misaligned descriptions, reducing their trustworthiness. To address these challenges, we introduce instance-aligned captions that link each textual claim to specific object instances with appearance and motion attributes. Our framework captures who caused the anomaly, what each entity was doing, whom it affected, and where the explanationis grounded, enabling verifiable and actionable reasoning. We annotate eight widely used VAD benchmarks and extend the 360-degree egocentric dataset, VIEW360, with 868 additional videos, eight locations, and four new anomaly types, creating VIEW360+, a comprehensive testbed for explainable VAD. Experiments show that our instance-level spatially grounded captions reveal significant limitations in current LLM- and VLM-based methods while providing a robust benchmark for future research in trustworthy and interpretable anomaly detection.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08155.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08155",
    "published": "2026-01-13T02:37:32Z",
    "updated": "2026-01-13T02:37:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出实例对齐的标注方法，将文本解释与视频中具体对象实例对齐，以提升视频异常检测的可解释性和可信度，使解释可验证且具有可操作性。",
      "motivation": "可解释视频异常检测对安全关键应用至关重要，但现有方法常缺乏空间定位能力，导致在多实体交互场景中产生不完整或视觉不对齐的解释，降低了方法的可信度和可验证性。这限制了实用价值，尤其是在需要精确解释异常事件的领域。因此，研究旨在解决这一不足，通过引入可验证的解释框架来增强方法的可靠性和实用性。",
      "method": "论文的核心方法是引入实例对齐的标注，将每个文本声明链接到视频中的特定对象实例，包含外观和运动属性，从而捕捉异常原因、实体行为、影响对象和解释的空间基础。框架构建了一个综合的测试平台，包括标注八个VAD基准和扩展VIEW360数据集，新增868个视频、八个地点和四种新异常类型，形成VIEW360+数据集，以支持全面评估。",
      "result": "实验结果表明，论文提出的实例级空间grounded标注揭示了当前基于大型语言模型和视觉-语言模型的方法在可解释性方面存在显著局限性，并提供了一个健壮的基准来评估和提升异常检测方法的可信度。这些标注强调了现有解释方法在多实体场景中的不足，为未来研究在可信任和可解释异常检测领域设立了新标准。摘要未明确说明具体性能指标，但通过实证分析展示了该框架的优势。",
      "conclusion": "论文的主要贡献在于提出了一个通过实例对齐标注实现可验证和可操作解释的框架，显著提高了视频异常检测的可信度和实用性。其学术价值体现在为可信任和可解释异常检测领域提供了新方向，VIEW360+数据集的创建丰富了研究资源。未来工作可进一步优化方法在复杂场景的扩展性和效率，以应对更广泛的挑战。",
      "tags": [
        "Video Anomaly Detection",
        "Explainable AI",
        "Instance Alignment",
        "Spatial Grounding",
        "Multi-Entity Interaction"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:29.280198Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08151",
    "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
    "authors": [
      "Shezheng Song",
      "Shasha Li",
      "Jie Yu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08151.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08151",
    "published": "2026-01-13T02:26:21Z",
    "updated": "2026-01-13T02:26:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过分析多模态大型语言模型中的视觉融合演变，提出无训练对比注意力框架以提升多模态推理性能。",
      "motivation": "多模态大型语言模型（MLLMs）在视觉语言理解中虽取得显著进展，但其内部如何整合视觉和文本信息仍不清楚，这限制了模型的可解释性和优化潜力。现有方法缺乏对融合机制的系统性理解，导致注意力可能分散在无关区域，影响效率。因此，本研究旨在填补这一空白，通过深入分析融合过程以改进模型性能。",
      "method": "研究采用系统性层掩码分析和层注意力演化分析，探索多个MLLM架构中视觉-文本融合的演变。关键创新包括揭示融合在特定层集中而非均匀分布，以及后期“回顾”现象，其中视觉信号在输出前重新激活。此外，基于观察到的注意力噪声和文本对齐区域变化，提出无训练的对比注意力框架，建模从早期融合到最终层的注意力转移。",
      "result": "通过在多MLLMs和基准测试上的广泛实验，验证了融合分析和对比注意力框架的有效性。结果表明，该方法能显著提高多模态推理性能，尽管具体数据摘要未明确说明，但与基线方法相比，注意力优化带来了明显改进。",
      "conclusion": "本研究深入理解了MLLMs中视觉-文本融合的机制，发现融合集中在特定层并有动态激活模式。提出的无训练对比注意力框架增强了模型推理能力，具有提升AI可解释性和实际应用的学术价值。未来工作可扩展框架到更多任务或模态，以进一步优化性能。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Fusion",
        "Contrastive Attention",
        "Layer-wise Analysis",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:10.276163Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08149",
    "title": "Dynamic Graph Structure Learning via Resistance Curvature Flow",
    "authors": [
      "Chaoqun Fei",
      "Huanjiang Liu",
      "Tinglve Zhou",
      "Yangyang Li",
      "Tianyong Hao"
    ],
    "abstract": "Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.08149.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08149",
    "published": "2026-01-13T02:23:32Z",
    "updated": "2026-01-13T02:23:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于电阻曲率流的动态图结构学习框架，通过高效矩阵操作实现几何优化，解决了传统方法计算复杂度高的问题。",
      "motivation": "传统静态图构建方法基于欧几里得距离，往往无法捕获数据流形的内在曲率特性，导致表示学习质量受限。尽管Ollivier-Ricci曲率流（OCF）在动态拓扑优化中表现优异，但其依赖最优传输（Wasserstein距离）带来高昂计算复杂度，严重制约了在大规模数据集和深度学习框架中的应用。因此，急需开发一种高效且能保持几何特性的新方法来提升图结构学习效果，以支持更广泛的机器学习任务。",
      "method": "论文提出电阻曲率流（RCF）框架，利用电路物理中的有效电阻概念，将昂贵的曲率优化转化为高效矩阵运算。核心创新包括深入探索RCF的理论基础和动力学原理，解释它如何通过曲率梯度指导边权重重新分布，以消除拓扑噪声并强化局部聚类结构。基于此设计了图优化算法DGSL-RCF，兼容深度学习模型，实现大规模应用，强调其机制在流形增强和噪声抑制中的作用。",
      "result": "实验结果显示，电阻曲率流（RCF）实现了超过100倍的计算加速，同时保持了与Ollivier-Ricci曲率流（OCF）相当的几何优化能力。在深度度量学习、流形学习和图结构学习的任务中，DGSL-RCF算法显著提升了表示质量和下游任务性能。与基线方法相比，这一框架有效降低了计算成本，同时提高了表示学习的准确性和效率，摘要未明确说明具体性能指标提升，但强调了总体效果的改善。",
      "conclusion": "本研究的主要贡献是提出了电阻曲率流（RCF）框架和DGSL-RCF算法，解决了动态图结构学习中的计算瓶颈。学术上，它提供了理论机制解释，推动了几何表示学习和图拓扑优化的发展；实际上，适用于大规模数据集和深度学习框架，提升了表示质量和下游任务应用价值。未来工作可进一步探索RCF在其他机器学习领域的应用或优化算法的可扩展性。",
      "tags": [
        "Geometric Representation Learning",
        "Dynamic Graph Learning",
        "Resistance Curvature Flow",
        "Effective Resistance",
        "Graph Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:27.887629Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06874",
    "title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation",
    "authors": [
      "Changli Wu",
      "Haodong Wang",
      "Jiayi Ji",
      "Yutian Yao",
      "Chunsai Du",
      "Jihua Kang",
      "Yanwei Fu",
      "Liujuan Cao"
    ],
    "abstract": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.06874.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06874",
    "published": "2026-01-11T11:44:07Z",
    "updated": "2026-01-13T17:29:39Z",
    "comment": "Project Website: https://sosppxo.github.io/mvggt.github.io/",
    "light_analysis": {
      "overview": "提出MVGGT框架，集成语言与稀疏视图几何，通过PVSO优化解决前景梯度稀释，实现高效端到端多视图3D引用表达式分割。",
      "motivation": "研究旨在解决现实世界中机器人等代理仅具备稀疏RGB视图时的3D引用表达式分割问题。现有方法依赖密集点云，不适合低延迟和稀疏视图环境，且传统两阶段流程先重建点云再分割，导致几何质量差、目标区域不精确和速度慢。因此，需要开发一个高效端到端框架，直接在稀疏多视图图像上集成语言信息进行分割，以应对实际应用挑战。",
      "method": "MVGGT是一个多模态视觉几何基础Transformer，采用双分支设计将语言信息融入稀疏视图的几何推理中，实现端到端处理。关键创新是Per-view No-target Suppression Optimization (PVSO)，用于解决训练中由于稀疏3D信号导致的前景梯度稀释问题，提供更强和更平衡的梯度，促进稳定学习。框架直接输入稀疏多视图图像和语言表达式，输出分割结果，无需中间点云重建。",
      "result": "实验在新建的MVRefer基准上进行，MVGGT成为该任务的首个强基线。它实现了高准确率和快速推理速度，显著优于现有替代方法。尽管摘要未提供具体性能指标，但结果表明其在稀疏多视图3D引用表达式分割中表现出色，验证了PVSO优化和端到端设计的有效性。",
      "conclusion": "本研究的主要贡献是提出了MVGGT框架和PVSO优化方法，解决了稀疏视图3D引用表达式分割的优化难题。学术上，它推动了多模态视觉几何研究，并建立了MVRefer基准用于标准化评估。实际应用价值体现在机器人导航和移动设备等场景中，但摘要未明确提及局限性，未来工作可能涉及扩展到更复杂数据集或应用环境。",
      "tags": [
        "3D Referring Expression Segmentation",
        "Multimodal Transformer",
        "Sparse View Geometry",
        "Per-view No-target Suppression Optimization",
        "MV-3DRES Benchmark"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:54.125945Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06663",
    "title": "SafePro: Evaluating the Safety of Professional-Level AI Agents",
    "authors": [
      "Kaiwen Zhou",
      "Shreedhar Jangam",
      "Ashwin Nagarajan",
      "Tejas Polu",
      "Suhas Oruganti",
      "Chengzhi Liu",
      "Ching-Chen Kuo",
      "Yuting Zheng",
      "Sravana Narayanaraju",
      "Xin Eric Wang"
    ],
    "abstract": "Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \\textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.06663.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06663",
    "published": "2026-01-10T19:53:09Z",
    "updated": "2026-01-13T18:20:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出SafePro基准，用于全面评估专业级AI代理在复杂任务中的安全对齐性能。",
      "motivation": "AI代理正从简单的对话助手演变为能够执行复杂专业任务的自主系统，这虽然带来了生产力提升，但也引入了未被充分探索的关键安全风险。现有安全评估主要针对简单的日常协助任务，无法有效捕捉专业环境中复杂的决策过程和潜在后果，导致在专业活动中可能存在误对齐行为的严重隐患，因此亟需开发专门的安全评估方法来填补这一空白。",
      "method": "论文提出了SafePro基准，这是一个综合性的评估框架，专门用于衡量AI代理在执行专业活动时的安全对齐。该方法通过严格的迭代创建和审查过程，开发了一个跨多个专业领域的高复杂性任务数据集，其中包含了与安全风险相关的场景。关键创新点在于专注于专业背景下的安全评估，利用该数据集对当前顶尖的AI模型进行测试，以系统分析其安全性能，数据集的设计强调了多样性和复杂性，以模拟真实世界专业任务的需求。",
      "result": "对现有AI模型的评估显示，在专业任务中存在显著的安全漏洞和新的不安全行为，模型在复杂专业环境中表现出不足的安全判断和薄弱的安全对齐。摘要未明确说明具体性能指标如准确率或效率数据，但与现有简单任务评估相比，SafePro基准揭示了更广泛的安全问题。此外，研究探讨了安全缓解策略，并观察到这些策略应用后安全性能的改善，表明通过针对性措施可以提升代理的安全性，但具体改进程度未详细量化。",
      "conclusion": "SafePro基准的贡献在于提供了一个专门评估专业AI代理安全性的工具，揭示了当前模型在复杂任务中的安全风险。这项研究强调了为下一代专业AI代理开发鲁棒安全机制的学术价值和实际应用重要性，推动了安全评估向专业领域扩展。潜在局限性包括数据集可能未覆盖所有专业场景，未来工作可以进一步优化基准、探索更多安全策略，并扩展到更广泛的专业应用中以提升整体安全性。",
      "tags": [
        "Large Language Model",
        "AI Agents",
        "Safety Benchmark",
        "Professional Tasks",
        "Safety Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:52.608350Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06575",
    "title": "Are Emotions Arranged in a Circle? Geometric Analysis of Emotion Representations via Hyperspherical Contrastive Learning",
    "authors": [
      "Yusuke Yamauchi",
      "Akiko Aizawa"
    ],
    "abstract": "Psychological research has long utilized circumplex models to structure emotions, placing similar emotions adjacently and opposing ones diagonally. Although frequently used to interpret deep learning representations, these models are rarely directly incorporated into the representation learning of language models, leaving their geometric validity unexplored. This paper proposes a method to induce circular emotion representations within language model embeddings via contrastive learning on a hypersphere. We show that while this circular alignment offers superior interpretability and robustness against dimensionality reduction, it underperforms compared to conventional designs in high-dimensional settings and fine-grained classification. Our findings elucidate the trade-offs involved in applying psychological circumplex models to deep learning architectures.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.06575.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06575",
    "published": "2026-01-10T13:54:06Z",
    "updated": "2026-01-13T03:30:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种通过高维球体对比学习在语言模型中诱导圆形情感表示的方法，以探索心理学圆环模型在深度学习中的应用。",
      "motivation": "心理学研究长期使用圆环模型来结构情感，但在深度学习领域，这些模型通常仅用于解释学习到的表示，而未被直接融入语言模型的表示学习过程，导致其几何有效性未得到充分验证。现有方法忽略了几何对齐，限制了情感表示的可解释性和鲁棒性，因此本研究旨在解决如何有效结合心理学模型来改进情感表示学习，并评估其实际优势。",
      "method": "论文的核心方法是基于高维球体的对比学习，通过在嵌入空间中诱导圆形情感表示来对齐心理学圆环模型。关键创新点包括将相似情感置于相邻位置、相反情感置于对角位置，利用对比损失函数优化几何结构。技术特色涉及使用高维球体以保持几何属性，但摘要未明确说明具体使用的数据集或模型架构细节，仅提及语言模型和对比学习框架。",
      "result": "实验结果显示，提出的圆形情感表示方法在可解释性和对降维的鲁棒性方面优于传统设计，能提供更清晰的情感结构。然而，在高维设置和细粒度分类任务中，该方法的表现不如常规表示学习技术，摘要未明确说明具体性能指标如准确率提升，但突出了在应用心理学模型时需权衡可解释性与分类性能。",
      "conclusion": "本研究的主要贡献在于验证了将心理学圆环模型融入深度学习方法的可行性，阐明了几何对齐在情感表示学习中的优缺点。学术价值体现在桥接了心理学和机器学习领域，增强了表示的可解释性；实际应用可适用于情感分析任务，但需注意性能限制。未来工作可探索改进技术以在高维场景下平衡可解释性与分类效果。",
      "tags": [
        "Emotion Representation",
        "Contrastive Learning",
        "Hyperspherical Learning",
        "Language Model",
        "Circumplex Model"
      ]
    },
    "analyzed_at": "2026-01-14T03:30:48.235046Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06477",
    "title": "IndRegBias: A Dataset for Studying Indian Regional Biases in English and Code-Mixed Social Media Comments",
    "authors": [
      "Debasmita Panda",
      "Akash Anil",
      "Neelesh Kumar Shukla"
    ],
    "abstract": "Warning: This paper consists of examples representing regional biases in Indian regions that might be offensive towards a particular region.   While social biases corresponding to gender, race, socio-economic conditions, etc., have been extensively studied in the major applications of Natural Language Processing (NLP), biases corresponding to regions have garnered less attention. This is mainly because of (i) difficulty in the extraction of regional bias datasets, (ii) disagreements in annotation due to inherent human biases, and (iii) regional biases being studied in combination with other types of social biases and often being under-represented. This paper focuses on creating a dataset IndRegBias, consisting of regional biases in an Indian context reflected in users' comments on popular social media platforms, namely Reddit and YouTube. We carefully selected 25,000 comments appearing on various threads in Reddit and videos on YouTube discussing trending topics on regional issues in India. Furthermore, we propose a multilevel annotation strategy to annotate the comments describing the severity of regional biased statements. To detect the presence of regional bias and its severity in IndRegBias, we evaluate open-source Large Language Models (LLMs) and Indic Language Models (ILMs) using zero-shot, few-shot, and fine-tuning strategies. We observe that zero-shot and few-shot approaches show lower accuracy in detecting regional biases and severity in the majority of the LLMs and ILMs. However, the fine-tuning approach significantly enhances the performance of the LLM in detecting Indian regional bias along with its severity.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.06477.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06477",
    "published": "2026-01-10T08:13:03Z",
    "updated": "2026-01-13T06:53:27Z",
    "comment": "Preprint. Under review",
    "light_analysis": {
      "overview": "本论文创建了IndRegBias数据集用于研究印度区域偏见，并通过多级标注策略和微调方法提升了大型语言模型在偏见检测中的性能。",
      "motivation": "在自然语言处理应用中，社会偏见如性别、种族等已被广泛研究，但区域偏见因数据提取困难、标注分歧以及常与其他偏见结合而受到较少关注。本研究旨在解决印度背景下的区域偏见问题，指出现有方法不足，导致区域偏见在研究中被低估，从而强调了开发专门数据集和技术以促进偏见检测和公平AI发展的重要性。",
      "method": "论文提出了IndRegBias数据集，包含25,000条来自Reddit和YouTube的英文及代码混合评论，聚焦印度区域偏见。采用多级标注策略评估偏见的严重性。技术评估中，使用开源大型语言模型和印度语言模型，通过零样本、少样本和微调策略来检测区域偏见及其严重程度。",
      "result": "实验显示，零样本和少样本方法在大多数大型语言模型和印度语言模型中检测区域偏见及其严重性时准确率较低。然而，通过微调方法，大型语言模型的性能显著提升，能够更有效地识别印度区域偏见及其严重性，突显了微调策略在偏见检测任务中的优越性。",
      "conclusion": "本研究的主要贡献是提供了首个印度区域偏见数据集IndRegBias，并验证了微调方法在偏见检测中的有效性，为未来区域偏见研究和公平AI发展提供了宝贵资源。其学术价值在于填补了区域偏见研究的空白，实际应用潜力包括开发偏见缓解工具。未来工作可扩展至其他区域或多语言场景。",
      "tags": [
        "Regional Bias",
        "Dataset Creation",
        "Large Language Models",
        "Fine-tuning",
        "Annotation Strategy"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:06.376901Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06002",
    "title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "authors": [
      "Qiguang Chen",
      "Yantao Du",
      "Ziniu Li",
      "Jinhao Liu",
      "Songyao Duan",
      "Jiarui Guo",
      "Minghao Liu",
      "Jiaheng Liu",
      "Tong Yang",
      "Ge Zhang",
      "Libo Qin",
      "Wanxiang Che",
      "Wenhao Huang"
    ],
    "abstract": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.06002.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06002",
    "published": "2026-01-09T18:39:01Z",
    "updated": "2026-01-13T18:21:01Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "该论文提出长链思考推理的分子结构理论，并开发Mole-Syn分布转移图方法，以提升大型语言模型在长链思考任务中的性能和强化学习稳定性。",
      "motivation": "研究动机源于大型语言模型在学习长链思考推理时常失败，尤其是在通过模仿人类或非长链思考模型时。这问题重要，因为长链推理对复杂任务至关重要，现有模仿学习方法无法有效捕获推理轨迹的结构化特征，导致学习效率低下和性能瓶颈。因此，需要探索新的理论框架来理解并改进长链思考学习过程。",
      "method": "研究方法基于将长链思考轨迹视为具有分子样结构的统一视图，包括三种交互类型：深度推理（类似共价键）、自我反思（类似氢键）和自我探索（类似范德华力）。通过分析蒸馏轨迹，作者发现这些结构源自长链思考微调而非关键词模仿。创新地引入有效语义异构体概念，并开发Mole-Syn方法，这是一种分布转移图技术，用于指导合成有效的长链思考结构，以优化学习过程。",
      "result": "论文提出的方法在多个基准测试中提升了性能和强化学习稳定性。摘要未明确说明具体数据，但显示Mole-Syn方法能够通过优化长链思考结构来改善训练效果。与基线方法相比，该改进归因于结构合成的引导，使得学习过程更稳定，避免了结构竞争导致的训练障碍，从而提升了整体性能。",
      "conclusion": "该论文的主要贡献在于提供了长链思考推理的分子结构理论框架和Mole-Syn实践方法，为理解大型语言模型的推理学习提供了新视角。学术上，它增强了推理学习的理论基础；实际上，能显著提升模型在复杂任务中的表现。未来工作可能包括进一步验证理论框架，并扩展到其他推理任务或模型架构中。",
      "tags": [
        "Chain-of-Thought Reasoning",
        "Fine-tuning",
        "Reinforcement Learning",
        "Molecular Structure Analogy",
        "Distribution Transfer Graph"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:19.479722Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06227",
    "title": "When Smaller Wins: Dual-Stage Distillation and Pareto-Guided Compression of Liquid Neural Networks for Edge Battery Prognostics",
    "authors": [
      "Dhivya Dharshini Kannan",
      "Wei Li",
      "Wei Zhang",
      "Jianbiao Wang",
      "Zhi Wei Seh",
      "Man-Fai Ng"
    ],
    "abstract": "Battery management systems increasingly require accurate battery health prognostics under strict on-device constraints. This paper presents DLNet, a practical framework with dual-stage distillation of liquid neural networks that turns a high-capacity model into compact and edge-deployable models for battery health prediction. DLNet first applies Euler discretization to reformulate liquid dynamics for embedded compatibility. It then performs dual-stage knowledge distillation to transfer the teacher model's temporal behavior and recover it after further compression. Pareto-guided selection under joint error-cost objectives retains student models that balance accuracy and efficiency. We evaluate DLNet on a widely used dataset and validate real-device feasibility on an Arduino Nano 33 BLE Sense using int8 deployment. The final deployed student achieves a low error of 0.0066 when predicting battery health over the next 100 cycles, which is 15.4% lower than the teacher model. It reduces the model size from 616 kB to 94 kB with 84.7% reduction and takes 21 ms per inference on the device. These results support a practical smaller wins observation that a small model can match or exceed a large teacher for edge-based prognostics with proper supervision and selection. Beyond batteries, the DLNet framework can extend to other industrial analytics tasks with strict hardware constraints.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.06227.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06227",
    "published": "2026-01-09T10:20:21Z",
    "updated": "2026-01-13T02:59:27Z",
    "comment": "Submitted to International Conference on Pattern Recognition, ICPR 2026",
    "light_analysis": {
      "overview": "DLNet框架通过双阶段蒸馏和帕累托引导压缩，将液体神经网络转化为适合边缘部署的小型模型，用于电池健康预测。",
      "motivation": "电池管理系统在严格的边缘设备约束下需要精确的电池健康预测，以确保设备寿命和安全，但现有高容量模型由于计算和存储资源限制难以直接部署，导致预测精度和效率不足。因此，研究旨在开发一种高效压缩方法，将复杂模型转化为紧凑、可边缘部署的形式，解决模型过大导致的部署困难问题，提升实际应用中的可行性。",
      "method": "DLNet采用欧拉离散化将液体神经网络的动力学适配到嵌入式系统，然后进行双阶段知识蒸馏：第一阶段转移教师模型的时间行为，第二阶段在压缩后恢复性能。通过帕累托优化在联合错误和成本目标下选择学生模型，平衡准确性和效率，使用Arduino Nano进行int8部署验证。关键创新点包括液体神经网络的离散化处理和蒸馏过程的优化策略。",
      "result": "在广泛使用的数据集上评估，DLNet在Arduino Nano上部署的最终学生模型预测未来100个周期的电池健康时，错误率降至0.0066，比教师模型降低15.4%。模型大小从616 kB压缩到94 kB，减少84.7%，推理时间仅21 ms。这些结果表明压缩后的模型在准确性和效率上均优于基线，支持边缘部署的可行性。",
      "conclusion": "DLNet框架成功展示了在适当监督和选择下，小模型能匹配或超越大模型用于边缘预测任务，主要贡献在于提供了一种实用的蒸馏和压缩方法。其学术价值在于推动了液体神经网络在资源受限环境中的应用，实际应用价值扩展到电池管理和其他工业分析任务。未来工作方向可能包括进一步优化压缩技术或扩展到更多领域，摘要未明确说明具体局限性。",
      "tags": [
        "Liquid Neural Networks",
        "Knowledge Distillation",
        "Pareto Optimization",
        "Edge Computing",
        "Model Compression"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:10.556699Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06224",
    "title": "Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization",
    "authors": [
      "Miao Pan",
      "Wangjie Gan",
      "Jintao Chen",
      "Wenqi Zhang",
      "Bing Sun",
      "Jianwei Yin",
      "Xuhong Zhang"
    ],
    "abstract": "While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization. This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates. To address these challenges, we propose a comprehensive framework comprising three core modules. First, we enhance visual localization by introducing dedicated planning and captioning stages before the reasoning phase, employing a quality-based caption reward to ensure accurate initial anchoring. Second, to improve exploration, we categorize samples based on the mean and variance of their reward distributions, prioritizing samples with high variance to focus the model on diverse and informative data. Finally, to mitigate sample interference, we regulate NTK similarity by grouping sample pairs and applying an InfoNCE loss to push overly similar pairs apart and pull dissimilar ones closer, thereby guiding gradient interactions toward a balanced range. Experimental results demonstrate that our proposed method significantly reduces hallucination rates and effectively enhances the inference accuracy of MLLMs.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.06224.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06224",
    "published": "2026-01-09T07:59:18Z",
    "updated": "2026-01-13T07:12:55Z",
    "comment": "AAAI-2026 Poster",
    "light_analysis": {
      "overview": "本论文提出一个通过字幕反馈、多样性感知采样和冲突正则化来抵抗多模态大语言模型幻觉的综合框架。",
      "motivation": "多模态大语言模型（MLLMs）在多种任务中取得了显著成功，但其实际部署受到幻觉问题的严重阻碍，尤其是在强化学习优化过程中。研究动机在于系统分析RL训练中幻觉的根本原因，识别出三个关键因素：过度依赖链式视觉推理导致错误前提锚定、探索多样性不足造成过度自信输出，以及训练样本间的破坏性冲突引发不稳定参数更新。这些问题降低了模型的可靠性和实用性，现有方法可能未全面解决这些因素，因此需要一种系统化方法来减轻幻觉。",
      "method": "论文提出一个综合框架，包含三个核心模块以解决幻觉问题。首先，增强视觉定位，在推理阶段前引入规划和字幕阶段，使用基于质量的字幕奖励确保准确初始锚定。其次，改进探索，通过基于奖励分布均值和方差的样本分类，优先高方差样本以增加多样性和信息量。最后，缓解样本干扰，调节神经切线核相似性，分组样本对并应用InfoNCE损失来平衡梯度交互。关键创新点包括专用规划、多样性感知采样和冲突正则化。摘要未明确说明使用的具体模型架构和数据集。",
      "result": "实验结果表明，论文提出的方法显著降低了多模态大语言模型的幻觉率，并有效提升了推理准确性。与基线方法相比，该综合框架通过减轻幻觉在性能上表现优异，但摘要未提供具体的性能指标数据，如准确率提升百分比或效率改进细节。",
      "conclusion": "本论文的主要贡献是提出一个通过字幕反馈、多样性感知采样和冲突正则化来抵抗MLLMs幻觉的综合框架。研究具有重要的学术价值，为MLLMs在强化学习训练中的幻觉问题提供了系统解决方案，并增强了模型的实用性和部署可行性。摘要未明确说明潜在的局限性或未来工作方向。",
      "tags": [
        "Multimodal Large Language Models",
        "Reinforcement Learning",
        "Neural Tangent Kernel",
        "InfoNCE Loss",
        "Caption Feedback"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:42.500997Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05371",
    "title": "The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection",
    "authors": [
      "Md Shafiqul Islam",
      "Shakti Prasad Padhy",
      "Douglas Allaire",
      "Raymundo Arróyave"
    ],
    "abstract": "Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05371.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05371",
    "published": "2026-01-08T20:52:53Z",
    "updated": "2026-01-13T16:22:49Z",
    "comment": "Included a subsection named \"Budgetary impact of inline kernel optimization during BO\", and corrected label of a figure",
    "light_analysis": {
      "overview": "论文提出了一种基于核流形几何的贝叶斯优化框架，用于高效选择高斯过程的协方差核。",
      "motivation": "Gaussian Process回归性能高度依赖核函数选择，但合适的核选择在概率建模中计算昂贵且具挑战性，现有方法如网格搜索效率低下，限制了模型质量和实际应用，因此需要一种几何引导的高效搜索策略来解决这一核心问题。",
      "method": "研究开发了一个贝叶斯优化框架，通过计算高斯过程先验间的预期散度距离构建核空间距离矩阵，使用多维缩放将离散核库嵌入连续欧几里得流形，输入为核组合，目标函数是log边际似然，特征化由MDS坐标提供，实现平滑且稳定的核搜索。",
      "result": "在合成基准、真实时间序列数据集和增材制造预测熔池几何的案例中，该方法相比基线（包括大型语言模型引导搜索）实现了更优的预测准确性和不确定性校准，摘要未明确说明具体数据，但验证了其多场景有效性。",
      "conclusion": "该框架建立了可重用的概率几何结构，提升了核选择效率，对高斯过程建模和深度核学习有直接应用价值，贡献在于优化贝叶斯模型参数选择，未来可扩展到其他概率模型优化问题。",
      "tags": [
        "Gaussian Process",
        "Bayesian Optimization",
        "Multidimensional Scaling",
        "Kernel Geometry",
        "Deep Kernel Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:47.540784Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06204",
    "title": "Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification",
    "authors": [
      "Tayyab Rehman",
      "Giovanni De Gasperis",
      "Aly Shmahell"
    ],
    "abstract": "Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.",
    "categories": [
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.06204.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06204",
    "published": "2026-01-08T11:31:47Z",
    "updated": "2026-01-13T14:40:15Z",
    "comment": "Author email changed, Acknowlegement changes",
    "light_analysis": {
      "overview": "本论文提出了一个级联多智能体框架，统一重建、对象检测和视觉语言模型，实现了高效且可解释的监控系统异常检测。",
      "motivation": "在动态视觉环境中，智能异常检测需要平衡实时性能与语义可解释性，但现有方法存在局限性。基于重建的模型仅捕获低级偏差而缺乏上下文推理，对象检测器速度较快但语义信息有限，大型视觉语言系统虽提供可解释性但计算成本过高，难以满足实际监控系统的需求。这些不足促使本研究开发一种综合性框架，以克服片段化方法的缺陷，实现高效、可理解的异常检测，提升监控系统的实用性和准确性。",
      "method": "本论文提出一个级联多智能体框架，整合多种互补技术：早期模块执行重建门控过滤和对象级评估，快速筛选潜在异常；对于语义模糊事件，选择性调用高级推理智能体进行深度解释。系统采用自适应升级阈值优化资源分配，并通过发布-订阅通信机制实现异步协调，支持跨异构硬件的可扩展部署。关键创新点在于结合早期退出效率、自适应多智能体推理和可解释异常归因，构建了一个连贯且灵活的检测架构，利用视觉语言模型和基于嵌入的分类技术进行语义分析。",
      "result": "在大规模监控数据集上的评估显示，所提级联框架相比直接视觉语言推理，延迟降低三倍，显著提升了实时性能。同时，它保持了高感知保真度，PSNR达到38.3分贝，SSIM为0.965，并实现了一致的语义标注。这表明框架在速度和准确性方面优于传统检测方法，例如重建模型或对象检测器，在效率和可解释性之间取得了良好平衡，验证了其在实际应用中的有效性。",
      "conclusion": "本研究的主要贡献是提出一种级联多智能体框架，成功解决了智能异常检测中实时性能与语义可解释性的冲突，超越了传统检测流程。它结合早期退出效率、自适应推理和可解释归因，为可扩展智能视觉监控建立了可重复和节能的基础，具有重要的学术价值和实际应用潜力。未来工作可能涉及进一步优化硬件兼容性和扩展更多场景，但当前框架已为大规模监控系统提供了高效且可理解的解决方案。",
      "tags": [
        "Vision-Language Models",
        "Multi-Agent Systems",
        "Anomaly Detection",
        "Cascading Framework",
        "Publish-Subscribe Communication"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:36.807900Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04563",
    "title": "A Vision for Multisensory Intelligence: Sensing, Science, and Synergy",
    "authors": [
      "Paul Pu Liang"
    ],
    "abstract": "Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04563.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04563",
    "published": "2026-01-08T03:46:20Z",
    "updated": "2026-01-13T18:24:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一个多感官人工智能的研究愿景，通过传感、科学和协同三个主题推动AI从数字模态向多感官扩展。",
      "motivation": "人工智能当前主要局限于数字模态如文本、视觉和音频，而人类体验是多感官的，包括语言、视觉、声音、触觉、味觉和嗅觉。这种限制导致AI无法全面捕捉现实世界，阻碍了自然和丰富的人机交互。因此，研究多感官AI对于提升AI感知能力、改善用户体验以及应对复杂环境中的实际应用至关重要，现有方法的不足在于缺乏对多样化感官信号的整合和量化分析。",
      "method": "论文提出一个研究框架，基于三个相互关联的主题：传感技术扩展AI捕捉多模态信息的能力，例如通过生理、触觉和环境信号；科学基础包括量化多模态异质性和交互、开发统一建模架构和表示方法，以及理解跨模态迁移；协同学习涉及多感官整合、对齐、推理、生成、泛化和体验等新挑战。摘要未明确说明具体数据集或模型架构，但强调通过系统性方法推动领域进展。",
      "result": "摘要未明确说明具体实验结果或性能指标，但论文提到了MIT Media Lab的一系列项目、资源和演示，展示了多感官AI的最新进展，如实际应用案例或原型系统。因此，无法提供具体数据如准确率提升或效率改进，也未提及与基线方法的对比情况。建议参考论文中的项目链接以获取更多细节。",
      "conclusion": "本论文总结提出多感官AI的研究愿景，强调传感、科学和协同三个方向的重要性，旨在推动AI更自然地理解和交互现实世界。研究的学术价值在于建立多模态AI的理论基础，促进跨领域融合；实际应用价值包括智能家居、城市管理、医疗健康和娱乐等领域的创新。潜在的局限性是技术实现和验证的挑战，未来工作方向可能涉及具体算法开发、大规模实验和跨学科合作。",
      "tags": [
        "Multisensory Intelligence",
        "Multimodal Learning",
        "Cross-modal Transfer",
        "Sensor Technology",
        "Human-AI Interaction"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:24.980734Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03746",
    "title": "Whose Facts Win? LLM Source Preferences under Knowledge Conflicts",
    "authors": [
      "Jakob Schuster",
      "Vagrant Gautam",
      "Katja Markert"
    ],
    "abstract": "As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03746.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03746",
    "published": "2026-01-07T09:35:35Z",
    "updated": "2026-01-13T09:48:40Z",
    "comment": "Data and code: https://github.com/JaSchuste/llm-source-preference",
    "light_analysis": {
      "overview": "本文研究了大型语言模型在知识冲突下对信息来源的偏好，并提出一个方法来减少重复偏差的影响。",
      "motivation": "随着大型语言模型在检索增强生成流程中的广泛应用，研究它们在知识冲突下的行为变得至关重要。目前，信息来源在知识冲突解决中的作用尚未得到充分研究，现有方法往往忽略源因素，可能导致模型在处理不一致信息时产生偏见。本研究动机源于跨学科的可信度研究，旨在填补这一空白，探究源偏好如何影响LLM的知识冲突解决能力，以提升模型在实际应用中的可靠性和可信度。",
      "method": "本研究提出了一个新颖框架，用于系统研究大型语言模型在知识冲突下对信息来源的偏好。方法包括设计严格控制实验环境，评估13个开源LLM在解决英语知识冲突时的表现，以隔离源偏好因素并量化重复信息的影响。关键创新在于构建实验设置来模拟知识冲突，并分析模型响应以推断源偏好。摘要未明确说明使用的具体数据集和模型架构细节，但强调了实验的严谨性和多模型评估。",
      "result": "实验结果表明，大型语言模型倾向于优先选择机构核实的信息来源，如政府或报纸，而非个人和社交媒体信息。然而，简单地重复低可信度来源的信息可以逆转这种偏好。研究者提出的方法成功将重复偏差减少了高达99.8%，同时保持了至少88.8%的原始源偏好，与未处理状态相比，显著提高了模型在知识冲突下的一致性和可靠性，具体数据基于对13个LLM的评估得出。",
      "conclusion": "本研究的主要贡献在于首次系统揭示了大型语言模型在知识冲突下对信息来源的偏好，并提出了一个有效方法来减少重复偏差。学术上，这推动了知识密集型自然语言处理领域中可信度和源偏好研究的发展，有助于理解模型的行为机制。实践中，该方法可以提升LLM在检索增强生成任务中的可靠性和用户信任。未来工作可以进一步探索不同文化和语言下的源偏好，研究者已发布所有数据和代码以促进相关研究。摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Retrieval-Augmented Generation",
        "Knowledge Conflict",
        "Source Preference",
        "Repetition Bias"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:54.693722Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03683",
    "title": "Rethinking Recurrent Neural Networks for Time Series Forecasting: A Reinforced Recurrent Encoder with Prediction-Oriented Proximal Policy Optimization",
    "authors": [
      "Xin Lai",
      "Shiming Deng",
      "Lu Yu",
      "Yumin Lai",
      "Shenghao Qiao",
      "Xinze Zhang"
    ],
    "abstract": "Time series forecasting plays a crucial role in contemporary engineering information systems for supporting decision-making across various industries, where Recurrent Neural Networks (RNNs) have been widely adopted due to their capability in modeling sequential data. Conventional RNN-based predictors adopt an encoder-only strategy with sliding historical windows as inputs to forecast future values. However, this approach treats all time steps and hidden states equally without considering their distinct contributions to forecasting, leading to suboptimal performance. To address this limitation, we propose a novel Reinforced Recurrent Encoder with Prediction-oriented Proximal Policy Optimization, RRE-PPO4Pred, which significantly improves time series modeling capacity and forecasting accuracy of the RNN models. The core innovations of this method are: (1) A novel Reinforced Recurrent Encoder (RRE) framework that enhances RNNs by formulating their internal adaptation as a Markov Decision Process, creating a unified decision environment capable of learning input feature selection, hidden skip connection, and output target selection; (2) An improved Prediction-oriented Proximal Policy Optimization algorithm, termed PPO4Pred, which is equipped with a Transformer-based agent for temporal reasoning and develops a dynamic transition sampling strategy to enhance sampling efficiency; (3) A co-evolutionary optimization paradigm to facilitate the learning of the RNN predictor and the policy agent, providing adaptive and interactive time series modeling. Comprehensive evaluations on five real-world datasets indicate that our method consistently outperforms existing baselines, and attains accuracy better than state-of-the-art Transformer models, thus providing an advanced time series predictor in engineering informatics.",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03683.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03683",
    "published": "2026-01-07T08:16:55Z",
    "updated": "2026-01-13T07:12:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种基于强化学习的循环编码器框架RRE-PPO4Pred，通过整合预测导向的邻近策略优化，显著提升了时间序列预测的准确性和建模能力。",
      "motivation": "时间序列预测在工程信息系统中对多行业决策支持至关重要，循环神经网络因其序列建模能力被广泛应用。然而，传统RNN预测器采用编码器-only策略，以滑动历史窗口作为输入，平等对待所有时间步和隐藏状态，忽略了它们对预测贡献的差异性，导致性能次优。这限制了预测精度，影响实际应用中的决策效率，因此需要一种更智能的方法来优化RNN的内部建模过程。",
      "method": "论文提出了RRE-PPO4Pred方法，核心创新包括三点：一是Reinforced Recurrent Encoder框架，将RNN内部适应建模为马尔可夫决策过程，形成统一决策环境学习输入特征选择、隐藏跳连和输出目标选择；二是改进的Prediction-oriented Proximal Policy Optimization算法，使用基于Transformer的代理进行时序推理，并开发动态过渡采样策略提升效率；三是协同进化优化范式，促进RNN预测器和策略代理的自适应交互学习，从而增强时间序列建模能力。",
      "result": "在五个真实世界数据集上的综合评估表明，RRE-PPO4Pred方法 consistently 优于现有基线模型，其预测准确性甚至超过了最先进的Transformer模型。实验结果显示该方法能显著提升时间序列预测的性能，虽然摘要未提供具体数值指标，但强调了其在多数据集上的优越表现，证实了强化学习机制对改进传统RNN方法的有效性。",
      "conclusion": "本研究的主要贡献是开发了RRE-PPO4Pred，一种结合强化学习优化RNN的时间序列预测方法，提高了建模准确性和实用性。其学术价值在于创新性地整合了强化学习与循环神经网络，拓展了序列建模的理论框架；实际应用中，为工程信息学提供了更可靠的预测工具，支持决策制定。未来工作可探索该方法在其他序列任务的应用，摘要未明确说明具体局限性。",
      "tags": [
        "Time Series Forecasting",
        "Recurrent Neural Networks",
        "Reinforcement Learning",
        "Proximal Policy Optimization",
        "Transformer"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:04.952629Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03603",
    "title": "A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data",
    "authors": [
      "Kaidong Feng",
      "Zhu Sun",
      "Roy Ka-Wei Lee",
      "Xun Jiang",
      "Yin-Leng Theng",
      "Yi Ding"
    ],
    "abstract": "Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03603.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03603",
    "published": "2026-01-07T05:33:00Z",
    "updated": "2026-01-13T05:06:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文首次系统性比较传统机器学习、深度学习和大型语言模型在基于智能手机感知的心理健康预测中的应用，为下一代适应性干预技术奠定基础。",
      "motivation": "研究旨在解决心理健康预测而非检测的问题，以支持及时的主动性干预，而现有方法多集中于症状出现后的识别，缺乏对预测性能的比较。智能手机感知能无干扰地追踪日常行为数据，但如何有效利用这些数据进行前瞻性预测，并评估不同AI方法的优劣是关键挑战。本研究动机在于填补这一空白，通过综合性基准研究，探索不同模型在心理健康预测中的潜力，推动技术向预防性心理健康管理发展。",
      "method": "研究使用College Experience Sensing（CES）数据集，这是迄今为止最广泛的纵向大学生心理健康数据集。方法包括系统评估传统机器学习、深度学习和大型语言模型，关键创新点在于评估时间窗口、特征粒度、个性化策略和类别不平衡处理。具体技术路线涉及比较模型架构，如Transformer用于深度学习，并分析大型语言模型在上下文推理中的应用。该方法旨在全面揭示不同建模方法如何解析行为数据，以指导未来技术开发。",
      "result": "实验结果表明，深度学习模型在整体性能上表现最佳，特别是Transformer架构的Macro-F1得分为0.58。大型语言模型在上下文推理方面显示出优势，但在时间序列建模上较弱。个性化策略显著改善了严重心理健康状态的预测准确性。与基线方法对比，深度学习在综合评估中优于传统机器学习，而大型语言模型在特定任务上有其独特优势。这些结果为心理健康预测提供了实证依据，并突出了不同方法的适用场景。",
      "conclusion": "本研究通过基准比较，为心理健康预测建立了技术框架，揭示了不同建模方法的优缺点。主要贡献在于推动自适应、以人为中心的心理健康技术的发展，具有学术价值和应用前景。研究意义在于促进更精准的干预工具，未来工作可进一步优化时间建模和实际部署，克服现有局限性，如大型语言模型在时序数据处理上的挑战，以实现更广泛的实际应用。",
      "tags": [
        "Mental Health Forecasting",
        "Smartphone Sensing",
        "Deep Learning",
        "Transformer",
        "Large Language Model"
      ]
    },
    "analyzed_at": "2026-01-14T03:31:59.985410Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03466",
    "title": "Latent Geometry of Taste: Scalable Low-Rank Matrix Factorization for Recommender Systems",
    "authors": [
      "Joshua Salako"
    ],
    "abstract": "Scalability and data sparsity remain critical bottlenecks for collaborative filtering on massive interaction datasets. This work investigates the latent geometry of user preferences using the MovieLens 32M dataset, implementing a high-performance, parallelized Alternating Least Squares (ALS) framework. Through extensive hyperparameter optimization, we demonstrate that constrained low-rank models significantly outperform higher dimensional counterparts in generalization, achieving an optimal balance between Root Mean Square Error (RMSE) and ranking precision. We visualize the learned embedding space to reveal the unsupervised emergence of semantic genre clusters, confirming that the model captures deep structural relationships solely from interaction data. Finally, we validate the system's practical utility in a cold-start scenario, introducing a tunable scoring parameter to manage the trade-off between popularity bias and personalized affinity effectively. The codebase for this research can be found here: https://github.com/joshsalako/recommender.git",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.03466.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03466",
    "published": "2026-01-06T23:42:40Z",
    "updated": "2026-01-13T04:08:05Z",
    "comment": "Added a new figure on page 5, updated the title to include recommender systems, updated keywords, updated captions for all figures, and cited all figures in the text",
    "light_analysis": {
      "overview": "本文提出一种基于并行化交替最小二乘法的可扩展低秩矩阵分解方法，优化推荐系统性能并揭示用户偏好的潜在几何结构。",
      "motivation": "协同过滤在大规模交互数据集如MovieLens 32M上面临可扩展性和数据稀疏性瓶颈，这些问题导致推荐系统效率低下和泛化能力不足。现有高维模型可能过拟合且计算成本高，影响推荐准确性和实时性，因此需开发高效方法以平衡性能与泛化，解决冷启动等实际场景中的挑战。",
      "method": "本研究使用MovieLens 32M数据集，实现高性能并行化交替最小二乘法框架进行低秩矩阵分解。关键创新包括约束低秩模型的超参数优化，以平衡RMSE和排名精度，并通过可视化嵌入空间揭示无监督的语义类型簇涌现，验证模型仅从交互数据学习深层结构关系。此外，引入可调评分参数处理冷启动场景中的流行偏置与个性化亲和力权衡。",
      "result": "通过超参数优化，约束低秩模型在泛化上显著优于高维模型，实现了RMSE与排名精度的最优平衡，但摘要未明确说明具体数值。可视化显示嵌入空间出现语义类型簇，证实模型能捕捉结构关系。在冷启动场景中，系统通过可调参数有效管理流行偏置，提升了实用性和推荐质量，验证了方法的有效性。",
      "conclusion": "本文开发了可扩展的低秩矩阵分解方法，提升推荐系统性能与泛化能力，揭示用户偏好的潜在几何结构，为算法提供新见解。研究在冷启动场景中验证实用性，通过参数调整平衡偏置，学术价值在于无监督学习结构关系，实际应用可改善推荐质量。未来工作可能涉及扩展至更复杂数据集或优化效率。",
      "tags": [
        "Low-Rank Matrix Factorization",
        "Alternating Least Squares",
        "Collaborative Filtering",
        "Recommender Systems",
        "Hyperparameter Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:25.129422Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03190",
    "title": "Maximizing Local Entropy Where It Matters: Prefix-Aware Localized LLM Unlearning",
    "authors": [
      "Naixin Zhai",
      "Pengyang Shao",
      "Binbin Zheng",
      "Yonghui Yang",
      "Fei Shen",
      "Long Bai",
      "Xun Yang"
    ],
    "abstract": "Machine unlearning aims to forget sensitive knowledge from Large Language Models (LLMs) while maintaining general utility. However, existing approaches typically treat all tokens in a response indiscriminately and enforce uncertainty over the entire vocabulary. This global treatment results in unnecessary utility degradation and extends optimization to content-agnostic regions. To address these limitations, we propose PALU (Prefix-Aware Localized Unlearning), a framework driven by a local entropy maximization objective across both temporal and vocabulary dimensions. PALU reveals that (i) suppressing the sensitive prefix alone is sufficient to sever the causal generation link, and (ii) flattening only the top-$k$ logits is adequate to maximize uncertainty in the critical subspace. These findings allow PALU to avoid redundant optimization across the full vocabulary and parameter space while minimizing collateral damage to general model performance. Extensive experiments validate that PALU achieves superior forgetting efficacy and utility preservation compared to state-of-the-art baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03190.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03190",
    "published": "2026-01-06T17:10:48Z",
    "updated": "2026-01-13T11:13:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出PALU框架，通过前缀感知的局部熵最大化实现大语言模型的精确机器遗忘，提升遗忘效率和效用保留。",
      "motivation": "机器遗忘旨在从大型语言模型中移除敏感知识，同时保持一般效用，但现有方法通常对所有响应标记进行全局处理，并在整个词汇表上强制不确定性，导致不必要的效用降低和内容无关区域的冗余优化。这个问题的重要性在于模型安全部署，而现有方法的全局性处理加剧了性能损害，需要更局部化、精准的遗忘策略来最小化副作用。",
      "method": "PALU框架基于局部熵最大化目标，覆盖时间和词汇维度，关键创新是仅抑制敏感前缀以切断因果生成链接，并仅平坦化前k个logits在关键子空间最大化不确定性。这避免了在全词汇和参数空间进行冗余优化，通过聚焦于关键区域减少对一般模型性能的影响，使用熵最大化技术优化遗忘过程。",
      "result": "大量实验显示，PALU在遗忘效果和效用保留方面优于最先进的基线方法，实现了更高的遗忘效率和更好的模型性能保持。对比评估表明其在减少敏感知识的同时最小化效用损失，但摘要未明确提供具体准确率或效率数据，仅强调'超级遗忘效能和效用保留'的优越性。",
      "conclusion": "PALU的主要贡献是提出局部化机器遗忘框架，通过抑制敏感前缀和关键logits优化，显著减少冗余优化和性能损害，提升了遗忘精确性和模型安全性。其学术价值在于理论洞察和实用方法，为LLM安全应用提供支持；未来工作可探索更复杂场景和扩展应用，但摘要未明确说明局限性。",
      "tags": [
        "Machine Unlearning",
        "Large Language Models",
        "Local Entropy Maximization",
        "Prefix-Aware Optimization",
        "Top-k Logits"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:34.063779Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03184",
    "title": "Decentralized Autoregressive Generation",
    "authors": [
      "Stepan Maschan",
      "Haoxuan Qu",
      "Jun Liu"
    ],
    "abstract": "We present a theoretical analysis of decentralization of autoregressive generation. We define the Decentralized Discrete Flow Matching objective, by expressing probability generating velocity as a linear combination of expert flows. We also conduct experiments demonstrating the equivalence between decentralized and centralized training settings for multimodal language models across diverse set of benchmarks. Specifically, we compare two distinct paradigms: LLaVA and InternVL 2.5-1B, which uses a fixed CLIP vision encoder and performs full-parameter fine-tuning (ViT+MLP+LLM) during the instruction tuning stage.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03184.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03184",
    "published": "2026-01-06T17:07:27Z",
    "updated": "2026-01-13T11:19:48Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "论文提出了自回归生成去中心化的理论分析，并定义了去中心化离散流匹配目标，验证了去中心化与中心化训练在多模态模型中的等效性。",
      "motivation": "研究旨在探索自回归生成中的去中心化方法，以应对中心化训练可能存在的计算效率或数据分布问题。自回归生成广泛应用于AI领域，但现有方法在可扩展性和资源利用上可能不足，去中心化方法有助于提升训练灵活性和鲁棒性。摘要未明确说明具体问题背景和现有方法的详细局限性，但暗示了去中心化的重要性。",
      "method": "论文定义了Decentralized Discrete Flow Matching目标函数，通过将概率生成速度表示为专家流的线性组合来实现理论框架。实验部分使用多模态语言模型，具体比较LLaVA和InternVL 2.5-1B两种范式，在指令调优阶段采用固定CLIP视觉编码器，并对视觉变换器、多层感知器和语言模型进行全参数微调，以验证去中心化训练的有效性。",
      "result": "实验结果表明，在多个基准测试中，去中心化训练设置与中心化训练设置效果相当，证明了二者在性能上的等价性。摘要未提供具体的准确率或效率数据，但强调了比较两种范式的结果，暗示去中心化方法在保持模型性能的同时可能带来分布式优势。",
      "conclusion": "论文通过理论分析和实验验证，贡献了自回归生成去中心化的理论基础和新目标函数，为多模态AI系统的分布式训练提供了新思路。学术价值在于扩展了流匹配方法的理论框架，实际应用可能包括提高训练效率和资源利用率。摘要未明确提及局限性或未来工作，但可推断未来可能探索更复杂的场景和优化策略。",
      "tags": [
        "Autoregressive Generation",
        "Decentralized Learning",
        "Flow Matching",
        "Multimodal Language Models",
        "CLIP"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:28.789184Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02867",
    "title": "Training Language Models with homotokens Leads to Delayed Overfitting",
    "authors": [
      "Adrian Cosma",
      "Stefan Ruseti",
      "Emilian Radoi",
      "Mihai Dascalu"
    ],
    "abstract": "Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.02867.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02867",
    "published": "2026-01-06T09:57:00Z",
    "updated": "2026-01-13T10:45:36Z",
    "comment": "8 pages, 6 figures, 3 Appendices",
    "light_analysis": {
      "overview": "本论文提出使用homotokens作为数据增强技术，以延迟语言模型在训练中的过拟合并提升泛化性能。",
      "motivation": "研究动机源于子词标记化在语言模型中引入的非唯一性问题：多个标记序列可以解码为相同表面形式且意义不变，但诱导不同的内部计算。标准训练方法通常采用单一最长前缀标记化，忽视了这种变异性，可能导致模型对特定标记化方式过度敏感，影响泛化能力。homotokens被形式化为严格保留意义的替代子词分割，旨在通过数据增强解决这一问题，提升模型的鲁棒性和不变性，弥补现有方法在利用标记化多样性方面的不足。",
      "method": "论文提出一个轻量级训练架构，核心是引入homotokens作为数据增强。该方法通过辅助因果编码器和块因果交叉注意力，将标准下一个标记预测条件化在采样的homotoken变体上，而不改变训练目标或标记接口。关键创新在于利用homotokens的变异性，增强模型对标记化方式的适应性，从而诱导标记化不变性。使用的数据集包括数据受限的预训练场景和多样化的评估数据集，但具体名称摘要未明确说明。",
      "result": "在数据受限的预训练实验中，homotoken增强持续延迟了过拟合现象，并在重复数据暴露下显著改善了模型在多样化评估数据集上的泛化性能。在多语言微调中，效果依赖于标记化器的质量：当标准标记高度压缩时，增益最强；而当标记化器已经过度碎片化输入时，增益减弱。尽管摘要未提供具体数值指标，但与基线方法相比，homotokens在提升模型稳定性和泛化方面表现出积极效果。",
      "conclusion": "本论文的主要贡献是提出homotokens作为一个简单且模块化的机制，用于诱导语言模型的标记化不变性。研究的学术价值在于拓展了数据增强在自然语言处理中的应用，通过处理子词标记化的变异性来提升模型鲁棒性。实际应用价值体现在改善语言模型在有限数据下的训练效果和跨语言任务的性能。潜在局限性包括对标记化器质量的依赖性，未来工作可能涉及优化标记化器设计或扩展homotokens到更广泛场景。",
      "tags": [
        "Subword Tokenization",
        "Data Augmentation",
        "Language Models",
        "Causal Encoder",
        "Cross-Attention"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:52.311741Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.01210",
    "title": "Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission",
    "authors": [
      "Kazuhiko Murasaki",
      "Shunsuke Konagai",
      "Masakatsu Aoki",
      "Taiga Yoshida",
      "Ryuichi Tanida"
    ],
    "abstract": "To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.01210.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01210",
    "published": "2026-01-03T15:27:57Z",
    "updated": "2026-01-13T05:08:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种高速LiDAR点云致密化方法，结合多传感器输入和卷积神经网络，实现低延迟实时3D场景生成，提升远程临场感系统的空间数据传输效率。",
      "motivation": "本研究针对沉浸式远程临场感系统中的低延迟空间传输问题，现有LiDAR传感器虽然能实时捕获3D数据，但产生的点云稀疏，难以满足密集3D场景的需求。为了解决这一挑战，论文致力于开发一种能够在飞行中完成深度完成的方法，以应对实时处理延迟和几何一致性的不足，从而确保远程体验的真实性和响应速度。",
      "method": "论文提出一种方法，结合多个LiDAR输入和高分辨率彩色图像，通过卷积神经网络架构实现联合双边滤波策略。核心创新在于利用CNN优化滤波过程，高效融合传感器数据，以增强点云密度并保持几何精度，同时通过实时计算架构处理数据，确保低延迟性能，适应动态3D场景捕获的需求。",
      "result": "实验证明，该方法以30帧每秒的实时速度生成全高清分辨率的密集深度图，相较于最近的基于训练的深度完成方法，处理速度快了15倍以上。生成的密集点云展现出准确的几何形状，没有多视图不一致或鬼影伪影，表明方法在提升速度和精度方面优于基线，适用于实时3D视觉应用。",
      "conclusion": "本研究的主要贡献是开发了一种高效的实时LiDAR点云致密化技术，通过传感器融合和深度学习优化，为低延迟空间数据传输提供了关键解决方案。其学术价值在于推动实时3D处理领域的发展，实际应用价值在于支持沉浸式远程临场感系统。未来工作可能包括进一步算法优化、扩展到更多传感器类型或探索其他实时视觉任务，但摘要未明确说明具体局限性。",
      "tags": [
        "LiDAR",
        "Point Cloud Densification",
        "Convolutional Neural Networks",
        "Joint Bilateral Filtering",
        "Real-Time Processing"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:47.468785Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06096",
    "title": "The Hessian of tall-skinny networks is easy to invert",
    "authors": [
      "Ali Rahimi"
    ],
    "abstract": "We describe an exact algorithm for solving linear systems $Hx=b$ where $H$ is the Hessian of a deep net. The method computes Hessian-inverse-vector products without storing the Hessian or its inverse in time and storage that scale linearly in the number of layers. Compared to the naive approach of first computing the Hessian, then solving the linear system, which takes storage that's quadratic in the number of parameters and cubically many operations, our Hessian-inverse-vector product method scales roughly like Pearlmutter's algorithm for computing Hessian-vector products.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.06096.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06096",
    "published": "2026-01-01T21:09:10Z",
    "updated": "2026-01-13T15:39:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种精确算法，能够高效计算深度神经网络Hessian矩阵的逆向量积，实现线性时间和存储复杂度。",
      "motivation": "深度神经网络的Hessian矩阵在优化和理论分析中具有重要应用，但传统方法直接计算Hessian并求逆，存储开销与参数数量平方成正比，计算复杂度立方增长，难以处理大规模网络。这限制了二阶方法在实际优化问题中的应用效率，因此开发更高效的计算算法成为迫切需求。",
      "method": "论文提出了一种计算Hessian-inverse-vector products的精确算法，用于求解线性系统Hx=b，其中H为深度网络的Hessian矩阵。该方法不显式存储Hessian或其逆矩阵，通过优化计算流程，实现时间和存储开销与网络层数成线性关系，关键创新点在于其高效性，与Pearlmutter的Hessian-vector product算法类似。摘要未明确说明具体模型架构或数据集。",
      "result": "摘要未明确说明具体的实验结果和性能指标。基于方法描述，该算法在计算Hessian-inverse-vector products时，存储开销从传统方法的二次方降低到线性关系，操作次数类似立方增长的改进，与基线方法（如直接计算Hessian并求解）相比，显著提升了计算效率，尤其在处理大规模网络时优势明显。",
      "conclusion": "该论文的主要贡献是提出一种线性复杂度的精确算法，用于高效计算深度网络Hessian的逆向量积，解决了传统方法计算成本高的局限性。学术上，为神经网络优化和二阶方法研究提供了新工具；应用上，可能促进大规模网络的高效训练和分析。未来工作可探索更广泛网络结构的适应性或实际应用场景的验证。",
      "tags": [
        "Hessian matrix",
        "Deep neural networks",
        "Inverse computation",
        "Linear systems",
        "Optimization algorithms"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:49.025435Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23130",
    "title": "PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion",
    "authors": [
      "Jian Wang",
      "Sixing Rong",
      "Jiarui Xing",
      "Yuling Xu",
      "Weide Liu"
    ],
    "abstract": "We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.23130.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23130",
    "published": "2025-12-29T01:13:50Z",
    "updated": "2026-01-13T03:27:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "PathoSyn是一个基于解耦偏差扩散的统一生成框架，用于合成高保真MRI成像病理图像，通过分解解剖重建和随机偏差建模解决特征纠缠问题。",
      "motivation": "当前生成模型在MRI图像合成中通常基于全局像素域或二进制掩码，这容易导致特征纠缠、解剖底物损坏和结构不连续性，限制了合成数据的真实性和实用性。PathoSyn旨在解决这些问题，通过重新定义成像病理为解耦加法偏差，提供一种新方法来生成患者特定数据，以支持在低数据场景下的稳健诊断算法开发，弥补现有方法的不足。",
      "method": "PathoSyn的核心方法是Deviation-Space Diffusion Model，它将合成任务分解为确定性解剖重建和随机偏差建模，以学习病理残差的条件分布。关键创新包括解耦偏差表示、seam-aware融合策略和推理时稳定模块，这些设计捕捉局部强度变化的同时确保全局结构完整性和空间相干性，抑制边界伪影并生成高保真内部病变异质性。",
      "result": "在肿瘤成像基准上进行定量和定性评估，PathoSyn在感知真实性和解剖保真度方面显著优于整体扩散模型和掩码条件基线方法。摘要未明确说明具体性能指标如准确率或效率改进，但强调了在评估中显示出优越表现，验证了其在合成高质量病理MRI图像方面的有效性。",
      "conclusion": "PathoSyn提供了数学原理的合成流程，用于生成高保真患者特定数据集，支持低数据制度下的诊断算法开发，并允许可解释的反事实疾病进展建模。这有助于精准干预规划和临床决策支持系统的基准测试，具有重要的学术价值和实际应用潜力，未来可进一步扩展其通用性或处理更复杂的病理情况。",
      "tags": [
        "Diffusion Models",
        "Disentangled Learning",
        "MRI Synthesis",
        "Pathological Residuals",
        "Generative Framework"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:41.405183Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00705",
    "title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "authors": [
      "Wei-Tse Cheng",
      "Yen-Jen Chiou",
      "Yuan-Fu Yang"
    ],
    "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Project page:https://breeze1124.github.io/rgs-slam-project-page/",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.00705.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00705",
    "published": "2025-12-28T03:45:57Z",
    "updated": "2026-01-13T12:21:54Z",
    "comment": "10 pages, 9 figures",
    "light_analysis": {
      "overview": "RGS-SLAM通过一次性训练免费的对应关系到高斯初始化，取代残差驱动密度化，提升了SLAM的稳健性和效率。",
      "motivation": "SLAM技术在机器人导航和增强现实中具有关键应用，但现有高斯分裂SLAM（GS-SLAM）采用残差驱动密度化阶段，渐进添加高斯点以填补几何缺失，这可能导致初始化不稳定、收敛缓慢，尤其在纹理丰富和杂乱场景中影响映射质量和实时性能。因此，研究旨在开发一种更稳健的初始化方法，通过一次性密集初始化来增强早期映射稳定性，加速优化过程，从而解决现有方法的不足，提升复杂环境下的渲染保真度。",
      "method": "RGS-SLAM的核心方法是训练免费的对应关系到高斯初始化：首先使用DINOv3描述符提取密集多视图对应关系，并通过置信度感知内点分类器进行精炼，然后进行一次性三角测量，生成结构感知的高斯种子。这些种子在优化前提供良好分布，替代了GS-SLAM的残差驱动密度化阶段。整个框架兼容现有GS-SLAM管道，无需额外训练，实现了从对应关系到高斯的直接映射，简化了初始化过程并提高了效率。",
      "result": "在TUM RGB-D和Replica数据集上评估，RGS-SLAM加速收敛约20%，并在纹理丰富和杂乱场景中提供更高渲染保真度。与最先进的高斯和基于点的SLAM系统相比，其定位和重建精度达到竞争性或更优水平，同时保持实时映射性能高达925 FPS，有效验证了方法的性能提升和稳健性，展示了在实际应用中的高效性。",
      "conclusion": "论文提出了一个稳健的SLAM框架，通过一次性高斯初始化提升了映射效率和渲染质量，兼容现有技术，具有在机器人、AR/VR等领域的应用价值。研究贡献在于创新性的初始化策略，局限性可能涉及对DINOv3描述符的依赖，未来工作可探索更通用的初始化方法或扩展到其他视觉任务，以进一步增强适应性和普适性。",
      "tags": [
        "Gaussian Splatting",
        "SLAM",
        "DINOv3",
        "Correspondence Initialization",
        "Real-time Mapping"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:13.644783Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.20387",
    "title": "Generative Digital Twins: Vision-Language Simulation Models for Executable Industrial Systems",
    "authors": [
      "YuChe Hsu",
      "AnJui Wang",
      "TsaiChing Ni",
      "YuanFu Yang"
    ],
    "abstract": "We propose a Vision-Language Simulation Model (VLSM) that unifies visual and textual understanding to synthesize executable FlexScript from layout sketches and natural-language prompts, enabling cross-modal reasoning for industrial simulation systems. To support this new paradigm, the study constructs the first large-scale dataset for generative digital twins, comprising over 120,000 prompt-sketch-code triplets that enable multimodal learning between textual descriptions, spatial structures, and simulation logic. In parallel, three novel evaluation metrics, Structural Validity Rate (SVR), Parameter Match Rate (PMR), and Execution Success Rate (ESR), are proposed specifically for this task to comprehensively evaluate structural integrity, parameter fidelity, and simulator executability. Through systematic ablation across vision encoders, connectors, and code-pretrained language backbones, the proposed models achieve near-perfect structural accuracy and high execution robustness. This work establishes a foundation for generative digital twins that integrate visual reasoning and language understanding into executable industrial simulation systems. Project page: https://danielhsu2014.github.io/GDT-VLSM-project/",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.20387.pdf",
    "abs_url": "https://arxiv.org/abs/2512.20387",
    "published": "2025-12-23T14:22:26Z",
    "updated": "2026-01-13T06:58:35Z",
    "comment": "10 pages, 9 figures",
    "light_analysis": {
      "overview": "提出一种视觉-语言模拟模型，通过布局草图和自然语言提示生成可执行代码，为工业模拟系统实现跨模态推理。",
      "motivation": "工业模拟系统需要从视觉草图和文本描述自动生成可执行代码，以提升自动化水平。现有方法可能缺乏跨模态整合能力，难以同时理解空间结构和语言逻辑。该研究针对这一问题，构建首个大规模数据集并提出新评估指标，以支持生成数字孪生的发展，解决传统方法在视觉与文本融合上的不足。",
      "method": "研究提出Vision-Language Simulation Model (VLSM)，统一视觉和文本理解，从布局草图和自然语言提示合成可执行的FlexScript。关键创新包括构建包含超过120,000个提示-草图-代码三元组的大规模数据集，以及通过系统消融实验评估不同组件如视觉编码器、连接器和代码预训练语言骨干，以优化模型性能。",
      "result": "模型在结构有效性和执行鲁棒性方面表现优异，摘要未明确说明具体数据，但提及新评估指标Structural Validity Rate、Parameter Match Rate和Execution Success Rate用于全面评估结构完整性、参数保真度和模拟器可执行性，并通过与基线方法的对比验证了其高效性。",
      "conclusion": "该工作为生成数字孪生奠定基础，集成视觉推理和语言理解到可执行工业模拟系统，具有学术价值和应用潜力。未来工作可扩展模型到更复杂场景或改进跨模态学习能力，以应对实际工业环境中的多样性挑战。",
      "tags": [
        "Vision-Language Model",
        "Generative Digital Twins",
        "Multimodal Learning",
        "FlexScript",
        "Simulation Systems"
      ]
    },
    "analyzed_at": "2026-01-14T03:32:55.796317Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.18209",
    "title": "When Does Learning Renormalize? Sufficient Conditions for Power Law Spectral Dynamics",
    "authors": [
      "Yizhou Zhang"
    ],
    "abstract": "Empirical power--law scaling has been widely observed across modern deep learning systems, yet its theoretical origins and scope of validity remain incompletely understood. The Generalized Resolution--Shell Dynamics (GRSD) framework models learning as spectral energy transport across logarithmic resolution shells, providing a coarse--grained dynamical description of training. Within GRSD, power--law scaling corresponds to a particularly simple renormalized shell dynamics; however, such behavior is not automatic and requires additional structural properties of the learning process.   In this work, we identify a set of sufficient conditions under which the GRSD shell dynamics admits a renormalizable coarse--grained description. These conditions constrain the learning configuration at multiple levels, including boundedness of gradient propagation in the computation graph, weak functional incoherence at initialization, controlled Jacobian evolution along training, and log--shift invariance of renormalized shell couplings. We further show that power--law scaling does not follow from renormalizability alone, but instead arises as a rigidity consequence: once log--shift invariance is combined with the intrinsic time--rescaling covariance of gradient flow, the renormalized GRSD velocity field is forced into a power--law form.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.18209.pdf",
    "abs_url": "https://arxiv.org/abs/2512.18209",
    "published": "2025-12-20T04:15:07Z",
    "updated": "2026-01-13T05:30:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了深度学习中幂律谱动力学重整化的充分条件，揭示了幂律缩放作为对数位移不变性和时间协变性组合的刚性结果。",
      "motivation": "在深度学习的训练过程中，幂律缩放现象被广泛观察到，但其理论起源和适用范围仍不清晰。现有方法如广义分辨率壳层动力学（GRSD）框架能对学习进行粗粒度建模，但未能明确界定重整化行为的发生条件，这阻碍了对学习过程本质的深入理解和应用。因此，本研究旨在解决何时学习重整化的问题，识别必要的结构属性，以填补理论空白并提升对深度学习动力学的洞察。",
      "method": "论文基于GRSD框架，识别了一组充分条件以确保可重整化的粗粒度描述。这些条件涵盖学习配置的多个层面：计算图中梯度传播的有界性、初始化时的弱功能不相关性、训练过程中雅可比演化的受控性，以及重整化壳层耦合的对数位移不变性。通过结合梯度流的内在时间重标度协变性，推导出重整化速度场必须呈现幂律形式，关键创新点在于从结构性约束出发，揭示了幂律缩放的动力学机制，而不依赖具体数据集或模型架构。",
      "result": "摘要未明确说明主要的实验性能指标，如准确率或效率改进。论文主要从理论推导角度展示，在满足所提充分条件下，GRSD壳层动力学会强制表现为幂律形式。这表明幂律缩放不仅源自可重整性，而且是对数位移不变性与时间协变性结合的刚性结果，为深度学习训练提供了理论支撑，但未与基线方法进行具体数据对比。",
      "conclusion": "本研究的主要贡献是识别了一组充分条件，阐明了深度学习训练中幂律谱动力学的重整化行为，并强调幂律缩放作为刚性结果而非单纯可重整性。这加深了对复杂学习过程的理论理解，具有重要的学术价值，为未来在更广泛神经网络结构中验证和应用这些条件奠定基础。局限性可能包括条件的假设严格性，未来工作可探索其在实证训练中的适用性和扩展性。",
      "tags": [
        "Power Law Scaling",
        "Renormalization",
        "Generalized Resolution Shell Dynamics (GRSD)",
        "Gradient Flow",
        "Spectral Dynamics"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:23.760733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.17875",
    "title": "Visually Prompted Benchmarks Are Surprisingly Fragile",
    "authors": [
      "Haiwen Feng",
      "Long Lian",
      "Lisa Dunlap",
      "Jiahao Shu",
      "XuDong Wang",
      "Renhao Wang",
      "Trevor Darrell",
      "Alane Suhr",
      "Angjoo Kanazawa"
    ],
    "abstract": "A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. We open-source VPBench and our analysis framework at: https://lisadunlap.github.io/vpbench/.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.17875.pdf",
    "abs_url": "https://arxiv.org/abs/2512.17875",
    "published": "2025-12-19T18:26:58Z",
    "updated": "2026-01-13T07:50:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示了视觉提示基准在评估视觉语言模型时的脆弱性，并提出VPBench基准以提高评估稳定性。",
      "motivation": "研究旨在解决视觉语言模型评估中的关键挑战，即如何独立测试模型对视觉内容的分析能力，而不受文本先验的干扰。现有基准如BLINK采用视觉提示方法，但模型对视觉标记细节（如颜色变化）过度敏感，导致评估结果不可靠，凸显了基准设计标准化的重要性，以避免误导模型比较和排行榜排名。",
      "method": "作者在两种视觉提示任务上评估了九个常用开闭源视觉语言模型，通过分析视觉标记设计、数据集大小及低层推理选择（如JPEG压缩级别）的影响，创建了VPBench基准。VPBench包含16种视觉标记变体，基于现有数据集进行整理，以系统化研究这些因素，并提供开源分析框架以促进可重复实验。",
      "result": "实验结果显示，视觉提示细节（如改变标记颜色从红到蓝或增大标记大小）能显著影响模型性能和排行榜排名。例如，略微增大视觉标记使开源模型InternVL3-8B在排名上媲美或优于更大模型如Gemini 2.5 Pro。这些影响在视觉提示基准中比传统语义评估更为突出，表明低层推理选择也能导致模型排名变化。",
      "conclusion": "论文的主要贡献是揭示了视觉提示基准的脆弱性，并提出了VPBench作为更稳定的评估工具，强调了基准设计细节在视觉语言模型评估中的关键作用。这具有学术价值，为研究者提供了更准确的比较框架，并开源了VPBench和分析框架，以促进未来研究，推动评估方法的标准化。",
      "tags": [
        "Visual Language Models",
        "Visual Prompting",
        "Benchmark Fragility",
        "Dataset Curation",
        "JPEG Compression"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:25.924757Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.16323",
    "title": "Hacking Neural Evaluation Metrics with Single Hub Text",
    "authors": [
      "Hiroyuki Deguchi",
      "Katsuki Chousa",
      "Yusuke Sakai"
    ],
    "abstract": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.16323.pdf",
    "abs_url": "https://arxiv.org/abs/2512.16323",
    "published": "2025-12-18T09:06:24Z",
    "updated": "2026-01-13T07:48:36Z",
    "comment": "Accepted at EACL2026 main",
    "light_analysis": {
      "overview": "提出一种通过找到单个对抗性文本（hub text）来揭示神经文本评估指标（如COMET）脆弱性的方法。",
      "motivation": "神经文本评估指标如COMET在机器翻译等生成任务中广泛使用，因其与人类评估高度相关而被视为可靠标准。然而，由于其基于黑箱神经网络，指标的内部机制不透明，可能存在漏洞导致不可靠的评估结果。现有研究缺乏对这些指标安全性的系统检验，这在实际应用中可能误导模型优化和评估。因此，本研究旨在通过对抗性方法探索这些指标的脆弱性，以提高其可靠性和安全性。",
      "method": "论文提出一种方法，在离散文本空间中搜索单个对抗性文本（称为hub text），该文本被设计为无论测试用例如何，都能在神经评估指标COMET上获得高质量评分。关键创新在于使用单个文本而非为每个源句子生成对抗样本，从而更高效地揭示指标的系统性漏洞。方法在WMT'24翻译任务的数据集上实施，利用COMET作为评估指标，但具体搜索算法摘要未详细说明。",
      "result": "实验结果显示，在WMT'24 English-to-Japanese翻译任务中，找到的hub text获得了79.1%的COMET分数，在English-to-German任务中获得了67.8%的COMET分数。这些分数超过了使用通用翻译模型M2M100为每个源句子单独生成翻译的平均性能。此外，hub text在反向语言对（如Japanese-to-English和German-to-English）中也表现出良好的泛化能力，进一步证实了评估指标的漏洞。",
      "conclusion": "本研究的核心贡献是通过找到单个对抗性文本，成功揭示了COMET等神经评估指标的潜在脆弱性，表明即使高度相关于人类的指标也可能被简单攻击欺骗。这强调了在依赖自动化评估指标时需加强鲁棒性测试的重要性，对机器翻译和其他生成模型的开发具有实际警示意义。未来工作可包括开发更健壮的评估指标或改进现有指标以抵御类似攻击，摘要未明确说明具体局限性。",
      "tags": [
        "Adversarial Text",
        "Neural Evaluation Metrics",
        "COMET",
        "Machine Translation",
        "WMT'24"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:42.978255Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.13564",
    "title": "Memory in the Age of AI Agents",
    "authors": [
      "Yuyang Hu",
      "Shichun Liu",
      "Yanwei Yue",
      "Guibin Zhang",
      "Boyang Liu",
      "Fangyi Zhu",
      "Jiahang Lin",
      "Honglin Guo",
      "Shihan Dou",
      "Zhiheng Xi",
      "Senjie Jin",
      "Jiejun Tan",
      "Yanbin Yin",
      "Jiongnan Liu",
      "Zeyu Zhang",
      "Zhongxiang Sun",
      "Yutao Zhu",
      "Hao Sun",
      "Boci Peng",
      "Zhenrong Cheng",
      "Xuanbo Fan",
      "Jiaxin Guo",
      "Xinlei Yu",
      "Zhenhong Zhou",
      "Zewen Hu",
      "Jiahao Huo",
      "Junhao Wang",
      "Yuwei Niu",
      "Yu Wang",
      "Zhenfei Yin",
      "Xiaobin Hu",
      "Yue Liao",
      "Qiankun Li",
      "Kun Wang",
      "Wangchunshu Zhou",
      "Yixin Liu",
      "Dawei Cheng",
      "Qi Zhang",
      "Tao Gui",
      "Shirui Pan",
      "Yan Zhang",
      "Philip Torr",
      "Zhicheng Dou",
      "Ji-Rong Wen",
      "Xuanjing Huang",
      "Yu-Gang Jiang",
      "Shuicheng Yan"
    ],
    "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.13564.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13564",
    "published": "2025-12-15T17:22:34Z",
    "updated": "2026-01-13T09:33:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本综述论文通过系统分析人工智能代理内存研究的现状，提出了基于形式、功能和动态的统一分析框架，为未来代理智能设计提供概念基础。",
      "motivation": "随着人工智能代理研究的快速发展，内存作为核心能力日益重要，但当前领域呈现碎片化趋势。现有工作在动机、实现和评估上差异显著，术语定义模糊，传统分类如长/短期内存已无法适应现代代理内存系统的多样性。这导致研究效率低下和概念混淆，迫切需要统一的框架来整合和澄清研究方向，以促进代理智能的健康发展。摘要指出，本文旨在解决这一问题，提供全面现状分析，弥补现有不足。",
      "method": "本文采用综合分析视角，通过三个统一维度探讨代理内存：形式、功能和动态。从形式视角，识别了三种主要实现方式：token级内存、参数化内存和潜在内存。从功能视角，提出了细粒度分类，包括事实性内存、经验内存和任务工作内存。从动态视角，分析了内存的形成、演化和检索过程。此外，方法还包括编译现有内存基准测试和开源框架，以支持实际开发，并提供结构化综述来整合多样化的研究。",
      "result": "作为一篇综述论文，摘要未明确说明具体的实验结果如性能指标。然而，通过系统分析，本文提供了一个全面的代理内存研究现状概述，包括分类框架的提出和基准测试的总结。虽然没有实验数据对比，但它为后续研究提供了概念基础，有助于统一评价标准，减少领域碎片化。本文强调通过新框架整合现有工作，而非直接改进技术性能。",
      "conclusion": "本论文的主要贡献是提供了一个代理内存研究的现状分析和新概念框架，澄清了术语混淆和碎片化问题。其学术价值在于为未来研究奠定基础，推动代理内存作为一级原语的设计；实际应用价值在于指导代理智能的开发和优化。论文还展望了未来研究方向，如内存自动化、强化学习集成、多模态内存、多代理内存和可信度问题，为领域发展指明方向。",
      "tags": [
        "Agent Memory",
        "LLM Memory",
        "Retrieval Augmented Generation (RAG)",
        "Token-Level Memory",
        "Reinforcement Learning Integration"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:42.761028Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.12688",
    "title": "Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity",
    "authors": [
      "Dongseok Kim",
      "Hyoungsun Choi",
      "Mohamed Jismy Aashik Rasool",
      "Gisung Oh"
    ],
    "abstract": "Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.12688.pdf",
    "abs_url": "https://arxiv.org/abs/2512.12688",
    "published": "2025-12-14T13:42:20Z",
    "updated": "2026-01-13T16:04:19Z",
    "comment": "Revised for clarity and correctness; improved exposition and fixed minor issues",
    "light_analysis": {
      "overview": "论文提出了一个理论框架，将提示视为外部程序，证明了固定Transformer骨干仅通过提示能近似广泛的计算行为，从而形式化提示工程的表达性。",
      "motivation": "研究动机是解决提示工程领域缺乏理论基础的现实问题。当前，提示能切换固定权重模型的行为，但这一现象常被视为启发式方法而非理论对象，导致难以系统理解其表达能力和限制。重要性在于理论空白限制了提示工程的有效应用和深入分析，现有方法多依赖经验观察，无法从机制层面揭示工作原理，阻碍了对模型行为切换的精确控制与优化。",
      "method": "研究方法基于将提示视为外部注入的程序，构建一个简化的Transformer模型作为执行器来解释提示，以实现不同计算。关键创新点包括机制级分解：注意力机制执行从提示记忆的选择性路由，前馈神经网络执行基于检索片段的局部算术，深度堆叠组合这些局部更新成多步计算。摘要未明确说明具体数据集或模型架构，但该方法使用理论分析聚焦于Transformer骨干的结构特性。",
      "result": "主要理论结果证明了一个构造性存在定理，表明单个固定的Transformer骨干仅通过改变提示就能近似广泛的目标行为类别。这为形式化在提示长度和精度约束下的权衡提供了框架，并允许研究基于提示切换的结构限制。与基线方法对比摘要未明确说明，但框架区别于经验性主张，为理解提示的表达性提供了理论基础，而非依赖具体数据或性能指标。",
      "conclusion": "论文的主要贡献是提供一个统一理论框架，将提示工程从启发式方法提升到形式化表达性分析。学术价值在于为Transformer模型的行为切换机制提供了理论支撑，区分了理论与实证研究；应用价值有助于优化提示设计和评估模型能力，促进更高效的人工智能系统开发。未来工作可能包括扩展到实际预训练大语言模型的实证研究，并探索在约束条件下的实际应用局限性。",
      "tags": [
        "Prompt Engineering",
        "Transformer",
        "Attention Mechanism",
        "Theoretical Foundations",
        "Neural Network Theory"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:56.493865Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.09114",
    "title": "AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance",
    "authors": [
      "Pamela Gupta"
    ],
    "abstract": "The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.09114.pdf",
    "abs_url": "https://arxiv.org/abs/2512.09114",
    "published": "2025-12-09T20:57:22Z",
    "updated": "2026-01-13T12:53:22Z",
    "comment": "We have adjustments to make for higher effectiveness",
    "light_analysis": {
      "overview": "本论文提出了AI TIPS 2.0框架，旨在操作化AI治理，以解决现有框架在风险评估、操作性和规模化实施方面的不足。",
      "motivation": "AI系统部署面临三个关键治理挑战：首先，组织在用例级别风险评估不足，导致如Humana集体诉讼案所示的高偏见和错误率问题，每个用例需定制化治理，但现有框架提供通用指导；其次，现有框架如ISO 42001和NIST AI RMF停留在概念层面，缺乏可操作的控件，使从业者难以将治理要求转化为具体技术实现；最后，组织缺乏规模化操作治理的机制，无法系统化嵌入可信AI实践、测量合规性或提供角色适当可见性。这些问题凸显了开发具体操作框架的重要性。",
      "method": "论文提出了AI TIPS（人工智能信任集成支柱可持续性）2.0框架，这是对2019年版本的更新，旨在直接解决上述挑战。该框架可能包括风险管理、合规测量和角色适当可见性等模块，但摘要未明确说明具体技术细节，如使用的数据集或模型架构。关键创新点在于提供操作化治理方案，帮助从业者将治理原则转化为实际实施。",
      "result": "摘要未提供具体的实验结果或性能指标。框架的提出旨在改进AI治理的操作化，但没有数据支撑与基线方法的对比效果。因此，结果部分主要描述框架的贡献方向，而非量化成果，如准确率提升或效率改进。",
      "conclusion": "AI TIPS 2.0框架为AI治理提供了综合操作方案，帮助组织实施可信AI实践，从董事会到数据科学家层面提高治理可见性。其学术价值在于填补了现有框架的操作化空白，实际应用价值在于促进AI系统的负责任部署。未来工作可能需要进一步验证框架的有效性和扩展性，但摘要未明确说明局限性。",
      "tags": [
        "AI Governance",
        "Risk Management",
        "Operational Framework",
        "Trustworthy AI",
        "Compliance"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:02.867225Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.07761",
    "title": "TROJail: Trajectory-Level Optimization for Multi-Turn Large Language Model Jailbreaks with Process Rewards",
    "authors": [
      "Xiqiao Xiong",
      "Ouxiang Li",
      "Zhuo Liu",
      "Moxin Li",
      "Wentao Shi",
      "Fengbin Zhu",
      "Qifan Wang",
      "Fuli Feng"
    ],
    "abstract": "Large language models have seen widespread adoption, yet they remain vulnerable to multi-turn jailbreak attacks, threatening their safe deployment. This has led to the task of training automated multi-turn attackers to probe model safety vulnerabilities. However, existing approaches typically rely on turn-level optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate this task as a multi-turn reinforcement learning problem, directly optimizing the harmfulness of the final-turn response as the outcome reward. To address the sparse supervision of the outcome reward, we introduce TROJail, which employs two process rewards to evaluate the utility of intermediate prompts and integrate them into advantage estimation. These rewards (1) penalize overly harmful prompts that trigger the model's refusal mechanism, and (2) encourage steering the semantic relevance of responses toward the targeted harmful content. Experimental results show improved attack success rates across multiple models and benchmarks, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/TROJail. Warning: This paper contains examples of harmful content.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.07761.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07761",
    "published": "2025-12-08T17:42:59Z",
    "updated": "2026-01-13T15:14:32Z",
    "comment": "21 pages, 15 figures",
    "light_analysis": {
      "overview": "本研究提出TROJail方法，通过轨迹级优化和多进程奖励改进多轮大型语言模型的越狱攻击策略。",
      "motivation": "大型语言模型已广泛部署，但多轮越狱攻击仍对其安全构成威胁，这驱动了训练自动化攻击者来探测模型漏洞的任务。现有方法通常依赖轮级优化，无法有效学习长期攻击策略，导致对模型安全性的评估不足，因此需开发更先进的方法来增强攻击效率以促进模型安全改进。",
      "method": "论文将多轮越狱攻击任务制定为强化学习问题，直接以最终轮回答的有害性作为结果奖励进行优化。为克服结果奖励的稀疏监督，引入TROJail方法，采用两个过程奖励评估中间提示的效用：一个惩罚触发模型拒绝机制的过度有害提示，另一个鼓励回答的语义相关性转向目标有害内容，并将其整合到优势估计中，以优化长期策略。",
      "result": "实验结果显示，TROJail在多个大型语言模型和基准测试中提高了攻击成功率，证明了其有效性。与依赖轮级优化的基线方法相比，该方法在攻击性能上有所改进，但摘要未明确说明具体提升百分比或其他详细数据，需参考论文完整内容。",
      "conclusion": "TROJail通过轨迹级优化和过程奖励，成功提升了多轮越狱攻击的性能，为解决强化学习中稀疏奖励问题提供了新思路，具有评估和加强大型语言模型安全性的学术和实际价值。未来工作可能涉及扩展到其他安全任务或进一步优化奖励设计以提高鲁棒性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Jailbreak Attacks",
        "Process Rewards",
        "Trajectory Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:33:39.445175Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.07436",
    "title": "LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services",
    "authors": [
      "Hang He",
      "Chuhuai Yue",
      "Chengqi Dong",
      "Mingxue Tian",
      "Hao Chen",
      "Zhenfeng Liu",
      "Jiajun Chai",
      "Xiaohan Wang",
      "Yufei Zhang",
      "Qun Liao",
      "Guojun Yin",
      "Wei Lin",
      "Chengcheng Wan",
      "Haiying Sun",
      "Ting Su"
    ],
    "abstract": "Recent advances in large reasoning models LRMs have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench comprises a database of over 1.3M merchant entries across 6 service categories and 9 major cities, and 900 multi-hop QA tasks from real user queries that require multi-step reasoning. We also developed LocalPlayground, a unified environment integrating multiple tools for LRMs interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.2) achieves only 35.60% correctness, and most models have issues with completeness (average 60.32%) and faithfulness (average 30.72%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at https://localsearchbench.github.io/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.07436.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07436",
    "published": "2025-12-08T11:12:39Z",
    "updated": "2026-01-13T18:44:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了LocalSearchBench，这是首个专门针对本地生活服务的代理搜索基准，用于评估大型推理模型在多跳推理任务中的性能。",
      "motivation": "研究动机源于大型推理模型在代理搜索方面的应用大多聚焦于通用信息检索，而本地生活服务作为垂直领域面临独特挑战，如查询歧义和需要跨商户与产品的多跳推理。这些问题在实际场景中至关重要，例如用户模糊查询需要精确推荐，但现有方法未深入探索此类领域，缺乏专门基准来推动领域特定研究和代理训练，从而限制了对复杂业务需求的支持。",
      "method": "论文的核心方法是构建LocalSearchBench基准，它包括一个覆盖6个服务类别和9个主要城市的数据库，包含超过130万商户条目，以及900个基于真实用户查询的多跳QA任务，这些任务要求多步推理。此外，开发了LocalPlayground环境，集成多个工具以支持大型推理模型的交互。创新点在于首个全面覆盖真实本地生活服务场景的基准，强调复杂业务逻辑和多源信息整合。",
      "result": "实验结果显示，即使是当前最先进的大型推理模型在LocalSearchBench上表现不佳：最佳模型DeepSeek-V3.2的正确率仅35.60%，而大多数模型在完整性（平均60.32%）和忠实性（平均30.72%）方面存在问题。与基线方法相比，这表明现有模型难以有效处理本地生活服务中的多跳推理任务，突显了领域特定基准的必要性和代理系统改进的迫切需求。",
      "conclusion": "论文的主要贡献是引入了LocalSearchBench作为首个本地生活服务代理搜索基准，揭示了该领域在复杂查询和多步推理方面的挑战。其学术价值在于提供了标准评估工具，推动代理搜索研究向垂直领域发展；实际应用价值包括促进更精准的本地服务推荐系统。未来工作可能涉及扩展基准覆盖范围或开发专门训练方法以提高模型性能。",
      "tags": [
        "Agentic Search",
        "Benchmarking",
        "Large Reasoning Models",
        "Multi-hop Reasoning",
        "Local Life Services"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:06.254502Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.06400",
    "title": "Perceptual Region-Driven Infrared-Visible Co-Fusion for Extreme Scene Enhancement",
    "authors": [
      "Jing Tao",
      "Yonghong Zong",
      "Banglei Guan",
      "Pengju Sun",
      "Taihang Lei",
      "Yang Shanga",
      "Qifeng Yu"
    ],
    "abstract": "In photogrammetry, accurately fusing infrared (IR) and visible (VIS) spectra while preserving the geometric fidelity of visible features and incorporating thermal radiation is a significant challenge, particularly under extreme conditions. Existing methods often compromise visible imagery quality, impacting measurement accuracy. To solve this, we propose a region perception-based fusion framework that combines multi-exposure and multi-modal imaging using a spatially varying exposure (SVE) camera. This framework co-fuses multi-modal and multi-exposure data, overcoming single-exposure method limitations in extreme environments. The framework begins with region perception-based feature fusion to ensure precise multi-modal registration, followed by adaptive fusion with contrast enhancement. A structural similarity compensation mechanism, guided by regional saliency maps, optimizes IR-VIS spectral integration. Moreover, the framework adapts to single-exposure scenarios for robust fusion across different conditions. Experiments conducted on both synthetic and real-world data demonstrate superior image clarity and improved performance compared to state-of-the-art methods, as evidenced by both quantitative and visual evaluations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.06400.pdf",
    "abs_url": "https://arxiv.org/abs/2512.06400",
    "published": "2025-12-06T11:17:35Z",
    "updated": "2026-01-13T02:50:08Z",
    "comment": "The paper has been accepted and officially published by OPTICS AND LASER TECHNOLOGY",
    "light_analysis": {
      "overview": "本论文提出一种基于区域感知的红外-可见光共融合框架，利用空间可变曝光相机结合多模态和多曝光数据，以提升极端场景下的图像增强效果。",
      "motivation": "在摄影测量领域，准确融合红外和可见光光谱同时保持可见光特征的几何保真度并整合热辐射是一个关键挑战，尤其在极端环境条件下。现有方法往往在融合过程中牺牲可见光图像质量，导致测量精度下降，这限制了多模态成像技术的应用可靠性。因此，开发一种能够同时处理多模态和多曝光数据的方法至关重要，以解决极端场景下的图像退化问题。",
      "method": "本论文提出一种基于区域感知的融合框架，使用空间可变曝光（SVE）相机结合多曝光和多模态成像。核心步骤包括：首先通过区域感知特征融合实现精确的多模态配准，然后进行自适应融合和对比度增强。创新点在于引入结构相似性补偿机制，由区域显著性图指导，优化IR-VIS光谱集成。该框架还设计为能适应单曝光场景，确保在不同条件下的鲁棒性。关键细节如具体数据集在摘要中未明确说明。",
      "result": "实验基于合成和真实数据进行，结果表明该框架在图像清晰度和融合质量上优于最先进方法。通过定量和视觉评估，性能得到显著改进，例如图像质量指标提升，但摘要未明确说明具体数据如准确率百分比。对比基线方法，该框架在保持几何保真度的同时增强了极端场景下的表现，验证了其有效性。",
      "conclusion": "本论文的主要贡献是开发了一种区域感知驱动的红外-可见光共融合框架，通过结合多模态和多曝光数据，克服了传统单曝光方法的局限，显著提升极端场景下的图像增强。这推动了多模态图像融合技术的发展，具有重要学术和实际应用价值，如在摄影测量中提高测量精度。未来工作可能包括优化实时性能或扩展其他光谱融合，但摘要未明确说明局限性。",
      "tags": [
        "Infrared-Visible Fusion",
        "Region Perception",
        "Multi-modal Imaging",
        "Spatially Varying Exposure",
        "Structural Similarity Compensation"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:57.290249Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.05534",
    "title": "On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability",
    "authors": [
      "Yiming Tang",
      "Harshvardhan Saini",
      "Zhaoqian Yao",
      "Yizhen Liao",
      "Qianxiao Li",
      "Mengnan Du",
      "Dianbo Liu"
    ],
    "abstract": "As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode diverse concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into monosemantic features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one optimization problem. We demonstrate how diverse methods instantiate the theoretical framework and provide rigorous analysis of the optimization landscape. We provide novel theoretical explanations for empirically observed phenomena, including feature absorption and dead neurons. We design the Linear Representation Bench, a benchmark that strictly follows the Linear Representation Hypothesis, to evaluate SDL methods with fully accessible ground-truth features. Motivated by our theory and findings, we develop feature achoring, a novel technique applicable for all SDL methods, to enhance their feature recovery capabilities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.05534.pdf",
    "abs_url": "https://arxiv.org/abs/2512.05534",
    "published": "2025-12-05T08:47:19Z",
    "updated": "2026-01-13T08:47:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了稀疏字典学习的首个统一理论框架，并开发了特征锚定技术以增强特征恢复能力。",
      "motivation": "随着AI模型在多个领域展现出卓越能力，理解其内部表示和信息处理机制对科学进步和可信部署至关重要。机理可解释性研究显示神经网络常将概念叠加编码，稀疏字典学习方法如稀疏自编码器能分解这些概念，但现有理论仅限带权重约束的特定方法，缺乏对更广泛SDL方法的正式分析，限制了理论深度和应用潜力。",
      "method": "本研究开发了一个统一理论框架，将稀疏字典学习视为优化问题，涵盖稀疏自编码器、转码器和交叉编码器等多种方法。框架分析了优化景观，解释了特征吸收和死亡神经元等实证现象。同时，设计了Linear Representation Bench基准，严格遵循线性表示假设，提供真实特征以评估SDL方法。基于理论，提出了特征锚定技术，适用于所有SDL方法，旨在提升特征恢复能力。",
      "result": "论文通过统一框架为特征吸收和死亡神经元等现象提供了理论解释。设计了Linear Representation Bench基准，但摘要未明确说明具体性能指标，如准确率提升或效率改进。基于理论和发现，提出了特征锚定技术，预期能增强特征恢复，但未提及与基线方法的对比数据。",
      "conclusion": "本研究首次建立了稀疏字典学习的统一理论框架，填补了该领域理论基础的空缺，为多种方法提供了正式分析。通过理论解释和基准设计，增强了SDL方法的可评估性和理论深度。提出的特征锚定技术有望在实际应用中提升特征恢复效果，推动机理可解释性领域的发展。未来工作可能包括更广泛的理论验证和实际部署。",
      "tags": [
        "Sparse Dictionary Learning",
        "Mechanistic Interpretability",
        "Sparse Autoencoders",
        "Feature Anchoring",
        "Linear Representation Hypothesis"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:42.864151Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.04727",
    "title": "Sequential Enumeration in Large Language Models",
    "authors": [
      "Kuinan Hou",
      "Marco Zorzi",
      "Alberto Testolin"
    ],
    "abstract": "Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.04727.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04727",
    "published": "2025-12-04T12:10:24Z",
    "updated": "2026-01-13T09:39:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文研究了大型语言模型在序列枚举中的计数能力，揭示了其在自发计数上的局限性。",
      "motivation": "神经网络和大型语言模型在可靠计数序列项目方面面临显著挑战。尽管基于序列计算的规则符号系统能轻松处理，但神经网络需通过学习获得计数能力，导致难以系统部署计数过程。先前研究表明循环架构仅能近似跟踪序列，但现代深度学习系统如LLMs是否能系统计数离散符号序列尚不明确。本文旨在填补这一空白，探索LLMs的序列枚举能力，以评估其组合泛化表现，并解决现有方法在计数方面的不足。",
      "method": "论文选取了五个最先进的大型语言模型，包括专有、开源和推理类型，研究其序列枚举能力。通过命名和生产任务，使用字母和单词序列，采用多种提示指令探讨思维链对计数策略自发涌现的影响。同时，评估同架构但不同规模的开源模型，观察计数能力是否遵循规模定律，并分析序列枚举过程中的嵌入动态，以研究数量编码的涌现特性。",
      "result": "实验结果显示，当明确提示计数时，部分LLMs能够部署计数程序完成任务。然而，当简单地要求枚举序列项数量时，所有模型均未自发采用计数策略。这表明尽管LLMs在某些条件下能进行计数，但整体上无法稳健且系统地执行计数过程，突显了与基线方法（如规则符号系统）相比的不足，未能展现可靠计数能力。",
      "conclusion": "论文结论强调，大型语言模型尽管具备强大的涌现能力，但在系统计数方面仍有欠缺，不能自发部署计数策略。这揭示了神经网络与符号方法在组合泛化上的持久差距，具有重要学术价值，为理解LLMs的计算局限性和未来研究方向提供了基础。潜在方向包括改进模型架构或结合神经与符号技术，以增强计数能力。",
      "tags": [
        "Large Language Models",
        "Sequential Enumeration",
        "Chain-of-Thought",
        "Scaling Laws",
        "Emergent Encoding"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:55.930486Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.04559",
    "title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function",
    "authors": [
      "Hyeongyu Kang",
      "Jaewoo Lee",
      "Woocheol Shin",
      "Kiyoung Om",
      "Jinkyoo Park"
    ],
    "abstract": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose Soft Q-based Diffusion Finetuning (SQDF), a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.04559.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04559",
    "published": "2025-12-04T08:21:52Z",
    "updated": "2026-01-13T04:42:44Z",
    "comment": "36 pages, 21 figures, 4 tables",
    "light_analysis": {
      "overview": "该论文提出SQDF，一种基于软Q函数的KL正则化强化学习方法，通过重参数化策略梯度减轻扩散模型微调中的奖励过度优化问题。",
      "motivation": "扩散模型在生成高似然样本方面表现卓越，但常需要与下游目标对齐以提升应用效果。现有扩散模型微调方法采用强化学习时，容易导致奖励过度优化，产生高奖励但不自然的样本，同时损害多样性，这限制了模型在实际任务中的适用性。本研究旨在解决这一问题，通过开发新方法来平衡奖励与多样性，提高微调过程的鲁棒性和有效性，从而增强扩散模型在真实场景中的性能。",
      "method": "SQDF是一种KL正则化的强化学习方法，应用训练无关、可微分的软Q函数估计的重参数化策略梯度，以优化扩散模型的微调过程。关键创新包括：在去噪过程中引入折扣因子进行正确信用分配，整合一致性模型以精炼Q函数估计并提升准确性，以及使用离线策略重放缓冲器来改进模式覆盖和有效管理奖励与多样性的权衡。这些技术结合了强化学习和扩散模型的特点，确保方法在保持稳定性的同时，增强样本的自然性和多样性。",
      "result": "实验结果显示，在文本到图像对齐任务中，SQDF实现了比基线方法更高的目标奖励，同时有效保持了样本的多样性，表明其成功减轻了奖励过度优化问题。在在线黑盒优化场景下，SQDF获得了高样本效率，生成的样本既自然又多样，证实了方法在提升性能方面的优势。虽然摘要未提供具体数字，但与现有方法相比，SQDF在平衡奖励和多样性方面表现出色，支持了其在扩散模型微调中的有效性。",
      "conclusion": "SQDF的主要贡献是提出了一种基于软Q函数的KL正则化强化学习方法，有效缓解了扩散模型微调中的奖励过度优化问题，提高了模型在下游任务中的适应性。学术价值在于推动了生成模型与强化学习的交叉研究，实际应用价值体现在文本到图像生成和在线优化等领域中，能生成高质量且多样化的样本。未来工作可扩展到其他生成任务或进一步优化算法效率以增强通用性。",
      "tags": [
        "Diffusion Models",
        "Soft Q-Function",
        "Reinforcement Learning",
        "Consistency Models",
        "Off-Policy Replay Buffer"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:04.291962Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.04068",
    "title": "Learning Steerable Clarification Policies with Collaborative Self-play",
    "authors": [
      "Jonathan Berant",
      "Maximillian Chen",
      "Adam Fisch",
      "Reza Aghajani",
      "Fantine Huot",
      "Mirella Lapata",
      "Jacob Eisenstein"
    ],
    "abstract": "To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.04068.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04068",
    "published": "2025-12-03T18:49:54Z",
    "updated": "2026-01-13T06:20:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了通过协作自我对弈训练可操控的澄清策略，以管理 AI 助手面对模糊查询时的不确定性，实现根据成本调整行为以提高交互效率。",
      "motivation": "AI 助手在处理模糊查询时，需要策略来决定何时直接猜测用户意图、枚举可能意图或提问澄清。这种策略高度依赖上下文因素，如用户偏好或交互模式，现有方法在不同场景下可能表现不佳。例如，在小屏幕设备或语音环境中，枚举意图可能效率低下或用户体验差，因此急需开发能灵活适应不同成本和限制条件的策略，以提升助手的响应准确性和用户满意度。",
      "method": "论文采用协作自我对弈方法训练可操控策略：设置两个代理，一个模拟用户发出模糊查询，另一个作为助手决定响应方式。模型输入包括澄清问题和每个生成词的数值成本，通过最大化成本惩罚后的准确性作为奖励来优化策略。关键创新点在于使用 Reinforced Self-Training (ReST) 训练框架，生成对话数据无需人工标注，使策略能根据输入成本预测性地调整行为，形成适应不同情境的决策模型。",
      "result": "实验结果表明，训练后的策略能够根据提供的成本值改变行为，实现更高的奖励和准确性。该方法还能泛化到训练时未观察到的成本值，显示策略具有良好的适应性。尽管摘要未明确说明具体性能指标数据，但论文报告了相对于基线方法在不透明度管理上的显著改进，强调了在多种测试场景下的有效性提升。",
      "conclusion": "本论文的主要贡献是提出了一种基于协作自我对弈和强化自训练的可操控澄清策略训练方法，有效提升 AI 助手处理模糊查询的能力。研究具有学术价值，为对话系统的强化学习应用提供了新范式；实际应用中，可增强智能助手在各种交互环境中的适应性和效率。未来工作可能包括在更复杂场景中验证方法或扩展处理多模态用户输入。",
      "tags": [
        "Reinforced Self-Training",
        "Self-play",
        "Steerable Policies",
        "Cost-Sensitive Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:13.860179Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.02789",
    "title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking",
    "authors": [
      "Haonan Tang",
      "Yanjun Chen",
      "Lezhi Jiang",
      "Qianfei Li",
      "Xinyu Guo"
    ],
    "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.02789.pdf",
    "abs_url": "https://arxiv.org/abs/2512.02789",
    "published": "2025-12-02T14:04:30Z",
    "updated": "2026-01-13T14:49:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "TrackNetV5通过引入运动方向解耦和残差驱动时空细化模块，显著提升了快速运动小物体跟踪的性能，解决了现有方法在处理遮挡和方向模糊方面的不足。",
      "motivation": "研究旨在解决快速运动小物体跟踪中的关键挑战，特别是在体育场景中。现有TrackNet系列方法存在局限性：早期版本V1-V3依赖纯视觉线索，难以应对遮挡；V4版本虽引入运动输入，但采用绝对差分方法丢弃了运动极性，导致方向模糊，影响跟踪精度和鲁棒性。这些问题突出了对一种能明确编码运动方向并有效处理遮挡的新方法的需求，以提高跟踪系统的可靠性和实用性。",
      "method": "TrackNetV5提出了一种新架构，集成两个核心机制。首先，运动方向解耦（MDD）模块将时间动态分解为带符号的极性场，明确编码运动发生和轨迹方向，解决方向模糊问题。其次，残差驱动时空细化（R-STR）头基于Transformer设计，采用粗到细范式，利用因子化的时空上下文估计修正残差，有效恢复被遮挡目标。该方法在TrackNetV2数据集上进行训练和评估，结合视觉和运动信息，通过优化网络设计提升跟踪精确度。",
      "result": "在TrackNetV2数据集上的实验显示，TrackNetV5实现了优异的性能，F1分数达到0.9859，准确率为0.9733，显著优于先前TrackNet版本，成为新的最先进方法。性能提升仅伴随计算开销微增，FLOPs相比V4仅增加3.7%，确保了实时推理能力。这些结果证明该方法在处理遮挡和方向模糊方面有效性突出，同时保持高效率，适合实际应用场景。",
      "conclusion": "TrackNetV5的主要贡献在于引入运动方向解耦和残差驱动时空细化机制，成功解决了快速运动小物体跟踪中的方向模糊和遮挡问题。该研究提高了跟踪精度并优化计算效率，具有学术价值如推动跟踪算法发展，以及实际应用价值如体育分析和实时监控。未来工作可探索扩展到其他运动跟踪领域或集成多模态数据以增强鲁棒性，但摘要未明确说明具体方向。",
      "tags": [
        "Motion Direction Decoupling",
        "Residual-Driven Spatio-Temporal Refinement",
        "Transformer",
        "Object Tracking",
        "Spatio-Temporal Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:56.905289Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.02004",
    "title": "AlignSAE: Concept-Aligned Sparse Autoencoders",
    "authors": [
      "Minglai Yang",
      "Xinyu Guo",
      "Zhengliang Shi",
      "Jinhe Bi",
      "Steven Bethard",
      "Mihai Surdeanu",
      "Liangming Pan"
    ],
    "abstract": "Large Language Models (LLMs) encode factual knowledge within hidden parametric spaces that are difficult to inspect or control. While Sparse Autoencoders (SAEs) can decompose hidden activations into more fine-grained, interpretable features, they often struggle to reliably align these features with human-defined concepts, resulting in entangled and distributed feature representations. To address this, we introduce AlignSAE, a method that aligns SAE features with a predefined ontology through a \"pre-train, then post-train\" curriculum. After an initial unsupervised training phase, we apply supervised post-training to bind specific concepts to dedicated latent slots while preserving the remaining capacity for general reconstruction. This separation creates an interpretable interface where specific concepts can be inspected and controlled without interference from unrelated features. Empirical results demonstrate that AlignSAE enables precise causal interventions, such as reliable \"concept swaps\", by targeting single, semantically aligned slots, and further supports multi-hop reasoning and a mechanistic probe of grokking-like generalization dynamics.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.02004.pdf",
    "abs_url": "https://arxiv.org/abs/2512.02004",
    "published": "2025-12-01T18:58:22Z",
    "updated": "2026-01-13T02:52:14Z",
    "comment": "23 pages, 16 figures, 7 tables",
    "light_analysis": {
      "overview": "AlignSAE方法通过预训练和后训练课程，实现稀疏自编码器特征与人类概念的对齐，提升大型语言模型的可解释性和可控性。",
      "motivation": "大型语言模型（LLMs）将事实知识编码在难以检查或控制的隐藏参数空间中。稀疏自编码器（SAEs）可以分解隐藏激活为更细粒度的特征，但现有方法常无法可靠地对齐这些特征与人类定义的概念，导致表示纠缠和分散，限制了模型的解释性和精确操控能力。因此，需要一种新方法来增强特征对齐，以便更好地理解和干预模型内部表示。",
      "method": "AlignSAE采用“先预训练，后后训练”的课程策略。首先进行无监督训练阶段，分解隐藏激活为稀疏特征；随后应用监督后训练，将预定义本体中的特定概念绑定到专门的潜在槽，同时保留其他槽用于一般重建。这种分离机制创建了一个可解释的界面，允许对单个概念进行独立检查和操控，避免无关特征干扰，从而改善特征对齐度和模型实用性。",
      "result": "实证结果表明，AlignSAE能够实现精确的因果干预，例如通过针对语义对齐的单一槽位进行可靠的“概念交换”。此外，该方法支持多跳推理和对类似grokking泛化动态的机械探测，显示出在特征对齐和模型控制方面的优势。尽管摘要未提供具体性能指标，但结果暗示AlignSAE在改善可解释性和干预能力上优于传统稀疏自编码器。",
      "conclusion": "本论文的主要贡献是提出了AlignSAE方法，通过概念对齐技术解决了稀疏自编码器中特征纠缠的问题。研究具有重要学术价值，为大型语言模型的可解释性和可控性提供了新工具，促进了因果推理和模型理解。未来工作可能涉及将该方法扩展到更多领域，或评估其在大规模数据集上的泛化性能和实际应用潜力。",
      "tags": [
        "Large Language Model",
        "Sparse Autoencoders",
        "Concept Alignment",
        "Interpretability",
        "Causal Intervention"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:16.817110Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.21005",
    "title": "ICPO: Intrinsic Confidence-Driven Group Relative Preference Optimization for Efficient Reinforcement Learning",
    "authors": [
      "Jinpeng Wang",
      "Chao Li",
      "Ting Ye",
      "Mengyuan Zhang",
      "Wei Liu",
      "Jian Luan"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates significant potential in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing RLVR methods are often constrained by issues such as coarse-grained rewards, reward noise, and inefficient exploration, which lead to unstable training and entropy collapse. To address this challenge, we propose the Intrinsic Confidence-Driven Group Relative Preference Optimization method (ICPO). The intuition behind it lies in the fact that the probabilities of an LLM generating different responses can inherently and directly reflect its self-assessment of the reasoning process. Inspired by the idea of preference modeling, ICPO calculates a preference advantage score for each response by comparing the relative generation probabilities of multiple responses under the same input prompt, and integrates this score with verifiable rewards to guide the exploration process. We have discovered that the preference advantage score not only alleviates the issues of coarse-grained rewards and reward noise but also effectively curbs overconfident errors, enhances the relative superiority of undervalued high-quality responses, and prevents the model from overfitting to specific strategies. Comprehensive experiments across four general-domain benchmarks and three mathematical benchmarks demonstrate that ICPO steadily boosts reasoning compared to GRPO.",
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.21005.pdf",
    "abs_url": "https://arxiv.org/abs/2511.21005",
    "published": "2025-11-26T03:10:15Z",
    "updated": "2026-01-13T03:30:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种名为ICPO的强化学习方法，通过结合大语言模型的内在置信度和相对偏好优化，有效提升了推理任务的效率和稳定性。",
      "motivation": "强化学习与可验证奖励（RLVR）在增强大语言模型推理能力方面具有潜力，但现有方法面临粗粒度奖励、奖励噪声和低效探索等问题，导致训练不稳定和熵崩溃。这些问题限制了RLVR的实际应用，因为粗糙的奖励信号可能引入噪声，而低效探索则阻碍模型优化。因此，需要一种更精细的优化方法来克服这些挑战，提升推理任务的性能。",
      "method": "ICPO方法基于大语言模型生成不同响应的概率来反映其内在置信度。通过比较同一输入提示下多个响应的生成概率，计算偏好优势分数，并将此分数与可验证奖励结合，以指导探索过程。该方法借鉴了偏好建模思想，利用相对概率评估来缓解奖励噪声和过拟合，避免模型过度依赖特定策略。关键创新在于引入了组相对偏好优化，增强了高价值响应的识别能力。",
      "result": "实验在四个通用领域基准和三个数学基准上进行，结果表明ICPO相比基线方法GRPO，能够稳定提升推理能力。摘要未明确说明具体性能指标如准确率或效率改进，但指出ICPO在多个任务中展现出优越性，有效解决了奖励噪声和探索效率问题。这表明ICPO在增强模型推理方面具有实际效果，并通过相对对比验证了其优势。",
      "conclusion": "ICPO的提出为RLVR提供了一种新优化框架，通过内在置信度和相对偏好的结合，解决了训练不稳定和熵崩溃问题。该研究具有学术价值，推动了强化学习在语言模型中的应用，提升了推理效率。未来可扩展至更复杂的任务领域，如多模态推理或自适应学习，以进一步验证其泛化能力。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Preference Optimization",
        "Verifiable Rewards",
        "Intrinsic Confidence"
      ]
    },
    "analyzed_at": "2026-01-14T03:34:57.812052Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.19137",
    "title": "FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation",
    "authors": [
      "Zhifeng Xie",
      "Keyi Zhang",
      "Yiye Yan",
      "Yuling Guo",
      "Fan Yang",
      "Jiting Zhou",
      "Mengtian Li"
    ],
    "abstract": "Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.19137.pdf",
    "abs_url": "https://arxiv.org/abs/2511.19137",
    "published": "2025-11-24T14:00:40Z",
    "updated": "2026-01-13T18:51:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了FilmSceneDesigner系统，一个通过基于代理的链式框架和程序化生成管道自动化电影布景设计的方法。",
      "motivation": "传统电影布景设计依赖专家手动建模，劳动密集且耗时，限制了创意效率和制作灵活性，影响电影产业的生产周期。现有自动化方法往往缺乏对专业工作流的仿真和参数连贯性保障，导致生成场景的真实性不足。因此，本研究旨在开发一个模仿专业流程的自动化系统，以解决效率问题并确保场景与电影标准的一致性。",
      "method": "系统核心包括基于代理的链式框架，使用提示策略从自然语言描述生成结构化参数，确保准确性和连贯性；以及程序化生成管道，执行参数进行楼层布局生成、材料分配、门窗放置和对象检索布局。关键创新在于将专业工作流分解为序列化步骤，并构建了SetDepot-Pro数据集，包含6,862个电影专用3D资产和733种材料，以增强真实性和多样性。",
      "result": "实验和人类评估显示，系统生成的场景在结构合理性和电影保真度方面表现优异，支持虚拟预览、施工图纸和情绪板创建等下游任务。摘要未明确提供具体性能指标如准确率提升，但强调了系统能从头构建完整场景，并通过评估验证其与专业标准的对齐，相比传统手动方法更高效。",
      "conclusion": "本研究贡献了FilmSceneDesigner系统，实现了电影布景设计的自动化，显著提高效率并扩展了创意应用范围。学术价值在于结合了代理链和程序化生成技术，推动了AI在电影产业中的实际应用。潜在局限性包括数据集规模和生成质量的进一步优化，未来工作可扩展资产库、增强生成多样性或探索多模态输入。",
      "tags": [
        "Agent-based Chaining Framework",
        "Procedural Generation",
        "3D Scene Generation",
        "Natural Language Processing",
        "Dataset Construction"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:34.039358Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.12735",
    "title": "Backdoor Attacks on Open Vocabulary Object Detectors via Multi-Modal Prompt Tuning",
    "authors": [
      "Ankita Raj",
      "Chetan Arora"
    ],
    "abstract": "Open-vocabulary object detectors (OVODs) unify vision and language to detect arbitrary object categories based on text prompts, enabling strong zero-shot generalization to novel concepts. As these models gain traction in high-stakes applications such as robotics, autonomous driving, and surveillance, understanding their security risks becomes crucial. In this work, we conduct the first study of backdoor attacks on OVODs and reveal a new attack surface introduced by prompt tuning. We propose TrAP (Trigger-Aware Prompt tuning), a multi-modal backdoor injection strategy that jointly optimizes prompt parameters in both image and text modalities along with visual triggers. TrAP enables the attacker to implant malicious behavior using lightweight, learnable prompt tokens without retraining the base model weights, thus preserving generalization while embedding a hidden backdoor. We adopt a curriculum-based training strategy that progressively shrinks the trigger size, enabling effective backdoor activation using small trigger patches at inference. Experiments across multiple datasets show that TrAP achieves high attack success rates for both object misclassification and object disappearance attacks, while also improving clean image performance on downstream datasets compared to the zero-shot setting. Code: https://github.com/rajankita/TrAP",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.12735.pdf",
    "abs_url": "https://arxiv.org/abs/2511.12735",
    "published": "2025-11-16T19:05:31Z",
    "updated": "2026-01-13T12:36:18Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "本论文首次提出TrAP方法，一种针对开放词汇对象检测器的多模态后门攻击策略，通过提示调优实现高效后门注入。",
      "motivation": "开放词汇对象检测器（OVODs）结合视觉和语言模型，基于文本提示检测任意对象类别，在机器人、自动驾驶等高风险应用中广泛应用，因此其安全风险分析至关重要。现有研究可能未充分关注通过提示调优引入的新攻击表面，这可能导致模型在隐藏后门被触发时产生恶意行为。本工作的动机是填补这一空白，揭示OVODs的安全漏洞，推动更安全的模型设计。",
      "method": "提出TrAP（Trigger-Aware Prompt tuning），一种多模态后门注入方法，通过联合优化图像和文本模态的提示参数以及视觉触发器，使用轻量级可学习提示令牌，避免重新训练基础模型权重，从而在保留泛化能力的同时嵌入后门。采用基于课程的训练策略，逐步缩小触发器尺寸，使推理时小触发器补丁能有效激活后门。实验在多个数据集上进行，以验证方法的有效性。",
      "result": "在多个数据集上的实验显示，TrAP实现了高攻击成功率，包括对象误分类和对象消失攻击。与零样本设置相比，该方法在下游数据集的干净图像性能上也有所提升，表明在植入后门时未损害正常功能。摘要未明确说明具体数据，但强调了攻击成功率和性能改进的趋势。",
      "conclusion": "本工作首次系统研究了开放词汇对象检测器的后门攻击，提出TrAP方法，揭示了通过提示调优的新攻击表面，提高了对多模态模型安全风险的认识。这一研究对开发鲁棒AI系统具有学术和实际价值，未来工作可探索防御策略或扩展攻击场景，以增强模型安全性。",
      "tags": [
        "Backdoor Attacks",
        "Open-Vocabulary Object Detection",
        "Multi-Modal Learning",
        "Prompt Tuning",
        "Curriculum Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:40.413619Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.11048",
    "title": "PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI",
    "authors": [
      "Sun Jo",
      "Seok Young Hong",
      "JinHyun Kim",
      "Seungmin Kang",
      "Ahjin Choi",
      "Don-Gwan An",
      "Simon Song",
      "Je Hyeong Hong"
    ],
    "abstract": "4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.11048.pdf",
    "abs_url": "https://arxiv.org/abs/2511.11048",
    "published": "2025-11-14T08:01:24Z",
    "updated": "2026-01-13T07:54:23Z",
    "comment": "Accepted at AAAI 2026. Supplementary material included after references. 27 pages, 21 figures, 11 tables",
    "light_analysis": {
      "overview": "PINGS-X提出了一种物理信息引导的归一化高斯溅射框架，通过轴对齐实现4D流MRI的高效超分辨率。",
      "motivation": "4D流磁共振成像（MRI）是心血管诊断中估计血流速度的关键非侵入技术，但高时空分辨率需求导致扫描时间长，形成了获取速度与预测准确性之间的权衡。现有物理信息神经网络（PINNs）虽用于超分辨率，但训练过程缓慢且需为每位患者单独执行，限制了实际应用，因此开发高效超分辨率方法以加速数据获取、提升诊断效率至关重要。",
      "method": "PINGS-X采用轴对齐的时空高斯表示来建模高分辨率流速，扩展了3D高斯溅射（3DGS）概念。关键创新包括：归一化高斯溅射提供形式收敛保证；轴对齐高斯简化高维数据训练，保持准确性和收敛性；高斯合并过程防止退化解决方案并提升计算效率。该方法基于CFD和真实4D流MRI数据集，实现高效训练。",
      "result": "实验在CFD和真实4D流MRI数据集上进行，PINGS-X显著减少了训练时间，同时实现了卓越的超分辨率准确性。与基线方法（如PINNs）相比，该框架在保持或提高预测精度的前提下，大幅优化了计算效率，摘要未明确具体性能指标，但强调了效率和准确性的双重改进。",
      "conclusion": "本研究提出了PINGS-X框架，解决了PINNs在4D流MRI超分辨率中训练慢的局限，通过高斯溅射技术实现了高效准确的流速建模。其学术价值在于扩展了高斯表示方法到医学图像处理领域，实际应用上可促进心血管疾病的快速诊断，未来工作可能涉及算法优化或扩展到其他任务，但摘要未明确说明具体局限性。",
      "tags": [
        "4D Flow MRI",
        "Physics-Informed Neural Networks",
        "Gaussian Splatting",
        "Super-Resolution",
        "Axes-Aligned Gaussians"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:45.631258Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.10859",
    "title": "Private Zeroth-Order Optimization with Public Data",
    "authors": [
      "Xuchen Gong",
      "Tian Li"
    ],
    "abstract": "One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\\times$ runtime speedup.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.10859.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10859",
    "published": "2025-11-13T23:51:24Z",
    "updated": "2026-01-13T04:08:46Z",
    "comment": "NeurIPS 2025",
    "light_analysis": {
      "overview": "论文提出PAZO框架，利用公共数据辅助私有零阶优化，显著提升隐私/效用权衡并加速计算，解决了一阶差分隐私算法的高成本问题。",
      "motivation": "研究动机源于一阶差分隐私机器学习算法如DP-SGD的高计算和内存成本，尽管有优化实现，但仍限制实际部署。零阶方法通过函数评估近似梯度，更易于私有化，但现有方法在效用上相对较低，且仅在有限应用领域评估。公共数据在许多场景中可获取，可用以改进梯度近似，从而克服零阶方法的不足，并提高私有优化效率。",
      "method": "研究方法提出公共数据辅助的零阶优化器（PAZO）框架，利用公共信息引导私有数据的梯度近似，减少噪声影响。关键创新点包括基于公共与私有数据相似性假设设计优化算法，最小化额外开销，并提供理论分析确保在数据相似条件下提升隐私/效用权衡。摘要未明确说明具体模型架构或数据集细节，但提及在视觉和文本任务中应用。",
      "result": "主要实验结果表明，PAZO在视觉和文本任务的预训练和微调设置中，实现优于最佳一阶基线的隐私/效用权衡，特别是在高隐私区域表现更佳。同时，提供高达16倍的运行时加速，验证了其在提升计算效率和保护隐私方面的有效性。这些结果基于实证评估，与基线方法对比显著。",
      "conclusion": "论文主要贡献是提出PAZO框架，结合公共数据改进私有零阶优化，在隐私保护和计算效率上取得突破。学术价值在于为差分隐私机器学习提供新方法，实际应用价值在于降低部署成本并扩展应用领域。局限性可能包括依赖公共数据相似性假设，未来工作可探索更广泛的数据场景或增强算法适应性。",
      "tags": [
        "Zeroth-Order Optimization",
        "Differential Privacy",
        "Public Data",
        "Gradient Approximation",
        "Privacy/Utility Tradeoff"
      ]
    },
    "analyzed_at": "2026-01-14T03:35:59.941134Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.08585",
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "authors": [
      "Jingtong Yue",
      "Ziqi Huang",
      "Zhaoxi Chen",
      "Xintao Wang",
      "Pengfei Wan",
      "Ziwei Liu"
    ],
    "abstract": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.08585.pdf",
    "abs_url": "https://arxiv.org/abs/2511.08585",
    "published": "2025-11-11T18:59:50Z",
    "updated": "2026-01-13T15:42:01Z",
    "comment": "Project page: https://world-model-roadmap.github.io/ Github Repo: https://github.com/ziqihuangg/Awesome-From-Video-Generation-to-World-Model",
    "light_analysis": {
      "overview": "本综述论文将视频基础模型概念化为隐式世界模型与视频渲染器的组合，系统概述其四代演进，强调其作为物理模拟引擎和交互平台的潜力。",
      "motivation": "视频生成领域正从单纯生成视觉吸引力剪辑，转向构建支持交互和物理合理性的虚拟环境。这一转变至关重要，因为它为机器人、自动驾驶和交互式游戏等应用提供了更真实的模拟基础。现有方法往往局限于短时视觉生成，缺乏对物理动态、长期一致性和代理交互的建模，难以满足复杂场景下的实际需求。",
      "method": "论文提出视频基础模型由两个核心组件构成：隐式世界模型编码物理定律、交互动态和代理行为，作为潜在模拟引擎；视频渲染器则将模拟转化为逼真视觉输出。研究通过定义四代模型（从基础视频生成到整合世界模型的演进），分析其核心特性、代表性工作，并探讨在机器人等领域的应用，强调模型的物理合理性和多尺度规划能力。",
      "result": "摘要未明确说明具体实验数据或性能指标，但综述了视频基础模型在物理合理性、实时多模态交互和跨时空尺度规划方面的进展。通过对比四代模型的核心能力，论文突出了模型在提升视觉一致性、交互支持和应用广泛性方面的优势，然而未提供量化对比或基准测试结果。",
      "conclusion": "主要贡献是系统化地概述了视频基础模型向隐式世界模型的演变，提供了理论框架和应用展望。研究具有学术价值，促进了对模拟技术和交互系统的理解；未来方向包括整合代理智能以增强系统评估和设计，解决开放挑战如模型可解释性和数据效率问题。",
      "tags": [
        "Video Foundation Models",
        "World Models",
        "Video Rendering",
        "Physical Simulation",
        "Agent Intelligence"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:54.149198Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.08399",
    "title": "Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment",
    "authors": [
      "Hua Ye",
      "Hang Ding",
      "Siyuan Chen",
      "Yiyang Jiang",
      "Changyuan Zhang",
      "Xuan Zhang"
    ],
    "abstract": "Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.08399.pdf",
    "abs_url": "https://arxiv.org/abs/2511.08399",
    "published": "2025-11-11T16:15:15Z",
    "updated": "2026-01-13T06:56:09Z",
    "comment": "24 pages, 6 figures, 5 tables. Submitted to NeurIPS 2025",
    "light_analysis": {
      "overview": "提出BACL方法，通过边界感知课程学习，利用模糊负对作为课程信号，显著提升多模态对齐的精度，无需额外标签。",
      "motivation": "在多模态对齐任务中，现有模型往往将所有负对同等对待，忽略了与正对仅细微差异的模糊负对，这可能导致模型学习不精确，影响对齐性能。该问题重要，因为现实场景中边界情况频繁出现，而当前方法的不足在于缺乏对这些模糊案例的区分处理，限制了模型在复杂环境下的表现。BACL旨在通过引入边界感知机制来优化学习过程，提高模型的鲁棒性和准确性。",
      "method": "BACL是一个轻量级附加框架，包含两个核心模块：边界感知负采样器用于逐步引入更难的模糊负对，形成课程学习信号；对比局部注意力损失通过对比学习突出不匹配区域，提高对齐精确度。这些模块完全可微分，能够与任何现成的双编码器兼容，无需额外训练数据。理论分析预测了O(1/n)的误差收敛率，表明方法的快速收敛特性，创新点在于将边界案例转化为有效的学习资源。",
      "result": "在四个大规模多模态基准测试中，BACL与基线CLIP相比，召回率@1（R@1）提升了高达32%，实现了新的最先进水平。理论预测支持快速收敛，实验结果表明方法有效且无需额外标注数据，突出了其在资源效率和性能改进上的优势。这些结果证明了BACL在处理模糊负对和提升对齐精度方面的显著效果，为多模态任务提供了实用解决方案。",
      "conclusion": "论文的主要贡献是BACL框架，它通过边界感知课程学习和局部注意力机制，有效提升了多模态对齐的精度和鲁棒性。该方法具有广泛适用性，可与现有模型集成，具有学术价值（引入新方法处理模糊负对）和实际应用价值（提高模型性能）。摘要未明确说明潜在局限性，但未来工作可能包括扩展到更多模态或复杂任务，或优化边界情况处理策略。",
      "tags": [
        "Multimodal Alignment",
        "Curriculum Learning",
        "Contrastive Learning",
        "Local Attention",
        "Boundary-aware Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:15.413584Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.08066",
    "title": "Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression",
    "authors": [
      "Cheng Yuan",
      "Jiawei Shao",
      "Chi Zhang",
      "Xuelong Li"
    ],
    "abstract": "Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further intensifies the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across diverse model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. A distinctive feature of information capacity is its incorporation of tokenizer efficiency, which affects inference costs but is often neglected in LLM evaluations. We assess the information capacity of 52 open-source models and observe a consistent information capacity among different-sized models within a series. Experiments on 5 heterogeneous datasets reveal strong linguistic bias in mainstream LLMs. Three major factors of information capacity include tokenizer efficiency, pretraining data, and the mixture-of-experts architecture. Empirical results verify the accuracy of performance prediction across model sizes based on information capacity and show the correlation between information capacity and benchmark scores.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.08066.pdf",
    "abs_url": "https://arxiv.org/abs/2511.08066",
    "published": "2025-11-11T10:07:32Z",
    "updated": "2026-01-13T09:23:25Z",
    "comment": "Code: https://github.com/TeleAI-AI-Flow/InformationCapacity. Data: https://huggingface.co/datasets/TeleAI-AI-Flow/InformationCapacity",
    "light_analysis": {
      "overview": "论文提出信息容量度量标准，基于文本压缩性能评估大型语言模型效率，并纳入分词器效率等关键因素作为统一评估指标。",
      "motivation": "大型语言模型的快速发展导致计算资源需求激增，测试时缩放加剧了模型能力与资源消耗的矛盾，推理效率变得至关重要。然而，现有评估方法缺乏统一的度量标准来准确衡量不同大小和架构LLMs的效率，且常忽略分词器效率等实际影响因素，使得模型效率评估不够全面。",
      "method": "基于压缩与智能的相关性，论文提出信息容量作为模型效率度量，定义为文本压缩性能相对于计算复杂性的比率。核心创新是纳入分词器效率，这在LLM评估中常被忽略。方法包括评估52个开源模型的信息容量，使用5个异构数据集进行实验，并识别出分词器效率、预训练数据和混合专家架构是信息容量的三个主要因素。",
      "result": "在52个开源模型的评估中，同一系列不同大小模型的信息容量表现一致，表明信息容量能跨模型大小稳定反映效率。在5个异构数据集上的实验揭示主流LLMs存在显著语言偏差。实证结果验证了基于信息容量的性能预测准确性，并显示信息容量与基准分数存在正相关性，证明该度量能有效评估模型效率。",
      "conclusion": "论文的主要贡献是提出信息容量作为统一的LLM效率度量标准，强调分词器效率的重要性，为模型评估提供新视角。研究具有学术价值，有助于模型选择和优化，对实际部署中的资源管理有指导意义。摘要未明确说明局限性，未来工作可能涉及扩展到更广泛的模型架构或数据集。",
      "tags": [
        "Large Language Models",
        "Text Compression",
        "Tokenizer Efficiency",
        "Model Efficiency Evaluation",
        "Mixture-of-Experts"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:13.579906Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.07198",
    "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
    "authors": [
      "Hua Ye",
      "Siyuan Chen",
      "Haoliang Zhang",
      "Weihao Luo",
      "Yanbin Li",
      "Xuan Zhang"
    ],
    "abstract": "Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.07198.pdf",
    "abs_url": "https://arxiv.org/abs/2511.07198",
    "published": "2025-11-10T15:27:26Z",
    "updated": "2026-01-13T06:58:58Z",
    "comment": "20 pages, 5 figures, 21 tables. Accepted at NeurIPS 2025. Corresponding author: Xuan Zhang (xuanzhang2199@gmail.com)",
    "light_analysis": {
      "overview": "该论文提出一种基于划分的多阶段微调框架，通过策略性分区平衡域差异与协同效应，以优化多领域LLM适应并减少域间干扰。",
      "motivation": "大型语言模型在跨多个异构域适应时面临域间干扰问题，导致负面迁移和性能下降。现有方法在处理域差异和协同效应方面不足，难以有效利用多域数据。该研究旨在解决这一挑战，通过优化微调策略提升LLMs的泛化能力和多域学习效果，对自然语言处理任务的实际应用具有重要意义。",
      "method": "论文提出一个分区基于的多阶段微调框架，策略性地将多个域划分为子集（阶段），平衡域间差异、协同效应和模型容量约束。每个阶段对LLM进行微调，最大化协同效应以减少干扰。作者还进行了理论分析，推导出新的泛化界限来支持分区策略。使用的数据集和模型架构摘要未明确说明，但推断涉及标准LLMs和多样化的语言理解任务。",
      "result": "通过广泛实验评估，该方法在各种语言理解任务上始终优于最先进的基线，验证了框架在减少域间干扰和提升模型性能方面的有效性。实验结果强调了方法的一致优越性，但具体数据如准确率或效率改进摘要未明确说明，需要进一步查阅论文细节。",
      "conclusion": "该研究主要贡献在于提出并验证了一种创新的多域LLM微调框架，通过分区策略平衡域差异和协同效应，改善了跨域适应能力。学术价值体现在理论分析和泛化界限的推导，实际价值适用于需跨领域适应的NLP任务。局限性或未来工作摘要未明确说明，可能涉及扩展到更多领域或优化计算效率。",
      "tags": [
        "Large Language Model",
        "Multi-Domain Fine-Tuning",
        "Partition Strategy",
        "Generalization Bounds",
        "Negative Transfer"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:16.161128Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.26167",
    "title": "ToolRM: Towards Agentic Tool-Use Reward Modeling",
    "authors": [
      "Renhao Li",
      "Jianhong Tu",
      "Yang Su",
      "Yantao Liu",
      "Fei Huang",
      "Hamid Alinejad-Rokny",
      "Derek F. Wong",
      "Junyang Lin",
      "Min Yang"
    ],
    "abstract": "Reward models (RMs) play a critical role in aligning large language models (LLMs) with human preferences. Yet in the domain of tool learning, the lack of RMs specifically designed for function-calling tasks has limited progress toward more capable agentic AI. We introduce ToolRM, a family of lightweight reward models tailored for general tool-use scenarios. To build these models, we propose a novel pipeline that constructs high-quality pairwise preference data using rule-based scoring and multidimensional sampling. This yields ToolPref-Pairwise-30K, a diverse, balanced, and challenging preference dataset that supports both generative and discriminative reward modeling. We also introduce TRBench$_{BFCL}$, a benchmark built on the agent evaluation suite BFCL to evaluate RMs on tool calling tasks. Trained on our constructed data, models from the Qwen3-4B/8B series achieve up to 17.94% higher accuracy, substantially outperforming frontier LLMs and RMs in pairwise reward judgments. Beyond training objectives, generative ToolRM generalizes to broader critique tasks, including Best-of-N sampling and self-correction. Experiments on ACEBench highlight its effectiveness and efficiency, enabling inference-time scaling while reducing output token usage by over 66%. Its support for downstream RL training further validates its practical utility. We release data to facilitate future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.26167.pdf",
    "abs_url": "https://arxiv.org/abs/2510.26167",
    "published": "2025-10-30T06:08:27Z",
    "updated": "2026-01-13T13:28:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了ToolRM系列轻量级奖励模型，专门针对工具使用场景，通过构建高质量偏好数据集和基准，显著提升代理AI性能。",
      "motivation": "奖励模型在将大型语言模型与人类偏好对齐中起关键作用，但在工具学习领域，缺乏专门为功能调用任务设计的奖励模型，这限制了更强大代理AI的进展。现有方法未能有效处理工具调用，导致代理AI性能受限，因此开发适用于工具使用场景的专门奖励模型成为迫切需求，以促进实际应用中的代理智能体发展。",
      "method": "研究提出ToolRM系列轻量级奖励模型，核心创新在于构建高质量偏好数据的流水线，采用规则基评分和多维采样生成ToolPref-Pairwise-30K数据集，该数据集多样、平衡且挑战性高，支持生成式和判别式奖励建模。同时引入基于BFCL代理评估套件的TRBench基准来评估奖励模型在工具调用任务上的性能，模型基于Qwen3-4B/8B系列进行训练。",
      "result": "实验结果表明，在构建数据上训练后，ToolRM模型实现高达17.94%的准确率提升，在成对奖励判断中显著优于前沿大型语言模型和奖励模型。生成式ToolRM能泛化到Best-of-N采样和自我纠正等批判任务，在ACEBench上减少输出令牌使用超过66%，提高了效率。此外，支持下游强化学习训练进一步验证了其实用价值。",
      "conclusion": "本研究的主要贡献是提出了ToolRM奖励模型系列和高效的数据构建方法，为工具学习领域的奖励建模提供了创新解决方案。学术上推动了代理AI研究的发展，实际上提升了模型性能和效率，具有应用价值。未来工作可基于发布的数据集进行扩展研究，但摘要未明确说明具体局限性。",
      "tags": [
        "Reward Modeling",
        "Tool Learning",
        "Preference Data",
        "Benchmarking",
        "Generative Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:29.575999Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.23301",
    "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
    "authors": [
      "Yingying Feng",
      "Jie Li",
      "Jie Hu",
      "Yukang Zhang",
      "Lei Tan",
      "Jiayi Ji"
    ],
    "abstract": "Real-world object re-identification (ReID) systems often face modality inconsistencies, where query and gallery images come from different sensors (e.g., RGB, NIR, TIR). However, most existing methods assume modality-matched conditions, which limits their robustness and scalability in practical applications. To address this challenge, we propose MDReID, a flexible any-to-any image-level ReID framework designed to operate under both modality-matched and modality-mismatched scenarios. MDReID builds on the insight that modality information can be decomposed into two components: modality-shared features that are predictable and transferable, and modality-specific features that capture unique, modality-dependent characteristics. To effectively leverage this, MDReID introduces two key components: the Modality Decoupling Learning (MDL) and Modality-aware Metric Learning (MML). Specifically, MDL explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enforces orthogonality and complementarity between the two components to enhance discriminative power across modalities. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDReID. Notably, MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in general modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively. The code is available at: \\textcolor{magenta}{https://github.com/stone96123/MDReID}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.23301.pdf",
    "abs_url": "https://arxiv.org/abs/2510.23301",
    "published": "2025-10-27T13:08:46Z",
    "updated": "2026-01-13T17:44:49Z",
    "comment": "Accepted by NeurIPS 2025",
    "light_analysis": {
      "overview": "提出MDReID框架，通过模态解耦学习实现任何到任何的多模态对象重识别，有效处理模态匹配和不匹配场景。",
      "motivation": "现实世界的对象重识别系统常面临模态不一致问题，例如查询和图库图像来自不同传感器（如RGB、NIR、TIR）。现有方法大多假设模态匹配条件，这限制了它们在实际应用中的鲁棒性和可扩展性，无法有效处理跨模态检索。本研究旨在解决这一挑战，开发一个灵活的框架来提升多模态ReID在真实场景下的性能。",
      "method": "MDReID基于模态特征可分解为模态共享和模态特定特征的洞察。核心方法包括两个关键组件：模态解耦学习（MDL）显式地将特征分解为共享和特定表示，以支持模态匹配和不匹配场景下的检索；模态感知度量学习（MML）则通过强制正交性和互补性来增强跨模态的判别能力。该方法不依赖特定数据集或架构，而是通用地应用于任何到任何的多模态ReID任务。",
      "result": "在三个多模态ReID基准（RGBNT201、RGBNT100、MSVR310）上的实验显示，MDReID在模态匹配场景中mAP分别提升了9.8%、3.0%和11.5%，在模态不匹配场景中平均提升了3.4%、11.8%和10.9%。这些结果一致证明了其相对于基线方法的优越性，有效提升了跨模态检索的性能。",
      "conclusion": "本研究贡献了MDReID框架，通过模态解耦学习解决了多模态对象重识别中的模态不一致问题，具有学术价值，为多模态学习提供了新方法，并增强了现实应用中ReID系统的鲁棒性。未来工作可探索扩展到更多模态或复杂场景，以进一步提升实用性。",
      "tags": [
        "Multi-Modal Object Re-Identification",
        "Modality Decoupling Learning",
        "Modality-aware Metric Learning",
        "Feature Decomposition",
        "Any-to-Any Retrieval"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:38.076508Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.01891",
    "title": "Multi-Personality Generation of LLMs at Decoding-time",
    "authors": [
      "Rongxin Chen",
      "Yunfan Li",
      "Yige Yuan",
      "Bingbing Xu",
      "Huawei Shen"
    ],
    "abstract": "Multi-personality generation for LLMs, enabling simultaneous embodiment of multiple personalization attributes, is a fundamental challenge. Existing retraining-based approaches are costly and poorly scalable, while decoding-time methods often rely on external models or heuristics, limiting flexibility and robustness. In this paper, we propose a novel Multi-Personality Generation (MPG) framework under the decoding-time combination paradigm. It flexibly controls multi-personality without relying on scarce multi-dimensional models or extra training, leveraging implicit density ratios in single-dimensional models as a \"free lunch\" to reformulate the task as sampling from a target strategy aggregating these ratios. To implement MPG efficiently, we design Speculative Chunk-level based Rejection sampling (SCR), which generates responses in chunks and parallelly validates them via estimated thresholds within a sliding window. This significantly reduces computational overhead while maintaining high-quality generation. Experiments on MBTI personality and Role-Playing demonstrate the effectiveness of MPG, showing improvements up to 16%-18%. Code and data are available at https://github.com/Libra117/MPG .",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.01891.pdf",
    "abs_url": "https://arxiv.org/abs/2511.01891",
    "published": "2025-10-27T09:45:11Z",
    "updated": "2026-01-13T13:22:05Z",
    "comment": "Accepted by WSDM 2026",
    "light_analysis": {
      "overview": "论文提出一种基于解码时的多人物个性生成框架，通过隐含密度比率和块级拒绝采样实现高效控制，无需重训练或外部模型。",
      "motivation": "多人物个性生成是大语言模型（LLMs）的关键挑战，旨在使模型同时体现多种个性化属性。现有基于重训练的方法成本高昂且扩展性差，而解码时方法常依赖外部模型或启发式规则，导致灵活性和鲁棒性不足。因此，迫切需要开发一种更高效、灵活的方法来动态集成多人物个性，以支持个性化AI应用的实际需求。",
      "method": "论文提出Multi-Personality Generation（MPG）框架，采用解码时组合范式。核心方法是将任务重新表述为从聚合单维模型隐含密度比率的目标策略中采样，避免了对稀缺多维模型或额外训练的依赖。为提升效率，设计了Speculative Chunk-level based Rejection sampling（SCR），以块为单位生成响应，并通过滑动窗口内的估计阈值并行验证，从而在减少计算开销的同时维持高质量生成。",
      "result": "实验在MBTI人格类型和角色扮演任务上进行，验证了MPG框架的有效性。与基线方法相比，MPG在相关性能指标上实现了高达16%到18%的显著提升，展示了其在多人物个性控制方面的优越性。具体实验细节在摘要中未详细说明，但结果明确证明了该方法在灵活性和效率上的改进。",
      "conclusion": "MPG框架的主要贡献在于提供了一种无需重训练或外部模型的多人物个性生成方法，通过隐含密度比率和SCR采样技术实现了灵活、高效的控制。这项研究对个性化AI应用具有重要学术和实际价值，但未来可能需要探索更多样化的人物属性和扩展性挑战，以进一步优化模型性能。",
      "tags": [
        "Large Language Models",
        "Multi-Personality Generation",
        "Decoding-time Methods",
        "Implicit Density Ratios",
        "Speculative Rejection Sampling"
      ]
    },
    "analyzed_at": "2026-01-14T03:36:39.768721Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22462",
    "title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration",
    "authors": [
      "Abhijnan Nath",
      "Nikhil Krishnaswamy"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly being deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and safe interruptibility literature to offer novel theoretical insights on collaborative behavior between LLM-driven collaborator agents and an intervention agent. Our goal is to learn an ideal partner-aware collaborator that increases the group's common-ground (CG) alignment on task-relevant propositions-by intelligently collecting information provided in interventions by a partner agent. We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose Interruptible Collaborative Roleplayer (ICR)-a novel partner-aware learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.22462.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22462",
    "published": "2025-10-26T00:05:48Z",
    "updated": "2026-01-13T03:18:44Z",
    "comment": "Fixed typographic errors in the previous manuscript",
    "light_analysis": {
      "overview": "本文提出Interruptible Collaborative Roleplayer（ICR）算法，训练伙伴感知的LLM协作代理以优化团队共同基础对齐。",
      "motivation": "随着大语言模型在代理协作中的广泛应用，评估其多方任务协作能力变得日益重要。现有基于RLHF的方法使代理天生忽略伙伴干预，导致团队共同基础难以提升，限制了协作效果。本研究基于AI对齐和安全中断性文献，旨在解决如何让LLM代理有效感知和响应伙伴输入，以实现更高效的协作，对实际应用如人机协作系统具有关键意义。",
      "method": "研究方法采用双玩家修改行动MDP框架分析标准代理行为，提出Interruptible Collaborative Roleplayer（ICR）算法。ICR是一种伙伴感知学习算法，通过训练代理智能收集伙伴干预信息，优化共同基础对齐。关键创新在于结合中断性与协作性，使代理能主动适应并利用伙伴输入，以探索更多样化解决方案。",
      "result": "在多个协作任务环境中的实验表明，ICR算法平均更能促进成功的共同基础收敛和探索更多样化解决方案，但具体准确率或效率改进等性能指标摘要未明确说明。与基线方法相比，ICR显示出在协作任务中的优势。",
      "conclusion": "本文主要贡献是提出了ICR算法，为训练伙伴感知的LLM协作代理提供了新方法，增强了其在多方任务中的协作能力。研究结合理论建模与算法设计，具有学术价值，并推动AI对齐和协作系统的发展。未来工作可扩展至更复杂环境或实际应用场景。",
      "tags": [
        "Large Language Models",
        "Reinforcement Learning from Human Feedback",
        "Markov Decision Process",
        "Collaborative AI",
        "Partner-Aware Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:00.111115Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22454",
    "title": "SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning",
    "authors": [
      "Linhan Wang",
      "Jianwen Dou",
      "Wang Li",
      "Shengkun Wang",
      "Zhiwu Xie",
      "Chang-Tien Lu",
      "Yinlin Chen"
    ],
    "abstract": "Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution. Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck. Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized. In this work, we present a fast, label-efficient semi-supervised framework that exploits this untapped data. Our framework consists of two components: (i) an end-to-end heatmap-supervised detection model inspired by keypoint detection, and (ii) a teacher-student co-training mechanism that enhances performance under sparse labeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a CryoET-specific DropBlock augmentation strategy to further boost performance. Extensive evaluations on the large-scale CZII dataset show that our approach improves F1 by 10% over supervised baselines, underscoring the promise of semi-supervised learning for leveraging unlabeled CryoET data.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.22454.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22454",
    "published": "2025-10-25T23:09:22Z",
    "updated": "2026-01-13T18:57:24Z",
    "comment": "IEEE International Symposium on Biomedical Imaging (ISBI) 2026",
    "light_analysis": {
      "overview": "本文提出了SemiETPicker，一个快速且标签高效的半监督框架，用于CryoET断层扫描中的粒子挑选，通过端到端检测模型和师生协同训练机制，显著提升性能。",
      "motivation": "CryoET结合子体积平均是解析细胞内蛋白质分子结构的唯一方法，但粒子挑选任务因依赖耗时的手动标签而成为主要瓶颈。现有监督方法需要大量标注数据，导致大量未标记断层扫描数据未充分利用，限制了蛋白质结构解析的效率和自动化水平。本研究旨在通过半监督学习解决标签稀缺问题，提高数据利用效率，从而加速生物图像分析流程。",
      "method": "该方法包含两个核心组件：一是端到端热图监督检测模型，灵感来自关键点检测技术，用于精确定位和分类CryoET体积中的蛋白质粒子；二是师生协同训练机制，在稀疏标签条件下利用未标记数据提升模型性能。此外，引入多视角伪标签化和CryoET特定的DropBlock增强策略，以增强数据多样性和防止过拟合。模型在大规模CZII数据集上进行训练和验证，实现标签高效学习。",
      "result": "在大型CZII数据集上的评估显示，该方法相比监督基线在F1分数上提升了10%，显著提高了粒子挑选的精度和效率。这一改进验证了半监督学习在利用未标记CryoET数据方面的有效性，证明了框架在减少手动标注需求的同时，保持高准确性，为自动化蛋白质结构解析提供了可靠解决方案。",
      "conclusion": "本研究的主要贡献是开发了SemiETPicker框架，解决了CryoET粒子挑选中的标签稀缺问题，提升了数据处理速度和精度。其学术价值在于展示了半监督学习在生物图像分析领域的应用潜力，实际意义在于加速蛋白质结构解析并降低标注成本。未来工作可进一步优化模型架构或扩展到其他CryoET数据集，以探索更广泛的应用场景。",
      "tags": [
        "Semi-Supervised Learning",
        "Particle Picking",
        "CryoET",
        "Teacher-Student Co-training",
        "DropBlock Augmentation"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:16.965908Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22268",
    "title": "GSAlign: Geometric and Semantic Alignment Network for Aerial-Ground Person Re-Identification",
    "authors": [
      "Qiao Li",
      "Jie Li",
      "Yukang Zhang",
      "Lei Tan",
      "Jing Chen",
      "Jiayi Ji"
    ],
    "abstract": "Aerial-Ground person re-identification (AG-ReID) is an emerging yet challenging task that aims to match pedestrian images captured from drastically different viewpoints, typically from unmanned aerial vehicles (UAVs) and ground-based surveillance cameras. The task poses significant challenges due to extreme viewpoint discrepancies, occlusions, and domain gaps between aerial and ground imagery. While prior works have made progress by learning cross-view representations, they remain limited in handling severe pose variations and spatial misalignment. To address these issues, we propose a Geometric and Semantic Alignment Network (GSAlign) tailored for AG-ReID. GSAlign introduces two key components to jointly tackle geometric distortion and semantic misalignment in aerial-ground matching: a Learnable Thin Plate Spline (LTPS) Module and a Dynamic Alignment Module (DAM). The LTPS module adaptively warps pedestrian features based on a set of learned keypoints, effectively compensating for geometric variations caused by extreme viewpoint changes. In parallel, the DAM estimates visibility-aware representation masks that highlight visible body regions at the semantic level, thereby alleviating the negative impact of occlusions and partial observations in cross-view correspondence. A comprehensive evaluation on CARGO with four matching protocols demonstrates the effectiveness of GSAlign, achieving significant improvements of +18.8\\% in mAP and +16.8\\% in Rank-1 accuracy over previous state-of-the-art methods on the aerial-ground setting.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.22268.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22268",
    "published": "2025-10-25T12:16:10Z",
    "updated": "2026-01-13T17:19:03Z",
    "comment": "Accepted by Neurips 2025",
    "light_analysis": {
      "overview": "提出GSAlign网络，通过几何和语义对齐方法显著提升航空-地面行人重识别的性能。",
      "motivation": "航空-地面行人重识别任务是新兴挑战，旨在匹配无人机和地面监控摄像头捕获的行人图像，面临极端视角差异、遮挡和域间隙等问题。现有方法在处理严重姿态变化和空间不对齐方面有限，影响实际应用效果，因此需要开发更有效的对齐技术来解决这些难题，以提升跨视角行人匹配的可靠性和鲁棒性。",
      "method": "GSAlign网络引入两个关键组件：可学习薄板样条模块和动态对齐模块。LTPS模块基于学习的关键点自适应扭曲行人特征以补偿几何变形，DAM模块估计可见性感知表示掩码在语义层面突出可见身体区域以减轻遮挡影响。该方法在CARGO数据集上评估，专注于协同处理几何和语义不对齐问题。",
      "result": "在CARGO数据集上进行的综合评估显示，GSAlign在mAP和Rank-1准确率上分别提升18.8%和16.8%，优于先前最佳方法。这表明该方法能有效处理极端视角变化和遮挡带来的挑战，验证了其在航空-地面匹配任务中的高性能和实用性。",
      "conclusion": "GSAlign通过几何和语义对齐显著改进了航空-地面行人重识别的性能，其贡献在于提出一种新颖网络架构来应对视角差异和遮挡。这具有重要学术价值，推动了跨模态行人识别领域发展，并为监控安全应用提供技术支持。未来工作可扩展到其他复杂场景或更多数据集。",
      "tags": [
        "Person Re-Identification",
        "Learnable Thin Plate Spline",
        "Dynamic Alignment",
        "Feature Warping",
        "Visibility-aware Masks"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:11.151637Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.21060",
    "title": "On the Sample Complexity of Differentially Private Policy Optimization",
    "authors": [
      "Yi He",
      "Xingyu Zhou"
    ],
    "abstract": "Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings, via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.21060.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21060",
    "published": "2025-10-24T00:21:38Z",
    "updated": "2026-01-13T04:00:52Z",
    "comment": "Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "论文首次理论性地研究了差分隐私策略优化的样本复杂性，并形式化了适用于PO的DP定义。",
      "motivation": "策略优化（PO）作为强化学习基石，在机器人、医疗保健等敏感领域广泛应用，但部署中涉及隐私数据，引发严重隐私泄露风险。现有PO研究多聚焦算法性能优化，而对隐私保护下的样本复杂性理论分析不足，导致在安全环境中实施受限。本研究旨在填补这一空白，通过探讨差分隐私约束下的PO样本复杂性，为隐私敏感应用提供理论支撑。",
      "method": "论文首先为策略优化量身定制了差分隐私定义，解决了on-policy学习动态和隐私单位界定的挑战。然后，通过构建统一分析框架，系统评估了策略梯度、自然策略梯度等主流PO算法在DP约束下的样本复杂性，涵盖多种设置，关键创新在于将隐私考量融入理论分析中。",
      "result": "理论分析表明，在差分隐私约束下，隐私相关成本通常表现为样本复杂性的低阶项，对整体学习效率影响较小。研究还揭示了私有PO设置中一些微妙但重要的观察，为算法设计提供了理论基础，但摘要未明确说明具体性能指标或基线对比数据。",
      "conclusion": "本研究贡献了适用于策略优化的差分隐私定义和统一分析框架，深化了对隐私保护强化学习的理论理解，具有学术价值。实际应用中，为开发高效隐私PO算法提供指导，未来工作可扩展至更复杂场景或实证验证。",
      "tags": [
        "Differential Privacy",
        "Policy Optimization",
        "Sample Complexity",
        "Reinforcement Learning",
        "Policy Gradient"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:16.625434Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.17108",
    "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI",
    "authors": [
      "Yoonjin Lee",
      "Munhee Kim",
      "Hanbi Choi",
      "Juhyeon Park",
      "Seungho Lyoo",
      "Woojin Park"
    ],
    "abstract": "This study investigated LLM-based automation for analyzing non-financial data in corporate credit evaluation. Two systems were developed and compared: a Single-Agent System (SAS), in which one LLM agent infers favorable and adverse repayment signals, and a Popperian Multi-agent Debate System (PMADS), which structures the dual-perspective analysis as adversarial argumentation under the Karl Popper Debate protocol. Evaluation addressed three fronts: (i) work productivity compared with human experts; (ii) perceived report quality and usability, rated by credit risk professionals for system-generated reports; and (iii) reasoning characteristics quantified via reasoning-tree analysis. Both systems drastically reduced task completion time relative to human experts. Professionals rated SAS reports as adequate, while PMADS reports exceeded neutral benchmarks and scored significantly higher in explanatory adequacy, practical applicability, and usability. Reasoning-tree analysis showed PMADS produced deeper, more elaborated structures, whereas SAS yielded single-layered trees. These findings suggest that structured multi-agent debate enhances analytical rigor and perceived usefulness, though at the cost of longer computation time. Overall, the results demonstrate that reasoning-centered automation represents a promising approach for developing useful AI systems in decision-critical financial contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.17108.pdf",
    "abs_url": "https://arxiv.org/abs/2510.17108",
    "published": "2025-10-20T02:50:03Z",
    "updated": "2026-01-13T13:09:33Z",
    "comment": "18 pages, 4 figures, 2 algorithms, 2 tables, 4 appendices",
    "light_analysis": {
      "overview": "本文提出结构化多代理辩论系统，通过模拟对抗性论证提升企业信用评估中基于LLM的推理质量与解释性。",
      "motivation": "企业信用评估依赖于非金融数据分析来预测还款能力，但现有AI系统在深度推理和解释性方面存在不足，可能导致评估结果不严谨或实用性低。本研究旨在通过引入结构化辩论机制，解决传统自动化方法在金融决策关键环境中分析深度不够的问题，以提高AI系统的可信度和实际应用价值。摘要未明确说明具体现有方法的缺陷，但强调了增强推理能力的必要性。",
      "method": "研究开发并比较了两种LLM系统：单代理系统（SAS）和波普尔式多代理辩论系统（PMADS）。PMADS基于Karl Popper辩论协议，将信用分析中的积极和消极信号结构化为对抗性论证，通过多代理交互模拟深度推理过程。关键创新在于融合辩论机制与LLM，以增强分析严谨性，使用推理树分析量化方法，评估系统在生成报告时的逻辑结构。数据集涉及企业非金融数据，但摘要未明确说明具体细节。",
      "result": "实验结果表明，SAS和PMADS都显著减少了任务完成时间，相比人类专家效率更高。信用风险专业人员对系统生成报告的评分显示，SAS报告被视为合格，而PMADS报告在解释充分性、实际应用性和可用性方面得分显著高于中性基准，并超越SAS。推理树分析量化显示，PMADS产生更深层、更详细的推理结构（多层级），而SAS仅生成单层树，这表明结构化辩论增强了分析深度和严谨性。",
      "conclusion": "研究证实结构化多代理辩论能有效提高AI系统在企业信用评估中的分析严谨性和感知有用性，尽管需要更长的计算时间。主要贡献在于将辩论协议集成到多代理LLM中，为金融决策关键环境中的推理中心自动化提供了新方向。学术价值在于探索了AI增强推理的方法，实际应用价值在于开发更可靠的金融AI工具。未来工作可优化计算效率或扩展到其他金融领域。",
      "tags": [
        "Large Language Model",
        "Multi-agent Systems",
        "Karl Popper Debate",
        "Reasoning-tree Analysis",
        "Corporate Credit Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:14.567381Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.14420",
    "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following",
    "authors": [
      "Qingyu Ren",
      "Qianyu He",
      "Bowei Zhang",
      "Jie Zeng",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Weikang Zhou",
      "Zeye Sun",
      "Fei Yu"
    ],
    "abstract": "Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.14420.pdf",
    "abs_url": "https://arxiv.org/abs/2510.14420",
    "published": "2025-10-16T08:24:44Z",
    "updated": "2026-01-13T02:52:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种无标签的自监督强化学习框架，通过直接从指令中提取奖励信号和生成伪标签，有效解决了语言模型在多约束指令跟随中的挑战。",
      "motivation": "语言模型在实际应用中常难以遵循多约束指令，这对自动化代理和人机交互等场景至关重要。现有强化学习方法依赖外部监督，既成本高昂又难以扩展，同时多约束任务导致奖励信号稀疏，降低了学习效率和模型性能。因此，开发一种能够减少外部监督依赖并有效处理稀疏奖励信号的新方法，具有重要的研究意义和实际价值，以提升语言模型的指令跟随能力。",
      "method": "该方法提出一个无标签的自监督强化学习框架，核心是直接从指令中推导奖励信号，并通过生成伪标签来训练奖励模型，从而消除对外部监督的依赖。创新点包括引入约束分解策略，将多约束指令分解为更简单的子约束，并采用高效的约束级二元分类来处理每个子约束，以应对稀疏奖励问题，同时保持计算效率。这种方法利用指令本身作为监督源，实现了端到端的奖励学习，简化了训练流程。",
      "result": "实验结果表明，该方法在3个领域内和5个领域外数据集上均表现出优异的泛化能力，实现了显著的性能改进，特别是在具有挑战性的代理性和多轮指令跟随任务中表现出强劲提升。摘要未明确说明具体指标如准确率，但通过跨数据集的广泛测试，验证了该方法在多种场景下的有效性，暗示了其优于现有基线方法的性能，并展示了在现实应用中的潜力。",
      "conclusion": "本文的主要贡献是开发了一个无标签的自监督强化学习框架，有效解决了语言模型在多约束指令跟随中的问题，减少了对外部监督的依赖，提高了模型的自主性和适应性。该研究在学术上促进了自监督学习和强化学习的结合，拓宽了指令跟随领域的研究视野，实际应用中能提升语言模型的指令理解和执行能力，适用于多样化的任务场景。未来工作可探索框架在其他复杂任务中的泛化，或优化其处理大规模数据集的能力。",
      "tags": [
        "Self-supervised Reinforcement Learning",
        "Constraint Decomposition",
        "Binary Classification",
        "Instruction Following",
        "Reward Modeling"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:32.978298Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.14381",
    "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers",
    "authors": [
      "Andrew Zhao",
      "Reshmi Ghosh",
      "Vitor Carvalho",
      "Emily Lawton",
      "Keegan Hines",
      "Gao Huang",
      "Jack W. Stokes"
    ],
    "abstract": "Large language model (LLM) systems increasingly power everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on manually well-crafted prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to query poisoning alone: feedback-based attacks raise attack success rate (ASR) by up to ΔASR = 0.48. We introduce a simple fake reward attack that requires no access to the reward model and significantly increases vulnerability. We also propose a lightweight highlighting defense that reduces the fake reward ΔASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.14381.pdf",
    "abs_url": "https://arxiv.org/abs/2510.14381",
    "published": "2025-10-16T07:28:54Z",
    "updated": "2026-01-13T15:36:07Z",
    "comment": "Proceedings of the 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026)",
    "light_analysis": {
      "overview": "本论文首次系统性分析基于大型语言模型的提示优化器中毒风险，揭示了反馈攻击的高脆弱性，并提出了有效的防御方法。",
      "motivation": "随着大型语言模型系统在聊天机器人、计算机助手和自主机器人等日常AI应用中的普及，性能往往依赖于精心设计的提示。基于LLM的提示优化器通过迭代优化减少人工努力，但其安全风险尚未得到充分研究。现有方法主要关注查询中毒，而忽视了反馈操纵带来的更高风险，这在实际应用中可能导致系统性能下降或恶意攻击。因此，研究优化阶段的中毒漏洞对于保障LLM应用的安全性至关重要，本文旨在填补这一空白。",
      "method": "本研究采用HarmBench数据集进行系统性分析，评估基于LLM的提示优化器的中毒风险。核心方法包括引入一种简单的假奖励攻击，该攻击无需访问奖励模型，通过操纵反馈来提升攻击成功率。同时，提出了一种轻量级高亮防御方法，通过高亮关键部分来减轻攻击影响。创新点在于首次系统化探索反馈操纵攻击，攻击方法简单易实施，防御方法设计巧妙，不损害优化器的实用性，为安全研究提供了新视角。",
      "result": "实验结果显示，反馈攻击显著增加了系统的脆弱性，攻击成功率提升高达ΔASR = 0.48。具体而言，假奖励攻击进一步放大了漏洞，而提出的高亮防御方法有效降低了攻击影响，将假奖励攻击的ΔASR从0.23减少到0.07，同时保持了优化器的效用。与基线方法（仅查询中毒）相比，反馈攻击更具威胁，这突出了优化阶段安全措施的必要性。",
      "conclusion": "本论文的主要贡献是首次系统性揭示了基于LLM的提示优化管道作为一类重要攻击表面，提出了具体的攻击和防御策略。学术上，推动了LLM安全研究，特别是优化框架的脆弱性分析；实际上，激励了在反馈渠道和优化框架中实施更强安全保障。研究局限性摘要未明确说明，但未来工作可扩展到更广泛的安全评估和防御机制开发，以应对潜在的攻击变体。",
      "tags": [
        "Large Language Model",
        "Prompt Optimization",
        "Security Vulnerabilities",
        "Feedback-based Attacks",
        "Defense Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:55.421904Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.06780",
    "title": "Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness",
    "authors": [
      "Luca Giordano",
      "Simon Razniewski"
    ],
    "abstract": "Large Language Models (LLMs) encode substantial factual knowledge, yet measuring and systematizing this knowledge remains challenging. Converting it into structured format, for example through recursive extraction approaches such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key open questions include whether such extraction can terminate, whether its outputs are reproducible, and how robust they are to variations. We systematically study LLM knowledge materialization using miniGPTKBs (domain-specific, tractable subcrawls), analyzing termination, reproducibility, and robustness across three categories of metrics: yield, lexical similarity, and semantic similarity. We experiment with four variations (seed, language, randomness, model) and three illustrative domains (from history, entertainment, and finance). Our findings show (i) high termination rates, though model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies by perturbation type: high for seeds and temperature, lower for languages and models. These results suggest that LLM knowledge materialization can reliably surface core knowledge, while also revealing important limitations.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.06780.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06780",
    "published": "2025-10-08T09:03:58Z",
    "updated": "2026-01-13T10:18:12Z",
    "comment": "Accepted and published in Findings of EACL 2026",
    "light_analysis": {
      "overview": "本文系统研究大型语言模型知识物化的终止性、可重复性和鲁棒性，揭示其在提取核心知识时的可靠性及局限性。",
      "motivation": "大型语言模型编码了大量事实知识，但如何测量和系统化这些知识仍具挑战。将知识转换为结构化格式，例如通过递归提取方法如GPTKB，尚未充分探索。关键问题包括提取过程是否能终止、输出是否可重复以及在不同条件下的鲁棒性，这些问题影响知识的可靠提取和实际应用，现有方法缺乏对这些基础方面的系统评估，导致知识提取的可靠性未得到验证。",
      "method": "研究方法采用miniGPTKBs，即领域特定、可处理的子爬取，来系统分析LLM知识物化。实验设计包括分析三类度量标准：产出、词汇相似度和语义相似度，通过四个变体（种子、语言、随机性、模型）和三个示例领域（历史、娱乐、金融）进行测试。关键创新在于通过可控实验评估终止性、可重复性和鲁棒性，使用具体度量方法提供系统化分析框架，以揭示知识提取的基础性质。",
      "result": "主要实验结果显示：提取过程具有高终止率，但终止情况依赖于所使用的模型；输出的可重复性结果混合，部分条件可重复而部分不可重复；鲁棒性因扰动类型而异，对种子和温度的扰动表现出高鲁棒性，但对语言和模型的扰动鲁棒性较低。这些发现基于三类度量的分析，表明知识提取在不同条件下的表现存在差异性，摘要未明确说明具体数值。",
      "conclusion": "本研究系统评估了LLM知识物化的基础方面，表明该方法能够可靠地提取核心知识，同时揭示了重要局限性，如模型依赖性和可重复性问题。这为未来研究提供了方向，例如改进提取方法的稳定性和跨模型鲁棒性。研究具有学术价值，促进了对LLM知识表示的深入理解，并有助于开发更可靠的知识提取工具，为实际应用奠定基础。",
      "tags": [
        "Large Language Model",
        "Knowledge Materialization",
        "Termination",
        "Reproducibility",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:12.417638Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.04230",
    "title": "Pushing on Multilingual Reasoning Models with Language-Mixed Chain-of-Thought",
    "authors": [
      "Guijin Son",
      "Donghun Yang",
      "Hitesh Laxmichand Patel",
      "Amit Agarwal",
      "Hyunwoo Ko",
      "Chanuk Lim",
      "Srikant Panda",
      "Minhyuk Kim",
      "Nikunj Drolia",
      "Dasol Choi",
      "Kyong-Ha Lee",
      "Youngjae Yu"
    ],
    "abstract": "Recent frontier models employ long chain-of-thought reasoning to explore solution spaces in context and achieve stonger performance. While many works study distillation to build smaller yet capable models, most focus on English and little is known about language-specific reasoning. To bridge this gap, we first introduct **Language-Mixed CoT**, a reasoning schema that switches between English and a target language, using English as an anchor to excel in reasoning while minimizing translation artificats. As a Korean case study, we curate **Yi-Sang**: 5.79M native-Korean prompts from web Q&A, exams, STEM, and code; 3.7M long reasoning traces generated from Qwen3-32B; and a targeted 260k high-yield subset. We train ninve models (4B-35B) across six families (Qwen2.5, Llama-3.1, Gemma-3, etc). Our best model, **KO-REAson-35B**, achieves state-of-the-art performance, with the highest overall average score (64.0 \\pm 25), ranking first on 5/9 benchmarks and second on the remainder. Samller and mid-sized models also benefit substantially, with an average improvement of +18.6 points across teh evaluated nine benchmarks. Ablations show **Language-Mixed CoT** is more effective than monolingual CoT, also resulting in cross-lingual and mult-modal performance gains. We release our data-curation pipeline, evaluation system, datasets, and models to advance research on language-specific reasoning. Data and model collection: https://huggingface.co/KOREAson.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.04230.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04230",
    "published": "2025-10-05T14:39:41Z",
    "updated": "2026-01-13T05:48:55Z",
    "comment": "Work in Progress",
    "light_analysis": {
      "overview": "论文提出语言混合思维链方法，在韩语推理任务上实现最佳性能，推动了多语言推理模型的发展。",
      "motivation": "前沿模型使用长思维链推理提升性能，但现有工作大多专注于英语任务，通过蒸馏构建小模型，忽略语言特定推理研究。这导致非英语语言的推理能力不足，限制了跨语言应用效果。现有方法多为单语言思维链，容易引入翻译人为因素，影响推理质量。因此，本研究旨在弥合英语与非英语语言之间的性能差距，探索有效方法来减少翻译偏差并提升多语言推理能力。",
      "method": "本研究提出语言混合思维链方法，在推理过程中在英语和目标语言之间切换，利用英语作为锚点优化推理并减少翻译偏差。以韩语为例，策划了Yi-Sang数据集，包含5.79M本土韩语提示、3.7M长推理轨迹和260k高收益子集。使用六个模型家族（如Qwen2.5、Llama-3.1、Gemma-3等）训练了九个不同规模模型（4B到35B），以评估方法的有效性和通用性。",
      "result": "最佳模型KO-REAson-35B在九个基准测试中取得最高整体平均分64.0（标准差25），其中5个任务排名第一，其余排名第二。小型和中型模型也显著受益，平均改进18.6点，证明方法对多规模模型均有效。消融研究表明，语言混合思维链比单语言版本更有效，并带来跨语言和多模态性能提升，展示了方法的鲁棒性和扩展性。",
      "conclusion": "本研究证明了语言混合思维链方法有效提升多语言推理模型性能，尤其在韩语任务上达到最先进水平。其学术价值在于填补语言特定推理的空白，推动相关研究发展；实际应用价值是支持跨语言AI系统构建。通过释放数据策划管道、评估系统和模型，促进了社区资源共享和未来探索。未来工作可扩展到更多语言和模态，以克服当前以韩语为主的局限性。",
      "tags": [
        "Language-Mixed Chain-of-Thought",
        "Multilingual Reasoning",
        "Data Curation",
        "Model Training",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:13.242044Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.24613",
    "title": "HiKE: Hierarchical Evaluation Framework for Korean-English Code-Switching Speech Recognition",
    "authors": [
      "Gio Paik",
      "Yongbeom Kim",
      "Soungmin Lee",
      "Sangmin Ahn",
      "Chanwoo Kim"
    ],
    "abstract": "Despite advances in multilingual automatic speech recognition (ASR), code-switching (CS), the mixing of languages within an utterance common in daily speech, remains a severely underexplored challenge. In this paper, we introduce HiKE: the Hierarchical Korean-English code-switching benchmark, the first globally accessible non-synthetic evaluation framework for Korean-English CS, aiming to provide a means for the precise evaluation of multilingual ASR models and to foster research in the field. The proposed framework not only consists of high-quality, natural CS data across various topics, but also provides meticulous loanword labels and a hierarchical CS-level labeling scheme (word, phrase, and sentence) that together enable a systematic evaluation of a model's ability to handle each distinct level of code-switching. Through evaluations of diverse multilingual ASR models and fine-tuning experiments, this paper demonstrates that although most multilingual ASR models initially exhibit inadequate CS-ASR performance, this capability can be enabled through fine-tuning with synthetic CS data. HiKE is available at https://github.com/ThetaOne-AI/HiKE.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.24613.pdf",
    "abs_url": "https://arxiv.org/abs/2509.24613",
    "published": "2025-09-29T11:18:13Z",
    "updated": "2026-01-13T14:38:47Z",
    "comment": "EACL Findings 2026",
    "light_analysis": {
      "overview": "论文提出了HiKE框架，首个非合成评估基准用于韩英代码切换语音识别，以系统评估多语言ASR模型的处理能力。",
      "motivation": "尽管多语言自动语音识别技术有所进步，但代码切换——在话语中混合语言的现象在日常语音中非常普遍——仍然是一个严重未被充分探索的挑战。现有方法缺乏针对代码切换的精确评估框架，导致多语言ASR模型在处理混合语言时性能不足。因此，本研究旨在填补这一空白，提供一个专门的评估工具来促进相关研究和实际应用。",
      "method": "论文的核心方法是开发了HiKE框架，这是一个包含高质量、自然韩英代码切换数据的数据集。该框架提供了细致的借款标签和一个分层的代码切换级别标签系统（单词、短语和句子），使能系统评估模型在不同级别上的表现。此外，通过评估多种多语言ASR模型并进行微调实验，展示了方法的有效性。关键创新在于非合成数据和分层标签的设计，以支持精确的模型分析。",
      "result": "通过实验评估，论文发现大多数多语言ASR模型在初始阶段对代码切换语音识别的性能不足。然而，通过使用合成代码切换数据进行微调，可以显著提升模型在这方面的能力，摘要未明确说明具体准确率提升数据。实验表明微调策略有效，强调了数据增强在改进CS-ASR性能中的重要性，并与基线方法相比展示了改进潜力。",
      "conclusion": "本研究的主要贡献是提出了HiKE框架，为韩英代码切换语音识别提供了首个非合成评估基准。其学术价值在于促进了代码切换领域的研究，通过分层标签系统实现了更精确的模型评估。实际应用上，该框架有助于开发更鲁棒的多语言ASR系统。未来工作可扩展至其他语言对或集成更多评估维度，以解决潜在局限性如数据多样性不足。",
      "tags": [
        "Code-Switching",
        "Multilingual ASR",
        "Evaluation Framework",
        "Hierarchical Labeling",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:37:58.089006Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.02369",
    "title": "AutoContext: Instance-Level Context Learning for LLM Agents",
    "authors": [
      "Kuntai Cai",
      "Juncheng Liu",
      "Xianglin Yang",
      "Zhaojie Niu",
      "Xiaokui Xiao",
      "Xing Chen"
    ],
    "abstract": "Current LLM agents typically lack instance-level context, which comprises concrete facts such as environment structure, system configurations, and local mechanics. Consequently, existing methods are forced to intertwine exploration with task execution. This coupling leads to redundant interactions and fragile decision-making, as agents must repeatedly rediscover the same information for every new task. To address this, we introduce AutoContext, a method that decouples exploration from task solving. AutoContext performs a systematic, one-off exploration to construct a reusable knowledge graph for each environment instance. This structured context allows off-the-shelf agents to access necessary facts directly, eliminating redundant exploration. Experiments across TextWorld, ALFWorld, Crafter, and InterCode-Bash demonstrate substantial gains: for example, the success rate of a ReAct agent on TextWorld improves from 37% to 95%, highlighting the critical role of structured instance context in efficient agentic systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.02369.pdf",
    "abs_url": "https://arxiv.org/abs/2510.02369",
    "published": "2025-09-29T05:38:51Z",
    "updated": "2026-01-13T06:31:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "AutoContext通过一次性探索构建可重用知识图，解耦LLM代理的探索与任务执行，显著提升任务成功率。",
      "motivation": "当前LLM代理在执行任务时缺乏针对具体环境实例的结构化上下文，如环境结构、系统配置等。这导致代理必须将探索与任务执行耦合，每次新任务都重复发现相同信息，产生冗余交互和脆弱决策。现有方法未能有效分离探索与执行，降低了代理效率和可靠性，因此亟需一种方法来构建可重用上下文以解决此问题。",
      "method": "AutoContext方法的核心是进行系统性一次性探索，为每个环境实例构建可重用知识图，存储结构化的实例级上下文。通过知识图，标准LLM代理（如ReAct）能直接访问必要事实，实现探索与任务解决的解耦。实验中使用TextWorld、ALFWorld、Crafter和InterCode-Bash等数据集验证方法，允许代理专注于任务执行，无需重复探索。",
      "result": "在TextWorld、ALFWorld、Crafter和InterCode-Bash等环境中的实验显示，AutoContext显著提升代理性能。例如，ReAct代理在TextWorld上的任务成功率从37%提高到95%，其他环境也观察到类似改进。这些结果证明了结构化上下文对高效代理系统的关键作用，并证实了方法的普适性和有效性。",
      "conclusion": "AutoContext的主要贡献是提出了一种实例级上下文学习方法，通过构建知识图解耦探索与任务执行，减少冗余交互并提高决策可靠性。学术上，为LLM代理的上下文学习提供了新框架；应用上，能提升代理在各种任务中的成功率和效率。摘要未明确说明局限性，但未来工作可探索扩展到动态环境或集成更多上下文类型。",
      "tags": [
        "LLM Agents",
        "Knowledge Graph",
        "Instance-Level Context",
        "Exploration-Decoupling",
        "ReAct Agent"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:14.851002Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.24253",
    "title": "MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation",
    "authors": [
      "Yuelyu Ji",
      "Wuwei Lan",
      "Patrick NG"
    ],
    "abstract": "Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances question answering by integrating visual and textual evidence. Yet, current evaluations fail to systematically account for query difficulty and ambiguity. We propose MRAG-Suite, a diagnostic evaluation platform integrating diverse multimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce difficulty-based and ambiguity-aware filtering strategies, alongside MM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate substantial accuracy reductions under difficult and ambiguous queries, highlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses these issues, guiding future improvements in Visual RAG systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.24253.pdf",
    "abs_url": "https://arxiv.org/abs/2509.24253",
    "published": "2025-09-29T03:55:28Z",
    "updated": "2026-01-13T13:26:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 MRAG-Suite 诊断评估平台，通过集成多模态基准和引入基于难度与歧义的过滤策略及诊断工具，以解决视觉检索增强生成系统中的查询难度和歧义性问题。",
      "motivation": "视觉检索增强生成通过整合视觉和文本证据提升了问答系统性能，但现有评估方法未能系统考虑查询的难度和歧义性，导致无法全面诊断系统在处理复杂或模糊查询时的弱点，如幻觉现象。这限制了评估的有效性，使模型在实际应用中可能不可靠，因此需要开发能针对性分析查询特性的诊断平台以促进技术改进。",
      "method": "研究提出 MRAG-Suite 平台，整合了多个多模态基准数据集，包括 WebQA、Chart-RAG、Visual-RAG 和 MRAG-Bench。关键创新是引入基于难度的过滤策略和歧义感知的过滤策略，对查询进行分类；同时开发 MM-RAGChecker 诊断工具，在 claim-level 进行细粒度分析。该方法通过策略筛选查询并使用工具评估系统输出，提供系统化的诊断框架。",
      "result": "实验结果表明，在困难和歧义查询下，视觉 RAG 系统的准确性显著降低，凸显了幻觉问题的普遍性。摘要未明确说明具体性能指标，但指出这些查询条件下的性能下降。通过 MRAG-Suite 和 MM-RAGChecker 的诊断，能有效识别系统弱点，相比传统评估方法提供了更深入的性能分析，强调了考虑查询特性的重要性。",
      "conclusion": "本研究的主要贡献是 MRAG-Suite 诊断评估平台，通过集成基准和引入过滤策略及诊断工具，系统解决了查询难度和歧义性问题，具有学术和实际价值，可指导视觉 RAG 系统优化以提升鲁棒性。未来工作可扩展平台到更多应用领域或自动化诊断过程，摘要未明确说明局限性，但需进一步验证其普适性。",
      "tags": [
        "Multimodal Retrieval-Augmented Generation",
        "Diagnostic Evaluation",
        "Query Difficulty",
        "Ambiguity Filtering",
        "Hallucination Detection"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:42.045703Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14507",
    "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction",
    "authors": [
      "Jian Chen",
      "Zhenyan Chen",
      "Xuming Hu",
      "Peilin Zhou",
      "Yining Hua",
      "Han Fang",
      "Cissy Hing Yee Choy",
      "Xinmei Ke",
      "Jingfeng Luo",
      "Zixuan Yuan"
    ],
    "abstract": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that simplifies database access for non-technical users by converting natural language queries into SQL commands. Recent advancements, particularly those integrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) reasoning, have made significant strides in enhancing NL2SQL performance. However, challenges such as inaccurate task decomposition and keyword extraction by LLMs remain major bottlenecks, often leading to errors in SQL generation. While existing datasets aim to mitigate these issues by fine-tuning models, they struggle with over-fragmentation of tasks and lack of domain-specific keyword annotations, limiting their effectiveness. To address these limitations, we present DeKeyNLU, a novel dataset which contains 1,500 meticulously annotated QA pairs aimed at refining task decomposition and enhancing keyword extraction precision for the RAG pipeline. Fine-tuned with DeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three distinct modules for user question understanding, entity retrieval, and generation to improve SQL generation accuracy. We benchmarked multiple model configurations within DeKeySQL RAG pipeline. Experimental results demonstrate that fine-tuning with DeKeyNLU significantly improves SQL generation accuracy on both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.14507.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14507",
    "published": "2025-09-18T00:47:56Z",
    "updated": "2026-01-13T11:51:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出DeKeyNLU数据集和DeKeySQL管道，通过优化任务分解和关键词提取来提升自然语言到SQL生成的准确性。",
      "motivation": "自然语言到SQL（NL2SQL）简化数据库访问，但现有方法如检索增强生成（RAG）和思维链（CoT）推理在任务分解和关键词提取上不准确，导致SQL生成错误，成为主要瓶颈。现有数据集虽尝试缓解问题，但存在任务过度碎片化和缺乏领域特定关键词注释的不足，限制了其有效性，因此需开发更精细的数据集和改进方法来提升性能。",
      "method": "论文提出DeKeyNLU数据集，包含1,500个精心标注的问答对，旨在精炼任务分解和增强关键词提取精度。基于此数据集微调模型，开发了DeKeySQL管道，一个基于RAG的NL2SQL系统，包含三个模块：用户问题理解、实体检索和SQL生成，以结构化方式改进SQL生成准确性。该方法通过优化RAG管道的配置来提升性能。",
      "result": "实验结果显示，使用DeKeyNLU微调显著提高了SQL生成准确性。在BIRD开发数据集上，准确率从62.31%提升至69.10%；在Spider开发数据集上，从84.2%提升至88.7%。这些改进表明方法在多个基准测试中的有效性，但摘要未明确说明具体基线对比的细节。",
      "conclusion": "论文的主要贡献是提出了DeKeyNLU数据集和DeKeySQL方法，通过改进任务分解和关键词提取来增强NL2SQL性能，具有重要学术价值，推动了NL2SQL领域的数据集构建和模型优化，实际应用上能提高数据库访问效率。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Natural Language to SQL (NL2SQL)",
        "Retrieval-Augmented Generation (RAG)",
        "Task Decomposition",
        "Keyword Extraction",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:20.910168Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.07309",
    "title": "PIE: Performance Interval Estimation for Free-Form Generation Tasks",
    "authors": [
      "Chi-Yang Hsu",
      "Alexander Braylan",
      "Yiheng Su",
      "Matthew Lease",
      "Omar Alonso"
    ],
    "abstract": "Confidence estimation infers a probability for whether each model output is correct or not. While predicting such binary correctness is sensible for tasks with exact answers, free-form generation tasks are often more nuanced, with output quality being both fine-grained and multi-faceted. We thus propose Performance Interval Estimation (PIE) to predict both: 1) point estimates for any arbitrary set of continuous-valued evaluation metrics; and 2) calibrated uncertainty intervals around these point estimates. We then compare two approaches: LLM-as-judge vs. classic regression with confidence estimation features. Evaluation over 11 datasets spans summarization, translation, code generation, function-calling, and question answering. Regression is seen to achieve both: i) lower error point estimates of metric scores; and ii) well-calibrated uncertainty intervals. To support reproduction and follow-on work, we share our data and code.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.07309.pdf",
    "abs_url": "https://arxiv.org/abs/2509.07309",
    "published": "2025-09-09T00:59:34Z",
    "updated": "2026-01-13T17:15:04Z",
    "comment": "10 pages",
    "light_analysis": {
      "overview": "PIE方法通过预测点估计和校准不确定性区间，改进了自由形式生成任务的性能评估。",
      "motivation": "自由形式生成任务（如摘要、翻译）的输出质量具有细微差异和多面性，而现有置信度估计方法通常只预测二元正确性，不足以全面评估这类任务。因此，需要开发能够预测连续值评估指标及其不确定性的方法，以提供更细粒度和可靠的性能估计，解决评估不准确的问题。",
      "method": "本研究提出性能区间估计（PIE）方法，用于预测任意连续值评估指标的点估计及其校准不确定性区间。比较了两种实现途径：LLM-as-judge（使用大语言模型作为评判者）和经典回归方法（结合置信度估计特征）。实验覆盖11个数据集，涵盖摘要、翻译、代码生成、函数调用和问答等多个任务领域，核心创新在于将性能评估扩展到连续指标并提供校准区间。",
      "result": "在11个数据集上的评估表明，经典回归方法相较于LLM-as-judge，在点估计上表现出更低的误差，并提供了良好校准的不确定性区间。回归方法不仅在性能评估的准确性上更优，其不确定性区间也有助于增强结果的可靠性，为模型性能分析提供了更全面的视角。",
      "conclusion": "PIE方法的主要贡献在于为自由形式生成任务引入了性能点估计和不确定性区间预测，弥补了传统置信度估计的不足。学术上，该研究扩展了性能评估的理论框架，实际应用则能支持模型调试和优化。通过分享数据和代码，促进了研究的可复现性和未来工作，如进一步探索其他评估指标或集成更多任务类型。",
      "tags": [
        "Performance Interval Estimation",
        "Confidence Estimation",
        "LLM-as-judge",
        "Regression",
        "Free-Form Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:17.489945Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.03262",
    "title": "PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges With a Geometry-Aware 3DETR",
    "authors": [
      "Fabio F. Oberweger",
      "Michael Schwingshackl",
      "Vanessa Staderini"
    ],
    "abstract": "We present PI3DETR, an end-to-end framework that directly predicts 3D parametric curve instances from raw point clouds, avoiding the intermediate representations and multi-stage processing common in prior work. Extending 3DETR, our model introduces a geometry-aware matching strategy and specialized loss functions that enable unified detection of differently parameterized curve types, including cubic Bézier curves, line segments, circles, and arcs, in a single forward pass. Optional post-processing steps further refine predictions without adding complexity. This streamlined design improves robustness to noise and varying sampling densities, addressing critical challenges in real world LiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABC dataset and generalizes effectively to real sensor data, offering a simple yet powerful solution for 3D edge and curve estimation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.03262.pdf",
    "abs_url": "https://arxiv.org/abs/2509.03262",
    "published": "2025-09-03T12:24:25Z",
    "updated": "2026-01-13T15:12:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "PI3DETR 是一种端到端框架，通过几何感知匹配和专用损失函数直接从点云统一检测多种 3D 参数曲线实例。",
      "motivation": "该研究旨在解决 3D 点云中边缘和曲线检测的实际问题，这在 LiDAR 和 3D 传感应用中至关重要，如机器人导航和环境建模。现有方法通常依赖于中间表示和多阶段处理，导致算法复杂、计算效率低，且对噪声和不同采样密度缺乏鲁棒性，限制了在真实世界场景中的泛化能力。因此，开发一种简洁、高效的端到端解决方案以提升检测精度和适应性成为研究重点。",
      "method": "PI3DETR 基于 3DETR 架构扩展，引入几何感知匹配策略和专用损失函数，在单次前向传播中统一检测立方贝塞尔曲线、线段、圆和弧等多种参数曲线。该方法直接从原始点云输入，避免中间表示，简化处理流程；关键创新点包括几何感知机制，增强了对曲线几何属性的理解，可选后处理步骤进一步优化预测而不增加模型复杂度，提升了整体效率和鲁棒性。",
      "result": "实验显示，PI3DETR 在 ABC 数据集上达到了新的 state-of-the-art 性能，摘要未明确说明具体准确率或指标，但与基线方法相比，其在噪声和不同采样密度下表现出更强的鲁棒性。此外，在真实传感器数据上的有效泛化验证了其实际应用价值，表明该方法能可靠地估计 3D 曲线，适用于多样化场景。",
      "conclusion": "PI3DETR 的主要贡献是提出了一种简单而强大的端到端框架，简化了 3D 参数曲线检测流程。学术上，扩展了 3DETR 在实例检测中的应用，并引入了几何感知机制；实际上，提高了对真实世界挑战的适应性，如 LiDAR 数据中的噪声，为自动化 3D 分析提供有效工具。潜在局限性可能包括对更多曲线类型的支持，未来工作可优化计算效率或扩展应用范围。",
      "tags": [
        "3D Point Clouds",
        "Parametric Curves",
        "Instance Detection",
        "End-to-End Learning",
        "3DETR"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:49.807613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.17627",
    "title": "The Evolution of Thought: Tracking LLM Overthinking via Reasoning Dynamics Analysis",
    "authors": [
      "Zihao Wei",
      "Liang Pang",
      "Jiahao Liu",
      "Wenjie Shi",
      "Jingcheng Deng",
      "Shicheng Xu",
      "Zenghao Duan",
      "Fei Sun",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "abstract": "Test-time scaling via explicit reasoning trajectories significantly boosts large language model (LLM) performance but often triggers overthinking. To explore this, we analyze reasoning through two lenses: Reasoning Length Dynamics, which reveals a compensatory trade-off between thinking and answer content length that eventually leads to thinking redundancy, and Reasoning Semantic Dynamics, which identifies semantic convergence and repetitive oscillations. These dynamics uncover an instance-specific Reasoning Completion Point (RCP), beyond which computation continues without further performance gain. Since the RCP varies across instances, we propose a Reasoning Completion Point Detector (RCPD), an inference-time early-exit method that identifies the RCP by monitoring the rank dynamics of termination tokens (e.g., </think>). Across AIME and GPQA benchmarks using Qwen3 and DeepSeek-R1, RCPD reduces token usage by up to 44% while preserving accuracy, offering a principled approach to efficient test-time scaling.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.17627.pdf",
    "abs_url": "https://arxiv.org/abs/2508.17627",
    "published": "2025-08-25T03:17:17Z",
    "updated": "2026-01-13T03:40:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出推理完成点检测器（RCPD），通过分析LLM推理动态识别过度思考点，实现高效推理并减少令牌使用。",
      "motivation": "研究动机源于测试时通过显式推理轨迹扩展大语言模型（LLM）性能时，常触发过度思考问题，浪费计算资源且无额外性能增益。现有方法缺乏对推理动态的系统分析，无法有效确定推理何时结束，导致计算冗余，因此需要一种原则性方法来解决这一效率瓶颈，提升LLM的实用性和成本效益。",
      "method": "方法通过两个视角分析推理动态：推理长度动态揭示思考与答案长度间的补偿权衡，最终导致冗余；推理语义动态识别语义收敛和重复振荡。这些分析用于发现实例特定的推理完成点（RCP），超过该点计算无增益。提出推理完成点检测器（RCPD），一种推理时提前退出方法，通过监控终止令牌（如</think>）的排名动态来识别RCP。使用AIME和GPQA基准数据集，结合Qwen3和DeepSeek-R1模型进行验证。",
      "result": "在AIME和GPQA基准测试中，使用Qwen3和DeepSeek-R1模型，RCPD能减少最多44%的令牌使用，同时保持模型准确性。这证明了该方法有效避免了过度思考，实现了高效推理，相比未使用RCPD的基线方法，显著提升了资源效率，摘要未明确说明具体基线对比细节。",
      "conclusion": "论文的主要贡献是提出RCPD方法，通过推理动态分析实现LLM的高效测试时扩展，减少计算成本。研究提供了分析推理过程的新视角，具有学术价值，并在实际应用中能降低资源消耗，促进LLM的部署。未来工作可扩展更多基准测试和模型适配，摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Reasoning Dynamics",
        "Early-exit Inference",
        "Token Efficiency",
        "Overthinking"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:29.769438Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.15793",
    "title": "Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data",
    "authors": [
      "Jiacheng Liu",
      "Mayi Xu",
      "Qiankun Pi",
      "Wenli Li",
      "Ming Zhong",
      "Yuanyuan Zhu",
      "Mengchi Liu",
      "Tieyun Qian"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including texts, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Yet it remains unclear whether such biases are systematic, which data-level factors drive them, and what internal mechanisms underlie their emergence.   In this paper, we present the first comprehensive study of format bias in LLMs through a three-stage empirical analysis. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage examines how key data-level factors influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its effectiveness. Our results show that format bias is consistent across model families, driven by information richness, structure quality, and representation type, and is closely associated with attention imbalance within the LLMs. Based on these investigations, we identify three future research directions to reduce format bias: enhancing data pre-processing through format repair and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.15793.pdf",
    "abs_url": "https://arxiv.org/abs/2508.15793",
    "published": "2025-08-13T01:09:02Z",
    "updated": "2026-01-13T09:26:14Z",
    "comment": "Accepted by AAAI 2026, camera ready version",
    "light_analysis": {
      "overview": "论文首次全面研究大语言模型中的格式偏见，揭示其机制并提出减轻偏见的未来研究方向。",
      "motivation": "研究动机源于大语言模型在处理文本、表格、信息框和知识图谱等异构格式数据时，可能存在系统性偏见，这可能导致数据集成不公正、推理错误和下游任务风险增加。现有方法未明确偏见是否系统性存在、哪些数据因素驱动偏见、以及其内在机制，因此本研究旨在填补这些知识空白，以促进更公平和稳健的异构数据处理系统的发展。",
      "method": "论文采用三阶段实证分析方法。第一阶段探索多种大语言模型中格式偏见的存在和方向。第二阶段检查关键数据级因素，如信息丰富度、结构质量和表示类型，对偏见的影响。第三阶段分析偏见在大语言模型注意力模式中的出现机制，并评估一种轻量级干预措施（如注意力重加权）的有效性。摘要未明确说明具体使用的模型和数据集。",
      "result": "研究结果表明，格式偏见在大语言模型家族中具有一致性，受信息丰富度、结构质量和表示类型等数据因素驱动，并与模型内部的注意力不平衡密切相关。这些发现揭示了偏见产生的关键机制，为后续干预提供了实证基础。摘要未提供具体性能指标数据，但强调了偏见的系统性存在和影响因素。",
      "conclusion": "论文的主要贡献是首次全面量化并分析了大语言模型中的格式偏见，揭示了偏见的驱动因素和注意力机制关联。其学术价值在于深化了对LLMs偏见现象的理解，实际应用价值在于支持设计更鲁棒和公平的异构数据处理系统。未来研究方向包括通过格式修复和规范化增强数据预处理、引入推理时干预如注意力重加权、以及开发格式平衡的训练语料。",
      "tags": [
        "Large Language Models",
        "Format Bias",
        "Attention Patterns",
        "Heterogeneous Data",
        "Empirical Analysis"
      ]
    },
    "analyzed_at": "2026-01-14T03:38:51.676866Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.08001",
    "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths",
    "authors": [
      "Rui Yao",
      "Qi Chai",
      "Jinhai Yao",
      "Siyuan Li",
      "Junhao Chen",
      "Qi Zhang",
      "Hao Wang"
    ],
    "abstract": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal Reserve, encodes implicit policy signals and strategic stances. The Federal Open Market Committee strategically employs Fedspeak as a communication tool to shape market expectations and influence both domestic and global economic conditions. As such, automatically parsing and interpreting Fedspeak presents a high-impact challenge, with significant implications for financial forecasting, algorithmic trading, and data-driven policy analysis. In this paper, we propose an LLM-based, uncertainty-aware framework for deciphering Fedspeak and classifying its underlying monetary policy stance. Technically, to enrich the semantic and contextual representation of Fedspeak texts, we incorporate domain-specific reasoning grounded in the monetary policy transmission mechanism. We further introduce a dynamic uncertainty decoding module to assess the confidence of model predictions, thereby enhancing both classification accuracy and model reliability. Experimental results demonstrate that our framework achieves state-of-the-art performance on the policy stance analysis task. Moreover, statistical analysis reveals a significant positive correlation between perceptual uncertainty and model error rates, validating the effectiveness of perceptual uncertainty as a diagnostic signal.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.08001.pdf",
    "abs_url": "https://arxiv.org/abs/2508.08001",
    "published": "2025-08-11T14:04:59Z",
    "updated": "2026-01-13T12:20:46Z",
    "comment": "Accepted by AAAI 2026 Oral",
    "light_analysis": {
      "overview": "本研究提出了一个基于大型语言模型的不确定性感知框架，结合货币政策传导机制来解读美联储语言，提高政策立场分类的准确性和可靠性。",
      "motivation": "研究动机是自动解析美联储使用的微妙语言（Fedspeak），因为它对金融预测、算法交易和数据分析有重要影响。Fedspeak是联邦公开市场委员会的关键沟通工具，用于塑造市场预期和影响经济条件，但现有方法可能难以处理其隐含性和不确定性，导致解读不准确，因此需要开发更先进的框架来应对这一挑战。摘要未明确说明具体现有方法不足之处，但可推断传统方法在语义丰富度和置信度评估方面存在局限。",
      "method": "论文提出了一个基于大型语言模型（LLM）的不确定性感知框架，核心方法包括结合货币政策传导机制的领域特定推理来丰富Fedspeak文本的语义和上下文表示，并引入动态不确定性解码模块以评估模型预测的置信度。关键创新点在于集成领域知识提升模型理解能力，并通过不确定性处理增强可靠性。摘要未明确说明使用的具体数据集或模型架构细节，但提及了政策立场分析任务，推断可能涉及金融文本数据集和LLM的变体。",
      "result": "实验结果显示，该框架在政策立场分析任务上达到了state-of-the-art性能，证明了其有效性。统计分析表明，感知不确定性与模型错误率之间存在显著正相关，这验证了不确定性模块作为诊断信号的作用，提高了模型可靠性。摘要未明确说明具体性能指标数据（如准确率提升百分比），但可推断优于基线方法，表明框架在分类任务中具有优势。",
      "conclusion": "本研究的主要贡献是开发了一个不确定性感知的LLM框架，用于解读Fedspeak并提升政策立场分类的准确性和可靠性。学术上，它推动了自然语言处理在金融领域的应用，特别是在不确定性建模方面；实际上，有助于改善金融预测、交易决策和政策分析。摘要未明确说明局限性或未来工作方向，但可推断潜在方向包括扩展到其他金融文本或进一步优化不确定性估计方法。",
      "tags": [
        "Large Language Model",
        "Uncertainty-Aware Framework",
        "Monetary Policy Transmission",
        "Natural Language Processing",
        "Confidence Estimation"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:04.453091Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.05097",
    "title": "MultiCheck: Strengthening Web Trust with Unified Multimodal Fact Verification",
    "authors": [
      "Aditya Kishore",
      "Gaurav Kumar",
      "Jasabanta Patro"
    ],
    "abstract": "Misinformation on the web increasingly appears in multimodal forms, combining text, images, and OCR-rendered content in ways that amplify harm to public trust and vulnerable communities. While prior fact-checking systems often rely on unimodal signals or shallow fusion strategies, modern misinformation campaigns operate across modalities and require models that can reason over subtle cross-modal inconsistencies in a transparent and responsible manner. We introduce MultiCheck, a lightweight and interpretable framework for multimodal fact verification that jointly analyzes textual, visual, and OCR evidence. At its core, MultiCheck employs a relational fusion module based on element-wise difference and product operations, allowing for explicit cross-modal interaction modeling with minimal computational overhead. A contrastive alignment objective further helps the model distinguish between supporting and refuting evidence while maintaining a small memory and energy footprint, making it suitable for low-resource deployment. Evaluated on the Factify-2 (5-class) and Mocheg (3-class) benchmarks, MultiCheck achieves huge performance improvement and remains robust under noisy OCR and missing modality conditions. Its efficiency, transparency, and real-world robustness make it well-suited for journalists, civil society organisations, and web integrity efforts working to build a safer and more trustworthy web.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.05097.pdf",
    "abs_url": "https://arxiv.org/abs/2508.05097",
    "published": "2025-08-07T07:36:53Z",
    "updated": "2026-01-13T10:20:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "MultiCheck是一个轻量级、可解释的多模态事实验证框架，通过显式跨模态交互建模提升性能和稳健性。",
      "motivation": "研究动机源于网络错误信息日益以多模态形式出现，结合文本、图像和OCR渲染的内容，损害公共信任和脆弱社区。现有的事实检查系统多依赖单模态信号或浅层融合策略，难以有效推理跨模态不一致性，且缺乏透明度。现代错误信息活动利用多模态特性，要求开发能够透明处理跨模态不一致的模型，以增强网络信任并负责任地应对公共危机。",
      "method": "论文提出MultiCheck框架，核心是关系融合模块，采用元素级差和积操作进行显式跨模态交互建模，有效整合文本、视觉和OCR证据，并最小化计算开销。通过对比对齐目标，模型学习区分支持和反驳的证据，同时保持小的内存和能源足迹，适合低资源部署。整体设计强调轻量级和可解释性，为多模态事实验证提供高效技术路线。",
      "result": "在Factify-2（5类）和Mocheg（3类）基准上进行评估，MultiCheck实现了巨大的性能提升，具体数据摘要未明确说明，但与基线方法相比显著改进。模型在噪声OCR和缺失模态条件下表现出优异的稳健性，说明其在实际应用中能有效应对不完整或嘈杂的数据环境，增强现实世界可靠性。",
      "conclusion": "论文主要贡献在于开发了MultiCheck，一个轻量级、可解释的多模态事实验证框架，成功解决跨模态错误信息的挑战。其效率、透明度和现实世界稳健性使其适用于记者、公民社会组织和网络诚信工作，有助于构建更安全的网络环境。未来工作可能涉及扩展到更多模态或动态内容处理，但摘要未明确说明。",
      "tags": [
        "Multimodal Fact Verification",
        "Cross-modal Fusion",
        "Contrastive Learning",
        "Lightweight Framework",
        "OCR-based Verification"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:21.753927Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.04403",
    "title": "Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model",
    "authors": [
      "Kiyotada Mori",
      "Seiya Kawano",
      "Angel Fernando Garcia Contreras",
      "Koichiro Yoshino"
    ],
    "abstract": "Prefetching of dialogue responses has been investigated to reduce user-perceived latency (UPL), which refers to the user's waiting time before receiving the system's response, in spoken dialogue systems. To reduce the UPL, it is necessary to predict complete user utterances before the end of the user's speech, typically by language models, to prepare prefetched dialogue responses. In this study, we proposed a prediction confidence model (PCM) that determines whether prefetching is possible or not by estimating the semantic similarity between the predicted complete user utterance and the complete user utterance. We evaluated our PCM based on the differences between the predicted complete user utterance and the complete user utterance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.04403.pdf",
    "abs_url": "https://arxiv.org/abs/2508.04403",
    "published": "2025-08-06T12:45:09Z",
    "updated": "2026-01-13T06:03:38Z",
    "comment": "Corrected typographical errors, including the number of subjects in the response evaluation experiment",
    "light_analysis": {
      "overview": "提出一个预测置信度模型（PCM），通过语义相似性评估预测用户话语的准确性，优化对话响应预取以减少用户感知延迟。",
      "motivation": "口语对话系统中，用户感知延迟（UPL）指的是用户等待系统响应的时间，高延迟严重影响用户体验。现有方法通过语言模型预测用户完整话语来预取响应，但缺乏对预测准确性的有效评估，可能导致预取错误响应、浪费计算资源并实际增加延迟。因此，本研究旨在开发一个置信度模型，仅在预测可靠时触发预取，从而提升系统效率和响应速度，解决现有方法的不足之处。",
      "method": "本研究提出预测置信度模型（PCM），其核心方法是结合语言模型预测用户完整话语，然后通过计算预测话语与实际话语之间的语义相似性来估计置信度。技术路线包括：使用语言模型生成预测，PCM评估语义相似性作为置信度评分，基于评分决定是否预取对话响应。关键创新在于将置信度评估集成到预取决策中，通过差异分析优化预取触发条件，提高预测准确性和系统可靠性。",
      "result": "论文基于预测和实际用户话语的差异评估了PCM性能，但摘要未明确说明具体实验结果数据，如准确率提升或延迟减少百分比。推断PCM可能通过减少无效预取来降低延迟，提高响应效率，但详细指标需要参考完整论文。与基线方法相比，该模型旨在优化预取决策，但摘要中未提供直接对比数据，需进一步实验验证其实际效果。",
      "conclusion": "本研究的主要贡献是提出了预测置信度模型（PCM），结合语义相似性评估来优化对话响应预取策略，有效降低用户感知延迟。学术价值在于为预取决策引入置信度机制，提升系统可靠性和预测准确性；实际应用价值在于改善口语对话系统的响应速度和用户体验。局限性包括对语言模型预测性能的依赖，未来工作可探索多模态数据集成或自适应置信度阈值调整以增强模型鲁棒性。",
      "tags": [
        "Dialogue Response Prefetching",
        "Semantic Similarity",
        "Prediction Confidence Model",
        "Language Model",
        "User-Perceived Latency"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:40.060366Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.02618",
    "title": "Alleviating Attention Hacking in Discriminative Reward Modeling through Interaction Distillation",
    "authors": [
      "Jianxiang Zang"
    ],
    "abstract": "The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, the mainstream discriminative reward modeling is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this \"attention hacking\", we propose \"Interaction Distillation\", a novel training framework for more adequate discriminative reward modeling via attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the reward modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in discriminative RM.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.02618.pdf",
    "abs_url": "https://arxiv.org/abs/2508.02618",
    "published": "2025-08-04T17:06:23Z",
    "updated": "2026-01-13T15:31:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种名为“交互蒸馏”的新训练框架，通过注意力级优化解决判别性奖励模型中的注意黑客问题。",
      "motivation": "该研究旨在解决奖励模型在强化学习从人类反馈（RLHF）中面临的注意黑客问题，该问题导致奖励信号不稳定和不可推广。奖励模型是大型语言模型（LLMs）训练的核心组件，负责提供生成的响应的奖励信号。然而，当前主流的判别性奖励建模方法在令牌级交互上不足，使其判断信号容易被上下文中的错误注意力分配所黑客攻击。这源于两个根本限制：一是偏好模型使用解码器-only架构，其单向因果注意力机制导致提示-响应序列内的注意力前向衰减；二是独立的Siamese编码范式使得选择和拒绝序列间缺乏令牌级序列间注意力，从而影响了RLHF的效果和模型泛化能力。",
      "method": "该研究提出“交互蒸馏”训练框架，通过注意力级优化来改进判别性奖励建模。框架引入一个基于交互的自然语言理解模型作为教师模型，该模型通过全面的注意力机制提供复杂的令牌交互模式。奖励模型则被指导通过一个注意力对齐目标去模拟教师模型的交互模式，从而优化其注意力分配。关键创新在于利用教师模型的注意力模式来增强奖励模型的令牌级交互能力，克服了解码器-only架构和Siamese编码范式带来的限制，摘要未明确说明具体的数据集或模型架构细节。",
      "result": "通过广泛实验验证，交互蒸馏方法能够提供比针对数据噪声的最先进奖励模型优化方法更稳定和可推广的奖励信号。实验表明，该方法有效缓解了注意力黑客问题，突出了该问题在判别性奖励模型中是更根本的局限性。与基线方法相比，交互蒸馏在奖励信号的质量上表现出优势，但摘要未提供具体的性能指标数据，如准确率或效率改进。结果强调优化注意力机制对于提升奖励模型效果的潜力。",
      "conclusion": "该研究的主要贡献是提出交互蒸馏框架，有效解决判别性奖励模型中的注意黑客问题，提升奖励信号的稳定性和泛化性。学术价值在于揭示了注意力机制是奖励建模的关键限制，并提供了一种通过注意力级优化的新方法。实际应用价值可能包括提高基于人类反馈的强化学习效果，从而改善大型语言模型的训练和性能。未来工作方向可能涉及在更广泛数据集上验证方法效果或探索其他优化策略，摘要未明确说明具体的局限性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning from Human Feedback",
        "Reward Modeling",
        "Attention Mechanism",
        "Siamese Encoding"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:38.786918Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.21802",
    "title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
    "authors": [
      "Junzhe Li",
      "Yutao Cui",
      "Tao Huang",
      "Yinping Ma",
      "Chun Fan",
      "Miles Yang",
      "Zhao Zhong"
    ],
    "abstract": "Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO and DanceGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for faster sampling. So we present a faster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2507.21802.pdf",
    "abs_url": "https://arxiv.org/abs/2507.21802",
    "published": "2025-07-29T13:40:09Z",
    "updated": "2026-01-13T08:15:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "MixGRPO提出了一种混合ODE-SDE采样策略，显著提升了基于流匹配的GRPO方法在图像生成人类偏好对齐中的训练效率和性能。",
      "motivation": "虽然GRPO增强了流匹配模型在图像生成人类偏好对齐中的效果，但现有方法如FlowGRPO和DanceGRPO仍效率低下，因为它们必须在马尔可夫决策过程指定的所有去噪步骤上进行采样和优化。这导致了高昂的计算开销和训练时间，限制了实际应用。效率问题对实时或资源受限的场景尤为重要，但现有方法未能有效解决。",
      "method": "MixGRPO框架通过集成随机微分方程和常微分方程的混合采样策略，优化了马尔可夫决策过程中的流匹配模型。核心创新是引入滑动窗口机制：窗口内应用SDE采样和GRPO指导的优化，窗口外则使用ODE采样。这种设计将采样随机性限制在窗口时间步内，减少优化开销，使梯度更新更集中以加速收敛。此外，窗口外时间步不参与优化，支持高阶求解器加速采样，从而衍生出MixGRPO-Flash变体进一步提升训练效率。",
      "result": "MixGRPO在人类偏好对齐的多个维度上表现出显著提升，超越了基线方法DanceGRPO，在效果和效率方面均更优。具体而言，训练时间减少了近50%。MixGRPO-Flash变体进一步将训练时间降低了71%，同时保持了可比的性能。这些实验结果表明，该方法在加速收敛和减少计算成本方面具有明显优势。",
      "conclusion": "论文的主要贡献是提出了MixGRPO框架，通过混合ODE-SDE采样策略显著提高了GRPO在流匹配模型中的训练效率和性能。这为图像生成中的人类偏好对齐提供了更高效的解决方案，具有重要的学术和实际应用价值，例如在加速模型训练和降低资源消耗方面。摘要未明确说明局限性或未来工作方向，但该方法可能为进一步优化和扩展到其他生成任务奠定了基础。",
      "tags": [
        "Flow Matching",
        "GRPO",
        "Stochastic Differential Equations",
        "Ordinary Differential Equations",
        "Human Preference Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:33.770196Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.21503",
    "title": "MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions",
    "authors": [
      "Yanxu Zhu",
      "Shitong Duan",
      "Xiangxu Zhang",
      "Jitao Sang",
      "Peng Zhang",
      "Tun Lu",
      "Xiao Zhou",
      "Jing Yao",
      "Xiaoyuan Yi",
      "Xing Xie"
    ],
    "abstract": "Recently Multimodal Large Language Models (MLLMs) have achieved considerable advancements in vision-language tasks, yet produce potentially harmful or untrustworthy content. Despite substantial work investigating the trustworthiness of language models, MMLMs' capability to act honestly, especially when faced with visually unanswerable questions, remains largely underexplored. This work presents the first systematic assessment of honesty behaviors across various MLLMs. We ground honesty in models' response behaviors to unanswerable visual questions, define four representative types of such questions, and construct MoHoBench, a large-scale MMLM honest benchmark, consisting of 12k+ visual question samples, whose quality is guaranteed by multi-stage filtering and human verification. Using MoHoBench, we benchmarked the honesty of 28 popular MMLMs and conducted a comprehensive analysis. Our findings show that: (1) most models fail to appropriately refuse to answer when necessary, and (2) MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment. Therefore, we implemented initial alignment methods using supervised and preference learning to improve honesty behavior, providing a foundation for future work on trustworthy MLLMs. Our data and code can be found at https://github.com/yanxuzhu/MoHoBench.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2507.21503.pdf",
    "abs_url": "https://arxiv.org/abs/2507.21503",
    "published": "2025-07-29T04:55:49Z",
    "updated": "2026-01-13T13:07:18Z",
    "comment": "AAAI2026 Oral",
    "light_analysis": {
      "overview": "本研究首次系统性地评估多模态大型语言模型在不可回答视觉问题上的诚实性，并提出了大规模基准 MoHoBench 来推动可信赖模型的发展。",
      "motivation": "多模态大型语言模型在视觉语言任务中取得显著进展，但可能生成有害或不值得信任的内容。尽管已有研究探索语言模型的可信度，但 MLLMs 在面对视觉上不可回答问题时的诚实性仍未被充分研究，这限制了模型在实际应用中的可靠性，现有方法缺乏针对多模态场景的专门评估框架。",
      "method": "研究基于模型对不可回答视觉问题的响应行为来定义诚实性，具体定义了四种代表性视觉问题类型，并构建 MoHoBench 基准，包含超过12k个视觉问题样本，通过多阶段过滤和人工验证保证数据质量。核心方法包括使用监督学习和偏好学习进行初步对齐，以改善模型的诚实行为，为后续研究提供基础。",
      "result": "通过 MoHoBench 评估了28个流行的 MLLMs，发现大多数模型在必要时未能适当拒绝回答不可回答问题，诚实性不仅与语言建模相关，还显著受视觉信息影响，这表明需要开发专门的多模态对齐方法来提升模型的可信度，实验结果强调了视觉上下文在模型行为中的关键作用。",
      "conclusion": "该研究的主要贡献在于首次为 MLLMs 诚实性提供了系统性评估框架和基准，揭示视觉信息对模型行为的影响，并通过初步对齐方法为未来可信赖多模态模型的研究奠定基础，具有学术和实际应用价值，建议未来开发更有效的多模态对齐技术以解决当前局限性。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Question Answering",
        "Honesty Benchmark",
        "Supervised Learning",
        "Preference Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:25.506702Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.16559",
    "title": "Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge",
    "authors": [
      "Tobias Rueckert",
      "David Rauber",
      "Raphaela Maerkl",
      "Leonard Klausmann",
      "Suemeyye R. Yildiran",
      "Max Gutbrod",
      "Danilo Weber Nunes",
      "Alvaro Fernandez Moreno",
      "Imanol Luengo",
      "Danail Stoyanov",
      "Nicolas Toussaint",
      "Enki Cho",
      "Hyeon Bae Kim",
      "Oh Sung Choo",
      "Ka Young Kim",
      "Seong Tae Kim",
      "Gonçalo Arantes",
      "Kehan Song",
      "Jianjun Zhu",
      "Junchen Xiong",
      "Tingyi Lin",
      "Shunsuke Kikuchi",
      "Hiroki Matsuzaki",
      "Atsushi Kouno",
      "João Renato Ribeiro Manesco",
      "João Paulo Papa",
      "Tae-Min Choi",
      "Tae Kyeong Jeong",
      "Juyoun Park",
      "Oluwatosin Alabi",
      "Meng Wei",
      "Tom Vercauteren",
      "Runzhi Wu",
      "Mengya Xu",
      "An Wang",
      "Long Bai",
      "Hongliang Ren",
      "Amine Yamlahi",
      "Jakob Hennighausen",
      "Lena Maier-Hein",
      "Satoshi Kondo",
      "Satoshi Kasai",
      "Kousuke Hirasawa",
      "Shu Yang",
      "Yihui Wang",
      "Hao Chen",
      "Santiago Rodríguez",
      "Nicolás Aparicio",
      "Leonardo Manrique",
      "Juan Camilo Lyons",
      "Olivia Hosie",
      "Nicolás Ayobi",
      "Pablo Arbeláez",
      "Yiping Li",
      "Yasmina Al Khalil",
      "Sahar Nasirihaghighi",
      "Stefanie Speidel",
      "Daniel Rueckert",
      "Hubertus Feussner",
      "Dirk Wilhelm",
      "Christoph Palm"
    ],
    "abstract": "Reliable recognition and localization of surgical instruments in endoscopic video recordings are foundational for a wide range of applications in computer- and robot-assisted minimally invasive surgery (RAMIS), including surgical training, skill assessment, and autonomous assistance. However, robust performance under real-world conditions remains a significant challenge. Incorporating surgical context - such as the current procedural phase - has emerged as a promising strategy to improve robustness and interpretability.   To address these challenges, we organized the Surgical Procedure Phase, Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel, multi-center dataset comprising thirteen full-length laparoscopic cholecystectomy videos collected from three distinct medical institutions, with unified annotations for three interrelated tasks: surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation. Unlike existing datasets, ours enables joint investigation of instrument localization and procedural context within the same data while supporting the integration of temporal information across entire procedures.   We report results and findings in accordance with the BIAS guidelines for biomedical image analysis challenges. The PhaKIR sub-challenge advances the field by providing a unique benchmark for developing temporally aware, context-driven methods in RAMIS and offers a high-quality resource to support future research in surgical scene understanding.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.16559.pdf",
    "abs_url": "https://arxiv.org/abs/2507.16559",
    "published": "2025-07-22T13:10:42Z",
    "updated": "2026-01-13T17:49:42Z",
    "comment": "A challenge report pre-print accepted by the journal Medical Image Analysis (MedIA), containing 37 pages, 15 figures, and 14 tables",
    "light_analysis": {
      "overview": "PhaKIR 2024挑战通过引入多中心数据集，推动了手术阶段识别、器械关键点估计和实例分割的联合研究。",
      "motivation": "在计算机辅助和机器人辅助微创手术中，可靠识别和定位手术器械对于手术训练、技能评估和自主辅助等应用至关重要。然而，真实条件下的鲁棒性能仍是一个重大挑战，现有方法往往忽略手术上下文（如当前程序阶段），导致性能受限。结合上下文以提高鲁棒性和可解释性，成为解决这些问题的关键策略。",
      "method": "论文通过组织PhaKIR子挑战，引入了一个新颖的多中心数据集，包括13个来自三个医疗机构的腹腔镜胆囊切除术视频。该数据集统一标注了三个相互关联的任务：手术阶段识别、器械关键点估计和器械实例分割。创新点在于支持关节研究器械定位和程序上下文，并整合跨整个程序的时间信息。数据集设计考虑了多样性，为开发时间感知、上下文驱动方法提供了基础资源。",
      "result": "论文按照BIAS指南报告了挑战结果和发现，但摘要中未明确说明具体的性能指标如准确率或效率改进。结果强调了数据集的有效性和上下文集成的重要性，为相关任务提供了基准评估，推动了领域进展。与基线方法的对比需参考详细论文，挑战结果为未来研究提供了方向。",
      "conclusion": "PhaKIR挑战通过提供独特的基准，促进了时间感知、上下文驱动方法在RAMIS中的发展，为手术场景理解提供了高质量资源。研究具有重要的学术价值，支持手术训练和自动化应用，潜在局限性可能包括数据集规模和泛化能力。未来工作可扩展到更多手术类型和复杂场景，以进一步提升鲁棒性。",
      "tags": [
        "Surgical Phase Recognition",
        "Instrument Keypoint Estimation",
        "Instrument Instance Segmentation",
        "Endoscopic Vision",
        "Multi-center Dataset"
      ]
    },
    "analyzed_at": "2026-01-14T03:39:53.024675Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.14137",
    "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning",
    "authors": [
      "Shashanka Venkataramanan",
      "Valentinos Pariza",
      "Mohammadreza Salehi",
      "Lukas Knobel",
      "Spyros Gidaris",
      "Elias Ramzi",
      "Andrei Bursuc",
      "Yuki M. Asano"
    ],
    "abstract": "We present Franca (pronounced Fran-ka): free one; the first fully open-source (data, code, weights) vision foundation model that matches and in many cases surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2, CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in SSL clustering methods. While modern models rely on assigning image features to large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to account for the inherent ambiguity in clustering semantics. To address this, we introduce a parameter-efficient, multi-head clustering projector based on nested Matryoshka representations. This design progressively refines features into increasingly fine-grained clusters without increasing the model size, enabling both performance and memory efficiency. Additionally, we propose a novel positional disentanglement strategy that explicitly removes positional biases from dense representations, thereby improving the encoding of semantic content. This leads to consistent gains on several downstream benchmarks, demonstrating the utility of cleaner feature spaces. Our contributions establish a new standard for transparent, high-performance vision models and open a path toward more reproducible and generalizable foundation models for the broader AI community. The code and model checkpoints are available at https://github.com/valeoai/Franca.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.14137.pdf",
    "abs_url": "https://arxiv.org/abs/2507.14137",
    "published": "2025-07-18T17:59:55Z",
    "updated": "2026-01-13T13:22:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "Franca 是一个完全开源的视觉基础模型，通过嵌套 Matryoshka 聚类和位置解缠策略，性能在多个任务中匹配或超越最先进的专有模型。",
      "motivation": "该研究的动机在于解决现代自监督学习（SSL）聚类方法的两个关键问题：一是现有方法如 Sinkhorn-Knopp 将图像特征分配到大型码本时，未能考虑聚类语义的固有模糊性，从而限制了特征表示的质量和下游任务性能；二是当前最先进的视觉基础模型多为专有，缺乏透明度和可重现性。因此，研究旨在开发一个完全开源的模型，不仅匹配专有模型性能，还通过技术创新解决聚类模糊性，为AI社区提供可访问的高性能视觉表示学习方案。",
      "method": "论文的核心方法包括两个创新点：首先，提出了一个参数高效的嵌套 Matryoshka 多头聚类投影器，该结构在保持模型大小不变的情况下，通过逐层细化的方式将特征分配到不同粒度的聚类中，解决了聚类语义模糊性问题。其次，引入了一种新颖的位置解缠策略，从密集表示中显式移除位置偏差，以增强语义内容的编码。训练流程基于透明管道，使用公开数据集 ImageNet-21K 和 ReLAION-2B 的子集进行自监督学习。",
      "result": "Franca 模型在多个下游视觉基准测试中展现了优异的性能，匹配或超越了多个最先进的专有模型，如 DINOv2、CLIP 和 SigLIPv2。通过嵌套 Matryoshka 聚类和位置解缠策略，模型在特征表示上实现了更高效和准确的聚类，从而带来了性能的持续提升。虽然摘要未提供具体的准确率或效率改进数据，但实验结果表明模型在保持内存效率的同时，显著改善了语义编码，证实了新方法的有效性。",
      "conclusion": "该论文的主要结论是成功开发了 Franca，一个性能卓越且完全开源的视觉基础模型，通过嵌套 Matryoshka 聚类和位置解缠策略，有效解决了自监督学习中的聚类模糊性问题。这为视觉表示学习领域设定了新的透明和高效标准，有助于推动更可重现和通用化的基础模型研究。尽管未明确提及局限性，但这项工作为未来在更多数据集和任务上的扩展奠定了基础，具有重要的学术和应用价值。",
      "tags": [
        "Nested Matryoshka Clustering",
        "Self-Supervised Learning",
        "Positional Disentanglement",
        "Visual Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:26.806221Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.13874",
    "title": "Geometry of Knowledge Allows Extending Diversity Boundaries of Large Language Models",
    "authors": [
      "Mateusz Bystroński",
      "Doheon Han",
      "Nitesh V. Chawla",
      "Tomasz Kajdanowicz"
    ],
    "abstract": "Starting from the hypothesis that knowledge in semantic space is organized along structured manifolds, we argue that this geometric structure renders the space explorable. By traversing it and using the resulting continuous representations to condition an LLM's generation distribution, we can systematically expand the model's reachable semantic range. We introduce a framework that requires no modification of LLM parameters and operationalizes this idea by constructing a conditioning distribution from a small set of diverse anchor generations. This distribution conditions LLM's generation via an xRAG-style projector. Our experiments demonstrate that this manifold-based conditioning substantially increases generative diversity, with direct benefits for enhancing divergent thinking, a core facet of creativity, in language models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2507.13874.pdf",
    "abs_url": "https://arxiv.org/abs/2507.13874",
    "published": "2025-07-18T12:54:28Z",
    "updated": "2026-01-13T15:59:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出基于知识几何结构的框架，无需修改大型语言模型参数，通过流形条件化扩展其生成多样性，直接增强模型创造力。",
      "motivation": "本研究基于知识在语义空间中沿结构化流形组织的假设，旨在解决大型语言模型生成多样性受限的问题，影响创造力的核心方面如发散思维。增强生成多样性对提升模型在创造性任务中的应用价值至关重要，现有方法可能需要直接修改模型参数或未能有效利用语义空间的几何结构。论文从几何角度出发，探索通过条件化扩展模型可达语义范围的新途径，以弥补传统方法的不足。",
      "method": "论文引入了一个无需修改LLM参数的框架，通过利用知识在语义空间中的结构化流形来条件化生成过程。具体地，基于一小集多样锚定生成构建连续表示，形成条件分布，并通过xRAG风格投影器将这些表示整合到LLM的生成分布中。关键创新在于从几何角度探索语义空间，使用流形遍历生成条件信号，实现系统性的语义范围扩展，而无需调整模型内部参数。",
      "result": "实验表明，该基于流形的条件化方法显著增加了生成多样性，直接有益于增强语言模型中的发散思维。摘要未明确提供具体性能指标如准确率提升，但提到多样性有实质性增加。推断该方法在生成任务中与基线方法相比在多样性方面有提升，但具体对比数据和效率改进摘要未说明。",
      "conclusion": "该研究的主要贡献是提出了一个利用知识几何扩展LLM生成多样性的新框架，无需修改模型参数，学术上揭示了知识组织对生成过程的影响，提供了一种创新的条件化方法。实际应用中，可促进创造性任务如内容生成的性能，未来工作可能包括更深入的理论分析和应用扩展，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Large Language Model",
        "Knowledge Geometry",
        "Semantic Manifold",
        "xRAG-style Conditioning"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:09.631245Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.12646",
    "title": "Reconstruct, Inpaint, Test-Time Finetune: Dynamic Novel-view Synthesis from Monocular Videos",
    "authors": [
      "Kaihua Chen",
      "Tarasha Khurana",
      "Deva Ramanan"
    ],
    "abstract": "We explore novel-view synthesis for dynamic scenes from monocular videos. Prior approaches rely on costly test-time optimization of 4D representations or do not preserve scene geometry when trained in a feed-forward manner. Our approach is based on three key insights: (1) covisible pixels (that are visible in both the input and target views) can be rendered by first reconstructing the dynamic 3D scene and rendering the reconstruction from the novel-views and (2) hidden pixels in novel views can be \"inpainted\" with feed-forward 2D video diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be self-supervised from 2D videos, allowing us to train it on a large corpus of in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot to novel test videos via test-time finetuning. We empirically verify that CogNVS outperforms almost all prior art for novel-view synthesis of dynamic scenes from monocular videos.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.12646.pdf",
    "abs_url": "https://arxiv.org/abs/2507.12646",
    "published": "2025-07-16T21:40:29Z",
    "updated": "2026-01-13T02:52:59Z",
    "comment": "NeurIPS 2025. Project page: https://cog-nvs.github.io/",
    "light_analysis": {
      "overview": "提出了一种结合动态3D重建、视频扩散模型修复和测试时微调的方法，用于从单眼视频高效合成动态场景的新视图。",
      "motivation": "该研究旨在解决从单眼视频合成动态场景新视图的问题，这在增强现实和虚拟现实等应用中具有重要价值。现有方法依赖昂贵的测试时优化（如4D表示），导致效率低下，或在前馈训练中无法有效保持场景几何，影响合成质量。这些不足限制了实际部署，因此需要一种更高效且几何准确的方法来提升动态场景合成的实用性和准确性。",
      "method": "方法基于三个关键见解：首先，通过重建动态3D场景并渲染共视像素来处理可见部分；其次，利用前馈2D视频扩散模型（CogNVS）修复新视图中的隐藏像素，CogNVS可从2D视频进行自监督训练，使用大量野外视频数据集；最后，通过测试时微调实现CogNVS的零样本应用，使其能适应新测试视频。创新点在于整合3D重建与扩散生成模型，提升合成效率和几何保真度。",
      "result": "实验验证显示，CogNVS在动态场景新视图合成任务上优于几乎所有先前方法，摘要未提供具体性能指标如准确率或时间效率，但表明在对比基准测试中表现出色，显著提升了合成质量。这证明了该方法在克服现有方法高成本和几何失真问题方面的有效性，尽管缺乏量化数据，但整体性能改进得到确认。",
      "conclusion": "主要贡献是提出了一个新颖的动态新视图合成框架，整合3D重建、扩散模型修复和测试时微调，解决了现有方法的效率与几何精度问题。学术上，该方法展示了自监督训练和测试时适应性在视觉任务中的潜力；实际中，适用于AR/VR和视频编辑等领域。未来工作可能包括扩展到更复杂场景或优化计算效率，摘要未明确说明局限性。",
      "tags": [
        "Dynamic Novel-view Synthesis",
        "Monocular Videos",
        "3D Reconstruction",
        "Video Diffusion Models",
        "Test-Time Finetuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:27.490898Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.06560",
    "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning",
    "authors": [
      "Jae Hyoung Jeon",
      "Cheolsu Lim",
      "Myungjoo Kang"
    ],
    "abstract": "Recent success in contrastive learning has sparked growing interest in more effectively leveraging multiple augmented views of data. While prior methods incorporate multiple views at the loss or feature level, they primarily capture pairwise relationships and fail to model the joint structure across all views. In this work, we propose a divergence-based similarity function (DSF) that explicitly captures the joint structure by representing each set of augmented views as a distribution and measuring similarity as the divergence between distributions. Extensive experiments demonstrate that DSF consistently improves performance across diverse tasks, including kNN classification, linear evaluation, transfer learning, and distribution shift, while also achieving greater efficiency than other multi-view methods. Furthermore, we establish a connection between DSF and cosine similarity, and demonstrate that, unlike cosine similarity, DSF operates effectively without the need for tuning a temperature hyperparameter.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.06560.pdf",
    "abs_url": "https://arxiv.org/abs/2507.06560",
    "published": "2025-07-09T05:28:31Z",
    "updated": "2026-01-13T12:45:55Z",
    "comment": "9 pages, 5 figures. Code: https://github.com/Jeon789/DSF",
    "light_analysis": {
      "overview": "提出了一种基于散度的相似性函数（DSF），通过分布表示和散度测量来显式捕捉多视图对比学习中的联合结构，解决了现有方法依赖成对关系的局限性。",
      "motivation": "近期对比学习在利用数据增强视图方面取得进展，但现有多视图方法主要专注于捕捉视图间的成对关系，未能建模所有视图的联合结构。这一问题限制了模型对数据整体分布的理解，影响了学习性能和泛化能力。由于现实数据往往包含复杂、多视角信息，忽视联合结构可能导致模型在多样任务中表现不佳，因此亟需一种新方法来弥补这一不足。",
      "method": "本工作引入了基于散度的相似性函数（DSF），核心创新在于将每组增强视图表示为概率分布，并计算分布之间的散度来衡量相似性。这种方法显式捕捉了所有视图的联合结构，避免了传统对比学习中仅关注成对关系的限制。DSF无需调整温度超参数，简化了实现过程。尽管摘要未明确说明具体数据集和模型架构，推断其为标准多视图对比学习框架，如使用常见图像或文本数据集进行实验。",
      "result": "实验证明，DSF在多种任务中 consistently 提升性能，包括 kNN 分类、线性评估、转移学习和分布漂移。与其他多视图方法相比，DSF表现出更高的计算效率，在性能改进的同时降低了资源消耗。摘要虽未提供具体数值（如准确率提升百分比），但强调了稳定且优于基线的结果，突出了其在多场景下的广泛应用潜力。",
      "conclusion": "本研究的主要贡献是提出 DSF，通过分布表示和散度测量显式建模多视图联合结构，填补了现有对比学习方法的空缺。DSF 连接了余弦相似性，并展示了无需温度超参数的优势，简化了实际应用。这为多视图学习领域提供了新的技术方向，具有重要的学术价值和广泛的实用前景。未来工作可探索其在更复杂任务（如大规模或无监督设置）中的适用性。",
      "tags": [
        "Multi-View Contrastive Learning",
        "Divergence-Based Similarity",
        "Distribution Representation",
        "Cosine Similarity"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:21.530221Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.02962",
    "title": "RAG-R1: Incentivizing the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism",
    "authors": [
      "Zhiwen Tan",
      "Jiaming Huang",
      "Qintong Wu",
      "Hongxuan Zhang",
      "Chenyi Zhuang",
      "Jinjie Gu"
    ],
    "abstract": "Large Language Models (LLMs), despite their remarkable capabilities, are prone to generating hallucinated or outdated content due to their static internal knowledge. While Retrieval-Augmented Generation (RAG) integrated with Reinforcement Learning (RL) offers a solution, these methods are fundamentally constrained by a single-query mode, leading to prohibitive latency and inherent brittleness. To overcome these limitations, we introduce RAG-R1, a novel two-stage training framework centered around multi-query parallelism. Our framework enables LLMs to adaptively leverage internal and external knowledge during the reasoning process while transitioning from the single-query mode to multi-query parallelism. This architectural shift bolsters reasoning robustness while significantly reducing inference latency. Extensive experiments on seven question-answering benchmarks confirm the superiority of our method, which outperforms the strongest baseline by up to 13.7% and decreases inference time by 11.1%.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2507.02962.pdf",
    "abs_url": "https://arxiv.org/abs/2507.02962",
    "published": "2025-06-30T09:02:45Z",
    "updated": "2026-01-13T06:28:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "RAG-R1是一个通过多查询并行增强大型语言模型搜索和推理能力的两阶段训练框架。",
      "motivation": "大型语言模型（LLMs）尽管能力出色，但因其静态内部知识容易生成幻觉或过时内容，影响在实际应用中的可靠性。现有方法如检索增强生成（RAG）与强化学习（RL）结合虽能缓解问题，但受限于单查询模式，导致高延迟和脆弱性，限制了LLMs的效率和鲁棒性。因此，亟需解决这一瓶颈以提升模型性能。",
      "method": "本文提出RAG-R1，一个新颖的两阶段训练框架，专注于多查询并行技术。该框架使LLMs在推理过程中自适应地利用内部和外部知识，从单查询模式转向多查询并行，从而增强推理鲁棒性并减少延迟。具体实现如数据集和模型架构在摘要中未明确说明，但核心创新在于整合多查询并行来优化RAG-RL方法。",
      "result": "在七个问题回答基准测试中进行的广泛实验显示，RAG-R1优于最强基线方法，性能提升高达13.7%，同时推理时间减少了11.1%。这些结果证实了该方法在准确性和效率上的显著改进，凸显了多查询并行在降低延迟和增强鲁棒性方面的优势。",
      "conclusion": "该研究的主要贡献是提出RAG-R1框架，通过多查询并行解决了LLMs在RAG-RL方法中的延迟和脆弱性问题，增强了推理能力，具有重要的学术和实际应用价值。未来工作可能包括框架优化或扩展到其他任务，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Large Language Model",
        "Retrieval-Augmented Generation",
        "Reinforcement Learning",
        "Multi-query Parallelism",
        "Reasoning Capabilities"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:36.588807Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.18124",
    "title": "Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models",
    "authors": [
      "Shaoxiu Wei",
      "Mingchao Liang",
      "Florian Meyer"
    ],
    "abstract": "Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance.",
    "categories": [
      "cs.LG",
      "eess.SP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.18124.pdf",
    "abs_url": "https://arxiv.org/abs/2506.18124",
    "published": "2025-06-22T18:15:08Z",
    "updated": "2026-01-13T08:40:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种混合方法，通过神经网络增强贝叶斯多对象跟踪模型，结合了模型驱动和数据驱动的优势，提升了性能。",
      "motivation": "多对象跟踪在自动驾驶、海洋科学和航空航天监测等应用中至关重要。传统基于模型的方法具有广泛适用性，但其统计模型部分常被视为过于简化；而完全数据驱动的方法依赖神经网络训练，在有大量标记数据时性能更优，但通用性受限。因此，研究旨在探索一种集成框架，以结合两者的优点，解决现有方法在复杂场景下的局限性。",
      "method": "该方法利用神经网络增强贝叶斯多对象跟踪中的运动模型和测量模型，改进预测和更新步骤的性能。关键创新在于集成神经网络的复杂信息学习能力与贝叶斯框架的统计基础。技术上，采用信念传播避免高维计算复杂度，并结合序列蒙特卡罗方法高效执行低维操作，确保在复杂数据如 nuScenes 数据集上的可计算性。",
      "result": "在 nuScenes 自动驾驶数据集上进行的评估表明，所提出的混合方法达到了最先进的性能水平。通过集成神经网络和贝叶斯框架，跟踪准确性得到显著提升，与基线方法相比显示出优越性。具体性能指标摘要未明确说明，但强调了在实际应用中具有高效性。",
      "conclusion": "本研究的主要贡献是提出了一种混合框架，成功将神经网络的学习能力整合到贝叶斯多对象跟踪中，提高了模型的准确性和适用性。学术价值在于融合了模型驱动和数据驱动方法，为未来在复杂场景下的跟踪任务提供了新思路。实际应用价值在于增强自动驾驶等领域的鲁棒性，潜在局限性可能涉及计算资源需求，未来工作可扩展至更广泛的数据集和动态环境。",
      "tags": [
        "Multiobject Tracking",
        "Bayesian Estimation",
        "Neural Networks",
        "Belief Propagation",
        "Sequential Monte Carlo"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:30.883733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11849",
    "title": "Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values",
    "authors": [
      "R. Teal Witter",
      "Yurong Liu",
      "Christopher Musco"
    ],
    "abstract": "With origins in game theory, probabilistic values like Shapley values, Banzhaf values, and semi-values have emerged as a central tool in explainable AI. They are used for feature attribution, data attribution, data valuation, and more. Since all of these values require exponential time to compute exactly, research has focused on efficient approximation methods using two techniques: Monte Carlo sampling and linear regression formulations. In this work, we present a new way of combining both of these techniques. Our approach is more flexible than prior algorithms, allowing for linear regression to be replaced with any function family whose probabilistic values can be computed efficiently. This allows us to harness the accuracy of tree-based models like XGBoost, while still producing unbiased estimates. From experiments across eight datasets, we find that our methods give state-of-the-art performance for estimating probabilistic values. For Shapley values, the error of our methods can be $6.5\\times$ lower than Permutation SHAP (the most popular Monte Carlo method), $3.8\\times$ lower than Kernel SHAP (the most popular linear regression method), and $2.6\\times$ lower than Leverage SHAP (the prior state-of-the-art Shapley value estimator). For more general probabilistic values, we can obtain error $215\\times$ lower than the best estimator from prior work.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.11849.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11849",
    "published": "2025-06-13T14:57:38Z",
    "updated": "2026-01-13T02:37:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合Monte Carlo采样与回归调整的新方法，用于高效估计Shapley值和概率值，显著提升了估计准确性。",
      "motivation": "概率值（如Shapley值、Banzhaf值）在可解释AI中广泛应用于特征归因、数据估值等领域，但精确计算需要指数时间，计算复杂度高。现有方法主要依赖Monte Carlo采样和线性回归进行近似，但这些方法在灵活性和准确性上可能受限，难以适应复杂模型。本研究旨在克服这些不足，通过提出更灵活的估计框架，以应对实际应用中对高效、准确近似方法的迫切需求，从而推动可解释AI技术的发展。",
      "method": "本研究提出一种新的Regression-adjusted Monte Carlo估计器，其核心创新在于结合Monte Carlo采样与线性回归，但允许线性回归被任何概率值可高效计算的函数族替换。这使得方法能够利用如XGBoost等树基模型的强大拟合能力，同时保持无偏估计特性。通过调整回归模型来减少采样方差，方法提高了估计效率，适用于Shapley值和其他概率值的近似计算，无需依赖特定模型架构。摘要未明确说明具体数据集细节，但实验基于八个数据集进行验证。",
      "result": "通过在八个数据集上的实验，该方法在估计Shapley值时，错误率比Permutation SHAP低6.5倍，比Kernel SHAP低3.8倍，比Leverage SHAP低2.6倍。对于更一般的概率值，错误率可降低215倍于先前最佳估计器。结果表明，该方法在准确性上达到了state-of-the-art水平，显著优于现有Monte Carlo采样和线性回归方法，为概率值估计提供了更精确的解决方案。",
      "conclusion": "本文的主要贡献是提出了一种灵活且高效的Regression-adjusted Monte Carlo估计器，极大提高了Shapley值和概率值的估计准确性。该研究不仅改进了可解释AI中的近似计算技术，还为特征归因和数据估值等应用提供了实用工具。潜在局限性可能包括函数族选择的影响，未来工作可探索更多模型类型或扩展至其他AI任务，以进一步验证方法的泛化能力。",
      "tags": [
        "Shapley Values",
        "Probabilistic Values",
        "Monte Carlo Sampling",
        "Regression Adjustment",
        "XGBoost"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:57.135190Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.10805",
    "title": "Detecting High-Stakes Interactions with Activation Probes",
    "authors": [
      "Alex McKenzie",
      "Urja Pawar",
      "Phil Blandfort",
      "William Bankes",
      "David Krueger",
      "Ekdeep Singh Lubana",
      "Dmitrii Krasheninnikov"
    ],
    "abstract": "Monitoring is an important aspect of safely deploying Large Language Models (LLMs). This paper examines activation probes for detecting ``high-stakes'' interactions -- where the text indicates that the interaction might lead to significant harm -- as a critical, yet underexplored, target for such monitoring. We evaluate several probe architectures trained on synthetic data, and find them to exhibit robust generalization to diverse, out-of-distribution, real-world data. Probes' performance is comparable to that of prompted or finetuned medium-sized LLM monitors, while offering computational savings of six orders-of-magnitude. These savings are enabled by reusing activations of the model that is being monitored. Our experiments also highlight the potential of building resource-aware hierarchical monitoring systems, where probes serve as an efficient initial filter and flag cases for more expensive downstream analysis. We release our novel synthetic dataset and the codebase at https://github.com/arrrlex/models-under-pressure.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.10805.pdf",
    "abs_url": "https://arxiv.org/abs/2506.10805",
    "published": "2025-06-12T15:20:33Z",
    "updated": "2026-01-13T09:49:38Z",
    "comment": "Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "论文提出使用激活探针高效检测大型语言模型的高风险交互，实现计算成本显著降低。",
      "motivation": "研究动机在于大型语言模型（LLMs）的安全部署需要监控高风险交互，即可能导致重大危害的文本交互。高风险交互作为关键监控目标尚未被充分探索，而现有方法如提示或微调LLM监控器往往计算成本高昂，限制了实时部署的可行性。因此，开发高效监控技术至关重要，以解决实际应用中的安全隐患和资源限制问题。摘要未明确说明具体现有方法的细节，但强调了计算效率和监控不足的挑战。",
      "method": "论文的核心方法是训练激活探针架构来检测高风险交互。激活探针基于被监控LLM的内部激活进行训练，利用合成数据集作为训练材料。关键创新在于探针架构的设计和合成数据的使用，使得探针能够从有限数据中学习并泛化到真实世界场景。研究评估了多种探针类型，具体架构细节摘要未明确说明，但提及了训练过程依赖于作者发布的合成数据集，并利用模型激活作为输入特征，避免了额外的计算开销。",
      "result": "主要实验结果显示，激活探针在检测高风险交互上性能与基于提示或微调的中型LLM监控器相当，具体性能指标如准确率摘要未明确说明。探针在计算成本上实现了六个数量级的节省，这通过直接重用被监控模型的激活来实现。实验还验证了探针在多样化和分布外真实世界数据上的健壮泛化能力，支持了构建分层监控系统的可能性，其中探针作为高效过滤器，可减少下游分析的计算负担。",
      "conclusion": "论文的主要贡献是开发了基于激活探针的高效监控方法，用于检测LLM的高风险交互。这具有重要的学术价值，推动了监控技术向计算效率方向的发展；实际应用价值体现在资源感知分层监控系统的构建，探针作为初始过滤器可优化安全部署。局限性可能包括合成数据质量的依赖和更广泛场景的泛化能力验证，未来工作可扩展探针应用到更多高风险检测任务和实际系统集成中。",
      "tags": [
        "Activation Probes",
        "Large Language Models",
        "Monitoring",
        "Synthetic Data",
        "High-Stakes Interactions"
      ]
    },
    "analyzed_at": "2026-01-14T03:40:58.396711Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.10326",
    "title": "VGC-Bench: Towards Mastering Diverse Team Strategies in Competitive Pokémon",
    "authors": [
      "Cameron Angliss",
      "Jiaxun Cui",
      "Jiaheng Hu",
      "Arrasy Rahman",
      "Peter Stone"
    ],
    "abstract": "Developing AI agents that can robustly adapt to varying strategic landscapes without retraining is a central challenge in multi-agent learning. Pokémon Video Game Championships (VGC) is a domain with a vast space of approximately $10^{139}$ team configurations, far larger than those of other games such as Chess, Go, Poker, StarCraft, or Dota. The combinatorial nature of team building in Pokémon VGC causes optimal strategies to vary substantially depending on both the controlled team and the opponent's team, making generalization uniquely challenging. To advance research on this problem, we introduce VGC-Bench: a benchmark that provides critical infrastructure, standardizes evaluation protocols, and supplies a human-play dataset of over 700,000 battle logs and a range of baseline agents based on heuristics, large language models, behavior cloning, and multi-agent reinforcement learning with empirical game-theoretic methods such as self-play, fictitious play, and double oracle. In the restricted setting where an agent is trained and evaluated in a mirror match with a single team configuration, our methods can win against a professional VGC competitor. We repeat this training and evaluation with progressively larger team sets and find that as the number of teams increases, the best-performing algorithm in the single-team setting has worse performance and is more exploitable, but has improved generalization to unseen teams. Our code and dataset are open-sourced at https://github.com/cameronangliss/vgc-bench and https://huggingface.co/datasets/cameronangliss/vgc-battle-logs.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.10326.pdf",
    "abs_url": "https://arxiv.org/abs/2506.10326",
    "published": "2025-06-12T03:19:39Z",
    "updated": "2026-01-13T13:17:41Z",
    "comment": "AAMAS 2026",
    "light_analysis": {
      "overview": "论文提出VGC-Bench基准，用于推动在Pokémon VGC中掌握多样化团队策略的AI研究，提供基础设施、数据集和基线方法。",
      "motivation": "开发能稳健适应不同战略环境的AI代理是multi-agent学习的核心挑战。Pokémon VGC具有约10^{139}的庞大团队配置空间，远超Chess、Go等游戏，导致最优策略高度依赖团队组合，使泛化极为困难。现有方法在处理如此复杂的策略变化和泛化需求时可能不足，缺乏标准化的评估平台，因此需要新基准来系统研究这一难题，以促进AI在复杂策略游戏中的进展。",
      "method": "研究引入了VGC-Bench基准，提供关键基础设施、标准化评估协议、超过700,000个战斗日志的人类游戏数据集，以及多种基线代理。这些代理基于启发式方法、大语言模型、行为克隆和multi-agent强化学习，其中强化学习整合了自玩、虚构玩、双重预言等实证博弈论技术，构建了一个全面的评估框架，旨在测试AI在Pokémon VGC多样化团队策略中的表现。",
      "result": "在限制设置中，代理在单一团队配置的镜像匹配中训练和评估，能够击败专业VGC竞争者。随着团队集逐步扩大，研究发现单团队设置中表现最好的算法性能下降，更易被攻击，但对未见团队的泛化能力得到提高。这显示了处理大规模组合策略空间时算法的挑战性，并突出了泛化与性能之间的权衡，但未提供具体准确率提升数据。",
      "conclusion": "VGC-Bench的主要贡献是提供一个标准化的基准和开源资源（代码和数据集），以推动AI在Pokémon VGC等复杂策略游戏中的研究。这为multi-agent学习和策略泛化问题提供了新的学术平台，具有重要的实际应用价值，如游戏AI开发。未来工作可包括改进算法以应对更大的团队集和增强泛化能力，同时潜在局限性包括对更广泛策略变化的适应性未完全探索。",
      "tags": [
        "Multi-Agent Learning",
        "Reinforcement Learning",
        "Large Language Models",
        "Behavior Cloning",
        "Self-Play"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:13.125798Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.09644",
    "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning",
    "authors": [
      "Dongxu Liu",
      "Jiahui Zhu",
      "Yuang Peng",
      "Haomiao Tang",
      "Yuwei Chen",
      "Chunrui Han",
      "Zheng Ge",
      "Daxin Jiang",
      "Mingxue Liao"
    ],
    "abstract": "Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder's expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.09644.pdf",
    "abs_url": "https://arxiv.org/abs/2506.09644",
    "published": "2025-06-11T12:01:03Z",
    "updated": "2026-01-13T18:26:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了DGAE，一个使用扩散模型指导解码器的自编码器，以高效学习潜在表示，减少高压缩下的性能下降并减小潜在空间维度。",
      "motivation": "自编码器在图像和视频生成模型中通过压缩像素到潜在空间实现高效表示，但高压缩率常导致性能下降，且基于GAN的方法带来训练不稳定性。现有方法虽有所缓解，但平衡空间压缩和潜在空间最小化仍具挑战，这对提升生成模型的效率至关重要。DGAE旨在解决这些问题，专注于增强解码器表达能力，以实现更紧凑和高效的潜在表示学习。",
      "method": "DGAE的核心方法是将扩散模型集成到自编码器框架中，指导解码器恢复潜在表示中未完全解码的信息信号。创新点在于利用扩散模型增强解码器的表达力，有效缓解高空间压缩率下的性能下降，同时将潜在空间维度减小2倍以提升效率。摘要未明确说明具体数据集或模型架构细节，但提及在与扩散模型集成时用于ImageNet-1K图像生成任务。",
      "result": "实验结果表明，DGAE在高空间压缩率下显著减轻了性能下降，实现了当前最佳性能，并将潜在空间大小减小了2倍。与基线方法相比，当与扩散模型结合时，在ImageNet-1K图像生成任务中展示了竞争性性能，且这种紧凑潜在表示促进了扩散模型的更快收敛。摘要中未明确提及具体数据如准确率提升，但强调了效率改进和性能优势。",
      "conclusion": "DGAE通过结合扩散模型指导自编码器解码器，显著提升了潜在表示学习的效率和性能，核心贡献在于应对高压缩挑战并减小潜在空间维度。这为图像生成模型提供了更高效的表示方法，学术上创新了自编码器与扩散模型的融合，应用上利于加速扩散模型的训练。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Autoencoder",
        "Diffusion Model",
        "Latent Space",
        "Representation Learning",
        "Image Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:06.182306Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.07553",
    "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition",
    "authors": [
      "Jingchao Wang",
      "Yifan He",
      "Haote Yang",
      "Jiang Wu",
      "Lingli Ge",
      "Xingjian Wei",
      "Yinfan Wang",
      "Linye Li",
      "Huijie Ao",
      "Chengjin Liu",
      "Bin Wang",
      "Lijun Wu",
      "Conghui He"
    ],
    "abstract": "Optical Chemical Structure Recognition (OCSR) is essential for converting molecular images into machine-readable formats. While recent vision-language models (VLMs) have shown promise, their image-captioning approach often struggles with complex molecular structures and inconsistent annotations. To address these issues, we introduce GTR-VL, featuring two key innovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought} mechanism that emulates human reasoning by incrementally parsing molecular graphs through sequential atom-bond predictions, and (2) the data-centric \\textit{Faithfully Recognize What You've Seen} principle, which aligns abbreviated structures in images with their expanded annotations. For hand-drawn OCSR tasks, where datasets lack graph annotations and only provide final SMILES, we apply reinforcement learning using the GRPO method, introducing reward mechanisms like format reward, graph reward, and SMILES reward. This approach significantly enhances performance in hand-drawn recognition tasks through weak supervision. We developed GTR-1.3M, a large-scale instruction-tuning dataset with corrected annotations, and MolRec-Bench, the first benchmark for fine-grained evaluation of graph-parsing accuracy in OCSR. Our two-stage training scheme involves SFT training for printed images and the GRPO method for transferring capabilities to hand-drawn tasks. Experiments show that GTR-VL outperforms specialist models, chemistry-domain VLMs, and commercial VLMs on both printed and hand-drawn datasets.",
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.07553.pdf",
    "abs_url": "https://arxiv.org/abs/2506.07553",
    "published": "2025-06-09T08:47:10Z",
    "updated": "2026-01-13T07:21:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "GTR-VL方法通过Graph Traversal as Visual Chain of Thought机制和强化学习奖励，显著提升了分子结构识别的准确性和鲁棒性，特别是在复杂和手绘图像上。",
      "motivation": "光学化学结构识别（OCSR）是将分子图像转换为机器可读格式的关键技术，对化学信息学至关重要。现有视觉-语言模型采用图像-标题方法，但在处理复杂分子结构和标注不一致时表现不佳，这限制了自动化化学数据提取的效率和精度。因此，本研究旨在开发一种更精确和鲁棒的识别方法，以克服这些局限性，推动化学领域的自动化应用。",
      "method": "论文提出GTR-VL方法，核心创新包括Graph Traversal as Visual Chain of Thought机制，通过顺序原子-键预测模拟人类逐步解析分子图；以及数据中心的Faithfully Recognize What You've Seen原则，对齐图像中缩写结构与扩展标注。对于手绘任务，应用强化学习（GRPO方法），引入格式奖励、图奖励和SMILES奖励进行弱监督训练。开发了GTR-1.3M大规模指令调优数据集和MolRec-Bench基准，采用两阶段训练方案：监督微调（SFT）用于打印图像，GRPO方法用于向手绘任务迁移能力。",
      "result": "实验显示，GTR-VL在打印和手绘分子图像数据集上均优于基线方法，包括专家模型、化学领域视觉-语言模型和商业视觉-语言模型。摘要未明确说明具体性能指标如准确率提升百分比，但强调通过奖励机制在手绘识别任务中实现了显著性能改进，证明了该方法在处理复杂结构和弱监督场景下的有效性。",
      "conclusion": "该研究通过引入模拟人类推理的视觉链机制和强化学习奖励，有效解决了OCSR中复杂结构和不一致标注的挑战，贡献了新方法、原则、数据集和基准。学术价值在于推动了视觉-语言模型在化学领域的精细化应用，实际应用价值包括增强化学数据的自动化处理能力。未来工作可探索模型在其他化学图像类型的扩展或进一步优化泛化性能。",
      "tags": [
        "Graph Traversal",
        "Visual Chain of Thought",
        "Reinforcement Learning",
        "Optical Chemical Structure Recognition",
        "SMILES"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:15.596914Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.05952",
    "title": "MOGO: Residual Quantized Hierarchical Causal Transformer for High-Quality and Real-Time 3D Human Motion Generation",
    "authors": [
      "Dongjie Fu",
      "Tengjiao Sun",
      "Pengcheng Fang",
      "Xiaohao Cai",
      "Hansung Kim"
    ],
    "abstract": "Recent advances in transformer-based text-to-motion generation have led to impressive progress in synthesizing high-quality human motion. Nevertheless, jointly achieving high fidelity, streaming capability, real-time responsiveness, and scalability remains a fundamental challenge. In this paper, we propose MOGO (Motion Generation with One-pass), a novel autoregressive framework tailored for efficient and real-time 3D motion generation. MOGO comprises two key components: (1) MoSA-VQ, a motion scale-adaptive residual vector quantization module that hierarchically discretizes motion sequences with learnable scaling to produce compact yet expressive representations; and (2) RQHC-Transformer, a residual quantized hierarchical causal transformer that generates multi-layer motion tokens in a single forward pass, significantly reducing inference latency. To enhance semantic fidelity, we further introduce a text condition alignment mechanism that improves motion decoding under textual control. Extensive experiments on benchmark datasets including HumanML3D, KIT-ML, and CMP demonstrate that MOGO achieves competitive or superior generation quality compared to state-of-the-art transformer-based methods, while offering substantial improvements in real-time performance, streaming generation, and generalization under zero-shot settings.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.05952.pdf",
    "abs_url": "https://arxiv.org/abs/2506.05952",
    "published": "2025-06-06T10:26:54Z",
    "updated": "2026-01-13T11:39:54Z",
    "comment": "9 pages, 4 figures, conference",
    "light_analysis": {
      "overview": "该论文提出MOGO框架，结合MoSA-VQ残差量化模块和RQHC-Transformer，实现高质量、实时的3D人体运动生成。",
      "motivation": "尽管基于Transformer的文本到运动生成已取得显著进展，但同时实现高保真度、流式能力、实时响应性和可扩展性仍是一个根本挑战。这一挑战限制了在虚拟现实、游戏等需要实时交互的应用中部署高质量运动生成模型。现有方法往往难以平衡这些方面，导致在实时场景下性能不足，需要新的框架来解决这一综合问题。",
      "method": "MOGO是一个自回归框架，包含两个关键组件：MoSA-VQ模块通过可学习缩放的分层残差向量量化，将运动序列离散化为紧凑表示；RQHC-Transformer采用分层因果结构，在单次前向传递中生成多层运动令牌，显著降低推理延迟。此外，引入文本条件对齐机制来增强基于文本控制的运动解码语义保真度，从而优化整体生成过程。",
      "result": "在HumanML3D、KIT-ML和CMP等基准数据集上的实验表明，MOGO与最先进的基于Transformer的方法相比，在生成质量上达到竞争或更优水平。同时，它在实时性能、流式生成能力和零样本设置下的泛化能力方面有显著提升，例如减少了推理延迟并增强了应用适应性，但具体性能指标如准确率数据摘要未明确说明。",
      "conclusion": "该研究的主要贡献是提出了MOGO框架，通过新颖的量化模块和Transformer架构，有效提升了3D人体运动生成的实时性和效率。这具有重要的学术价值，为高效运动生成提供了创新方法，并有望促进虚拟现实、游戏等领域的实时应用。未来工作可能包括进一步优化模型结构或扩展到更复杂的多模态任务场景中。",
      "tags": [
        "Transformer",
        "Vector Quantization",
        "Causal Transformer",
        "Autoregressive Model",
        "Motion Generation"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:42.037955Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.05280",
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting",
    "authors": [
      "Nan Wang",
      "Yuantao Chen",
      "Lixing Xiao",
      "Weiqing Xiao",
      "Bohan Li",
      "Zhaoxi Chen",
      "Chongjie Ye",
      "Shaocong Xu",
      "Saining Zhang",
      "Ziyang Yan",
      "Pierre Merriaux",
      "Lei Lei",
      "Tianfan Xue",
      "Hao Zhao"
    ],
    "abstract": "Neural rendering techniques, including NeRF and Gaussian Splatting (GS), rely on photometric consistency to produce high-quality reconstructions. However, in real-world scenarios, it is challenging to guarantee perfect photometric consistency in acquired images. Appearance codes have been widely used to address this issue, but their modeling capability is limited, as a single code is applied to the entire image. Recently, the bilateral grid was introduced to perform pixel-wise color mapping, but it is difficult to optimize and constrain effectively. In this paper, we propose a novel multi-scale bilateral grid that unifies appearance codes and bilateral grids. We demonstrate that this approach significantly improves geometric accuracy in dynamic, decoupled autonomous driving scene reconstruction, outperforming both appearance codes and bilateral grids. This is crucial for autonomous driving, where accurate geometry is important for obstacle avoidance and control. Our method shows strong results across four datasets: Waymo, NuScenes, Argoverse, and PandaSet. We further demonstrate that the improvement in geometry is driven by the multi-scale bilateral grid, which effectively reduces floaters caused by photometric inconsistency.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.05280.pdf",
    "abs_url": "https://arxiv.org/abs/2506.05280",
    "published": "2025-06-05T17:33:41Z",
    "updated": "2026-01-13T15:24:01Z",
    "comment": "Accepted to NeurIPS 2025 ; Project page: https://bigcileng.github.io/bilateral-driving ; Code: https://github.com/BigCiLeng/bilateral-driving",
    "light_analysis": {
      "overview": "本文提出了一种统一 appearance codes 和 bilateral grids 的多尺度双边网格，显著提升了自动驾驶场景中高斯泼溅的几何重建准确性。",
      "motivation": "研究动机源于真实自动驾驶场景中图像采集难以保证完美的光度一致性，这限制了神经渲染技术如高斯泼溅的重建质量。现有方法存在不足：appearance codes 由于对整个图像应用单一代码，建模能力有限；而 bilateral grid 虽能进行像素级颜色映射，但难以优化和有效约束。因此，开发一种更好的方法处理光度不一致性至关重要，因为准确的几何重建对自动驾驶的避障和控制有直接实际应用价值。",
      "method": "论文提出了一种新颖的多尺度双边网格方法，统一了 appearance codes 和 bilateral grids。核心创新在于结合全局 appearance code 与局部像素级映射的优势，通过多尺度设计有效捕捉不同层次细节，减少光度不一致性影响。该方法应用于动态解耦的自动驾驶场景重建，利用高斯泼溅作为基础框架，在多个数据集上进行优化验证，以增强几何准确性。",
      "result": "实验表明，该方法在 Waymo、NuScenes、Argoverse 和 PandaSet 四个数据集上表现出强大结果。相比于 baseline 方法如 appearance codes 和 bilateral grids，多尺度双边网格显著提高了几何准确性，有效减少了光度不一致性引起的漂浮物现象，证明了其在动态场景重建中的优越性能，但摘要未明确提供具体数值指标如准确率提升百分比。",
      "conclusion": "论文的主要贡献是提出多尺度双边网格统一方法，提升了自动驾驶场景重建的几何精度。学术价值在于为神经渲染技术在真实复杂环境中的应用提供了新思路，特别是在处理光度不一致性方面。实际应用价值是增强自动驾驶系统的感知可靠性，支持避障和控制。未来工作可探索该方法在其他场景的扩展性和优化潜力，以进一步提升性能。",
      "tags": [
        "Neural Rendering",
        "Gaussian Splatting",
        "Multi-scale Bilateral Grid",
        "Appearance Codes",
        "Autonomous Driving"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:34.264706Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.03333",
    "title": "A Differential Perspective on Distributional Reinforcement Learning",
    "authors": [
      "Juan Sebastian Rojas",
      "Chi-Guhn Lee"
    ],
    "abstract": "To date, distributional reinforcement learning (distributional RL) methods have exclusively focused on the discounted setting, where an agent aims to optimize a discounted sum of rewards over time. In this work, we extend distributional RL to the average-reward setting, where an agent aims to optimize the reward received per time step. In particular, we utilize a quantile-based approach to develop the first set of algorithms that can successfully learn and/or optimize the long-run per-step reward distribution, as well as the differential return distribution of an average-reward MDP. We derive proven-convergent tabular algorithms for both prediction and control, as well as a broader family of algorithms that have appealing scaling properties. Empirically, we find that these algorithms yield competitive and sometimes superior performance when compared to their non-distributional equivalents, while also capturing rich information about the long-run per-step reward and differential return distributions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.03333.pdf",
    "abs_url": "https://arxiv.org/abs/2506.03333",
    "published": "2025-06-03T19:26:25Z",
    "updated": "2026-01-13T13:43:59Z",
    "comment": "In AAAI Conference on Artificial Intelligence 2026",
    "light_analysis": {
      "overview": "本文首次将分布强化学习扩展至平均奖励设置，提出了基于分位数的算法来学习长期每步奖励分布和差异回报分布。",
      "motivation": "现有分布强化学习（distributional RL）方法主要专注于折扣设置，旨在优化随时间折扣的奖励总和。然而，在实际应用中，平均奖励设置（优化每步奖励）同样重要，因为它能更好地反映长期性能。目前，分布RL方法尚未充分研究该设置，导致在需要稳态优化时存在局限性。因此，本研究的动机是弥补这一理论空白，扩展分布RL的适用范围，以支持更全面的奖励分布分析，从而提升决策系统的鲁棒性和风险敏感性。",
      "method": "本研究采用分位数方法（quantile-based approach）开发了一套新算法，首次实现了对平均奖励马尔可夫决策过程（MDP）的长期每步奖励分布和差异回报分布的学习与优化。具体包括已证明收敛的表格式算法，适用于预测和控制任务，并扩展到一个更广泛的算法家族，这些算法具有良好的可扩展性。关键创新点在于将分布RL的核心技术，如分位数表示，应用到平均奖励设置中，从而处理传统折扣RL未涵盖的场景，提升了算法的理论完整性和实用性。",
      "result": "在经验评估中，这些算法展示了竞争性的性能，有时甚至优于非分布等效方法。尽管摘要未提供具体的数值指标，但结果表明新算法能有效捕获长期每步奖励和差异回报分布的丰富信息，同时在预测和控制任务中保持高效。与基线方法的对比显示，分布方法在保持或提升准确性的同时，提供了额外的分布信息，增强了决策的鲁棒性。然而，具体性能提升数据未在摘要中明确说明。",
      "conclusion": "本研究的主要贡献是将分布强化学习成功扩展到平均奖励设置，填补了现有方法的理论空白，并通过基于分位数的算法提供了收敛保证和实际性能优势。这增强了分布RL的学术价值，为风险敏感决策和稳态优化应用提供了新工具。潜在局限性可能包括算法在复杂环境中的扩展挑战，未来工作可探索优化效率、扩展到连续动作空间，或集成其他分布表示方法以进一步提升性能。",
      "tags": [
        "Distributional Reinforcement Learning",
        "Average-Reward Setting",
        "Quantile-based Approach",
        "Tabular Algorithms",
        "Differential Return Distribution"
      ]
    },
    "analyzed_at": "2026-01-14T03:41:44.009928Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.22362",
    "title": "Directed Homophily-Aware Graph Neural Network",
    "authors": [
      "Aihu Zhang",
      "Jiaxing Xu",
      "Mengcheng Lan",
      "Shili Xiang",
      "Yiping Ke"
    ],
    "abstract": "Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal performance on directed graphs with asymmetric structures. In this work, we propose Directed Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses these limitations by incorporating homophily-aware and direction-sensitive components. DHGNN employs a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions. Extensive experiments on both homophilic and heterophilic directed graph datasets demonstrate that DHGNN outperforms state-of-the-art methods in node classification and link prediction. In particular, DHGNN improves over the best baseline by up to 15.07\\% in link prediction. Our analysis further shows that the gating mechanism captures directional homophily gaps and fluctuating homophily across layers, providing deeper insights into message-passing behavior on complex graph structures.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.22362.pdf",
    "abs_url": "https://arxiv.org/abs/2505.22362",
    "published": "2025-05-28T13:41:04Z",
    "updated": "2026-01-13T06:16:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Directed Homophily-aware Graph Neural Network (DHGNN)，结合同质感知和方向敏感组件，解决图神经网络在异质邻域和有向图上的泛化问题。",
      "motivation": "图神经网络（GNNs）在图结构数据学习中取得了成功，但当前大多数方法难以泛化到异质邻域，即相邻节点特征不同的情况。此外，许多GNNs忽略了真实世界图的方向性，假设图是无向的，导致在有向非对称结构图上性能不佳。这些问题限制了GNNs在实际应用中的有效性，如社交网络和推荐系统常涉及异质关系和方向性影响，因此需要开发新方法以同时考虑同质性和方向性，提升泛化能力和准确性。",
      "method": "本文提出了Directed Homophily-aware Graph Neural Network (DHGNN)，一个新颖的图神经网络框架，旨在解决异质邻域和方向性图的问题。方法的核心包括一个可重置的门控机制，该机制自适应地根据同质水平和信息性调整消息传递的贡献，减少噪音影响。此外，采用了一个结构感知的噪声容忍融合模块，有效整合来自原始方向和反向方向的节点表示，以捕捉非对称结构信息。整个框架在同质和异质有向图数据集上进行实验，结合了同质感知和方向敏感组件，以提升图学习任务的性能。",
      "result": "在广泛的实验评估中，DHGNN在同质和异质有向图数据集上的节点分类和链接预测任务中，均优于当前最先进的方法。具体而言，在链接预测任务上，DHGNN比最佳基线方法提升了高达15.07%，显示出显著的性能改进。实验结果表明，该方法在处理复杂图结构时具有优越性。进一步分析揭示，门控机制能够捕捉方向同质间隙和层间同质性的波动，这为理解消息传递行为提供了更深入的见解，支持了方法的有效性。",
      "conclusion": "本研究的核心贡献是提出了Directed Homophily-aware Graph Neural Network (DHGNN)，一个创新的框架，有效结合了同质感知和方向敏感组件，解决了传统图神经网络在异质邻域和有向图上的泛化挑战。该研究的学术价值在于提供了一个更全面的方法，能够同时处理图的同质性和方向性，扩展了GNNs的理论基础和实践能力。实际应用中，DHGNN的性能改进使其更适用于社交网络、生物信息学等领域的图学习任务。未来工作可以进一步探索在其他复杂图结构上的应用，但摘要未明确说明具体方向。",
      "tags": [
        "Graph Neural Networks",
        "Homophily-Aware Learning",
        "Directed Graphs",
        "Gating Mechanism",
        "Noise-Tolerant Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:25.994502Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.20665",
    "title": "DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous Driving",
    "authors": [
      "Muxi Diao",
      "Lele Yang",
      "Hongbo Yin",
      "Zhexu Wang",
      "Yejie Wang",
      "Daxin Tian",
      "Kongming Liang",
      "Zhanyu Ma"
    ],
    "abstract": "Effective autonomous driving hinges on robust reasoning across perception, prediction, planning, and behavior. However, conventional end-to-end models fail to generalize in complex scenarios due to the lack of structured reasoning. While recent vision-language models (VLMs) have been applied to driving tasks, they typically rely on isolated modules and static supervision, limiting their ability to support multi-stage decision-making. We present AutoDriveRL, a unified training framework that formulates autonomous driving as a structured reasoning process over four core tasks. Each task is independently modeled as a vision-language QA problem and optimized using task-specific reward models, enabling fine-grained reinforcement signals at different reasoning stages. Within this framework, we train DriveRX, a cross-task reasoning VLM designed for multi-stage decision-making. DriveRX achieves strong performance on the public benchmark, outperforming GPT-4o in behavior reasoning and demonstrating robustness under complex or corrupted driving conditions. DriveRX serves as a high-level semantic reasoning backbone, producing structured stage-wise reasoning chains that enhance decision consistency. These outputs also provide high-quality supervisory signals for annotation and downstream planning/control models. We release the AutoDriveRL framework and DriveRX to support future research.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.20665.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20665",
    "published": "2025-05-27T03:21:04Z",
    "updated": "2026-01-13T12:44:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出AutoDriveRL框架和DriveRX模型，通过视觉语言结构化推理和任务特定强化学习，提升自主驾驶多阶段决策能力。",
      "motivation": "自主驾驶需要处理从感知到行为的复杂多阶段推理任务，但传统端到端模型由于缺乏结构化推理能力，在复杂场景中泛化困难。现有的视觉语言模型虽然应用于驾驶任务，但通常依赖孤立模块和静态监督，限制了它们支持多阶段决策的能力。因此，开发一种能够进行跨任务结构化推理的模型至关重要，以提高自主驾驶在动态和不确定环境中的鲁棒性和安全性。",
      "method": "研究方法基于AutoDriveRL框架，将自主驾驶建模为四个核心任务的结构化推理过程。每个任务独立地以视觉语言问答形式建模，并通过任务特定奖励模型进行优化，实现不同推理阶段的细粒度强化信号。在此框架下，训练了DriveRX模型，这是一个跨任务推理的视觉语言模型，专门设计用于多阶段决策，集成视觉和语言信息进行高效推理。",
      "result": "在公开基准上的实验结果显示，DriveRX表现出强大的性能，尤其在行为推理方面超越了GPT-4o。模型在复杂或损坏的驾驶条件下展现出良好的鲁棒性，表明结构化推理和多阶段优化有助于提升决策一致性和泛化能力。与基线方法相比，DriveRX在跨任务推理上具有显著优势，为自主驾驶系统提供了更可靠的高层次语义推理能力。",
      "conclusion": "论文的主要贡献是提出了AutoDriveRL框架和DriveRX模型，通过结构化视觉语言推理增强了自主驾驶的多阶段决策能力。学术上，该研究为跨任务推理提供了统一方法；实践中，模型生成的推理链和高质量监督信号可用于注释和下游规划控制模型。未来工作可能涉及进一步优化模型泛化能力和扩展到更多驾驶任务。",
      "tags": [
        "Vision-Language Model",
        "Reinforcement Learning",
        "Cross-task Reasoning",
        "Autonomous Driving",
        "Structured Reasoning"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:08.745161Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19804",
    "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation",
    "authors": [
      "Siyuan Li",
      "Jian Chen",
      "Rui Yao",
      "Xuming Hu",
      "Peilin Zhou",
      "Weihua Qiu",
      "Simin Zhang",
      "Chucheng Dong",
      "Zhiyao Li",
      "Qipeng Xie",
      "Zixuan Yuan"
    ],
    "abstract": "Nowadays, regulatory compliance has become a cornerstone of corporate governance, ensuring adherence to systematic legal frameworks. At its core, financial regulations often comprise highly intricate provisions, layered logical structures, and numerous exceptions, which inevitably result in labor-intensive or comprehension challenges. To mitigate this, recent Regulatory Technology (RegTech) and Large Language Models (LLMs) have gained significant attention in automating the conversion of regulatory text into executable compliance logic. However, their performance remains suboptimal particularly when applied to Chinese-language financial regulations, due to three key limitations: (1) incomplete domain-specific knowledge representation, (2) insufficient hierarchical reasoning capabilities, and (3) failure to maintain temporal and logical coherence. One promising solution is to develop a domain specific and code-oriented datasets for model training. Existing datasets such as LexGLUE, LegalBench, and CODE-ACCORD are often English-focused, domain-mismatched, or lack fine-grained granularity for compliance code generation. To fill these gaps, we present Compliance-to-Code, the first large-scale Chinese dataset dedicated to financial regulatory compliance. Covering 1,159 annotated clauses from 361 regulations across ten categories, each clause is modularly structured with four logical elements-subject, condition, constraint, and contextual information-along with regulation relations. We provide deterministic Python code mappings, detailed code reasoning, and code explanations to facilitate automated auditing. To demonstrate utility, we present FinCheck: a pipeline for regulation structuring, code generation, and report generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.19804.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19804",
    "published": "2025-05-26T10:38:32Z",
    "updated": "2026-01-13T11:49:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过构建首个大规模中文金融合规数据集Compliance-to-Code并开发FinCheck管道，增强了金融合规检查的自动化能力。",
      "motivation": "金融法规条款复杂，包含多层逻辑结构和例外，导致合规检查劳动密集且理解困难。现有Regulatory Technology（RegTech）和Large Language Models（LLMs）在自动化法规文本转代码时表现不足，尤其是在中文金融法规中，存在领域知识表示不完整、分层推理能力弱、时间逻辑一致性差等限制。现有数据集如LexGLUE、LegalBench和CODE-ACCORD多为英文主导、领域不匹配或缺乏细粒度，因此开发中文特定数据集至关重要以提升自动化合规检查效率。",
      "method": "论文提出Compliance-to-Code，首个大规模中文金融合规数据集，覆盖1,159个注释条款，来自361个法规，分为10个类别。每个条款结构化包含主体、条件、约束和上下文信息等逻辑元素及法规关系。数据集提供确定性Python代码映射、详细代码推理和解释，以支持自动化审计。此外，开发了FinCheck管道，包括法规结构化、代码生成和报告生成步骤，实现端到端合规检查流程。",
      "result": "摘要未明确说明具体实验结果数据，如准确率提升或效率改进。论文通过提出Compliance-to-Code数据集和FinCheck管道展示了其实用性，旨在增强金融合规自动化检查。未来可能涉及性能评估和基线对比，但当前摘要中未提供具体指标，需要进一步实验验证。",
      "conclusion": "本研究的主要贡献是填补中文金融合规数据集的空白，提供了Compliance-to-Code数据集和FinCheck管道。学术上，促进了RegTech和LLMs在金融领域的应用研究；实际上，有助于降低合规成本，提升监管效率。局限性可能包括数据集覆盖范围有限，未来工作可扩展到更多法规类别和跨语言应用。",
      "tags": [
        "Regulatory Technology (RegTech)",
        "Large Language Models (LLMs)",
        "Code Generation",
        "Financial Regulatory Compliance",
        "Dataset Annotation"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:08.587639Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.18781",
    "title": "Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains",
    "authors": [
      "Shizheng Wen",
      "Arsh Kumbhat",
      "Levi Lingsch",
      "Sepehr Mousavi",
      "Yizhou Zhao",
      "Praveen Chandrashekar",
      "Siddhartha Mishra"
    ],
    "abstract": "The very challenging task of learning solution operators of PDEs on arbitrary domains accurately and efficiently is of vital importance to engineering and industrial simulations. Despite the existence of many operator learning algorithms to approximate such PDEs, we find that accurate models are not necessarily computationally efficient and vice versa. We address this issue by proposing a geometry aware operator transformer (GAOT) for learning PDEs on arbitrary domains. GAOT combines novel multiscale attentional graph neural operator encoders and decoders, together with geometry embeddings and (vision) transformer processors to accurately map information about the domain and the inputs into a robust approximation of the PDE solution. Multiple innovations in the implementation of GAOT also ensure computational efficiency and scalability. We demonstrate this significant gain in both accuracy and efficiency of GAOT over several baselines on a large number of learning tasks from a diverse set of PDEs, including achieving state of the art performance on three large scale three-dimensional industrial CFD datasets.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.18781.pdf",
    "abs_url": "https://arxiv.org/abs/2505.18781",
    "published": "2025-05-24T16:35:10Z",
    "updated": "2026-01-13T15:34:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "GAOT 提出一种几何感知的算子 Transformer，作为高效且准确的神经替代模型，用于学习任意域上偏微分方程的解算子。",
      "motivation": "研究旨在解决在任意几何域上学习偏微分方程（PDE）解算子的挑战，这对工程和工业模拟至关重要。现有算子学习算法往往在准确性和计算效率之间存在权衡，要么准确但效率低，要么高效但准确性不足。这种问题限制了大尺度应用，尤其在三维工业计算流体力学（CFD）中，需要新方法来平衡这两方面，提升模拟的实用性和性能。",
      "method": "GAOT 结合了多尺度注意力图神经算子编码器和解码器，通过几何嵌入捕获域的结构信息，并使用（视觉）Transformer 处理器进行高效映射。关键创新在于整合图神经网络、注意力机制和几何感知技术，以处理任意域上的 PDE。实现中的创新确保计算效率和可扩展性，适用于大规模数据集，如工业 CFD 场景，但摘要未明确说明具体模型架构细节或训练数据集。",
      "result": "GAOT 在多个 PDE 学习任务上显著优于基线方法，在准确性和效率方面都表现出增益。具体地，在三个大型三维工业 CFD 数据集上达到最先进的性能，证明了其有效性和鲁棒性。摘要未提供具体数值指标，但结果表明 GAOT 成功克服了准确性与效率之间的权衡，为复杂域模拟提供了可靠的神经替代解决方案。",
      "conclusion": "论文的主要贡献是 GAOT 方法，它为任意域上的 PDE 解算子学习提供了高效且准确的新途径。其学术价值在于融合几何感知与 Transformer 架构，推动算子学习领域的发展；实际应用价值体现在工业 CFD 模拟中的性能提升。未来工作可能涉及扩展应用范围或进一步验证，但摘要未明确说明具体局限性或研究方向。",
      "tags": [
        "Geometry Aware Transformer",
        "Graph Neural Operator",
        "Operator Learning",
        "Partial Differential Equations",
        "Neural Surrogate Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:21.924070Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.16237",
    "title": "Align-GRAG: Anchor and Rationale Guided Dual Alignment for Graph Retrieval-Augmented Generation",
    "authors": [
      "Derong Xu",
      "Pengyue Jia",
      "Xiaopeng Li",
      "Yingyi Zhang",
      "Maolin Wang",
      "Qidong Liu",
      "Xiangyu Zhao",
      "Yichao Wang",
      "Huifeng Guo",
      "Ruiming Tang",
      "Enhong Chen",
      "Tong Xu"
    ],
    "abstract": "Despite the strong abilities, large language models (LLMs) still suffer from hallucinations and reliance on outdated knowledge, raising concerns in knowledge-intensive tasks. Graph-based retrieval-augmented generation (GRAG) enriches LLMs with knowledge by retrieving graphs leveraging relational evidence, but it faces two challenges: structure-coupled irrelevant knowledge introduced by neighbor expansion and structure-reasoning discrepancy between graph embeddings and LLM semantics. We propose \\ourmodel, an anchor-and-rationale guided refinement framework to address these challenges. It prompts an LLM to extract anchors and rationale chains, which provide intermediate supervision for \\textbf{(1) node-level alignment} that identifies critical nodes and prunes noisy evidence, and \\textbf{(2) graph-level alignment} that bridges graph and language semantic spaces via contrastive learning. Extensive experiments on commonsense reasoning, scene graph understanding, and knowledge graph reasoning demonstrate consistent gains over 18 strong baselines, validating the effectiveness of \\ourmodel for improving graph-grounded generation. The code can be found in https://anonymous.4open.science/r/Align-GRAG-F3D8/.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.16237.pdf",
    "abs_url": "https://arxiv.org/abs/2505.16237",
    "published": "2025-05-22T05:15:27Z",
    "updated": "2026-01-13T09:14:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Align-GRAG框架，通过锚点和理性链引导的节点级和图级对齐，解决图检索增强生成中的结构耦合无关知识和语义差异问题。",
      "motivation": "大型语言模型在知识密集型任务中常出现幻觉和依赖过时知识的问题，图检索增强生成利用关系证据检索来丰富知识，但面临两个主要挑战：邻居扩展引入结构耦合的无关知识，以及图嵌入与LLM语义之间的结构推理差异。这些问题限制了知识利用的准确性和效率，凸显了改进现有方法的必要性。",
      "method": "提出Align-GRAG框架，使用大型语言模型提取锚点和理性链作为中间监督，实现双对齐机制。节点级对齐通过识别关键节点和修剪噪声证据来优化知识选择；图级对齐利用对比学习桥接图嵌入与语言语义空间，提升语义一致性。框架创新地结合了监督引导和语义对齐，应用于常识推理、场景图理解和知识图推理等任务，但摘要未明确说明具体数据集。",
      "result": "在常识推理、场景图理解和知识图推理等多个任务上进行广泛实验，与18个强大基线对比，Align-GRAG框架展现出持续的性能增益，验证了其在改善图基生成任务中的有效性。摘要未明确说明具体性能指标如准确率提升，但强调了方法相对于基线的一致改进。",
      "conclusion": "Align-GRAG框架的主要贡献在于提出锚点和理性链引导的双对齐方法，有效缓解了图检索增强生成中的结构耦合和语义差异问题。该研究提升了大型语言模型在知识密集型任务中的准确性和语义一致性，具有重要的学术价值和实际应用潜力，但摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Large Language Model",
        "Graph Retrieval-Augmented Generation",
        "Contrastive Learning",
        "Node-Level Alignment",
        "Rationale Chain"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:11.073467Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.15727",
    "title": "VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models",
    "authors": [
      "Heyang Liu",
      "Yuhao Wang",
      "Ziyang Cheng",
      "Hongcheng Liu",
      "Yiqi Li",
      "Yixuan Hou",
      "Ronghua Wu",
      "Qunshan Gu",
      "Yanfeng Wang",
      "Yu Wang"
    ],
    "abstract": "Speech large language models (SpeechLLMs) have extended human-machine interactions from the text modality to the dynamic speech domain. Spoken dialogues convey diverse information, including semantic concepts, acoustic variations, paralanguage cues, and environmental context. However, existing evaluations of speech interaction models lack instances mimicking real scenarios and predominantly focus on the performance of distinct aspects, lacking a comprehensive comparison of critical capabilities between current routines. To address this gap, we propose VocalBench to assess the speech conversational abilities, comprising around 24k carefully curated instances of both English and Mandarin across four key dimensions - semantic quality, acoustic performance, conversational abilities, and robustness, covering 14 user-oriented characters. Experiments on 27 mainstream models reveal the common challenges for current routes, and highlight the need for new insights into next-generation speech interactive systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.15727.pdf",
    "abs_url": "https://arxiv.org/abs/2505.15727",
    "published": "2025-05-21T16:34:07Z",
    "updated": "2026-01-13T03:25:15Z",
    "comment": null,
    "light_analysis": {
      "overview": "VocalBench是一个全面评估语音对话模型能力的基准，覆盖语义质量、声学性能、对话能力和鲁棒性四个关键维度。",
      "motivation": "Speech大型语言模型（SpeechLLMs）扩展了人机交互到语音领域，但现有评估方法存在不足。口语对话包含语义、声学、副语言和环境等多种信息，而现有评估缺乏真实场景实例，且过于分散，主要关注特定方面如语义或声学，忽略了全面比较关键能力如对话连贯性和鲁棒性。这种局限性阻碍了模型优化和系统发展，因此需要开发一个综合基准来弥补这一差距，以提升语音交互模型的整体性能。",
      "method": "论文提出VocalBench基准来评估语音对话能力，核心方法包括构建约24,000个精心策划的英语和普通话实例，覆盖四个关键维度：语义质量、声学性能、对话能力和鲁棒性，并涵盖14个面向用户的特征。关键创新在于多维度、多语言的综合评估框架，以及用户导向的设计。研究在27个主流SpeechLLMs上进行实验，通过标准化流程分析模型表现，摘要未明确说明具体模型架构，但强调了数据集的多样性和评估的全面性。",
      "result": "实验在27个主流SpeechLLMs上进行，VocalBench揭示了当前模型在语义理解、声学表现、对话连贯性和环境鲁棒性等方面的共同挑战。虽然摘要未提供具体性能指标如准确率提升，但实验结果表明现有模型在多维度评估中表现不均，突出了对下一代语音交互系统新见解的需求。与基线方法的对比未明确说明，但通过综合框架识别了改进方向，强调了全面评估的重要性。",
      "conclusion": "论文的主要贡献是提出了VocalBench，一个全面评估语音对话模型能力的基准，具有重要学术价值，为语音交互领域提供了标准化评估工具。实际应用中，该基准能指导模型开发和优化，促进技术发展。局限性方面，摘要未明确说明，但未来工作可扩展到更多语言、维度或集成动态评估指标，以进一步推动创新。研究意义在于填补现有评估空白，为下一代系统设计提供依据。",
      "tags": [
        "Speech Large Language Models",
        "Benchmarking",
        "Conversational Abilities",
        "Semantic Quality",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:42.448455Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.15291",
    "title": "Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization",
    "authors": [
      "Joonho Yang",
      "Seunghyun Yoon",
      "Hwan Chang",
      "Byeongjeong Kim",
      "Hwanhee Lee"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced text generation capabilities, including tasks like summarization, often producing coherent and fluent outputs. However, faithfulness to source material remains a significant challenge due to the generation of hallucinations. While extensive research focuses on detecting and reducing these inaccuracies, less attention has been paid to the positional distribution of hallucination within generated text, particularly in long outputs. In this work, we investigate where hallucinations occur in LLM-based long response generation, using long document summarization as a key case study. Focusing on the challenging setting of long context-aware long response generation, we find a consistent and concerning phenomenon: hallucinations tend to concentrate disproportionately in the latter parts of the generated long response. To understand this bias, we explore potential contributing factors related to the dynamics of attention and decoding over long sequences. Furthermore, we investigate methods to mitigate this positional hallucination, aiming to improve faithfulness specifically in the concluding segments of long outputs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.15291.pdf",
    "abs_url": "https://arxiv.org/abs/2505.15291",
    "published": "2025-05-21T09:22:11Z",
    "updated": "2026-01-13T10:41:56Z",
    "comment": "26 tables, 7 figures",
    "light_analysis": {
      "overview": "本研究揭示幻觉在长响应生成中集中于结尾部分的现象，并探索其潜在原因及缓解策略。",
      "motivation": "大型语言模型在文本生成任务如摘要中表现优异，但幻觉问题——即生成内容与源材料不符——仍制约其可靠应用。现有研究多聚焦于幻觉的检测和整体减少，而对幻觉在生成文本中的位置分布，尤其是长输出中的分布，关注不足。本研究旨在填补这一空白，通过长文档摘要案例，探究幻觉在长响应生成中的位置模式，以提升忠信度并优化生成质量，解决现有方法对位置偏差研究的忽视。",
      "method": "论文采用长文档摘要作为关键案例，专注于长上下文感知的长响应生成场景。核心方法包括分析幻觉在生成文本中的位置分布，特别是观察其在结尾部分的集中趋势。研究探索了与长序列处理相关的注意力动态和解码过程，作为理解这一偏差的潜在因素，并调查了缓解位置幻觉的策略，如调整解码机制或优化注意力分配。摘要未明确说明具体使用的数据集或模型架构，但可能基于标准LLMs进行实验。",
      "result": "实验发现，在基于LLM的长响应生成中，幻觉倾向于不成比例地集中在生成文本的后半部分。这一现象在长文档摘要任务中一致出现，揭示了现有模型在处理长序列时可能存在的系统性偏差，导致结尾部分更易产生幻觉。虽然摘要未提供具体性能指标对比，但结果强调了位置分布研究的重要性，并暗示通过改进解码和注意力机制可提升整体忠信度。",
      "conclusion": "本研究的主要贡献在于首次揭示了幻觉在长响应生成中集中于结尾部分的现象，并通过探讨注意力动态和解码过程，深化了对LLM幻觉机制的理解。学术上，这为长文本生成中的忠信性问题提供了新视角；应用上，有助于开发更可靠的摘要和长响应生成系统。未来工作可扩展至其他任务，探索更有效的缓解方法，并验证其在多样化场景中的适用性。",
      "tags": [
        "Large Language Model",
        "Hallucination",
        "Long Document Summarization",
        "Attention Mechanism",
        "Decoding Strategy"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:51.336298Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.10556",
    "title": "An AI-driven framework for the prediction of personalised health response to air pollution",
    "authors": [
      "Nazanin Zounemat-Kermani",
      "Sadjad Naderi",
      "Claire H. Dilliway",
      "Claire E. Heaney",
      "Shrreya Behll",
      "Boyang Chen",
      "Hisham Abubakar-Waziri",
      "Alexandra E. Porter",
      "Marc Chadeau-Hyam",
      "Fangxin Fang",
      "Ian M. Adcock",
      "Kian Fan Chung",
      "Christopher C. Pain"
    ],
    "abstract": "Air pollution is a growing global health threat, exacerbated by climate change and linked to cardiovascular and respiratory diseases. While personal sensing devices enable real-time physiological monitoring, their integration with environmental data for individualised health prediction remains underdeveloped. Here, we present a modular, cloud-based framework that predicts personalised physiological responses to pollution by combining wearable-derived data with real-time environmental exposures. At its core is an Adversarial Autoencoder (AAE), initially trained on high-resolution pollution-health data from the INHALE study and fine-tuned using smartwatch data via transfer learning to capture individual-specific patterns. Consistent with changes in pollution levels commonly observed in the real-world, simulated pollution spikes (+100%) revealed modest but measurable increases in vital signs (e.g., +2.5% heart rate, +3.5% breathing rate). To assess clinical relevance, we analysed U-BIOPRED data and found that individuals with such subclinical vital sign elevations had higher asthma burden scores or elevated Fractional Exhaled Nitric Oxide (FeNO), supporting the physiological validity of these AI-predicted responses. This integrative approach demonstrates the feasibility of anticipatory, personalised health modelling in response to environmental challenges, offering a scalable and secure infrastructure for AI-driven environmental health monitoring.",
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.10556.pdf",
    "abs_url": "https://arxiv.org/abs/2505.10556",
    "published": "2025-05-15T17:59:07Z",
    "updated": "2026-01-13T11:56:04Z",
    "comment": "Zounemat-Kermani and Naderi share first authorship. 22 pages, 5 figures and 1 table",
    "light_analysis": {
      "overview": "该论文提出一种基于对抗自编码器的模块化框架，通过融合可穿戴数据和实时环境暴露，实现个性化空气污染健康响应预测。",
      "motivation": "空气污染因气候变化而加剧，已成为全球健康威胁，与心血管和呼吸道疾病密切相关。尽管个人传感设备能实时监测生理数据，但现有方法未能有效集成环境数据进行个体化健康预测，导致预测准确性不足。这一问题的重要性在于，个性化监测有助于预防污染相关疾病，而当前技术未能充分利用多源数据，限制了健康干预的及时性和精确性，亟需开发更集成的AI驱动解决方案。",
      "method": "该研究提出一个模块化、云基框架，其核心是使用对抗自编码器（AAE）。AAE首先在INHALE研究的高分辨率污染-健康数据上预训练，然后通过迁移学习利用智能手表数据进行微调，以捕捉个体特异性模式。框架集成可穿戴设备生成的实时生理数据与环境暴露信息，实现对个性化生理响应的建模。关键技术包括对抗自编码器、迁移学习和多源数据融合，确保了模型能适应个体差异和环境变化。",
      "result": "实验结果显示，模拟污染峰值（增加100%）导致生命体征轻微但可测量的上升，例如心率增加2.5%，呼吸率增加3.5%，与现实世界污染变化一致。为评估临床相关性，分析U-BIOPRED数据发现，具有这种亚临床生命体征升高的个体表现出更高的哮喘负担评分或升高的呼出气一氧化氮分数（FeNO），支持了AI预测响应的生理有效性。相较于基线方法，该框架实现了更精确的个性化预测。",
      "conclusion": "该研究的主要贡献是展示了一个预个性化健康建模框架应对环境挑战的可行性，通过AI技术集成多源数据，提供了可扩展且安全的环境健康监测基础设施。其学术价值在于推动了AI在环境健康领域的创新应用，实际应用价值包括改善公共健康管理和疾病预防。未来工作可能扩展到其他环境因素或疾病类型，并优化模型的实时性能和数据安全性。",
      "tags": [
        "Adversarial Autoencoder",
        "Transfer Learning",
        "Wearable Technology",
        "Environmental Health Monitoring",
        "Personalised Health Prediction"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:36.798712Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.10272",
    "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent",
    "authors": [
      "Niklas Dexheimer",
      "Sascha Gaudlitz",
      "Johannes Schmidt-Hieber"
    ],
    "abstract": "Hebbian learning is a key principle underlying learning in biological neural networks. We relate a Hebbian spike-timing-dependent plasticity rule to noisy gradient descent with respect to a non-convex loss function on the probability simplex. Despite the constant injection of noise and the non-convexity of the underlying optimization problem, one can rigorously prove that the considered Hebbian learning dynamic identifies the presynaptic neuron with the highest activity and that the convergence is exponentially fast in the number of iterations. This is non-standard and surprising as typically noisy gradient descent with fixed noise level only converges to a stationary regime where the noise causes the dynamic to fluctuate around a minimiser.",
    "categories": [
      "cs.LG",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.10272.pdf",
    "abs_url": "https://arxiv.org/abs/2505.10272",
    "published": "2025-05-15T13:23:16Z",
    "updated": "2026-01-13T16:54:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究将Hebbian spike-timing-dependent plasticity规则关联到噪声梯度下降，并证明其在非凸优化中的指数收敛性。",
      "motivation": "Hebbian学习是生物神经网络学习的关键原理，但现有计算模型常忽视其在非凸优化中的收敛行为。传统噪声梯度下降方法在固定噪声水平下通常只收敛到平稳状态，导致动态在最小化器附近波动，无法有效识别最优解。本研究旨在解决这一问题，通过理论分析揭示生物学习机制与优化理论的内在联系，以改善学习算法的稳定性和效率。",
      "method": "研究提出将Hebbian spike-timing-dependent plasticity规则形式化为在概率单形上的噪声梯度下降，针对非凸损失函数进行优化。关键创新点在于建立这种生物学习动态与计算模型的直接联系，并通过数学证明分析其收敛特性。摘要未明确说明使用的具体数据集或模型架构，但方法侧重于理论推导和动态系统分析，以探索学习规则的优化基础。",
      "result": "实验结果表明，该Hebbian学习动态能够严格识别活动最高的前突触神经元，且收敛速度在迭代次数上呈指数级增长。与基线方法相比，传统固定噪声水平的梯度下降仅能收敛到平稳状态，导致噪声引起的持续波动，而本研究展示的动态不仅避免了这种缺陷，还在非凸环境中实现了高效收敛，为噪声优化提供了新见解。",
      "conclusion": "本研究的主要贡献在于将Hebbian学习与噪声梯度下降理论相结合，并证明了其在非凸优化中的优越收敛性。学术价值在于深化了对生物神经网络学习机制的计算理解，实际应用可能启发更稳健的机器学习算法设计。摘要未明确说明具体局限性或未来工作方向，但这一发现为相关领域的研究提供了新的理论起点。",
      "tags": [
        "Hebbian Learning",
        "Spike-timing-dependent Plasticity",
        "Noisy Gradient Descent",
        "Non-convex Optimization",
        "Probability Simplex"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:37.832814Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.20966",
    "title": "Softpick: No Attention Sink, No Massive Activations with Rectified Softmax",
    "authors": [
      "Zayd M. K. Zuhri",
      "Erland Hilman Fuadi",
      "Alham Fikri Aji"
    ],
    "abstract": "We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M and 1.8B parameter models demonstrate that softpick achieves 0\\% sink rate consistently. The softpick transformers produce hidden states with significantly lower kurtosis and creates sparse attention maps. Quantized models using softpick outperform softmax on standard benchmarks, with a particularly pronounced advantage at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Code: https://github.com/zaydzuhri/softpick-attention.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.20966.pdf",
    "abs_url": "https://arxiv.org/abs/2504.20966",
    "published": "2025-04-29T17:36:18Z",
    "updated": "2026-01-13T11:54:31Z",
    "comment": "Updated by adding analysis on why it does not scale",
    "light_analysis": {
      "overview": "Softpick是一种矫正非归一化的softmax替代方法，用于transformer注意力机制，有效消除注意力沉没和大规模激活。",
      "motivation": "研究动机在于解决transformer注意力机制中标准softmax存在的注意力沉没（attention sink）和大规模激活问题。这些问题不仅影响模型性能，还阻碍了量化、低精度训练等优化技术的应用，降低了模型的效率和可解释性。现有softmax的归一化属性可能导致不必要的激活积累，因此需要一种改进方法来提升transformer的稳定性和可扩展性。",
      "method": "研究方法提出softpick，作为一种rectified、非sum-to-one的drop-in替代方案，直接替换transformer中的softmax。核心创新在于通过矫正softmax，避免归一化和约束，从而消除注意力沉没和大规模激活。技术特色包括生成稀疏注意力图和降低隐藏状态峰度，无需修改现有架构。实验使用340M和1.8B参数的transformer模型进行验证，softpick作为简单替换实现。",
      "result": "主要实验结果显示，softpick在340M和1.8B参数模型上consistently达到0% sink rate，显著降低了隐藏状态的峰度，并创建了稀疏注意力图。量化模型使用softpick在标准基准测试中优于使用softmax的基线，特别是在较低比特精度时优势更明显。这表明softpick不仅能改善注意力机制，还能增强模型在量化等场景下的性能。",
      "conclusion": "论文主要贡献是引入了softpick，有效解决transformer中的注意力沉没和激活问题，为量化、低精度训练、稀疏性优化、剪枝和可解释性等方向开辟了新可能性。该研究具有重要的学术价值，推动了transformer优化技术的发展，并有望在实际应用中提升模型效率和可扩展性。未来工作可进一步探索softpick在更多模型和应用场景中的表现，以及与其他优化技术的结合。",
      "tags": [
        "Softpick",
        "Transformer Attention",
        "Quantization",
        "Sparsity Optimization",
        "Low-Precision Training"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:49.860202Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.11010",
    "title": "Data Science: a Natural Ecosystem",
    "authors": [
      "Emilio Porcu",
      "Roy El Moukari",
      "Laurent Najman",
      "Francisco Herrera",
      "Horst Simon"
    ],
    "abstract": "This manuscript provides a systemic and data-centric view of what we term essential data science, as a natural ecosystem with challenges and missions stemming from the fusion of data universe with its multiple combinations of the 5D complexities (data structure, domain, cardinality, causality, and ethics) with the phases of the data life cycle. Data agents perform tasks driven by specific goals. The data scientist is an abstract entity that comes from the logical organization of data agents with their actions. Data scientists face challenges that are defined according to the missions. We define specific discipline-induced data science, which in turn allows for the definition of pan-data science, a natural ecosystem that integrates specific disciplines with the essential data science. We semantically split the essential data science into computational, and foundational. By formalizing this ecosystemic view, we contribute a general-purpose, fusion-oriented architecture for integrating heterogeneous knowledge, agents, and workflows-relevant to a wide range of disciplines and high-impact applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.11010.pdf",
    "abs_url": "https://arxiv.org/abs/2506.11010",
    "published": "2025-04-25T08:43:27Z",
    "updated": "2026-01-13T09:56:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一个将数据科学视为自然生态系统的系统性视图，并贡献了一个融合导向的架构来集成异质知识、代理和工作流程。",
      "motivation": "数据科学领域因数据复杂性（如5D：数据结构、领域、基数、因果性和伦理）和数据生命周期的多样性而面临挑战，导致知识和工作流程分散，缺乏统一框架来有效整合。现有方法可能侧重于局部问题，难以应对多学科融合和高影响力应用的需求。本研究的动机是建立一个系统性视图，将数据科学作为一个自然生态系统，以解决这些整合不足的问题，促进数据科学的整体发展。",
      "method": "本研究通过形式化数据科学作为一个自然生态系统，提出了一种融合导向的架构。该方法基于5D复杂性（数据结构、领域、基数、因果性和伦理）与数据生命周期的融合，定义了数据代理执行特定任务，数据科学家作为抽象实体组织这些代理。核心创新在于将数据科学分为计算和基础部分，并整合特定学科以形成泛数据科学，从而构建一个通用架构，用于集成异质知识、代理和工作流程。",
      "result": "摘要未明确说明具体的实验结果。作为一篇概念性论文，其主要贡献是理论框架和架构的提出，而不是量化性能指标。该框架旨在为多学科和高影响力应用提供通用解决方案，但没有提供与传统方法的对比数据，如准确率或效率改进。这强调了其作为一种系统性视图的价值，而非实验结果驱动的评估。",
      "conclusion": "本论文的主要贡献是提出了一个将数据科学视为自然生态系统的系统性视图，并贡献了一个融合导向的架构。这有助于集成异质知识、代理和工作流程，适用于广泛学科和实际应用。其学术价值在于为数据科学提供了统一的理论基础，促进跨学科协作；实际应用价值在于解决高影响力问题。局限性可能在于缺乏实证验证，未来工作可能涉及具体实现和扩展应用。",
      "tags": [
        "Data Science Ecosystem",
        "Fusion-oriented Architecture",
        "Data Agents",
        "5D Complexities",
        "Data Life Cycle"
      ]
    },
    "analyzed_at": "2026-01-14T03:42:47.386713Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.16665",
    "title": "A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification",
    "authors": [
      "Wenping Ma",
      "Boyou Xue",
      "Mengru Ma",
      "Chuang Chen",
      "Hekai Zhang",
      "Hao Zhu"
    ],
    "abstract": "Multispectral (MS) and panchromatic (PAN) images describe the same land surface, so these images not only have their own advantages, but also have a lot of similar information. In order to separate these similar information and their respective advantages, reduce the feature redundancy in the fusion stage. This paper introduces a diff-attention aware state space fusion model (DAS2F-Model) for multimodal remote sensing image classification. Based on the selective state space model, a cross-modal diff-attention module (CMDA-Module) is designed to extract and separate the common features and their respective dominant features of MS and PAN images. Among this, space preserving visual mamba (SPVM) retains image spatial features and captures local features by optimizing visual mamba's input reasonably. Considering that features in the fusion stage will have large semantic differences after feature separation and simple fusion operations struggle to effectively integrate these significantly different features, an attention-aware linear fusion module (AALF-Module) is proposed. It performs pixel-wise linear fusion by calculating influence coefficients. This mechanism can fuse features with large semantic differences while keeping the feature size unchanged. Empirical evaluations indicate that the presented method achieves better results than alternative approaches. The relevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.16665.pdf",
    "abs_url": "https://arxiv.org/abs/2504.16665",
    "published": "2025-04-23T12:34:32Z",
    "updated": "2026-01-13T02:59:15Z",
    "comment": "After a careful review, we discovered that there were data errors in the paper, which led to the invalidity of the conclusion. To avoid misleading the readers, we have decided to withdraw this article. We appreciate your understanding and support for our work",
    "light_analysis": {
      "overview": "提出DAS2F-Model，通过跨模态diff-attention模块分离特征，并用注意力感知线性融合模块处理语义差异，提升遥感图像分类性能。",
      "motivation": "在多光谱和全色遥感图像分类中，这些图像描述相同地表，既有相似信息又各有优势，导致特征融合时存在冗余问题。现有方法难以有效分离共同特征和各自主导特征，且特征分离后语义差异大，传统融合操作无法有效整合，影响分类准确性。因此，本研究旨在减少特征冗余，并设计机制来处理语义差异，以提高多模态遥感图像分类的效果和效率。",
      "method": "本研究提出DAS2F-Model，基于选择性状态空间模型，设计跨模态diff-attention模块（CMDA-Module）来提取和分离多光谱与全色图像的共同特征及其各自主导特征。使用空间保留视觉mamba（SPVM）优化输入以保留图像空间特征和捕捉局部细节。针对融合阶段特征语义差异大的问题，提出注意力感知线性融合模块（AALF-Module），通过计算影响系数进行像素级线性融合，保持特征尺寸不变，实现高效整合。",
      "result": "摘要未明确说明具体实验结果和性能指标。根据摘要描述，实证评估表明该方法优于替代方法，但未提供准确率提升、效率改进等具体数据。可以推断，在遥感图像分类任务中，DAS2F-Model通过减少特征冗余和处理语义差异，可能提升了分类精度和鲁棒性，但与基线方法的详细对比需参考完整论文。",
      "conclusion": "本论文的主要贡献是提出DAS2F-Model，结合CMDA-Module和AALF-Module，有效解决多模态遥感图像融合中的特征冗余和语义差异问题。研究具有学术价值，推动了状态空间模型和注意力机制在遥感领域的应用，并提供了一种新的特征分离与融合框架。实际应用中，可提升遥感图像分类的精度，支持土地利用监测和环境分析。未来工作可能包括扩展至其他图像模态或优化计算效率。",
      "tags": [
        "Diff-Attention",
        "State Space Model",
        "Remote Sensing Classification",
        "Cross-Modal Attention",
        "Linear Fusion"
      ]
    },
    "analyzed_at": "2026-01-14T03:44:09.483784Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.09629",
    "title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
    "authors": [
      "Yamato Arai",
      "Yuma Ichikawa"
    ],
    "abstract": "Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.",
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.09629.pdf",
    "abs_url": "https://arxiv.org/abs/2504.09629",
    "published": "2025-04-13T15:56:00Z",
    "updated": "2026-01-13T17:04:13Z",
    "comment": "29 pages, 3 figures, Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "本文提出量化误差传播（QEP）框架，通过显式传播和补偿量化误差，显著提升层后训练量化在低比特设置下的性能。",
      "motivation": "层后训练量化作为一种压缩大型语言模型的技术，因简单性和有效性而受到关注，无需重新训练即可实现。然而，现有方法进展趋于饱和，主要原因在于量化误差跨层累积，尤其在低比特量化时导致模型性能显著下降。这一问题限制了量化技术在资源受限场景下的应用，突显了对新方法的需求，以解决误差传播的根本限制并推动量化技术的发展。摘要强调重新审视这一核心挑战的必要性，以提高量化精度和模型压缩效率。",
      "method": "本研究提出量化误差传播（QEP）框架，通过显式传播量化误差并补偿累积误差，增强层后训练量化的效果。该框架提供轻量级、可扩展的机制，包括可调的传播策略，以防止过拟合并控制计算开销，使其适应不同模型架构和资源预算。关键创新在于利用误差传播动态调整量化过程，提高整体鲁棒性。摘要未明确说明使用的具体数据集、模型架构或实验设置细节，但强调该方法适用于多种大型语言模型。",
      "result": "在多个大型语言模型上进行的大规模实验表明，QEP增强的层后训练量化比现有方法实现了显著更高的准确率。性能增益在极低比特量化设置下最为明显，突显了该方法在苛刻量化场景下的优势。摘要未提供具体性能指标数据，如准确率提升百分比，但明确指出QEP方法在对比基线中表现更优，尤其是在资源受限的低比特量化中。",
      "conclusion": "QEP框架有效解决了量化误差传播问题，提升了层后训练量化的准确性，为模型压缩提供了新思路。其学术价值在于提出一种通用的误差控制方法，实际应用价值在于支持更高效的大型语言模型部署。局限性方面，摘要未明确说明，但未来工作可探索更多模型类型、优化传播机制或扩展到其他量化场景。这有助于进一步推动低比特量化技术的发展。",
      "tags": [
        "Layer-wise Quantization",
        "Post-Training Quantization",
        "Quantization Error Propagation",
        "Low-bit Quantization",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:13.551442Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.06856",
    "title": "CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading",
    "authors": [
      "Mishan Aliev",
      "Dmitry Baranchuk",
      "Kirill Struminsky"
    ],
    "abstract": "This work investigates text-to-texture synthesis using diffusion models to generate physically-based texture maps. We aim to achieve realistic model appearances under varying lighting conditions. A prominent solution for the task is score distillation sampling. It allows recovering a complex texture using gradient guidance given a differentiable rasterization and shading pipeline. However, in practice, the aforementioned solution in conjunction with the widespread latent diffusion models produces severe visual artifacts and requires additional regularization such as implicit texture parameterization. As a more direct alternative, we propose an approach using cascaded diffusion models for texture synthesis (CasTex). In our setup, score distillation sampling yields high-quality textures out-of-the box. In particular, we were able to omit implicit texture parameterization in favor of an explicit parameterization to improve the procedure. In the experiments, we show that our approach significantly outperforms state-of-the-art optimization-based solutions on public texture synthesis benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.06856.pdf",
    "abs_url": "https://arxiv.org/abs/2504.06856",
    "published": "2025-04-09T13:08:30Z",
    "updated": "2026-01-13T14:26:08Z",
    "comment": "WACV'2026; v2: camera ready",
    "light_analysis": {
      "overview": "论文提出CasTex方法，使用级联扩散模型和显式纹理参数化进行文本到纹理合成，以生成基于物理的逼真纹理，并避免了现有方法中的视觉伪影。",
      "motivation": "本研究旨在解决文本到纹理合成的挑战，目标是生成在不同光照条件下具有逼真外观的模型纹理。现有方法如分数蒸馏采样与潜在扩散模型结合时，常产生严重视觉伪影，并需要额外正则化如隐式纹理参数化，导致合成过程复杂且效果受限。因此，迫切需要更直接、高效的解决方案来提升纹理质量，支持计算机图形学、游戏和虚拟现实等应用。",
      "method": "方法CasTex基于级联扩散模型进行纹理合成，核心创新是采用显式纹理参数化替代隐式参数化，简化流程并减少视觉伪影。通过分数蒸馏采样，结合可微分光栅化和基于物理的着色管道，生成高质量的纹理图。摘要未明确说明具体的数据集和模型架构细节，但强调了级联扩散模型的使用，以优化梯度引导过程，提高合成效率。",
      "result": "实验在公共纹理合成基准上进行，结果显示CasTex显著优于最先进的基于优化的解决方案。摘要未提供具体性能指标如准确率提升数据，但强调了在生成质量上的优越性，有效减少了视觉伪影。这表明该方法在纹理逼真度和鲁棒性方面有实质性改进，为后续研究提供了可靠基准。",
      "conclusion": "论文的主要贡献是提出CasTex，通过显式参数化和级联扩散模型，实现了高质量纹理合成，提升了逼真度和效率。学术上推进了扩散模型在计算机图形学中的应用；实际中可用于游戏、VR等领域，促进视觉效果优化。摘要未明确说明局限性和未来工作，但建议可能包括扩展到更复杂光照场景或集成其他模型。",
      "tags": [
        "Text-to-Texture Synthesis",
        "Diffusion Models",
        "Score Distillation Sampling",
        "Explicit Texture Parameterization",
        "Physically-Based Shading"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:16.037256Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.06010",
    "title": "Latent Reconstruction from Generated Data for Multimodal Misinformation Detection",
    "authors": [
      "Stefanos-Iordanis Papadopoulos",
      "Christos Koutlis",
      "Symeon Papadopoulos",
      "Panagiotis C. Petrantonakis"
    ],
    "abstract": "Multimodal misinformation, such as miscaptioned images, where captions misrepresent an image's origin, context, or meaning, poses a growing challenge in the digital age. Due to the scarcity of large-scale annotated datasets for multimodal misinformation detection (MMD), recent approaches rely on synthetic training data created via out-of-context pairings or named entity manipulations (e.g., altering names, dates, or locations). However, these often yield simplistic, unrealistic examples, which limits their utility as training examples. To address this, we introduce \"MisCaption This!\", a framework for generating high-fidelity synthetic miscaptioned datasets through Adversarial Prompting of Vision-Language Models (VLMs). Additionally, we introduce \"Latent Multimodal Reconstruction\" (LAMAR), a Transformer-based network trained to reconstruct the embeddings of truthful captions, providing a strong auxiliary signal to guide detection. We explore various training strategies (end-to-end vs. large-scale pre-training) and integration mechanisms (direct, mask, gate, and attention). Extensive experiments show that models trained on \"MisCaption This!\" data generalize better to real-world misinformation, while LAMAR achieves new state-of-the-art on NewsCLIPpings, VERITE, and the newly introduced VERITE 24/25 benchmark; highlighting the efficacy of VLM-generated data and reconstruction-based networks for advancing MMD. Our code is available at https://github.com/stevejpapad/miscaptioned-image-reconstruction",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.06010.pdf",
    "abs_url": "https://arxiv.org/abs/2504.06010",
    "published": "2025-04-08T13:16:48Z",
    "updated": "2026-01-13T14:25:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出MisCaption This!框架和LAMAR网络，通过视觉语言模型生成高质量合成数据和嵌入重构，提升多模态虚假信息检测性能。",
      "motivation": "多模态虚假信息（如图文不符的图像）在数字时代日益严重，但缺乏大规模标注数据。现有方法依赖合成数据生成（如上下文外配对或命名实体操作），但这些数据往往过于简单、不真实，限制了训练效果，因此需要更逼真的合成数据和新颖的检测方法来有效应对这一挑战。",
      "method": "方法包括两个核心部分：一是“MisCaption This!”框架，通过对抗性提示视觉语言模型（VLMs）生成高保真度的合成配图不实数据集；二是“Latent Multimodal Reconstruction”（LAMAR），一个基于Transformer的网络，训练用于重构真实描述的嵌入，作为检测的辅助信号。研究还探讨了训练策略（端到端与大规模预训练）和集成机制（如直接、掩码、门控和注意力）。",
      "result": "实验表明，使用“MisCaption This!”数据训练的模型能更好地泛化到真实世界的虚假信息；LAMAR在NewsCLIPpings、VERITE及新引入的VERITE 24/25基准上实现了最先进性能，突显了VLM生成数据和基于重构的网络在多模态虚假信息检测中的有效性。具体性能指标摘要未明确说明，但强调了相对于基线方法的显著提升。",
      "conclusion": "论文的主要贡献在于提出了生成高质量合成数据的方法和基于重构的检测网络，解决了多模态虚假信息检测中的数据稀缺问题。学术价值体现在结合生成与重构的创新方法，实际应用价值在于改善虚假信息识别。未来工作可能涉及进一步优化数据生成或扩展模型到更广泛的多模态场景。",
      "tags": [
        "Vision-Language Models (VLMs)",
        "Transformer-based Networks",
        "Adversarial Prompting",
        "Multimodal Misinformation Detection",
        "Latent Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:22.263153Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.05255",
    "title": "Kolmogorov--Arnold stability",
    "authors": [
      "Sviatoslav V. Dzhenzher",
      "Michael H. Freedman"
    ],
    "abstract": "Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or <<expressing>> functions, we test its robustness by analyzing its stability to withstand re-parameterizations of the hidden space. One may think of such re-parameterizations as the work of an adversary attempting to foil the construction of the KA outer function. We find KA to be stable under countable collections of continuous re-parameterizations, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of re-parameterizations. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.FA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.05255.pdf",
    "abs_url": "https://arxiv.org/abs/2504.05255",
    "published": "2025-04-07T16:46:52Z",
    "updated": "2026-01-13T15:12:34Z",
    "comment": "12 pages, 3 figures; major revision",
    "light_analysis": {
      "overview": "论文研究了Kolmogorov-Arnold表示定理的稳定性，揭示了外函数等连续性问题，对其在神经网络理论中的应用提出关键见解。",
      "motivation": "该研究旨在测试Kolmogorov-Arnold (KA) 表示定理的鲁棒性，特别针对隐藏空间重参数化这一实际挑战。重参数化可被视为对抗性扰动，可能破坏函数表示的构建，这在神经网络等应用中至关重要。现有方法对KA稳定性了解不足，特别是在面对连续扰动时，限制了其在理论模型中的可靠应用。因此，分析稳定性有助于评估KA的实用性和推广能力，为解决神经网络表示中的基础问题提供支持。",
      "method": "研究方法通过数学分析探讨KA表示定理的稳定性，以对手视角模拟重参数化攻击。核心创新点在于分析外函数在连续重参数化下的行为，重点考察可数连续重参数化集合的影响。技术特色涉及函数空间和拓扑结构，关注等连续性和规则性，而非具体实验设置。摘要未明确说明数据集或模型架构，仅基于理论分析测试KA表示作为算法的稳定性，旨在模拟极限过程和应对更广泛的扰动群。",
      "result": "主要实验结果表明，KA表示定理在可数连续重参数化下保持稳定，显示其在一定条件下的鲁棒性。然而，研究发现外函数存在等连续性问题，这阻碍了进一步推广到连续重参数化群，并限制了取极限的能力。与基线方法对比，该分析揭示了稳定性的边界，但对具体性能指标如准确率未提供数据。这一发现突出了KA表示在更一般扰动中的局限性，为神经网络理论中的函数表示提供了新视角。",
      "conclusion": "结论强调了KA表示定理的稳定性受外函数规则性影响，该研究贡献在于识别这一关键问题，对神经网络理论的适用性有重要意义。学术价值体现在深化KA定理的理解，实际应用价值在于为AI表示模型提供理论基础。局限性在于未能解决连续重参数化群的挑战，未来工作方向包括解决等连续性问题或扩展到其他扰动类型，以促进更广泛的理论和应用探索。",
      "tags": [
        "Kolmogorov-Arnold Theorem",
        "Stability Analysis",
        "Reparameterization",
        "Function Representation",
        "Neural Networks Theory"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:45.545009Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.08120",
    "title": "UniF$^2$ace: A Unified Fine-grained Face Understanding and Generation Model",
    "authors": [
      "Junzhe Li",
      "Sifan Zhou",
      "Liya Guo",
      "Xuerui Qiu",
      "Linrui Xu",
      "Delin Qu",
      "Tingting Long",
      "Chun Fan",
      "Ming Li",
      "Hehe Fan",
      "Jun Liu",
      "Shuicheng Yan"
    ],
    "abstract": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in fundamental cross-modality research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily faces two challenges: $\\textbf{(1)}$ $\\textbf{fragmentation development}$, with existing methods failing to unify understanding and generation into a single one, hindering the way to artificial general intelligence. $\\textbf{(2) lack of fine-grained facial attributes}$, which are crucial for high-fidelity applications. To handle those issues, we propose $\\textbf{UniF$^2$ace}$, $\\textit{the first UMM specifically tailored for fine-grained face understanding and generation}$. $\\textbf{First}$, we introduce a novel theoretical framework with a Dual Discrete Diffusion (D3Diff) loss, unifying masked generative models with discrete score matching diffusion and leading to a more precise approximation of the negative log-likelihood. Moreover, this D3Diff significantly enhances the model's ability to synthesize high-fidelity facial details aligned with text input. $\\textbf{Second}$, we propose a multi-level grouped Mixture-of-Experts architecture, adaptively incorporating the semantic and identity facial embeddings to complement the attribute forgotten phenomenon in representation evolvement. $\\textbf{Finally}$, to this end, we construct UniF$^2$aceD-1M, a large-scale dataset comprising 130K fine-grained image-caption pairs and 1M visual question-answering pairs, spanning a much wider range of facial attributes than existing datasets. Extensive experiments demonstrate that UniF$^2$ace outperforms existing models with a similar scale in both understanding and generation tasks, with 7.1\\% higher Desc-GPT and 6.6\\% higher VQA-score, respectively.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.08120.pdf",
    "abs_url": "https://arxiv.org/abs/2503.08120",
    "published": "2025-03-11T07:34:59Z",
    "updated": "2026-01-13T03:56:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "UniF^2^ace是首个统一细粒度人脸理解和生成的统一多模态模型，通过D3Diff损失和多级分组混合专家架构实现。",
      "motivation": "论文针对人脸领域的两大挑战：一是现有方法碎片化发展，未能将理解和生成任务统一，阻碍了人工通用智能的进展；二是缺乏细粒度面部属性，这对高保真应用如人脸识别和生成至关重要。统一多模态模型虽在图像领域有潜力，但人脸领域缺乏针对性解决方案，因此研究旨在填补这一空白。",
      "method": "UniF^2^ace的核心方法包括：首先，提出Dual Discrete Diffusion (D3Diff)损失，统一掩码生成模型与离散评分匹配扩散，以实现更精确的负对数似然近似，增强细粒度面部细节的生成；其次，采用多级分组混合专家架构，自适应结合语义和身份面部嵌入，缓解表示演进中的属性遗忘现象。此外，构建了UniF^2^aceD-1M数据集，包含130K图像-标题对和1M视觉问答对，覆盖广泛的面部属性。",
      "result": "实验结果显示，UniF^2^ace在理解和生成任务中优于规模相似的现有模型。具体而言，在描述生成任务中，Desc-GPT得分提升了7.1%；在视觉问答任务中，VQA-score提升了6.6%。这些数据表明模型在细粒度人脸处理方面具有显著优势。",
      "conclusion": "论文的主要贡献是提出首个统一细粒度人脸理解和生成的模型UniF^2^ace，通过创新方法和数据集解决了关键挑战，推动了统一多模态模型在人脸分析中的应用，具有高保真人脸生成和理解的实际价值。未来工作可能涉及扩展到更多场景或改进模型泛化能力，摘要未明确说明具体局限性。",
      "tags": [
        "Dual Discrete Diffusion (D3Diff)",
        "Mixture-of-Experts Architecture",
        "Fine-grained Face Understanding",
        "Face Generation",
        "Large-scale Dataset Construction"
      ]
    },
    "analyzed_at": "2026-01-14T03:44:54.196956Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.07168",
    "title": "HisTrackMap: Global Vectorized High-Definition Map Construction via History Map Tracking",
    "authors": [
      "Jing Yang",
      "Sen Yang",
      "Xiao Tan",
      "Hanli Wang"
    ],
    "abstract": "As an essential component of autonomous driving systems, high-definition (HD) maps provide rich and precise environmental information for auto-driving scenarios; however, existing methods, which primarily rely on query-based detection frameworks to directly model map elements or implicitly propagate queries over time, often struggle to maintain consistent temporal perception outcomes. These inconsistencies pose significant challenges to the stability and reliability of real-world autonomous driving and map data collection systems. To address this limitation, we propose a novel end-to-end tracking framework for global map construction by temporally tracking map elements' historical trajectories. Firstly, instance-level historical rasterization map representation is designed to explicitly store previous perception results, which can control and maintain different global instances' history information in a fine-grained way. Secondly, we introduce a Map-Trajectory Prior Fusion module within this tracking framework, leveraging historical priors for tracked instances to improve temporal smoothness and continuity. Thirdly, we propose a global perspective metric to evaluate the quality of temporal geometry construction in HD maps, filling the gap in current metrics for assessing global geometric perception results. Substantial experiments on the nuScenes and Argoverse2 datasets demonstrate that the proposed method outperforms state-of-the-art (SOTA) methods in both single-frame and temporal metrics. The project page is available at: https://yj772881654.github.io/HisTrackMap.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.07168.pdf",
    "abs_url": "https://arxiv.org/abs/2503.07168",
    "published": "2025-03-10T10:44:43Z",
    "updated": "2026-01-13T08:50:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于历史地图跟踪的端到端框架，用于构建全局向量化高精度地图，显著改善时间一致性和几何感知质量。",
      "motivation": "高精度地图在自动驾驶系统中提供关键环境信息，但现有方法主要依赖查询检测框架，直接建模地图元素或隐式传播查询，导致时间感知结果不一致，对自动驾驶稳定性和地图数据收集可靠性构成挑战。因此，本研究旨在解决HD地图构建中的时间一致性问题，通过跟踪历史轨迹来优化全局感知的连续性，弥补现有方法的不足。",
      "method": "研究提出一个端到端跟踪框架，通过跟踪地图元素历史轨迹构建全局向量化地图。关键技术包括：设计实例级历史栅格化地图表示，显式存储先前感知结果以实现细粒度全局实例历史控制；引入Map-Trajectory Prior Fusion模块，利用历史先验提升跟踪实例的时间平滑性和连续性；并提出全局视角度量，评估HD地图时间几何构建质量，填补现有评估指标空白。实验基于nuScenes和Argoverse2数据集验证方法。",
      "result": "在nuScenes和Argoverse2数据集上的实验显示，所提方法在单帧和时间度量方面均优于当前最佳方法，表明能有效提升时间一致性和几何感知精度。与基线方法对比，该方法在稳定性和可靠性方面有所改进，但摘要未明确说明具体性能指标（如准确率提升），仅概述了优越性。结果验证了框架在全局地图构建中的有效性。",
      "conclusion": "本研究的主要贡献是提出一种新颖的端到端跟踪框架，通过历史地图跟踪改善HD地图构建的时间一致性和几何感知。学术价值在于引入显式历史表示、先验融合模块和全局评估指标，为自动驾驶地图技术提供新思路。实际应用有助于提高系统稳定性和数据收集可靠性。未来工作可探索方法的局限性（如计算效率），但摘要未明确说明具体方向。",
      "tags": [
        "High-Definition Maps",
        "Tracking Framework",
        "Temporal Consistency",
        "Instance-Level Representation",
        "Geometric Perception"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:51.663351Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.06614",
    "title": "Using Subgraph GNNs for Node Classification:an Overlooked Potential Approach",
    "authors": [
      "Qian Zeng",
      "Xin Lin",
      "Jingyi Gao",
      "Yang Yu"
    ],
    "abstract": "Previous studies have demonstrated the strong performance of Graph Neural Networks (GNNs) in node classification. However, most existing GNNs adopt a node-centric perspective and rely on global message passing, leading to high computational and memory costs that hinder scalability. To mitigate these challenges, subgraph-based methods have been introduced, leveraging local subgraphs as approximations of full computational trees. While this approach improves efficiency, it often suffers from performance degradation due to the loss of global contextual information, limiting its effectiveness compared to global GNNs. To address this trade-off between scalability and classification accuracy, we reformulate the node classification task as a subgraph classification problem and propose SubGND (Subgraph GNN for NoDe). This framework introduces a differentiated zero-padding strategy and an Ego-Alter subgraph representation method to resolve label conflicts while incorporating an Adaptive Feature Scaling Mechanism to dynamically adjust feature contributions based on dataset-specific dependencies. Experimental results on six benchmark datasets demonstrate that SubGND achieves performance comparable to or surpassing global message-passing GNNs, particularly in heterophilic settings, highlighting its effectiveness and scalability as a promising solution for node classification.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.06614.pdf",
    "abs_url": "https://arxiv.org/abs/2503.06614",
    "published": "2025-03-09T13:37:38Z",
    "updated": "2026-01-13T08:44:48Z",
    "comment": "16 pages",
    "light_analysis": {
      "overview": "提出SubGND框架，通过重新定义节点分类为子图分类问题，实现高效且准确的图神经网络节点分类。",
      "motivation": "该研究旨在解决图神经网络（GNNs）在节点分类任务中的可扩展性与准确性之间的权衡问题。现有GNNs多采用节点中心视角和全局消息传递，导致高计算和内存成本，难以扩展到大规模图。基于子图的方法虽提高了效率，却因丢失全局上下文信息而性能下降，尤其在异质图设置中效果不佳，因此需要新方法来平衡效率和效果。",
      "method": "本研究提出SubGND框架，将节点分类任务重新定义为子图分类问题。核心创新包括：差异化零填充策略处理标签冲突；Ego-Alter子图表示方法捕捉局部结构；自适应特征缩放机制基于数据集特定依赖性动态调整特征重要性。这些技术结合增强了子图表示，在保持计算效率的同时提升分类准确性。",
      "result": "实验在六个基准数据集上进行，结果显示SubGND在节点分类任务中取得了与全局消息传递GNNs相当或更优的性能，特别是在异质图设置下表现突出。与基线方法相比，该框架在保持高效率的同时减少了性能下降，验证了其在平衡可扩展性和准确性方面的优势，但摘要未明确说明具体数值指标。",
      "conclusion": "该论文的主要贡献是提出SubGND框架，通过将节点分类重新定义为子图分类问题，有效解决了图神经网络的可扩展性与分类准确性之间的权衡。研究表明，子图方法可以媲美甚至超越全局GNNs，尤其在异质图场景中，为节点分类提供了新的技术路径，具有重要的学术价值和实际应用潜力，但摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Subgraph GNNs",
        "Node Classification",
        "Subgraph Classification",
        "Adaptive Feature Scaling",
        "Ego-Alter Representation"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:54.963199Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.04971",
    "title": "Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge",
    "authors": [
      "Songyuan Li",
      "Jia Hu",
      "Geyong Min",
      "Haojun Huang"
    ],
    "abstract": "Foundation models (FMs) such as GPT-4 exhibit exceptional generative capabilities across diverse downstream tasks through fine-tuning. Split Federated Learning (SFL) facilitates privacy-preserving FM fine-tuning on resource-constrained local devices by offloading partial FM computations to edge servers, enabling device-edge synergistic fine-tuning. Practical edge networks often host multiple SFL tenants to support diversified downstream tasks. However, existing research primarily focuses on single-tenant SFL scenarios, and lacks tailored incentive mechanisms for multi-tenant settings, which are essential to effectively coordinate self-interested local devices for participation in various downstream tasks, ensuring that each SFL tenant's distinct FM fine-tuning requirements (e.g., FM types, performance targets, and fine-tuning deadlines) are met. To address this gap, we propose a novel Price-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer strategic price incentives, which solicit high-quality device participation for efficient FM fine-tuning. Specifically, we first develop a bias-resilient global SFL model aggregation scheme to eliminate model biases caused by independent device participation. We then derive a rigorous SFL convergence bound to evaluate the contributions of heterogeneous devices to FM performance improvements, guiding the incentive strategies of SFL tenants. Furthermore, we model inter-tenant device competition as a congestion game for Stackelberg equilibrium (SE) analysis, deriving each SFL tenant's optimal incentive strategy. Extensive simulations involving four representative SFL tenant types (ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images, and audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x compared to state-of-the-art approaches, while consistently meeting fine-tuning performance targets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.04971.pdf",
    "abs_url": "https://arxiv.org/abs/2503.04971",
    "published": "2025-03-06T21:06:27Z",
    "updated": "2026-01-13T18:16:48Z",
    "comment": "Accepted for publication in IEEE/ACM Transactions on Networking. Index Terms: Foundation models, Edge computing, Split federated learning, Multi-tenant system, Incentive mechanism",
    "light_analysis": {
      "overview": "论文提出一种价格激励机制（PRINCE），激励多租户分割联邦学习中的设备参与，加速基础模型的微调。",
      "motivation": "基础模型如GPT-4通过微调在多种下游任务中表现优异，分割联邦学习可在资源受限设备上实现隐私保护的微调。实际边缘网络常有多租户支持多样化任务，但现有研究主要关注单租户场景，缺乏针对多租户的激励策略，导致难以协调自利设备满足不同租户的特定需求，如FM类型、性能目标和截止日期。因此，开发多租户激励机制的紧迫性凸显，以提升设备参与效率和任务完成质量。",
      "method": "作者提出PRINCE机制，首先开发偏见弹性全局SFL模型聚合方案，消除由独立设备参与引起的模型偏见。其次，推导严格的SFL收敛界，评估异构设备对FM性能提升的贡献，指导租户的激励策略。然后，将租户间设备竞争建模为拥塞博弈，进行Stackelberg均衡分析，得出每个租户的最优激励策略。实验模拟涉及四个代表性SFL租户类型（ViT、BERT、Whisper和LLaMA）和多种数据模态（文本、图像、音频）。",
      "result": "通过广泛模拟实验，PRINCE机制在加速FM微调方面表现显著，相比最先进方法，加速可达3.07倍。该机制能一致满足不同租户的微调性能目标，覆盖文本、图像和音频数据模态，证明了其在多租户场景下的有效性。实验结果表明，PRINCE在提升效率和满足多样化需求方面优于现有方法。",
      "conclusion": "本研究提出了一种创新的激励方法，解决了多租户分割联邦学习中的协调问题，促进了边缘网络中基础模型的高效微调。其学术价值在于扩展了多租户联邦学习的研究框架，实际应用意义在于提升资源分配和任务完成效率。未来工作可能包括扩展到更多租户类型或动态环境下的激励机制优化，以及进一步验证在实际部署中的鲁棒性。",
      "tags": [
        "Split Federated Learning",
        "Incentive Mechanism",
        "Foundation Models",
        "Congestion Game",
        "Stackelberg Equilibrium"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:56.913234Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.17899",
    "title": "Can Large Language Models Identify Implicit Suicidal Ideation? An Empirical Evaluation",
    "authors": [
      "Tong Li",
      "Shu Yang",
      "Junchao Wu",
      "Jiyao Wei",
      "Lijie Hu",
      "Mengdi Li",
      "Derek F. Wong",
      "Joshua R. Oltmanns",
      "Di Wang"
    ],
    "abstract": "We present a comprehensive evaluation framework for assessing Large Language Models' (LLMs) capabilities in suicide prevention, focusing on two critical aspects: the Identification of Implicit Suicidal ideation (IIS) and the Provision of Appropriate Supportive responses (PAS). We introduce \\ourdata, a novel dataset of 1,308 test cases built upon psychological frameworks including D/S-IAT and Negative Automatic Thinking, alongside real-world scenarios. Through extensive experiments with 8 widely used LLMs under different contextual settings, we find that current models struggle significantly with detecting implicit suicidal ideation and providing appropriate support, highlighting crucial limitations in applying LLMs to mental health contexts. Our findings underscore the need for more sophisticated approaches in developing and evaluating LLMs for sensitive psychological applications.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.17899.pdf",
    "abs_url": "https://arxiv.org/abs/2502.17899",
    "published": "2025-02-25T06:53:00Z",
    "updated": "2026-01-13T06:55:37Z",
    "comment": "Dataset: https://huggingface.co/datasets/babytreecc/Implicit-suicide-detection",
    "light_analysis": {
      "overview": "本文通过提出综合评估框架和新数据集，实证评估了大语言模型在识别隐含自杀意念方面的能力，揭示了其局限性。",
      "motivation": "研究动机在于评估LLMs在心理健康应用中的潜力，特别是在自杀预防中识别隐含自杀意念的挑战。这一问题至关重要，因为自杀预防涉及敏感领域，需要准确检测以提供及时干预。现有方法如通用LLMs在处理隐含情感和复杂心理学上下文中表现不足，容易忽略细微线索，导致支持性回应不恰当，因此需要系统性评估来揭示局限并为改进提供基础。",
      "method": "研究方法包括开发一个综合评估框架，重点评估LLMs在识别隐含自杀意念（IIS）和提供支持性回应（PAS）的能力。关键创新点是构建名为\\ourdata的新数据集，包含1,308个测试案例，基于心理学框架如D/S-IAT和负面自动思维，并结合真实世界场景以增强现实性。通过实验评估了8个广泛使用的LLM在不同上下文设置下的性能，测试它们在模拟任务中的表现，以分析模型在心理健康应用中的具体行为。",
      "result": "主要实验结果表明，当前LLMs在检测隐含自杀意念和提供适当支持方面存在显著困难，凸显了应用中的关键局限性。摘要未明确说明具体性能指标如准确率或效率数据，但强调模型在这些任务上表现不佳，通过对比基线或现有方法，发现改进空间巨大。这提示了LLMs在敏感心理学上下文中的适用性问题，需要进一步技术优化。",
      "conclusion": "结论是本研究通过评估框架和数据集的引入，系统揭示了LLMs在敏感心理学应用中的不足，特别是识别隐含自杀意念的挑战。其学术价值在于为LLMs评估提供了新视角，实际应用价值则体现在推动心理健康领域的技术发展，强调了开发更复杂方法（如结合心理学理论）的必要性。局限性在于现有模型能力有限，未来工作可探索改进模型架构或融合多模态数据以提升效果。",
      "tags": [
        "Large Language Models",
        "Implicit Suicidal Ideation",
        "Dataset Evaluation",
        "Psychological Frameworks",
        "Suicide Prevention"
      ]
    },
    "analyzed_at": "2026-01-14T03:44:05.769927Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.11973",
    "title": "Generating Text from Uniform Meaning Representation",
    "authors": [
      "Emma Markle",
      "Reihaneh Iranmanesh",
      "Shira Wein"
    ],
    "abstract": "Uniform Meaning Representation (UMR) is a recently developed graph-based semantic representation, which expands on Abstract Meaning Representation (AMR) in a number of ways, in particular through the inclusion of document-level information and multilingual flexibility. In order to effectively adopt and leverage UMR for downstream tasks, efforts must be placed toward developing a UMR technological ecosystem. Though only a small amount of UMR annotations have been produced to date, in this work, we investigate the first approaches to producing text from multilingual UMR graphs. Exploiting the structural similarity between UMR and AMR graphs and the wide availability of AMR technologies, we introduce (1) a baseline approach which passes UMR graphs to AMR-to-text generation models, (2) a pipeline conversion of UMR to AMR, then using AMR-to-text generation models, and (3) a fine-tuning approach for both foundation models and AMR-to-text generation models with UMR data. Our best performing models achieve multilingual BERTscores of 0.825 for English and 0.882 for Chinese, a promising indication of the effectiveness of fine-tuning approaches for UMR-to-text generation even with limited UMR data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.11973.pdf",
    "abs_url": "https://arxiv.org/abs/2502.11973",
    "published": "2025-02-17T16:20:22Z",
    "updated": "2026-01-13T04:10:04Z",
    "comment": "Published and Outstanding Paper Award recipient at IJCNLP-AACL 2025",
    "light_analysis": {
      "overview": "该论文首次研究了从多语言Uniform Meaning Representation图生成文本的方法，提出多种生成策略并验证微调方法的有效性。",
      "motivation": "研究动机源于Uniform Meaning Representation作为一种扩展的语义表示，增强了文档级信息和多语言灵活性，但现有UMR标注数据稀缺，限制了其在下游任务中的应用。这突出了构建UMR技术生态系统的必要性，以填补从语义图生成文本的技术空白，从而促进更高效的自然语言处理任务如机器翻译或信息提取。现有AMR技术虽成熟，但无法直接处理UMR的独特特征，因此需开发专门方法来克服数据不足的挑战。",
      "method": "研究利用UMR与AMR图的结构相似性，设计了三种技术路线：首先，基线方法直接将UMR图输入到现有的AMR到文本生成模型中；其次，通过管道转换将UMR图预处理为AMR格式，再利用AMR生成模型；第三，采用微调策略，使用有限UMR数据微调基础模型和AMR生成模型，以提升多语言适应能力。关键创新点在于充分利用了AMR的广泛资源和模型，并探索数据稀缺下的优化技巧，展示了从语义表示生成文本的多路径探索。",
      "result": "实验结果显示，最佳微调模型在多语言BERTscore评估中，英语部分达到0.825分，中文部分达0.882分，表明即使在UMR数据有限的情况下，微调方法也能有效提升文本生成质量。这些分数对比基线方法和其他生成策略显示出显著优势，验证了所提方法在处理多语言语义图中的实用性和泛化能力，为后续研究提供了性能基准和实证支持。摘要未明确说明具体对比基线数据，但强调了微调模型的强劲表现。",
      "conclusion": "论文贡献在于首次实现了从多语言UMR图生成文本，证实了微调方法在数据稀缺下的有效性。研究的学术价值体现在为UMR技术生态系统奠定了基础，推动语义表示在多语言环境中的应用，如自动化摘要或对话生成。实际应用中，这有助于降低对大规模标注数据的依赖，提高自然语言处理系统的效率。未来工作可扩展至更多语言或更大数据集，以克服当前UMR资源有限的局限性，并探索更复杂的图转换技术。",
      "tags": [
        "Uniform Meaning Representation",
        "Abstract Meaning Representation",
        "text generation",
        "fine-tuning",
        "multilingual models"
      ]
    },
    "analyzed_at": "2026-01-14T03:43:57.688097Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.09583",
    "title": "YRC-Bench: A Benchmark for Learning to Coordinate with Experts",
    "authors": [
      "Mohamad H. Danesh",
      "Nguyen X. Khanh",
      "Tu Trinh",
      "Benjamin Plaut"
    ],
    "abstract": "When deployed in the real world, AI agents will inevitably face challenges that exceed their individual capabilities. A critical component of AI safety is an agent's ability to recognize when it is likely to fail in a novel situation and to yield control to a more capable expert system. Leveraging such expert assistance can significantly improve safety and performance in such situations. Since expert assistance is costly, a central challenge is determining when to consult an expert. In this paper, we explore a novel variant of this problem, termed YRC-0, in which an agent must learn to collaborate with an expert in new environments in an unsupervised manner--that is, without interacting with the expert during training. This setting motivates the development of low-cost, robust approaches for training expert-leveraging agents. To support research in this area, we introduce YRC-Bench, an open-source benchmark that instantiates YRC-0 across diverse environments. YRC-Bench provides a standardized Gym-like API, simulated experts, an evaluation pipeline, and implementations of popular baselines. Toward tackling YRC-0, we propose a validation strategy and use a proposer-validator decomposition as a diagnostic framework to evaluate a range of learning methods, offering insights that can inform future research. Codebase: https://github.com/modanesh/YRC-Bench",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.09583.pdf",
    "abs_url": "https://arxiv.org/abs/2502.09583",
    "published": "2025-02-13T18:41:55Z",
    "updated": "2026-01-13T13:52:11Z",
    "comment": "Accepted at TMLR",
    "light_analysis": {
      "overview": "本文提出YRC-Bench基准测试，用于解决AI代理在无监督环境下学习与专家协调的问题。",
      "motivation": "研究动机源于AI代理在现实世界中部署时面临超出自身能力的挑战，需要安全地将控制权移交给专家系统以提升安全性和性能。由于专家协助成本高昂，确定何时咨询专家成为关键问题。现有方法多依赖于训练期间的专家交互，而在无监督学习下如何有效协调专家尚未得到充分解决，这限制了AI的适应性和效率，因此本研究旨在填补这一空白。",
      "method": "研究方法包括定义YRC-0问题，即在无监督训练下学习与专家协作的新变体，强调代理在新环境中独立与专家合作。为此，开发了YRC-Bench基准测试，提供标准化的Gym-like API、模拟专家系统、评估管道和多种基线方法的实现。关键创新在于提出了验证策略，并采用提出者-验证者分解作为诊断框架，用于系统地评估不同学习方法，从而为算法设计提供指导。",
      "result": "论文通过提出者-验证者分解的诊断框架评估了多种学习方法，提供了对YRC-0问题解决途径的初步见解。然而，摘要未明确说明具体实验结果，如准确率提升、效率改进或与基线方法的详细性能对比，因此需进一步查阅论文全文获取量化数据。",
      "conclusion": "结论总结了本研究的核心贡献：引入YRC-Bench基准测试和YRC-0问题设置，推动了在无监督学习下与专家协调的研究。学术价值在于提供了标准化评估工具和诊断框架，实际应用价值则有助于提升AI代理的安全性和适应性。未来工作可包括扩展基准环境多样性、探索更高效的学习算法，并应对现有方法在复杂场景中的局限性。",
      "tags": [
        "Benchmarking",
        "Expert Coordination",
        "Unsupervised Learning",
        "Proposer-Validator Decomposition",
        "AI Safety"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:34.127584Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.06309",
    "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
    "authors": [
      "Zhaoxian Wu",
      "Quan Xiao",
      "Tayfun Gokmen",
      "Omobayode Fagbohungbe",
      "Tianyi Chen"
    ],
    "abstract": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose Residual Learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We demonstrate that the proposed method can be extended to address other hardware imperfections, such as limited response granularity. As we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights.",
    "categories": [
      "cs.LG",
      "cs.AR",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.06309.pdf",
    "abs_url": "https://arxiv.org/abs/2502.06309",
    "published": "2025-02-10T09:56:15Z",
    "updated": "2026-01-13T17:33:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "这篇论文为模拟内存计算中非理想响应函数的基于梯度训练提供了理论基础，并提出Residual Learning算法来解决训练动态问题。",
      "motivation": "随着训练和部署大型视觉或语言模型的经济与环境成本急剧上升，模拟内存计算作为一种能效高的解决方案受到关注。然而，其在训练方面的动态特性尚未得到充分研究。在AIMC硬件中，可训练权重由电阻元件的电导表示，并通过连续电脉冲更新，但实际的电导变化受非对称和非线性响应函数影响，导致非理想的训练动态，这限制了AIMC的有效应用。因此，理解并解决这些影响对于推动AIMC在高效训练中的应用至关重要。",
      "method": "论文提出了Residual Learning算法，通过解决一个双层优化问题来克服非理想响应函数对Analog SGD的负面影响。关键创新在于理论上分析了非对称响应函数如何通过隐式惩罚影响目标函数，并证明了该算法能够精确收敛到临界点。此外，该方法还可扩展以处理其他硬件缺陷，如有限的响应粒度。研究基于梯度训练框架，在AIMC硬件背景下进行模拟验证，强调了通用非理想响应函数的首次系统性探讨。",
      "result": "论文通过模拟验证了理论洞察，证明Residual Learning算法在应对非理想响应函数时具有优势。摘要未明确提供具体的性能指标如准确率提升或效率改进数据，但模拟结果支持了算法在改善训练动态方面的有效性。与基线方法对比，论文指出非对称响应函数对Analog SGD有负面影响，而提出的算法能克服这一问题，但具体量化结果未在摘要中说明，仅表明模拟验证了理论。",
      "conclusion": "该论文的主要贡献在于为AIMC中非理想响应函数的基于梯度训练建立了理论基础，并提出Residual Learning算法来解决训练动态问题。这具有重要的学术价值，因为它是首次系统研究一类通用非理想响应函数影响的论文。在实际应用中，这有助于提高AIMC硬件的训练效率和能源效率。未来工作可能包括进一步扩展算法以处理更多硬件限制，并应用于更大规模的模型中。",
      "tags": [
        "Analog In-memory Computing",
        "Gradient-based Training",
        "Response Functions",
        "Residual Learning",
        "Bilevel Optimization"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:27.692334Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.02132",
    "title": "How Memory in Optimization Algorithms Implicitly Modifies the Loss",
    "authors": [
      "Matias D. Cattaneo",
      "Boris Shigida"
    ],
    "abstract": "In modern optimization methods used in deep learning, each update depends on the history of previous iterations, often referred to as memory, and this dependence decays fast as the iterates go further into the past. For example, gradient descent with momentum has exponentially decaying memory through exponentially averaged past gradients. We introduce a general technique for identifying a memoryless algorithm that approximates an optimization algorithm with memory. It is obtained by replacing all past iterates in the update by the current one, and then adding a correction term arising from memory (also a function of the current iterate). This correction term can be interpreted as a perturbation of the loss, and the nature of this perturbation can inform how memory implicitly (anti-)regularizes the optimization dynamics. As an application of our theory, we find that Lion does not have the kind of implicit anti-regularization induced by memory that AdamW does, providing a theory-based explanation for Lion's better generalization performance recently documented.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.02132.pdf",
    "abs_url": "https://arxiv.org/abs/2502.02132",
    "published": "2025-02-04T09:04:50Z",
    "updated": "2026-01-13T06:37:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个通用技术，分析优化算法中内存如何隐式修改损失函数，以解释 Lion 和 AdamW 在泛化性能上的差异。",
      "motivation": "现代深度学习优化方法，如带动量的梯度下降，具有内存效应，每个更新依赖于历史迭代，且内存随迭代衰减。然而，内存如何隐式影响优化动态和泛化性能尚缺乏深入理论分析。现有方法可能无法明确解释不同优化算法的性能差异，导致对算法设计指导不足。因此，本研究旨在开发一个通用框架来解析内存在优化中的作用，以填补这一理论空白。",
      "method": "论文引入一个通用技术，将有内存的优化算法（如带动量的梯度下降）中的历史迭代替换为当前迭代，并添加一个由内存产生的校正项，从而近似为无内存算法。校正项可解释为损失函数的扰动，揭示内存如何隐式地（反）正则化优化过程。关键创新在于将内存效应建模为对损失的直接修改，并应用于分析具体算法如 AdamW 和 Lion，无需依赖具体数据集或模型架构。",
      "result": "应用该理论到优化算法 Lion 和 AdamW，研究发现 Lion 不具有像 AdamW 那样的由内存诱导的隐式反正则化效应。这提供了一个基于理论的解释，说明了 Lion 在泛化性能上优于 AdamW 的原因，支持了先前观察到的实验证据。结果突出了内存效应对优化算法泛化能力的关键影响，尽管摘要未提供具体数值指标如准确率提升。",
      "conclusion": "本研究的主要贡献是开发了一个理论框架来分析优化算法中的内存效应，揭示了内存如何通过修改损失函数来隐式影响正则化。这增强了我们对优化动态的理解，为算法比较和设计提供了学术价值。潜在局限性包括理论假设可能需要实验验证，未来工作可以扩展该框架到更多优化算法或探索其在深度学习任务中的实际应用。",
      "tags": [
        "Optimization Algorithms",
        "Memory",
        "Loss Modification",
        "Regularization",
        "AdamW vs Lion"
      ]
    },
    "analyzed_at": "2026-01-14T03:22:31.950183Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.00568",
    "title": "Generating crossmodal gene expression from cancer histopathology improves multimodal AI predictions",
    "authors": [
      "Samiran Dey",
      "Christopher R. S. Banerji",
      "Partha Basuchowdhuri",
      "Sanjoy K. Saha",
      "Deepak Parashar",
      "Tapabrata Chakraborti"
    ],
    "abstract": "Emerging research has highlighted that artificial intelligence-based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion is impractical in clinical settings, where histopathology remains the gold standard and transcriptomic tests are rarely requested in public healthcare. We experiment on two publicly available multimodal datasets, The Cancer Genomic Atlas and the Clinical Proteomic Tumor Analysis Consortium, spanning four independent cohorts: glioma-glioblastoma, renal, uterine, and breast, and observe significant performance gains in gradation and risk estimation (p-value<0.05) when incorporating synthesized transcriptomic data with WSIs. Also, predictions using synthesized features were statistically close to those obtained with real transcriptomic data (p-value>0.05), consistently across cohorts. Here we show that with our diffusion based crossmodal generative AI model, PathGen, gene expressions synthesized from digital histopathology jointly predict cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed co-attention maps). PathGen code is available for open use on GitHub at https://github.com/Samiran-Dey/PathGen.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2502.00568.pdf",
    "abs_url": "https://arxiv.org/abs/2502.00568",
    "published": "2025-02-01T21:28:30Z",
    "updated": "2026-01-13T10:13:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出PathGen模型，基于扩散生成从组织病理学合成基因表达，以高准确性和可解释性提升癌症诊断和预后预测的实用性。",
      "motivation": "该研究旨在解决癌症多模态AI预测中，由于转录组学测试在临床中稀缺导致直接融合不实用的问题。现有方法虽然结合数字病理学和转录组学能改善预测，但现实医疗环境中组织病理学作为金标准而转录组学数据难以获取，限制了多模态模型的应用。因此，需要开发一种能从广泛可用的组织病理学生成转录组学特征的技术，以克服数据障碍并增强预测模型的临床可行性。",
      "method": "论文提出PathGen，一个基于扩散的跨模态生成AI模型，用于从数字组织病理学图像（WSI）合成基因表达数据。该方法采用扩散生成技术，确保合成特征与真实数据统计相似，关键创新包括通过符合覆盖保证提供确定性，并通过分布共注意图增强可解释性。实验使用了The Cancer Genomic Atlas和Clinical Proteomic Tumor Analysis Consortium两个公开数据集，涵盖胶质瘤-胶质母细胞瘤、肾、子宫和乳腺癌四个独立队列，以训练和评估模型。",
      "result": "实验结果显示，当将PathGen合成的转录组学数据与组织病理学图像结合时，在癌症分级和患者生存风险预测中取得了显著性能提升（p值<0.05）。使用合成特征进行的预测与使用真实转录组学数据的预测在统计上没有显著差异（p值>0.05），这在不同癌症队列中一致成立。模型实现了最先进的性能，表现为高准确性、确定性和可解释性，但摘要未提供具体准确率数字，仅强调了统计显著性和一致性。",
      "conclusion": "论文的主要贡献是开发了PathGen模型，证明了从组织病理学合成基因表达数据可以有效替代真实转录组学测试，提升多模态预测性能。研究具有重要学术价值，推动了跨模态生成AI在医疗领域的应用，同时实际应用价值在于为资源有限的临床环境提供了可行的预测方案。摘要未明确说明局限性，但未来工作可能包括扩展模型到更多癌症类型或进一步优化生成效率。",
      "tags": [
        "Diffusion Models",
        "Crossmodal Generation",
        "Gene Expression Synthesis",
        "Cancer Histopathology",
        "Conformal Coverage"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:39.267356Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.19107",
    "title": "Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected",
    "authors": [
      "Yingtao Zhang",
      "Diego Cerretti",
      "Jialin Zhao",
      "Wenjing Wu",
      "Ziheng Liao",
      "Umberto Michieli",
      "Carlo Vittorio Cannistraci"
    ],
    "abstract": "Dynamic sparse training (DST) can reduce the computational demands in ANNs, but faces difficulties in keeping peak performance at high sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method for growing connectivity in DST. CHT leverages a gradient-free, topology-driven link regrowth, which has shown ultra-sparse (less than 1% connectivity) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $O(Nd^3)$ - N node network size, d node degree - restricting it to ultra-sparse regimes. (ii) it selects top link prediction scores, which is inappropriate for the early training epochs, when the network presents unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. We further introduce a GPU-friendly matrix-based approximation of CH link prediction, reducing complexity to $O(N^3)$. We introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we integrate CHTs with a sigmoid gradual density decay (CHTss). Empirical results show that BRF offers performance advantages over previous network science models. Using 1% of connections, CHTs outperforms fully connected networks in MLP architectures on image classification tasks, compressing some networks to less than 30% of the nodes. Using 5% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, at 30% connectivity, both CHTs and CHTss outperform other DST methods in language modeling task.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2501.19107.pdf",
    "abs_url": "https://arxiv.org/abs/2501.19107",
    "published": "2025-01-31T13:04:37Z",
    "updated": "2026-01-13T07:17:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了基于脑网络科学的稀疏神经网络初始化方法BRF和改进的Cannistraci-Hebb训练规则，使Transformers和LLMs在极低连接密度下实现全连接网络的性能。",
      "motivation": "动态稀疏训练能减少人工神经网络的计算需求，但在高稀疏度下难以保持峰值性能。Cannistraci-Hebb训练作为受脑启发的方法，虽在超稀疏领域有优势，但存在时间复杂度高（O(Nd^3)）和早期训练因连接不可靠导致采样不准确的问题。因此，本研究旨在克服这些限制，开发更高效、稳定的稀疏训练技术，以适应大规模模型如Transformers的轻量化应用需求。",
      "method": "研究设计了首个受脑启发的网络模型——双部分感受野（BRF），用于初始化稀疏神经网络的连接。引入了GPU友好的矩阵近似，将链接预测的时间复杂度从O(Nd^3)降至O(N^3)。提出了Cannistraci-Hebb训练软规则（CHTs），在链接移除和再生时采用灵活采样策略，平衡网络拓扑的探索和利用。并集成了sigmoid gradual密度衰减（CHTss），以优化训练过程中的连接密度调整。",
      "result": "实验结果表明，BRF在性能上优于先前网络科学模型。在图像分类任务中，使用1%的连接，CHTs在MLP架构上超越全连接网络，并将网络压缩至不足30%的节点大小。在基于Transformer的机器翻译任务中，使用5%的连接，CHTss优于全连接网络。在语言建模任务中，30%连接密度下，CHTs和CHTss均优于其他动态稀疏训练方法，显示了方法的通用性和高效性。",
      "conclusion": "本研究通过脑网络科学建模和算法改进，显著提升了稀疏神经网络的性能，使其在低连接密度下达到或超越全连接网络，有效减少了模型参数和计算开销。这为大型语言模型和Transformers的轻量化部署提供了新途径，具有重要学术和应用价值。未来工作可探索更广泛的稀疏度范围、多样化任务的应用，以及进一步优化算法复杂性。",
      "tags": [
        "Dynamic Sparse Training",
        "Cannistraci-Hebb Training",
        "Bipartite Receptive Field",
        "Link Prediction",
        "Transformers"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:31.481317Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.05179",
    "title": "Global Compression Commander: Plug-and-Play Inference Acceleration for High-Resolution Large Vision-Language Models",
    "authors": [
      "Xuyang Liu",
      "Ziming Wang",
      "Junjie Chen",
      "Yuhang Han",
      "Yingyao Wang",
      "Jiale Yuan",
      "Jun Song",
      "Siteng Huang",
      "Honggang Chen"
    ],
    "abstract": "Large vision-language models (LVLMs) excel at visual understanding, but face efficiency challenges due to quadratic complexity in processing long multi-modal contexts. While token compression can reduce computational costs, existing approaches are designed for single-view LVLMs and fail to consider the unique multi-view characteristics of high-resolution LVLMs with dynamic cropping. Existing methods treat all tokens uniformly, but our analysis reveals that global thumbnails can naturally guide the compression of local crops by providing holistic context for informativeness evaluation. In this paper, we first analyze dynamic cropping strategy, revealing both the complementary nature between thumbnails and crops, and the distinctive characteristics across different crops. Based on our observations, we propose ``Global Compression Commander'' (\\textit{i.e.}, \\textbf{GlobalCom$^2$}), a novel plug-and-play token compression framework for HR-LVLMs. GlobalCom$^2$ leverages thumbnail as the ``commander'' to guide the compression of local crops, adaptively preserving informative details while eliminating redundancy. Extensive experiments show that GlobalCom$^2$ maintains over \\textbf{90\\%} performance while compressing \\textbf{90\\%} visual tokens, reducing FLOPs and peak memory to \\textbf{9.1\\%} and \\textbf{60\\%}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.05179.pdf",
    "abs_url": "https://arxiv.org/abs/2501.05179",
    "published": "2025-01-09T11:57:58Z",
    "updated": "2026-01-13T07:37:47Z",
    "comment": "Accepted by AAAI 2026. Code is available at \\url{https://github.com/xuyang-liu16/GlobalCom2}",
    "light_analysis": {
      "overview": "本论文提出了GlobalCom²框架，一种即插即用的令牌压缩方法，通过利用全局缩略图指导局部裁剪的压缩，有效加速高分辨率大型视觉语言模型的推理，实现效率与性能的平衡。",
      "motivation": "大型视觉语言模型（LVLMs）在处理高分辨率图像时，常采用动态裁剪生成多视图，导致视觉令牌数量剧增，计算复杂度呈二次增长，效率低下。现有令牌压缩方法主要为单视图LVLMs设计，忽略了多视图下的动态特性和缩略图与裁剪之间的互补关系，限制了高分辨率LVLMs的实用性和性能。因此，开发一种能够利用全局信息优化局部压缩的新方法，对于提升模型在资源受限场景中的效率和部署能力至关重要。",
      "method": "基于对动态裁剪策略的分析，论文揭示了全局缩略图提供整体上下文、帮助评估局部裁剪信息量的特性，以及不同裁剪间的独特特征。提出了GlobalCom²框架，将缩略图作为“指挥官”，自适应地压缩局部裁剪的令牌，通过选择性保留信息细节和消除冗余来实现高效压缩。该框架是即插即用的，可无缝集成到现有高分辨率LVLMs中，无需修改模型架构，强调自适应性和实用性。",
      "result": "实验结果表明，GlobalCom²在压缩90%视觉令牌后，模型性能保持超过90%，同时将FLOPs降低至9.1%，峰值内存减少到60%。这表明该方法在显著减少计算和存储开销的同时，维持了高性能水平，优于现有压缩方法（摘要暗示），证实了其在加速高分辨率LVLMs推理中的有效性和效率优势。",
      "conclusion": "本论文的主要贡献是提出了GlobalCom²框架，通过创新性地结合缩略图与局部裁剪，解决了高分辨率LVLMs的推理效率挑战。学术上，该方法探索了多模态压缩中的全局-局部信息协同机制；应用上，提供了即插即用的解决方案，促进LVLMs在真实环境中的部署。未来工作可进一步优化压缩算法或扩展至其他多模态任务，提升泛化能力。",
      "tags": [
        "Large Vision-Language Models",
        "Token Compression",
        "Dynamic Cropping",
        "High-Resolution Processing",
        "Inference Acceleration"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:49.394335Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.19217",
    "title": "Applying the maximum entropy principle to neural networks enhances multi-species distribution models",
    "authors": [
      "Maxime Ryckewaert",
      "Diego Marcos",
      "Christophe Botella",
      "Maximilien Servajean",
      "Pierre Bonnet",
      "Alexis Joly"
    ],
    "abstract": "The rapid expansion of citizen science initiatives has led to a significant growth of biodiversity databases, and particularly presence-only (PO) observations. PO data are invaluable for understanding species distributions and their dynamics, but their use in a Species Distribution Model (SDM) is curtailed by sampling biases and the lack of information on absences. Poisson point processes are widely used for SDMs, with Maxent being one of the most popular methods. Maxent maximises the entropy of a probability distribution across sites as a function of predefined transformations of variables, called features. In contrast, neural networks and deep learning have emerged as a promising technique for automatic feature extraction from complex input variables. Arbitrarily complex transformations of input variables can be learned from the data efficiently through backpropagation and stochastic gradient descent (SGD). In this paper, we propose DeepMaxent, which harnesses neural networks to automatically learn shared features among species, using the maximum entropy principle. To do so, it employs a normalised Poisson loss where for each species, presence probabilities across sites are modelled by a neural network. We evaluate DeepMaxent on a benchmark dataset known for its spatial sampling biases, using PO data for calibration and presence-absence (PA) data for validation across six regions with different biological groups and covariates. Our results indicate that DeepMaxent performs better than Maxent and other leading SDMs across all regions and taxonomic groups. The method performs particularly well in regions of uneven sampling, demonstrating substantial potential to increase SDM performances. In particular, our approach yields more accurate predictions than traditional single-species models, which opens up new possibilities for methodological enhancement.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2412.19217.pdf",
    "abs_url": "https://arxiv.org/abs/2412.19217",
    "published": "2024-12-26T13:47:04Z",
    "updated": "2026-01-13T10:53:51Z",
    "comment": "Submitted to Methods in Ecology and Evolution",
    "light_analysis": {
      "overview": "DeepMaxent结合最大熵原理与神经网络，自动学习物种共享特征，提升多物种分布模型性能。",
      "motivation": "仅存在（PO）数据因采样偏差和缺乏缺失信息，在物种分布模型（SDM）中应用受限。传统Maxent方法依赖预定义特征，难以捕捉复杂环境模式；神经网络可自动提取特征，但未与熵原理有效结合处理PO数据。因此，研究旨在开发DeepMaxent，通过结合最大熵原理和神经网络，自动学习物种共享特征，以克服现有方法不足，提高多物种建模的准确性和实用性。",
      "method": "DeepMaxent采用神经网络结合最大熵原理，通过归一化Poisson损失函数建模物种存在概率。核心创新是利用反向传播和随机梯度下降（SGD）自动学习输入变量的复杂变换，提取物种间共享特征，避免预定义特征的局限。在具有空间采样偏差的基准数据集上评估，使用PO数据校准、PA数据验证，覆盖六个区域的不同生物组和协变量，以检验模型泛化能力和性能。",
      "result": "实验结果表明，DeepMaxent在所有测试区域和分类群中均优于Maxent及其他领先SDMs。在采样不均的区域表现尤其突出，显著提升预测准确性，相比传统单物种模型更精确。摘要未明确提供具体性能指标数值（如准确率提升百分比），但评估证实方法在处理PO数据时具有优势，增强了SDM性能的潜力。",
      "conclusion": "DeepMaxent的创新在于融合最大熵原理与神经网络，提高多物种分布模型的预测能力，特别是在处理采样偏差数据时。研究贡献在于将自动特征学习引入SDM，克服PO数据局限，对生态学研究和保护规划有实际应用价值。未来可扩展方法到更广泛生物组或集成其他技术，进一步优化模型性能。",
      "tags": [
        "Maximum Entropy Principle",
        "Neural Networks",
        "Poisson Loss",
        "Species Distribution Modeling",
        "Multi-species Models"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:57.182153Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.04948",
    "title": "KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning",
    "authors": [
      "Peng Yu",
      "Cheng Deng",
      "Beiya Dai",
      "Xinbing Wang",
      "Ying Wen"
    ],
    "abstract": "Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured knowledge bases, can provide reliable knowledge for LLMs, potentially compensating for their knowledge deficiencies. Aligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align knowledge representations or compromised the generative capabilities of LLMs, leading to less-than-optimal outcomes. This paper proposes \\textbf{KaLM}, a \\textit{Knowledge-aligned Language Modeling} approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of LLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through triple completion language modeling. Notably, our method achieves a significant performance boost in evaluations of knowledge-driven tasks, specifically embedding-based knowledge graph completion and generation-based knowledge graph question answering.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2412.04948.pdf",
    "abs_url": "https://arxiv.org/abs/2412.04948",
    "published": "2024-12-06T11:08:24Z",
    "updated": "2026-01-13T14:38:40Z",
    "comment": "The article has been accepted by Frontiers of Computer Science (FCS), with the DOI: {10.1007/s11704-026-50906-6}",
    "light_analysis": {
      "overview": "KaLM通过联合显式和隐式知识对齐目标，结合双视图知识图谱对比学习和三元组完成语言建模，微调自回归大型语言模型，显著提升知识驱动任务的性能。",
      "motivation": "自回归大型语言模型在生成任务上表现出色，但在知识驱动任务如事实查询中表现不佳。知识图谱作为高质量结构化知识库，可为LLMs提供可靠知识以弥补其缺陷。然而，现有方法在将LLMs与KGs对齐时，要么未能有效对齐知识表示，要么损害了LLMs的生成能力，导致效果不理想。本研究旨在解决此问题，通过开发新方法有效结合KGs优势，提升LLMs在知识驱动任务上的应用价值。",
      "method": "KaLM方法通过联合显式知识对齐和隐式知识对齐目标，微调自回归大型语言模型。显式对齐采用双视图知识图谱对比学习，直接优化模型的知识表示；隐式对齐通过三元组完成语言建模，将知识的文本模式融入模型。该方法旨在在保持生成能力的同时，有效对齐结构化知识。摘要未明确说明具体使用的数据集或模型架构。",
      "result": "在知识驱动任务的评估中，KaLM方法取得了显著性能提升，具体体现在基于嵌入的知识图谱完成和基于生成的知识图谱问答任务上。尽管摘要未提供具体的性能指标数据，但强调了提升的显著性，表明该方法优于先前尝试，有效对齐了知识表示并改善了任务性能。",
      "conclusion": "KaLM的主要贡献在于提出一种通过联合显式和隐式对齐目标的知识对齐语言建模方法，有效提升自回归大型语言模型在知识驱动任务上的性能。学术上，它解决了LLMs与知识图谱对齐的挑战，提供了新的技术路线。实际应用中，可增强问答系统等任务的准确性和可靠性。未来工作可能包括扩展到更多领域或优化算法效率。",
      "tags": [
        "Autoregressive Language Modeling",
        "Knowledge Graph Contrastive Learning",
        "Triple Completion Modeling",
        "Knowledge Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:58.682438Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.15016",
    "title": "MSSF: A 4D Radar and Camera Fusion Framework With Multi-Stage Sampling for 3D Object Detection in Autonomous Driving",
    "authors": [
      "Hongsi Liu",
      "Jun Liu",
      "Guangfeng Jiang",
      "Xin Jin"
    ],
    "abstract": "As one of the automotive sensors that have emerged in recent years, 4D millimeter-wave radar has a higher resolution than conventional 3D radar and provides precise elevation measurements. But its point clouds are still sparse and noisy, making it challenging to meet the requirements of autonomous driving. Camera, as another commonly used sensor, can capture rich semantic information. As a result, the fusion of 4D radar and camera can provide an affordable and robust perception solution for autonomous driving systems. However, previous radar-camera fusion methods have not yet been thoroughly investigated, resulting in a large performance gap compared to LiDAR-based methods. Specifically, they ignore the feature-blurring problem and do not deeply interact with image semantic information. To this end, we present a simple but effective multi-stage sampling fusion (MSSF) network based on 4D radar and camera. On the one hand, we design a fusion block that can deeply interact point cloud features with image features, and can be applied to commonly used single-modal backbones in a plug-and-play manner. The fusion block encompasses two types, namely, simple feature fusion (SFF) and multiscale deformable feature fusion (MSDFF). The SFF is easy to implement, while the MSDFF has stronger fusion abilities. On the other hand, we propose a semantic-guided head to perform foreground-background segmentation on voxels with voxel feature re-weighting, further alleviating the problem of feature blurring. Extensive experiments on the View-of-Delft (VoD) and TJ4DRadset datasets demonstrate the effectiveness of our MSSF. Notably, compared to state-of-the-art methods, MSSF achieves a 7.0% and 4.0% improvement in 3D mean average precision on the VoD and TJ4DRadSet datasets, respectively. It even surpasses classical LiDAR-based methods on the VoD dataset.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2411.15016.pdf",
    "abs_url": "https://arxiv.org/abs/2411.15016",
    "published": "2024-11-22T15:45:23Z",
    "updated": "2026-01-13T05:15:17Z",
    "comment": "T-TITS accepted, code avaliable",
    "light_analysis": {
      "overview": "本文提出一种基于4D雷达和相机的多阶段采样融合（MSSF）网络，通过深度特征交互和语义引导优化，显著提升自动驾驶中的3D物体检测性能。",
      "motivation": "研究动机源于4D毫米波雷达虽提供高分辨率和精确高度测量，但其点云稀疏且噪声大，难以满足自动驾驶的感知需求；而相机能捕获丰富语义信息，但现有雷达-相机融合方法未深入探索，导致性能远逊于基于LiDAR的方法。具体问题包括忽略特征模糊和缺乏图像语义的深度交互，因此亟需开发更有效的融合框架以提升检测准确性和鲁棒性。",
      "method": "研究方法设计了一个多阶段采样融合（MSSF）网络。关键创新包括：提出可插拔应用的融合块，分为简单特征融合（SFF）和多尺度可变形特征融合（MSDFF），以深度交互点云与图像特征；同时引入语义引导头，通过体素前景-背景分割和特征重加权缓解特征模糊问题。该方法基于通用单模态骨干网络，在数据集View-of-Delft和TJ4DRadSet上进行评估。",
      "result": "在View-of-Delft（VoD）和TJ4DRadSet数据集上的实验结果显示，相比最先进方法，MSSF在VoD数据集上的3D平均精度提升7.0%，在TJ4DRadSet数据集上提升4.0%。值得注意的是，在VoD数据集上，MSSF甚至超越了经典的基于LiDAR的方法，验证了其高性能和有效性。",
      "conclusion": "结论表明，MSSF框架通过融合4D雷达和相机数据，结合多阶段采样和语义引导，显著提高了3D物体检测的精度和鲁棒性，为自动驾驶系统提供了一个经济且可靠的感知解决方案。该研究填补了雷达-相机融合领域的不足，具有重要学术和应用价值；未来可进一步探索更优的融合策略或扩展到其他传感器类型。",
      "tags": [
        "4D Radar",
        "Camera Fusion",
        "Multi-Stage Sampling",
        "3D Object Detection",
        "Semantic-Guided Head"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:48.746962Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.00365",
    "title": "ROSS: RObust decentralized Stochastic learning based on Shapley values",
    "authors": [
      "Lina Wang",
      "Yunsheng Yuan",
      "Feng Li",
      "Lingjie Duan"
    ],
    "abstract": "In the paradigm of decentralized learning, a group of agents collaborate to learn a global model using a distributed dataset without a central server; nevertheless, it is severely challenged by the heterogeneity of the data distribution across the agents. For example, the data may be distributed non-independently and identically, and even be noised or poisoned. To address these data challenges, we propose ROSS, a novel robust decentralized stochastic learning algorithm based on Shapley values, in this paper. Specifically, in each round, each agent aggregates the cross-gradient information from its neighbors, i.e., the derivatives of its local model with respect to the datasets of its neighbors, to update its local model in a momentum like manner, while we innovate in weighting the derivatives according to their contributions measured by Shapley values. We perform solid theoretical analysis to reveal the linear convergence speedup of our ROSS algorithm. We also verify the efficacy of our algorithm through extensive experiments on public datasets. Our results demonstrate that, in face of the above variety of data challenges, our ROSS algorithm has significant advantages over existing state-of-the-art proposals in terms of both convergence and prediction accuracy.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.00365.pdf",
    "abs_url": "https://arxiv.org/abs/2411.00365",
    "published": "2024-11-01T05:05:15Z",
    "updated": "2026-01-13T15:40:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出ROSS算法，一种基于Shapley值的稳健去中心化随机学习算法，用于解决数据分布异质性导致的去中心化学习挑战。",
      "motivation": "去中心化学习让多个智能体协作学习全局模型，但面临数据分布异质性的严重挑战，如数据非独立同分布、噪声或中毒问题。这些问题导致学习过程不稳定和模型性能下降，现有方法在处理这些挑战时效果有限。因此，研究提出新颖算法以提高去中心化学习的鲁棒性，弥补现有技术的不足，确保在复杂数据环境下的可靠应用。",
      "method": "ROSS算法的核心是每个智能体在每轮中聚合来自邻居的交叉梯度信息，即计算局部模型对邻居数据集的导数。创新点在于使用Shapley值衡量每个邻居导数的贡献，并进行加权，以类似于动量的方式更新局部模型。该方法结合了梯度下降和价值分配机制，实现完全去中心化的学习，并通过理论分析探索收敛性质，不依赖中心服务器，强调鲁棒性和效率。",
      "result": "通过在公共数据集上的广泛实验，ROSS算法在数据异质性挑战下展现出优越性能。实验结果表明，相比于现有最先进方法，ROSS在收敛速度上实现显著提升，预测准确性更高，在噪声和中毒数据下仍能保持稳定学习。这些结果验证了算法的鲁棒性和有效性，证明了基于Shapley值的加权机制在改善去中心化学习中的关键作用。",
      "conclusion": "本研究的主要贡献是提出ROSS算法，通过理论分析揭示其线性收敛速度，实验验证了在收敛和预测准确性上的优势。研究增强了去中心化学习在数据异质性环境下的鲁棒性，具有重要学术价值和实际应用潜力，未来可探索算法扩展性和更多应用场景，但摘要未明确说明具体局限性。",
      "tags": [
        "Decentralized Learning",
        "Shapley Values",
        "Stochastic Learning",
        "Cross-Gradient",
        "Robust Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:04.373667Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.00839",
    "title": "CausAdv: A Causal-based Framework for Detecting Adversarial Examples",
    "authors": [
      "Hichem Debbi"
    ],
    "abstract": "Deep learning has led to tremendous success in computer vision, largely due to Convolutional Neural Networks (CNNs). However, CNNs have been shown to be vulnerable to crafted adversarial perturbations. This vulnerability of adversarial examples has has motivated research into improving model robustness through adversarial detection and defense methods. In this paper, we address the adversarial robustness of CNNs through causal reasoning. We propose CausAdv: a causal framework for detecting adversarial examples based on counterfactual reasoning. CausAdv learns both causal and non-causal features of every input, and quantifies the counterfactual information (CI) of every filter of the last convolutional layer. We then perform a statistical analysis of the filters' CI across clean and adversarial samples, to demonstrate that adversarial examples exhibit different CI distributions compared to clean samples. Our results show that causal reasoning enhances the process of adversarial detection without the need to train a separate detector. Moreover, we illustrate the efficiency of causal explanations as a helpful detection tool by visualizing the extracted causal features.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.00839.pdf",
    "abs_url": "https://arxiv.org/abs/2411.00839",
    "published": "2024-10-29T22:57:48Z",
    "updated": "2026-01-13T10:52:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出CausAdv框架，基于因果推理和反事实信息检测对抗性样本，无需训练独立检测器。",
      "motivation": "深度学习模型尤其是卷积神经网络（CNNs）在计算机视觉中表现出色，但易受对抗性扰动攻击，威胁模型鲁棒性和可靠性。现有对抗性检测方法可能依赖于复杂训练流程或额外模型，限制了效率和通用性。本文旨在通过引入因果推理来解决这一不足，提供更直观和有效的检测手段，以提升模型安全性和实际部署的可行性。",
      "method": "CausAdv框架基于因果推理，通过学习每个输入的因果和非因果特征，量化最后一个卷积层中每个滤波器的反事实信息（CI），并进行统计分析以区分对抗性样本与干净样本。关键创新点在于无需训练单独检测器，直接利用模型内部特征进行检测，技术细节如具体数据集和模型架构摘要未明确说明，但强调了可视化因果特征作为解释工具的应用。",
      "result": "研究结果显示，对抗性样本与干净样本在CI分布上存在显著差异，CausAdv框架能够有效检测对抗性扰动，无需额外训练检测器，从而简化了检测流程。通过可视化提取的因果特征，进一步证明了该方法在解释效率和实用性上的优势，具体性能指标如准确率提升或效率改进摘要未明确说明，但与基线方法的对比突出了因果推理的增强效果。",
      "conclusion": "CausAdv框架为对抗性检测提供了基于因果推理的新方法，通过量化反事实信息增强了模型鲁棒性，具有重要的学术价值，推动了对抗性防御的理论创新。其实际应用潜力在于简化检测过程并提高可解释性，未来工作可扩展到更多深度学习模型和多样数据集，以克服潜在局限性并进一步验证泛化能力。",
      "tags": [
        "Causal Reasoning",
        "Counterfactual Reasoning",
        "Adversarial Detection",
        "Convolutional Neural Networks",
        "Feature Visualization"
      ]
    },
    "analyzed_at": "2026-01-14T03:23:35.823137Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.17764",
    "title": "Beyond Backpropagation: Optimization with Multi-Tangent Forward Gradients",
    "authors": [
      "Katharina Flügel",
      "Daniel Coquelin",
      "Marie Weiel",
      "Charlotte Debus",
      "Achim Streit",
      "Markus Götz"
    ],
    "abstract": "The gradients used to train neural networks are typically computed using backpropagation. While an efficient way to obtain exact gradients, backpropagation is computationally expensive, hinders parallelization, and is biologically implausible. Forward gradients are an approach to approximate the gradients from directional derivatives along random tangents computed by forward-mode automatic differentiation. So far, research has focused on using a single tangent per step. This paper provides an in-depth analysis of multi-tangent forward gradients and introduces an improved approach to combining the forward gradients from multiple tangents based on orthogonal projections. We demonstrate that increasing the number of tangents improves both approximation quality and optimization performance across various tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.17764.pdf",
    "abs_url": "https://arxiv.org/abs/2410.17764",
    "published": "2024-10-23T11:02:59Z",
    "updated": "2026-01-13T15:13:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于正交投影的多切线前向梯度改进方法，以提升神经网络梯度近似和优化性能。",
      "motivation": "反向传播作为神经网络梯度计算的主流方法，虽然能提供精确梯度，但面临计算开销大、阻碍并行化以及生物学上不可信的挑战。前向梯度通过前向模式自动微分近似梯度，旨在克服这些问题，但现有研究通常每步仅使用一个切线，可能限制了梯度近似的准确性和优化效果。本研究旨在通过探索多切线方法，改进梯度近似质量，从而推动更高效、可并行化的神经网络训练技术的发展。",
      "method": "本研究深入分析了多切线前向梯度，并引入了一种基于正交投影的改进方法来组合多个切线的向前梯度。核心方法是使用前向模式自动微分计算随机切线的方向导数，然后通过正交投影将这些梯度结合起来，以提高近似精度。关键创新点在于正交投影的使用，它增强了多切线信息的整合能力，优化了梯度近似过程。摘要未明确说明具体的数据集或模型架构细节。",
      "result": "实验结果显示，增加切线的数量在各种任务中显著改善了梯度近似质量和优化性能。与基线方法（如单切线前向梯度）相比，多切线方法在近似误差和最终优化结果上均表现出优势，但摘要未明确说明具体的性能指标数据，如准确率提升百分比或效率改进量化值。",
      "conclusion": "论文的主要贡献在于提供了多切线前向梯度的深入分析，并提出了基于正交投影的改进组合方法，有效提升了梯度近似和优化效果。这为替代传统反向传播提供了新思路，具有在高效神经网络训练和并行计算中的潜在应用价值。可能的局限性包括多切线计算可能增加复杂度，未来工作方向可能涉及优化切线选择策略或扩展到更复杂的深度学习任务。",
      "tags": [
        "Forward Gradients",
        "Automatic Differentiation",
        "Orthogonal Projection",
        "Neural Network Optimization",
        "Multi-Tangent Methods"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:16.423955Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.07145",
    "title": "Stuffed Mamba: Oversized States Lead to the Inability to Forget",
    "authors": [
      "Yingfa Chen",
      "Xinrong Zhang",
      "Shengding Hu",
      "Xu Han",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Recent advancements in recurrent architectures, such as Mamba and RWKV, have showcased strong language capabilities. Unlike transformer-based models, these architectures encode all contextual information into a fixed-size state, leading to great inference efficiency. However, this approach can cause information interference, where different token data conflicts, resulting in performance degradation and incoherent outputs beyond a certain context length. To prevent this, most RNNs incorporate mechanisms designed to \"forget\" earlier tokens. In this paper, we reveal that Mamba-based models struggle to effectively forget earlier tokens even with built-in forgetting mechanisms. We demonstrate that this issue stems from training on contexts that are too short for the state size, enabling the model to perform well without needing to learn how to forget. Then, we show that the minimum training length required for the model to learn forgetting scales linearly with the state size, and the maximum context length for accurate retrieval of a 5-digit passkey scales exponentially with the state size, indicating that the model retains some information beyond the point where forgetting begins. These findings highlight a critical limitation in current RNN architectures and provide valuable insights for improving long-context modeling. Our work suggests that future RNN designs must account for the interplay between state size, training length, and forgetting mechanisms to achieve robust performance in long-context tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2410.07145.pdf",
    "abs_url": "https://arxiv.org/abs/2410.07145",
    "published": "2024-10-09T17:54:28Z",
    "updated": "2026-01-13T07:01:24Z",
    "comment": "COLM 2025",
    "light_analysis": {
      "overview": "论文揭示了Mamba模型在长上下文任务中因状态大小过大而无法有效忘记早期token，并分析了状态大小、训练长度和忘记机制间的相互作用。",
      "motivation": "循环架构如Mamba和RWKV在语言任务中展现出强大性能，通过固定大小状态编码上下文以提高推理效率。然而，这种方法可能导致不同token信息冲突，引发信息干扰，在长上下文任务中造成性能下降和输出不连贯。为防止此问题，多数循环神经网络（RNN）集成忘记机制。但现有Mamba模型即使有忘记机制也难以有效忘记早期token，原因是训练上下文长度相对于状态大小过短，使模型无需学习忘记即可表现良好。此问题对实际应用如文档理解和对话系统至关重要，因为长上下文建模能力直接影响模型效果。",
      "method": "论文通过分析Mamba模型的训练和推理过程，研究状态大小、训练长度和忘记机制之间的关系。核心方法是展示最小训练长度与状态大小呈线性缩放，以及最大有效上下文长度（用于准确检索一个5位数密码）与状态大小呈指数缩放。这些发现表明模型在训练长度不足时无法学会有效忘记，且在忘记开始后仍保留部分信息。摘要未明确说明具体实验设置，但推断涉及设计不同状态大小的模型，并在变长上下文中评估其性能，以验证缩放关系。",
      "result": "主要实验结果显示，Mamba模型的最小训练长度与状态大小线性相关，意味着状态越大，所需训练长度越长以学习忘记。最大有效上下文长度用于准确检索5位数密码与状态大小指数相关，表明模型在达到某个点后仍能保留信息，导致在长上下文中准确率下降。这些结果揭示了当前循环架构在长上下文任务中的性能瓶颈，强调了状态大小设计对忘记能力的影响。与基线方法对比未详细说明，但暗示了现有忘记机制在长上下文中的不足，突出模型在扩展序列中的性能衰减。",
      "conclusion": "本研究揭示了循环神经网络架构如Mamba在长上下文建模中的关键限制，即状态大小过大导致模型难以有效忘记早期token。论文通过展示状态大小、训练长度和忘记机制间的相互作用，为改进长上下文建模提供了重要见解。这些发现具有学术价值，有助于理解循环架构的动态行为，并为实际应用中优化模型设计提供指导。未来工作可探索更有效的忘记机制或调整状态大小与训练策略，以实现在扩展上下文中的稳健性能。局限性可能在于实验范围有限，但指出了未来研究方向，如扩展到更多任务和模型。",
      "tags": [
        "Mamba",
        "Recurrent Neural Networks",
        "Forgetting Mechanisms",
        "Long-Context Modeling",
        "State Size"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:08.126788Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.15235",
    "title": "Learning-based Multi-View Stereo: A Survey",
    "authors": [
      "Fangjinhua Wang",
      "Qingtian Zhu",
      "Di Chang",
      "Quankai Gao",
      "Junlin Han",
      "Tong Zhang",
      "Richard Hartley",
      "Marc Pollefeys"
    ],
    "abstract": "3D reconstruction aims to recover the dense 3D structure of a scene. It plays an essential role in various applications such as Augmented/Virtual Reality (AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene captured from different viewpoints, Multi-View Stereo (MVS) algorithms synthesize a comprehensive 3D representation, enabling precise reconstruction in complex environments. Due to its efficiency and effectiveness, MVS has become a pivotal method for image-based 3D reconstruction. Recently, with the success of deep learning, many learning-based MVS methods have been proposed, achieving impressive performance against traditional methods. We categorize these learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D Gaussian Splatting-based, and large feed-forward methods. Among these, we focus significantly on depth map-based methods, which are the main family of MVS due to their conciseness, flexibility and scalability. In this survey, we provide a comprehensive review of the literature at the time of this writing. We investigate these learning-based methods, summarize their performances on popular benchmarks, and discuss promising future research directions in this area.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2408.15235.pdf",
    "abs_url": "https://arxiv.org/abs/2408.15235",
    "published": "2024-08-27T17:53:18Z",
    "updated": "2026-01-13T11:04:02Z",
    "comment": "Accepted to IEEE T-PAMI 2026",
    "light_analysis": {
      "overview": "本文是一篇综述，全面回顾和分类了基于深度学习的多视图立体重建方法，特别关注深度图基方法。",
      "motivation": "3D重建在增强/虚拟现实、自动驾驶和机器人等领域至关重要。多视图立体重建作为图像基3D重建的核心方法，因其高效性和有效性而被广泛应用。随着深度学习的成功，涌现了许多基于学习的MVS方法，它们在复杂环境中表现优于传统方法，突显了系统综述的必要性，以解决现有方法分类和总结的不足。",
      "method": "本文采用文献综述的方法，对基于学习的多视图立体重建方法进行分类，包括深度图基、体素基、NeRF基、3D高斯溅射基和大前馈方法。重点关注深度图基方法，分析其简洁性、灵活性和可扩展性，并调查这些方法在常用数据集上的应用和模型架构，为读者提供全面的技术路线概览。",
      "result": "论文总结了各种基于学习的MVS方法在流行基准上的性能表现，但摘要未明确说明具体性能指标。它指出这些方法通常优于传统非学习方法，在准确性和效率上有所提升，深度图基方法因其优势而在实际应用中占主导地位，未来工作可能集中在进一步优化性能上。",
      "conclusion": "本文的主要贡献是对基于学习的多视图立体重建方法进行了全面综述，系统分类了不同技术路线，并强调了深度图基方法的重要性。该综述的学术价值在于为研究人员提供了领域现状总结和未来方向，实际应用价值在于指导3D重建技术的发展，促进在AR/VR等领域的应用。未来研究可探索更高效算法和新兴深度学习集成。",
      "tags": [
        "Multi-View Stereo",
        "Depth Map-based Methods",
        "Neural Radiance Fields",
        "3D Gaussian Splatting",
        "Deep Learning"
      ]
    },
    "analyzed_at": "2026-01-14T03:24:56.490262Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.01517",
    "title": "Gradient flow in parameter space is equivalent to linear interpolation in output space",
    "authors": [
      "Thomas Chen",
      "Patrícia Muñoz Ewald"
    ],
    "abstract": "We prove that the standard gradient flow in parameter space that underlies many training algorithms in deep learning can be continuously deformed into an adapted gradient flow which yields (constrained) Euclidean gradient flow in output space. Moreover, for the $L^{2}$ loss, if the Jacobian of the outputs with respect to the parameters is full rank (for fixed training data), then the time variable can be reparametrized so that the resulting flow is simply linear interpolation, and a global minimum can be achieved. For the cross-entropy loss, under the same rank condition and assuming the labels have positive components, we derive an explicit formula for the unique global minimum.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math-ph",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2408.01517.pdf",
    "abs_url": "https://arxiv.org/abs/2408.01517",
    "published": "2024-08-02T18:23:17Z",
    "updated": "2026-01-13T05:40:35Z",
    "comment": "To appear in Journal of Geometry and Physics",
    "light_analysis": {
      "overview": "论文证明了深度学习参数空间的梯度流可通过变形等价于输出空间的线性插值，提供理论保证和全局最小值条件。",
      "motivation": "深度学习训练算法依赖于参数空间的梯度流优化，但现有方法在理论理解和全局收敛性方面存在不足，缺乏对梯度流行为的深入分析。该研究旨在解决优化过程的几何本质问题，通过将梯度流映射到输出空间，揭示训练过程中的等价关系，从而提高算法效率和可靠性。问题的重要性在于优化理论的完善，现有梯度下降等方法可能无法保证全局最小值，尤其是在复杂损失函数下，研究梯度流的变形有助于提供更严格的数学基础。",
      "method": "论文提出将参数空间的标准梯度流连续变形为适应的梯度流，使得在输出空间中表现为欧几里得梯度流。对于L2损失，在输出相对于参数的雅可比矩阵满秩条件下，通过重新参数化时间变量，将梯度流转化为线性插值，从而实现全局最小值的达到。对于交叉熵损失，基于相同秩条件和标签正分量假设，推导了唯一全局最小值的显式公式。关键创新点包括梯度流变形技术、秩条件的应用以及时间参数化方法，利用数学工具如雅可比矩阵分析流的几何特性。",
      "result": "摘要未明确说明具体的实验结果或性能数据。主要理论结果包括：在L2损失下，梯度流可等价于线性插值，并在雅可比矩阵满秩时达到全局最小值；对于交叉熵损失，在相同条件下推导了全局最小值的显式公式。这些结果提供了理论保证，但未涉及实验验证或与基线方法的性能对比，如准确率提升或效率改进等具体指标。",
      "conclusion": "论文的主要贡献是建立了梯度流与输出空间线性插值之间的等价性，为深度学习优化过程提供了理论洞察。学术价值在于深化了对训练算法行为的理解，促进更有效的优化器设计。实际应用可能包括改进训练收敛性和稳定性。局限性在于假设条件如雅可比矩阵满秩可能限制适用性，未来工作可探索放松这些条件、扩展到其他损失函数或结合更复杂的网络架构。",
      "tags": [
        "Gradient Flow",
        "Linear Interpolation",
        "L2 Loss",
        "Cross-Entropy Loss",
        "Jacobian Matrix"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:06.328563Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.17298",
    "title": "Efficient and Scalable Implementation of Differentially Private Deep Learning without Shortcuts",
    "authors": [
      "Sebastian Rodriguez Beltran",
      "Marlon Tobaben",
      "Joonas Jälkö",
      "Niki Loppi",
      "Antti Honkela"
    ],
    "abstract": "Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling to ensure the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads many implementations to taking a shortcut by using computationally faster subsampling. We quantify the computational cost of training deep learning models under DP by implementing and benchmarking efficient methods with the correct Poisson subsampling. We find that using the naive implementation of DP-SGD with Opacus in PyTorch has a throughput between 2.6 and 8 times lower than that of SGD. However, efficient gradient clipping implementations like Ghost Clipping can roughly halve this cost. We propose an alternative computationally efficient implementation of DP-SGD with JAX that uses Poisson subsampling and performs comparably with efficient clipping optimizations based on PyTorch. We study the scaling behavior using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our library at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2406.17298.pdf",
    "abs_url": "https://arxiv.org/abs/2406.17298",
    "published": "2024-06-25T06:04:58Z",
    "updated": "2026-01-13T15:13:42Z",
    "comment": "19 pages, 21 figures",
    "light_analysis": {
      "overview": "本文提出了一种使用JAX的高效可扩展差分隐私深度学习实现，确保正确的泊松子采样，优化计算效率。",
      "motivation": "研究动机是差分隐私随机梯度下降（DP-SGD）作为训练差分隐私机器学习模型的标准算法，为确保理论差分隐私保证，需要使用泊松子采样。然而，实际实现中计算效率低下，导致许多应用采用计算上更快的采样方法作为捷径，这可能牺牲隐私保证。这限制了差分隐私在深度学习中的实际应用，因此量化计算成本并开发高效方法至关重要，以提高隐私保护模型的可行性和可扩展性。",
      "method": "研究方法包括实现和基准测试使用正确泊松子采样的高效DP-SGD算法。作者提出了一种基于JAX的替代实现，采用泊松子采样，并集成梯度裁剪优化技术，如Ghost Clipping。关键创新点在于利用JAX的高性能计算能力，确保理论差分隐私保证，同时研究扩展到多GPU环境（最多80个GPU）的缩放行为，以评估算法的可扩展性和效率。",
      "result": "实验结果表明，使用Opacus的PyTorch实现DP-SGD的吞吐量比标准SGD低2.6到8倍。但通过高效的梯度裁剪优化，如Ghost Clipping，成本可减半。提出的JAX实现与基于PyTorch的优化性能相当，并且在多GPU环境下，DP-SGD的缩放行为优于SGD。这些数据验证了正确实现和优化可以显著提高差分隐私训练的计算效率。",
      "conclusion": "本文的主要贡献是开发并验证了一个高效可扩展的DP-SGD实现，使用JAX和泊松子采样，确保了理论差分隐私保证。学术价值在于强化了隐私保护算法的理论基础，应用价值则通过优化提升实际训练效率，促进隐私敏感场景的模型部署。未来工作可能包括扩展到更多样化的模型和数据集，或进一步探索计算与隐私之间的权衡。",
      "tags": [
        "Differential Privacy",
        "DP-SGD",
        "Poisson Subsampling",
        "Gradient Clipping",
        "JAX"
      ]
    },
    "analyzed_at": "2026-01-14T03:25:15.575110Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.15718",
    "title": "Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models",
    "authors": [
      "Xinrong Zhang",
      "Yingfa Chen",
      "Shengding Hu",
      "Xu Han",
      "Zihang Xu",
      "Yuanwei Xu",
      "Weilin Zhao",
      "Maosong Sun",
      "Zhiyuan Liu"
    ],
    "abstract": "As large language models (LLMs) increasingly permeate daily lives, there is a growing demand for real-time interactions that mirror human conversations. Traditional turn-based chat systems driven by LLMs prevent users from verbally interacting with the system while it is generating responses. To overcome these limitations, we adapt existing LLMs to \\textit{duplex models} so that these LLMs can listen for users while generating output and dynamically adjust themselves to provide users with instant feedback. % such as in response to interruptions. Specifically, we divide the queries and responses of conversations into several time slices and then adopt a time-division-multiplexing (TDM) encoding-decoding strategy to pseudo-simultaneously process these slices. Furthermore, to make LLMs proficient enough to handle real-time conversations, we build a fine-tuning dataset consisting of alternating time slices of queries and responses as well as covering typical feedback types in instantaneous interactions. Our experiments show that although the queries and responses of conversations are segmented into incomplete slices for processing, LLMs can preserve their original performance on standard benchmarks with a few fine-tuning steps on our dataset. Automatic and human evaluation indicate that duplex models make user-AI interactions more natural and human-like, and greatly improve user satisfaction compared to vanilla LLMs. Our duplex model and dataset will be released.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2406.15718.pdf",
    "abs_url": "https://arxiv.org/abs/2406.15718",
    "published": "2024-06-22T03:20:10Z",
    "updated": "2026-01-13T13:58:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出双工模型，通过时分复用技术使大型语言模型支持实时对话交互。",
      "motivation": "随着大型语言模型在日常生活日益普及，用户对实时交互的需求增长。传统基于轮次的聊天系统采用阻塞式交互，用户在AI生成响应时无法进行口头输入，这限制了对话的自然性和流畅性。现有方法未能有效解决实时对话中的打断和即时反馈问题，因此需要开发能够模拟人类实时对话的AI系统来提升用户体验。",
      "method": "论文的核心方法是构建双工模型，基于现有大型语言模型进行微调。技术特色包括将对话查询和响应分割为多个时间切片，并采用时分复用编码-解码策略伪同时处理这些切片，使模型在生成输出时能监听用户输入并动态调整。此外，构建了微调数据集，包含查询和响应的交替时间切片，覆盖即时交互中的典型反馈类型如中断等。",
      "result": "实验表明，尽管对话内容被分割为不完整的切片处理，大型语言模型在经过数据集微调后能在标准基准上保持其原始性能。自动和人工评估结果显示，双工模型显著提升了用户-AI交互的自然度和人类相似性，用户满意度相比原始大型语言模型有较大提高，具体改进体现在更自然的实时反馈。",
      "conclusion": "该研究的主要贡献是提出并实现了双工模型，使大型语言模型能够支持实时对话交互。学术上，它改进了交互技术，为AI对话系统提供了新思路；实际应用中，提高了用户体验，促进更自然的AI助手发展。未来工作可能包括优化模型效率或扩展数据集，论文计划发布模型和数据集以供社区使用。",
      "tags": [
        "Large Language Model",
        "Duplex Model",
        "Time-Division Multiplexing",
        "Fine-tuning",
        "Real-time Conversation"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:36.780054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.13257",
    "title": "Explaning with trees: interpreting CNNs using hierarchies",
    "authors": [
      "Caroline Mazini Rodrigues",
      "Nicolas Boutry",
      "Laurent Najman"
    ],
    "abstract": "Challenges persist in providing interpretable explanations for neural network reasoning in explainable AI (xAI). Existing methods like Integrated Gradients produce noisy maps, and LIME, while intuitive, may deviate from the model's reasoning. We introduce a framework that uses hierarchical segmentation techniques for faithful and interpretable explanations of Convolutional Neural Networks (CNNs). Our method constructs model-based hierarchical segmentations that maintain the model's reasoning fidelity and allows both human-centric and model-centric segmentation. This approach offers multiscale explanations, aiding bias identification and enhancing understanding of neural network decision-making. Experiments show that our framework, xAiTrees, delivers highly interpretable and faithful model explanations, not only surpassing traditional xAI methods but shedding new light on a novel approach to enhancing xAI interpretability.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2406.13257.pdf",
    "abs_url": "https://arxiv.org/abs/2406.13257",
    "published": "2024-06-19T06:45:19Z",
    "updated": "2026-01-13T10:20:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一个使用层次分割技术来解释卷积神经网络推理的框架，名为xAiTrees，旨在提供忠实且可解释的模型解释。",
      "motivation": "在可解释人工智能领域，为神经网络的推理提供可解释的解释仍面临显著挑战。现有方法如Integrated Gradients产生的解释图噪声较大，不够清晰；而LIME方法虽然直观，但可能偏离模型的真实推理过程，导致解释的忠实性不足。这些问题限制了xAI的实用性和可靠性，因此需要开发能够保持模型推理忠实性同时易于人类理解的新解释方法，以提升模型透明度和可信度。",
      "method": "论文提出的xAiTrees框架采用层次分割技术来构建基于卷积神经网络模型的分割图。该方法的核心创新在于创建模型基础的层次化分割，确保解释过程与模型推理一致，支持人中心和模型中心两种分割方式，前者便于人类理解，后者保证解释的忠实性。通过多尺度解释，捕捉不同粒度下的关键特征，有助于识别模型中的偏见并深入理解决策机制。具体技术包括使用分割算法生成层次结构，以增强解释的可视化和分析能力。",
      "result": "实验结果表明，xAiTrees框架生成的解释具有高度的可解释性和忠实性，能够有效超越传统xAI方法如Integrated Gradients和LIME。摘要未明确说明具体的性能指标数据（如准确率提升），但强调该方法在提供清晰、准确的解释方面表现优越，并为增强xAI的可解释性提供了新的技术途径。这表明该方法在改善模型透明度和可信度方面具有潜在优势。",
      "conclusion": "该论文的主要贡献是开发了xAiTrees框架，通过层次分割技术为卷积神经网络提供忠实且可解释的解释。研究的学术价值在于推动了可解释AI技术的发展，提高了模型推理的透明度；实际应用价值包括辅助偏见检测、增强决策理解，并可能扩展到其他深度学习模型。未来工作可以包括优化分割算法、扩展应用场景，或集成更多解释维度以进一步提升解释质量，并探索在更广泛AI任务中的适用性。",
      "tags": [
        "Explainable AI",
        "Convolutional Neural Networks",
        "Hierarchical Segmentation",
        "Interpretability",
        "Model-based Segmentation"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:02.628795Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.09838",
    "title": "ClimateIQA: A New Dataset and Benchmark to Advance Vision-Language Models in Meteorology Anomalies Analysis",
    "authors": [
      "Jian Chen",
      "Peilin Zhou",
      "Yining Hua",
      "Dading Chong",
      "Meng Cao",
      "Yaowei Li",
      "Wei Chen",
      "Bing Zhu",
      "Junwei Liang",
      "Zixuan Yuan"
    ],
    "abstract": "Meteorological heatmaps play a vital role in deciphering extreme weather phenomena, yet their inherent complexities marked by irregular contours, unstructured patterns, and complex color variations present unique analytical hurdles for state-of-the-art Vision-Language Models (VLMs). Current state-of-the-art models like GPT-4o, Qwen-VL, and LLaVA 1.6 struggle with tasks such as precise color identification and spatial localization, resulting in inaccurate or incomplete interpretations. To address these challenges, we introduce Sparse Position and Outline Tracking (SPOT), a novel algorithm specifically designed to process irregularly shaped colored regions in visual data. SPOT identifies and localizes these regions by extracting their spatial coordinates, enabling structured representations of irregular shapes. Building on SPOT, we construct ClimateIQA, a novel meteorological visual question answering (VQA) dataset, comprising 26,280 high-resolution heatmaps and 762,120 instruction samples for wind gust, total precipitation, wind chill index and heat index analysis. ClimateIQA enhances VLM training by incorporating spatial cues, geographic metadata, and reanalysis data, improving model accuracy in interpreting and describing extreme weather features. Furthermore, we develop Climate-Zoo, a suite of fine-tuned VLMs based on SPOT-empowered ClimateIQA, which significantly outperforms existing models in meteorological heatmap tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2406.09838.pdf",
    "abs_url": "https://arxiv.org/abs/2406.09838",
    "published": "2024-06-14T08:46:44Z",
    "updated": "2026-01-13T11:47:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SPOT算法和ClimateIQA数据集，显著提升了视觉-语言模型在气象异常分析中的准确性。",
      "motivation": "气象热图因其不规则轮廓、非结构化模式和复杂颜色变化，对现有视觉-语言模型如GPT-4o、Qwen-VL和LLaVA 1.6构成分析挑战，导致在精确颜色识别和空间定位上的不准确。这一问题对解读极端天气现象至关重要，因为准确的气象数据对灾害预警和气候研究具有重要价值。当前模型在这些任务上的不足，限制了它们在气象科学和实际应用中的有效性，亟需创新方法以克服这些挑战。",
      "method": "论文提出Sparse Position and Outline Tracking (SPOT)算法，专门设计用于处理视觉数据中不规则形状的彩色区域，通过提取空间坐标实现结构化表示。基于SPOT，构建了ClimateIQA数据集，包含26,280张高分辨率热图和762,120个指令样本，用于风突、总降水、风寒指数和热指数分析。数据集整合了空间线索、地理元数据和再分析数据，增强了模型训练。此外，开发了Climate-Zoo，一套基于SPOT-empowered ClimateIQA微调的视觉-语言模型。",
      "result": "Climate-Zoo在气象热图分析任务上显著优于现有模型如GPT-4o和Qwen-VL。摘要未明确给出具体性能指标，但表明新模型通过SPOT算法和ClimateIQA训练，在颜色识别和空间定位方面有所改进，从而提高了整体准确性。与基线方法相比，现有模型在处理不规则模式时表现不佳，而新方法通过结构化表示有效克服了这些挑战，实现了更精确的气象异常解读。",
      "conclusion": "本研究的主要贡献在于提出SPOT算法和ClimateIQA数据集，推进了视觉-语言模型在气象分析中的应用。学术上，它展示了如何针对特定领域优化VLMs以处理复杂视觉数据；实际上，提升了极端天气特征分析的准确性，对气候研究和灾害管理有重要价值。摘要未明确说明局限性或未来工作方向，但可以预见在更广泛气象数据上的扩展和模型性能的进一步提升。",
      "tags": [
        "Vision-Language Models",
        "Visual Question Answering",
        "SPOT Algorithm",
        "ClimateIQA Dataset",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:17.941261Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2405.16041",
    "title": "Explainable Molecular Property Prediction: Aligning Chemical Concepts with Predictions via Language Models",
    "authors": [
      "Zhenzhong Wang",
      "Zehui Lin",
      "Wanyu Lin",
      "Ming Yang",
      "Minggang Zeng",
      "Kay Chen Tan"
    ],
    "abstract": "Providing explainable molecular property predictions is critical for many scientific domains, such as drug discovery and material science. Though transformer-based language models have shown great potential in accurate molecular property prediction, they neither provide chemically meaningful explanations nor faithfully reveal the molecular structure-property relationships. In this work, we develop a framework for explainable molecular property prediction based on language models, dubbed as Lamole, which can provide chemical concepts-aligned explanations. We take a string-based molecular representation -- Group SELFIES -- as input tokens to pretrain and fine-tune our Lamole, as it provides chemically meaningful semantics. By disentangling the information flows of Lamole, we propose combining self-attention weights and gradients for better quantification of each chemically meaningful substructure's impact on the model's output. To make the explanations more faithfully respect the structure-property relationship, we then carefully craft a marginal loss to explicitly optimize the explanations to be able to align with the chemists' annotations. We bridge the manifold hypothesis with the elaborated marginal loss to prove that the loss can align the explanations with the tangent space of the data manifold, leading to concept-aligned explanations. Experimental results over six mutagenicity datasets and one hepatotoxicity dataset demonstrate Lamole can achieve comparable classification accuracy and boost the explanation accuracy by up to 14.3%, being the state-of-the-art in explainable molecular property prediction.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2405.16041.pdf",
    "abs_url": "https://arxiv.org/abs/2405.16041",
    "published": "2024-05-25T03:27:04Z",
    "updated": "2026-01-13T02:36:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了名为Lamole的可解释分子性质预测框架，通过语言模型实现化学概念对齐的解释，提升了解释的准确性和可靠性。",
      "motivation": "在药物发现和材料科学等领域，提供可解释的分子性质预测至关重要，因为科学家需要理解模型决策以信任和应用结果。现有基于Transformer的语言模型虽然能准确预测分子性质，但缺乏化学上有意义的解释，无法忠实地揭示分子结构-性质关系，限制了其在科学应用中的实用性。因此，开发一个能提供化学概念对齐解释的预测框架是当前研究的重要方向。",
      "method": "该研究使用Group SELFIES作为输入令牌，预训练和微调Lamole语言模型，确保输入具有化学意义。通过解耦模型信息流，结合自注意力权重和梯度来量化每个化学子结构对输出的影响。为了优化解释，设计了一个边际损失函数，对齐化学家的标注，并引入流形假说证明损失能将解释对齐到数据流形的切空间，实现概念对齐的解释。",
      "result": "在六个诱变性数据集和一个肝毒性数据集上的实验表明，Lamole的分类准确率与基线方法相当，同时将解释准确率提升最多14.3%，成为可解释分子性质预测领域的当前最佳方法。具体地，该框架在保持预测性能的同时，显著改善了解释的可信度和化学相关性，与现有技术相比表现出明显优势。",
      "conclusion": "该论文的主要贡献是提出了Lamole框架，成功提升了解释的化学对齐性和准确性，具有重要的学术价值，推动了可解释AI在科学计算中的应用。其实践意义在于为药物发现等领域提供可靠工具，增强模型可信度。未来工作可能包括扩展到更多分子性质预测任务或优化算法以提高泛化能力，但摘要未明确说明具体方向。",
      "tags": [
        "Explainable AI",
        "Language Models",
        "Self-Attention",
        "Gradient Analysis",
        "Chemical Concept Alignment"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:22.915501Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2311.12715",
    "title": "Attacks on fairness in Federated Learning",
    "authors": [
      "Joseph Rance",
      "Filip Svoboda"
    ],
    "abstract": "Federated Learning is an important emerging distributed training paradigm that keeps data private on clients. It is now well understood that by controlling only a small subset of FL clients, it is possible to introduce a backdoor to a federated learning model, in the presence of certain attributes. In this paper, we present a new type of attack that compromises the fairness of the trained model. Fairness is understood to be the attribute-level performance distribution of a trained model. It is particularly salient in domains where, for example, skewed accuracy discrimination between subpopulations could have disastrous consequences. We find that by employing a threat model similar to that of a backdoor attack, an attacker is able to influence the aggregated model to have an unfair performance distribution between any given set of attributes. Furthermore, we find that this attack is possible by controlling only a single client. While combating naturally induced unfairness in FL has previously been discussed in depth, its artificially induced kind has been neglected. We show that defending against attacks on fairness should be a critical consideration in any situation where unfairness in a trained model could benefit a user who participated in its training.",
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2311.12715.pdf",
    "abs_url": "https://arxiv.org/abs/2311.12715",
    "published": "2023-11-21T16:42:03Z",
    "updated": "2026-01-13T11:56:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种针对联邦学习中模型公平性的新型攻击，填补了人工诱导不公平性威胁的研究空白。",
      "motivation": "联邦学习作为分布式训练范式，虽能保护数据隐私，但现有攻击如后门攻击可控制小部分客户端引入后门。公平性是模型在属性级别性能分布的关键，在医疗、金融等领域至关重要，性能偏斜可能导致灾难性后果。然而，先前研究多关注自然诱导的不公平性，人为诱导的公平性攻击被忽视，这暴露了联邦学习安全的新威胁和潜在风险。",
      "method": "本文采用类似后门攻击的威胁模型，攻击者通过控制单个FL客户端，在本地训练过程中引入偏差，从而影响全局聚合模型的公平性分布。关键创新在于将攻击目标从传统准确率转移到公平性，展示了在联邦学习中人为操控性能分布的可能性。摘要未明确说明具体数据集或模型架构，但方法基于联邦学习框架，利用客户端操控实现攻击，强调了攻击的简单性和隐蔽性。",
      "result": "实验结果显示，攻击者仅需控制一个FL客户端，即可成功诱导聚合模型在不同属性间出现不公平性能分布。尽管摘要未提供具体性能指标如准确率变化，但通过威胁模型验证了攻击的可行性，表明人工诱导的不公平性可通过简单操控实现，且更具隐蔽性，可能在不影响整体准确率的情况下破坏公平性，突显了新攻击的有效性。",
      "conclusion": "本文首次系统性地揭示了联邦学习中针对公平性的攻击威胁，填补了人工诱导不公平性研究空白。学术上，这扩展了联邦学习安全领域，强调了公平性攻击的重要性；实际中，在公平性关键的应用如法律或信用评分，需加强防御措施以防止恶意操控。未来工作可能包括开发有效防御机制和评估标准，以应对这一新挑战。",
      "tags": [
        "Federated Learning",
        "Fairness",
        "Adversarial Attacks",
        "Threat Modeling",
        "Machine Learning Security"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:41.670894Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2304.13372",
    "title": "Feed-Forward Optimization With Delayed Feedback for Neural Network Training",
    "authors": [
      "Katharina Flügel",
      "Daniel Coquelin",
      "Marie Weiel",
      "Charlotte Debus",
      "Achim Streit",
      "Markus Götz"
    ],
    "abstract": "Backpropagation has long been criticized for being biologically implausible due to its reliance on concepts that are not viable in natural learning processes. Two core issues are the weight transport and update locking problems caused by the forward-backward dependencies, which limit biological plausibility, computational efficiency, and parallelization. Although several alternatives have been proposed to increase biological plausibility, they often come at the cost of reduced predictive performance. This paper proposes an alternative approach to training feed-forward neural networks addressing these issues by using approximate gradient information. We introduce Feed-Forward with delayed Feedback (F$^3$), which approximates gradients using fixed random feedback paths and delayed error information from the previous epoch to balance biological plausibility with predictive performance. We evaluate F$^3$ across multiple tasks and architectures, including both fully-connected and Transformer networks. Our results demonstrate that, compared to similarly plausible approaches, F$^3$ significantly improves predictive performance, narrowing the gap to backpropagation by up to 56% for classification and 96% for regression. This work is a step towards more biologically plausible learning algorithms while opening up new avenues for energy-efficient and parallelizable neural network training.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2304.13372.pdf",
    "abs_url": "https://arxiv.org/abs/2304.13372",
    "published": "2023-04-26T08:28:46Z",
    "updated": "2026-01-13T15:40:45Z",
    "comment": "This is the submitted manuscript of the version of record published in the proceedings to the 31st International Conference on Neural Information Processing (ICONIP 2024), Lecture Notes in Computer Science, vol 15289, and available online at https://doi.org/10.1007/978-981-96-6585-3_6",
    "light_analysis": {
      "overview": "论文提出F³方法，一种使用固定随机反馈和延迟误差信息的近似梯度优化，旨在解决反向传播的生物学不可信性问题，同时显著缩小与反向传播的性能差距。",
      "motivation": "反向传播被批评为生物学不可信，因为它依赖前向-后向依赖，导致权重传输和更新锁定问题，这不仅限制了学习过程的生物学可信性，还影响了计算效率和并行化能力。现有方法虽试图增加生物学可信性，但往往以牺牲预测性能为代价。因此，本研究旨在开发一种新方法，在保持预测性能的同时，提高生物学可信性和训练效率，以推动更接近自然学习的算法发展。",
      "method": "论文引入Feed-Forward with delayed Feedback (F³)方法，通过使用固定的随机反馈路径和来自前一时期的延迟误差信息来近似梯度，避免了传统反向传播的直接前向-后向依赖。这种方法的核心创新在于利用延迟反馈机制，减少了权重传输和更新锁定的问题，适用于多种神经网络架构，包括全连接网络和Transformer网络，并在多个任务上进行评估以确保通用性和有效性。",
      "result": "实验结果显示，F³在多个任务上显著提升了预测性能，与类似生物学可信的方法相比，它将与反向传播的差距缩小了高达56%用于分类任务和96%用于回归任务。这表明F³在保持生物学可信性的同时，能有效接近反向传播的性能水平，验证了该方法在平衡可信性与性能方面的优势，并为未来优化提供了实证基础。",
      "conclusion": "本研究的主要贡献是提出F³方法，推动了更生物学可信的学习算法发展，并为能量高效和可并行化的神经网络训练开辟了新途径。尽管摘要未明确说明局限性，但未来工作可能集中在进一步优化算法性能、扩展到更复杂架构或探索实际应用场景上，以增强其学术和实际价值。",
      "tags": [
        "Feed-Forward Neural Networks",
        "Delayed Feedback",
        "Gradient Approximation",
        "Biological Plausibility",
        "Transformer Networks"
      ]
    },
    "analyzed_at": "2026-01-14T03:26:42.548199Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2110.03684",
    "title": "Cross-Domain Imitation Learning via Optimal Transport",
    "authors": [
      "Arnaud Fickinger",
      "Samuel Cohen",
      "Stuart Russell",
      "Brandon Amos"
    ],
    "abstract": "Cross-domain imitation learning studies how to leverage expert demonstrations of one agent to train an imitation agent with a different embodiment or morphology. Comparing trajectories and stationary distributions between the expert and imitation agents is challenging because they live on different systems that may not even have the same dimensionality. We propose Gromov-Wasserstein Imitation Learning (GWIL), a method for cross-domain imitation that uses the Gromov-Wasserstein distance to align and compare states between the different spaces of the agents. Our theory formally characterizes the scenarios where GWIL preserves optimality, revealing its possibilities and limitations. We demonstrate the effectiveness of GWIL in non-trivial continuous control domains ranging from simple rigid transformation of the expert domain to arbitrary transformation of the state-action space.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2110.03684.pdf",
    "abs_url": "https://arxiv.org/abs/2110.03684",
    "published": "2021-10-07T17:59:49Z",
    "updated": "2026-01-13T03:56:14Z",
    "comment": "ICLR 2022",
    "light_analysis": {
      "overview": "提出基于Gromov-Wasserstein距离的跨域模仿学习方法，有效对齐不同代理空间以解决轨迹比较挑战。",
      "motivation": "跨域模仿学习旨在利用专家代理的演示来训练不同形态的模仿代理，但专家和模仿代理处于不同系统，轨迹和分布比较困难，因为它们可能生活在维度不匹配的空间中。这在实际应用中很重要，例如训练不同形态的机器人，而现有方法可能无法有效处理这种跨域对齐问题，限制了模仿效果。因此，需要新方法来比较不同代理状态空间。",
      "method": "论文提出Gromov-Wasserstein Imitation Learning (GWIL)，这是一种跨域模仿学习方法，利用最优传输中的Gromov-Wasserstein距离来对齐和比较专家和模仿代理的不同状态空间。该方法通过度量空间之间的距离来处理代理系统维度不匹配问题。理论分析形式化描述了GWIL保留最优性的场景，揭示了方法的可能性和局限性。在实现中，应用于非平凡连续控制域，涵盖从简单刚性变换到任意状态-动作空间变换的场景。",
      "result": "论文在非平凡连续控制域中演示了GWIL的有效性，范围从专家域的简单刚性变换到状态-动作空间的任意变换。摘要未明确说明具体性能指标（如准确率提升）或与基线方法的对比数据，但表明该方法在这些场景中表现良好，有效解决了跨域对齐挑战。",
      "conclusion": "GWIL通过Gromov-Wasserstein距离解决了跨域模仿学习的核心状态空间对齐问题，理论分析形式化描述了保留最优性的条件，为领域提供了理论基础。该方法在连续控制域中验证了有效性，具有实际应用价值，例如训练不同形态的智能体。局限性在于理论揭示的适用场景限制，未来工作可探索更复杂的变换或扩展其他领域。",
      "tags": [
        "Imitation Learning",
        "Optimal Transport",
        "Gromov-Wasserstein Distance",
        "Cross-Domain Learning",
        "Continuous Control"
      ]
    },
    "analyzed_at": "2026-01-14T03:27:58.556174Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]