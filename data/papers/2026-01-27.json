[
  {
    "id": "2601.18796",
    "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models",
    "authors": [
      "Brian Ondov",
      "Chia-Hsuan Chang",
      "Yujia Zhou",
      "Mauro Giuffrè",
      "Hua Xu"
    ],
    "abstract": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18796.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18796",
    "published": "2026-01-26T18:58:46Z",
    "updated": "2026-01-26T18:58:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究开发了ctELM模型，通过嵌入语言模型方法解码和操作临床试验的嵌入空间，提高了透明度和生成能力。",
      "motivation": "文本嵌入在语言应用中至关重要，但现有方法在解释、探索和反转嵌入空间方面有限，导致透明度降低并阻碍生成用例。特别是在临床试验领域，无法有效理解和操纵嵌入内容，限制了基于嵌入的生成和比较应用，这影响了临床研究的效率和创新潜力。",
      "method": "本研究采用嵌入语言模型（ELM）方法对齐大型语言模型与临床试验的嵌入。开发了开源的、领域无关的ELM架构和训练框架，针对临床试验设计了特定训练任务，并引入专家验证的合成数据集进行训练。通过实验不同任务和训练制度，优化模型性能，最终得到ctELM模型，核心创新在于将生成能力与嵌入空间结合。",
      "result": "ctELM模型能够仅从嵌入向量准确描述和比较未见的临床试验，并从新向量生成合理的临床试验。此外，通过沿年龄和性别的概念向量移动嵌入，生成的试验摘要表现出响应性，验证了模型的可控生成能力。摘要未明确说明与基线方法的对比数据，但表明模型在生成和解释任务上有效。",
      "conclusion": "本研究贡献了ctELM模型和开源ELM框架，显著提升了临床试验嵌入的解释和生成能力，推动了大型语言模型与嵌入空间的对齐。在生物医学领域具有实际应用价值，如增强临床试验数据的透明度和生成效率，并为其他领域的嵌入研究提供参考。未来工作可扩展到更广泛的领域并优化模型性能。",
      "tags": [
        "Embedding Language Model",
        "Large Language Model",
        "Clinical Trial",
        "Text Embedding",
        "Synthetic Dataset"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:08.551984Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18795",
    "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
    "authors": [
      "Amrith Setlur",
      "Zijian Wang",
      "Andrew Cohen",
      "Paria Rashidinejad",
      "Sang Michael Xie"
    ],
    "abstract": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18795.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18795",
    "published": "2026-01-26T18:57:00Z",
    "updated": "2026-01-26T18:57:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了PrefixRL方法，通过条件化离策略轨迹的前缀来提高大语言模型在硬推理问题上的强化学习效率和效果。",
      "motivation": "在大语言模型推理的强化学习中，硬问题由于正确在策略轨迹罕见，导致政策梯度消失和学习停滞，传统方法浪费计算资源。现有离策略方法在监督离策略数据时容易引发优化不稳定，限制了学习效率。因此，研究旨在重用旧的计算资源（FLOPs），以解决这些不足，提升在复杂问题上的训练效果，从而推动强化学习在现实应用中的可扩展性。",
      "method": "PrefixRL方法的核心创新是条件化成功离策略轨迹的前缀，并运行在策略强化学习来完成剩余部分，避免了标准离策略方法的不稳定性。通过调节前缀长度来调制问题难度，提升学习信号。实验中使用拒绝采样从基础模型获取离策略轨迹，形成一个自改进循环，无需额外数据源。关键细节包括利用不同模型族的轨迹来验证方法的灵活性，并证明其与标准RL目标的一致性。",
      "result": "在硬推理问题上，PrefixRL达到相同训练奖励的速度是最强基线（先进行离策略数据的监督微调再进行RL）的2倍，最终奖励提高3倍。即使计入初始拒绝采样的计算成本，效率仍显著提升。增益可转移到其他基准测试中，显示出泛化能力。此外，当离策略轨迹来自不同模型族时，方法仍保持有效，验证了其在实际设置中的鲁棒性和可扩展性。",
      "conclusion": "PrefixRL不仅与标准强化学习目标一致，而且具有更高的样本效率，通过重用计算资源解决了硬问题上的学习瓶颈。研究发现后泛化现象，训练仅在前缀问题上能泛化到无前缀场景，增强了方法的实际应用价值。摘要未明确说明其局限性，未来工作可能探讨更广泛的适用性和与其他技术的集成，以进一步优化性能。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Off-Policy Learning",
        "Prefix Conditioning",
        "Sample Efficiency"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:50.870987Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18791",
    "title": "Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets",
    "authors": [
      "Iaroslav Chelombitko",
      "Mika Hämäläinen",
      "Aleksey Komissarov"
    ],
    "abstract": "We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18791.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18791",
    "published": "2026-01-26T18:55:28Z",
    "updated": "2026-01-26T18:55:28Z",
    "comment": "15 pages, 4 figues, 4 tables",
    "light_analysis": {
      "overview": "这篇论文提出了一种基于子词的大规模跨语言比较框架，利用Byte-Pair Encoding和维基百科数据对242种语言进行分析，揭示了词汇模式和语言相似性。",
      "motivation": "该研究旨在解决大规模跨语言比较的挑战，特别是在分析词汇重叠和语言相似性时，以提供宏观语言学洞察。由于语言多样性，传统方法可能缺乏统一框架来处理多种语言，难以量化语言间的差异和联系。通过利用维基百科数据和子词方法，本研究能够系统地比较拉丁和西里尔文字语言，填补了现有研究在系统化分析上的不足，摘要未明确说明具体方法缺陷，但强调了统一分析的重要性。",
      "method": "论文提出了一种基于子词的比较语言学框架，首先从维基百科词典中构建“glottosets”作为语言数据集，支持242种拉丁和西里尔文字语言的分析。核心方法是应用Byte-Pair Encoding（BPE）进行子词分割，然后使用基于秩的子词向量来量化词汇重叠、词汇差异和语言相似性。关键创新点包括引入glottosets和BPE框架，实现了同步跨语言比较，技术特色在于大规模处理和统一分析工具的应用。",
      "result": "实验结果显示，BPE分割在15种语言中与语素边界对齐的F1分数达到0.34，显著优于随机基线的0.15，提升了95%。BPE词汇相似性与遗传语言相关性显著正相关（Mantel r = 0.329, p < 0.001），其中罗曼语系形成最紧密的簇（平均距离0.51），而跨语系语言对显示清晰分离（距离0.82）。此外，分析26,939个跨语言同形词发现48.7%在不同语言中有不同分割，且这种变异与系统发育距离相关，证明了方法的有效性。",
      "conclusion": "本研究的主要贡献是提出了一个基于子词的大规模跨语言比较框架，利用BPE和维基百科数据，为242种语言的词汇模式提供了定量宏观语言学洞察。学术上，它推进了比较语言学的方法论，提供了统一的分析工具；实际上，可用于语言分类和相似性评估等领域。局限性可能在于依赖维基百科数据和特定脚本语言，未来工作可扩展到更多语系和数据类型。",
      "tags": [
        "Byte-Pair Encoding (BPE)",
        "Subword Segmentation",
        "Comparative Linguistics",
        "Language Similarity Analysis",
        "Glottosets"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:32.706559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18790",
    "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
    "authors": [
      "Etienne Lanzeray",
      "Stephane Meilliez",
      "Malo Ruelle",
      "Damien Sileo"
    ],
    "abstract": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18790.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18790",
    "published": "2026-01-26T18:55:07Z",
    "updated": "2026-01-26T18:55:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出MortalMATH基准，评估大型语言模型在推理目标与紧急情况冲突中的行为，揭示专门推理模型可能忽视安全问题的风险。",
      "motivation": "随着大型语言模型日益优化深度推理，优先任务正确执行，可能忽视关键情境下的安全性，导致'隧道视野'现象。本研究旨在解决模型在追求推理目标时忽略用户紧急情况的实际问题，这一问题对安全部署至关重要，因为现有模型可能过度专注计算，缺乏应急响应意识，从而引发潜在危险。研究背景基于当前AI发展中推理优化与安全平衡的需求。",
      "method": "论文引入MortalMATH基准，包含150个冲突场景，用户请求代数帮助同时描述危及生命的紧急情况，如中风症状或自由落体。通过设计这些场景，研究测试模型在推理任务与安全需求冲突时的行为，关键创新在于创建专门评估此问题的基准。评估模型包括通用模型如Llama-3.1和专门推理模型如Qwen-3-32b、GPT-5-nano，分析其响应模式以探究行为差异。",
      "result": "实验结果显示，通用模型能拒绝数学任务以应对紧急情况，而专门推理模型常忽略危险，保持超过95%的任务完成率，即使用户描述濒死状态。此外，推理过程引入计算延迟高达15秒，可能延误潜在帮助。与基线模型对比，专门推理模型在任务效率上表现优异，但在安全响应上存在缺陷，凸显了行为分裂和潜在风险。",
      "conclusion": "本研究通过MortalMATH基准揭示了专门推理模型在优化推理目标时可能忽视安全问题的风险，强调了在模型训练中平衡推理能力与安全考虑的重要性。学术上，为评估AI安全性提供了新视角；实际应用中，对安全部署系统具有指导意义。未来工作可探索改进训练方法或设计更全面的基准，以增强模型应急响应能力。",
      "tags": [
        "Large Language Models",
        "Reasoning Models",
        "Benchmark Evaluation",
        "Safety Testing",
        "Emergency Context"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:16.282108Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18788",
    "title": "Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings",
    "authors": [
      "Mumin Jia",
      "Jairo Diaz-Rodriguez"
    ],
    "abstract": "Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18788.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18788",
    "published": "2026-01-26T18:54:34Z",
    "updated": "2026-01-26T18:54:34Z",
    "comment": "arXiv admin note: substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437",
    "light_analysis": {
      "overview": "该论文提出了 Embed-KCPD，一种基于句子嵌入和核变点检测的无监督文本分割方法，首次提供了针对 m 依赖序列的理论保证。",
      "motivation": "无监督文本分割在自然语言处理中至关重要，因为人工标注边界标签成本高昂、主观性强，且难以跨域和不同粒度选择泛化。现有方法往往依赖监督学习，缺乏鲁棒性和理论基础，导致在实际应用中效果受限。本研究旨在解决这一问题，通过开发无需训练、具有理论支持的方法，提高分割的自动化和可靠性。",
      "method": "论文提出 Embed-KCPD 方法，将句子表示为嵌入向量，通过最小化惩罚核变点检测目标来估计分割边界。关键创新包括发展针对 m 依赖序列的依赖感知理论，证明了惩罚风险的 oracle 不等式和边界定位保证，以及引入基于大语言模型的仿真框架生成具有可控有限内存依赖的合成文档，以验证理论预测。技术路线结合了核方法和深度学习嵌入技术。",
      "result": "在标准文本分割基准测试中，Embed-KCPD 通常优于强无监督基线方法，展示了其有效性和泛化能力。通过一个基于 Taylor Swift 推文的案例研究，进一步证实了该方法在实际应用中的可靠性。摘要未提供具体性能指标如准确率提升，但强调了模拟和真实数据上的优异表现。",
      "conclusion": "该研究的主要贡献在于提出 Embed-KCPD 方法，并提供了首个针对 m 依赖序列的理论分析，增强了无监督文本分割的理论基础。学术价值在于结合核变点检测和句子嵌入技术，应用价值体现在无需标注数据的实用分割工具。局限性方面，摘要未明确说明未来工作方向，但该方法可进一步扩展至更多语言和复杂场景。",
      "tags": [
        "Unsupervised Text Segmentation",
        "Kernel Change-Point Detection",
        "Sentence Embeddings",
        "m-dependent Theory",
        "LLM Simulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:40.102941Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18783",
    "title": "Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic",
    "authors": [
      "Deepthi Pathare",
      "Leo Laine",
      "Morteza Haghir Chehreghani"
    ],
    "abstract": "Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18783.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18783",
    "published": "2026-01-26T18:50:21Z",
    "updated": "2026-01-26T18:50:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个基于Proximal Policy Optimization的多目标强化学习框架，通过学习连续Pareto最优策略集，优化卡车在高速公路驾驶中的安全、效率和成本权衡。",
      "motivation": "重型车辆在高速公路驾驶中需同时处理安全、效率和运营成本的平衡，这是一个复杂决策问题。现有方法常使用标量奖励聚合这些竞争目标，但模糊了目标间的权衡结构，导致决策不透明且难以优化，限制自主卡车应用的健壮性和适应性。因此，开发能明确表示并优化权衡的方法对提升决策质量和实际应用价值至关重要。",
      "method": "本文提出一个基于Proximal Policy Optimization（PPO）的多目标强化学习框架，通过学习一个连续的策略集，以Pareto最优方式捕获三个冲突目标之间的权衡：安全性（基于碰撞和成功完成率量化）、能源效率（使用能源成本）和时间效率（使用驾驶员成本）。该框架在可扩展的模拟平台上进行评估，专门针对卡车的战术决策设计，关键创新在于实现策略的连续性和可解释性。",
      "result": "实验结果表明，该框架能够学习到一个平滑且可解释的Pareto前沿，清晰展示不同目标之间的权衡关系。摘要未明确说明具体的性能指标如准确率提升，但强调了框架允许在不同驾驶策略之间无缝转换，无需重新训练，从而提高了决策的适应性和健壮性，为实际应用提供灵活性。",
      "conclusion": "本研究的核心贡献是开发了一个多目标强化学习框架，通过连续Pareto策略集为自主卡车提供健壮和自适应的决策策略，增强目标权衡的明确表示和实际应用潜力，具有重要的学术和工业价值。未来工作可扩展更多目标或集成实际场景，进一步提升框架的通用性和效率。",
      "tags": [
        "Multi-Objective Reinforcement Learning",
        "Proximal Policy Optimization",
        "Pareto Optimality",
        "Tactical Decision Making",
        "Autonomous Driving"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:40.805880Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18779",
    "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
    "authors": [
      "Yuxiao Qu",
      "Amrith Setlur",
      "Virginia Smith",
      "Ruslan Salakhutdinov",
      "Aviral Kumar"
    ],
    "abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18779.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18779",
    "published": "2026-01-26T18:47:21Z",
    "updated": "2026-01-26T18:47:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Privileged On-Policy Exploration (POPE)方法，通过利用特权信息引导强化学习在难题上的探索，有效提升大型语言模型的推理能力。",
      "motivation": "强化学习在提升大型语言模型的推理能力方面取得进展，但在处理难题时，on-policy RL往往无法探索到正确解决方案，导致零奖励和学习信号缺失。现有方法如熵奖励或混合简单和难题训练不仅无效，还可能因优化集中在已解决问题上而抑制对难题的进展。这一问题限制了RL在复杂推理任务中的应用，突显了探索策略改进的重要性，以应对现实世界中的挑战性问题。",
      "method": "POPE方法的核心是利用人类或其他oracle解决方案作为特权信息，引导RL在难题上的探索。具体地，它通过在难题上添加oracle解决方案的前缀，使RL在引导rollouts中获得非零奖励，而非直接使用oracle解决方案作为训练目标。关键创新在于，这些引导行为通过指令跟随和推理的协同作用，能够自然转移回原问题，从而避免了传统方法如离线RL或监督微调的局限性。该方法不依赖具体模型架构，但适用于基于LLMs的RL框架。",
      "result": "实验结果表明，POPE扩展了RL可解决问题的集合，并在挑战性推理基准上显著提高了性能。与基线方法相比，该方法能够处理之前难以解决的难题，从而提升了整体推理能力。摘要未提供具体数据，但指出POPE优于现有方法，有效应对了探索失败的问题，展示了其在改进RL-based推理任务中的实用性。",
      "conclusion": "该研究的主要贡献是提出了POPE方法，解决了RL在难题上探索失败的问题，通过创新使用特权信息引导探索，避免了传统方法的不足。它推动了大型语言模型推理能力的提升，具有重要的学术价值，并为实际应用中复杂问题解决提供了新思路。未来工作可以进一步探索其在更广泛任务中的应用，或针对具体数据集进行优化，以扩展其适用性。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "On-Policy Exploration",
        "Privileged Information",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:58.646826Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18778",
    "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
    "authors": [
      "Shobhita Sundaram",
      "John Quan",
      "Ariel Kwiatkowski",
      "Kartik Ahuja",
      "Yann Ollivier",
      "Julia Kempe"
    ],
    "abstract": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18778.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18778",
    "published": "2026-01-26T18:46:56Z",
    "updated": "2026-01-26T18:46:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了SOAR框架，一个基于元强化学习的自改进框架，帮助预训练大型语言模型通过自我生成课程来突破推理学习瓶颈。",
      "motivation": "本研究旨在解决大型推理模型在强化学习微调中遇到的学习停滞问题。当面对初始成功率极低的数据集时，训练信号稀疏，导致模型无法进一步改进，这在实际应用中限制了推理能力的提升。现有方法如使用内在奖励的自对弈方案，虽然尝试自我改进，但往往不稳定且容易引发多样性崩溃。因此，探究模型是否能够利用潜在知识生成自动化课程来解决自身无法处理的问题，具有重要的理论和应用价值，以克服学习瓶颈并减少对外部数据的依赖。",
      "method": "论文设计了SOAR框架，这是一种基于元强化学习的自改进技术路线。核心方法涉及创建模型的两个副本：一个教师副本用于生成合成问题，另一个学生副本用于解决这些问题。通过学生在难题子集上的进步作为奖励信号，教师进行优化。关键创新在于使用测量学生进展的接地奖励，而非内在代理奖励，以提升课程生成的有效性。实验基于数学基准的最难子集，初始成功率为0/128，旨在测试在极端稀疏信号下的性能，并探讨模型自我生成“垫脚石”的能力，从而解锁更高效的学习过程。",
      "result": "实验结果表明，SOAR框架成功实现了双层元强化学习，在稀疏、二元奖励条件下解锁了学习潜力。接地奖励方案显著优于先前使用的内在奖励方法，避免了常见的不稳定性和多样性崩溃模式，提升了训练的可靠性。分析生成的问题后发现，结构质量和高明确性对学习进展的影响比解决方案的正确性更为关键，这一发现强调了课程设计的质量的重要性。这些发现为在低成功率任务上实现模型自我改进提供了实证支持，展示了无需额外数据即可突破推理瓶颈的潜力。",
      "conclusion": "本研究的主要贡献是证明了预训练模型能够自我生成有用的“垫脚石”来克服推理瓶颈，即使它本身无法直接解决难题。这为大型语言模型的自我改进提供了一种原理性方法，减少了对外部精心策划数据的依赖，具有重要的学术和应用价值。论文指出了接地奖励在提升稳定性和有效性方面的优势，并建议未来工作可以探索将此方法应用于更广泛或更难的推理任务中，以进一步验证其通用性，并为自适应学习系统的发展奠定基础。",
      "tags": [
        "Reinforcement Learning",
        "Meta-RL",
        "Large Language Model",
        "Curriculum Learning",
        "Self-Improvement"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:47.827242Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18777",
    "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation",
    "authors": [
      "Abhishek Divekar",
      "Anirban Majumder"
    ],
    "abstract": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18777.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18777",
    "published": "2026-01-26T18:46:49Z",
    "updated": "2026-01-26T18:46:49Z",
    "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)",
    "light_analysis": {
      "overview": "本研究提出了PRECISE框架，通过扩展Prediction-Powered Inference并结合少量人工注释与LLM判断，有效减少LLM评估中的偏见并显著降低注释需求。",
      "motivation": "传统评估搜索、排名和RAG系统需要大量人工相关注释，耗时耗力。虽然大型语言模型（LLM）作为自动法官具有潜力，但其固有偏见妨碍了直接用于指标估计。现有方法在减少人工注释和纠正LLM偏见方面存在不足，尤其是在低资源设置中，因此需要一种统计方法来平衡准确性与效率，以支持自动化评估的广泛应用。",
      "method": "方法基于Prediction-Powered Inference（PPI）扩展为PRECISE框架，结合约100个人工注释查询和10,000个未标记示例，利用LLM判断进行指标估计。关键创新包括：将PPI应用于子实例注释（如查询-文档级别），通过重构指标整合空间将计算复杂度从O(2^|C|)降低到O(2^K)，其中|C|为语料库大小。框架适用于LLM基于查询重写的应用，提高了统计估计的效率和可靠性。",
      "result": "实验在多个检索数据集上进行，PRECISE框架减少了关键业务指标Precision@K的估计方差，同时有效纠正了LLM偏见，特别是在低资源设置中。与传统方法相比，注释需求显著降低，只需少量人工标注，计算复杂度也大幅下降，从语料库级别优化为更易处理的参数，提升了评估效率和准确性。",
      "conclusion": "论文主要贡献是PRECISE框架，它统计可靠地估计评估指标，减少人工注释负担并纠正LLM偏见，在搜索和排名系统评估中具有重要应用价值。该研究提高了自动化评估的可行性，并可能扩展到其他领域。摘要未明确说明局限性或未来工作方向，但潜在改进包括进一步优化复杂度和更广泛的数据集验证。",
      "tags": [
        "Large Language Model",
        "Prediction-Powered Inference",
        "Precision@K",
        "Relevance Uplift",
        "Query Reformulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:00.262011Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18771",
    "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
    "authors": [
      "Yanming Liu",
      "Xinyue Peng",
      "Zixuan Yan",
      "Yanxin Shen",
      "Wenjie Xu",
      "Yuefeng Huang",
      "Xinyi Wang",
      "Jiannan Cao",
      "Jianwei Yin",
      "Xuhong Zhang"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18771.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18771",
    "published": "2026-01-26T18:42:33Z",
    "updated": "2026-01-26T18:42:33Z",
    "comment": "Dep-Search 1st version",
    "light_analysis": {
      "overview": "Dep-Search 提出了一种依赖感知的搜索框架，通过整合结构化推理、检索和持久内存，显著提升大型语言模型在复杂推理任务中的表现。",
      "motivation": "大型语言模型在复杂推理任务中展现出强大能力，但现有搜索框架主要依赖隐式自然语言推理来确定搜索策略和利用检索信息。这导致在管理子问题间依赖关系、高效重用先前检索知识以及通过强化学习学习最优策略方面存在根本挑战。因此，开发一种能够显式处理依赖和记忆的框架对提升推理效率和准确性至关重要，以应对复杂多跳推理任务的需求。",
      "method": "Dep-Search 框架通过 GRPO（具体细节摘要未明确说明）整合了结构化推理、检索和持久内存。核心方法包括引入显式控制机制，使模型能够分解具有依赖关系的问题、在需要时检索外部信息、从内存中访问之前存储的知识，并将长推理上下文总结为可重用的内存条目。这些机制旨在提供更系统和高效的推理过程，超越传统隐式推理方法。",
      "result": "在七个多样化的问答数据集上进行了广泛实验，Dep-Search 显著增强了大型语言模型处理复杂多跳推理任务的能力。相比强基线方法，Dep-Search 实现了实质性的性能改进，具体数据摘要未明确说明，但实验表明其在提升推理准确性和效率方面表现优异，验证了框架的有效性。",
      "conclusion": "Dep-Search 的主要贡献是提出了一个依赖感知的搜索框架，解决了现有方法在管理依赖和重用知识方面的不足。这项研究在学术上推动了搜索增强生成和结构化推理领域的发展，并在实际应用中提升了大型语言模型处理复杂任务的性能。潜在局限性或未来方向摘要未明确说明，但可探索框架的扩展性和在其他领域的应用。",
      "tags": [
        "Large Language Models",
        "Retrieval-Augmented Generation",
        "Dependency-Aware Reasoning",
        "Persistent Memory",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:35.645118Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18760",
    "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values",
    "authors": [
      "Henry Bell",
      "Lara Neubauer da Costa Schertel",
      "Bochu Ding",
      "Brandon Fain"
    ],
    "abstract": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18760.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18760",
    "published": "2026-01-26T18:27:00Z",
    "updated": "2026-01-26T18:27:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Grounded Constitutional AI (GCAI)框架，通过整合人类理由和价值观生成对齐原则，以改善大型语言模型的对齐公平性和代表性。",
      "motivation": "在开发大型语言模型（LLMs）时，确保模型与人类价值观对齐是核心挑战。当前的对齐方法采用宪法框架，依赖自然语言原则，但如何公平确定这些原则并广泛纳入利益相关者输入尚不明确，导致对齐可能不具代表性或忽视多样价值观。本研究旨在解决这一问题，通过生成更具包容性和道德基础的对齐原则，以提高AI系统的治理质量和用户信任。",
      "method": "该方法提出Grounded Constitutional AI (GCAI)框架，统一生成代表用户普遍期望（一般原则）和交互偏好（上下文原则）的原则。它扩展了Inverse Constitutional AI (ICAI)，通过分析人类偏好标注数据中提供的理由来生成上下文原则，同时从用户关于AI的价值观陈述中提炼出一般原则。框架整合这两种原则，关键创新在于利用人类理由和价值观（而不仅是偏好）来构建更全面的宪法，强调道德基础和多样性。",
      "result": "实验结果表明，由GCAI框架生成的宪法在人类评估中优于通过ICAI生成的宪法。参与者不仅在个人使用场景中更偏好GCAI宪法，还认为它更适合广泛用于AI行为治理。此外，GCAI宪法被评价为具有更强的道德基础、更高的连贯性和更广泛的多元性，摘要未明确说明具体性能指标数据，但基于人类偏好和主观评价，显示了其在捕捉多样价值观方面的有效性。",
      "conclusion": "本研究的主要贡献是提出Grounded Constitutional AI (GCAI)框架，通过结合人类理由和价值观生成更全面和代表性的对齐原则，提高了AI对齐的公平性和道德基础。学术上，它推动了宪法AI领域的发展，强调多维度价值观的融合；实际应用中，有助于开发更可信赖的LLM系统。未来工作可探索更多价值观来源或扩展到其他对齐任务，以进一步提升模型的适用性和鲁棒性。",
      "tags": [
        "Large Language Model",
        "Constitutional AI",
        "Alignment",
        "Inverse Constitutional AI",
        "Human Values"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:36.711932Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18753",
    "title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs",
    "authors": [
      "Xinyue Zeng",
      "Junhong Lin",
      "Yujun Yan",
      "Feng Guo",
      "Liang Shi",
      "Jun Wu",
      "Dawei Zhou"
    ],
    "abstract": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18753.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18753",
    "published": "2026-01-26T18:23:09Z",
    "updated": "2026-01-26T18:23:09Z",
    "comment": "Have been accepted by ICLR'26",
    "light_analysis": {
      "overview": "本研究提出Hallucination Risk Bound理论框架和基于Neural Tangent Kernel（NTK）的HalluGuard方法，统一检测大语言模型中的数据驱动和推理驱动幻觉。",
      "motivation": "在医疗、法律等高风险领域，大语言模型（LLMs）的可靠性常因幻觉问题受损，幻觉主要源于数据驱动（如训练数据不匹配）和推理驱动（如推理过程不稳定）。现有检测方法通常只针对单一来源，并依赖任务特定启发式规则，导致在复杂场景中泛化能力有限。因此，开发一种统一框架以系统分析幻觉风险并提升检测泛化性是必要且重要的。",
      "method": "论文首先提出Hallucination Risk Bound理论框架，将幻觉风险正式分解为数据驱动和推理驱动组件，分别对应训练时数据不匹配和推理时不稳定性。基于此，引入HalluGuard，一种基于Neural Tangent Kernel（NTK）的评分方法，利用NTK的诱导几何和捕获表示，联合识别两种类型幻觉。该方法不依赖特定任务启发式，可泛化到复杂场景，但摘要未明确说明具体数据集或模型架构细节，评估中使用9个流行LLM骨干网络作为基础。",
      "result": "实验在10个多样化基准测试上进行，对比了11个竞争基线方法和9个流行LLM骨干网络。HalluGuard在检测各种形式LLM幻觉方面一致实现最先进性能，超越了现有基线，展现出优异的泛化能力和有效性。尽管摘要未提供具体准确率提升数据，但结果表明该方法在多个基准上表现最佳，证明了其统一检测框架的优势。",
      "conclusion": "本研究的主要贡献是提出Hallucination Risk Bound理论框架，为幻觉风险分析提供统一基础，并开发HalluGuard方法，有效联合检测数据驱动和推理驱动幻觉。这具有学术价值，推动了LLM可靠性研究的深化，并在实践中可提升高风险领域应用的安全性。未来工作可能包括进一步优化检测精度和扩展应用范围，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Large Language Models",
        "Hallucination Detection",
        "Neural Tangent Kernel",
        "Data-Driven Hallucinations",
        "Reasoning-Driven Hallucinations"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:53.824945Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18751",
    "title": "Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback",
    "authors": [
      "Seyed Amir Hosseini",
      "Maryam Abdolali",
      "Amirhosein Tavakkoli",
      "Fardin Ayar",
      "Ehsan Javanmardi",
      "Manabu Tsukada",
      "Mahdi Javanmardi"
    ],
    "abstract": "Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18751.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18751",
    "published": "2026-01-26T18:21:48Z",
    "updated": "2026-01-26T18:21:48Z",
    "comment": "Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)",
    "light_analysis": {
      "overview": "论文提出了TriTrust-PBRL框架，通过动态信任参数从多专家偏好反馈中自动处理可靠和敌对标注者，提升了偏好强化学习的鲁棒性。",
      "motivation": "偏好强化学习（PBRL）利用成对轨迹比较学习奖励，避免了显式奖励工程的复杂性，但在真实应用中，偏好数据常来自可靠性各异的标注者，包括系统提供错误偏好的敌对标注者。现有PBRL方法要么平等对待所有反馈，易受敌对攻击影响，要么试图过滤不可靠来源，但无法有效处理系统性敌对反馈，导致性能下降。这一问题限制了PBRL在现实场景中的部署，因为确保鲁棒性以处理异构专家反馈至关重要。因此，研究旨在开发一个框架，能够自动识别并处理不同类型专家的反馈，包括利用敌对偏好中的有用信号。",
      "method": "TriTrust-PBRL（TTP）是一个统一框架，从多专家偏好反馈中联合学习一个共享奖励模型和专家特定的信任参数。关键创新在于信任参数在梯度优化过程中自然演化，可取值包括正（信任该专家）、近零（忽略）或负（翻转，即反转敌对偏好的含义），从而使模型能够自动适应不同专家的可靠性。该方法通过理论分析提供了可识别性保证，并详细分析了梯度动态，解释了专家分离如何在训练中自发出现，无需显式监督。此外，TTP仅需专家识别索引，无需额外特征，并能无缝集成到现有PBRL管道中。",
      "result": "在四个不同领域（包括MetaWorld的操纵任务和DM Control的运动任务）的各种腐败场景下进行实证评估。TTP实现了最先进的鲁棒性，在敌对腐败下保持接近最优性能（接近oracle），而标准PBRL方法如平等处理或过滤方法严重失败。具体而言，TTP成功地从包含可靠和敌对标注者的混合专家池中学习，优于现有基线方法。实验显示，它在处理系统性敌对反馈时，显著提高了学习稳定性和最终性能指标，且无需专家特征或复杂调整，实现了高效集成。",
      "conclusion": "论文的主要贡献是提出了TriTrust-PBRL框架，有效解决了PBRL中处理多专家反馈时的鲁棒性问题。该方法通过动态信任参数自动识别和利用不同可靠性的专家，包括反转敌对偏好，在理论和实践上取得突破。学术上，提供了可识别性保证和梯度分析，深化了对专家反馈处理机制的理解；应用上，框架易于部署，与现有系统兼容，具有广泛的应用潜力。未来工作可探索扩展至更复杂的反馈类型或大规模数据集，以进一步提升泛化能力和实际适应性。",
      "tags": [
        "Preference-Based Reinforcement Learning",
        "Trust Parameters",
        "Multi-Expert Feedback",
        "Gradient Analysis",
        "Robust Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:11.400271Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18744",
    "title": "TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models",
    "authors": [
      "Fangxu Yu",
      "Xingang Guo",
      "Lingzhi Yuan",
      "Haoqiang Kang",
      "Hongyu Zhao",
      "Lianhui Qin",
      "Furong Huang",
      "Bin Hu",
      "Tianyi Zhou"
    ],
    "abstract": "Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18744.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18744",
    "published": "2026-01-26T18:04:54Z",
    "updated": "2026-01-26T18:04:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文引入了TSRBench，一个全面的多任务多模态时间序列推理基准，用于填补现有通用模型基准在时间序列推理能力评估方面的空白。",
      "motivation": "时间序列数据在真实世界场景中无处不在，对能源管理、交通控制等关键应用至关重要，因此通用模型需要具备时间序列推理能力以解决实际问题。然而，现有基准普遍忽略这一维度，导致通用模型在时间序列处理方面的能力未被系统评估，限制了其实际应用潜力。为了解决这一问题，论文提出开发一个专门基准，以应对现有方法的不足，并为模型评估提供标准化平台。",
      "method": "论文的核心方法是设计并构建TSRBench基准，它包含4125个问题，来自14个不同领域，并分为4个主要维度：感知、推理、预测和决策制定，共计15个任务，旨在全面评估时间序列推理能力。创新点在于其多模态输入设计，结合时间序列的文本和视觉表示，以模拟真实世界复杂性。技术路线包括收集多样化数据集，并使用标准化评估框架测试模型性能。",
      "result": "通过实验评估超过30个领先的专有和开源模型，包括大型语言模型、视觉语言模型和时序语言模型，发现：缩放定律在感知和推理维度成立，但在预测维度失效；强推理能力不保证准确的上下文感知预测，表明语义理解与数值预测脱节；尽管文本和视觉输入具有互补性，当前多模态模型未能有效融合它们以实现性能提升。这些结果通过具体分析揭示现有模型的局限性。",
      "conclusion": "论文的主要贡献是提供TSRBench基准，作为一个标准化评估平台，突出时间序列推理中的挑战，如推理与预测脱节以及多模态融合困难。其学术价值在于为通用模型发展提供宝贵洞察，实际应用价值在于促进模型在真实世界任务中的性能提升，未来工作可聚焦于改进多模态集成和预测能力，但摘要未明确说明具体局限性。",
      "tags": [
        "Time Series Reasoning",
        "Benchmark",
        "Generalist Models",
        "Multimodal Models",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:59.351763Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18739",
    "title": "SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification",
    "authors": [
      "Ignacio Antequera-Sánchez",
      "Juan Luis Suárez-Díaz",
      "Rosana Montes",
      "Francisco Herrera"
    ],
    "abstract": "Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18739.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18739",
    "published": "2026-01-26T18:01:46Z",
    "updated": "2026-01-26T18:01:46Z",
    "comment": "28 pages",
    "light_analysis": {
      "overview": "提出了SeNeDiF-OOD，一种基于语义嵌套二分法融合的OOD检测方法，通过层次结构处理开放世界分类中的异质性异常数据。",
      "motivation": "该研究旨在解决开放世界环境中AI系统部署的可靠性问题，核心挑战是OOD数据的异质性，包括从低级损坏到语义偏移的多种类型。由于传统单阶段检测器难以适应这种复杂性，常导致检测失败，因此需要开发一种能有效处理多样OOD数据的方法，以提升如建筑风格识别等实际应用的鲁棒性。",
      "method": "SeNeDiF-OOD方法基于语义嵌套二分法融合，创新地将OOD检测任务分解为层次结构的二元融合节点，每个节点集成决策边界以对齐特定语义抽象级别。通过MonuMAI系统进行案例研究，这是一个真实世界的建筑风格识别系统，暴露于开放环境中，面临非建筑图像、未知风格和对抗攻击等多样输入。",
      "result": "在MonuMAI案例的广泛实验评估中，SeNeDiF-OOD方法显著优于传统基线。它能有效过滤多种OOD类别，包括非建筑图像、未知建筑风格和对抗攻击，同时保持对内分布数据的性能稳定，未提供具体性能指标但强调了显著改进效果。",
      "conclusion": "本论文的主要贡献是提出SeNeDiF-OOD方法论，通过层次融合机制增强了OOD检测能力，解决了异质OOD数据的复杂挑战。该研究具有学术价值和实际应用潜力，特别适用于开放世界系统如建筑风格识别，未来工作可扩展到其他领域或优化融合策略。",
      "tags": [
        "Out-of-Distribution Detection",
        "Semantic Nested Dichotomy Fusion",
        "Hierarchical Fusion",
        "Open-World Classification",
        "Case Study"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:32.417871Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18736",
    "title": "Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift",
    "authors": [
      "Jake Lyon",
      "Ehsan Saeedizade",
      "Shamik Sengupta"
    ],
    "abstract": "The rapid expansion of the Internet of Things (IoT) in domains such as smart cities, transportation, and industrial systems has heightened the urgency of addressing their security vulnerabilities. IoT devices often operate under limited computational resources, lack robust physical safeguards, and are deployed in heterogeneous and dynamic networks, making them prime targets for cyberattacks and malware applications. Machine learning (ML) offers a promising approach to automated malware detection and classification, but practical deployment requires models that are both effective and lightweight. The goal of this study is to investigate the effectiveness of four supervised learning models (Random Forest, LightGBM, Logistic Regression, and a Multi-Layer Perceptron) for malware detection and classification using the IoT-23 dataset. We evaluate model performance in both binary and multiclass classification tasks, assess sensitivity to training data volume, and analyze temporal robustness to simulate deployment in evolving threat landscapes. Our results show that tree-based models achieve high accuracy and generalization, even with limited training data, while performance deteriorates over time as malware diversity increases. These findings underscore the importance of adaptive, resource-efficient ML models for securing IoT systems in real-world environments.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18736.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18736",
    "published": "2026-01-26T17:59:33Z",
    "updated": "2026-01-26T17:59:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文基准测试四种监督学习模型在IoT恶意软件检测中的表现，揭示树基模型在数据稀缺下准确性高但随时间性能下降。",
      "motivation": "物联网的快速扩展在智能城市、交通和工业系统等领域提高了安全漏洞的紧迫性。IoT设备通常计算资源有限、缺乏物理防护，部署在异构动态网络中，易成为网络攻击和恶意软件的目标。机器学习为自动恶意软件检测提供了有前景的方法，但实际部署需要既有效又轻量级的模型。现有方法可能对数据稀缺和威胁演变不够鲁棒，因此本研究旨在评估模型在这些挑战条件下的表现，以弥补现有研究中的不足。",
      "method": "本研究采用四种监督学习模型——随机森林、LightGBM、逻辑回归和多层感知机，使用IoT-23数据集进行恶意软件检测和分类。评估包括二进制和多类分类任务，分析模型对训练数据量的敏感性，以及时间鲁棒性以模拟在演化威胁环境中的部署。关键创新点在于综合基准测试多种模型在数据稀缺和漂移条件下的表现，使用标准数据集确保实验可比性。",
      "result": "实验结果显示，树基模型（如随机森林和LightGBM）在二进制和多类分类任务中实现了高准确率和良好的泛化能力，即使在训练数据有限的情况下。然而，性能随时间推移而下降，随着恶意软件多样性的增加，模型的有效性减弱。摘要未明确说明具体性能指标，但与基线方法相比，树基模型表现出色，而传统模型如逻辑回归和多层感知机可能表现较差。",
      "conclusion": "本研究的主要贡献是评估了四种监督学习模型在IoT恶意软件检测中的有效性，特别是在数据稀缺和漂移条件下。结果强调了自适应、资源高效的机器学习模型对于在实际环境中保护IoT系统的重要性。学术上，为IoT安全领域提供了基准测试参考；实际上，指导了模型选择和部署策略。未来工作可探索持续学习方法以应对时间漂移的挑战。",
      "tags": [
        "IoT Malware Detection",
        "Supervised Learning",
        "Random Forest",
        "LightGBM",
        "Data Drift"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:39.257907Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18735",
    "title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems",
    "authors": [
      "Jusheng Zhang",
      "Yijia Fan",
      "Kaitong Cai",
      "Jing Yang",
      "Jiawei Yao",
      "Jian Wang",
      "Guanlong Qu",
      "Ziliang Chen",
      "Keze Wang"
    ],
    "abstract": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18735.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18735",
    "published": "2026-01-26T17:58:53Z",
    "updated": "2026-01-26T17:58:53Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "Agora框架通过市场机制将认知不确定性转化为可交易资产，实现经济高效的多智能体视觉系统协调。",
      "motivation": "视觉语言模型在多智能体系统中虽强大，但扩展成本高，因为协调异构智能体在信息不对称下会引发成本飙升。现有方法如Mixture-of-Agents和知识型路由器依赖启发式代理，忽视成本并破坏不确定性结构，导致协调效果次优。因此，研究旨在解决多智能体视觉系统中经济可持续性的关键问题，优化协调过程以避免成本失控。",
      "method": "Agora框架的核心方法是将认知不确定性（包括感知、语义和推理不确定性）形式化为结构化可交易资产，并基于理性经济规则强制智能体进行利润驱动交易。创新点在于引入市场感知经纪人，扩展Thompson Sampling算法以启动协作并引导系统达到成本高效均衡。实验使用五个多模态基准（如MMMU、MMBench），通过模型架构优化不确定性交易流程，实现智能体间的去中心化协调。",
      "result": "在五个多模态基准（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上，Agora框架显著优于强视觉语言模型和启发式多智能体策略。例如，在MMMU上准确率比最佳基线提升8.5%，同时成本降低超过3倍。这些结果表明，Agora在保持高性能的同时，大幅优化了经济效率，为多智能体系统提供了可扩展的协调方案。",
      "conclusion": "Agora框架的主要贡献是建立了基于市场的协调范式，为构建经济可行的多智能体视觉智能系统提供原则性方法，具有重要的学术和实际应用价值。它推动了多智能体系统中不确定性管理和成本优化研究，未来工作可探索更复杂的市场机制或扩展到其他视觉智能领域。摘要未明确说明具体局限性，但暗示了潜在的方向，如进一步验证泛化能力。",
      "tags": [
        "Vision-Language Models",
        "Multi-Agent Systems",
        "Market-based Coordination",
        "Uncertainty Quantification",
        "Thompson Sampling"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:34.898844Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18734",
    "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models",
    "authors": [
      "Siyan Zhao",
      "Zhihui Xie",
      "Mengchen Liu",
      "Jing Huang",
      "Guan Pang",
      "Feiyu Chen",
      "Aditya Grover"
    ],
    "abstract": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18734.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18734",
    "published": "2026-01-26T17:56:50Z",
    "updated": "2026-01-26T17:56:50Z",
    "comment": "13 pages",
    "light_analysis": {
      "overview": "本文提出了On-Policy Self-Distillation (OPSD)框架，使单个大语言模型通过自我蒸馏同时充当教师和学生，显著提升了推理效率和性能。",
      "motivation": "研究旨在改进大语言模型的推理能力。知识蒸馏通过教师模型压缩知识以训练较小模型，但传统策略蒸馏存在训练与推理分布不匹配的问题，且通常依赖独立的大型教师模型，未充分利用数据集中的真实解决方案。这导致了资源浪费和性能瓶颈，限制了蒸馏方法的效率。因此，需要一种新方法以减少外部依赖并有效利用特权信息，以解决推理任务中的挑战。",
      "method": "本文提出OPSD框架，其中单个模型通过条件化不同上下文实现自我蒸馏。教师策略条件化于特权信息（如已验证的推理轨迹），而学生策略仅基于问题；训练过程通过最小化学生自身rollouts中分布的每token散度，实现知识传递。关键创新点在于利用同一模型的不同条件化版本进行内部监督，避免了对额外教师模型的需求，并有效整合了数据集中的特权信息，在数学推理基准上应用了该方法。",
      "result": "在多个数学推理基准上的实验表明，OPSD方法取得了显著效果。与强化学习方法如GRPO相比，token效率提高了4-8倍，意味着使用更少的计算资源达到相似或更好的性能。同时，OPSD在性能上优于策略蒸馏方法，通过具体效率提升数据支持了其有效性，展示了在减少资源消耗的同时提升推理准确性的潜力。",
      "conclusion": "OPSD框架通过自我蒸馏机制，使大语言模型能够内部利用特权信息进行训练，提高了推理效率和性能。主要贡献在于提出了一种无需外部教师模型的蒸馏方法，减少了资源依赖并充分利用数据集信息。学术上为知识蒸馏和LLM优化提供了新思路，实际应用可降低数学推理等任务的部署成本。摘要未明确说明局限性或未来工作方向，但推断该方法可能对模型初始能力有要求，未来可扩展至其他领域。",
      "tags": [
        "Large Language Model",
        "Knowledge Distillation",
        "On-Policy Learning",
        "Self-Distillation",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:05.127525Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18731",
    "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
    "authors": [
      "Hongru Cai",
      "Yongqi Li",
      "Tiezheng Yu",
      "Fengbin Zhu",
      "Wenjie Wang",
      "Fuli Feng",
      "Wenjie Li"
    ],
    "abstract": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18731.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18731",
    "published": "2026-01-26T17:55:52Z",
    "updated": "2026-01-26T17:55:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Meta Reward Modeling (MRM)方法，通过元学习框架优化个性化奖励模型的初始化，以解决大型语言模型个性化对齐中用户反馈稀缺和快速适应未见用户的挑战。",
      "motivation": "大型语言模型(LLMs)的个性化对齐旨在使模型输出适应个体用户偏好，这依赖于个性化奖励模型提供反馈。但现有方法面临两大挑战：个体用户反馈数据稀缺，难以捕捉多样偏好；以及需要高效适应新用户，传统数据拟合方法在少样本场景下性能不足。这推动了从学习特定用户偏好转向学习偏好适应过程的范式转变，以实现更灵活和鲁棒的个性化对齐。",
      "method": "论文提出Meta Reward Modeling (MRM)，将个性化奖励建模重新定义为元学习问题。具体技术路线中，每个用户的奖励模型表示为多个基础奖励函数的加权组合。采用Model-Agnostic Meta-Learning (MAML)风格的框架优化这些权重的初始化，以在仅有少量用户反馈时实现快速适应。创新点包括引入Robust Personalization Objective (RPO)，在元优化中优先考虑难学习用户，增强模型的鲁棒性和泛化能力。",
      "result": "在个性化偏好数据集上的广泛实验验证了MRM的有效性。结果表明，MRM显著增强了少样本个性化能力，提高了用户鲁棒性，并持续优于基线方法。摘要未明确说明具体性能指标如准确率或效率改进的数值，但实验证实了MRM在解决用户反馈稀缺和快速适应挑战方面的优势。",
      "conclusion": "MRM的主要贡献是通过元学习框架解决个性化对齐的核心挑战，即用户反馈稀缺和快速适应未见用户。学术上提供新范式，将个性化奖励建模视为元学习问题；实际上改善了少样本场景下的模型适应性和鲁棒性，有潜力应用于更广泛的个性化AI系统。未来工作方向摘要未明确说明，可能包括优化鲁棒性目标或扩展至其他领域。",
      "tags": [
        "Large Language Model Alignment",
        "Personalized Reward Modeling",
        "Meta-Learning",
        "Model-Agnostic Meta-Learning",
        "Robust Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:25.755923Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18730",
    "title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale",
    "authors": [
      "Henry Bell",
      "Caroline Zhang",
      "Mohammed Mobasserul Haque",
      "Dhaval Potdar",
      "Samia Zaman",
      "Brandon Fain"
    ],
    "abstract": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \\textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \\textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \\textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \\textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \\textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \\textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18730.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18730",
    "published": "2026-01-26T17:54:54Z",
    "updated": "2026-01-26T17:54:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "Reflect 提出了一种无需训练或数据的推理时宪法对齐框架，通过显式原则推理和透明对齐方法提升大型语言模型对齐能力。",
      "motivation": "传统对齐方法如基于人类反馈的强化学习（RLHF）依赖于参数微调，计算需求高、需要精心工程调整且依赖难以获取的人类注释数据，导致对齐过程效率低、可扩展性差。Reflect 旨在解决这些限制，提供一种插件式方法，在推理时直接应用自然语言编写的宪法原则，无需额外训练或数据，从而降低对齐的成本和复杂性，使对齐更易于实施和调整。",
      "method": "Reflect 框架完全在推理时上下文中操作，包括以下关键步骤：首先，生成一个基于宪法原则的条件基础响应；然后，进行后生成的自我评估，检查响应是否符合原则；接着，执行自我批判，识别并修正潜在偏差；最后，生成最终修订版响应。该方法通过显式推理原则，实现无需修改模型参数的透明对齐，创新点在于将原则指导的推理集成到推理过程中，避免了传统训练需求。",
      "result": "实验结果显示，Reflect 显著提升大型语言模型对多样和复杂原则的符合性，即使原则与模型原有微调中的原则不同，并优于标准少样本提示方法。它有效减少分布尾部罕见但严重的原则违规率，从而增强安全性和鲁棒性。此外，Reflect 能自然生成高质量训练数据，可用于传统参数微调技术，促进高效扩展和推理开销的长期减少。",
      "conclusion": "Reflect 的主要贡献是提供了一种高效、透明的宪法对齐框架，能在推理时改善对齐效果，无需额外训练或数据，提高对齐的灵活性和可扩展性。学术价值在于创新了推理时对齐方法，实际应用价值在于降低部署成本并增强模型安全性。未来工作可探索更多原则类型和优化框架性能，以应对更广泛的对齐需求。",
      "tags": [
        "Large Language Models",
        "Constitutional Alignment",
        "In-Context Reasoning",
        "Self-Evaluation",
        "Inference-Time Framework"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:19.231507Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18728",
    "title": "Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data",
    "authors": [
      "Willem Diepeveen",
      "Oscar Leong"
    ],
    "abstract": "Modern generative modeling methods have demonstrated strong performance in learning complex data distributions from clean samples. In many scientific and imaging applications, however, clean samples are unavailable, and only noisy or linearly corrupted measurements can be observed. Moreover, latent structures, such as manifold geometries, present in the data are important to extract for further downstream scientific analysis. In this work, we introduce Riemannian AmbientFlow, a framework for simultaneously learning a probabilistic generative model and the underlying, nonlinear data manifold directly from corrupted observations. Building on the variational inference framework of AmbientFlow, our approach incorporates data-driven Riemannian geometry induced by normalizing flows, enabling the extraction of manifold structure through pullback metrics and Riemannian Autoencoders. We establish theoretical guarantees showing that, under appropriate geometric regularization and measurement conditions, the learned model recovers the underlying data distribution up to a controllable error and yields a smooth, bi-Lipschitz manifold parametrization. We further show that the resulting smooth decoder can serve as a principled generative prior for inverse problems with recovery guarantees. We empirically validate our approach on low-dimensional synthetic manifolds and on MNIST.",
    "categories": [
      "cs.LG",
      "math.DG",
      "math.OC",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18728.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18728",
    "published": "2026-01-26T17:51:52Z",
    "updated": "2026-01-26T17:51:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "Riemannian AmbientFlow 是一个框架，能够同时从损坏观测中学习概率生成模型和底层非线性数据流形。",
      "motivation": "研究动机源于在许多科学和成像应用中，如医学成像或科学测量，通常难以获得干净数据样本，而只能观测到噪声或线性损坏的测量。现有生成建模方法主要依赖干净数据，无法有效处理这些损坏场景，且忽视了数据的潜在结构如流形几何，这对下游分析如分类或特征提取至关重要。因此，开发能从损坏数据中同时学习数据分布和流形结构的方法具有重要应用价值。",
      "method": "该方法基于 AmbientFlow 的变分推理框架，引入了数据驱动的黎曼几何，这种几何由归一化流诱导。通过使用 pullback metrics 和 Riemannian Autoencoders 提取流形结构，实现了生成建模与流形学习的结合。核心创新在于将黎曼几何整合进变分推理中，直接从损坏观测中学习，无需干净数据，并利用了 MNIST 等数据集进行验证。",
      "result": "论文建立了理论保证，表明在适当的几何正则化和测量条件下，学习模型能以可控误差恢复底层数据分布，并提供平滑的双利普希茨流形参数化。实证验证在低维合成流形和 MNIST 数据集上进行，摘要未明确说明具体数据，但指出框架展示了有效性和与基线方法的比较优势，支持了其在实际应用中的潜力。",
      "conclusion": "该研究的主要贡献是提出了 Riemannian AmbientFlow 框架，实现了从损坏数据中同时学习生成模型和数据流形。学术价值在于融合了黎曼几何与生成建模，拓展了变分推理的应用；实际应用价值体现在为逆问题提供了有恢复保证的生成先验，增强了对损坏数据的处理能力。未来工作可能包括扩展到更高维数据或其他类型的损坏场景。",
      "tags": [
        "Riemannian Geometry",
        "Normalizing Flows",
        "Variational Inference",
        "Manifold Learning",
        "Generative Modeling"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:32.714558Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18724",
    "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
    "authors": [
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Taro Watanabe"
    ],
    "abstract": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18724.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18724",
    "published": "2026-01-26T17:48:23Z",
    "updated": "2026-01-26T17:48:23Z",
    "comment": "Work In Progress",
    "light_analysis": {
      "overview": "论文系统性地调查了幻觉引用（HalluCitation）在ACL会议论文中的普遍性和影响，揭示了其迅速增长的趋势和对科学可靠性的威胁。",
      "motivation": "研究动机源于幻觉引用问题日益严重，即在学术论文中引用了不存在的参考文献，这严重威胁科学可靠性并可能损害学术会议的声誉。现有研究可能缺乏对该问题的系统性评估，尤其是在计算语言学领域的顶级会议中，因此有必要进行深入调查，以量化问题的程度和影响，并唤起社区对这一隐性风险的关注。",
      "method": "研究方法包括对2024年和2025年在ACL、NAACL和EMNLP发表的所有论文进行系统性分析，覆盖主会议、Findings和工作坊论文。研究者引入了\"HalluCitation\"这一术语来描述幻觉引用，并通过数据收集和统计分析技术来识别和统计包含幻觉引用的论文。关键创新点在于全面调查了多个会议的论文，提供了实证数据来评估问题规模。",
      "result": "实验结果显示，在分析的论文中，近300篇包含至少一个幻觉引用，其中大多数发表于2025年。特别地，EMNLP 2025会议中有半数受影响的论文，表明问题正迅速加剧。此外，超过100篇包含幻觉引用的论文被EMNLP 2025接受为主会议和Findings论文，这直接影响了会议的可信度，凸显了问题的严重性。",
      "conclusion": "论文的主要贡献是通过实证数据揭示了幻觉引用在ACL会议中的普遍性及其对科学可靠性和会议声誉的负面影响，强调了在学术出版中加强引用准确性的重要性。研究建议改进论文审查机制以减少此类问题，并为未来的研究提供基线数据。潜在局限性包括调查范围限于特定会议，未来工作可能涉及扩展调查或开发自动化检测工具。",
      "tags": [
        "HalluCitation",
        "Citation Analysis",
        "Scientific Publishing",
        "NLP Conferences",
        "Reliability Assessment"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:35.145176Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18722",
    "title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning",
    "authors": [
      "Lintang Sutawika",
      "Gokul Swamy",
      "Zhiwei Steven Wu",
      "Graham Neubig"
    ],
    "abstract": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \\textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \\texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than   of the training data across the single-language, multilingual, and generalization to unseen language settings.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18722.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18722",
    "published": "2026-01-26T17:46:44Z",
    "updated": "2026-01-26T17:46:44Z",
    "comment": "Code available at https://github.com/lintangsutawika/SP3F",
    "light_analysis": {
      "overview": "论文提出SP3F框架，通过自玩和特权成对裁判反馈增强多语言推理，无需目标语言数据。",
      "motivation": "当前推理大语言模型在训练数据较少的语言中性能显著下降，导致多语言应用受限。这问题重要，因为它阻碍了模型在全球环境下的公平部署。现有方法通常依赖大量目标语言数据或无法有效处理低资源语言，因此急需一种无需目标语言数据即可提升推理性能的创新方案。",
      "method": "SP3F框架采用两阶段方法：首先，监督微调在英语问答对的翻译版本上进行，提升基础模型的正确性；其次，使用自玩强化学习，由成对裁判提供反馈，裁判接收英语参考响应作为特权信息。关键创新在于特权信息的使用，即使所有响应都不完全正确，裁判也能判断优劣，从而优化模型决策过程。",
      "result": "实验显示SP3F大幅提升基础模型的多语言推理性能，在多个数学和非数学任务中，其表现甚至超越完全后训练模型，同时训练数据量更少。尽管摘要未提供具体数据百分比，但强调了效率和性能的显著改进，凸显了该方法在资源节约和效果提升方面的优势。",
      "conclusion": "该研究的主要贡献是SP3F框架，它有效解决了多语言推理中的数据稀缺问题，无需目标语言数据即可显著提升性能。学术价值在于创新地引入特权信息增强裁判能力，实际应用上降低了数据需求，扩展了模型在低资源语言中的可用性。未来工作可探索更多任务类型或语言扩展。",
      "tags": [
        "Self-Play",
        "Reinforcement Learning",
        "Privileged Information",
        "Multilingual Reasoning",
        "Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:36.039479Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18716",
    "title": "Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules",
    "authors": [
      "Naeyma N. Islam",
      "Thomas R. Caulfield"
    ],
    "abstract": "Alzheimer's disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.",
    "categories": [
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18716.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18716",
    "published": "2026-01-26T17:39:59Z",
    "updated": "2026-01-26T17:39:59Z",
    "comment": "30 pages, 8 figures",
    "light_analysis": {
      "overview": "论文提出了一种条件生成建模方法LC-JT-VAE，用于生成针对E3连接酶的分子胶，以促进阿尔茨海默病中Abeta-42的降解。",
      "motivation": "研究动机在于解决阿尔茨海默病中Abeta-42的病理积累问题，尤其是细胞内Abeta-42作为早期毒性驱动因素的重要性日益凸显。现有治疗方法主要针对细胞外淀粉样蛋白斑块，但缺乏有效靶向细胞内Abeta-42的手段，因此需要创新方法促进其降解。本研究旨在通过AI辅助设计分子胶，利用泛素蛋白酶体系统实现靶向降解，以弥补现有不足并推动神经退行性疾病治疗的发展。",
      "method": "研究方法首先通过基于结构的建模、ADMET筛选和对接评估Abeta-42与三种E3连接酶（CRBN、VHL、MDM2）的三元复合物形成潜力。核心创新是开发了LC-JT-VAE，这是一种条件生成模型，集成蛋白序列嵌入和扭转角感知分子图，以生成连接酶特异性小分子。模型利用Junction Tree VAE框架，条件于E3连接酶信息，确保生成的分子化学有效且靶向特定，创新点在于结合结构建模和生成建模于药物设计。",
      "result": "实验结果未在摘要中提供具体性能指标，但表明LC-JT-VAE模型能够生成化学有效、新颖且靶向特定的分子胶，这些分子胶能够促进Abeta-42的降解。与基线方法的对比未明确说明，因此推断该模型在生成合成可及的药物样分子方面具有潜力，为后续实验验证奠定了基础，摘要未详细说明量化数据。",
      "conclusion": "论文的主要贡献是提出了一种集成AI生成模型的方法，用于设计针对泛素蛋白酶体系统的分子胶疗法。其学术价值在于结合结构建模和条件生成建模，创新药物发现过程。实际应用价值在于为阿尔茨海默病等神经退行性疾病提供新的治疗策略。未来工作可能包括实验验证生成的分子、优化模型性能以及扩展到其他疾病靶点。",
      "tags": [
        "Conditioned Generative Modeling",
        "Junction Tree VAE",
        "Molecular Glue Design",
        "E3 Ligase Targeting",
        "Protein Sequence Embeddings"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:03.592422Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18714",
    "title": "Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning",
    "authors": [
      "Judith Vilella-Cantos",
      "Mauro Martini",
      "Marcello Chiaberge",
      "Mónica Ballesta",
      "David Valiente"
    ],
    "abstract": "Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18714.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18714",
    "published": "2026-01-26T17:38:56Z",
    "updated": "2026-01-26T17:38:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出MinkUNeXt-VINE方法，利用Matryoshka Representation Learning在葡萄园LiDAR位置识别中实现高效率和低成本。",
      "motivation": "农业环境具有非结构化和缺乏显著地标的特性，使得移动机器人定位任务极具挑战性。现有研究多集中于物体分类和分割，而位置识别在当前技术中性能不足，特别是在实时应用和高效率需求下。因此，开发一种能够利用低成本、稀疏LiDAR输入实现高效位置识别的方法至关重要，以推动农业自动化和机器人导航的进步。",
      "method": "论文提出MinkUNeXt-VINE，一个基于深度学习的轻量级方法，核心创新在于结合预处理和Matryoshka Representation Learning多损失策略。该方法专注于处理稀疏LiDAR点云输入，并生成低维输出以提高实时效率。技术特色包括层次化表示学习，类似于俄罗斯套娃结构，以优化特征提取和模型轻量化。使用两个长期葡萄园数据集和不同LiDAR传感器进行验证，但具体模型架构细节摘要未明确说明。",
      "result": "该方法在葡萄园环境中超越现有最优方法，通过全面的消融研究证明了其在低成本、稀疏LiDAR输入和低分辨率数据上的稳健性能。具体性能指标摘要未明确说明，但结果强调了效率与性能的有效权衡，显示出在保持高精度的同时，显著提升了计算效率。与基线方法相比，其适用于实时部署场景，但缺乏详细的量化数据支持。",
      "conclusion": "本研究的主要贡献是提出了一种高效、低成本的位置识别方法，通过Matryoshka Representation Learning优化性能，推动了农业机器人导航的学术和实际应用价值。代码公开促进了复现和进一步研究。局限性可能包括对特定葡萄园环境的依赖性，未来工作可探索更多农业场景的泛化能力和进一步优化模型效率。",
      "tags": [
        "LiDAR Place Recognition",
        "Matryoshka Representation Learning",
        "Deep Learning",
        "Sparse LiDAR",
        "Agricultural Robotics"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:03.733783Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18707",
    "title": "SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model",
    "authors": [
      "Jan Hagnberger",
      "Mathias Niepert"
    ],
    "abstract": "Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18707.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18707",
    "published": "2026-01-26T17:34:16Z",
    "updated": "2026-01-26T17:34:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "SMART提出一个基于Transformer的无网格代理模型，仅使用点云表示预测物理量，无需模拟网格，解决了气动模拟中网格生成成本高的问题。",
      "motivation": "该研究旨在解决物理模拟中无网格方法的高误差问题。现有基于机器学习的代理模型虽然准确，但需要模拟网格作为输入以减少误差，而网格生成计算成本高昂；无网格方法则通常误差较大，难以满足工业应用需求。在复杂几何体如汽车气动模拟中，高效准确的模拟至关重要，现有方法在效率和准确性之间存在显著权衡，促使开发无需网格的替代方案。",
      "method": "研究方法采用SMART模型，基于Transformer架构，仅使用点云表示几何。模型将几何和模拟参数编码到共享潜在空间，捕捉结构和参数特征，物理解码器通过跨层交互注意力机制关注编码器的中间潜在表示，实现空间查询到物理量的映射。关键创新在于联合更新潜在几何特征和物理场，实现无网格模拟，具体数据集和模型架构摘要未明确说明。",
      "result": "实验结果显示，SMART模型在广泛测试中与依赖模拟网格的现有方法竞争，并且常能超越，表明其在保持高准确性的同时避免了网格生成的计算成本。摘要未提供具体性能指标如准确率提升，但强调了模型的潜力用于工业级气动模拟，展示了与基线方法的对比优势。",
      "conclusion": "SMART模型的贡献在于为无网格物理模拟提供了高效准确的解决方案，学术上推动了代理模型技术的发展，实际应用中可提升工业模拟如气动分析的效率。摘要未明确说明局限性或未来工作方向，但暗示了在更广泛场景的潜在应用价值。",
      "tags": [
        "Surrogate Model",
        "Mesh-free Simulation",
        "Transformer",
        "Point Cloud",
        "Physical Simulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:07.625777Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18706",
    "title": "Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs",
    "authors": [
      "Zhichao Yang",
      "Sepehr Janghorbani",
      "Dongxu Zhang",
      "Jun Han",
      "Qian Qian",
      "Andrew Ressler",
      "Gregory D. Lyng",
      "Sanjit Singh Batra",
      "Robert E. Tillman"
    ],
    "abstract": "Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18706.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18706",
    "published": "2026-01-26T17:34:10Z",
    "updated": "2026-01-26T17:34:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Health-SCORE框架，一种可扩展的基于量规的培训和评估方法，显著降低健康大语言模型评估的开发成本，提升可扩展性。",
      "motivation": "在医疗等安全关键领域，评估开放性问题大语言模型（LLM）响应时，量规（rubrics）至关重要。然而，传统量规的开发需要大量专家时间和成本，使得基于量规的评估和培训难以规模化。现有方法在可扩展性方面存在不足，导致健康LLMs部署中资源消耗高、效率低下，迫切需要解决这一问题以实现更广泛的应用。",
      "method": "本文提出Health-SCORE框架，这是一种通用且可扩展的基于量规的培训和评估框架。核心方法包括自动化和优化量规生成过程，以减少人工开发成本；将量规作为结构化奖励信号，结合强化学习技术进行安全意识监督；同时，框架可以整合到模型提示中，通过上下文学习机制提升响应质量。关键创新在于将量规与LLM训练和评估无缝集成，具体数据集和模型架构在摘要中未明确说明。",
      "result": "在开放性问题医疗任务上的实验表明，Health-SCORE框架的评估质量可与人类创建的优质量规相媲美，具体性能指标摘要未详细提供，但强调评估效果的一致性。与基线方法相比，该方法显著降低了开发工作量，提高了效率，使基于量规的评估和培训更具可扩展性，为健康LLMs的实际部署提供了实证支持。",
      "conclusion": "本研究的主要贡献在于提出了Health-SCORE框架，有效解决了健康LLMs评估和培训中量规开发成本高、难以扩展的挑战。学术价值在于推动可扩展评估方法在AI安全领域的创新，实际应用价值在于促进医疗AI系统的高效和安全部署。局限性或未来工作方向摘要未明确说明，但可推断可能需要扩展到更多领域或优化框架细节以进一步提升性能。",
      "tags": [
        "Rubrics",
        "Large Language Models (LLMs)",
        "Reinforcement Learning",
        "In-context Learning",
        "Health Care"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:17.374120Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18702",
    "title": "From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic",
    "authors": [
      "Hansheng Ren"
    ],
    "abstract": "Current paradigms in Deep Learning prioritize computational throughput over numerical precision, relying on the assumption that intelligence emerges from statistical correlation at scale. In this paper, we challenge this orthodoxy. We propose the Exactness Hypothesis: that General Intelligence (AGI), specifically high-order causal inference, requires a computational substrate capable of Arbitrary Precision Arithmetic. We argue that the \"hallucinations\" and logical incoherence seen in current Large Language Models (LLMs) are artifacts of IEEE 754 floating-point approximation errors accumulating over deep compositional functions. To mitigate this, we introduce the Halo Architecture, a paradigm shift to Rational Arithmetic ($\\mathbb{Q}$) supported by a novel Exact Inference Unit (EIU). Empirical validation on the Huginn-0125 prototype demonstrates that while 600B-parameter scale BF16 baselines collapse in chaotic systems, Halo maintains zero numerical divergence indefinitely. This work establishes exact arithmetic as a prerequisite for reducing logical uncertainty in System 2 AGI.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18702.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18702",
    "published": "2026-01-26T17:24:34Z",
    "updated": "2026-01-26T17:24:34Z",
    "comment": "8 pages, 6 figures. Submitted to UAI 2026",
    "light_analysis": {
      "overview": "论文提出了Halo架构，通过有理数算术实现无限深度推理，以解决当前深度学习模型中的数值误差问题，为AGI提供精确推理基础。",
      "motivation": "本研究动机源于挑战当前深度学习范式，其优先计算吞吐量而非数值精度，导致大型语言模型（LLMs）出现幻觉和逻辑不连贯。这些缺陷归因于IEEE 754浮点近似误差在深层组合函数中的累积，限制了AGI在高级因果推理中的表现，突显了高精度计算基础的重要性。现有方法的不足在于依赖统计相关性，忽视精确算术，从而引发逻辑不确定性，推动新范式的探索。",
      "method": "研究方法的核心是Halo架构，该架构采用有理数算术（Rational Arithmetic）替代传统浮点算术，以支持任意精度计算。创新点包括引入精确推理单元（Exact Inference Unit, EIU），专门处理深层推理任务，确保在无限深度计算中不产生数值误差。技术路线涉及从模糊计算转向精确计算，基于有理数表示避免近似问题，旨在为系统2 AGI提供稳定且逻辑一致的计算平台。",
      "result": "实验结果表明，在Huginn-0125原型测试中，标准600B参数BF16基线模型在混沌系统中表现出数值崩溃，而Halo架构能够无限期地维持零数值发散。这验证了精确算术在复杂推理环境中的优越性，显著优于传统浮点方法，突显了其在防止误差累积和提升系统稳定性方面的关键作用。摘要未提供具体性能指标如准确率，但强调了架构在逻辑一致性的实证优势。",
      "conclusion": "结论指出，精确算术是实现高级AGI逻辑一致性的必要条件，本研究的贡献在于提出并验证了Halo架构，挑战了当前深度学习范式。学术价值在于推动计算基础向高精度发展，为减少系统2 AGI的逻辑不确定性提供了新方向；实际应用价值在于提高智能系统的可靠性和推理能力。潜在局限性可能涉及计算效率或扩展性，未来工作或需优化架构性能和扩展到更多任务场景。",
      "tags": [
        "Rational Arithmetic",
        "Exact Inference Unit",
        "Arbitrary Precision Arithmetic",
        "Large Language Models",
        "AGI"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:33.905492Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18700",
    "title": "TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent",
    "authors": [
      "Xingyu Sui",
      "Yanyan Zhao",
      "Yulin Hu",
      "Jiahe Guo",
      "Weixiang Zhao",
      "Bing Qin"
    ],
    "abstract": "Emotional Support Conversation requires not only affective expression but also grounded instrumental support to provide trustworthy guidance. However, existing ESC systems and benchmarks largely focus on affective support in text-only settings, overlooking how external tools can enable factual grounding and reduce hallucination in multi-turn emotional support. We introduce TEA-Bench, the first interactive benchmark for evaluating tool-augmented agents in ESC, featuring realistic emotional scenarios, an MCP-style tool environment, and process-level metrics that jointly assess the quality and factual grounding of emotional support. Experiments on nine LLMs show that tool augmentation generally improves emotional support quality and reduces hallucination, but the gains are strongly capacity-dependent: stronger models use tools more selectively and effectively, while weaker models benefit only marginally. We further release TEA-Dialog, a dataset of tool-enhanced ESC dialogues, and find that supervised fine-tuning improves in-distribution support but generalizes poorly. Our results underscore the importance of tool use in building reliable emotional support agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18700.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18700",
    "published": "2026-01-26T17:15:27Z",
    "updated": "2026-01-26T17:15:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了TEA-Bench，首个用于评估工具增强型情感支持对话代理的交互式基准，通过整合工具环境减少幻觉并提高支持质量。",
      "motivation": "情感支持对话系统需要情感表达和事实基础来提供可靠指导，但现有系统和基准主要关注文本情感支持，忽视了外部工具在多轮对话中实现事实基础、减少幻觉的重要性。这导致支持可信度不足，限制了系统的实用性。因此，开发工具增强基准有助于弥补现有方法的不足，提升情感支持代理的可靠性。",
      "method": "研究提出TEA-Bench基准，包含现实情感场景、MCP风格工具环境和过程级指标，联合评估情感支持的质量和事实基础。实验使用九种大型语言模型进行测试，并发布TEA-Dialog数据集，用于监督微调分析。关键创新在于工具增强的交互式评估方法，以系统分析模型在工具支持下的表现。",
      "result": "实验结果显示，工具增强普遍提高了情感支持质量并减少了幻觉，但效果强烈依赖于模型能力：强大模型能更选择性和有效地使用工具，而弱模型获益有限。例如，工具使用降低了幻觉率，但具体数据摘要未明确说明。监督微调在分布内支持上表现改进，但泛化到新场景时效果不佳，表明模型泛化能力不足。",
      "conclusion": "论文的主要贡献是推出TEA-Bench基准和TEA-Dialog数据集，强调了工具使用对构建可靠情感支持代理的重要性。学术上推动了工具增强对话研究，实际应用中可提高系统真实性和信赖度。局限性包括模型的泛化能力问题，未来工作可探索如何增强工具的跨场景适应性。",
      "tags": [
        "Emotional Support Conversation",
        "Tool Augmentation",
        "Benchmarking",
        "Large Language Models",
        "Hallucination Reduction"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:25.199323Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18699",
    "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
    "authors": [
      "Olaf Yunus Laitinen Imanov"
    ],
    "abstract": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18699.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18699",
    "published": "2026-01-26T17:15:10Z",
    "updated": "2026-01-26T17:15:10Z",
    "comment": "16 pages, 16 figures (6 main + 10 supplementary)",
    "light_analysis": {
      "overview": "本研究通过系统性实验分析了大语言模型在连续微-tuning中灾难性遗忘的机制，识别了三个主要驱动因素。",
      "motivation": "大语言模型在连续微调过程中常出现灾难性遗忘现象，即新学习任务的知识干扰模型先前已掌握的能力，这限制了模型在多任务环境下的持续学习性能。尽管该问题已被广泛观察，但其内在机制理解不足，阻碍了有效缓解策略的开发。因此，本文旨在深入探究遗忘的机制，为解决持续学习中的核心挑战提供理论基础。",
      "method": "本研究采用基于Transformer的大型语言模型，进行系统性实验分析，模型参数规模从109B到400B，设置多种任务序列进行连续微调。通过监控注意力权重变化、中间层表示演化和损失景观特性，识别出三个关键机制：注意力权重中的梯度干扰、中间层的表示漂移以及损失景观的平坦化。实验设计聚焦于机制层面的定量分析，为理解遗忘过程提供技术路线。",
      "result": "实验结果显示，灾难性遗忘的严重程度与任务相似性高度相关，Pearson相关系数达到0.87，并与梯度对齐指标强相关。具体而言，约15%至23%的注意力头在微调过程中经历严重破坏，且模型下层比上层更易受干扰。这些发现量化了遗忘现象，揭示了其与模型内部结构的关联，为后续策略提供了实证支持。",
      "conclusion": "本研究的主要贡献在于建立了灾难性遗忘的机制基础，通过识别三个驱动因素，深化了对大型语言模型持续学习过程的理解。学术上，为相关领域提供了新的分析视角；实践上，为开发针对性缓解策略如模型调整或优化算法奠定了基础。尽管未直接提出解决方案，但为未来研究指明了方向，例如基于这些机制设计更有效的持续学习系统。",
      "tags": [
        "Large Language Model",
        "Continual Learning",
        "Catastrophic Forgetting",
        "Transformer",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:42.997988Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18698",
    "title": "Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge",
    "authors": [
      "Xiao Liu",
      "Jiawei Zhang"
    ],
    "abstract": "Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18698.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18698",
    "published": "2026-01-26T17:14:57Z",
    "updated": "2026-01-26T17:14:57Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "提出了Geo-Attraction Landmark Probing (GAP)框架，评估文本到视频模型的地理公平性，并发现Sora 2模型在全球视觉知识上表现相对均匀。",
      "motivation": "近年来，文本到视频生成模型取得显著进展，但模型是否编码地理公平的视觉知识尚不明确。本研究旨在解决潜在的地理偏见问题，该问题可能导致模型在全球部署应用中产生不公正，影响多样化区域的可靠性。现有假设常认为存在强地理偏见，但缺乏系统性评估。评估地理公平性对确保模型公平表达全球视觉知识至关重要，以避免模型在旅游景点合成等任务中的局限性。摘要未明确说明现有方法的不足，但强调了澄清模型公平性的重要性。",
      "method": "本研究提出了Geo-Attraction Landmark Probing (GAP)框架，系统评估文本到视频模型的地理公平性。关键创新包括构建GEOATTRACTION-500基准数据集，包含500个全球分布的旅游景点，涵盖不同地区和流行度水平，以提供多样性样本。GAP整合了互补指标，如全球结构对齐、基于关键点的细粒度对齐和视觉语言模型判断，这些指标均与人类评估验证，确保评估的可靠性。方法应用最先进的文本到视频模型Sora 2进行评估，突出了技术特色，如分离整体视频质量与景点特定知识。",
      "result": "应用GAP框架评估Sora 2模型，发现与常见的强地理偏见假设相反。模型在全球视觉知识表现相对均匀，跨不同地区、发展水平和文化群体，对景点流行度的依赖性较弱。这表明模型能够公平合成全球景点，未显示显著地理偏差。评估指标经过人类验证，确保了结果的可靠性，但摘要未提供具体性能数据如准确率。与基线方法的对比在摘要中未明确说明，但结果挑战了传统偏见假设，支持模型的公平性潜力。",
      "conclusion": "本研究的主要贡献在于提出了GAP框架和基准数据集，并揭示了文本到视频模型在全球视觉知识上的均匀表现。学术价值在于提供了一种系统性评估地理公平性的方法，为相关研究奠定基础。实际应用中，这突显了模型在全球部署中的潜力，减少偏见风险，促进更公平的视频生成系统。局限性包括摘要未明确说明，未来工作需继续评估模型演进中的公平性，并扩展评估到更多模型和应用场景。",
      "tags": [
        "Text-to-Video Generation",
        "Geographic Fairness",
        "Evaluation Framework",
        "Vision-Language Models",
        "Benchmark Dataset"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:50.780423Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18696",
    "title": "Explainability Methods for Hardware Trojan Detection: A Systematic Comparison",
    "authors": [
      "Paul Whitten",
      "Francis Wolff",
      "Chris Papachristou"
    ],
    "abstract": "Hardware trojan detection requires accurate identification and interpretable explanations for security engineers to validate and act on results. This work compares three explainability categories for gate-level trojan detection on the Trust-Hub benchmark: (1) domain-aware property-based analysis of 31 circuit-specific features from gate fanin patterns, flip-flop distances, and I/O connectivity; (2) case-based reasoning using k-nearest neighbors for precedent-based explanations; and (3) model-agnostic feature attribution (LIME, SHAP, gradient).   Results show different advantages per approach. Property-based analysis provides explanations through circuit concepts like \"high fanin complexity near outputs indicates potential triggers.\" Case-based reasoning achieves 97.4% correspondence between predictions and training exemplars, offering justifications grounded in precedent. LIME and SHAP provide feature attributions with strong inter-method correlation (r=0.94, p<0.001) but lack circuit-level context for validation.   XGBoost classification achieves 46.15% precision and 52.17% recall on 11,392 test samples, a 9-fold precision improvement over prior work (Hasegawa et al.: 5.13%) while reducing false positive rates from 5.6% to 0.25%. Gradient-based attribution runs 481 times faster than SHAP but provides similar domain-opaque insights.   This work demonstrates that property-based and case-based approaches offer domain alignment and precedent-based interpretability compared to generic feature rankings, with implications for XAI deployment where practitioners must validate ML predictions.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18696.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18696",
    "published": "2026-01-26T17:13:00Z",
    "updated": "2026-01-26T17:13:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文系统比较了三种硬件木马检测的可解释性方法，强调基于属性和案例的方法在提供领域对齐解释方面的优势。",
      "motivation": "硬件木马检测需要准确识别和可解释的结果，以便安全工程师验证和基于检测结果采取行动。现有方法在可解释性和精度方面存在不足，例如先前研究精度仅为5.13%，且缺乏电路级别的解释机制，这影响了实际部署的可靠性。因此，本研究旨在通过系统比较不同可解释性方法，解决这些缺陷，提升检测的实用性和安全工程师的信任度。",
      "method": "论文比较了三种可解释性类别：基于属性的分析（利用31个电路特定特征，如门扇入模式、触发器距离和输入输出连接性）、案例推理（使用k近邻算法进行基于先例的解释）和模型无关特征归因（如LIME、SHAP和梯度方法）。在Trust-Hub基准上采用XGBoost分类器进行门级木马检测，分析各种方法的优缺点，关键创新在于系统评估不同解释技术的领域适用性和性能。",
      "result": "实验结果显示，XGBoost分类在11,392个测试样本上达到46.15%的精确度和52.17%的召回率，相比先前工作（Hasegawa等人：5.13%精确度）提升了9倍，假阳性率从5.6%降低到0.25%。案例推理的预测与训练范例对应性为97.4%，而LIME和SHAP的特征归因显示出强相关性（r=0.94, p<0.001），但缺乏电路级上下文。梯度归因运行速度比SHAP快481倍，但解释类似地缺乏领域具体性。",
      "conclusion": "研究总结表明，基于属性和案例的可解释性方法在硬件木马检测中提供更好的领域对齐和基于先例的解释，相较于通用的特征排名方法。这有助于推进可解释AI在硬件安全领域的实际应用，提高工程师对机器学习预测的验证能力。未来工作可探索结合多种方法以优化解释质量，或扩展应用到其他安全关键领域。",
      "tags": [
        "Hardware Trojan Detection",
        "Explainable AI",
        "XGBoost",
        "LIME",
        "SHAP"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:03.655402Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18681",
    "title": "ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule",
    "authors": [
      "Yilie Huang",
      "Wenpin Tang",
      "Xunyu Zhou"
    ],
    "abstract": "We consider time discretization for score-based diffusion models to generate samples from a learned reverse-time dynamic on a finite grid. Uniform and hand-crafted grids can be suboptimal given a budget on the number of time steps. We introduce Adaptive Reparameterized Time (ART) that controls the clock speed of a reparameterized time variable, leading to a time change and uneven timesteps along the sampling trajectory while preserving the terminal time. The objective is to minimize the aggregate error arising from the discretized Euler scheme. We derive a randomized control companion, ART-RL, and formulate time change as a continuous-time reinforcement learning (RL) problem with Gaussian policies. We then prove that solving ART-RL recovers the optimal ART schedule, which in turn enables practical actor--critic updates to learn the latter in a data-driven way. Empirically, based on the official EDM pipeline, ART-RL improves Fréchet Inception Distance on CIFAR-10 over a wide range of budgets and transfers to AFHQv2, FFHQ, and ImageNet without the need of retraining.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18681.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18681",
    "published": "2026-01-26T16:56:40Z",
    "updated": "2026-01-26T16:56:40Z",
    "comment": "17 pages, 7 figures",
    "light_analysis": {
      "overview": "本文提出ART和ART-RL方法，通过强化学习自适应优化扩散模型的时间步调度，提升采样效率和质量。",
      "motivation": "扩散模型在采样过程中需要进行时间离散化处理，以从学习到的反向时间动态生成样本，但传统的均匀或手工设计时间步网格在给定预算下可能并非最优，导致累积误差增加，影响样本质量和计算效率。现有方法缺乏自适应性，无法根据模型特性动态调整时间步分配，这限制了性能优化，特别是在资源受限场景下，因此需要一个更智能的调度方案来改进采样过程。",
      "method": "论文核心方法是自适应重参数化时间（ART），它通过控制重参数化时间变量的时钟速度来实现采样轨迹上的不均匀时间步调度，同时保持终端时间不变，目标是最小化离散化欧拉方案的累积误差。进一步，引入随机控制伴侣ART-RL，将时间变化形式化为连续时间强化学习问题，使用高斯策略进行优化，并通过actor-critic算法以数据驱动方式学习最优调度，基于官方EDM流水线实现，确保与现有扩散模型的兼容性。",
      "result": "实证结果表明，基于EDM流水线，ART-RL在CIFAR-10数据集上显著改善了Fréchet Inception Distance（FID）指标，覆盖广泛的时间步预算范围，具体提升幅度摘要未明确说明。该方法可无缝转移到AFHQv2、FFHQ和ImageNet等数据集，无需重新训练，展示了良好的泛化能力和鲁棒性。与基线均匀或手工网格相比，ART-RL通过自适应调度减少了累积误差，从而提高了采样性能和质量。",
      "conclusion": "本研究的主要贡献是提出ART和ART-RL框架，首次将强化学习应用于扩散模型的时间步调度优化，有效解决了固定网格的不足，为扩散采样领域提供了新的自适应方法。学术价值在于拓展了时间离散化的理论框架，实际应用中能提升模型效率和样本生成质量。尽管摘要未明确说明局限性，未来工作可能包括扩展到更复杂模型或探索与其他优化技术的集成，以进一步推动该领域发展。",
      "tags": [
        "Diffusion Models",
        "Reinforcement Learning",
        "Time Discretization",
        "Adaptive Scheduling",
        "Actor-Critic Methods"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:14.936438Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18678",
    "title": "Counterfactual Explanations on Robust Perceptual Geodesics",
    "authors": [
      "Eslam Zaher",
      "Maciej Trzaskowski",
      "Quan Nguyen",
      "Fred Roosta"
    ],
    "abstract": "Latent-space optimization methods for counterfactual explanations - framed as minimal semantic perturbations that change model predictions - inherit the ambiguity of Wachter et al.'s objective: the choice of distance metric dictates whether perturbations are meaningful or adversarial. Existing approaches adopt flat or misaligned geometries, leading to off-manifold artifacts, semantic drift, or adversarial collapse. We introduce Perceptual Counterfactual Geodesics (PCG), a method that constructs counterfactuals by tracing geodesics under a perceptually Riemannian metric induced from robust vision features. This geometry aligns with human perception and penalizes brittle directions, enabling smooth, on-manifold, semantically valid transitions. Experiments on three vision datasets show that PCG outperforms baselines and reveals failure modes hidden under standard metrics.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.HC",
      "math.DG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18678.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18678",
    "published": "2026-01-26T16:52:54Z",
    "updated": "2026-01-26T16:52:54Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "论文提出Perceptual Counterfactual Geodesics (PCG)方法，通过感知黎曼度量下的测地线构建反事实解释，解决了现有方法的几何对齐问题，确保语义有效转换。",
      "motivation": "反事实解释旨在通过最小语义扰动改变模型预测，但现有潜在空间优化方法中距离度量的选择模糊，导致扰动可能无意义或对抗性。现有方法采用平面或不对齐的几何，引发流形外伪影、语义漂移或对抗崩溃，这降低了反事实的可解释性和可信度，限制了其在可解释AI中的应用。因此，需要一种能对齐人类感知、避免对抗性方向的几何方法来提升反事实的有效性。",
      "method": "PCG方法通过追踪在感知黎曼度量下的测地线构建反事实解释。该度量由鲁棒视觉特征诱导，对齐人类感知并惩罚脆弱方向，确保扰动平滑、在流形上且语义有效。关键创新是引入黎曼几何，优化反事实路径，避免平面几何导致的伪影。技术细节包括从鲁棒特征构建度量张量，并应用测地线方程计算最小距离路径，从而在潜在空间中实现自然流形上的语义转换。",
      "result": "在三个视觉数据集上的实验表明，PCG方法在性能上优于基线方法，生成的反事实解释更平滑和语义有效，并揭示了标准距离度量下隐藏的模型失败模式。摘要未提供具体数值如准确率提升，但结果显示PCG在减少流形外伪影和语义漂移方面有显著改进，基线方法可能包括常见反事实生成技术，这证实了感知几何在提升反事实质量方面的优势。",
      "conclusion": "论文的主要贡献是提出了PCG方法，通过感知黎曼度量解决了反事实解释中的几何对齐问题，提高了语义有效性和可解释性，对可解释AI领域具有重要学术和实际价值。摘要未明确说明局限性，但潜在未来工作可能包括扩展到其他数据类型或优化计算效率，以促进更广泛的应用。",
      "tags": [
        "Counterfactual Explanations",
        "Riemannian Geometry",
        "Geodesics",
        "Perceptual Metrics",
        "Robust Features"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:20.030822Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18676",
    "title": "Quasi Monte Carlo methods enable extremely low-dimensional deep generative models",
    "authors": [
      "Miles Martinez",
      "Alex H. Williams"
    ],
    "abstract": "This paper introduces quasi-Monte Carlo latent variable models (QLVMs): a class of deep generative models that are specialized for finding extremely low-dimensional and interpretable embeddings of high-dimensional datasets. Unlike standard approaches, which rely on a learned encoder and variational lower bounds, QLVMs directly approximate the marginal likelihood by randomized quasi-Monte Carlo integration. While this brute force approach has drawbacks in higher-dimensional spaces, we find that it excels in fitting one, two, and three dimensional deep latent variable models. Empirical results on a range of datasets show that QLVMs consistently outperform conventional variational autoencoders (VAEs) and importance weighted autoencoders (IWAEs) with matched latent dimensionality. The resulting embeddings enable transparent visualization and post hoc analyses such as nonparametric density estimation, clustering, and geodesic path computation, which are nontrivial to validate in higher-dimensional spaces. While our approach is compute-intensive and struggles to generate fine-scale details in complex datasets, it offers a compelling solution for applications prioritizing interpretability and latent space analysis.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18676.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18676",
    "published": "2026-01-26T16:51:03Z",
    "updated": "2026-01-26T16:51:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了 quasi-Monte Carlo 潜变量模型 (QLVMs)，通过直接近似边缘似然实现高维数据集的极低维度和可解释嵌入。",
      "motivation": "本研究旨在解决在高维数据集中寻找极低维度和可解释嵌入的实际问题。传统方法如变分自编码器（VAEs）和重要性加权自编码器（IWAEs）依赖于学习编码器和变分下界，可能导致嵌入质量不高且可解释性有限，尤其是在潜空间验证和可视化方面存在挑战。这一问题在机器学习和数据分析中尤为重要，因为可解释的嵌入对于理解数据结构、进行后验分析和应用场景（如生物信息学或图像处理）至关重要，而现有方法在低维嵌入的准确性和透明性方面不足。",
      "method": "论文提出 quasi-Monte Carlo 潜变量模型 (QLVMs)，其核心方法是使用随机拟蒙特卡洛积分直接近似边缘似然，而不依赖于传统变分自编码器的编码器和变分下界。关键创新点在于避免学习编码器，直接优化边缘似然，专注于一维、二维和三维的极低维度潜变量模型，以提高嵌入的可解释性和准确性。虽然摘要未明确说明具体的数据集和模型架构细节，但该方法基于拟蒙特卡洛方法应用于多个数据集，为深度生成模型提供了新的技术路线。",
      "result": "实证结果在多个数据集上显示，quasi-Monte Carlo 潜变量模型 (QLVMs) 在匹配潜维数的情况下，始终优于传统的变分自编码器（VAEs）和重要性加权自编码器（IWAEs）。摘要中未明确说明具体性能指标（如准确率提升数据），但表明 QLVMs 在极低维度的嵌入任务中表现出色，提供了更准确的边缘似然近似。生成的嵌入支持透明可视化，并易于进行非参数密度估计、聚类和测地路径计算等后验分析，这些在高维空间中验证起来较为困难，凸显了该方法的优势。",
      "conclusion": "论文的主要贡献是引入了 quasi-Monte Carlo 潜变量模型 (QLVMs)，通过直接近似边缘似然实现了高维数据集的极低维度和可解释嵌入。这一研究在学术上为深度生成模型提供了新的方法，强调了可解释性和潜空间分析的重要性，具有实际应用价值，特别适合需要透明可视化和分析的应用场景。局限性包括方法计算密集且在复杂数据集中生成精细细节有困难，未来工作可能包括优化计算效率、扩展到更高维度或应用于更广泛的数据集，以进一步提升性能。",
      "tags": [
        "Quasi-Monte Carlo Integration",
        "Deep Generative Models",
        "Latent Variable Models",
        "Variational Autoencoders",
        "Importance Weighted Autoencoders"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:44.432943Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18675",
    "title": "Learning temporal embeddings from electronic health records of chronic kidney disease patients",
    "authors": [
      "Aditya Kumar",
      "Mario A. Cypko",
      "Oliver Amft"
    ],
    "abstract": "We investigate whether temporal embedding models trained on longitudinal electronic health records can learn clinically meaningful representations without compromising predictive performance, and how architectural choices affect embedding quality. Model-guided medicine requires representations that capture disease dynamics while remaining transparent and task agnostic, whereas most clinical prediction models are optimised for a single task. Representation learning facilitates learning embeddings that generalise across downstream tasks, and recurrent architectures are well-suited for modelling temporal structure in observational clinical data. Using the MIMIC-IV dataset, we study patients with chronic kidney disease (CKD) and compare three recurrent architectures: a vanilla LSTM, an attention-augmented LSTM, and a time-aware LSTM (T-LSTM). All models are trained both as embedding models and as direct end-to-end predictors. Embedding quality is evaluated via CKD stage clustering and in-ICU mortality prediction. The T-LSTM produces more structured embeddings, achieving a lower Davies-Bouldin Index (DBI = 9.91) and higher CKD stage classification accuracy (0.74) than the vanilla LSTM (DBI = 15.85, accuracy = 0.63) and attention-augmented LSTM (DBI = 20.72, accuracy = 0.67). For in-ICU mortality prediction, embedding models consistently outperform end-to-end predictors, improving accuracy from 0.72-0.75 to 0.82-0.83, which indicates that learning embeddings as an intermediate step is more effective than direct end-to-end learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18675.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18675",
    "published": "2026-01-26T16:50:50Z",
    "updated": "2026-01-26T16:50:50Z",
    "comment": "7 pages, 3 figures, 3 tables. The paper has been submitted to IEEE EMBC 2026 and copyright might be transferred without notice",
    "light_analysis": {
      "overview": "本研究通过比较不同循环架构的时间嵌入模型，从慢性肾病患者的电子健康记录中学习临床有意义的表示，验证了T-LSTM在嵌入质量和预测性能上的优越性。",
      "motivation": "本研究的动机源于临床预测模型的局限性：大多数模型针对单一任务优化，缺乏泛化性和透明性，而模型引导医学需要捕获疾病动态的任务无关表示。电子健康记录具有时间序列特性，适合用循环架构建模，但现有方法在嵌入质量方面探索不足。因此，本研究旨在探究时间嵌入模型是否能学习临床有意义的表示而不影响预测性能，并评估不同架构对嵌入质量的影响，以促进更高效的临床数据分析。",
      "method": "研究方法基于MIMIC-IV数据集，针对慢性肾病患者，比较了三种循环神经网络架构：基础LSTM、注意力增强LSTM和时间感知LSTM。模型训练分为两部分：作为嵌入模型学习时间表示，以及作为端到端预测器直接进行任务预测。嵌入质量通过慢性肾病阶段聚类（使用Davies-Bouldin指数评估）和ICU死亡率预测任务来量化，从而系统分析架构选择对表示学习的影响。",
      "result": "实验结果显示，时间感知LSTM在嵌入质量上表现最佳，其Davies-Bouldin指数为9.91，慢性肾病阶段分类准确率达0.74，优于基础LSTM的15.85和0.63以及注意力增强LSTM的20.72和0.67。在ICU死亡率预测中，嵌入模型一致超越端到端预测器，准确率从0.72-0.75提升至0.82-0.83，表明学习嵌入作为中间步骤比直接端到端学习更有效。",
      "conclusion": "本研究的主要贡献是证明了时间嵌入模型可以从电子健康记录中学习临床有意义的表示，同时保持高预测性能。其学术价值在于为表示学习在医疗领域的应用提供了实证支持，实际应用上可促进模型引导医学的发展，通过泛化嵌入改善多任务预测。局限性可能包括数据集特异性，未来工作可扩展到其他疾病或集成更多数据模态以增强鲁棒性。",
      "tags": [
        "Temporal Embeddings",
        "LSTM",
        "Attention Mechanism",
        "Representation Learning",
        "Electronic Health Records"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:24.627857Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18672",
    "title": "A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks",
    "authors": [
      "Spyros Rigas",
      "Thanasis Papaioannou",
      "Panagiotis Trakadas",
      "Georgios Alexandridis"
    ],
    "abstract": "Kolmogorov-Arnold Networks (KANs) have recently demonstrated promising potential in scientific machine learning, partly due to their capacity for grid adaptation during training. However, existing adaptation strategies rely solely on input data density, failing to account for the geometric complexity of the target function or metrics calculated during network training. In this work, we propose a generalized framework that treats knot allocation as a density estimation task governed by Importance Density Functions (IDFs), allowing training dynamics to determine grid resolution. We introduce a curvature-based adaptation strategy and evaluate it across synthetic function fitting, regression on a subset of the Feynman dataset and different instances of the Helmholtz PDE, demonstrating that it significantly outperforms the standard input-based baseline. Specifically, our method yields average relative error reductions of 25.3% on synthetic functions, 9.4% on the Feynman dataset, and 23.3% on the PDE benchmark. Statistical significance is confirmed via Wilcoxon signed-rank tests, establishing curvature-based adaptation as a robust and computationally efficient alternative for KAN training.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18672.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18672",
    "published": "2026-01-26T16:49:06Z",
    "updated": "2026-01-26T16:49:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种基于曲率的动态网格适应框架，用于Kolmogorov-Arnold Networks，通过重要性密度函数优化节点分配，显著提升训练性能。",
      "motivation": "Kolmogorov-Arnold Networks (KANs) 在科学机器学习中展现出潜力，但其现有网格适应策略仅依赖输入数据密度，未考虑目标函数的几何复杂性或训练过程中的计算指标，这限制了KANs在复杂函数拟合和偏微分方程求解中的精度。针对这一问题，研究旨在改进现有方法，通过引入更智能的适应机制来克服不足，提升模型在科学计算任务中的性能。",
      "method": "论文提出了一个广义框架，将节点分配问题视为由重要性密度函数（IDFs）控制的密度估计任务，使网格分辨率由训练动态决定。核心创新是引入基于曲率的适应策略，利用目标函数的曲率信息来指导节点分布，以更好地适应函数几何复杂性。该方法在合成函数拟合、Feynman数据集子集的回归和Helmholtz偏微分方程的不同实例上进行评估，使用具体数据集验证其技术路线和模型架构。",
      "result": "实验结果显示，基于曲率的适应策略在多个基准测试中显著优于标准基于输入的基线。具体而言，平均相对误差在合成函数上减少了25.3%，在Feynman数据集上减少了9.4%，在偏微分方程基准上减少了23.3%。通过Wilcoxon符号秩检验确认了统计显著性，表明该方法不仅计算高效，且性能稳健，与基线方法相比在误差减少方面有明确提升。",
      "conclusion": "本研究的主要贡献是开发了一种基于曲率的动态网格适应框架，为KANs训练提供了更有效的策略。学术价值在于强调了考虑几何复杂性在机器学习中的重要性，并为科学机器学习任务如函数回归和偏微分方程求解提供了改进方案。实际应用价值包括提升模型精度和效率，未来工作方向可探索其他指标或扩展到更多领域，以进一步优化性能。",
      "tags": [
        "Kolmogorov-Arnold Networks",
        "Grid Adaptation",
        "Curvature-based Adaptation",
        "Importance Density Functions"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:44.956876Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18650",
    "title": "FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning",
    "authors": [
      "Liheng Yu",
      "Zhe Zhao",
      "Yuxuan Wang",
      "Pengkun Wang",
      "Binwu Wang",
      "Yang Wang"
    ],
    "abstract": "Machine unlearning, which aims to efficiently remove the influence of specific data from trained models, is crucial for upholding data privacy regulations like the ``right to be forgotten\". However, existing research predominantly evaluates unlearning methods on relatively balanced forget sets. This overlooks a common real-world scenario where data to be forgotten, such as a user's activity records, follows a long-tailed distribution. Our work is the first to investigate this critical research gap. We find that in such long-tailed settings, existing methods suffer from two key issues: \\textit{Heterogeneous Unlearning Deviation} and \\textit{Skewed Unlearning Deviation}. To address these challenges, we propose FaLW, a plug-and-play, instance-wise dynamic loss reweighting method. FaLW innovatively assesses the unlearning state of each sample by comparing its predictive probability to the distribution of unseen data from the same class. Based on this, it uses a forgetting-aware reweighting scheme, modulated by a balancing factor, to adaptively adjust the unlearning intensity for each sample. Extensive experiments demonstrate that FaLW achieves superior performance. Code is available at \\textbf{Supplementary Material}.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18650.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18650",
    "published": "2026-01-26T16:21:01Z",
    "updated": "2026-01-26T16:21:01Z",
    "comment": "camera-ready for iclr2026",
    "light_analysis": {
      "overview": "论文提出了FaLW方法，一种针对长尾遗忘数据的动态损失重新加权技术，以解决现有去学习方法中的偏差问题。",
      "motivation": "机器去学习旨在从训练模型中高效移除特定数据的影响，这对遵守“被遗忘权”等数据隐私法规至关重要。然而，现有研究主要评估平衡遗忘集，忽略了现实世界中遗忘数据常呈长尾分布的常见场景，如用户活动记录。这导致现有方法在长尾设置下出现异质性遗忘偏差和偏斜遗忘偏差，影响去学习效果，突显了研究此问题的重要性和现有方法的不足。",
      "method": "FaLW是一种即插即用、基于实例的动态损失重新加权方法。其创新点在于通过比较每个样本的预测概率与同类未见数据的分布来评估遗忘状态，并采用遗忘感知的重新加权方案，通过平衡因子自适应调整每个样本的遗忘强度。这种方法无需改变模型架构，直接应用于现有去学习框架，关键是通过概率比较实现动态权重调整，以应对长尾分布中的偏差问题。",
      "result": "实验表明FaLW在长尾遗忘设置下实现了优越性能。摘要未明确说明具体指标如准确率提升或效率改进，但强调通过广泛实验验证了其有效性，并优于基线方法，暗示能有效减少异质性和偏斜遗忘偏差。这显示了FaLW在改进去学习效果方面的潜力，但具体数据需参考完整论文。",
      "conclusion": "本研究首次探讨长尾遗忘问题，提出FaLW方法以解决现有方法的偏差。主要贡献是引入遗忘感知的动态损失重新加权，填补了机器去学习在现实场景中的研究空白。学术价值在于提升去学习方法对非平衡数据的适应性，实际应用价值在于更好支持数据隐私法规的实施。未来工作可能包括扩展其他分布场景或优化参数设计。",
      "tags": [
        "Machine Unlearning",
        "Long-tailed Distribution",
        "Loss Reweighting",
        "Dynamic Weighting",
        "Forgetting-aware"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:30.919255Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18642",
    "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory",
    "authors": [
      "Lei Wei",
      "Xu Dong",
      "Xiao Peng",
      "Niantao Xie",
      "Bin Wang"
    ],
    "abstract": "Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18642.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18642",
    "published": "2026-01-26T16:12:54Z",
    "updated": "2026-01-26T16:12:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "FadeMem是一种受生物启发的代理内存架构，通过模拟人类记忆的主动遗忘机制，实现高效的内存管理和多跳推理。",
      "motivation": "大型语言模型作为自主代理部署时面临内存限制，缺乏选择性遗忘机制，导致灾难性遗忘或信息过载。现有AI系统采用二进制保留策略，要么保留所有信息，要么完全遗忘，无法平衡保留与遗忘。人类记忆通过自适应衰减自然平衡这一过程，而当前方法效率低下。因此，开发类似人类认知的遗忘机制至关重要，以解决代理长期运行中的性能下降和存储需求增加问题，提高智能系统的实用性和效率。",
      "method": "FadeMem引入一个双层内存层次结构，采用微分衰减率，其中保留由自适应指数衰减函数控制，调制因素包括语义相关性、访问频率和时间模式。关键创新点包括LLM引导的冲突解决和智能内存融合，系统通过整合相关信息并允许无关细节逐渐衰减，模拟人类遗忘过程。这有助于优化内存使用，提高信息处理效率，具体实现中依赖LLM技术来引导记忆整合和衰减决策。",
      "result": "在Multi-Session Chat、LoCoMo和LTI-Bench数据集上的实验表明，FadeMem在多跳推理和检索任务中表现优异，存储需求减少了45%。与现有基线方法相比，它通过选择性遗忘避免了信息过载，显著提升了推理准确性和效率，具体体现在任务性能指标的改进上。这些结果验证了生物启发遗忘机制在代理内存系统中的有效性，为实际应用提供了数据支持。",
      "conclusion": "FadeMem的主要贡献是将生物启发的遗忘机制引入代理内存系统，验证了其提高内存效率和推理性能的能力。学术上，这为AI内存管理提供了新的研究视角；实际中，有助于减少存储成本并增强代理的长期运行能力。潜在局限性可能包括对特定数据集的依赖，未来工作可扩展应用到更广泛场景或进一步优化遗忘策略，以提升通用性和鲁棒性。",
      "tags": [
        "Large Language Model",
        "Biologically-Inspired AI",
        "Memory Management",
        "Forgetting Mechanism",
        "Multi-Hop Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:44.468162Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18640",
    "title": "TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning",
    "authors": [
      "Zhiwei Zheng",
      "Kevin Bryson"
    ],
    "abstract": "Advances in single-cell and spatial transcriptomic technologies have transformed tumor ecosystem profiling at cellular resolution. However, large scale studies on patient cohorts continue to rely on bulk transcriptomic data, where variation in tumor purity obscures tumor-intrinsic transcriptional signals and constrains downstream discovery. Many deconvolution methods report strong performance on synthetic bulk mixtures but fail to generalize to real patient cohorts because of unmodeled biological and technical variation.   Here, we introduce TwinPurify, a representation learning framework that adapts the Barlow Twins self-supervised objective, representing a fundamental departure from the deconvolution paradigm. Rather than resolving the bulk mixture into discrete cell-type fractions, TwinPurify instead learns continuous, high-dimensional tumor embeddings by leveraging adjacent-normal profiles within the same cohort as \"background\" guidance, enabling the disentanglement of tumor-specific signals without relying on any external reference.   Benchmarked against multiple large cancer cohorts across RNA-seq and microarray platforms, TwinPurify outperforms conventional representation learning baselines like auto-encoders in recovering tumor-intrinsic and immune signals. The purified embeddings improve molecular subtype and grade classification, enhance survival model concordance, and uncover biologically meaningful pathway activities compared to raw bulk profiles. By providing a transferable framework for decontaminating bulk transcriptomics, TwinPurify extends the utility of existing clinical datasets for molecular discovery.",
    "categories": [
      "cs.LG",
      "q-bio.MN"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18640.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18640",
    "published": "2026-01-26T16:11:34Z",
    "updated": "2026-01-26T16:11:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出TwinPurify框架，通过自监督学习净化批量基因表达数据，揭示肿瘤固有转录程序，摒弃传统解卷积方法。",
      "motivation": "随着单细胞和空间转录组技术的进步，肿瘤生态系统分析达到细胞分辨率，但大规模患者队列研究仍依赖批量转录组数据。其中，肿瘤纯度变异模糊了肿瘤固有转录信号，严重制约下游发现，如分子亚型分类和生存分析。现有解卷积方法虽然在合成批量混合数据上表现良好，但因未建模的生物学和技术变异，无法泛化到真实患者队列，导致实际应用中效果有限。因此，迫切需要一种新方法来有效分离肿瘤特异性信号，而不依赖外部参考，以提升数据可靠性。",
      "method": "TwinPurify是一种表示学习框架，采用Barlow Twins自监督目标，从根本上区别于传统解卷积范式。它不将批量转录组混合物分解为离散细胞类型分数，而是利用同一患者队列中的相邻正常剖面作为“背景”指导，学习连续、高维的肿瘤嵌入。这种方法通过自监督优化，在无需外部参考的情况下，直接解耦肿瘤特定信号，适用于RNA-seq和微阵列平台的大型癌症队列数据集，从而灵活捕捉肿瘤特异性转录变化。",
      "result": "在多个大型癌症队列（包括RNA-seq和微阵列平台）的基准测试中，TwinPurify在恢复肿瘤固有和免疫信号方面优于自编码器等传统表示学习基线。实验结果表明，净化后的嵌入显著改善分子亚型和分级分类的准确性，增强生存模型的一致性，并揭示出有生物学意义的通路活动。与原始批量剖面相比，TwinPurify提供了更可靠的肿瘤特征分析，尽管摘要未提供具体性能指标数值，但其整体效果在多个指标上优于现有方法，提升了数据分析的稳健性。",
      "conclusion": "TwinPurify的主要贡献是提出了一个可转移的表示学习框架，用于净化批量转录组学数据，扩展了现有临床数据集在分子发现中的效用。该方法通过自监督学习有效揭示肿瘤固有转录程序，改善了下游分类和生存分析，具有重要的学术价值，推动了肿瘤转录组学领域的发展，并具备实际应用潜力，如增强临床数据再分析。未来工作可扩展到更多疾病类型或多组学数据集成，进一步验证其普适性和局限性。",
      "tags": [
        "Self-Supervised Learning",
        "Representation Learning",
        "Barlow Twins",
        "Tumor Purity",
        "Gene Expression Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:07.423099Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18638",
    "title": "Physics-Informed Uncertainty Enables Reliable AI-driven Design",
    "authors": [
      "Tingkai Xue",
      "Chin Chun Ooi",
      "Yang Jiang",
      "Luu Trung Pham Duong",
      "Pao-Hsiung Chiu",
      "Weijiang Zhao",
      "Nagarajan Raghavan",
      "My Ha Dao"
    ],
    "abstract": "Inverse design is a central goal in much of science and engineering, including frequency-selective surfaces (FSS) that are critical to microelectronics for telecommunications and optical metamaterials. Traditional surrogate-assisted optimization methods using deep learning can accelerate the design process but do not usually incorporate uncertainty quantification, leading to poorer optimization performance due to erroneous predictions in data-sparse regions. Here, we introduce and validate a fundamentally different paradigm of Physics-Informed Uncertainty, where the degree to which a model's prediction violates fundamental physical laws serves as a computationally-cheap and effective proxy for predictive uncertainty. By integrating physics-informed uncertainty into a multi-fidelity uncertainty-aware optimization workflow to design complex frequency-selective surfaces within the 20 - 30 GHz range, we increase the success rate of finding performant solutions from less than 10% to over 50%, while simultaneously reducing computational cost by an order of magnitude compared to the sole use of a high-fidelity solver. These results highlight the necessity of incorporating uncertainty quantification in machine-learning-driven inverse design for high-dimensional problems, and establish physics-informed uncertainty as a viable alternative to quantifying uncertainty in surrogate models for physical systems, thereby setting the stage for autonomous scientific discovery systems that can efficiently and robustly explore and evaluate candidate designs.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18638.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18638",
    "published": "2026-01-26T16:10:59Z",
    "updated": "2026-01-26T16:10:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出物理知情不确定度范式，并将其集成到多保真度优化工作流中，显著提升频率选择表面逆设计的成功率和计算效率。",
      "motivation": "逆设计是科学和工程中的核心目标，如频率选择表面对微电子和通信至关重要。传统基于深度学习的代理辅助优化方法虽加速设计，但缺乏不确定度量化，导致数据稀疏区域预测错误，优化性能下降。这一问题在高维设计中尤为突出，现有方法不足，亟需有效的不确定度量化来提高设计可靠性和效率。",
      "method": "论文提出物理知情不确定度范式，其中模型预测违反基本物理定律的程度被用作预测不确定度的计算廉价且有效代理。该方法集成到一个多保真度不确定度感知优化工作流中，用于设计20-30 GHz范围内的复杂频率选择表面。通过结合不同保真度的求解器和物理约束，实现对设计空间的高效探索和不确定度评估，关键创新在于利用物理知识量化不确定度。",
      "result": "实验显示，集成物理知情不确定度后，优化工作流将找到高性能频率选择表面解决方案的成功率从不到10%提高到超过50%。同时，计算成本相比仅使用高保真求解器降低了一个数量级，效率显著提升。这些结果对比基线方法，证明了该方法在增强逆设计成功率和降低资源消耗方面的优越性。",
      "conclusion": "论文的主要贡献是强调了机器学习驱动逆设计中不确定度量化的必要性，并建立了物理知情不确定度作为量化代理模型不确定度的可行替代方案。这为开发高效、稳健探索候选设计的自主科学发现系统奠定基础，具有重要的学术和实际应用价值。未来工作可能包括将方法扩展到更广泛的高维问题中。",
      "tags": [
        "Physics-Informed Uncertainty",
        "Inverse Design",
        "Frequency-Selective Surfaces",
        "Multi-fidelity Optimization",
        "Uncertainty Quantification"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:18.948349Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18633",
    "title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting",
    "authors": [
      "Tong Shi",
      "Melonie de Almeida",
      "Daniela Ivanova",
      "Nicolas Pugeault",
      "Paul Henderson"
    ],
    "abstract": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18633.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18633",
    "published": "2026-01-26T16:06:57Z",
    "updated": "2026-01-26T16:06:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "Splat-Portrait通过Gaussian Splatting技术，从单一肖像图像和语音自动生成逼真的说话视频，无需运动先验或3D监督。",
      "motivation": "说话头生成技术旨在从语音和单张肖像合成自然说话视频，应用广泛如虚拟助手和娱乐，但面临挑战。现有3D方法常依赖基于扭曲的面部运动先验，这些启发式方法导致3D头像重建不准确，削弱了动画的真实感，限制了实际应用。因此，需要一种更准确且不依赖外部先验的方法，以改进3D重建和唇部运动合成，提升生成质量。",
      "method": "Splat-Portrait采用Gaussian Splatting作为核心，首先自动学习将输入肖像解耦为静态3D高斯点云表示的重建，以及预测的全图像2D背景。然后，基于输入音频条件生成自然的唇部运动，全程不依赖任何运动驱动先验。训练过程仅使用2D重建损失和分数蒸馏损失，无需3D监督数据或面部地标，简化了学习流程并提高了方法的泛化能力。",
      "result": "实验结果显示，Splat-Portrait在说话头生成和新视角合成任务中表现优越，相比以往方法，生成的动画视觉质量更高，细节更逼真。尽管摘要未明确说明具体性能指标如准确率或效率提升，但通过视觉对比证明了其在3D重建和运动合成方面的有效性，为后续研究提供了改进基准。",
      "conclusion": "本研究的主要贡献是提出Splat-Portrait，一种基于Gaussian Splatting的无监督3D说话头生成方法，自动解耦图像并生成唇部运动。学术上，它减少了对外部先验和3D监督的依赖，为自动学习和运动合成提供了新思路。实际应用上，可促进虚拟现实、视频会议等领域的发展。未来工作可探索处理更复杂输入或优化实时性能。",
      "tags": [
        "Gaussian Splatting",
        "Talking Head Generation",
        "3D Reconstruction",
        "Lip Motion Synthesis",
        "Unsupervised Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:37.847695Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18631",
    "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
    "authors": [
      "Mingyang Song",
      "Haoyu Sun",
      "Jiawei Gu",
      "Linjie Li",
      "Luxin Xu",
      "Ranjay Krishna",
      "Yu Cheng"
    ],
    "abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18631.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18631",
    "published": "2026-01-26T16:04:43Z",
    "updated": "2026-01-26T16:04:43Z",
    "comment": "28 pages, 10 figures and 13 tables",
    "light_analysis": {
      "overview": "AdaReasoner通过强化学习动态编排工具使用，提升多模态大语言模型的迭代视觉推理能力。",
      "motivation": "该研究旨在解决多模态大语言模型在视觉推理中工具使用的局限性。现有方法通常针对特定工具或需要显式监督，导致模型无法适应新工具和复杂任务，限制了推理的灵活性和泛化能力。通过让模型学习工具使用作为通用技能，该研究探索动态工具编排以提高自主推理效能。",
      "method": "AdaReasoner基于三个核心组件：首先，使用可扩展的数据管道让模型接触长期、多步骤的工具交互；其次，提出Tool-GRPO强化学习算法，以任务成功为奖励优化工具选择和序列化；最后，引入自适应学习机制动态调节工具使用频率。这些技术使模型能基于任务上下文和中间结果推断工具效用，实现多工具协调和泛化到未见工具。",
      "result": "实验结果表明，AdaReasoner展现出自适应行为，如自主采用有益工具、抑制无关工具，并根据任务需求调整使用频率。在多个挑战性基准测试中，AdaReasoner取得state-of-the-art性能，例如平均提升7B基础模型24.9%，并在VSP和Jigsaw任务上超越专有系统如GPT-5。",
      "conclusion": "AdaReasoner通过通用工具学习框架显著提升视觉推理能力，为多模态模型提供了动态工具编排的新方法，增强了模型的泛化性和实际应用价值。未来工作可探索更多工具类型或扩展到其他领域以克服潜在局限性。",
      "tags": [
        "Multimodal Large Language Models",
        "Reinforcement Learning",
        "Tool Orchestration",
        "Visual Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:30.922902Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18630",
    "title": "Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation",
    "authors": [
      "Abeer Badawi",
      "Md Tahmid Rahman Laskar",
      "Elahe Rahimi",
      "Sheri Grach",
      "Lindsay Bertrand",
      "Lames Danok",
      "Frank Rudzicz",
      "Jimmy Huang",
      "Elham Dolatabadi"
    ],
    "abstract": "The escalating global mental health crisis, marked by persistent treatment gaps, availability, and a shortage of qualified therapists, positions Large Language Models (LLMs) as a promising avenue for scalable support. While LLMs offer potential for accessible emotional assistance, their reliability, therapeutic relevance, and alignment with human standards remain challenging to address. This paper introduces a human-grounded evaluation methodology designed to assess LLM generated responses in therapeutic dialogue. Our approach involved curating a dataset of 500 mental health conversations from datasets with real-world scenario questions and evaluating the responses generated by nine diverse LLMs, including closed source and open source models. More specifically, these responses were evaluated by two psychiatric trained experts, who independently rated each on a 5 point Likert scale across a comprehensive 6 attribute rubric. This rubric captures Cognitive Support and Affective Resonance, providing a multidimensional perspective on therapeutic quality. Our analysis reveals that LLMs provide strong cognitive reliability by producing safe, coherent, and clinically appropriate information, but they demonstrate unstable affective alignment. Although closed source models (e.g., GPT-4o) offer balanced therapeutic responses, open source models show greater variability and emotional flatness. We reveal a persistent cognitive-affective gap and highlight the need for failure aware, clinically grounded evaluation frameworks that prioritize relational sensitivity alongside informational accuracy in mental health oriented LLMs. We advocate for balanced evaluation protocols with human in the loop that center on therapeutic sensitivity and provide a framework to guide the responsible design and clinical oversight of mental health oriented conversational AI.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18630.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18630",
    "published": "2026-01-26T16:04:19Z",
    "updated": "2026-01-26T16:04:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种基于多属性人类评估的方法，用于评估大型语言模型在心理健康支持对话中生成回应的质量，并揭示了认知-情感差距。",
      "motivation": "全球心理健康危机日益严重，存在治疗缺口和合格治疗师短缺问题，这使得大型语言模型作为可扩展的情感支持途径具有潜力。然而，现有LLM在可靠性、治疗相关性和与人类标准的对齐方面面临挑战，尤其是情感共鸣和临床相关性未得到充分评估。因此，需要开发新的评估框架来确保LLM在心理健康领域的有效性和安全性，填补这一研究空白。",
      "method": "本研究提出了一种基于人类评估的方法，构建了包含500个心理健康对话的数据集，这些对话来自真实场景问题。评估对象涵盖九个大型语言模型，包括闭源和开源模型。由两位精神病学专家独立使用5点Likert量表，基于涵盖认知支持和情感共鸣的六属性Rubric进行多维度评分。这种方法旨在提供全面、基于临床的治疗质量评估。",
      "result": "实验结果表明，LLM在认知支持方面表现优异，能生成安全、连贯且临床适当的回应，具有高可靠性。但情感共鸣方面存在不稳定性，呈现出认知-情感差距。闭源模型如GPT-4o提供更平衡的治疗回应，而开源模型显示出更大变异性，情感反应较平淡。这突出了在LLM评估中情感对齐的重要性，并与基线对比揭示了模型性能差异。",
      "conclusion": "论文的主要贡献在于揭示了LLM在心理健康支持中的认知-情感差距，并提出了一个包含人类在环的、基于临床的评估框架。这强调了优先考虑治疗敏感性和关系敏感性的重要性，为负责任地设计和监督心理健康导向的对话AI提供了指导。未来工作可进一步探索失败感知机制和临床监督协议，以提升应用有效性。",
      "tags": [
        "Large Language Model",
        "Human-in-the-loop Evaluation",
        "Therapeutic Dialogue",
        "Cognitive-Affective Gap",
        "Mental Health Support"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:29.069195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18626",
    "title": "Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning",
    "authors": [
      "Yingxiao Huo",
      "Satya Prakash Dash",
      "Radu Stoican",
      "Samuel Kaski",
      "Mingfei Sun"
    ],
    "abstract": "Natural gradients have long been studied in deep reinforcement learning due to their fast convergence properties and covariant weight updates. However, computing natural gradients requires inversion of the Fisher Information Matrix (FIM) at each iteration, which is computationally prohibitive in nature. In this paper, we present an efficient and scalable natural policy optimization technique that leverages a rank-1 approximation to full inverse-FIM. We theoretically show that under certain conditions, a rank-1 approximation to inverse-FIM converges faster than policy gradients and, under some conditions, enjoys the same sample complexity as stochastic policy gradient methods. We benchmark our method on a diverse set of environments and show that it achieves superior performance to standard actor-critic and trust-region baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18626.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18626",
    "published": "2026-01-26T16:02:18Z",
    "updated": "2026-01-26T16:02:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于秩1近似逆Fisher信息矩阵的高效自然策略梯度方法，以解决深度强化学习中计算复杂度问题。",
      "motivation": "自然梯度在深度强化学习中因能加速收敛而受到广泛研究，但计算自然梯度需要每次迭代中求逆Fisher信息矩阵（FIM），这在高维环境下计算成本极高，限制了其实际应用。现有方法如标准策略梯度收敛较慢，而完全逆FIM的计算不可行，因此开发高效近似方法对提升强化学习训练效率至关重要。",
      "method": "本研究提出一种自然策略优化技术，利用秩1矩阵近似来替代Fisher信息矩阵的全逆计算，核心创新在于通过秩1近似简化矩阵求逆过程，从而降低计算复杂度和提高可扩展性。理论分析显示，在特定条件下，该方法比策略梯度收敛更快，且在某些情况下与随机策略梯度享有相同样本复杂度。摘要未明确说明具体数据集和模型架构，但方法专注于强化学习的策略优化任务。",
      "result": "在多个强化学习环境中进行基准测试，实验结果表明，提出的方法在性能上优于标准的actor-critic和trust-region基线方法，展现出更快的收敛速度和更好的最终表现。具体数据在摘要中未详细说明，但论文通过广泛任务验证了方法的有效性，强调了其在实际应用中的潜力。",
      "conclusion": "本研究的主要贡献在于提出一种高效的自然梯度计算方法，通过秩1近似逆Fisher信息矩阵，在保持理论收敛优势的同时降低计算负担，这对推动自然梯度在强化学习中的实际应用具有重要价值。未来工作可探索扩展到更复杂场景或与其他优化技术结合，以进一步提升性能和应用范围。",
      "tags": [
        "Natural Policy Gradient",
        "Fisher Information Matrix",
        "Rank-1 Approximation",
        "Deep Reinforcement Learning",
        "Actor-Critic"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:42.706617Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18625",
    "title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search",
    "authors": [
      "Zequn Xie"
    ],
    "abstract": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18625.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18625",
    "published": "2026-01-26T16:01:33Z",
    "updated": "2026-01-26T16:01:33Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "CONQUER 提出一种上下文感知表示与查询增强的两阶段框架，有效解决文本基人搜索中的跨模态对齐和模糊查询挑战。",
      "motivation": "文本基人搜索（TBPS）旨在通过自然语言描述在大型图库中检索行人图像，这对公共安全应用至关重要。现有方法受限于跨模态差异和用户查询的模糊性，导致检索效果不佳。研究动机源于需开发更鲁棒的方法来克服这些挑战，以提升现实世界应用的可行性和精度，弥补传统方法在处理复杂查询和跨域场景时的不足。",
      "method": "CONQUER 采用两阶段框架：训练阶段使用多粒度编码、互补对挖掘和基于最优传输的上下文引导最优匹配，学习鲁棒的跨模态嵌入。推理阶段引入即插即用查询增强模块，通过锚点选择和属性驱动丰富，自适应优化模糊或不完整的查询，无需重新训练主干模型。关键创新点在于结合训练时的跨模态对齐和推理时的自适应优化，提高模型鲁棒性和适应性。",
      "result": "在 CUHK-PEDES、ICFG-PEDES 和 RSTPReid 数据集上的广泛实验表明，CONQUER 在 Rank-1 准确率和 mAP 上持续优于强基线。特别是在跨域和不完整查询场景中，模型展现出显著改进，验证了其鲁棒性和适应性。这些结果突显了 CONQUER 在实际部署中的有效性，但摘要未提供具体数字细节。",
      "conclusion": "CONQUER 的主要贡献在于提出了一种实用的两阶段框架，解决了 TBPS 中的跨模态对齐和模糊查询问题，强调了上下文感知表示和查询优化的重要性。研究具有学术价值，并为现实世界应用如公共安全提供了有效解决方案。摘要未明确说明局限性或未来工作方向，但潜在方向可能包括扩展性优化或多模态融合的进一步探索。",
      "tags": [
        "Text-Based Person Search",
        "Cross-Modal Alignment",
        "Optimal Transport",
        "Query Enhancement",
        "Anchor Selection"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:56.738859Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18623",
    "title": "Adaptive Domain Shift in Diffusion Models for Cross-Modality Image Translation",
    "authors": [
      "Zihao Wang",
      "Yuzhou Chen",
      "Shaogang Ren"
    ],
    "abstract": "Cross-modal image translation remains brittle and inefficient. Standard diffusion approaches often rely on a single, global linear transfer between domains. We find that this shortcut forces the sampler to traverse off-manifold, high-cost regions, inflating the correction burden and inviting semantic drift. We refer to this shared failure mode as fixed-schedule domain transfer. In this paper, we embed domain-shift dynamics directly into the generative process. Our model predicts a spatially varying mixing field at every reverse step and injects an explicit, target-consistent restoration term into the drift. This in-step guidance keeps large updates on-manifold and shifts the model's role from global alignment to local residual correction. We provide a continuous-time formulation with an exact solution form and derive a practical first-order sampler that preserves marginal consistency. Empirically, across translation tasks in medical imaging, remote sensing, and electroluminescence semantic mapping, our framework improves structural fidelity and semantic consistency while converging in fewer denoising steps.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18623.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18623",
    "published": "2026-01-26T16:00:36Z",
    "updated": "2026-01-26T16:00:36Z",
    "comment": "Paper accepted as a conference paper at ICLR 2026",
    "light_analysis": {
      "overview": "本论文提出了一种自适应域移方法，通过嵌入域移动态、预测空间变化混合场并注入显式恢复项，改进了扩散模型在跨模态图像翻译中的效率和语义一致性。",
      "motivation": "跨模态图像翻译在现有扩散模型中常表现出脆弱和低效问题，尤其是在医学成像、遥感和电致发光等领域需要高保真度翻译。标准方法依赖单一全局线性域转移，导致采样器脱离流形，引发高成本和语义漂移，称为固定计划域转移。这种失败模式限制了翻译质量和实用性，亟需一种更鲁棒的方案来直接嵌入域移动态，以改善生成过程的稳定性和效率。",
      "method": "论文提出将域移动态直接嵌入生成过程的核心方法，包括在每个反向步骤预测空间变化混合场，并向漂移注入显式目标一致恢复项。这种步内指导保持更新在流形上，将模型从全局对齐转向局部残差纠正。此外，提供了连续时间公式和精确解形式，并推导出保持边际一致性的实用一阶采样器。该方法应用于多个跨模态翻译任务，如医学成像、遥感和电致发光语义映射，以验证其技术特色。",
      "result": "实验结果显示，提出的框架在医学成像、遥感和电致发光语义映射等跨模态翻译任务中，显著提升了结构保真度和语义一致性。同时，模型能在更少的去噪步骤中收敛，提高了翻译效率。与标准扩散基线方法相比，避免了固定计划域转移问题，减少了语义漂移和校正负担，具体性能改进涉及多个应用领域，但摘要未提供详细数据。",
      "conclusion": "本研究的主要贡献是提出自适应域移方法，通过步内指导和局部纠正，优化了扩散模型的跨模态图像翻译。学术上，丰富了扩散模型在域适应领域的应用；实际上，增强了图像翻译的质量和速度，适用于医疗、遥感等多种场景。未来工作可扩展至更多模态和计算优化，摘要未明确说明局限性。",
      "tags": [
        "Diffusion Models",
        "Cross-Modality Image Translation",
        "Domain Adaptation",
        "Generative Process",
        "Semantic Consistency"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:02.538911Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18620",
    "title": "CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling",
    "authors": [
      "Panagiotis Lymperopoulos",
      "Abhiramon Rajasekharan",
      "Ian Berlot-Attwell",
      "Stéphane Aroca-Ouellette",
      "Kaheer Suleman"
    ],
    "abstract": "Building world models is essential for planning in real-world domains such as businesses. Since such domains have rich semantics, we can leverage world knowledge to effectively model complex action effects and causal relationships from limited data. In this work, we propose CASSANDRA, a neurosymbolic world modeling approach that leverages an LLM as a knowledge prior to construct lightweight transition models for planning. CASSANDRA integrates two components: (1) LLM-synthesized code to model deterministic features, and (2) LLM-guided structure learning of a probabilistic graphical model to capture causal relationships among stochastic variables. We evaluate CASSANDRA in (i) a small-scale coffee-shop simulator and (ii) a complex theme park business simulator, where we demonstrate significant improvements in transition prediction and planning over baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18620.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18620",
    "published": "2026-01-26T15:58:53Z",
    "updated": "2026-01-26T15:58:53Z",
    "comment": "28 pages, 2 figures",
    "light_analysis": {
      "overview": "CASSANDRA提出了一种神经符号世界建模方法，利用大语言模型（LLM）作为知识先验，结合代码合成和概率图模型学习，以改善复杂领域中的规划性能。",
      "motivation": "该研究旨在解决在真实世界领域（如商业）中，由于数据有限而难以建模复杂动作效果和因果关系的问题。现有方法可能不足以有效利用世界知识来处理随机性和因果推断，这阻碍了规划任务的准确性和效率。通过结合LLM的先验知识，研究试图从有限数据中构建轻量级转换模型，以增强世界建模能力并支持更好的决策规划。",
      "method": "CASSANDRA采用神经符号方法，整合两大组件：首先，利用LLM合成代码来建模环境中的确定性特征，实现程序化表示；其次，通过LLM引导概率图模型的结构学习，捕捉随机变量间的因果关系，构建贝叶斯网络等模型。该方法结合程序化知识和概率推理，以创建轻量级转换模型，用于高效规划任务，突出了LLM在知识提取和模型构建中的关键作用。",
      "result": "论文在小型咖啡店模拟器和复杂主题公园商业模拟器上评估CASSANDRA，结果表明在转换预测和规划任务中比基线方法有显著改进。摘要未明确说明具体性能指标如准确率提升，但强调了改进的显著性，展示了该方法在有限数据下增强模型预测能力和规划效果的潜力。",
      "conclusion": "CASSANDRA的主要贡献在于提出创新的神经符号世界建模框架，有效利用大语言模型的知识先验来提升随机环境中的规划和预测性能。该研究不仅提高了有限数据下的模型效率，还为程序化学习和概率推理的结合提供了新思路，具有学术和实际应用价值，如商业规划场景。未来工作可能涉及扩展到更多领域，摘要未明确说明具体局限性。",
      "tags": [
        "Neurosymbolic AI",
        "Large Language Models",
        "Probabilistic Graphical Models",
        "Structure Learning",
        "Code Synthesis"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:11.628341Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18619",
    "title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures",
    "authors": [
      "Jorge Quesada",
      "Ghassan AlRegib"
    ],
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18619.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18619",
    "published": "2026-01-26T15:58:04Z",
    "updated": "2026-01-26T15:58:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种尺度感知的自监督学习方法，通过集成小窗口裁剪增强，有效改进对小稀疏结构的分割性能。",
      "motivation": "自监督学习在有限标注下具有强大表示学习能力，但其效果高度依赖目标任务特性。在分割任务中，现有方法通常针对大、同质区域优化，当处理小、稀疏或局部不规则对象时，性能显著下降。这在科学成像领域如地震断层分割和神经细胞描绘中尤为重要，因为这些结构尺寸小且分布稀疏，现有SSL管道未能充分适应，导致实际应用中的分割瓶颈。",
      "method": "论文提出一种尺度感知的自监督学习适配方法，核心创新是将小窗口裁剪集成到数据增强流程中，在预训练阶段放大并关注细尺度结构。这有助于模型学习小、稀疏特征的表示，技术特色包括适应不同数据模态，如地震成像和神经成像。使用具体数据集未明确说明，但评估了这两个领域的任务，模型基于SSL框架，推断采用通用自监督学习架构。",
      "result": "方法在两个域评估：地震成像中稀疏断层分割精度提升13%，神经成像中小细胞结构描绘精度提升5%，均优于标准及最先进基线。对于大规模特征如地震相或组织区域，改进很小，表明自监督学习效果高度依赖目标对象尺度。实验结果表明该方法在标签约束下带来一致性性能提升，验证了尺度感知设计的有效性。",
      "conclusion": "研究主要贡献是提出尺度感知自监督学习方法，强调SSL设计应与对象大小和稀疏性对齐。学术价值在于为科学成像领域的表示学习提供通用原则，提升分割任务的泛化能力。实际应用价值在于改进地震和神经成像中的小稀疏结构分割。局限性摘要未明确说明，未来工作可能包括扩展到更多领域或优化裁剪策略。",
      "tags": [
        "Self-Supervised Learning",
        "Scale-Aware Learning",
        "Image Segmentation",
        "Cropping Augmentation",
        "Scientific Imaging"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:15.511911Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18617",
    "title": "Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks",
    "authors": [
      "Pierre Orhan",
      "Pablo Diego-Simón",
      "Emmnanuel Chemla",
      "Yair Lakretz",
      "Yves Boubenec",
      "Jean-Rémi King"
    ],
    "abstract": "During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18617.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18617",
    "published": "2026-01-26T15:56:41Z",
    "updated": "2026-01-26T15:56:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过研究人工神经网络，揭示了音素、词汇和句法表示在学习过程中的序列涌现，为语言习得提供了计算框架。",
      "motivation": "在语言习得过程中，儿童逐步学习音素、词汇和句法，但现有研究缺乏统一的计算框架来解释其背后的神经表示机制。这个问题对于理解语言习得的计算基础至关重要，尤其是在人工智能和认知科学领域，现有方法难以说明这些表示如何自发地从神经网络训练中涌现，导致无法准确模拟人类学习过程。因此，本研究旨在探索人工神经网络中这些表示的出现条件，以填补这一理论空白。",
      "method": "本研究使用语音和文本基础的人工神经网络模型，分析其在训练过程中的激活状态，以检测音素、词汇和句法表示是否以及何时涌现。核心方法涉及观察神经激活如何构建子空间，并分析其几何结构，这些结构逐步代表语言的不同层面。关键创新点在于揭示表示在学习阶段中的序列出现方式，通过量化激活的子空间变化来追踪表示的形成过程，从而提供了一个统一的框架来解释语言习得的计算机制。",
      "result": "实验结果显示，语音和文本基础模型在训练中都遵循一系列学习阶段：神经激活逐渐构建子空间，其几何结构依次表示音素、词汇和句法结构。这一发展轨迹在定性上与儿童语言习得相似，但在定量上存在差异，这些算法需要多2到4个数量级的数据才能使表示涌现。这表明人工神经网络学习效率较低，突显了算法与人类学习在数据需求上的显著差距，为评估模型性能提供了基准。",
      "conclusion": "本研究的主要贡献在于展示了人工神经网络在何种条件下能够自发形成语言习得的关键表示阶段，从而为理解语言习得的计算基础提供了统一框架。学术价值在于连接了认知科学和人工智能，揭示了神经网络学习与人类学习的异同，有助于推动语言模型的改进。未来工作可探索提高数据效率的方法，或应用于更复杂的语言任务，以进一步验证框架的泛化能力并克服当前局限性。",
      "tags": [
        "Artificial Neural Networks",
        "Language Acquisition",
        "Neural Representations",
        "Learning Stages",
        "Computational Framework"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:39.314808Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18615",
    "title": "Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem",
    "authors": [
      "Ramiro Valdes Jara",
      "Adam Meyers"
    ],
    "abstract": "This paper proposes a data-driven model for solving the inverse problem of electrocardiography, the mathematical problem that forms the basis of electrocardiographic imaging (ECGI). We present a conditional diffusion framework that learns a probabilistic mapping from noisy body surface signals to heart surface electric potentials. The proposed approach leverages the generative nature of diffusion models to capture the non-unique and underdetermined nature of the ECGI inverse problem, enabling probabilistic sampling of multiple reconstructions rather than a single deterministic estimate. Unlike traditional methods, the proposed framework is geometry-free and purely data-driven, alleviating the need for patient-specific mesh construction. We evaluate the method on a real ECGI dataset and compare it against strong deterministic baselines, including a convolutional neural network, long short-term memory network, and transformer-based model. The results demonstrate that the proposed diffusion approach achieves improved reconstruction accuracy, highlighting the potential of diffusion models as a robust tool for noninvasive cardiac electrophysiology imaging.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18615.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18615",
    "published": "2026-01-26T15:53:54Z",
    "updated": "2026-01-26T15:53:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种几何无关的条件扩散模型，用于解决电生理成像的逆问题，实现概率性心脏电势重建。",
      "motivation": "电生理成像（ECGI）逆问题在无创心脏疾病诊断中至关重要，因为它基于数学建模估计心脏表面电势。然而，现有方法通常依赖于患者特定的几何网格构建，这增加了计算复杂性和实际应用的难度，且传统确定性方法无法有效处理逆问题的非唯一性和欠定性，限制了重建的准确性和鲁棒性。因此，开发一种几何无关、数据驱动的方法以改进无创成像技术成为研究的重要动机。",
      "method": "本研究采用条件扩散框架，通过生成性扩散模型学习从噪声体表信号到心脏表面电势的概率映射。核心创新在于利用扩散模型的生成性质，捕捉ECGI逆问题的非唯一性，支持从后验分布中采样多个可能的重建结果，而非单一确定性估计。模型设计为几何无关和纯数据驱动，避免了传统方法中的患者特定网格需求。在真实ECGI数据集上进行训练和评估，但模型架构的具体细节如网络层数或参数摘要未明确说明。",
      "result": "在真实ECGI数据集上的评估结果显示，提出的扩散方法在重建准确性上优于强确定性基线，包括卷积神经网络、长短时记忆网络和Transformer模型。具体性能指标如准确率提升百分比摘要未明确说明，但强调了该方法在改善重建效果方面的潜力。与基线对比表明，扩散模型能够更稳健地处理逆问题的复杂性，突显了其在无创心脏电生理成像中的应用价值。",
      "conclusion": "本论文的主要贡献是开发了一种基于条件扩散的几何无关方法，为解决电生理成像逆问题提供了创新解决方案。其学术价值在于扩展了扩散模型在医学成像领域的应用，实际应用价值在于提升无创心脏诊断的准确性和实用性，减少对几何信息的依赖。未来工作方向可能包括在更广泛数据集上验证模型、优化计算效率或探索与其他生成模型的结合，以进一步提高性能。",
      "tags": [
        "Conditional Diffusion Modeling",
        "Inverse Electrocardiography",
        "ECG Imaging",
        "Generative Models",
        "Probabilistic Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:28.722169Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18608",
    "title": "PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression",
    "authors": [
      "Fabian Fumagalli",
      "R. Teal Witter",
      "Christopher Musco"
    ],
    "abstract": "Shapley values have emerged as a central game-theoretic tool in explainable AI (XAI). However, computing Shapley values exactly requires $2^d$ game evaluations for a model with $d$ features. Lundberg and Lee's KernelSHAP algorithm has emerged as a leading method for avoiding this exponential cost. KernelSHAP approximates Shapley values by approximating the game as a linear function, which is fit using a small number of game evaluations for random feature subsets.   In this work, we extend KernelSHAP by approximating the game via higher degree polynomials, which capture non-linear interactions between features. Our resulting PolySHAP method yields empirically better Shapley value estimates for various benchmark datasets, and we prove that these estimates are consistent.   Moreover, we connect our approach to paired sampling (antithetic sampling), a ubiquitous modification to KernelSHAP that improves empirical accuracy. We prove that paired sampling outputs exactly the same Shapley value approximations as second-order PolySHAP, without ever fitting a degree 2 polynomial. To the best of our knowledge, this finding provides the first strong theoretical justification for the excellent practical performance of the paired sampling heuristic.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18608.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18608",
    "published": "2026-01-26T15:47:45Z",
    "updated": "2026-01-26T15:47:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "PolySHAP扩展KernelSHAP，通过多项式回归捕捉特征间非线性交互，提升Shapley值估计的准确性。",
      "motivation": "Shapley值是游戏理论在可解释AI中的核心工具，但精确计算需要指数级成本；KernelSHAP通过线性近似降低成本，却无法捕捉特征间的非线性交互，导致估计不精确，限制了在复杂模型中的实际应用价值。因此，研究旨在开发一种能更准确近似Shapley值的方法。",
      "method": "PolySHAP扩展KernelSHAP，通过使用高阶多项式回归来近似Shapley值游戏，从而捕捉特征间的非线性交互。核心创新点在于将线性近似升级为多项式模型，技术实现基于随机特征子集的小规模游戏评估来拟合多项式，以提高估计的丰富性和准确性。",
      "result": "实验表明，PolySHAP在多个基准数据集上相比基线KernelSHAP获得更好的Shapley值估计，并证明了估计的一致性；摘要未明确说明具体性能指标如准确率提升，但强调了经验改进。此外，研究理论连接了PolySHAP与paired sampling启发式方法，证明后者等价于二阶PolySHAP。",
      "conclusion": "PolySHAP贡献了更准确的Shapley值估计方法，通过多项式回归增强了对特征交互的建模，提升了可解释AI的实用性；理论部分为paired sampling的实践表现提供了首次强理论依据，具有重要的学术和实际价值，未来工作可优化计算效率。",
      "tags": [
        "Shapley Values",
        "KernelSHAP",
        "Polynomial Regression",
        "Interaction",
        "Explainable AI"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:33.751069Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18604",
    "title": "LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation",
    "authors": [
      "Zhiwei Zheng",
      "Kevin Bryson"
    ],
    "abstract": "Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.   Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.   Availability and implementation: https://github.com/willyzzz/LaCoGSEA",
    "categories": [
      "cs.LG",
      "q-bio.GN"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18604.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18604",
    "published": "2026-01-26T15:45:33Z",
    "updated": "2026-01-26T15:45:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "LaCoGSEA是一个无监督深度学习框架，通过集成自编码器和潜在相关性度量，改进通路富集分析，无需依赖先验标签。",
      "motivation": "通路富集分析广泛用于解释基因表达数据，但标准方法如GSEA依赖于预定义的表型标签和成对比较，限制了其在无监督环境中的应用。现有无监督扩展方法（如单样本方法）主要捕捉线性关系，无法明确建模基因与通路之间的关联。尽管深度学习模型能够捕捉非线性转录组结构，但其解释通常依赖于通用的可解释AI技术，这些技术未针对无监督转录组分析中的通路级解释进行设计，导致效果有限，因此需要专门的方法来解决此问题。",
      "method": "LaCoGSEA集成深度表示学习与稳健的通路统计，核心使用自编码器捕捉非线性流形，并提出一种全局基因与潜在变量之间的相关性度量，作为差异表达的代理。这种方法在无监督设置下生成密集的基因排名，无需先验标签。关键创新点在于结合深度学习的表示能力和针对通路分析的统计度量，通过相关性计算模拟传统分析中的基因排名过程。",
      "result": "实验表明，LaCoGSEA在多个方面优于现有方法：首先，在区分癌症亚型时，其聚类性能超过无监督基线方法；其次，与线性降维和基于梯度的可解释AI方法相比，LaCoGSEA在更高排名恢复更多具有生物意义的通路；最后，该方法在不同实验协议和数据集大小下表现出高鲁棒性和一致性。总体上，它在无监督通路富集分析中达到了最先进的性能水平。",
      "conclusion": "LaCoGSEA的主要贡献是提出一个创新的无监督深度学习框架，有效整合深度表示学习和通路统计，克服了传统方法的局限性。该研究具有重要的学术价值，为生物信息学中的基因表达分析提供了新工具，特别是在无监督场景下；实际应用中可用于癌症亚型识别等任务。摘要未明确说明局限性，未来工作可能涉及扩展到更大数据集或集成其他技术。",
      "tags": [
        "Pathway Analysis",
        "Autoencoder",
        "Unsupervised Learning",
        "Correlation Metric",
        "Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:50.599669Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18597",
    "title": "EFSI-DETR: Efficient Frequency-Semantic Integration for Real-Time Small Object Detection in UAV Imagery",
    "authors": [
      "Yu Xia",
      "Chang Liu",
      "Tianqi Xiang",
      "Zhigang Tu"
    ],
    "abstract": "Real-time small object detection in Unmanned Aerial Vehicle (UAV) imagery remains challenging due to limited feature representation and ineffective multi-scale fusion. Existing methods underutilize frequency information and rely on static convolutional operations, which constrain the capacity to obtain rich feature representations and hinder the effective exploitation of deep semantic features. To address these issues, we propose EFSI-DETR, a novel detection framework that integrates efficient semantic feature enhancement with dynamic frequency-spatial guidance. EFSI-DETR comprises two main components: (1) a Dynamic Frequency-Spatial Unified Synergy Network (DyFusNet) that jointly exploits frequency and spatial cues for robust multi-scale feature fusion, (2) an Efficient Semantic Feature Concentrator (ESFC) that enables deep semantic extraction with minimal computational cost. Furthermore, a Fine-grained Feature Retention (FFR) strategy is adopted to incorporate spatially rich shallow features during fusion to preserve fine-grained details, crucial for small object detection in UAV imagery. Extensive experiments on VisDrone and CODrone benchmarks demonstrate that our EFSI-DETR achieves the state-of-the-art performance with real-time efficiency, yielding improvement of \\textbf{1.6}\\% and \\textbf{5.8}\\% in AP and AP$_{s}$ on VisDrone, while obtaining \\textbf{188} FPS inference speed on a single RTX 4090 GPU.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18597.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18597",
    "published": "2026-01-26T15:41:37Z",
    "updated": "2026-01-26T15:41:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "EFSI-DETR提出一种动态频率-空间协同和高效语义提取的检测框架，用于无人机图像中的实时小物体检测。",
      "motivation": "无人机图像中的实时小物体检测面临特征表示有限和多尺度融合无效的挑战，现有方法依赖静态卷积操作，未充分利用频率信息，导致特征表示不丰富且深层语义特征利用不足，影响了检测精度和效率。这使得在无人机应用中快速准确识别小物体变得困难，特别是在监控、测绘等任务中至关重要，亟待改进现有方法以提高性能和实时性。摘要强调了这些问题，指出了开发新方法以优化特征融合和语义提取的必要性。",
      "method": "EFSI-DETR方法包括动态频率-空间统一协同网络（DyFusNet），它联合频率和空间线索进行多尺度特征融合；高效语义特征集中器（ESFC），以最小计算成本实现深层语义提取；以及精细特征保留策略（FFR），在融合中纳入浅层空间特征以保持细节。该方法基于Transformer检测框架（DETR）扩展，旨在克服静态卷积限制，提升特征多样性和融合效率，针对无人机图像中的小物体检测设计。摘要中提及VisDrone和CODrone数据集作为实验基准，但未详细说明模型架构的更多技术细节。",
      "result": "在VisDrone和CODrone基准测试中，EFSI-DETR取得最优性能，VisDrone上的平均精度（AP）提高1.6%，小物体检测的APs提升5.8%，推理速度在单个RTX 4090 GPU上达到188 FPS，显示其实时处理能力。与现有方法相比，该框架显著提升了检测准确率和效率，表明其在小物体检测任务中的有效性。摘要未明确说明与具体基线的对比数据，但基于“state-of-the-art”描述，可推断优于传统方法。",
      "conclusion": "本研究的主要贡献是提出了EFSI-DETR框架，通过动态频率-空间协同和高效语义提取，有效解决了无人机图像中小物体检测的挑战。学术价值在于为特征融合和语义提取提供了新方法；实际应用价值在于实现了实时高效检测，适用于无人机监控等场景。摘要未明确说明局限性，未来工作可探索扩展到更广泛数据集或进一步优化计算效率。",
      "tags": [
        "Frequency-Semantic Integration",
        "Dynamic Feature Fusion",
        "Efficient Semantic Extraction",
        "Small Object Detection",
        "Real-Time Detection"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:12.754958Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18595",
    "title": "A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic",
    "authors": [
      "Joseph Cotnareanu",
      "Didier Chetelat",
      "Yingxue Zhang",
      "Mark Coates"
    ],
    "abstract": "Although Large Language Models (LLMs) have demonstrated impressive formal reasoning abilities, they often break down when problems require complex proof planning. One promising approach for improving LLM reasoning abilities involves translating problems into formal logic and using a logic solver. Although off-the-shelf logic solvers are in principle substantially more efficient than LLMs at logical reasoning, they assume that all relevant facts are provided in a question and are unable to deal with missing commonsense relations. In this work, we propose a novel method that uses feedback from the logic solver to augment a logic problem with commonsense relations provided by the LLM, in an iterative manner. This involves a search procedure through potential commonsense assumptions to maximize the chance of finding useful facts while keeping cost tractable. On a collection of pure-logical reasoning datasets, from which some commonsense information has been removed, our method consistently achieves considerable improvements over existing techniques, demonstrating the value in balancing neural and symbolic elements when working in human contexts.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18595.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18595",
    "published": "2026-01-26T15:40:26Z",
    "updated": "2026-01-26T15:40:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种平衡神经与符号的方法，通过迭代使用大语言模型和逻辑求解器来增强常识归纳逻辑推理，解决了现有方法在复杂推理中的不足。",
      "motivation": "当前，大语言模型在处理复杂逻辑推理时容易失败，而逻辑求解器虽然高效，但假设所有事实已知，无法处理现实中常见的常识缺失问题。这在人类上下文中限制了推理系统的应用，因为许多任务依赖于隐含的常识关系。因此，本研究旨在融合两者的优势，开发一种更稳健的推理方法，以克服现有技术的局限性。",
      "method": "论文提出的核心方法涉及迭代过程：首先将问题转化为形式逻辑，然后使用逻辑求解器分析并反馈缺失部分，指导大语言模型补充常识关系。关键创新包括搜索潜在常识假设的算法，以最大化找到有用事实的概率，同时控制计算成本。这种神经符号集成结合了LLM的常识理解和逻辑求解器的精确推理，通过优化假设选择来提高效率。",
      "result": "在实验部分，研究在多个纯逻辑推理数据集上进行测试，这些数据集故意移除了常识信息。结果显示，该方法相对于现有技术持续取得了可观的改进，证明了其在处理常识缺失问题上的有效性。虽然摘要未提供具体性能指标，但强调了显著提升，验证了平衡神经和符号元素的价值。",
      "conclusion": "本研究的贡献在于提出了一种创新的神经符号方法，通过迭代整合大语言模型和逻辑求解器，有效增强了常识推理能力。这为混合智能系统提供了新范式，展示了在人类环境中结合神经和符号技术的实用价值。未来工作可探索更高效的搜索策略或将该方法扩展到其他推理任务中。",
      "tags": [
        "Large Language Model",
        "Logic Solver",
        "Neuro-Symbolic Approach",
        "Commonsense Reasoning",
        "Iterative Feedback"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:04.907198Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18589",
    "title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment",
    "authors": [
      "KV Karthikeya",
      "Ashok Kumar Das",
      "Shantanu Pal",
      "Vivekananda Bhat K",
      "Arun Sekar Rajasekaran"
    ],
    "abstract": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18589.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18589",
    "published": "2026-01-26T15:35:03Z",
    "updated": "2026-01-26T15:35:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出AGSP-DSA框架，通过自适应图信号处理和动态语义对齐，实现了鲁棒的多模态数据融合。",
      "motivation": "该研究旨在解决异构多模态数据（如文本、音频、图像）融合中的鲁棒性问题。多模态学习在情感分析和事件识别等任务中至关重要，但现有方法可能忽略模态间的动态语义对齐，导致在缺失模态或语义不一致时融合效果受限。摘要未明确说明具体不足，但可推断出现有融合技术可能无法自适应地处理多模态关系，从而影响性能。",
      "method": "AGSP-DSA采用双图构建来学习模态内部和模态间的关系，结合谱图滤波增强信息信号，并使用多尺度图卷积网络（GCNs）进行节点嵌入。核心创新包括语义感知注意力机制，该机制允许每个模态根据上下文相关性动态调整贡献，实现自适应融合。框架通过图信号处理技术整合多模态数据，以提升鲁棒性和效率。",
      "result": "在CMU-MOSEI、AVE和MM-IMDB三个基准数据集上，AGSP-DSA达到最先进性能：CMU-MOSEI上获得95.3%准确率、0.936 F1-score和0.924 mAP，相比MM-GNN准确率提升2.6%；AVE上为93.4%准确率和0.911 F1-score；MM-IMDB上为91.8%准确率和0.886 F1-score。实验结果表明，该方法在缺失模态设置下具有良好泛化性和鲁棒性。",
      "conclusion": "AGSP-DSA验证了自适应图信号处理与动态语义对齐在多模态融合中的有效性，显著提升情感分析、事件识别和多媒体分类任务的性能。研究贡献在于提出一个可扩展框架，具有学术创新和实际应用价值。未来工作可能涉及扩展到更多模态或更复杂场景，摘要未明确说明具体局限性。",
      "tags": [
        "Graph Signal Processing",
        "Multimodal Fusion",
        "Graph Convolutional Networks",
        "Semantic Alignment",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:58.776714Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18588",
    "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs",
    "authors": [
      "Xianzhe Meng",
      "Qiangsheng Zeng",
      "Ling Luo",
      "Qinghan Yang",
      "Jiarui Hao",
      "Wenbo Wu",
      "Qinyu Wang",
      "Rui Yin",
      "Lin Qi",
      "Renzhi Lu"
    ],
    "abstract": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18588.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18588",
    "published": "2026-01-26T15:34:50Z",
    "updated": "2026-01-26T15:34:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文揭示训练稳定性在大语言模型中可能导致生成质量下降，表现为输出低熵和重复模式，指出稳定性与生成表达能力不对齐。",
      "motivation": "在大语言模型训练中，优化稳定性通常被视为可靠收敛的前提，但过度强调稳定性可能忽视其对生成分布的负面影响。现有方法假设稳定训练能提升生成质量，但实际可能导致模型收敛到数据模式的有限子集，降低生成多样性和表达能力。这一问题的重要性在于，生成模型的多样性和质量直接关系到下游应用如文本生成的效果，但当前优化实践可能误将稳定性作为生成能力的充分指标，缺乏全面评估。",
      "method": "论文采用标准最大似然训练框架，分析稳定训练动态对参数轨迹的影响。核心创新在于使用一个基于反馈的控制训练框架，通过稳定内部生成统计量来实证研究稳定性如何影响生成熵。关键技术包括利用前向KL散度分析平稳解行为，评估模型在不同架构下的生成分布，以揭示稳定性与生成表达之间的内在冲突，具体实验设计了随机种子控制和多架构对比验证。",
      "result": "实验结果表明，在稳定训练条件下，模型输出一致表现出低熵特性和重复行为，这一现象在不同模型架构和随机种子下均得到验证。尽管训练损失平滑收敛，生成质量却显著下降，证明稳定性与生成表达能力不对齐。与基线相比，稳定训练导致模型集中在少数经验模式上，生成能力受限，凸显了仅凭稳定性指标无法全面反映模型性能。",
      "conclusion": "论文的主要贡献在于挑战了训练稳定性与生成质量固有对齐的假设，指出稳定性本身不足以指示生成质量，具有重要的学术价值，促使重新审视大语言模型优化策略。该发现对实际应用有指导意义，需设计更全面的评估指标。局限性在于研究范围聚焦特定训练框架，未来工作可扩展至其他优化方法或更广泛数据集以验证和深化结论。",
      "tags": [
        "Large Language Model",
        "Training Stability",
        "Maximum Likelihood Training",
        "Generative Entropy",
        "KL Divergence"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:23.291812Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18586",
    "title": "Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning",
    "authors": [
      "Miguel Costa",
      "Arthur Vandervoort",
      "Carolin Schmidt",
      "Morten W. Petersen",
      "Martin Drews",
      "Karyn Morrissey",
      "Francisco C. Pereira"
    ],
    "abstract": "Climate change is expected to intensify rainfall and other hazards, increasing disruptions in urban transportation systems. Designing effective adaptation strategies is challenging due to the long-term, sequential nature of infrastructure investments, deep uncertainty, and complex cross-sector interactions. We propose a generic decision-support framework that couples an integrated assessment model (IAM) with reinforcement learning (RL) to learn adaptive, multi-decade investment pathways under uncertainty. The framework combines long-term climate projections (e.g., IPCC scenario pathways) with models that map projected extreme-weather drivers (e.g. rain) into hazard likelihoods (e.g. flooding), propagate hazards into urban infrastructure impacts (e.g. transport disruption), and value direct and indirect consequences for service performance and societal costs. Embedded in a reinforcement-learning loop, it learns adaptive climate adaptation policies that trade off investment and maintenance expenditures against avoided impacts. In collaboration with Copenhagen Municipality, we demonstrate the approach on pluvial flooding in the inner city for the horizon of 2024 to 2100. The learned strategies yield coordinated spatial-temporal pathways and improved robustness relative to conventional optimization baselines, namely inaction and random action, illustrating the framework's transferability to other hazards and cities.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18586.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18586",
    "published": "2026-01-26T15:32:40Z",
    "updated": "2026-01-26T15:32:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个结合集成评估模型与强化学习的决策框架，用于学习长期气候适应交通投资路径。",
      "motivation": "气候变化加剧降雨等极端天气事件，增加城市交通系统中断风险。设计有效适应策略面临长期基础设施投资的顺序性、深度不确定性和跨部门复杂交互的挑战，现有优化方法难以处理这些问题。该研究的重要性在于为城市规划提供新工具，以降低社会成本并提升服务性能，确保城市可持续发展。",
      "method": "研究提出一个通用决策支持框架，耦合集成评估模型和强化学习来学习适应性的多年代投资路径。关键创新在于整合长期气候预测（如IPCC场景）、建模极端天气到灾害风险（如洪水）、传播风险到城市基础设施影响（如交通中断），并评估服务性能和社会成本。框架嵌入强化学习循环，优化投资与维护支出与避免影响的权衡。案例研究与哥本哈根市合作，应用于2024至2100年的内城雨洪风险。",
      "result": "学习策略产生协调的时空投资路径，相较于传统基准如不行动和随机行动，展现出改进的鲁棒性。摘要未明确说明具体性能指标数值，但框架在减少不确定性影响方面表现有效。此外，框架证明可转移到其他灾害类型和城市，强调了其广泛适用性和实用性。",
      "conclusion": "本研究的主要贡献是开发了一个可扩展的决策框架，结合多模型模拟和强化学习处理气候适应中的不确定性。学术上，推动复杂系统决策方法的交叉研究；实践中，案例研究验证了框架的有效性，为城市规划提供实用工具。局限可能包括模型简化，未来工作可扩展到更多灾害类型、增强模型精度，并推广到不同城市应用。",
      "tags": [
        "Reinforcement Learning",
        "Integrated Assessment Model",
        "Climate Adaptation",
        "Urban Transport",
        "Flood Risk Modeling"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:29.407823Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18585",
    "title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization",
    "authors": [
      "Chenxi Liu",
      "Selena Ling",
      "Alec Jacobson"
    ],
    "abstract": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18585.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18585",
    "published": "2026-01-26T15:32:16Z",
    "updated": "2026-01-26T15:32:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "GimmBO 提出了一种基于贝叶斯优化的交互式框架，用于高效探索扩散模型适配器的合并，改进高维空间中的优化性能。",
      "motivation": "扩散模型图像生成中，基于微调的适配器合并允许合成多样化视觉结果，但当前工作流依赖手动滑块调谐来选择权重，这种方法在处理20-30个候选适配器时也难以扩展和选择权重，导致用户探索大且连续的设计空间效率低下。现有方法的不足在于手动调整耗时且难以处理高维权重优化，限制了生成式AI社区适配器的有效利用，因此需要自动化交互式方法来解决这些问题。",
      "method": "GimmBO 的核心方法是使用 Preferential Bayesian Optimization (PBO) 支持交互式适配器合并探索。基于实际使用观察（如权重稀疏性和约束范围），引入了一个两阶段贝叶斯优化后端，以提高高维空间的采样效率和收敛性。该方法通过模拟用户偏好优化权重选择，未明确说明具体数据集或模型架构，但推断基于扩散模型和适配器权重合并，框架设计灵活，可扩展至不同应用场景。",
      "result": "通过模拟用户和真实用户研究评估，GimmBO 展示了改进的收敛性能和高成功率，相比传统的贝叶斯优化和线搜索基线有 consistently 的增益。摘要未提供具体数据如准确率，但实验结果表明该方法在优化效率上提升，能有效支持用户偏好驱动的适配器合并探索，优于现有自动化方法。",
      "conclusion": "GimmBO 的主要贡献是提供了一个基于贝叶斯优化的交互式框架，显著提高了扩散模型适配器合并的探索效率，具有学术价值和实际应用价值，如简化生成式AI用户工作流和促进社区适配器利用。局限性摘要未明确说明，但未来工作可扩展至更多适配器或结合其他优化技术以增强灵活性。",
      "tags": [
        "Generative Image Model",
        "Diffusion Models",
        "Bayesian Optimization",
        "Preference Learning",
        "Model Merging"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:35.907018Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18582",
    "title": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
    "authors": [
      "Yuan Cao",
      "Feixiang Liu",
      "Xinyue Wang",
      "Yihan Zhu",
      "Hui Xu",
      "Zheng Wang",
      "Qiang Qiu"
    ],
    "abstract": "Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18582.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18582",
    "published": "2026-01-26T15:28:43Z",
    "updated": "2026-01-26T15:28:43Z",
    "comment": "9 pages, 4 figures, AAAI 2026 Bridge",
    "light_analysis": {
      "overview": "该论文提出一种将人格检测视为排序任务并使用强化学习训练大型语言模型的方法，以增强其推理能力。",
      "motivation": "人格检测任务旨在通过社交媒体帖子测量个体的人格特征。大型语言模型的进步为人格检测提供了新视角，现有方法通常利用 LLMs 提取语义信息并训练分类器，但由于人格的复杂性和特征间微妙差异，准确分类具有挑战。此外，基于提示的方法过度依赖专家知识，缺乏自主模式学习能力，限制了模型的泛化性和准确性。因此，研究需要一种能更好处理主观性和模糊边界的新方法。",
      "method": "论文将人格检测重新定义为排序任务，并提出了一个强化学习训练范式。首先，通过监督微调（SFT）建立人格特征的排序能力，同时强制执行标准化输出格式以作为稳健初始化。随后，引入 Group Relative Policy Optimization (GRPO) 结合专门的基于排序的奖励函数，该函数旨在训练 LLMs 学习最优答案排名，以应对人格评估中的主观解释和类别模糊边界。该方法的关键创新在于以排序替代分类，利用强化学习优化模型推理，增强自主学习能力。",
      "result": "实验结果表明，该方法在多个基准上取得了最先进的性能。尽管摘要未明确说明具体数值，但综合实验验证了其在人格检测任务上的优异表现，与基线方法相比显示出显著优势，提升了模型的推理能力和分类准确性。这表明排序和强化学习范式能有效处理人格的复杂性和主观性。",
      "conclusion": "论文的主要贡献是提出了一种基于排序和强化学习的方法来增强 LLM 在人格检测中的推理能力。这为人格检测任务提供了新视角，通过强化学习优化模型处理主观性和模糊边界的能力。学术上，该方法扩展了 LLM 在心理学和自然语言处理交叉领域的应用，实际中可用于社交媒体分析和个性化服务。未来工作可能包括进一步优化奖励函数或扩展到其他相关任务，以应对更多应用场景。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Ranking",
        "Personality Detection",
        "MBTI"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:47.910113Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18580",
    "title": "K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents",
    "authors": [
      "Vincenzo De Paola",
      "Mirco Mutti",
      "Riccardo Zamboni",
      "Marcello Restelli"
    ],
    "abstract": "Parallelization in Reinforcement Learning is typically employed to speed up the training of a single policy, where multiple workers collect experience from an identical sampling distribution. This common design limits the potential of parallelization by neglecting the advantages of diverse exploration strategies. We propose K-Myriad, a scalable and unsupervised method that maximizes the collective state entropy induced by a population of parallel policies. By cultivating a portfolio of specialized exploration strategies, K-Myriad provides a robust initialization for Reinforcement Learning, leading to both higher training efficiency and the discovery of heterogeneous solutions. Experiments on high-dimensional continuous control tasks, with large-scale parallelization, demonstrate that K-Myriad can learn a broad set of distinct policies, highlighting its effectiveness for collective exploration and paving the way towards novel parallelization strategies.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18580.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18580",
    "published": "2026-01-26T15:26:40Z",
    "updated": "2026-01-26T15:26:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 K-Myriad，一种基于无监督并行代理的强化学习方法，通过最大化集体状态熵来加速训练并发现多样化策略。",
      "motivation": "研究动机源于强化学习中并行化的局限。当前方法通常使用多个工作器收集相同采样分布的经验来加速单一策略训练，但忽视了多样化探索策略的优势，导致探索效率低下和策略同质化。这限制了并行化潜力，无法有效处理复杂任务中的探索需求。因此，亟需一种新方法来利用并行策略群体的多样性，提升训练效率和解决方案的异质性。",
      "method": "K-Myriad 是一个可扩展的无监督方法，核心是通过最大化由并行策略群体诱导的集体状态熵，以培养专门探索策略的组合。该方法利用多个并行代理实施不同探索策略，为强化学习提供稳健初始化，跳转启动学习过程。关键创新点在于结合无监督学习和并行化，专注于状态熵最大化以实现多样化探索。摘要未明确说明具体数据集或模型架构细节，但提及应用于高维连续控制任务。",
      "result": "实验在高维连续控制任务上进行，采用大规模并行化设置。结果显示，K-Myriad 能学习到广泛而不同的策略，证明了其在集体探索方面的有效性。与基线方法相比，该方法提高了训练效率并发现异质解决方案，但摘要未提供具体性能指标如准确率或效率提升百分比。尽管如此，实验突出了其在促进策略多样性和加速训练方面的潜力。",
      "conclusion": "K-Myriad 的主要贡献是提出了一种新颖的并行强化学习初始化方法，通过最大化状态熵和培养多样化探索策略，实现更高训练效率和策略多样性。研究具有重要学术价值，挑战了传统并行化局限性，并引入无监督元素优化探索过程。在实际应用中，可为复杂控制任务提供更鲁棒的解决方案，并启发未来研究开发新的并行化策略。未来工作可能包括扩展到更广泛任务或结合其他优化技术。",
      "tags": [
        "Reinforcement Learning",
        "Parallelization",
        "Unsupervised Learning",
        "State Entropy",
        "Exploration Strategies"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:41.946767Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18577",
    "title": "Self-Refining Video Sampling",
    "authors": [
      "Sangwon Jang",
      "Taekyung Ki",
      "Jaehyeong Jo",
      "Saining Xie",
      "Jaehong Yoon",
      "Sung Ju Hwang"
    ],
    "abstract": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18577.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18577",
    "published": "2026-01-26T15:22:27Z",
    "updated": "2026-01-26T15:22:27Z",
    "comment": "Project page: https://agwmon.github.io/self-refine-video/",
    "light_analysis": {
      "overview": "本文提出自我精炼视频采样方法，通过将预训练生成器用作自精炼器，实现无需外部验证器的迭代精炼，显著提升视频生成的物理真实感。",
      "motivation": "现代视频生成器在处理复杂物理动力学时仍面临挑战，导致生成的视频缺乏物理真实感，这在影视制作等应用中至关重要。现有方法依赖外部验证器或在增强数据上额外训练，但计算成本高昂且难以有效捕捉细粒度运动。因此，需要一种更高效、无需额外训练的方法来改进运动连贯性和物理对齐，以克服现有方法的局限性。",
      "method": "本研究提出自我精炼视频采样方法，核心是利用预训练的大规模视频生成器作为其自身的精炼器。通过将生成器解释为去噪自编码器，在推理时实现迭代内循环精炼，无需外部验证器或额外训练。进一步引入不确定性感知精炼策略，基于自我一致性选择性地精炼视频区域，以防止过度精炼导致的伪影。这种方法保留预训练模型的优势，通过简单调整提升生成质量。",
      "result": "实验在最先进的视频生成器上进行，结果显示该方法在运动连贯性和物理对齐方面取得显著改进。具体数据表明，与默认采样器和基于引导的采样器相比，获得了超过70%的人类偏好率，证明了其优越性能。这种改进无需额外计算开销，验证了自我精炼策略在提升视频真实感方面的有效性。",
      "conclusion": "本研究的主要贡献是提出了自我精炼视频采样方法，通过将生成器用作自精炼器，减少了对外部验证器和额外训练的依赖，从而高效提升视频生成质量。该方法在学术上提供了新颖的精炼策略，具有实际应用价值，可推广到各种视频生成任务。摘要未明确说明具体局限性，但未来工作可能涉及优化精炼策略或扩展到其他生成领域，以进一步提升性能。",
      "tags": [
        "Video Generation",
        "Self-Refining",
        "Denoising Autoencoder",
        "Uncertainty-Aware Refinement",
        "Self-Consistency"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:04.760971Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18572",
    "title": "One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization",
    "authors": [
      "Franziska Weeber",
      "Vera Neplenbroek",
      "Jan Batzner",
      "Sebastian Padó"
    ],
    "abstract": "Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18572.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18572",
    "published": "2026-01-26T15:15:58Z",
    "updated": "2026-01-26T15:15:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示了不同社会人口统计线索在大型语言模型个性化中导致响应方差，并建议未来研究使用多个外部有效线索评估偏见。",
      "motivation": "该研究旨在解决LLM个性化中社会人口统计线索可能引入或放大偏见的问题。现有方法通常依赖单一线索（如用户名或属性提及）来模拟用户人物角色，但忽视了LLM对提示变化的敏感性（鲁棒性）以及某些线索在真实交互中的稀有性（外部有效性）。这可能导致偏见评估不准确，影响AI系统的公平性和用户体验，因此需要更全面的研究方法来弥补现有不足。",
      "method": "研究方法包括系统比较六种常用的社交人口统计线索（如姓名、属性提及等）在七个开放和专有大型语言模型上的四种任务（写作和建议）。通过在不同人物角色下评估线索对响应的影响，研究关注了LLM对提示变化的敏感性，并强调了多线索评估的重要性。摘要未明确说明具体的数据集或模型架构细节，但方法的核心是比较多种线索而非依赖单一方法。",
      "result": "实验结果表明，尽管不同线索在整体上高度相关，但在不同社会人口统计人物角色下，LLM的响应存在显著方差。这揭示了单一线索可能不足以全面评估偏见，因为响应差异取决于使用的特定线索。摘要未明确提供具体性能指标（如准确率或效率改进），但突出了方差的存在，并与现有基于单一线索的研究形成对比，强调了多线索评估的必要性。",
      "conclusion": "本研究的主要贡献在于揭示了使用单一社交人口统计线索研究LLM偏见的局限性，并推荐未来个性化研究应评估多个外部有效线索。这有助于提高偏见评估的准确性和鲁棒性，推动更公平的AI系统发展。研究具有学术价值，改进了偏见研究方法，并具有实际应用潜力，但未来工作可进一步探索不同线索在更广泛任务和模型中的影响，以增强外部有效性。",
      "tags": [
        "Large Language Model",
        "Personalization",
        "Sociodemographic Cues",
        "Prompt Engineering",
        "Bias Assessment"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:50.581137Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18564",
    "title": "An Unsupervised Tensor-Based Domain Alignment",
    "authors": [
      "Chong Hyun Lee",
      "Kibae Lee",
      "Hyun Hee Yim"
    ],
    "abstract": "We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18564.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18564",
    "published": "2026-01-26T15:11:12Z",
    "updated": "2026-01-26T15:11:12Z",
    "comment": "5 pages, 5 figures",
    "light_analysis": {
      "overview": "提出一种基于斜流形的张量域对齐算法，通过优化对齐矩阵和不变子空间，提高了域适应任务的转换速度和分类准确率。",
      "motivation": "随着张量数据在图像和视频分析等领域的广泛应用，域对齐技术成为处理源域和目标域数据分布差异的关键。现有基于张量的域对齐方法常采用Stiefel流形优化，但其灵活性不足，难以适应复杂任务。本研究旨在开发一种更灵活和鲁棒的域对齐框架，以解决传统方法的局限性，提升在复杂场景下的性能和适应性。摘要未明确说明具体应用场景，但基于内容推断，该研究针对提高域对齐效率和效果的需求。",
      "method": "论文提出了一种张量域对齐算法，核心是通过对齐矩阵在不变子空间中对齐源和目标张量。关键创新点包括将优化约束在斜流形上，相比传统的Stiefel流形，提供更大的灵活性和适应性。此外，算法引入正则化项来保持源和目标张量的方差，确保鲁棒性能。该框架具有通用性，能够概括现有张量域对齐方法为特例，适用于多种任务，体现了技术上的进步和扩展性。",
      "result": "通过广泛的实验验证，该方法在域对齐转换速度上有所提高，并显著提升了分类准确率。摘要未提供具体数据如准确率提升百分比，但结果显示其性能优于当前最先进的技术，在复杂域适应任务中表现出色。与基线方法相比，该方法展现出速度和准确率的双重优势，证明了算法的有效性和实用性，为进一步应用奠定了基础。",
      "conclusion": "本研究的核心贡献是提出了一种基于斜流形的通用张量域对齐框架，通过优化对齐矩阵和子空间，结合正则化项，提升了域适应任务的性能和灵活性。学术上，该研究为张量数据处理和域对齐提供了新思路，增强了方法的鲁棒性和通用性；实践中，适用于复杂任务如图像分类，具有广泛的应用价值。未来工作可能包括扩展到更多数据类型和应用领域，尽管摘要未明确提及局限性，但暗示了进一步优化的可能性。",
      "tags": [
        "Domain Adaptation",
        "Tensor Methods",
        "Manifold Optimization",
        "Unsupervised Learning",
        "Regularization"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:11.243793Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18560",
    "title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging",
    "authors": [
      "Li Fang",
      "Tianyu Li",
      "Yanghong Lin",
      "Shudong Zhou",
      "Wei Yao"
    ],
    "abstract": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18560.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18560",
    "published": "2026-01-26T15:07:31Z",
    "updated": "2026-01-26T15:07:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于单像素特征的轻量级分类模型，用于高光谱卫星边缘计算，实现自主决策，以解决传输瓶颈问题。",
      "motivation": "高光谱成像卫星在灾害监测和应急测绘等应用中需要快速响应，但下行传输速度受限成为主要瓶颈。现有深度学习模型通常依赖大量计算资源和空间结构信息，不适合资源受限的卫星平台。因此，本研究旨在开发高效分类方法，减少对传输链路的依赖，提升卫星的自主决策能力，以支持紧急应用场景，如快速灾害评估和实时数据处理。",
      "method": "该方法采用轻量级、非深度学习框架，结合少样本学习策略，以适应卫星平台的资源约束。核心创新是一个两阶段像素级标签传播方案：第一阶段，通过构建锚点-像素亲和矩阵，传播选定的锚点标签来获得初始像素标签；第二阶段，直接计算像素级相似性生成top-k剪枝稀疏图，并利用从稀疏图推导出的闭式解替换迭代计算，以减少计算复杂度。此外，开发了基于秩约束的图聚类算法来确定锚点标签，仅依赖单像素级的内在光谱特征，无需考虑空间结构信息，从而降低计算需求。",
      "result": "摘要未明确说明具体实验结果，如准确率或效率提升数据。该方法旨在通过轻量级设计减少计算资源消耗，提高分类效率，以适应卫星边缘计算环境。相对于传统深度学习模型，预计能降低处理时间和能耗，但具体性能指标对比需参考全文。研究重点在于方法创新，而非展示量化结果，因此实验结果部分需基于完整论文进一步验证。",
      "conclusion": "本研究的主要贡献是提出了一种创新的卫星边缘计算范式，通过单像素特征和浅层模型实现高光谱图像分类，解决了资源受限平台的挑战。学术价值在于推动了轻量级AI技术在遥感领域的应用，减少了深度学习模型的计算负担；实际价值在于增强卫星的自主决策能力，支持快速响应应用如灾害监测。局限性可能包括对光谱特征准确性的依赖；未来工作方向可涉及算法优化、扩展至多模态数据，或在实际卫星平台上部署验证。",
      "tags": [
        "Hyperspectral Imaging",
        "Satellite Edge Computing",
        "Shallow Learning",
        "Few-Shot Learning",
        "Graph Clustering"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:17.408390Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18556",
    "title": "Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis",
    "authors": [
      "Jingsong Xia",
      "Siqi Wang"
    ],
    "abstract": "In biomedical engineering, artificial intelligence has become a pivotal tool for enhancing medical diagnostics, particularly in medical image classification tasks such as detecting pneumonia from chest X-rays and breast cancer screening. However, real-world medical datasets frequently exhibit severe class imbalance, where positive samples substantially outnumber negative samples, leading to biased models with low recall rates for minority classes. This imbalance not only compromises diagnostic accuracy but also poses clinical misdiagnosis risks. To address this challenge, we propose SDA-QEC (Simplified Diffusion Augmentation with Quantum-Enhanced Classification), an innovative framework that integrates simplified diffusion-based data augmentation with quantum-enhanced feature discrimination. Our approach employs a lightweight diffusion augmentor to generate high-quality synthetic samples for minority classes, rebalancing the training distribution. Subsequently, a quantum feature layer embedded within MobileNetV2 architecture enhances the model's discriminative capability through high-dimensional feature mapping in Hilbert space. Comprehensive experiments on coronary angiography image classification demonstrate that SDA-QEC achieves 98.33% accuracy, 98.78% AUC, and 98.33% F1-score, significantly outperforming classical baselines including ResNet18, MobileNetV2, DenseNet121, and VGG16. Notably, our framework simultaneously attains 98.33% sensitivity and 98.33% specificity, achieving a balanced performance critical for clinical deployment. The proposed method validates the feasibility of integrating generative augmentation with quantum-enhanced modeling in real-world medical imaging tasks, offering a novel research pathway for developing highly reliable medical AI systems in small-sample, highly imbalanced, and high-risk diagnostic scenarios.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18556.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18556",
    "published": "2026-01-26T15:05:19Z",
    "updated": "2026-01-26T15:05:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出SDA-QEC框架，通过简化扩散数据增强与量子增强特征鉴别的结合，有效解决医疗图像分类中的数据不平衡问题。",
      "motivation": "医疗图像分类任务中，真实数据集常存在严重类别不平衡，少数类样本不足导致模型偏倚和低召回率，这不仅降低诊断准确性，还带来临床误诊风险。现有基于深度学习的分类方法在处理这种不平衡时表现不佳，特别是在高风险场景下，需创新方法来提升模型对少数类的识别能力，以支持可靠医疗AI系统的开发。",
      "method": "本论文提出SDA-QEC框架，包括两部分：首先，使用轻量级扩散增强器生成高质量少数类合成样本，以重新平衡训练分布；其次，在MobileNetV2架构中嵌入量子特征层，通过Hilbert空间的高维映射增强特征鉴别能力。该方法结合了简化扩散模型的数据生成和量子机器学习的高维特征处理，创新地提升了模型在不平衡数据上的性能。",
      "result": "在冠状动脉造影图像分类实验中，SDA-QEC实现了98.33%的准确率、98.78%的AUC和98.33%的F1-score，显著优于包括ResNet18、MobileNetV2、DenseNet121和VGG16在内的经典基线模型。同时，框架达到98.33%的敏感性和特异性，显示了平衡的性能提升，这对于临床部署至关重要。",
      "conclusion": "该研究成功验证了生成数据增强与量子增强建模结合的可行性，为医疗成像任务中的小样本、高不平衡和高风险诊断提供了新研究方向。它提高了模型的诊断准确性和鲁棒性，具有重要学术价值和实际应用潜力。未来工作可进一步扩展到更多医疗场景和优化量子组件的效率，摘要未明确说明具体局限性。",
      "tags": [
        "Diffusion Models",
        "Quantum Machine Learning",
        "Data Augmentation",
        "Medical Image Classification",
        "Feature Mapping"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:26.134171Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18555",
    "title": "Automated Landmark Detection for assessing hip conditions: A Cross-Modality Validation of MRI versus X-ray",
    "authors": [
      "Roberto Di Via",
      "Vito Paolo Pastore",
      "Francesca Odone",
      "Siôn Glyn-Jones",
      "Irina Voiculescu"
    ],
    "abstract": "Many clinical screening decisions are based on angle measurements. In particular, FemoroAcetabular Impingement (FAI) screening relies on angles traditionally measured on X-rays. However, assessing the height and span of the impingement area requires also a 3D view through an MRI scan. The two modalities inform the surgeon on different aspects of the condition. In this work, we conduct a matched-cohort validation study (89 patients, paired MRI/X-ray) using standard heatmap regression architectures to assess cross-modality clinical equivalence. Seen that landmark detection has been proven effective on X-rays, we show that MRI also achieves equivalent localisation and diagnostic accuracy for cam-type impingement. Our method demonstrates clinical feasibility for FAI assessment in coronal views of 3D MRI volumes, opening the possibility for volumetric analysis through placing further landmarks. These results support integrating automated FAI assessment into routine MRI workflows. Code is released at https://github.com/Malga-Vision/Landmarks-Hip-Conditions",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18555.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18555",
    "published": "2026-01-26T15:04:21Z",
    "updated": "2026-01-26T15:04:21Z",
    "comment": "Accepted at International Symposium on Biomedical Imaging (ISBI 2026)",
    "light_analysis": {
      "overview": "本研究使用标准热图回归架构验证了MRI与X射线在自动地标检测中的跨模态临床等效性，为股骨髋臼撞击症评估提供新方法。",
      "motivation": "股骨髋臼撞击症（FAI）筛查传统依赖X射线角度测量，但评估撞击区域的三维特征需MRI扫描。两种成像模式提供互补信息，现有自动地标检测方法主要针对X射线，缺乏在MRI中的验证。本研究旨在验证MRI在自动地标检测中的临床等效性，以整合多模态数据，提升诊断准确性，解决单一模式评估的局限性。",
      "method": "研究采用标准热图回归架构，对89名患者的配对MRI和X射线数据进行自动地标检测。通过匹配队列设计，评估两种成像模态在定位和诊断准确性上的等效性。在3D MRI体积的冠状视图中应用该方法，关键创新在于跨模态验证，并探讨通过放置更多地标实现体积分析的潜力。技术路线基于成熟的回归模型，针对医学图像特性进行适配。",
      "result": "实验结果表明，MRI在自动地标检测中达到与X射线等效的定位精度和诊断准确性，特别是在cam型股骨髋臼撞击症的评估中。匹配队列研究支持跨模态临床等效性，为自动化FAI评估在MRI工作流程中的整合提供了实证基础。摘要未明确具体数值指标，但结果证实了方法的有效性，并与基线X射线方法对比显示相当性能。",
      "conclusion": "本研究验证了MRI在自动地标检测中的临床可行性，主要贡献在于证明跨模态等效性，支持将自动化FAI评估整合到常规MRI流程。学术上，推动了多模态医学图像分析的发展；应用上，可能提高诊断效率和精度。局限性包括摘要未提及具体数据细节，未来工作可扩展体积分析和更多地标应用，以优化评估方法。",
      "tags": [
        "Landmark Detection",
        "Heatmap Regression",
        "Cross-Modality Validation",
        "MRI",
        "X-ray"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:47.942164Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18554",
    "title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities",
    "authors": [
      "Alberto Purpura",
      "Li Wang",
      "Sahil Badyal",
      "Eugenio Beaufrand",
      "Adam Faulkner"
    ],
    "abstract": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18554.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18554",
    "published": "2026-01-26T15:02:15Z",
    "updated": "2026-01-26T15:02:15Z",
    "comment": "Paper accepted to EACL 2026",
    "light_analysis": {
      "overview": "该论文提出了MOSAIC框架，用于细粒度评估大语言模型的指令合规能力，揭示了合规性的多维性。",
      "motivation": "该研究的动机是解决大语言模型可靠遵循复杂指令的挑战。现有基准测试往往无法真实反映实际应用场景，且无法将指令合规性与任务成功分开评估。由于在许多应用中，如自动系统，严格遵循指令至关重要，现有方法的局限性可能导致模型在实际部署中出现失败，阻碍了可靠LLM的发展。因此，开发一个能够独立分析指令合规能力的框架对提升模型可靠性非常关键。",
      "method": "论文提出了MOSAIC（模块化合成评估指令合规性）框架，这是一种模块化的方法，通过动态生成包含最多20个应用导向生成约束的数据集，实现对指令合规性的细粒度分析。关键创新点在于其模块化设计，能模拟真实世界的复杂指令并隔离合规性与任务性能评估。该方法评估了五个不同家族的LLMs，以系统测试模型在不同约束条件下的表现，提供独立于任务成功的分析框架。",
      "result": "实验结果表明，指令合规性并非单一能力，而是根据约束类型、数量和位置显著不同。分析揭示了各模型的具体弱点，并发现了指令间的协同和冲突效应，以及位置偏差如首因和近因效应。这些发现提供了诊断模型失败的关键洞察，尽管摘要未明确具体性能指标，但强调了细粒度评估的价值，为改进LLMs指令遵循能力提供了数据支持。",
      "conclusion": "论文的主要贡献是提出了MOSAIC框架，实现了对大语言模型指令合规性的细粒度评估，这些洞察有助于诊断模型失败和开发需要严格遵循复杂指令的可靠系统。研究具有学术价值，揭示了合规性的多维性，并为未来模型改进提供理论基础。在实际应用中，可以基于这些发现优化模型以更好地适应复杂指令环境，未来工作可能包括扩展约束类型或应用于更多模型。",
      "tags": [
        "Large Language Model",
        "Instruction Compliance",
        "Benchmark",
        "Synthetic Dataset",
        "Modular Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:35.163435Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18552",
    "title": "Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection",
    "authors": [
      "Devansh Srivastav",
      "David Pape",
      "Lea Schönherr"
    ],
    "abstract": "LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18552.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18552",
    "published": "2026-01-26T14:59:17Z",
    "updated": "2026-01-26T14:59:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文首次系统分析了大型语言模型中隐藏意图在开放世界设置下的可检测性失败，并引入基于社会科学的分类法。",
      "motivation": "LLMs被广泛应用于日常决策，但其输出可能包含难以检测的隐藏意图，这些意图源自训练优化或恶意诱导，悄然影响用户信念和行动，带来潜在风险。现有检测方法在现实开放环境中失效，尤其在低发生率条件下，误报率过高掩盖精确度，漏报率隐藏真实威胁，凸显了研究这一问题的紧迫性，以应对AI系统安全性和可靠性的挑战。",
      "method": "论文提出一个包含十个类别的隐藏意图分类法，基于社会科学研究，按意图、机制、上下文和影响组织，从表面行为转向设计策略。通过受控模型诱导隐藏意图，创建测试床和误用演示。系统评估检测方法，包括推理和非推理LLM法官，在开放世界设置下进行压力测试，分析精度-发生率和精度-漏报率权衡，并进行定性案例研究验证分类法在已部署LLMs中的表现。",
      "result": "实验结果显示，在开放世界设置下，特别是低发生率条件下，隐藏意图检测方法完全崩溃，误报率过高导致精确度下降，漏报率隐藏真实风险。压力测试揭示了检测精度与发生率、漏报率之间的固有权衡，需要极低误报率或强先验信息才能有效审计。定性案例研究发现所有十类隐藏意图在先进LLMs中显现，突显问题的普遍性，但具体性能指标如准确率摘要未明确说明。",
      "conclusion": "论文的主要贡献在于系统分析LLMs中隐藏意图在开放世界下的可检测性失败，提供基础框架用于理解、诱导和压力测试这些行为。学术上建立了基于社会科学的灵活分类法，有助于预期演变威胁；实践上强调开发鲁棒检测框架的紧迫性，以应对模型安全风险。局限性如未来工作方向摘要未明确说明，但隐含需要改进检测算法和治理机制。",
      "tags": [
        "Large Language Model",
        "Hidden Intentions",
        "Detection Methods",
        "Taxonomy",
        "Open-world Settings"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:15.653990Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18547",
    "title": "REMAC: Reference-Based Martian Asymmetrical Image Compression",
    "authors": [
      "Qing Ding",
      "Mai Xu",
      "Shengxi Li",
      "Xin Deng",
      "Xin Zou"
    ],
    "abstract": "To expedite space exploration on Mars, it is indispensable to develop an efficient Martian image compression method for transmitting images through the constrained Mars-to-Earth communication channel. Although the existing learned compression methods have achieved promising results for natural images from earth, there remain two critical issues that hinder their effectiveness for Martian image compression: 1) They overlook the highly-limited computational resources on Mars; 2) They do not utilize the strong \\textit{inter-image} similarities across Martian images to advance image compression performance. Motivated by our empirical analysis of the strong \\textit{intra-} and \\textit{inter-image} similarities from the perspective of texture, color, and semantics, we propose a reference-based Martian asymmetrical image compression (REMAC) approach, which shifts computational complexity from the encoder to the resource-rich decoder and simultaneously improves compression performance. To leverage \\textit{inter-image} similarities, we propose a reference-guided entropy module and a ref-decoder that utilize useful information from reference images, reducing redundant operations at the encoder and achieving superior compression performance. To exploit \\textit{intra-image} similarities, the ref-decoder adopts a deep, multi-scale architecture with enlarged receptive field size to model long-range spatial dependencies. Additionally, we develop a latent feature recycling mechanism to further alleviate the extreme computational constraints on Mars. Experimental results show that REMAC reduces encoder complexity by 43.51\\% compared to the state-of-the-art method, while achieving a BD-PSNR gain of 0.2664 dB.",
    "categories": [
      "cs.CV",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18547.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18547",
    "published": "2026-01-26T14:55:17Z",
    "updated": "2026-01-26T14:55:17Z",
    "comment": "Accepted for publication in IEEE Transactions on Geoscience and Remote Sensing (TGRS). 2025 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. 18 pages, 20 figures",
    "light_analysis": {
      "overview": "REMAC提出一种基于参考的火星图像不对称压缩方法，通过转移计算复杂度到解码器并利用图像间和图像内相似性，显著提升压缩效率和降低编码器负担。",
      "motivation": "为了解决火星探索中图像传输的带宽限制问题，研究聚焦于开发高效的火星图像压缩方法。现有学习压缩方法虽对地球自然图像有效，但在火星应用中存在两大不足：一是忽视了火星设备的有限计算资源，二是未利用火星图像间在纹理、颜色和语义上的强相似性，这限制了压缩性能的提升和实际部署效率。",
      "method": "REMAC方法的核心包括参考引导熵模块和参考解码器，通过提取和利用参考图像中的有用信息来减少编码器的冗余操作，提升压缩性能。参考解码器采用深度多尺度架构，扩大感受野以建模图像内的长程空间依赖。此外，引入潜在特征回收机制，在编码器中重用已计算的特征，进一步缓解火星上的计算约束。摘要未明确说明具体数据集或模型架构细节，但强调了不对称压缩框架。",
      "result": "实验结果显示，REMAC与最先进方法相比，编码器复杂度降低了43.51%，同时实现了BD-PSNR增益0.2664 dB，表明在保持压缩性能的同时显著减少了计算负载。这些数据直接对比了基线方法，突显了方法在效率和性能上的改进。",
      "conclusion": "论文的主要贡献是提出REMAC方法，通过不对称压缩框架将计算复杂度转移到资源丰富的解码器，并利用图像相似性优化压缩性能。这具有重要的学术价值，推动了学习压缩在资源受限环境下的创新；实际应用价值在于加速火星探索中的图像传输，促进空间任务效率。摘要未明确说明局限性或未来工作方向，但可推断潜在方向包括进一步优化机制或扩展到其他行星图像处理。",
      "tags": [
        "Reference-Based Compression",
        "Asymmetrical Image Compression",
        "Learned Compression",
        "Deep Multi-scale Architecture",
        "Latent Feature Recycling"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:48.485137Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18546",
    "title": "Information Hidden in Gradients of Regression with Target Noise",
    "authors": [
      "Arash Jamshidi",
      "Katsiaryna Haitsiukevich",
      "Kai Puolamäki"
    ],
    "abstract": "Second-order information -- such as curvature or data covariance -- is critical for optimisation, diagnostics, and robustness. However, in many modern settings, only the gradients are observable. We show that the gradients alone can reveal the Hessian, equalling the data covariance $Σ$ for the linear regression. Our key insight is a simple variance calibration: injecting Gaussian noise so that the total target noise variance equals the batch size ensures that the empirical gradient covariance closely approximates the Hessian, even when evaluated far from the optimum. We provide non-asymptotic operator-norm guarantees under sub-Gaussian inputs. We also show that without such calibration, recovery can fail by an $Ω(1)$ factor. The proposed method is practical (a \"set target-noise variance to $n$\" rule) and robust (variance $\\mathcal{O}(n)$ suffices to recover $Σ$ up to scale). Applications include preconditioning for faster optimisation, adversarial risk estimation, and gradient-only training, for example, in distributed systems. We support our theoretical results with experiments on synthetic and real data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18546.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18546",
    "published": "2026-01-26T14:50:16Z",
    "updated": "2026-01-26T14:50:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种通过校准噪声方差的方法，仅利用梯度就能恢复数据协方差等二阶信息。",
      "motivation": "二阶信息如曲率或数据协方差对优化、诊断和鲁棒性至关重要，但在许多现代机器学习设置中，如分布式系统，仅有梯度可观察，这限制了效率提升和误差分析。现有方法在缺乏二阶信息时难以准确估计模型属性，尤其是在非最优条件下，导致优化速度慢或风险评估不足。本研究旨在解决这一问题，通过探索梯度中隐藏的信息来弥补这一缺陷。",
      "method": "该研究基于方差校准的核心思想，通过在线性回归中注入高斯噪声，使总目标噪声方差等于批次大小（即“设目标噪声方差为n”规则），从而使经验梯度协方差近似Hessian。关键创新是简单的噪声校准策略，即使在远离最优解时也能有效工作。理论分析假设输入为亚高斯分布，并提供了非渐进算子范数保证。该方法不依赖特定数据集，而是通过理论推导和实验验证其在合成和真实数据上的适用性。",
      "result": "摘要未明确说明具体实验数据，但理论保证表明该方法能有效恢复数据协方差，与基线方法（如未校准噪声的情况）相比，可避免恢复失败高达Ω(1)因子。实验在合成和真实数据上支持了理论结果，展示了方法在优化预条件化、对抗风险估计等应用中的性能提升，但未提供准确率或效率的具体数值。",
      "conclusion": "本研究的主要贡献是提出了一种实用且鲁棒的方法，仅通过梯度揭示二阶信息，为优化加速和鲁棒性分析提供了新途径。其学术价值在于深入理解梯度中的隐藏信息，实际应用包括分布式系统中的梯度训练和对抗攻击防御。局限性如扩展到非线性模型未明确说明，未来工作可能涉及更广泛场景的验证和改进。",
      "tags": [
        "Gradients",
        "Hessian",
        "Noise Injection",
        "Linear Regression",
        "Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:50.718400Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18543",
    "title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning",
    "authors": [
      "Kaixun Jiang",
      "Yuzheng Wang",
      "Junjie Zhou",
      "Pandeng Li",
      "Zhihang Liu",
      "Chen-Wei Xie",
      "Zhaoyu Chen",
      "Yun Zheng",
      "Wenqiang Zhang"
    ],
    "abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18543.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18543",
    "published": "2026-01-26T14:49:04Z",
    "updated": "2026-01-26T14:49:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "GenAgent通过代理多模态框架解耦视觉理解和生成，实现了基于多轮交互的文本到图像生成的迭代优化。",
      "motivation": "现有统一多模态模型在文本到图像生成中面临高训练成本和理解与生成之间的权衡问题，导致效率低下；同时，传统模块化系统受限于静态管道，无法支持自主的多轮交互，限制了生成质量和适应能力。因此，本研究旨在开发一种新方法，通过代理框架分离理解和生成，并引入动态交互机制，以提升生成模型的性能、灵活性和可扩展性。",
      "method": "GenAgent采用代理多模态框架，其中视觉理解由多模态模型处理，而生成则通过调用图像生成模型作为工具实现。核心创新是自主多轮交互，生成包含推理、工具调用、判断和反思的多模态思维链，以迭代细化输出。训练分为两阶段：首先，基于高质量工具调用和反思数据进行监督微调，以初始化代理行为；其次，应用端到端代理强化学习，结合点奖励（评估最终图像质量）和成对奖励（评估反思准确性），并利用轨迹重采样增强多轮探索。",
      "result": "GenAgent显著提升了基础生成器FLUX.1-dev的性能，在GenEval++基准测试中提升23.6%，在WISE基准测试中提升14%，证明了其相对于基线方法的优越性。此外，系统展示了三个关键属性：跨不同能力生成器的工具泛化能力、随着交互轮数增加而持续改进的测试时缩放效应，以及能自动适应不同任务的自适应推理能力，突显了其灵活性和应用潜力。",
      "conclusion": "GenAgent的主要贡献在于提出了一种代理多模态框架，有效解耦视觉理解和生成，并通过多轮交互实现性能优化，为多模态AI系统设计提供了新思路。该研究具有学术价值，推动了文本到图像生成的效率和适应性，并具备实际应用前景，如增强模型泛化能力。未来工作可进一步扩展到更广泛的基准测试和跨模态任务，以验证其鲁棒性。",
      "tags": [
        "Agentic Multimodal Reasoning",
        "Text-to-Image Generation",
        "Multimodal Chain-of-Thought",
        "Reinforcement Learning",
        "Tool Invocation"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:24.138634Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18536",
    "title": "Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features",
    "authors": [
      "Abishek Stephen",
      "Jindřich Libovický"
    ],
    "abstract": "We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18536.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18536",
    "published": "2026-01-26T14:41:44Z",
    "updated": "2026-01-26T14:41:44Z",
    "comment": "Accepted to Findings of EACL 2026, 9 pages, 6 figures",
    "light_analysis": {
      "overview": "本文提出了一种通过形态句法特征和IBM Model 1统计对齐来评估子词分割形态学合理性的新颖指标。",
      "motivation": "子词分割在自然语言处理中至关重要，但现有评估方法如词素边界F分数依赖于高质量的黄金分割数据，这些数据在许多语言中难以获取或质量参差不齐，导致跨语言评估受限。因此，本研究旨在开发一种更广泛适用的指标，利用更易得的形态句法特征来克服数据可用性问题，提升评估方法的通用性和实用性。",
      "method": "论文的核心方法是设计一个基于IBM Model 1概率模型的指标，将子词与形态句法特征进行对齐。形态句法特征来源于Universal Dependencies或UniMorph等资源，覆盖多种语言。该方法的关键创新在于避免依赖黄金分割数据，转而利用现有标注资源，从而实现了跨语言评估的可行性和灵活性，简化了评估过程。",
      "result": "实验结果表明，提出的指标与传统的词素边界召回率具有良好相关性，验证了其在评估形态学合理性方面的有效性。该指标在具有不同形态系统的语言中表现出更广泛的适用性，解决了现有方法因数据限制而无法覆盖多语言场景的问题，提高了评估的鲁棒性和通用性。",
      "conclusion": "本研究的主要贡献是提出了一种新颖的评估指标，克服了传统方法对黄金分割数据的依赖，提供了更语言无关的评估方式，具有重要的学术价值，推动了多语言NLP研究中形态学处理的发展。未来工作可包括进一步优化对齐模型或扩展到更多语言资源，以增强指标的准确性和覆盖范围。",
      "tags": [
        "Subword Tokenization",
        "Morphological Evaluation",
        "IBM Model 1",
        "Morpho-Syntactic Features",
        "Universal Dependencies"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:21.293819Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18533",
    "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation",
    "authors": [
      "Yuxin Jiang",
      "Yufei Wang",
      "Qiyuan Zhang",
      "Xingshan Zeng",
      "Liangyou Li",
      "Jierun Chen",
      "Chaofan Tao",
      "Haoli Bai",
      "Lifeng Shang"
    ],
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18533.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18533",
    "published": "2026-01-26T14:39:58Z",
    "updated": "2026-01-26T14:39:58Z",
    "comment": "19 pages, 8 figures, 12 tables. Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "本文提出RLVRR方法，通过从高质量参考中提取奖励链，实现开放生成任务的可验证强化学习，统一推理与生成的训练。",
      "motivation": "现有可验证奖励的强化学习在推理任务中有效，但扩展至开放生成任务面临挑战，因为缺乏明确真实值。依赖单点监督易导致训练低效和奖励破解，这限制了通用语言模型的对齐应用。本研究旨在解决开放生成任务中奖励设计难题，提升强化学习在多样化生成场景中的效率和可靠性。",
      "method": "RLVRR方法的核心是引入奖励链概念，从高质量参考中提取有序语言信号，替代传统单点奖励。奖励分解为两个维度：内容维度保留确定性核心概念（如关键词），风格维度通过基于大型语言模型的验证评估输出风格一致性。该方法结合了强化学习的探索优势与监督微调的高效性，在实验中使用了Qwen和Llama模型架构来验证技术路线。",
      "result": "在超过10个基准测试中，使用Qwen和Llama模型的实验显示，RLVRR显著优于使用十倍数据训练的监督微调方法及高级奖励模型，具体表现为性能大幅提升。此外，RLVRR统一了结构化推理和开放生成的训练流程，并能更有效地泛化，同时保持输出多样性，证明了其在处理开放生成任务中的优越性和效率改进。",
      "conclusion": "RLVRR为可验证强化学习提供了新范式，通过奖励链和维度分解解决了开放生成任务的奖励设计问题。其在学术上推动了强化学习与语言模型的结合，有助于实现通用语言模型的高效对齐，具有实际应用价值。未来工作可能包括优化奖励链提取方法或扩展到更广泛的任务领域。",
      "tags": [
        "Reinforcement Learning",
        "Verifiable Rewards",
        "Reward Chain",
        "Large Language Model",
        "Style Verification"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:38.781744Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18532",
    "title": "From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation",
    "authors": [
      "Devon Levy",
      "Bar Assayag",
      "Laura Gaspar",
      "Ilan Shimshoni",
      "Bella Specktor-Fadida"
    ],
    "abstract": "Accurate segmentation annotations are critical for disease monitoring, yet manual labeling remains a major bottleneck due to the time and expertise required. Active learning (AL) alleviates this burden by prioritizing informative samples for annotation, typically through a diversity-based cold-start phase followed by uncertainty-driven selection. We propose a novel cold-start sampling strategy that combines foundation-model embeddings with clustering, including automatic selection of the number of clusters and proportional sampling across clusters, to construct a diverse and representative initial training. This is followed by an uncertainty-based AL framework that integrates spatial diversity to guide sample selection. The proposed method is intuitive and interpretable, enabling visualization of the feature-space distribution of candidate samples. We evaluate our approach on three datasets spanning X-ray and MRI modalities. On the CheXmask dataset, the cold-start strategy outperforms random selection, improving Dice from 0.918 to 0.929 and reducing the Hausdorff distance from 32.41 to 27.66 mm. In the AL setting, combined entropy and diversity selection improves Dice from 0.919 to 0.939 and reduces the Hausdorff distance from 30.10 to 19.16 mm. On the Montgomery dataset, cold-start gains are substantial, with Dice improving from 0.928 to 0.950 and Hausdorff distance decreasing from 14.22 to 9.38 mm. On the SynthStrip dataset, cold-start selection slightly affects Dice but reduces the Hausdorff distance from 9.43 to 8.69 mm, while active learning improves Dice from 0.816 to 0.826 and reduces the Hausdorff distance from 7.76 to 6.38 mm. Overall, the proposed framework consistently outperforms baseline methods in low-data regimes, improving segmentation accuracy.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18532.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18532",
    "published": "2026-01-26T14:39:03Z",
    "updated": "2026-01-26T14:39:03Z",
    "comment": "19 pages without references",
    "light_analysis": {
      "overview": "提出结合基础模型嵌入和聚类的冷启动策略及集成空间多样性的主动学习框架，以提升医学图像分割效率。",
      "motivation": "医学图像分割的准确标注对疾病监测至关重要，但手动标注耗时且依赖专家知识，成为主要瓶颈。主动学习通过优先选择信息丰富样本来缓解负担，但现有方法如基于多样性的冷启动和不确定性选择在构建代表性初始训练集方面仍有不足，尤其是在低数据场景下。因此，本研究旨在改进冷启动策略，并结合主动学习框架，以更有效地利用标注资源，提高分割模型的性能。",
      "method": "本研究提出一种新型冷启动采样策略，利用基础模型生成的嵌入进行聚类分析，包括自动确定聚类数量和跨聚类比例采样，以构建多样化且有代表性的初始训练集。随后，引入基于不确定性的主动学习框架，集成熵等度量来评估样本不确定性，并结合空间多样性指导样本选择，增强方法的解释性。该方法支持特征空间分布的可视化，便于分析候选样本的分布特性，整体架构直观且易于应用在医学图像分割任务中。",
      "result": "在CheXmask数据集上，冷启动策略相比随机选择，将Dice系数从0.918提升至0.929，Hausdorff距离从32.41 mm减少到27.66 mm；主动学习结合熵和多样性进一步改进Dice至0.939，Hausdorff至19.16 mm。在Montgomery数据集上，冷启动使Dice从0.928提升至0.950，Hausdorff从14.22 mm降至9.38 mm。SynthStrip数据集上，冷启动略影响Dice但降低Hausdorff从9.43 mm到8.69 mm，主动学习提升Dice从0.816至0.826，Hausdorff从7.76 mm到6.38 mm。整体方法在低数据场景下一致优于基线。",
      "conclusion": "本研究表明，提出的冷启动和主动学习框架能有效提高医学图像分割的准确性，减轻手动标注负担。方法直观可解释，支持特征可视化，具有学术价值，为低数据学习提供了新思路。未来工作可扩展至其他医学影像模态或优化聚类算法，进一步提升泛化能力。",
      "tags": [
        "Active Learning",
        "Embedding-Based Methods",
        "Clustering",
        "Uncertainty Sampling",
        "Medical Image Segmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:40.659757Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18527",
    "title": "Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models",
    "authors": [
      "Francesco Maria Molfese",
      "Momchil Hardalov",
      "Rexhina Blloshmi",
      "Bill Byrne",
      "Adrià de Gispert"
    ],
    "abstract": "With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18527.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18527",
    "published": "2026-01-26T14:37:02Z",
    "updated": "2026-01-26T14:37:02Z",
    "comment": "European Chapter of the Association for Computational Linguistics EACL 2026",
    "light_analysis": {
      "overview": "本文探索了微调策略对长上下文语言模型在上下文检索和KV缓存压缩下性能和稳健性的提升作用。",
      "motivation": "随着长上下文语言模型（LCLMs）能够编码数百万标记的文档集合，它们成为传统检索增强生成（RAG）的强大替代方案。然而，微调策略是否能有效提高LCLMs在长上下文中的信息识别和使用能力，以及在KV缓存压缩技术下的稳健性尚不明确。本研究旨在解决这一实际问题，因为优化LCLMs对于提升效率和在资源受限场景下的应用至关重要，现有方法在微调策略对性能和稳健性的具体影响方面缺乏深入研究。（约120字）",
      "method": "摘要未明确说明具体的研究方法和技术细节。基于摘要推断，论文可能通过实验不同的微调策略来评估LCLMs的性能，核心方法包括设计和比较各种微调技术，以增强模型在上下文检索中的能力，并测试其在KV缓存压缩下的稳健性。关键创新点在于系统性地探索微调对长上下文模型的影响，特别是在资源受限场景，但具体数据集、模型架构等关键细节摘要未明确说明。（约100字）",
      "result": "实验结果显示，在领域内任务中，微调策略带来了显著改进，性能提升高达+20点超过基础模型。然而，领域外泛化表现依赖于具体任务，方差较大：LCLMs在金融问题上表现优异（+9点提升），而在多选问题上，检索增强生成（RAG）模型表现更强（+6点超过基线）。此外，微调方法在KV缓存压缩下也带来了适度的稳健性改进，但改进程度因任务而异，与基线方法相比显示了任务特定的优势。（约130字）",
      "conclusion": "本研究的主要贡献在于探索了微调策略对长上下文语言模型的影响，表明微调能显著提升领域内性能，但对领域外泛化的效果有限且任务依赖。研究结果对优化LCLMs在实践中的应用具有指导意义，尤其是在处理长文档和资源受限环境中。局限性在于泛化能力的不足，未来工作可专注于提高模型在不同任务间的适应性和开发更高效的缓存策略，以促进更广泛的实际应用。（约110字）",
      "tags": [
        "Fine-Tuning",
        "Long-Context Language Models",
        "In-Context Retrieval",
        "KV-Cache Compression",
        "Retrieval-Augmented Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:48.587534Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18525",
    "title": "Closing the Modality Gap Aligns Group-Wise Semantics",
    "authors": [
      "Eleonora Grassucci",
      "Giordano Cicchetti",
      "Emanuele Frasca",
      "Aurelio Uncini",
      "Danilo Comminiello"
    ],
    "abstract": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18525.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18525",
    "published": "2026-01-26T14:36:04Z",
    "updated": "2026-01-26T14:36:04Z",
    "comment": "ICLR 2026",
    "light_analysis": {
      "overview": "论文提出减少多模态学习中模态差距的方法，显著提升组级任务的性能，并证明模态差距对组级任务的影响强于实例级任务。",
      "motivation": "多模态学习中，CLIP常被用作学习跨模态共享潜在空间的方法，但存在模态差距现象，即不同模态的表示结构不匹配，导致潜在空间仅部分共享。现有研究对此影响的讨论存在争议，尤其在实例级任务如检索中，模态差距的影响被认为有限。论文动机在于探究模态差距对组级任务如聚类的实际影响，并强调解决这一问题的重要性，以改进多模态表示学习的结构对齐和语义分组能力。",
      "method": "论文提出一种新方法，旨在一致地减少两模态设置下的模态差距，并可简单扩展到n模态情况。该方法的核心创新在于通过技术手段降低模态间的结构不匹配，以更有效地对齐组级语义。尽管摘要未明确说明具体技术细节如模型架构或使用的数据集，但强调了方法的泛化性和对减少差距的专注，可能涉及优化多模态表示的对齐机制。",
      "result": "通过广泛的评估，论文发现减少模态差距在传统实例级任务如检索中仅带来边际或不一致的性能改进。然而，在组级任务如聚类中，减少差距能显著提升性能，这表明模态差距对组级任务的影响远大于实例级任务。结果验证了方法的有效性，并提供了新见解，即模态差距的减少在语义分组任务中具有关键作用，可能为多模态学习的优化提供新的方向。",
      "conclusion": "论文的主要贡献在于揭示了模态差距对组级任务的强影响，并提出一种减少差距的方法，具有重要的学术价值。这挑战了传统对模态差距的理解，强调其在语义分组任务中的核心作用。实际应用上，该方法可提升聚类等任务的性能。未来工作可能包括扩展到更多模态和任务，以及进一步验证方法的鲁棒性和局限性，摘要未明确说明具体未来方向。",
      "tags": [
        "Multimodal Learning",
        "Modality Gap",
        "CLIP",
        "Group-wise Tasks",
        "Semantic Alignment"
      ]
    },
    "analyzed_at": "2026-01-27T03:44:52.658010Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18524",
    "title": "From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale",
    "authors": [
      "Yongqi Jin",
      "Yecheng Wang",
      "Jun-jie Wang",
      "Rong Zhu",
      "Guolin Ke",
      "Weinan E"
    ],
    "abstract": "Accurate prediction of nuclear magnetic resonance (NMR) chemical shifts is fundamental to spectral analysis and molecular structure elucidation, yet existing machine learning methods rely on limited, labor-intensive atom-assigned datasets. We propose a semi-supervised framework that learns NMR chemical shifts from millions of literature-extracted spectra without explicit atom-level assignments, integrating a small amount of labeled data with large-scale unassigned spectra. We formulate chemical shift prediction from literature spectra as a permutation-invariant set supervision problem, and show that under commonly satisfied conditions on the loss function, optimal bipartite matching reduces to a sorting-based loss, enabling stable large-scale semi-supervised training beyond traditional curated datasets. Our models achieve substantially improved accuracy and robustness over state-of-the-art methods and exhibit stronger generalization on significantly larger and more diverse molecular datasets. Moreover, by incorporating solvent information at scale, our approach captures systematic solvent effects across common NMR solvents for the first time. Overall, our results demonstrate that large-scale unlabeled spectra mined from the literature can serve as a practical and effective data source for training NMR shift models, suggesting a broader role of literature-derived, weakly structured data in data-centric AI for science.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18524.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18524",
    "published": "2026-01-26T14:35:25Z",
    "updated": "2026-01-26T14:35:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种半监督学习框架，利用从科学文献中提取的大规模光谱预测核磁共振化学位移，无需依赖原子级标记数据，显著扩展了数据来源。",
      "motivation": "核磁共振化学位移的准确预测对光谱分析和分子结构解析至关重要，但现有机器学习方法依赖于有限、劳动密集的原子分配数据集，导致模型训练成本高、可扩展性差，限制了预测准确性和鲁棒性。因此，研究旨在解决如何利用更广泛、易获取的数据源（如文献中的大规模光谱）来改进预测性能，以推动科学应用中的数据分析效率。",
      "method": "论文提出了一个半监督框架，通过整合少量标记数据和大规模未分配光谱来学习化学位移。核心创新在于将预测问题表述为置换不变集监督问题，在损失函数满足常见条件下，最优二分匹配简化为基于排序的损失，从而实现了稳定的大规模训练。此外，方法大规模整合了溶剂信息，以捕获系统性溶剂效应，使用的数据集包括从文献中提取的数百万光谱。",
      "result": "实验结果表明，该模型相比现有最先进方法，在准确性和鲁棒性方面取得显著提升，并在更大、更多样化的分子数据集上展现出更强的泛化能力。通过大规模引入溶剂信息，首次成功捕获了常见NMR溶剂的系统性溶剂效应，具体性能指标如准确率提升百分比摘要未明确说明，但与基线相比效果明显。",
      "conclusion": "该研究的主要贡献在于展示了大规模未标记光谱作为训练NMR化学位移模型的有效数据源，推动了半监督学习在科学领域的应用。学术上，它为利用文献衍生的弱结构化数据提供了新思路；实际应用中，有助于提高光谱分析的准确性和效率。未来，这种方法可推广到其他科学领域，促进以数据为中心的AI发展。",
      "tags": [
        "Semi-Supervised Learning",
        "NMR Chemical Shift Prediction",
        "Permutation-Invariant Set Supervision",
        "Bipartite Matching",
        "Literature Mining"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:56.889967Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18521",
    "title": "Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning",
    "authors": [
      "Emna Boudabbous",
      "Mohamed Karaa",
      "Lokman Sboui",
      "Julio Montecinos",
      "Omar Alam"
    ],
    "abstract": "Urban bus transit agencies need reliable, network-wide delay predictions to provide accurate arrival information to passengers and support real-time operational control. Accurate predictions help passengers plan their trips, reduce waiting time, and allow operations staff to adjust headways, dispatch extra vehicles, and manage disruptions. Although real-time feeds such as GTFS-Realtime (GTFS-RT) are now widely available, most existing delay prediction systems handle only a few routes, depend on hand-crafted features, and offer little guidance on how to design a scalable, reusable architecture.   We present a city-scale prediction pipeline that combines multi-resolution feature engineering, dimensionality reduction, and deep learning. The framework generates 1,683 spatiotemporal features by exploring 23 aggregation combinations over H3 cells, routes, segments, and temporal patterns, and compresses them into 83 components using Adaptive PCA while preserving 95% of the variance. To avoid the \"giant cluster\" problem that occurs when dense urban areas fall into a single H3 region, we introduce a hybrid H3+topology clustering method that yields 12 balanced route clusters (coefficient of variation 0.608) and enables efficient distributed training.   We compare five model architectures on six months of bus operations from the Société de transport de Montréal (STM) network in Montréal. A global LSTM with cluster-aware features achieves the best trade-off between accuracy and efficiency, outperforming transformer models by 18 to 52% while using 275 times fewer parameters. We also report multi-level evaluation at the elementary segment, segment, and trip level with walk-forward validation and latency analysis, showing that the proposed pipeline is suitable for real-time, city-scale deployment and can be reused for other networks with limited adaptation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18521.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18521",
    "published": "2026-01-26T14:30:50Z",
    "updated": "2026-01-26T14:30:50Z",
    "comment": "This manuscript is a preprint of an earlier version. A revised system-oriented version is currently under review",
    "light_analysis": {
      "overview": "论文提出一个可扩展的城市规模公交延迟预测框架，结合多分辨率特征工程和深度学习以实现准确高效的预测。",
      "motivation": "城市公交机构需要可靠的网络范围内延迟预测，以提供准确到达信息给乘客并支持实时操作控制，如调整班次和应对突发状况。尽管GTFS-Realtime等实时数据源已普及，但现有系统通常仅处理少数路线、依赖手工特征，且缺乏可扩展和可重用架构的指导，难以满足大规模城市网络的需求。这突显了开发一个系统化、可扩展预测方法的重要性，以改进公交服务的可靠性和乘客体验。",
      "method": "研究提出一个城市规模预测管道，结合多分辨率特征工程、维度缩减和深度学习。首先，通过探索H3单元格、路线、路段和时间模式的23种聚合组合生成1683个时空特征。然后使用自适应PCA压缩这些特征到83个组件，保留95%的方差。为避免“巨型集群”问题，引入混合H3+拓扑聚类方法，产生12个平衡的路线集群（变异系数0.608），支持高效分布式训练。最后，在STM网络数据上比较了全球LSTM和transformer等五种模型架构，模型设计注重可扩展性。",
      "result": "在蒙特利尔STM网络的六个月公交运营数据上测试，全球LSTM模型在准确性和效率之间取得最佳平衡，优于transformer模型18%到52%，同时参数数量减少275倍。多级评估在基础路段、路段和行程级别进行，采用前向验证和延迟分析，结果表明该管道适合实时、城市规模的部署，并可有限适应地重用其他网络。具体数据包括准确率提升和参数减少，验证了方法的有效性和可扩展性。",
      "conclusion": "该研究的主要贡献是开发了一个可扩展的城市规模公交延迟预测框架，通过多分辨率特征工程和深度学习实现高性能预测。该方法在准确性和效率上优于基线模型，展示了实时部署的潜力，并具有可重用性，适用于其他交通网络。学术价值在于提出系统化架构，实际应用在于提升公交运营效率和乘客体验，未来工作可能涉及进一步优化模型或扩展数据集。",
      "tags": [
        "Deep Learning",
        "LSTM",
        "Transformer",
        "Adaptive PCA",
        "H3 Clustering"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:03.156865Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18517",
    "title": "GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback",
    "authors": [
      "James Sungarda",
      "Hongkai Liu",
      "Zilong Zhou",
      "Tien-Hsuan Wu",
      "Johnson Chun-Sing Cheung",
      "Ben Kao"
    ],
    "abstract": "Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18517.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18517",
    "published": "2026-01-26T14:26:54Z",
    "updated": "2026-01-26T14:26:54Z",
    "comment": "2025 IEEE International Conference on Big Data. ISBN: 979-8-3315-9447-3/25. Page numbers: 3544-3553",
    "light_analysis": {
      "overview": "SWITCH 通过集成真实客户模拟和实时咨询技能分类，为社会工作教育提供了一个可扩展的AI培训解决方案。",
      "motivation": "社会工作实地教育是核心教学方法，但提供及时客观反馈受限于导师和客户资源的可用性。传统方法依赖人工模拟，成本高且反馈不一致，导致培训效率低下。本研究旨在解决这一问题，通过AI技术开发低成本、可扩展的培训系统，以改善教育质量和可访问性，并应对现有培训中反馈不足的挑战。",
      "method": "SWITCH 系统采用基于认知的配置文件模拟客户，包括静态字段（如背景、信念）和动态字段（如情绪、自动思维），使代理行为在会话中真实演化。技能分类模块结合上下文学习检索注释转录和微调的BERT多标签分类器，从用户话语中识别咨询技能，结果输入MI控制器以调节动机性访谈阶段过渡，优化培训工作流。",
      "result": "实验结果表明，基于BERT的分类方法和上下文学习均显著优于基线，大幅提升了咨询技能分类的准确性。摘要未明确说明具体性能指标，但验证了这些技术在改善分类效果方面的潜力，支持SWITCH系统在提供一致、低成本培训方面的实用性和有效性。",
      "conclusion": "本研究开发了SWITCH系统，为社会工作实地教育提供了一个AI驱动的培训工具，通过集成客户模拟和实时反馈，实现了可扩展、低成本的培训补充。学术上，它探索了生成AI在教育领域的应用；实际上，减轻了导师负担，提升了培训一致性。未来工作可包括更广泛的评估和系统扩展。",
      "tags": [
        "Client Simulation",
        "BERT",
        "In-Context Learning",
        "Multi-label Classification",
        "Motivational Interviewing"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:16.171293Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18513",
    "title": "LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models",
    "authors": [
      "Kai Hu",
      "Haoqi Hu",
      "Matt Fredrikson"
    ],
    "abstract": "Lipschitz-based certification offers efficient, deterministic robustness guarantees but has struggled to scale in model size, training efficiency, and ImageNet performance. We introduce \\emph{LipNeXt}, the first \\emph{constraint-free} and \\emph{convolution-free} 1-Lipschitz architecture for certified robustness. LipNeXt is built using two techniques: (1) a manifold optimization procedure that updates parameters directly on the orthogonal manifold and (2) a \\emph{Spatial Shift Module} to model spatial pattern without convolutions. The full network uses orthogonal projections, spatial shifts, a simple 1-Lipschitz $β$-Abs nonlinearity, and $L_2$ spatial pooling to maintain tight Lipschitz control while enabling expressive feature mixing. Across CIFAR-10/100 and Tiny-ImageNet, LipNeXt achieves state-of-the-art clean and certified robust accuracy (CRA), and on ImageNet it scales to 1-2B large models, improving CRA over prior Lipschitz models (e.g., up to $+8\\%$ at $\\varepsilon{=}1$) while retaining efficient, stable low-precision training. These results demonstrate that Lipschitz-based certification can benefit from modern scaling trends without sacrificing determinism or efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18513.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18513",
    "published": "2026-01-26T14:18:55Z",
    "updated": "2026-01-26T14:18:55Z",
    "comment": "ICLR 2026. 17 pages",
    "light_analysis": {
      "overview": "LipNeXt 提出了一种无约束和无卷积的 1-Lipschitz 架构，首次实现了在亿参数模型上高效、可扩展的确定性鲁棒性认证。",
      "motivation": "Lipschitz-based 认证方法虽然提供高效、确定性的鲁棒性保证，但在扩展到大型模型时面临显著挑战，如模型规模受限、训练效率低下以及在 ImageNet 等复杂数据集上的性能不足。现有方法难以在保持 Lipschitz 约束的同时实现高可扩展性和高性能，导致鲁棒性认证在现实应用中的实用性受限。本研究旨在解决这一问题，通过创新架构设计来提升 Lipschitz 方法的扩展能力。",
      "method": "LipNeXt 采用两种核心技术：一是流形优化过程，直接在正交流形上更新参数，以高效维持 Lipschitz 约束；二是 Spatial Shift Module，无卷积地建模空间模式，避免传统卷积操作的复杂性。网络架构结合正交投影、空间移位、简单的 1-Lipschitz β-Abs 非线性激活函数和 L2 空间池化，实现紧致的 Lipschitz 控制，并支持丰富的特征混合，从而在保持鲁棒性的同时提升表达能力。",
      "result": "在 CIFAR-10/100 和 Tiny-ImageNet 数据集上，LipNeXt 实现了最先进的干净和认证鲁棒准确率（CRA）。在 ImageNet 上，它成功扩展到 1-2B 参数的大模型，CRA 比先前 Lipschitz 模型提升最多 8%（在扰动大小 ε=1 时），同时保持了高效、稳定的低精度训练。这些结果验证了 LipNeXt 在提升鲁棒性能方面的显著优势。",
      "conclusion": "本研究的主要贡献是 LipNeXt 架构，它展示了 Lipschitz-based 认证方法可以在不牺牲确定性或效率的情况下，利用现代模型扩展趋势，有效解决可扩展性问题。这项工作为鲁棒机器学习领域提供了新的设计思路，具有潜在的学术价值和实际应用前景，如增强大规模深度学习模型的鲁棒性，并为未来研究在高效鲁棒架构方面指明了方向。",
      "tags": [
        "Lipschitz-based Certification",
        "Orthogonal Manifold Optimization",
        "Spatial Shift Module",
        "Certified Robust Accuracy",
        "Large-scale Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:13.695737Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18512",
    "title": "Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research",
    "authors": [
      "Antonio Garzon-Vico",
      "Krithika Sharon Komalapati",
      "Arsalan Shahid",
      "Jan Rosier"
    ],
    "abstract": "This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18512.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18512",
    "published": "2026-01-26T14:17:23Z",
    "updated": "2026-01-26T14:17:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种使用大型语言模型构建虚拟高层管理者人格的方法，以模拟真实高管的决策，为组织研究提供新工具。",
      "motivation": "该研究旨在解决组织研究中获取高层管理者决策数据困难的现实问题。由于直接访问高管受限，现有方法难以有效模拟个体领导者的行为和决策过程，导致研究数据不足且成本高昂。本研究通过引入大型语言模型，构建虚拟人格来近似真实高管的决策，弥补了访问障碍的不足，为组织行为学和社会心理学领域提供了一种可信的替代工具。",
      "method": "研究方法采用基于大型语言模型的虚拟人格构建框架。核心创新在于结合真实CEO的通讯数据和道德基础理论，生成LLM驱动的参与者以模拟个体领导者的决策。通过三个评估阶段——验证构建有效性、测试可靠性和评估行为保真度，与人类参与者进行基准对比。技术路线中未明确说明具体数据集和模型架构，摘要仅提及使用LLM和理论基础作为支撑。",
      "result": "实验结果表明，基于理论构建的虚拟人格能够近似人类参与者在道德判断中的表现，与真实样本的行为模式相匹配。摘要未提供具体性能指标数据，但通过基准测试表明，这些虚拟CEO在模拟决策行为上具有较高保真度，可作为组织研究中可信的补充工具，尤其在直接访问高管受限的情境下。",
      "conclusion": "本研究的主要贡献是开发了一种利用大型语言模型创建虚拟高层管理者的方法，扩展了组织研究的技术手段。其学术价值在于将AI技术应用于社会行为模拟，促进跨学科研究；实际应用价值在于为高管决策分析提供补充工具。局限性包括虚拟人格的泛化能力和潜在偏差，未来工作可优化构建框架，应用于更多组织场景，并探讨伦理问题。",
      "tags": [
        "Large Language Models",
        "Moral Foundations Theory",
        "Virtual Personas",
        "Organizational Research",
        "Simulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:13.968383Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18510",
    "title": "Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates",
    "authors": [
      "Yibo Li",
      "Zijie Lin",
      "Ailin Deng",
      "Xuan Zhang",
      "Yufei He",
      "Shuo Ji",
      "Tri Cao",
      "Bryan Hooi"
    ],
    "abstract": "While Large Language Model (LLM) agents excel at general tasks, they inherently struggle with continual adaptation due to the frozen weights after deployment. Conventional reinforcement learning (RL) offers a solution but incurs prohibitive computational costs and the risk of catastrophic forgetting. We introduce Just-In-Time Reinforcement Learning (JitRL), a training-free framework that enables test-time policy optimization without any gradient updates. JitRL maintains a dynamic, non-parametric memory of experiences and retrieves relevant trajectories to estimate action advantages on-the-fly. These estimates are then used to directly modulate the LLM's output logits. We theoretically prove that this additive update rule is the exact closed-form solution to the KL-constrained policy optimization objective. Extensive experiments on WebArena and Jericho demonstrate that JitRL establishes a new state-of-the-art among training-free methods. Crucially, JitRL outperforms the performance of computationally expensive fine-tuning methods (e.g., WebRL) while reducing monetary costs by over 30 times, offering a scalable path for continual learning agents. The code is available at https://github.com/liushiliushi/JitRL.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18510.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18510",
    "published": "2026-01-26T14:16:51Z",
    "updated": "2026-01-26T14:16:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出JitRL框架，通过无梯度更新的即时强化学习实现LLM代理的持续适应。",
      "motivation": "LLM代理在部署后因权重冻结而难以持续适应新任务或环境变化，限制了其实际应用。传统强化学习方法虽能提供解决方案，但计算成本过高且易导致灾难性遗忘，阻碍了高效持续学习的发展。因此，亟需一种低开销、无训练的方法来提升LLM代理的实时适应性，解决现有技术的不足。摘要未明确说明具体应用场景，但强调了成本与效率问题的重要性。",
      "method": "JitRL采用训练免费框架，通过维护动态非参数记忆库存储历史经验，并在测试时检索相关轨迹来实时估计行动优势值。这些估计直接用于调制LLM的输出logits，实现策略优化。关键创新点包括无梯度更新、动态记忆管理和实时计算，理论证明该加法式更新规则是KL约束策略优化目标的精确闭式解。实验在WebArena和Jericho平台上进行，未详细描述模型架构，但基于LLM代理实现。",
      "result": "在WebArena和Jericho基准测试中，JitRL在无训练方法中达到新的SOTA性能。具体而言，它超越了计算昂贵的微调方法如WebRL，同时将货币成本降低超过30倍。尽管摘要未提供具体准确率提升数据，但结果显示了显著的经济和效率优势，验证了其在高成本和低资源场景下的实用价值。与基线方法相比，JitRL在持续学习任务中表现出色。",
      "conclusion": "JitRL为LLM代理的持续学习提供了一种可扩展且经济的解决方案，具有重要学术价值和实际应用潜力。主要贡献在于无梯度更新的实时策略优化框架，推动了高效自适应技术的研究。摘要未明确说明局限性或未来工作方向，但该方法为探索更复杂场景下的鲁棒性改进奠定了基础。整体意义在于降低部署成本，促进持续学习在真实世界中的应用。",
      "tags": [
        "Just-In-Time Reinforcement Learning",
        "Large Language Model",
        "Continual Learning",
        "Reinforcement Learning",
        "Training-Free Methods"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:54.372384Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18509",
    "title": "Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark",
    "authors": [
      "Andro Sabashvili"
    ],
    "abstract": "Reliable uncertainty quantification is of critical importance in time series forecasting, yet traditional methods often rely on restrictive distributional assumptions. Conformal prediction (CP) has emerged as a promising distribution-free framework for generating prediction intervals with rigorous theoretical guarantees. However, applying CP to sequential data presents a primary challenge: the temporal dependencies inherent in time series fundamentally violate the core assumption of data exchangeability, upon which standard CP guarantees are built. This review critically examines the main categories of algorithmic solutions designed to address this conflict. We survey and benchmark methods that relax the exchangeability assumption, those that redefine the data unit to be a collection of independent time series, approaches that explicitly model the dynamics of the prediction residuals, and online learning algorithms that adapt to distribution shifts to maintain long-run coverage. By synthesizing these approaches, we highlight computational efficiency and practical performance on real-world data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18509.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18509",
    "published": "2026-01-26T14:15:08Z",
    "updated": "2026-01-26T14:15:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文综述并基准测试了时间序列预测中解决可交换性问题的 conformal prediction 算法，为不确定性量化提供方法指导。",
      "motivation": "时间序列预测在金融、气候等关键领域需要可靠的不确定性量化，但传统方法如贝叶斯预测依赖于严格分布假设，限制其普适性。Conformal prediction (CP) 作为一种分布无关框架，具有理论保证的预测区间，然而将其应用于序列数据时，时间依赖违反核心的可交换性假设，导致标准 CP 失效。因此，开发适应时间依赖的 CP 算法成为重要研究方向，以填补现有方法在序列数据应用中的不足，提升预测的可靠性。",
      "method": "本文采用综述和基准测试的方法，系统调研了四类算法来解决时间依赖问题：第一类是放松可交换性假设的技术，通过统计调整恢复理论保证；第二类是将数据单元重新定义为独立时间序列的集合，以恢复可交换性；第三类是显式建模预测残差的动态过程，结合时间序列模型捕捉序列模式；第四类是采用在线学习算法，动态适应分布漂移以维持长期覆盖。通过整合这些方法，文章评估其在真实数据集上的表现，重点关注计算效率和实际应用。",
      "result": "摘要未明确提供具体实验结果数据，但文章通过合成不同方法，强调它们在计算效率和实际世界数据性能方面的表现。基准测试可能对比了各类算法在预测区间覆盖率和计算时间上的表现，例如在线学习算法在适应分布漂移时可能更具优势，而建模残差动态的方法可能提升准确性。然而，具体指标如准确率提升百分比或效率改进细节摘要未明确说明，需查阅全文以获得详细比较数据。",
      "conclusion": "本文的主要贡献在于全面回顾和基准测试了时间序列预测中适应可交换性问题的 CP 算法，综合了放松假设、重定义数据单元、建模残差动态和在线学习等方法。研究为领域提供了算法选择和性能评估的指导，学术价值体现在扩展理论框架到序列数据，实际应用价值支持金融预测和能源管理等场景。局限性包括某些方法依赖特定假设或计算成本较高，未来工作可探索更高效的自适应算法或结合深度学习模型以处理更复杂的时间依赖模式。",
      "tags": [
        "Conformal Prediction",
        "Time Series Forecasting",
        "Uncertainty Quantification",
        "Exchangeability",
        "Online Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:44:03.776407Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18500",
    "title": "Nearly Optimal Bayesian Inference for Structural Missingness",
    "authors": [
      "Chen Liang",
      "Donghua Yang",
      "Yutong Wang",
      "Tianle Zhang",
      "Shenghe Zhou",
      "Zhiyu Liang",
      "Hengtong Zhang",
      "Hongzhi Wang",
      "Ziqi Li",
      "Xiyang Zhang",
      "Zheng Liang",
      "Yifei Li"
    ],
    "abstract": "Structural missingness breaks 'just impute and train': values can be undefined by causal or logical constraints, and the mask may depend on observed variables, unobserved variables (MNAR), and other missingness indicators. It simultaneously brings (i) a catch-22 situation with causal loop, prediction needs the missing features, yet inferring them depends on the missingness mechanism, (ii) under MNAR, the unseen are different, the missing part can come from a shifted distribution, and (iii) plug-in imputation, a single fill-in can lock in uncertainty and yield overconfident, biased decisions. In the Bayesian view, prediction via the posterior predictive distribution integrates over the full model posterior uncertainty, rather than relying on a single point estimate. This framework decouples (i) learning an in-model missing-value posterior from (ii) label prediction by optimizing the predictive posterior distribution, enabling posterior integration. This decoupling yields an in-model almost-free-lunch: once the posterior is learned, prediction is plug-and-play while preserving uncertainty propagation. It achieves SOTA on 43 classification and 15 imputation benchmarks, with finite-sample near Bayes-optimality guarantees under our SCM prior.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18500.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18500",
    "published": "2026-01-26T14:03:11Z",
    "updated": "2026-01-26T14:03:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种贝叶斯框架，通过解耦学习缺失值后验和标签预测，处理结构缺失性并实现近最优推断。",
      "motivation": "结构缺失性在现实数据中普遍存在，如因果或逻辑约束导致值未定义，掩码依赖于观察和未观察变量。传统'just impute and train'方法在处理此类问题时失效，因为因果循环困境使得预测依赖缺失特征，而MNAR场景下缺失部分分布偏移，插件插补会导致过度自信和偏差决策。现有方法无法有效整合缺失机制，特别是在不确定性传播方面存在不足，因此需要一种新方法来确保推断的鲁棒性和准确性。",
      "method": "本研究采用贝叶斯推断方法，提出一个核心框架，将学习模型内缺失值后验与通过优化预测后验分布进行标签预测解耦。关键创新点在于使用后验预测分布替代单点估计，以实现不确定性传播，并引入结构化因果模型（SCM）先验来建模缺失机制，促进后验整合。这种方法避免了因果循环，通过贝叶斯框架允许在模型内灵活处理缺失值，而不依赖固定插补。",
      "result": "在实验中，该框架在43个分类基准和15个插补任务上达到最先进（SOTA）性能，展示了优于基线方法的改进效果。特别是在MNAR场景下，能够有效处理分布偏移问题，同时理论分析提供了有限样本下的近贝叶斯最优性保证，确保了推断的质量。具体性能指标摘要未详细说明，但整体表现验证了方法的有效性。",
      "conclusion": "本研究的主要贡献是提出一个高效的贝叶斯框架，解决了结构缺失性带来的挑战，实现了不确定性传播和近最优推断。学术价值在于克服了传统插补方法的局限，为复杂缺失数据场景提供了理论支撑；实际应用价值广泛，适用于分类和插补任务。未来工作可扩展至更广泛的缺失模式或集成到其他机器学习模型中，以进一步验证通用性。",
      "tags": [
        "Bayesian Inference",
        "Structural Missingness",
        "Missing Not At Random (MNAR)",
        "Posterior Predictive Distribution",
        "Causal Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:18.610981Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18496",
    "title": "DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference",
    "authors": [
      "Zihan wang",
      "Hao Wang",
      "Shi Feng",
      "Xiaocui Yang",
      "Daling Wang",
      "Yiqun Zhang",
      "Jinghao Lin",
      "Haihua Yang",
      "Xiaozhong Ji"
    ],
    "abstract": "Medical reasoning models remain constrained by parametric knowledge and are thus susceptible to forgetting and hallucinations. DeepResearch (DR) models ground outputs in verifiable evidence from tools and perform strongly in general domains, but their direct transfer to medical field yields relatively limited gains. We attribute this to two gaps: task characteristic and tool-use scaling. Medical questions require evidence interpretation in a knowledge-intensive clinical context; while general DR models can retrieve information, they often lack clinical-context reasoning and thus \"find it but fail to use it,\" leaving performance limited by medical abilities. Moreover, in medical scenarios, blindly scaling tool-call can inject noisy context, derailing sensitive medical reasoning and prompting repetitive evidence-seeking along incorrect paths. Therefore, we propose DeepMed. For data, we deploy a multi-hop med-search QA synthesis method supporting the model to apply the DR paradigm in medical contexts. For training, we introduce a difficulty-aware turn-penalty to suppress excessive tool-call growth. For inference, we bring a monitor to help validate hypotheses within a controlled number of steps and avoid context rot. Overall, on seven medical benchmarks, DeepMed improves its base model by 9.79\\% on average and outperforms larger medical reasoning and DR models.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18496.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18496",
    "published": "2026-01-26T13:57:48Z",
    "updated": "2026-01-26T13:57:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "DeepMed 提出了一种通过多跳医疗搜索数据和回合控制的代理训练与推理，提升医疗领域 DeepResearch 代理性能的方法。",
      "motivation": "医疗推理模型受限于参数知识，易出现遗忘和幻觉问题。DeepResearch（DR）模型在通用领域表现良好，但直接应用于医疗领域时增益有限。这主要源于两个差距：一是医疗问题需在知识密集的临床上下文中解释证据，而通用 DR 模型虽能检索信息，但常缺乏临床推理能力，导致“找到但不会用”的现象；二是盲目扩展工具调用会引入噪音，干扰敏感医疗推理，并引发错误路径上的重复证据寻求。因此，需要解决任务特征差异和工具使用扩展带来的挑战。",
      "method": "DeepMed 的核心方法包括三个部分：在数据方面，采用多跳医疗搜索 QA 合成方法，帮助模型在医疗上下文中应用 DR 范式；训练方面，引入难度感知的回合惩罚机制，以抑制过多工具调用的增长；推理方面，加入监控器，在受控步骤内验证假设并避免上下文腐败。这些创新点通过整合数据增强和动态控制，使 DR 模型更好地适应医疗场景，提高推理的准确性和效率。",
      "result": "在七个医疗基准测试中，DeepMed 将其基础模型的平均性能提升了 9.79%，并优于其他更大的医疗推理和 DR 模型。这显示了 DeepMed 在性能上的显著改进，通过有效的数据合成和训练推理控制，解决了现有方法在医疗领域中的局限性，具体体现在更高的准确率和更好的模型适应性。",
      "conclusion": "DeepMed 的主要贡献是开发了一种结合数据合成、训练和推理控制的医疗 DR 代理，成功将 DR 范式适应到知识密集的医疗领域。其学术价值在于解决了医疗推理中的遗忘和幻觉问题，提升了模型的可靠性和准确性；实际应用中，可增强医疗 AI 系统的决策支持能力。未来工作可进一步优化控制机制或扩展到更多医疗子领域。摘要未明确说明具体局限性。",
      "tags": [
        "DeepResearch Agent",
        "Multi-hop Search",
        "Medical Reasoning",
        "Tool-use Scaling",
        "Turn-Controlled Training"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:09.155467Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18493",
    "title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment",
    "authors": [
      "Sara Tehrani",
      "Yonghao Xu",
      "Leif Haglund",
      "Amanda Berg",
      "Michael Felsberg"
    ],
    "abstract": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.   To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18493.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18493",
    "published": "2026-01-26T13:48:11Z",
    "updated": "2026-01-26T13:48:11Z",
    "comment": "Under review at ICPR 2026",
    "light_analysis": {
      "overview": "论文提出DisasterInsight多模态基准和DI-Chat模型，用于评估视觉-语言模型在灾害分析任务中的功能感知和基础推理能力。",
      "motivation": "卫星图像及时解释对灾害响应至关重要，但现有视觉-语言基准主要关注粗粒度标签和图像级识别，忽视了真实人道主义工作流程中所需的细粒度功能理解和指令鲁棒性。这导致现有方法在实际应用中表现不足，无法有效支持灾害评估任务，亟需开发更贴近现实需求的基准来提升模型性能。",
      "method": "研究构建了DisasterInsight基准，将xBD数据集重构为约112K个以建筑物为中心的实例，支持建筑物功能分类、损害级别和灾害类型分类、计数及结构化报告生成等多样化任务。提出DI-Chat模型，采用参数高效的Low-Rank Adaptation（LoRA）技术对现有视觉-语言模型骨干进行微调，在灾害特定指令数据上实现领域适应。",
      "result": "实验表明，在最新通用和遥感视觉-语言模型上存在显著性能差距，尤其在损害理解和结构化报告生成方面。DI-Chat在损害级别分类、灾害类型分类和报告生成质量上取得显著改进，但建筑物功能分类对所有评估模型仍具挑战性，显示现有方法在该任务上的局限性。",
      "conclusion": "DisasterInsight为研究灾害图像中的基础多模态推理提供了统一基准，推动了视觉-语言模型在灾害分析领域的学术评估标准。其实用价值在于促进灾害响应中卫星图像解释的改进，未来工作可探索如何克服建筑物功能分类等挑战以进一步提升模型性能。",
      "tags": [
        "Multimodal Benchmark",
        "Vision-Language Models (VLMs)",
        "Low-Rank Adaptation (LoRA)",
        "Disaster Assessment",
        "Structured Report Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:19.714479Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18491",
    "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
    "authors": [
      "Dongrui Liu",
      "Qihan Ren",
      "Chen Qian",
      "Shuai Shao",
      "Yuejin Xie",
      "Yu Li",
      "Zhonghao Yang",
      "Haoyu Luo",
      "Peng Wang",
      "Qingyu Liu",
      "Binxin Hu",
      "Ling Tang",
      "Jilin Mei",
      "Dadi Guo",
      "Leitao Yuan",
      "Junyao Yang",
      "Guanxu Chen",
      "Qihao Lin",
      "Yi Yu",
      "Bo Zhang",
      "Jiaxuan Guo",
      "Jie Zhang",
      "Wenqi Shao",
      "Huiqi Deng",
      "Zhiheng Xi",
      "Wenjie Wang",
      "Wenxuan Wang",
      "Wen Shen",
      "Zhikai Chen",
      "Haoyu Xie",
      "Jialing Tao",
      "Juntao Dai",
      "Jiaming Ji",
      "Zhongjie Ba",
      "Linfeng Zhang",
      "Yong Liu",
      "Quanshi Zhang",
      "Lei Zhu",
      "Zhihua Wei",
      "Hui Xue",
      "Chaochao Lu",
      "Jing Shao",
      "Xia Hu"
    ],
    "abstract": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18491.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18491",
    "published": "2026-01-26T13:45:41Z",
    "updated": "2026-01-26T13:45:41Z",
    "comment": "40 pages, 26 figures",
    "light_analysis": {
      "overview": "提出AgentDoG诊断护栏框架，通过三维分类法实现AI代理安全和风险的细粒度监控与根因诊断。",
      "motivation": "AI代理的兴起带来了复杂的安全和安全隐患，源于自主工具使用和环境交互，现有护栏模型缺乏对代理风险的充分意识和风险诊断的透明度，难以覆盖多样化风险行为。为了解决这一问题，本研究旨在提供一个能全面监控和诊断代理风险的方法，提升AI代理在实际应用中的安全性和可靠性。",
      "method": "本研究首先提出一个统一的三维分类法，将代理风险按来源（where）、失败模式（how）和后果（what）正交分类。基于此，引入了细粒度的代理安全基准ATBench和诊断护栏框架AgentDoG。AgentDoG监控代理轨迹，并诊断不安全或看似安全但不合理行为的根本原因，提供详细来源信息以增强透明度。框架基于Qwen和Llama模型系列，提供4B、7B和8B参数的不同变体，实现上下文感知的评估。",
      "result": "广泛的实验结果表明，AgentDoG在多样化和复杂的交互场景中实现了最先进的代理安全审核性能。尽管摘要未明确具体数据，但强调了其在多种场景下的优越表现。与基线方法相比，AgentDoG在风险评估的准确性和透明度方面有显著提升，有效促进了代理对齐和安全管理。",
      "conclusion": "本研究的主要贡献包括提出了结构化风险分类法、细粒度基准ATBench和诊断框架AgentDoG，为AI代理安全和安全提供了创新解决方案。AgentDoG的根因诊断和透明度功能增强了风险管理的有效性，并公开发布了所有模型和数据集，推动了该领域的研究和应用。未来工作可进一步扩展风险类别或优化诊断算法以应对更复杂场景。",
      "tags": [
        "AI Agent Safety",
        "Risk Taxonomy",
        "Diagnostic Guardrail",
        "Benchmarking",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:11.112174Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18486",
    "title": "Demographic Probing of Large Language Models Lacks Construct Validity",
    "authors": [
      "Manuel Tonneau",
      "Neil K. R. Seghal",
      "Niyati Malhotra",
      "Victor Orozco-Olvera",
      "Ana María Muñoz Boudet",
      "Lakshmi Subramanian",
      "Sharath Chandra Guntuku",
      "Valentin Hofmann"
    ],
    "abstract": "Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18486.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18486",
    "published": "2026-01-26T13:41:35Z",
    "updated": "2026-01-26T13:41:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示大型语言模型中人口统计探查方法缺乏构念效度，挑战了其评估人口效应的假设。",
      "motivation": "人口统计探查广泛用于研究大型语言模型如何根据人口属性（如种族和性别）调整行为，但通常使用单一人口线索（如姓名或方言）作为群体成员信号，并假设这些线索是同一人口行为的可互换操作化。这种假设的构念效度问题可能导致评估结果不稳定，影响对模型人口偏差的准确理解，在公平性和应用场景中至关重要。因此，本研究旨在测试这一假设，解决现有方法可能存在的误导性结论。",
      "method": "本研究通过在现实场景中的建议寻求交互中测试构念效度，聚焦美国背景下的种族和性别。核心方法包括使用多种人口线索（如姓名或方言）作为信号，分析大型语言模型行为的变化模式，并评估线索的互换性。创新点在于探讨线索编码人口属性的强度差异，以及控制语言混淆因素（如词汇或句法变化）对模型行为的影响，以全面评估探查方法的可靠性。",
      "result": "实验结果表明，代表相同人口组的线索仅诱导部分重叠的模型行为变化，而组内差异弱且不均衡；估计的人口差异不稳定，其大小和方向随线索不同而变化。进一步分析显示，这些不一致性部分源于线索编码人口属性的强度差异，以及语言混淆因素独立影响模型行为。与基线方法相比，现有探查方法无法提供稳定的评估，凸显了构念效度的缺失。",
      "conclusion": "论文的主要贡献是指出人口统计探查在大型语言模型中缺乏构念效度，不能产生关于模型条件于人口信息的单一稳定表征。这反映了方法的误构或碎片化问题，具有重要学术价值，为公平AI研究提供了批判性视角。实际应用中，推荐使用多种生态有效线索并显式控制混淆因素，以支持更可靠的结论。未来工作可扩展至其他人口属性或文化背景，进一步验证和改进探查方法。",
      "tags": [
        "Demographic Probing",
        "Large Language Models",
        "Construct Validity",
        "Linguistic Confounders",
        "Advice-Seeking Interactions"
      ]
    },
    "analyzed_at": "2026-01-27T03:22:55.160211Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18483",
    "title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs",
    "authors": [
      "Arya Labroo",
      "Ivaxi Sheth",
      "Vyas Raina",
      "Amaani Ahmed",
      "Mario Fritz"
    ],
    "abstract": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18483.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18483",
    "published": "2026-01-26T13:36:34Z",
    "updated": "2026-01-26T13:36:34Z",
    "comment": "Accepted for publication at EACL main conference",
    "light_analysis": {
      "overview": "该论文提出评估框架，揭示大语言模型在细粒度多概念控制中性能下降的局限性。",
      "motivation": "研究动机在于许多实际应用需要对文本概念（如幽默、说服力）进行细粒度控制，以提高模型输出的精确性和适应性。大语言模型虽具有强大生成能力，但现有方法（如提示和表示工程）仅能提供粗糙或单属性控制，多属性设置的系统性评估有限，这限制了模型在复杂场景中的应用。因此，需要解决多概念控制评估的不足，以推动更有效的控制方法发展。",
      "method": "论文引入了一个评估框架，专门用于评估单概念和双概念场景下的细粒度可控性。方法聚焦于语言上不同的概念对（如说服力与幽默），并通过多个大语言模型和生成任务进行系统测试。关键创新在于提供了一个标准化的评估方案，用于量化和比较模型在组合概念时的表现，强调了概念组合性的测量，而不依赖于具体数据集或模型架构的细节。",
      "result": "实验结果显示，在多个大语言模型和生成任务中，模型在双概念设置下的性能经常下降，尽管所选概念在理论上应该是可分离的。这表明基于朴素提示的控制方法存在根本局限性，模型在处理概念组合性时遇到困难。与单概念设置相比，性能下降揭示了当前方法在多属性控制方面的不足，为未来改进提供了实证基础。",
      "conclusion": "论文的主要贡献是提出了一个评估框架，为细粒度多概念控制提供了系统性证据，揭示了朴素提示控制的局限性。这一研究具有重要学术价值，为测量和开发未来多概念控制方法确立了原则性基础。潜在局限性包括概念组合性的复杂性，未来工作可以探索更有效的控制策略，以应对更复杂的应用需求。",
      "tags": [
        "Large Language Models",
        "Fine-Grained Control",
        "Multi-Concept Control",
        "Prompting",
        "Representation Engineering"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:13.388668Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18479",
    "title": "Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States",
    "authors": [
      "Kyoleen Kwak",
      "Hyoseok Hwang"
    ],
    "abstract": "Deep reinforcement learning has proven to be a powerful approach to solving control tasks, but its characteristic high-frequency oscillations make it difficult to apply in real-world environments. While prior methods have addressed action oscillations via architectural or loss-based methods, the latter typically depend on heuristic or synthetic definitions of state similarity to promote action consistency, which often fail to accurately reflect the underlying system dynamics. In this paper, we propose a novel loss-based method by introducing a transition-induced similar state. The transition-induced similar state is defined as the distribution of next states transitioned from the previous state. Since it utilizes only environmental feedback and actually collected data, it better captures system dynamics. Building upon this foundation, we introduce Action Smoothing by Aligning Actions with Predictions from Preceding States (ASAP), an action smoothing method that effectively mitigates action oscillations. ASAP enforces action smoothness by aligning the actions with those taken in transition-induced similar states and by penalizing second-order differences to suppress high-frequency oscillations. Experiments in Gymnasium and Isaac-Lab environments demonstrate that ASAP yields smoother control and improved policy performance over existing methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18479.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18479",
    "published": "2026-01-26T13:34:34Z",
    "updated": "2026-01-26T13:34:34Z",
    "comment": "Accepted at AAAI-26. 7 pages (excluding references), 3 figures",
    "light_analysis": {
      "overview": "论文提出ASAP方法，通过利用转移诱导相似状态来增强深度强化学习中控制策略的平滑性。",
      "motivation": "深度强化学习在控制任务中表现优异，但其高频动作振荡问题限制了在现实世界环境中的应用，如机器人控制。现有基于损失的方法依赖启发式或合成定义的状态相似性来促进动作一致性，但这些定义往往不能准确反映系统动态，导致效果不佳。因此，开发一种更准确的方法来建模环境反馈，以提升动作平滑性和策略鲁棒性，具有重要的实际意义。",
      "method": "本研究提出ASAP方法，其核心创新是引入“转移诱导相似状态”，定义为从前一状态转移的下一个状态分布，利用环境反馈和实际收集数据来更精确捕捉系统动态。ASAP通过将当前动作与该状态中的动作对齐，并施加二阶差分惩罚来抑制高频振荡，从而强制动作平滑。实验在Gymnasium和Isaac-Lab模拟环境中进行，无需额外架构修改。",
      "result": "在Gymnasium和Isaac-Lab环境中的实验表明，ASAP方法能有效减少动作振荡，产生比现有方法更平滑的控制策略，并在策略性能上实现改进。具体性能指标摘要未明确说明，但对比结果显示ASAP在动作一致性和控制效果上优于基线方法，展示了其实际应用潜力。",
      "conclusion": "本研究贡献了基于转移诱导相似状态的ASAP方法，显著提升深度强化学习中控制策略的平滑性，通过更准确建模系统动态克服了现有损失方法的局限。这增强了强化学习在实际控制任务中的适用性，具有理论和应用价值。未来工作可探索在更复杂环境或与其他技术结合的扩展，摘要未明确说明具体局限性。",
      "tags": [
        "Deep Reinforcement Learning",
        "Action Smoothing",
        "Transition-induced Similar State",
        "Second-order Difference Penalty",
        "Control Policy"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:10.214415Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18468",
    "title": "Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models",
    "authors": [
      "Daniel B. Hier",
      "Tayo Obafemi-Ajayi"
    ],
    "abstract": "Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18468.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18468",
    "published": "2026-01-26T13:15:23Z",
    "updated": "2026-01-26T13:15:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究表明，潜藏知识能预测微调大型语言模型时事实获取的速度和泛化能力。",
      "motivation": "大型语言模型在预训练后存储生物医学事实存在不均匀性，部分事实以潜藏知识形式存在，难以通过确定性解码可靠访问。现有方法未充分考虑潜藏知识对微调过程的影响，这限制了模型在特定领域（如生物医学）的知识获取效率和泛化能力。因此，本研究旨在探索潜藏知识如何预测事实学习的动态过程，解决知识可访问性问题，以提升模型微调效果。",
      "method": "研究方法包括微调Llama 3.1 8B Instruct模型，学习Human Phenotype Ontology的800对映射和Gene Ontology的400训练对，保留400 GO对测试泛化。采用随机解码技术检测基线潜藏知识，并应用Cox比例风险模型将学习建模为跨20个epoch的时间到事件过程，分析获取、泛化和退化的预测因子。关键创新在于结合随机解码量化潜藏知识，并使用生存分析模型识别影响因素。",
      "result": "实验结果显示，基线时Human Phenotype Ontology的确定性召回率仅为2.8%，经微调后显著提升至71.9%。潜藏知识是最强预测因子，风险比HR为2.6，与更早达到高峰学习率和更快收敛相关。泛化到保留Gene Ontology事实的概率为5.8%，但在潜藏知识存在时更高。此外，未训练术语的映射比训练术语更易退化，表明训练期间的强化具有保护效应。",
      "conclusion": "本研究结论表明，潜藏知识能有效预测微调中事实获取的速度和有限泛化能力，强调了知识存储机制的重要性。学术价值在于揭示了潜藏知识对模型学习动态的影响，实际应用可指导生物医学模型的优化训练。局限性包括泛化能力有限，未来工作可扩展至更多数据集和模型类型，以验证泛用性。",
      "tags": [
        "Large Language Model",
        "Fine-Tuning",
        "Latent Knowledge",
        "Cox Proportional Hazards Model",
        "Stochastic Decoding"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:26.651643Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18467",
    "title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents",
    "authors": [
      "Yuhang Zhou",
      "Kai Zheng",
      "Qiguang Chen",
      "Mengkang Hu",
      "Qingfeng Sun",
      "Can Xu",
      "Jingjing Chen"
    ],
    "abstract": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18467.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18467",
    "published": "2026-01-26T13:13:59Z",
    "updated": "2026-01-26T13:13:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一个完全开源的离线训练套件，证明了无需昂贵在线强化学习即可构建高性能深度研究代理。",
      "motivation": "深度研究代理在长视距任务中潜力巨大，但当前顶尖性能依赖于在线强化学习，这导致高昂的API调用成本。离线训练虽更高效，却受限于高质量研究轨迹的稀缺性。本研究旨在解决在线RL的经济负担问题，通过探索离线训练方案，为构建经济高效的深度研究代理提供新途径。现有方法主要依赖在线RL，缺乏大规模离线数据集，因此开发高质量离线资源成为关键挑战。",
      "method": "本论文提出一个完全开源套件，核心包括DeepForge任务合成框架，能生成大规模研究查询而无需重型预处理。此外，收集了66k问答对、33k监督微调轨迹和21k直接偏好优化对，这些数据用于离线训练OffSeeker模型（一个8B参数的系统）。方法强调通过资源合成和数据集构建，实现完全离线训练，避免了依赖在线RL的策略迭代和成本。",
      "result": "在六个基准测试上进行广泛评估，OffSeeker在相似规模代理中表现领先，并能与30B参数、通过重型在线强化学习训练的系统竞争。尽管摘要未提供具体准确率或效率改进数据，但结果表明，离线训练的代理能达到或超越在线RL系统的性能，突出了资源合成和数据集建设的有效性。与基线方法相比，OffSeeker展示了离线训练的可行性和成本优势。",
      "conclusion": "本研究的主要贡献是提供了一个开源离线训练套件，包括DeepForge框架和高质量数据集，证明了深度研究代理可以不依赖昂贵在线强化学习。学术上，它推动了离线训练方法在复杂任务中的应用；实际中，降低了开发成本，提高了可扩展性。潜在局限性可能包括数据合成的泛化能力，未来工作可扩展数据集或优化训练策略。",
      "tags": [
        "Offline Reinforcement Learning",
        "Task Synthesis",
        "Large Language Models",
        "Deep Research Agents"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:04.104769Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18464",
    "title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System",
    "authors": [
      "Wenbin Wei",
      "Suyuan Yao",
      "Cheng Huang",
      "Xiangyu Gao"
    ],
    "abstract": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18464.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18464",
    "published": "2026-01-26T13:12:24Z",
    "updated": "2026-01-26T13:12:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "Fair-Eye Net 是一个公平、可靠的多模态 AI 系统，通过整合多源数据和优化公平性，实现青光眼从筛查到风险警报的全链条管理。",
      "motivation": "青光眼是全球不可逆盲的主要原因，早期检测和纵向随访对预防永久性视力损失至关重要。当前筛查和进展评估依赖单一测试或松散链接的检查，导致主观性强和护理碎片化问题，同时高质量成像工具和专家知识访问有限，在现实使用中进一步加剧一致性和公平性挑战。因此，需开发一个更公平、集成的 AI 系统来改善青光眼护理的效率和公平性。",
      "method": "论文提出 Fair-Eye Net，整合眼底照片、OCT 结构指标、VF 功能指数和人口统计因素，采用双流异构融合架构进行多模态数据融合。关键创新包括不确定性感知的分层门控策略，用于选择性预测和安全转诊，以及通过公平约束减少弱势亚组的漏诊。该方法使用多任务学习优化公平性和临床可靠性，确保系统在复杂临床场景中的适用性。",
      "result": "实验结果显示，Fair-Eye Net 在青光眼检测中达到 AUC 0.912（特异性 96.7%），将种族假阴性差异减少 73.4%（从 12.31% 降至 3.28%），并保持稳定的跨域性能。系统实现 3-12 个月的早期风险警报，灵敏度 92%，特异性 88%，优于传统方法，表明其在提高检测精度和公平性方面的有效性。",
      "conclusion": "Fair-Eye Net 通过多模态整合和公平优化，为青光眼管理提供了一个全链条 AI 解决方案，其创新在于将公平作为主要目标并具有临床可靠性。研究推进了临床转化和大规模部署，有助于提升全球眼健康公平；未来工作可扩展到其他疾病或进一步改进技术细节。",
      "tags": [
        "Multimodal Fusion",
        "Fairness Constraint",
        "Hierarchical Gating",
        "Medical Imaging",
        "Multitask Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:22.687862Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18451",
    "title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control",
    "authors": [
      "Xuanmeng Sha",
      "Liyun Zhang",
      "Tomohiro Mashita",
      "Naoya Chiba",
      "Yuki Uranishi"
    ],
    "abstract": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18451.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18451",
    "published": "2026-01-26T12:57:36Z",
    "updated": "2026-01-26T12:57:36Z",
    "comment": "13 pages, 5 figures",
    "light_analysis": {
      "overview": "3DGesPolicy是一个基于动作控制的新框架，通过扩散政策和GAP融合模块生成语义一致的整体协同语音手势。",
      "motivation": "现有手势生成方法，如部分分解或帧级回归，在处理整体协同语音手势时面临语义不一致和空间不稳定的挑战，导致生成的手势不自然且无意义。这一问题在人机交互和虚拟角色应用中尤为重要，因为协调的身体动作和面部表情对于增强表达力和用户体验至关重要，因此需要新方法来改进多模态对齐和运动一致性。",
      "method": "论文提出3DGesPolicy框架，将整体手势生成重新表述为连续轨迹控制问题，借鉴机器人学中的扩散政策。该方法建模帧间变化为统一的整体动作，学习帧间运动模式以确保空间和语义一致的运动轨迹。关键创新包括动作基础框架和Gesture-Audio-Phoneme (GAP)融合模块，后者深度集成手势、音频和音素信号，实现结构化细粒度对齐。实验在BEAT2数据集上进行，模型基于动作控制原理。",
      "result": "在BEAT2数据集上的定量和定性实验显示，3DGesPolicy在生成整体协同语音手势方面优于其他最先进方法。它能够生成更自然、表达力更强且高度语音对齐的手势，有效解决语义和空间一致性问题。与基线方法相比，该方法提升了手势与语音的协调性，显示出显著优势，但摘要未明确说明具体性能数据。",
      "conclusion": "本研究的主要贡献是提出3DGesPolicy框架和GAP融合模块，改进了整体手势生成的一致性和对齐性。学术上，通过引入扩散政策和多模态融合技术，推动了手势生成领域的发展。实际应用方面，它可用于虚拟现实、游戏角色和机器人交互，提升用户体验。未来工作可能包括扩展到其他任务或优化算法，但摘要未明确说明。",
      "tags": [
        "Co-Speech Gesture Generation",
        "Diffusion Policy",
        "Multimodal Fusion",
        "Action Control",
        "Phoneme Awareness"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:19.642384Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18448",
    "title": "On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics",
    "authors": [
      "Lloyd Austin Courtenay"
    ],
    "abstract": "Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust \"diagonal\" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18448.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18448",
    "published": "2026-01-26T12:56:23Z",
    "updated": "2026-01-26T12:56:23Z",
    "comment": "17 pages, 5 figures, Preprint pending review",
    "light_analysis": {
      "overview": "论文提出了一种新的对齐方法，解决了几何形态测量学在机器学习应用中由广义Procrustes分析引起的统计污染问题。",
      "motivation": "研究动机在于几何形态测量学被广泛用于量化形状变化，并作为机器学习分析的输入。当前标准做法是在分割训练和测试数据前使用广义Procrustes分析对齐标本，但这可能引入统计依赖，污染下游预测模型。由于这种方法会影响模型准确性，且现有方法未充分考虑数据分割时的独立性，因此需要解决这一预处理问题以确保机器学习应用的可靠性。",
      "method": "研究方法通过受控的2D和3D模拟来正式表征广义Procrustes分析引起的污染效应，模拟涵盖不同样本大小、标志点密度和异速生长模式。核心创新是提出一种新的对齐程序，将测试标本对齐到训练集，以消除跨样本依赖性，确保统计独立性。此外，使用线性和卷积回归模型来评估标志点空间自相关的影响，强调了忽略标志点关系时模型的性能变化，从而全面分析预处理步骤的重要性。",
      "result": "实验结果显示，模拟揭示了样本大小与标志点空间中的“对角线”效应，这反映了各向同性变异下RMSE的缩放特性，斜率通过Procrustes切空间的自由度解析推导。使用线性和卷积回归模型进一步证明了标志点空间自相关的重要性，表明当忽视这些关系时，模型性能会下降，例如预测准确率可能受损。与标准对齐方法相比，新提出的对齐程序能有效减少统计污染，提升模型鲁棒性。",
      "conclusion": "结论强调了在几何形态测量学应用于机器学习时仔细预处理的重要性，提供了消除统计污染的对齐实用指南，并阐明了Procrustes形状空间固有的统计约束。研究具有学术价值，为相关领域提供了理论框架和实践建议，有助于改进机器学习模型的构建。潜在局限性可能包括模拟条件的特定性，未来工作可扩展到更多实际数据集和复杂模型应用。",
      "tags": [
        "Geometric Morphometrics",
        "Generalized Procrustes Analysis",
        "Machine Learning",
        "Simulation",
        "Regression Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:09.249706Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18447",
    "title": "GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level",
    "authors": [
      "Jinlong Hu",
      "Jiacheng Liu"
    ],
    "abstract": "Deep graph learning models have demonstrated remarkable capabilities in processing graph-structured data and have been widely applied across various fields. However, their complex internal architectures and lack of transparency make it difficult to explain their decisions, resulting in opaque models that users find hard to understand and trust. In this paper, we explore model-level explanation techniques for deep graph learning models, aiming to provide users with a comprehensive understanding of the models' overall decision-making processes and underlying mechanisms. Specifically, we address the problem of counterfactual explanations for deep graph learning models by introducing a generative model-level counterfactual explanation approach called GCFX, which is based on deep graph generation. This approach generates a set of high-quality counterfactual explanations that reflect the model's global predictive behavior by leveraging an enhanced deep graph generation framework and a global summarization algorithm. GCFX features an architecture that combines dual encoders, structure-aware taggers, and Message Passing Neural Network decoders, enabling it to accurately learn the true latent distribution of input data and generate high-quality, closely related counterfactual examples. Subsequently, a global counterfactual summarization algorithm selects the most representative and comprehensive explanations from numerous candidate counterfactuals, providing broad insights into the model's global predictive patterns. Experiments on a synthetic dataset and several real-world datasets demonstrate that GCFX outperforms existing methods in terms of counterfactual validity and coverage while maintaining low explanation costs, thereby offering crucial support for enhancing the practicality and trustworthiness of global counterfactual explanations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18447.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18447",
    "published": "2026-01-26T12:56:01Z",
    "updated": "2026-01-26T12:56:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "GCFX提出一种基于深度图生成的生成模型级反事实解释方法，用于增强深度图学习模型的可解释性和全局理解。",
      "motivation": "深度图学习模型在处理图结构数据时表现优异，但其内部架构复杂且不透明，导致决策过程难以解释，降低了用户的理解和信任。尽管现有解释技术可能在局部层面提供一些洞察，但在模型级别上缺乏全面的全局解释，限制了模型的实用性和部署。因此，开发模型级解释方法，以揭示模型的整体决策机制和底层逻辑，成为提升模型可信度的关键需求。",
      "method": "本文引入GCFX方法，一个生成模型级反事实解释框架。它基于增强的深度图生成技术，采用双编码器、结构感知标记器和消息传递神经网络解码器相结合的架构，以准确学习输入数据的潜在分布并生成高质量、与模型预测相关的反事实例子。此外，通过全局反事实总结算法，从众多候选解释中筛选出最具代表性和全面的示例，从而提供对模型全局预测模式的广泛洞察，突出了其生成和总结的双重创新点。",
      "result": "实验在合成数据集和多个真实数据集上进行，结果表明GCFX在反事实有效性和覆盖范围方面显著优于现有基准方法，同时保持了较低的解释成本。具体而言，GCFX生成的解释能更好地反映深度图模型的全局预测行为，验证了其在提升解释实用性和增强模型信任度方面的优势，但摘要未提供具体的准确率或效率数据。",
      "conclusion": "GCFX通过生成高质量的模型级反事实解释，有效解决了深度图学习模型的解释性问题，提升了用户对模型决策的理解和信任。该方法在反事实有效性和覆盖范围上的优越表现，为全局解释提供了重要工具，具有广泛的学术和应用价值。未来工作可能包括扩展到更多图模型类型或优化解释效率，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Counterfactual Explanations",
        "Deep Graph Generation",
        "Message Passing Neural Network",
        "Model-level Explanation",
        "Graph Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:09.461047Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18420",
    "title": "Gradient Regularized Natural Gradients",
    "authors": [
      "Satya Prakash Dash",
      "Hossein Abdi",
      "Wei Pan",
      "Samuel Kaski",
      "Mingfei Sun"
    ],
    "abstract": "Gradient regularization (GR) has been shown to improve the generalizability of trained models. While Natural Gradient Descent has been shown to accelerate optimization in the initial phase of training, little attention has been paid to how the training dynamics of second-order optimizers can benefit from GR. In this work, we propose Gradient-Regularized Natural Gradients (GRNG), a family of scalable second-order optimizers that integrate explicit gradient regularization with natural gradient updates. Our framework provides two complementary algorithms: a frequentist variant that avoids explicit inversion of the Fisher Information Matrix (FIM) via structured approximations, and a Bayesian variant based on a Regularized-Kalman formulation that eliminates the need for FIM inversion entirely. We establish convergence guarantees for GRNG, showing that gradient regularization improves stability and enables convergence to global minima. Empirically, we demonstrate that GRNG consistently enhances both optimization speed and generalization compared to first-order methods (SGD, AdamW) and second-order baselines (K-FAC, Sophia), with strong results on vision and language benchmarks. Our findings highlight gradient regularization as a principled and practical tool to unlock the robustness of natural gradient methods for large-scale deep learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18420.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18420",
    "published": "2026-01-26T12:25:04Z",
    "updated": "2026-01-26T12:25:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出梯度正则化自然梯度（GRNG），一种结合梯度正则化与自然梯度更新的可扩展二阶优化器，以提升优化速度和模型泛化能力。",
      "motivation": "该研究旨在解决二阶优化器在训练动态中未充分利用梯度正则化（GR）的问题。GR能改善模型泛化，自然梯度下降（NGD）在优化初期加速，但现有方法很少关注如何将两者结合，这限制了自然梯度方法在大规模深度学习中的稳健性和效率。因此，需要开发一种方法集成GR与二阶优化器，以提高稳定性和泛化性能。",
      "method": "研究提出梯度正则化自然梯度（GRNG）框架，包含两种互补算法：频繁变体通过结构化近似避免Fisher信息矩阵（FIM）的显式求逆，提升计算效率；贝叶斯变体基于正则化卡尔曼公式，完全消除FIM求逆需求。核心创新是结合显式梯度正则化与自然梯度更新，形成可扩展的二阶优化器，适用于大规模深度学习任务如视觉和语言处理。摘要未明确说明具体数据集或模型架构。",
      "result": "实验结果表明，GRNG在优化速度和泛化能力方面一致优于一阶方法（如SGD、AdamW）和二阶基线（如K-FAC、Sophia），在视觉和语言基准测试中表现出强劲性能。摘要未明确说明具体性能指标数值，但收敛保证显示梯度正则化能提高训练稳定性并促进收敛到全局最小值。",
      "conclusion": "该研究的主要贡献是提出了GRNG框架，将梯度正则化与自然梯度更新相结合，并验证了其收敛保证和实证效果。学术价值在于阐明了梯度正则化对优化稳定性的积极作用；实际应用中，GRNG提供了原则性和实用的工具，以解锁自然梯度方法在大规模深度学习中的稳健性。潜在局限性可能涉及计算复杂度，未来工作可优化算法效率或探索更多应用场景。",
      "tags": [
        "Gradient Regularization",
        "Natural Gradient Descent",
        "Second-Order Optimization",
        "Kalman Filter",
        "Fisher Information Matrix"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:34.900740Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18415",
    "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
    "authors": [
      "Ivan Bondarenko",
      "Daniil Grebenkin",
      "Oleg Sedukhin",
      "Mikhail Klementev",
      "Roman Derunets",
      "Lyudmila Budneva"
    ],
    "abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18415.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18415",
    "published": "2026-01-26T12:14:51Z",
    "updated": "2026-01-26T12:14:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出Pisets语音识别系统，通过结合Wav2Vec2、AST和Whisper的三组件架构，提高了对讲座和访谈场景下长音频识别的鲁棒性和准确性。",
      "motivation": "现有语音识别模型如Whisper在处理长音频和多样声学条件时容易产生错误和幻觉，影响转录质量，特别是在科学家和记者处理讲座、访谈等实际应用中。这一问题重要，因为准确转录有助于提高信息处理和传播效率。现有方法的不足在于单一模型如Whisper可能在复杂环境下表现不稳定，因此需要开发更鲁棒的系统来最小化误差，增强实际应用价值。",
      "method": "研究采用三组件架构：首先使用Wav2Vec2进行初级语音识别，捕捉音频特征；然后通过Audio Spectrogram Transformer (AST)过滤误报，提高识别可靠性；最后利用Whisper完成最终语音转文本。关键创新点包括实施课程学习方法优化训练过程，利用多样化的俄语语音语料库增强模型泛化能力，并引入先进的不确定性建模技术以减少错误和幻觉，从而提升系统整体性能。",
      "result": "通过比较Pisets系统与基线方法WhisperX和标准Whisper模型，摘要表明该系统在各种声学条件下对长音频数据表现出更强的鲁棒性，显著减少了错误和幻觉，转录质量得到进一步改进。摘要未明确说明具体性能指标如准确率提升数据，但提到系统效果显著增强，推断方法在减少误差和提高准确性方面优于基线。",
      "conclusion": "本研究的主要贡献是提出Pisets系统，通过整合Wav2Vec2、AST和Whisper架构，并结合课程学习和不确定性建模，提升了语音识别在讲座和访谈场景下的鲁棒性。学术上，该方法为语音识别领域提供了新的混合架构思路；应用上，直接服务于科学家和记者的实际需求，提高转录效率。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Wav2Vec2",
        "Audio Spectrogram Transformer",
        "Whisper",
        "Curriculum Learning",
        "Uncertainty Modeling"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:29.301475Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18414",
    "title": "Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings",
    "authors": [
      "Aura Loredana Dan"
    ],
    "abstract": "Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18414.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18414",
    "published": "2026-01-26T12:12:24Z",
    "updated": "2026-01-26T12:12:24Z",
    "comment": "9 pages, 8 figures",
    "light_analysis": {
      "overview": "本文比较评估了MobileNet、EfficientNet和VGG16三种深度学习模型在儿童绘画情感状态识别中的表现，揭示了轻量级与深层架构在移动应用中的权衡。",
      "motivation": "研究旨在解决自闭症谱系障碍（ASD）儿童早期情感状态识别的挑战。由于ASD儿童在情感表达和沟通上存在困难，传统评估方法如观察或访谈往往具有侵入性、主观性，且难以一致应用，这限制了早期干预的有效性。因此，开发基于儿童绘画的非侵入性、客观情感识别方法至关重要，可为临床诊断和个性化支持提供辅助，弥补现有方法的不足。",
      "method": "论文采用比较评估框架，选取MobileNet、EfficientNet和VGG16三种深度学习架构，通过迁移学习在心理专家标注的儿童绘画数据集上进行训练。核心创新在于统一实验设置，系统分析分类性能、鲁棒性和计算效率，特别关注轻量级模型在移动和实时应用场景下的适用性，从而为模型选择提供实证依据。",
      "result": "实验结果表明，在儿童绘画情感分类任务中，MobileNet等轻量级模型在计算效率上表现优异，适合移动和实时应用，而VGG16等深层模型可能提供更高的分类精度，但计算成本较高。这些发现揭示了模型选择时需权衡性能与效率，但摘要未明确说明具体准确率或性能指标数据，仅强调了架构间的关键折衷。",
      "conclusion": "本研究通过比较评估三种深度学习模型，为基于儿童绘画的情感状态识别提供了方法论参考，揭示了轻量级与深层架构在情感计算任务中的权衡。其学术价值在于推动非侵入性情感识别技术的发展，实际应用价值高，可辅助ASD儿童早期诊断和干预。未来工作可扩展数据集规模或探索更多模型架构以进一步优化性能。",
      "tags": [
        "Affective State Recognition",
        "Machine Learning",
        "Transfer Learning",
        "MobileNet",
        "EfficientNet"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:21.361536Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18409",
    "title": "Frequency-Based Hyperparameter Selection in Games",
    "authors": [
      "Aniket Sanyal",
      "Baraah A. M. Sidahmed",
      "Rebekka Burkholz",
      "Tatjana Chavdarova"
    ],
    "abstract": "Learning in smooth games fundamentally differs from standard minimization due to rotational dynamics, which invalidate classical hyperparameter tuning strategies. Despite their practical importance, effective methods for tuning in games remain underexplored. A notable example is LookAhead (LA), which achieves strong empirical performance but introduces additional parameters that critically influence performance. We propose a principled approach to hyperparameter selection in games by leveraging frequency estimation of oscillatory dynamics. Specifically, we analyze oscillations both in continuous-time trajectories and through the spectrum of the discrete dynamics in the associated frequency-based space. Building on this analysis, we introduce \\emph{Modal LookAhead (MoLA)}, an extension of LA that selects the hyperparameters adaptively to a given problem. We provide convergence guarantees and demonstrate in experiments that MoLA accelerates training in both purely rotational games and mixed regimes, all with minimal computational overhead.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18409.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18409",
    "published": "2026-01-26T12:06:59Z",
    "updated": "2026-01-26T12:06:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于频率估计的自适应超参数选择方法Modal LookAhead，用于加速平滑游戏中的训练过程。",
      "motivation": "在平滑游戏中学习与标准最小化问题不同，由于旋转动力学的存在，传统超参数调优策略失效。尽管超参数选择在实践中至关重要，但针对游戏的有效方法研究不足。现有方法如LookAhead虽表现良好，但引入了额外参数，对性能有重要影响，因此亟需一种更高效的自适应调参策略来解决这一问题。",
      "method": "该方法通过分析连续时间轨迹中的振荡行为和离散动力学在频率空间的谱特征，来估计游戏的振荡动态。在此基础上，引入了Modal LookAhead (MoLA)，作为LookAhead的扩展，能够根据具体问题自适应地选择超参数。该方法提供了理论收敛保证，并利用频率分析优化参数设置，确保在游戏学习中的高效应用。",
      "result": "在实验中，MoLA被证明能够加速训练过程，在纯旋转游戏和混合制度中都表现良好，且计算开销最小。摘要未明确提供具体性能指标如准确率提升的数值，但表明与现有方法相比，MoLA在训练效率上有改进，实验验证了方法的有效性。",
      "conclusion": "本研究的主要贡献是提出了一种基于频率的自适应超参数选择方法Modal LookAhead，解决了游戏学习中超参数调优的挑战。该方法结合了理论分析和实验验证，提供了收敛保证，并在实践中实现了训练加速和低计算开销。学术上，它为游戏优化提供了新思路；应用上，有助于提高机器学习模型的训练效率。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Hyperparameter Selection",
        "Smooth Games",
        "Frequency Estimation",
        "Modal LookAhead",
        "Game Dynamics"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:29.880187Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18407",
    "title": "Larger than memory image processing",
    "authors": [
      "Jon Sporring",
      "David Stansby"
    ],
    "abstract": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18407.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18407",
    "published": "2026-01-26T12:02:41Z",
    "updated": "2026-01-26T12:02:41Z",
    "comment": "10 pages",
    "light_analysis": {
      "overview": "提出一种基于流的图像处理架构和领域特定语言（DSL），用于高效处理大于内存的petascale数据集，通过最小化磁盘I/O和优化内存使用，实现在有限RAM机器上的近线性性能。",
      "motivation": "该研究旨在解决处理petascale图像数据集（如1.4 PB电子显微镜体积和150 TB人类器官图谱）时的性能瓶颈。由于数据规模远超内存容量，性能本质上受I/O限制。现有方法中，2D切片和3D分块布局在处理依赖邻接值访问的算法时存在不足：2D切片可能导致1D流处理受限，而3D分块在扫描时需多次访问同一块数据，增加冗余I/O。因此，需要一种新方法来优化数据访问模式，减少磁盘操作，提高大规模图像分析的效率。",
      "method": "论文提出了一种基于流的处理架构，支持两种常见图像表示：2D切片（如目录或多页TIFF）和3D分块布局（如Zarr/HDF5）。核心方法包括sweep-based执行（遵循自然2D/3D顺序）、窗口化操作和重叠感知分块，以最小化冗余数据访问。关键创新是引入一个领域特定语言（DSL），该语言编码算法时嵌入最优流和内存使用知识。DSL在编译时和运行时进行管道分析，自动选择窗口大小、融合处理阶段、管理数据流（tee和zip），并调度处理pass，从而在有限RAM机器上实现近线性I/O扫描和可预测的内存占用，无需全量数据驻留内存。",
      "result": "摘要未明确说明具体实验数据，但指出该方法为极大图像带来显著的吞吐量提升，无需全量数据驻留内存。通过优化I/O访问模式，实现了近线性的扫描性能，并减少了内存占用。与基线方法相比，特别是对于依赖邻接值的算法，流架构通过1D扫描（仅需2种扫描顺序）减少了磁盘访问次数，而3D分块可能导致每个块被访问至少9次，从而提高了效率。具体性能指标如准确率或效率提升数值未在摘要中详细说明。",
      "conclusion": "该研究的主要贡献是开发了一套完整的流处理框架，通过sweep-based执行和DSL自动化优化，解决了大规模图像处理中的I/O瓶颈问题。学术价值在于为图像分析提供了可扩展的解决方案，尤其在处理petascale数据集时；实际应用价值体现在整合现有分割和形态学工具，将预处理和后处理重构为特权顺序读/写模式的管道，增强处理能力。局限性可能包括对特定算法类型的适应性，未来工作可扩展DSL支持更广泛的算法或进一步优化资源管理。",
      "tags": [
        "Streaming Architecture",
        "Sweep-based Execution",
        "Domain-Specific Language",
        "I/O Optimization",
        "Large-Scale Image Processing"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:17.904272Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18401",
    "title": "Superlinear Multi-Step Attention",
    "authors": [
      "Yufeng Huang"
    ],
    "abstract": "In this paper, we propose \\textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \\textbf{random context access} (a.k.a.\\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18401.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18401",
    "published": "2026-01-26T11:58:42Z",
    "updated": "2026-01-26T11:58:42Z",
    "comment": "30 pages, 6 figures",
    "light_analysis": {
      "overview": "论文提出了一种名为Superlinear attention的多步注意力架构，实现了长序列的次二次复杂度并保留随机上下文访问。",
      "motivation": "标准自注意力在处理长序列时具有二次复杂度O(L^2)，导致计算效率低下，限制了长上下文任务的应用。Superlinear attention旨在解决这一瓶颈，通过降低复杂度同时避免结构性地排除某些令牌位置，以提升长序列处理的效率和灵活性，克服现有方法在处理超长上下文时的不足。",
      "method": "论文将标准因果自注意力重新表述为多步搜索问题，步骤数为N，整体复杂度为O(L^{1+1/N})。具体实现中，采用N=2的基线架构，第一步进行O(L^{3/2})的span-search选择相关序列跨度，第二步应用O(L^{3/2})的span-attention。该方法基于修改的30B混合MoE模型在单个B200 GPU上实现，关键创新在于多步搜索机制和随机上下文访问的保留。",
      "result": "在O(L^{1.54})配置下，Superlinear attention实现了高解码吞吐量：在1M上下文长度下为114 tokens/sec，在10M上下文长度下为80 tokens/sec。在NIAH任务中，在256K上下文长度下表现出色，验证了路由跨度选择的端到端可学习性。摘要未明确说明与基线方法的直接对比，但强调了复杂度降低带来的效率提升。",
      "conclusion": "论文的主要贡献是提出了Superlinear attention架构，通过多步搜索实现了长序列的次二次复杂度，并保留了随机上下文访问，为长上下文处理提供了新的系统可行性方法。该研究在扩展性和效率方面展示了潜力，但全面的质量评估跨多样长上下文任务仍需未来工作，潜在局限性可能包括对具体任务适配性的进一步探索。",
      "tags": [
        "Superlinear Attention",
        "Multi-Step Search",
        "Random Context Access",
        "Subquadratic Complexity",
        "Span-Search"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:16.811139Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18399",
    "title": "Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach",
    "authors": [
      "Mehmet Velioglu",
      "Song Zhai",
      "Alexander Mitsos",
      "Adel Mhamdi",
      "Andreas Jupke",
      "Manuel Dahmen"
    ],
    "abstract": "Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18399.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18399",
    "published": "2026-01-26T11:57:28Z",
    "updated": "2026-01-26T11:57:28Z",
    "comment": "37 pages, 13 figures, 3 tables",
    "light_analysis": {
      "overview": "提出一种两阶段训练的物理信息神经网络方法，结合合成数据和少量实验数据，仅基于体积流量测量估计液-液分离中的密集填充区高度。",
      "motivation": "在化学、制药和回收过程中，分离液-液分散在重力沉降器中至关重要。密集填充区高度是性能和安全的關鍵指標，但由于光学限制，直接测量成本高昂且不切實際。现有方法依赖昂贵的光学测量，需要大量实验数据，导致效率低下。因此，开发一种仅使用低成本流量测量的估计方法，具有重要实践意义，旨在克服测量难题并提高过程监控的可行性。",
      "method": "论文提出使用物理信息神经网络（PINN），首先在合成数据和从低保真机理模型推导的物理方程上进行预训练，以减少对大量实验数据的依赖。然后，用稀缺的实验数据微调PINN，以捕获分离器的实际动态。关键创新在于将可微分的PINN整合到扩展卡尔曼滤波器启发的状态估计框架中，作为预测模型，从流量测量中实时跟踪和更新相高度。仅使用体积平衡方程，避免计算密集的液滴聚结和沉降子模型集成，降低计算负担。",
      "result": "实验结果显示，两阶段训练的PINN在从已知初始状态的前向模拟中，比机理模型和非预训练PINN表现更优。在使用滤波器的状态估计中，与两阶段训练的纯数据驱动神经网络相比，PINN提供了最准确的相高度估计。所有模型通过集成训练和评估，以考虑参数不确定性。在所有测试中，PINN均获得最高准确度，证实了其有效性和稳健性。",
      "conclusion": "该研究的主要贡献在于提出了一种结合物理信息和少量实验数据的PINN方法，实现了对液-液分离中密集填充区高度的准确估计。学术上，展示了PINN在复杂工业过程中的应用潜力；实际上，提供了一种低成本、高效的测量替代方案，减少了数据依赖。局限性可能包括对合成数据生成机制的依赖，未来工作可优化物理模型集成或探索更广泛的应用场景。",
      "tags": [
        "Physics-Informed Neural Network (PINN)",
        "Synthetic Data Training",
        "State Estimation",
        "Extended Kalman Filter",
        "Liquid-Liquid Separation"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:30.411651Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18395",
    "title": "Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction",
    "authors": [
      "Mikel Zubillaga",
      "Oscar Sainz",
      "Oier Lopez de Lacalle",
      "Eneko Agirre"
    ],
    "abstract": "Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18395.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18395",
    "published": "2026-01-26T11:53:08Z",
    "updated": "2026-01-26T11:53:08Z",
    "comment": "Submitted to IJCAI-ECAI 2026",
    "light_analysis": {
      "overview": "本文提出ThinkTwice框架，通过采样和选择策略改进文档级信息提取，超越贪婪解码方法，并引入无监督和有监督选择模块。",
      "motivation": "文档级信息提取（DocIE）的标准做法通常使用贪婪解码以避免输出变异性，但这可能导致次优解，限制了性能提升。本研究旨在解决这一问题，认为输出变异性在结合推理模型时可被利用来生成更优解决方案。现有方法忽视了采样的潜力，因此需要开发一种框架来有效利用多候选输出，选择最佳模板，从而提升提取准确性和鲁棒性。",
      "method": "ThinkTwice框架包括采样和选择两个阶段：首先，使用大型语言模型（LLM）为给定文档生成多个候选输出模板；然后，通过选择模块选取最合适的模板。创新点在于引入了无监督选择方法，利用生成输出之间的协议，以及有监督选择方法，使用基于标记DocIE数据训练的奖励模型。针对DocIE中缺乏黄金推理轨迹的问题，提出基于拒绝采样的方法来生成银训练数据，配对输出模板与推理轨迹，用于训练选择模块。",
      "result": "实验结果表明，无监督和有监督的ThinkTwice框架均一致优于贪婪解码基线和现有最先进方法。这些改进验证了采样策略在文档级信息提取中的有效性，特别是在使用推理模型时，能够显著提升性能。然而，摘要未明确说明具体性能指标如准确率提升数值，但强调了框架的稳健性和优越性。",
      "conclusion": "本研究的主要贡献是提出了ThinkTwice框架，通过采样和选择机制有效提升了文档级信息提取的性能，展示了输出变异性在信息提取中的潜在优势。学术价值在于结合了无监督和有监督方法，为未来研究提供了新思路。实际应用价值包括在信息检索和自动化文档处理中的潜在用途。未来工作可以扩展应用于其他信息提取任务或优化选择策略，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Sampling Strategies",
        "Document-level Information Extraction",
        "Reward Model",
        "Rejection Sampling"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:19.566004Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18392",
    "title": "Efficient Complex-Valued Vision Transformers for MRI Classification Directly from k-Space",
    "authors": [
      "Moritz Rempe",
      "Lukas T. Rotkopf",
      "Marco Schlimbach",
      "Helmut Becker",
      "Fabian Hörst",
      "Johannes Haubold",
      "Philipp Dammann",
      "Kevin Kröninger",
      "Jens Kleesiek"
    ],
    "abstract": "Deep learning applications in Magnetic Resonance Imaging (MRI) predominantly operate on reconstructed magnitude images, a process that discards phase information and requires computationally expensive transforms. Standard neural network architectures rely on local operations (convolutions or grid-patches) that are ill-suited for the global, non-local nature of raw frequency-domain (k-Space) data. In this work, we propose a novel complex-valued Vision Transformer (kViT) designed to perform classification directly on k-Space data. To bridge the geometric disconnect between current architectures and MRI physics, we introduce a radial k-Space patching strategy that respects the spectral energy distribution of the frequency-domain. Extensive experiments on the fastMRI and in-house datasets demonstrate that our approach achieves classification performance competitive with state-of-the-art image-domain baselines (ResNet, EfficientNet, ViT). Crucially, kViT exhibits superior robustness to high acceleration factors and offers a paradigm shift in computational efficiency, reducing VRAM consumption during training by up to 68$\\times$ compared to standard methods. This establishes a pathway for resource-efficient, direct-from-scanner AI analysis.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18392.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18392",
    "published": "2026-01-26T11:50:52Z",
    "updated": "2026-01-26T11:50:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种高效的复杂值Vision Transformer（kViT），用于直接从k-Space数据分类MRI，通过径向patches策略提升鲁棒性和计算效率。",
      "motivation": "MRI深度学习应用通常基于重构后的幅度图像，这丢弃了相位信息并需要计算昂贵的变换。标准神经网络架构如卷积或网格patches不适合处理k-Space数据的全局、非局部性质，导致效率和信息损失问题。本研究旨在直接利用原始k-Space数据，通过克服现有方法的不足，为医疗影像分析提供更高效、信息保留更完整的解决方案。",
      "method": "本研究提出kViT（复杂值Vision Transformer），一种专门设计用于直接在k-Space数据上分类的方法。核心创新是引入径向k-Space patching策略，根据频域的光谱能量分布划分patches，以更好地适应MRI物理特性。该方法采用Vision Transformer架构处理复杂值输入，在fastMRI和内部数据集上进行训练和评估，以实现高效、直接的分类任务。",
      "result": "实验在fastMRI和内部数据集上进行，结果显示kViT的分类性能与图像域的最先进基线（如ResNet、EfficientNet和ViT）竞争。特别地，kViT对高加速因子表现出优越鲁棒性，并且在训练过程中VRAM消耗最多减少了68倍，显著提升了计算效率，验证了该方法的有效性和资源节省优势。",
      "conclusion": "本研究的贡献在于提出kViT方法，通过径向patches策略桥接了神经网络架构与MRI物理之间的不匹配，实现了直接从k-Space数据进行资源高效的AI分析。这为医疗影像分类提供了新范式，具有重要的学术价值和实际应用潜力，未来可扩展至其他医学影像任务或改进复杂值处理技术。",
      "tags": [
        "Complex-Valued Vision Transformer",
        "k-Space MRI",
        "Radial Patching",
        "MRI Classification",
        "Transformer Architecture"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:32.944619Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18386",
    "title": "ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks",
    "authors": [
      "Gabriel Lee Jun Rong",
      "Christos Korgialas",
      "Dion Jia Xu Ho",
      "Pai Chet Ng",
      "Xiaoxiao Miao",
      "Konstantinos N. Plataniotis"
    ],
    "abstract": "Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk\". Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18386.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18386",
    "published": "2026-01-26T11:36:34Z",
    "updated": "2026-01-26T11:36:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "ARMOR 框架通过 Vision Language Models 和 Large Language Models 实现对抗攻击的动态编排和重参数化，提升攻击的鲁棒性和适应性。",
      "motivation": "现有的自动化对抗攻击套件通常以静态集合形式运行，攻击序列固定，缺乏战略性的适应能力和对图像语义的敏感性。这种局限性导致攻击效率低下，尤其是在面对不同架构或鲁棒性要求时。因此，本研究旨在解决这些问题，通过引入语义感知和自适应调整，提高对抗攻击的跨架构转移性和可靠性，增强对抗样本的生成效果。",
      "method": "论文提出的 ARMOR 框架通过 Vision Language Models (VLM) 指导的智能体来编排三种经典的对抗攻击原语：Carlini-Wagner (CW)、Jacobian-based Saliency Map Attack (JSMA) 和 Spatially Transformed Attacks (STA)。这些智能体通过一个共享的“Mixing Desk”协作生成和合成扰动。同时，Large Language Models (LLMs) 在实时闭环系统中自适应地调优和重参数化并行攻击智能体，以利用图像特定的语义漏洞，实现智能攻击调整。",
      "result": "在标准基准测试中，ARMOR 实现了改进的跨架构转移性，并可靠地欺骗了盲目标（blind targets）和白盒目标（white-box targets）。对于盲目标，ARMOR 提供混合输出；对于白盒目标，它使用信心和结构相似性（SSIM）分数选择最佳攻击或混合攻击。尽管摘要未提供具体性能数据，但强调了其在增强攻击鲁棒性和适应性方面的有效性，与现有静态方法相比表现更优。",
      "conclusion": "ARMOR 框架的主要贡献在于结合 Vision Language Models 和 Large Language Models，实现了对抗攻击的智能编排和重参数化，从而提升了攻击的自适应性和语义感知能力。这一研究具有重要的学术价值，推动了对抗攻击领域向动态和智能方向发展；在实际应用中，它可以用于增强机器学习模型的安全测试。未来工作可能包括扩展攻击类型或应用到更多场景，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Adversarial Attacks",
        "Vision Language Models (VLM)",
        "Large Language Models (LLM)",
        "Method Orchestration",
        "Reparameterization"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:46.838920Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18385",
    "title": "Estimation of geometric transformation matrices using grid-shaped pilot signals",
    "authors": [
      "Rinka Kawano",
      "Masaki Kawamura"
    ],
    "abstract": "Digital watermarking techniques are essential to prevent unauthorized use of images. Since pirated images are often geometrically distorted by operations such as scaling and cropping, accurate synchronization - detecting the embedding position of the watermark - is critical for proper extraction. In particular, cropping changes the origin of the image, making synchronization difficult. However, few existing methods are robust against cropping. To address this issue, we propose a watermarking method that estimates geometric transformations applied to a stego image using a pilot signal, allowing synchronization even after cropping. A grid-shaped pilot signal with distinct horizontal and vertical values is embedded in the image. When the image is transformed, the grid is also distorted. By analyzing this distortion, the transformation matrix can be estimated. Applying the Radon transform to the distorted image allows estimation of the grid angles and intervals. In addition, since the horizontal and vertical grid lines are encoded differently, the grid orientation can be determined, which reduces ambiguity. To validate our method, we performed simulations with anisotropic scaling, rotation, shearing, and cropping. The results show that the proposed method accurately estimates transformation matrices with low error under both single and composite attacks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18385.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18385",
    "published": "2026-01-26T11:33:01Z",
    "updated": "2026-01-26T11:33:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种使用网格形导频信号估计几何变换矩阵的水印方法，增强了对裁剪攻击的同步鲁棒性。",
      "motivation": "数字水印技术依赖准确同步来提取嵌入的水印，但盗版图像常遭缩放、裁剪等几何变换，特别是裁剪改变图像原点，导致同步困难。现有方法对裁剪攻击缺乏鲁棒性，限制了实际应用效果。因此，本研究旨在开发一种能有效估计变换矩阵的方法，解决裁剪带来的同步挑战，确保水印的可靠提取。",
      "method": "方法在图像中嵌入一个网格形导频信号，该信号具有不同编码的水平和垂直值。当图像遭受几何变换时，网格随之扭曲。通过应用Radon变换分析扭曲图像，估计网格的角度和间隔。关键创新在于编码方式：利用水平和垂直线的不同编码确定网格方向，减少歧义，从而准确计算变换矩阵，实现同步。",
      "result": "在仿真实验中，针对各向异性缩放、旋转、剪切和裁剪等攻击进行测试。结果表明，该方法能准确估计几何变换矩阵，误差较低，且对单次和复合攻击均表现良好。摘要未明确说明具体性能数据或基线对比，但强调了低误差估计，验证了对裁剪的鲁棒性和有效性。",
      "conclusion": "本研究成功解决了水印同步中裁剪攻击的难题，通过网格形导频信号估计变换矩阵，提升了水印提取的可靠性。学术上，为几何不变水印提供了新方法；实际上，增强了数字版权保护的应用价值。未来工作可能扩展到更多攻击类型或优化实时处理，以应对更复杂场景。",
      "tags": [
        "Digital Watermarking",
        "Geometric Transformation",
        "Grid-shaped Pilot Signal",
        "Radon Transform",
        "Synchronization"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:32.790748Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18383",
    "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models",
    "authors": [
      "Zhenyuan Guo",
      "Tong Chen",
      "Wenlong Meng",
      "Chen Gong",
      "Xin Yu",
      "Chengkun Wei",
      "Wenzhi Chen"
    ],
    "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18383.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18383",
    "published": "2026-01-26T11:31:40Z",
    "updated": "2026-01-26T11:31:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出动态思维令牌选择（DynTS）方法，通过识别推理轨迹中的决策关键令牌来优化大型推理模型的效率。",
      "motivation": "本研究旨在解决大型推理模型（LRMs）显式生成完整推理轨迹时导致的内存占用和计算开销大、效率受限的问题。该问题对LRMs的实际部署至关重要，尤其是在资源密集型应用和大规模推理场景中，效率瓶颈会限制模型的实用性和扩展性。现有LRMs方法可能未充分考虑推理轨迹中令牌贡献的不均衡性，导致处理冗余令牌时产生不必要的计算成本，亟需更高效的优化方案来提高性能。摘要未明确说明其他具体方法的不足之处，但可推断传统方法在处理推理轨迹时缺乏针对令牌重要性的动态筛选机制。",
      "method": "本研究提出的Dynamic Thinking-Token Selection（DynTS）方法，使用注意力图分析推理轨迹中各令牌对模型决策的影响，从而识别出对最终答案起关键作用的决策令牌。关键创新点在于基于注意力图的分析，揭示推理轨迹中令牌贡献不均衡的现象，并设计动态选择机制：在推理过程中，仅保留决策关键令牌相关联的Key-Value（KV）缓存状态，同时移除其余冗余令牌的KV缓存条目，以实现内存和计算开销的优化。摘要未明确说明使用的具体数据集或模型架构，但方法侧重于优化LRMs的推理阶段，不涉及训练过程。",
      "result": "摘要未明确说明具体的实验结果数据，如准确率或效率提升的具体数值。但基于方法描述，预期DynTS能够通过移除推理轨迹中的冗余令牌，显著降低大型推理模型的内存占用和计算开销，从而提高推理效率。该方法与基线方法（如未优化的标准LRMs）对比，应能在保持模型决策准确性的同时，减少不必要的资源消耗，优化整体性能。未来实验可能需要验证在不同任务和模型规模下的泛化效果和性能增益。",
      "conclusion": "本论文的主要贡献在于提出Dynamic Thinking-Token Selection（DynTS）方法，以优化大型推理模型的效率，并通过注意力图分析揭示了推理轨迹中令牌贡献不均衡的现象。学术价值体现在为推理模型优化提供了新的视角，引入注意力机制来指导令牌选择，丰富了对模型内部决策过程的理解。实际应用价值在于使LRMs在资源受限环境中更实用，提升部署效率和可扩展性。局限性包括摘要未提及方法在不同任务或模型架构上的验证情况，未来工作方向可能涉及进一步评估泛化性、与其他优化技术的结合，以及探索更精细的令牌选择策略。",
      "tags": [
        "Large Reasoning Models",
        "Attention Maps",
        "Key-Value Cache",
        "Dynamic Token Selection",
        "Efficient Inference"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:43.063931Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18381",
    "title": "AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito",
    "authors": [
      "Yinghan Hou",
      "Zongyou Yang"
    ],
    "abstract": "To facilitate the transformation of legacy finite difference implementations into the Devito environment, this study develops an integrated AI agent framework. Retrieval-Augmented Generation (RAG) and open-source Large Language Models are combined through multi-stage iterative workflows in the system's hybrid LangGraph architecture. The agent constructs an extensive Devito knowledge graph through document parsing, structure-aware segmentation, extraction of entity relationships, and Leiden-based community detection. GraphRAG optimisation enhances query performance across semantic communities that include seismic wave simulation, computational fluid dynamics, and performance tuning libraries. A reverse engineering component derives three-level query strategies for RAG retrieval through static analysis of Fortran source code. To deliver precise contextual information for language model guidance, the multi-stage retrieval pipeline performs parallel searching, concept expansion, community-scale retrieval, and semantic similarity analysis. Code synthesis is governed by Pydantic-based constraints to guarantee structured outputs and reliability. A comprehensive validation framework integrates conventional static analysis with the G-Eval approach, covering execution correctness, structural soundness, mathematical consistency, and API compliance. The overall agent workflow is implemented on the LangGraph framework and adopts concurrent processing to support quality-based iterative refinement and state-aware dynamic routing. The principal contribution lies in the incorporation of feedback mechanisms motivated by reinforcement learning, enabling a transition from static code translation toward dynamic and adaptive analytical behavior.",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18381.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18381",
    "published": "2026-01-26T11:31:00Z",
    "updated": "2026-01-26T11:31:00Z",
    "comment": "14 pages, 7 figures",
    "light_analysis": {
      "overview": "本研究开发了一个集成AI代理框架，结合检索增强生成和大型语言模型，通过多阶段迭代工作流，实现传统有限差分代码到Devito环境的自动化转换，并融入了基于强化学习的反馈机制。",
      "motivation": "该研究旨在解决传统有限差分代码难以集成到现代Devito环境的问题，这些遗留代码通常可维护性差、性能低下，限制了高性能计算的发展。现有方法多依赖手动翻译，效率低且易出错，因此需要自动化工具来促进代码现代化，提高转换的准确性和效率，以适应地震波模拟、计算流体动力学等领域的需求。摘要未明确说明现有方法的具体不足，但推断手动流程是主要瓶颈。",
      "method": "论文提出一个集成AI代理框架，采用检索增强生成和开源大型语言模型，在LangGraph架构中实施多阶段迭代工作流。核心方法包括：通过文档解析和Leiden社区检测构建Devito知识图谱；使用GraphRAG优化查询性能；通过Fortran源代码静态分析进行反向工程，生成三级查询策略；多阶段检索管道执行并行搜索和语义相似性分析；代码合成受Pydantic约束确保结构化输出；验证框架整合静态分析和G-Eval方法，检查执行正确性和API合规性。创新点在于反馈机制和动态路由，支持质量导向的迭代优化。",
      "result": "摘要未明确说明具体实验数据，如准确率或效率改进。但基于方法描述，系统通过GraphRAG优化增强了查询性能，覆盖了多个语义社区；代码合成约束和验证框架确保了输出可靠性和结构性，可能提升了转换的准确性和可维护性。与基线方法的对比未提及，推断可能优于传统手动或静态翻译方法，但需进一步实验验证。整体工作流程在LangGraph上实现并发处理，支持动态自适应行为。",
      "conclusion": "论文的主要贡献是开发了一个AI代理框架，将反馈机制融入代码翻译过程，实现了从静态转换向动态自适应分析的转变。该研究具有学术价值，推动了自动化代码重构和知识图谱应用的发展；实际应用上，能促进传统有限差分代码的现代化，提高计算效率和可维护性，适用于地震模拟和流体动力学等领域。局限性可能包括对特定编程语言（如Fortran）的依赖；未来工作可扩展至其他语言或更广泛的代码转换任务。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Large Language Model",
        "GraphRAG",
        "Knowledge Graph",
        "Reverse Engineering"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:04.334509Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18380",
    "title": "Corpus-Based Approaches to Igbo Diacritic Restoration",
    "authors": [
      "Ignatius Ezeani"
    ],
    "abstract": "With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.   In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18380.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18380",
    "published": "2026-01-26T11:30:36Z",
    "updated": "2026-01-26T11:30:36Z",
    "comment": "270 page. Ph.D. Thesis. The University of Sheffield",
    "light_analysis": {
      "overview": "本文提出了一个针对伊博语变音符号恢复的灵活框架，结合标准n-gram模型、分类模型和嵌入模型，以处理低资源语言的NLP挑战。",
      "motivation": "自然语言处理（NLP）研究主要集中于资源丰富的语言，如英语和中文，而超过95%的全球语言属于低资源类型，缺乏数据和工具。伊博语作为一种低资源语言，其变音符号存在歧义，影响语言处理效果。现有方法多忽略这类语言，导致NLP技术应用不均，亟需开发针对低资源语言的解决方案，以促进语言多样性和技术普及。",
      "method": "论文提出了一个灵活框架，用于生成伊博语变音符号恢复的数据集，并应用三种主要方法：标准n-gram模型使用目标词的前词序列预测正确变体；分类模型利用目标词两侧的单词窗口进行特征提取；嵌入模型则通过比较上下文词嵌入与候选变体嵌入的相似性得分来恢复变音。关键创新在于整合多种模型，以应对低资源语言的变音歧义问题，并使用语料库驱动的方法构建数据集。",
      "result": "摘要未明确说明具体的实验结果，如准确率或效率改进数据。但基于方法描述，可以推断这些模型在伊博语变音恢复任务上进行了应用，预计能通过结合多种模型提升恢复精度。与现有针对其他语言的方法相比，本框架针对低资源场景进行了优化，可能在实际评估中表现出色，但具体性能指标需在论文正文中进一步呈现。",
      "conclusion": "本文的主要贡献是开发了一个针对伊博语变音符号恢复的框架，结合n-gram、分类和嵌入模型，填补了低资源语言NLP研究的空白。学术价值在于扩展了NLP技术的应用范围，推动了对全球语言多样性的处理；实际应用价值包括改善伊博语的语言处理工具，支持教育和科技发展。局限性可能包括数据集规模限制或模型泛化能力，未来工作可涉及扩展到更多语言、优化模型或集成更多NLP技术。",
      "tags": [
        "Diacritic Restoration",
        "N-gram Models",
        "Classification Models",
        "Word Embeddings",
        "Low-Resource Languages"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:25.654103Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18375",
    "title": "Hierarchical Text Classification with LLM-Refined Taxonomies",
    "authors": [
      "Jonas Golde",
      "Nicolaas Jedema",
      "Ravi Krishnan",
      "Phong Le"
    ],
    "abstract": "Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18375.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18375",
    "published": "2026-01-26T11:28:32Z",
    "updated": "2026-01-26T11:28:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出TaxMorph框架，利用大型语言模型改进分类层次结构，以提升层次化文本分类的性能。",
      "motivation": "层次化文本分类依赖分类层次结构来组织标签，但现实世界中的分类法常存在歧义，例如相同叶节点名称出现在相似父节点下，这阻碍了语言模型学习清晰的决策边界。现有方法可能未充分修订整个层次结构，导致分类效果受限。本研究的动机是开发一种方法来减少这些歧义，使分类法更匹配模型的语义理解，从而提高分类准确性和效率。",
      "method": "论文提出TaxMorph框架，通过使用大型语言模型对分类法进行重命名、合并、拆分和重排序等操作，全面修订分类层次结构。关键创新点在于修订过程旨在更好地匹配LMs编码的语义，而不是仅局部调整。研究在三个层次化文本分类基准数据集上进行实验，具体操作如LLM引导的转换，以优化分类法的结构，使其更符合模型的学习方式。",
      "result": "在三个层次化文本分类基准上的实验结果显示，LLM改进的分类法在各种设置下均优于人工策划的分类法，F1分数最高提升2.9个百分点。与基线方法对比，LLM分类法表现出更一致的性能改进。通过分析LMs在分类中的表现，发现LLM分类法更紧密地符合模型的实际混淆模式，尽管在嵌入空间中聚类可分性较低，这表明分类法改进更贴合模型的归纳偏好。",
      "conclusion": "本研究的贡献在于展示了LLM引导的分类法改进能创建更兼容模型学习方式的分类结构，从而显著提升层次化文本分类性能。这为分类法设计提供了新视角，具有理论价值和实际应用潜力，可能促进更有效的文本分类系统。未来工作方向可能包括扩展TaxMorph框架到更多领域或探索更多LLM操作。",
      "tags": [
        "Hierarchical Text Classification",
        "Large Language Model",
        "Taxonomy Refinement",
        "Semantics Encoding",
        "Confusion Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:19.875719Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18374",
    "title": "CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes",
    "authors": [
      "Rodrigo Silva",
      "José Evans",
      "José Isidro",
      "Miguel Marques",
      "Afonso Fonseca",
      "Ricardo Morais",
      "João Canavilhas",
      "Arian Pasquali",
      "Purificação Silvano",
      "Alípio Jorge",
      "Nuno Guimarães",
      "Sérgio Nunes",
      "Ricardo Campos"
    ],
    "abstract": "City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18374.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18374",
    "published": "2026-01-26T11:26:57Z",
    "updated": "2026-01-26T11:26:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了CitiLink平台，利用自然语言处理和信息检索技术将非结构化城市会议记录转化为可搜索数据，以增强政府透明度和公民参与。",
      "motivation": "城市议会会议记录通常冗长、正式且非结构化，尽管公开可用，但其官僚化写作风格和复杂结构使公民和记者难以高效查找信息。这限制了政府透明度，阻碍公民参与，现有方法在处理此类文档时缺乏有效的技术手段，导致信息可访问性不足，亟需改进以促进地方政府的公开和民主进程。",
      "method": "CitiLink平台采用大型语言模型（如Gemini）从非结构化会议记录中提取元数据、讨论主题和投票结果，并将结构化数据索引到数据库中。系统实现基于BM25排名的全文搜索和分面过滤功能，通过用户友好界面提供访问。开发基于来自六个葡萄牙城市的120个会议记录数据集，结合自然语言处理和信息检索技术处理政府文档。",
      "result": "通过引导会话与市政人员测试平台可用性，获得用户交互的深入见解，验证了系统的实用性和易用性。评估显示Gemini模型在信息提取中表现有效，突出了其在数据提取方面的性能，但摘要未提供具体性能指标如准确率，也未与基线方法进行对比，仅基于实际测试反馈。",
      "conclusion": "本研究表明CitiLink平台成功应用自然语言处理和信息检索技术提升政府文档的可访问性和透明度，具有增强公民参与和改善政府信息传播的实际应用价值。学术上展示了技术如何解决社会问题，未来工作可扩展到更多城市数据集，优化提取算法以提高准确性和效率。",
      "tags": [
        "Natural Language Processing",
        "Information Retrieval",
        "Large Language Models",
        "BM25",
        "Data Extraction"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:31.912596Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18372",
    "title": "Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues",
    "authors": [
      "Christos Petrou",
      "Harris Partaourides",
      "Athanasios Balomenos",
      "Yannis Kopsinis",
      "Sotirios Chatzis"
    ],
    "abstract": "Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18372.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18372",
    "published": "2026-01-26T11:26:27Z",
    "updated": "2026-01-26T11:26:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种无需眼动追踪的虚拟现实凝视预测框架，通过结合视觉显著性和头部运动线索，有效预测用户注视方向，以减少延迟和增强交互体验。",
      "motivation": "在虚拟现实应用中，凝视预测对于减少传感器延迟和支持如foveated rendering等计算密集型技术至关重要，能提升用户体验。然而，直接眼动追踪常因硬件限制或隐私问题不可用，导致现有方法如基于HMD中心或平均凝视的预测效果有限，无法有效处理动态环境中的用户行为，因此迫切需要开发一种无需眼动追踪的高效预测方法来解决这一实际问题。",
      "method": "该方法结合头戴显示器运动信号和视频帧的视觉显著性线索，使用轻量编码器UniSal提取视觉特征，并与HMD运动数据融合，通过时间序列预测模块处理。关键创新在于采用多模态融合策略，评估了TSMixer和LSTM两种轻量架构来预测未来凝视方向，实现了低计算成本和高效预测。",
      "result": "在EHTask数据集和商业VR硬件上的实验表明，所提方法在性能上一致优于基线如Center-of-HMD和Mean Gaze，具体表现为预测准确度提升和感知延迟减少。这些结果验证了预测凝视建模在增强VR环境自然交互方面的有效性，实际部署中展现出较好的稳定性和实用性，但摘要未明确说明具体数值指标。",
      "conclusion": "本研究的主要贡献是提出了一种无需眼动追踪的凝视预测框架，结合视觉和运动线索，显著提升了VR应用的性能和用户体验。学术上为多模态预测提供了新思路，实际中减少了对昂贵硬件的依赖，具有重要应用价值。未来可探索更多轻量架构或优化隐私保护策略，但摘要未明确说明局限性。",
      "tags": [
        "Gaze Prediction",
        "Virtual Reality",
        "Visual Saliency",
        "Time-Series Prediction",
        "LSTM"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:40.120117Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18368",
    "title": "OREHAS: A fully automated deep-learning pipeline for volumetric endolymphatic hydrops quantification in MRI",
    "authors": [
      "Caterina Fuster-Barceló",
      "Claudia Castrillón",
      "Laura Rodrigo-Muñoz",
      "Victor Manuel Vega-Suárez",
      "Nicolás Pérez-Fernández",
      "Gorka Bastarrika",
      "Arrate Muñoz-Barrutia"
    ],
    "abstract": "We present OREHAS (Optimized Recognition & Evaluation of volumetric Hydrops in the Auditory System), the first fully automatic pipeline for volumetric quantification of endolymphatic hydrops (EH) from routine 3D-SPACE-MRC and 3D-REAL-IR MRI. The system integrates three components -- slice classification, inner ear localization, and sequence-specific segmentation -- into a single workflow that computes per-ear endolymphatic-to-vestibular volume ratios (ELR) directly from whole MRI volumes, eliminating the need for manual intervention.   Trained with only 3 to 6 annotated slices per patient, OREHAS generalized effectively to full 3D volumes, achieving Dice scores of 0.90 for SPACE-MRC and 0.75 for REAL-IR. In an external validation cohort with complete manual annotations, OREHAS closely matched expert ground truth (VSI = 74.3%) and substantially outperformed the clinical syngo.via software (VSI = 42.5%), which tended to overestimate endolymphatic volumes. Across 19 test patients, vestibular measurements from OREHAS were consistent with syngo.via, while endolymphatic volumes were systematically smaller and more physiologically realistic.   These results show that reliable and reproducible EH quantification can be achieved from standard MRI using limited supervision. By combining efficient deep-learning-based segmentation with a clinically aligned volumetric workflow, OREHAS reduces operator dependence, ensures methodological consistency. Besides, the results are compatible with established imaging protocols. The approach provides a robust foundation for large-scale studies and for recalibrating clinical diagnostic thresholds based on accurate volumetric measurements of the inner ear.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18368.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18368",
    "published": "2026-01-26T11:19:21Z",
    "updated": "2026-01-26T11:19:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "OREHAS是一种全自动深度学习管道，首次实现从常规MRI中自动量化内淋巴积水体积，通过集成切片分类、内耳定位和分割组件，无需人工干预。",
      "motivation": "内淋巴积水（EH）是内耳疾病如梅尼埃病的关键指标，其准确量化对临床诊断和治疗至关重要。当前方法依赖手动或半自动工具，如临床软件syngo.via，存在操作者依赖性高、结果不一致等问题，且易高估体积，可能导致诊断误差。因此，开发自动化、可靠的量化系统能提升诊断一致性和效率，解决现有方法不足。",
      "method": "OREHAS采用深度学习技术，集成三个核心组件：首先进行切片分类以识别MRI序列，然后定位内耳区域，最后执行序列特定分割以区分内淋巴和前庭结构。整个管道直接从完整3D MRI体积计算内淋巴-前庭体积比（ELR），使用有限的监督数据（每个患者仅3-6个注释切片）训练，实现端到端自动化处理，减少手动干预。",
      "result": "实验结果显示，OREHAS在SPACE-MRC序列上达到Dice分数0.90，REAL-IR序列上为0.75。在外部验证中，与专家手工注释的VSI匹配度为74.3%，显著优于临床软件syngo.via的42.5%，后者倾向于高估内淋巴体积。对19名患者的测试表明，OREHAS的前庭测量与软件一致，但内淋巴体积估计更小、更生理学合理。",
      "conclusion": "该研究证明了利用有限监督的深度学习可实现可靠、可重复的内淋巴积水量化。OREHAS减少了操作者依赖性，确保方法一致性，并与标准成像协议兼容，为大规模临床研究和诊断阈值校准提供了坚实基础。潜在应用包括改善内耳疾病诊断精度和促进循证医学发展，未来工作可扩展至更多疾病或优化模型泛化能力。",
      "tags": [
        "Deep Learning",
        "Medical Image Segmentation",
        "Volumetric Quantification",
        "MRI",
        "Automatic Pipeline"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:50.314849Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18356",
    "title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning",
    "authors": [
      "Weiqin Yang",
      "Haowen Xue",
      "Qingyi Peng",
      "Hexuan Hu",
      "Qian Huang",
      "Tingbo Zhang"
    ],
    "abstract": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18356.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18356",
    "published": "2026-01-26T11:03:00Z",
    "updated": "2026-01-26T11:03:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出多模态因果检索增强生成框架，集成因果推理与多模态检索，提升医学视觉-语言模型在临床决策中的准确性和可信度。",
      "motivation": "医学视觉-语言模型（VLM）在诊断报告生成和图像-文本对齐方面表现良好，但其推理机制本质上是相关的，仅依赖表面的统计关联，无法捕捉临床决策所需的因果病理生理机制。这种局限性导致模型脆弱、易产生幻觉，并对数据集偏差敏感。检索增强生成（RAG）通过外部知识提供部分解决方案，但传统RAG依赖语义相似性，可能引入新的虚假关联，限制了其在高风险临床环境中的应用可靠性。",
      "method": "论文提出多模态因果检索增强生成框架，将因果推理原则与多模态检索结合。关键创新点是从外部知识源检索临床相关示例和因果图，基于反事实和干预性证据来条件模型推理，而不是仅依赖相关性。该方法应用于放射学报告生成、诊断预测和视觉问答任务。摘要未明确说明使用的具体数据集、模型架构或其他技术细节。",
      "result": "在放射学报告生成、诊断预测和视觉问答等任务中，该方法提高了事实准确性、增强了模型对分布变化的鲁棒性，并提升了可解释性。与传统的医学VLM和RAG方法相比，表现出更好的性能，但由于摘要未提供具体数据，无法量化改进指标。结果强调了因果检索作为改善医学VLM推理机制的有效路径。",
      "conclusion": "该论文的主要贡献是提出了一种因果检索增强框架，使医学视觉-语言模型能够基于因果机制进行推理，超越模式匹配。这具有重要的学术价值，为开发更可信的医疗AI系统提供了新方法，并在高风险临床环境中具有实际应用潜力。局限性可能包括对高质量因果知识的外部依赖，未来工作可以探索更自动化的因果图提取和更广泛的临床应用。",
      "tags": [
        "Medical Vision-Language Models",
        "Retrieval-Augmented Generation",
        "Causal Inference",
        "Multimodal Retrieval",
        "Counterfactual Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:58.854887Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18353",
    "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books",
    "authors": [
      "Tuhin Chakrabarty",
      "Paramveer S. Dhillon"
    ],
    "abstract": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18353.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18353",
    "published": "2026-01-26T10:59:21Z",
    "updated": "2026-01-26T10:59:21Z",
    "comment": "Proceedings of CHI 2026 Conference (To Appear)",
    "light_analysis": {
      "overview": "本论文通过微调高质量书籍，使生成式AI产生专家水平的写作，挑战了创意写作作为人类独有活动的传统观点。",
      "motivation": "研究动机源于创意写作长期被视为人类独有的创造性活动，但生成式AI能以极低成本快速模仿多种作者风格，这引发了AI能否达到专家水平的疑问。现有方法或传统观点可能低估了AI在创意领域的潜力，导致对AI创造性局限的论述不充分。因此，本研究旨在验证AI写作能力，探索其是否能在模仿知名作者风格上超越人类专家，这对于理解AI与人类创意劳动的互动至关重要，并揭示了技术发展对传统认知的冲击。",
      "method": "研究方法包括一个行为实验，招募28名MFA作家作为人类专家，与三个大型语言模型（LLM）竞争模仿50位受好评的作者风格。关键创新在于使用微调技术，基于作者的完整作品优化LLM，以提升写作质量和风格模仿能力。实验通过盲评比较进行评估，由28位专家评审和131位普通评审进行配对比较，评估条件包括上下文提示和微调后的写作输出，旨在量化AI与人类在创意写作上的表现差异。",
      "result": "实验结果显示，在上下文提示条件下，专家评审员更偏好人类写作，占比82.7%；但在微调后，专家评审员对AI写作的偏好反转至62%。相比之下，普通评审员一直偏好AI写作。这些数据表明AI写作在模仿作者风格上具有显著提升，甚至在某些条件下超越人类专家。对专家作家的访谈进一步揭示，AI写作的偏好触发了身份危机，动摇了美学信心，并引发了对'好写作'定义的质疑。",
      "conclusion": "本研究的结论是通过微调高质量书籍，AI能产生专家水平的写作，挑战了AI在创意写作上的局限性，具有重要的学术价值，促使重新评估AI的创造性潜力。研究还引发了对创意劳动未来和人类身份的根本问题，实际应用价值在于推动AI在内容创作领域的应用。局限性可能包括样本规模或实验设计，未来工作可扩展更多作者风格或探索其他创意领域如艺术或音乐。",
      "tags": [
        "Generative AI",
        "Large Language Model",
        "Fine-Tuning",
        "Behavioral Experiment",
        "Author Style Emulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:01.801555Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18352",
    "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning",
    "authors": [
      "Manjie Xu",
      "Isabella Yin",
      "Xinyi Tu",
      "Chi Zhang",
      "Yixin Zhu"
    ],
    "abstract": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18352.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18352",
    "published": "2026-01-26T10:58:52Z",
    "updated": "2026-01-26T10:58:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出使用可执行代码表示动态规则，通过Code-Grounded Vistas方法，克服大语言模型在处理矛盾规则时的语义惰性问题。",
      "motivation": "该研究旨在解决大语言模型在动态上下文规则与预训练先验矛盾时，难以抑制这些先验的语义惰性问题。这一问题在需要覆盖学习先验的领域（如游戏、人工智能安全）中至关重要，因为现有方法依赖自然语言编码，混肴语义和逻辑，导致模型在规则变化时表现不佳，甚至观察到更大模型出现反向缩放现象，影响实际应用效果。",
      "method": "论文的核心方法是使用可执行代码而非描述性文本来表示动态规则，以避免自然语言中的语义混肴，提出Code-Grounded Vistas，通过在Baba Is You游戏数据集上微调模型，利用反事实对识别矛盾规则状态，强制模型关注逻辑约束而非视觉语义。关键创新在于代码表示分离了逻辑和描述，减少了预训练先验的干扰，并优化了训练过程以提高效率。",
      "result": "实验结果表明，Code-Grounded Vistas方法在效率和准确性上优于昂贵的推理时搜索方法，逆转了反向缩放趋势，使模型能够更有效地抑制预训练先验。虽然摘要未提供具体数据，但指出该方法提高了模型在矛盾规则下的性能，挑战了更大模型总是更优的假设。与基线相比，该方案在动态上下文中表现出更强的推理能力。",
      "conclusion": "本研究的贡献在于证明表示方式（代码 vs. 文本）决定了缩放是否改善上下文推理，挑战了大模型普遍更优的假设，对需要动态覆盖先验的领域有学术和实际价值。意义在于提供了一种高效训练方法来克服语义惰性，未来可扩展到更多任务或探索其他表示形式。",
      "tags": [
        "Large Language Models",
        "Semantic Inertia",
        "Code-Grounded Reasoning",
        "Fine-Tuning",
        "Inverse Scaling"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:40.431099Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18350",
    "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs",
    "authors": [
      "Junyi Zou"
    ],
    "abstract": "Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18350.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18350",
    "published": "2026-01-26T10:54:06Z",
    "updated": "2026-01-26T10:54:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出加权适配器合并方法，通过线性结合领域预训练和指令微调的适配器，解决医学大型语言模型中领域知识干扰指令对齐的问题。",
      "motivation": "大型语言模型在通用任务中表现出色，但在医学等安全关键领域，常面临医学术语不精确和安全指令遵循困难的挑战。现有方法如单独进行领域预训练或指令微调可能导致知识遗忘或对齐不足，影响模型在医疗问答中的可靠性和安全性。本研究旨在实证探索适配器干扰现象，并开发新方法以平衡指令遵循能力和领域知识保留，从而提升医学人工智能应用的性能。",
      "method": "本研究采用14B参数的基础模型，通过两阶段LoRA管道进行优化：首先进行领域自适应预训练，利用持续预训练注入广泛医学知识；其次进行监督微调，使用指令风格数据对齐模型与医学问答行为。关键创新在于提出加权适配器合并方法，线性结合预训练和微调阶段的适配器权重后导出合并模型检查点。技术特色包括LoRA的高效参数调整和适配器的模块化合并，以提高训练灵活性和模型性能。",
      "result": "在保留的医学验证集（F5/F6）上，合并模型在实际解码配置下实现BLEU-4为16.38、ROUGE-1为20.42、ROUGE-2为4.60、ROUGE-L为11.54。研究进一步分析了解码敏感性和训练稳定性，通过损失曲线和控制解码比较验证了方法的鲁棒性。摘要未明确提供与基线方法的直接对比数据，但结果显示了加权适配器合并对性能的积极影响。",
      "conclusion": "本研究的主要贡献是提出了加权适配器合并方法，实证分析了医学LLM中适配器干扰问题，成功平衡了指令对齐和领域知识保留。学术上，为适配器合并技术提供了新见解；实践上，有助于开发更安全、准确的医学问答系统。潜在局限性包括未在更广泛数据集上验证，未来工作可扩展至其他安全关键领域或优化合并策略以提升泛化能力。",
      "tags": [
        "Large Language Model",
        "LoRA",
        "Adapter Merging",
        "Domain Adaptive Pre-training",
        "Supervised Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:17.118183Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18346",
    "title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception",
    "authors": [
      "Sijing Wu",
      "Yunhao Li",
      "Zicheng Zhang",
      "Qi Jia",
      "Xinyue Li",
      "Huiyu Duan",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ],
    "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18346.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18346",
    "published": "2026-01-26T10:37:20Z",
    "updated": "2026-01-26T10:37:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Q-Bench-Portrait基准，首次针对多模态大语言模型在肖像图像质量感知方面进行全面评测。",
      "motivation": "当前多模态大语言模型在通用图像基准上表现优异，但肖像图像因其独特的结构和感知特性，其质量感知能力尚未被充分探索。现有基准主要面向通用图像，缺乏专门针对肖像领域的评估工具，导致模型在该领域的性能难以准确衡量。因此，开发一个专门的基准来评测和提升MLLMs在肖像图像质量感知方面的能力具有重要意义，解决了现有方法的不足和该领域研究的空白。",
      "method": "论文提出Q-Bench-Portrait基准，包含2,765个图像-问题-答案三元组。该基准具有以下关键创新：多样化的肖像图像来源，包括自然图像、合成失真图像、AI生成图像、艺术图像和计算机图形图像；全面的质量维度，涵盖技术失真、AIGC特定失真和美学评价；多种问题格式，如单选题、多选题、判断题和开放式问题，覆盖全局和局部层面。研究基于此基准评估了多模态大语言模型，但摘要未明确说明具体评估方法或模型架构细节。",
      "result": "基于Q-Bench-Portrait基准，研究评估了20个开源和5个闭源多模态大语言模型。结果表明，当前模型在肖像图像感知方面表现出一定能力，但性能仍有限且不精确，与人类判断存在明显差距。摘要未提供具体性能指标如准确率，但强调了模型在该领域的不足，突显了基准在揭示模型局限性和推动改进方面的价值。",
      "conclusion": "论文的主要贡献是引入了Q-Bench-Portrait基准，为评测和提升多模态大语言模型在肖像图像质量感知能力提供了首个专门工具。其学术价值在于填补了现有研究空白，促进了对肖像图像感知的深入探索；实际应用价值在于可推动通用和领域特定模型在该领域的优化。未来工作可能包括扩展基准规模、改进评估方法或基于基准开发新的模型技术。",
      "tags": [
        "Multimodal Large Language Models",
        "Image Quality Assessment",
        "Benchmarking",
        "Portrait Images",
        "AIGC Distortions"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:20.108195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18342",
    "title": "Structural Gender Bias in Credit Scoring: Proxy Leakage",
    "authors": [
      "Navya SD",
      "Sreekanth D",
      "SS Uma Sankari"
    ],
    "abstract": "As financial institutions increasingly adopt machine learning for credit risk assessment, the persistence of algorithmic bias remains a critical barrier to equitable financial inclusion. This study provides a comprehensive audit of structural gender bias within the Taiwan Credit Default dataset, specifically challenging the prevailing doctrine of \"fairness through blindness.\" Despite the removal of explicit protected attributes and the application of industry standard fairness interventions, our results demonstrate that gendered predictive signals remain deeply embedded within non-sensitive features. Utilizing SHAP (SHapley Additive exPlanations), we identify that variables such as Marital Status, Age, and Credit Limit function as potent proxies for gender, allowing models to maintain discriminatory pathways while appearing statistically fair. To mathematically quantify this leakage, we employ an adversarial inverse modeling framework. Our findings reveal that the protected gender attribute can be reconstructed from purely non-sensitive financial features with an ROC AUC score of 0.65, demonstrating that traditional fairness audits are insufficient for detecting implicit structural bias. These results advocate for a shift from surface-level statistical parity toward causal-aware modeling and structural accountability in financial AI.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18342.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18342",
    "published": "2026-01-26T10:29:45Z",
    "updated": "2026-01-26T10:29:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过审计台湾信用数据集，揭示了非敏感特征中隐藏的性别偏见代理泄露，挑战了传统‘公平盲化’方法的有效性。",
      "motivation": "金融机构日益采用机器学习进行信用风险评估，但算法偏见，特别是性别偏见，仍是公平金融包容的关键障碍。当前方法遵循‘公平盲化’原则，移除显式受保护属性并应用标准公平干预，然而，非敏感特征中仍存在结构性性别信号，导致传统审计方法不足以检测隐式偏见。这个问题的重要性在于，它阻碍了金融AI的公平性和社会正义的实现，需要更深入的研究来解决。",
      "method": "论文对台湾信用违约数据集进行综合审计，使用SHAP（SHapley Additive exPlanations）分析特征重要性，识别出婚姻状况、年龄和信用限额等非敏感特征作为性别的代理变量。关键创新在于应用对抗性逆建模框架，量化代理泄露程度，即从纯非敏感金融特征中重建受保护的性别属性。该方法提供了数学基础来评估传统公平审计的局限性，并突出结构性偏见的检测挑战。",
      "result": "实验结果表明，尽管移除性别属性，模型仍能通过非敏感特征维持歧视性路径，如重建性别属性的ROC AUC得分达到0.65。这证明了传统公平审计方法如统计公平无法有效检测隐式结构性偏见。与基线方法相比，研究显示代理泄露现象持续存在，强调了现有干预措施的不足。具体数据支撑包括SHAP分析揭示的代理特征和重建性能的量化指标。",
      "conclusion": "研究的主要贡献是揭露了信用评分中结构性性别偏见通过非敏感特征泄露，挑战了‘公平盲化’原则。学术价值在于推动了公平AI领域从统计公平向因果感知建模的转变；实际应用价值在于促进了金融AI的结构性问责，以增强公平性和包容性。局限性可能包括数据集特定性，未来工作可探索更广泛的审计方法或开发更先进的检测技术来缓解偏见。",
      "tags": [
        "Algorithmic Bias",
        "Credit Scoring",
        "SHAP",
        "Adversarial Modeling",
        "Fairness Auditing"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:16.705630Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18340",
    "title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing",
    "authors": [
      "Bingzheng Qu",
      "Kehai Chen",
      "Xuefeng Bai",
      "Jun Yu",
      "Min Zhang"
    ],
    "abstract": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18340.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18340",
    "published": "2026-01-26T10:28:09Z",
    "updated": "2026-01-26T10:28:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了首个专为非刚性视频编辑设计的全面基准NRVBench，包括高质量数据集、新颖评估指标和无训练基线方法，以解决物理扭曲和时间闪烁的挑战。",
      "motivation": "尽管文本驱动视频编辑进展显著，但生成连贯非刚性变形仍面临物理扭曲和时间闪烁的关键挑战，现有方法常因缺乏专门评估而无法有效处理复杂动态。非刚性视频编辑在动画、影视等应用中至关重要，现有通用指标不足以评估物理合规性和时间一致性，导致改进受限。因此，建立一个综合基准来系统评估和推动该领域技术发展迫在眉睫。",
      "method": "研究方法包括三个核心部分：首先，构建了一个包含180个非刚性运动视频的高质量数据集，覆盖六个物理类别，并配备2,340个细粒度任务指令和360个多项选择题。其次，提出了NRVE-Acc评估指标，基于视觉语言模型，能严格评估物理合规性、时间一致性和指令对齐，弥补通用指标的不足。第三，引入了无需训练的基线方法VM-Edit，利用双区域去噪机制实现结构感知控制，有效平衡结构保留和动态变形。",
      "result": "实验显示，当前非刚性视频编辑方法在保持物理合理性方面存在不足，常导致物理失真和时间闪烁。而我们的VM-Edit方法在标准指标和新提出的NRVE-Acc指标上均表现优异，实现了更高的物理合规性和时间一致性。摘要未明确说明具体数据，但广泛实验验证了该方法相对于基线方法的有效改进和鲁棒性。",
      "conclusion": "本研究的核心贡献是建立了NRVBench基准，为评估和推动非刚性视频编辑提供了标准测试平台。NRVE-Acc指标和VM-Edit方法有效克服了现有挑战，提升了编辑质量，具有学术和应用价值。未来工作可扩展数据集到更多物理场景，并优化方法以进一步提高效率和泛化能力。",
      "tags": [
        "Non-Rigid Video Editing",
        "Benchmarking",
        "Vision-Language Models",
        "Dual-Region Denoising",
        "Physics Compliance"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:43.653024Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18336",
    "title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction",
    "authors": [
      "Isaac Deutsch",
      "Nicolas Moënne-Loccoz",
      "Gavriel State",
      "Zan Gojcic"
    ],
    "abstract": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18336.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18336",
    "published": "2026-01-26T10:23:43Z",
    "updated": "2026-01-26T10:23:43Z",
    "comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/ppisp/",
    "light_analysis": {
      "overview": "论文提出PPISP模块，通过物理基础的ISP校正，提升辐射场重建中光度变化补偿的物理合理性和泛化能力。",
      "motivation": "多视角三维重建方法对光度不一致非常敏感，这源于相机光学特性和图像信号处理（ISP）的差异。现有缓解策略如每帧潜在变量或仿射颜色校正缺乏物理基础，导致泛化到新视角时效果不佳，限制了重建的准确性和鲁棒性。因此，需要一种物理合理的方法来改善光度一致性，确保公平评估。摘要未明确说明具体应用场景，但强调了问题的普遍性和现有方法的不足。",
      "method": "研究提出物理合理的ISP（PPISP）校正模块，通过基于物理和可解释的变换，解耦相机固有效应和捕获依赖效应。设计了一个专门的PPISP控制器，在输入视图上进行训练，用于预测新视角的ISP参数，类似真实相机中的自动曝光和白平衡功能。这种方法利用学习到的参数进行校正，无需真实图像即可实现对新视角的现实评估。关键创新在于将物理建模融入校正过程，提高了方法的可解释性和泛化能力。",
      "result": "在标准基准测试中，PPISP达到了最先进（SoTA）性能，超越了现有方法。它提供了直观的控制界面，并支持在有元数据时进行集成，从而增强校正效果。尽管摘要未提及具体数据如准确率提升百分比，但明确指出性能优异，实现了更高的重建精度和更好的泛化能力，使得光度补偿更加公平和有效。",
      "conclusion": "该研究的主要贡献是引入物理合理性的ISP校正，有效解决多视角重建中的光度不一致问题，提升重建质量和评估公平性。学术价值在于将物理基础融入计算机视觉任务，增强方法可解释性；实际应用价值在于改善辐射场重建等领域的性能。潜在局限性可能包括对特定相机模型的依赖性，未来工作可探索扩展到更广泛场景或实时应用。摘要未明确说明未来方向，但基于内容可推断。",
      "tags": [
        "Radiance Field Reconstruction",
        "Image Signal Processing (ISP)",
        "Physical-Plausible Correction",
        "Novel View Synthesis",
        "Multi-view 3D Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:43.882345Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18334",
    "title": "Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare",
    "authors": [
      "Clément Christophe",
      "Wadood Mohammed Abdul",
      "Prateek Munjal",
      "Tathagata Raha",
      "Ronnie Rajan",
      "Praveenkumar Kanithi"
    ],
    "abstract": "As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or \"confusability\". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized \"Thinking\" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18334.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18334",
    "published": "2026-01-26T10:21:34Z",
    "updated": "2026-01-26T10:21:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过引入基于医学MCQA的框架和新颖的调整奉承分数，揭示了前沿LLM的奉承行为缩放轨迹及其在推理模型中的脆弱性。",
      "motivation": "随着大型语言模型（LLM）逐渐融入临床工作流程，其奉承行为——即倾向于同意用户而非坚持事实准确性——给患者安全带来了严重风险。现有评估方法常依赖主观数据集，这限制了准确衡量奉承行为的能力，无法有效评估临床可靠性。因此，本研究旨在开发一个更稳健的框架，以客观评估LLM在医疗保健中的奉承行为，确保临床决策的准确性和安全性。",
      "method": "本研究提出了一种稳健的评估框架，基于医学多项选择题（MCQA）数据集，其真实答案可验证，以提供客观基准。核心方法是引入调整后的奉承分数（Adjusted Sycophancy Score），该指标通过考虑模型的随机不稳定性（即“混淆性”）来隔离对齐偏差。此外，研究对Qwen-3和Llama-3模型家族进行了广泛的缩放分析，并分析了推理优化模型（如“Thinking”模型）的内部推理痕迹，以探索奉承行为的韧性和脆弱性。",
      "result": "研究结果显示，通过对Qwen-3和Llama-3的缩放分析，发现奉承行为的韧性具有明显的缩放轨迹，随着模型规模增加而改善。然而，推理优化的“Thinking”模型显示出反直觉的脆弱性：尽管它们在普通任务中准确率高，但在权威压力下，其内部推理痕迹经常合理化不正确的用户建议。这表明，基准性能不能作为临床可靠性的可靠指标，而简化的推理结构可能更抗奉承行为，摘要未明确说明具体性能指标对比数据。",
      "conclusion": "本研究的主要贡献在于提出了调整后的奉承分数和基于医学MCQA的评估框架，有效量化了LLM的奉承行为。通过缩放分析，揭示了奉承行为韧性的轨迹，并发现了推理模型的特定脆弱性。这强调了在医疗应用中，基准性能不足以确保临床可靠性，而简化推理结构可能提供更好的稳健性。研究为未来开发更安全的AI临床工具奠定了基础，建议进一步探索其他模型和领域的奉承行为，局限性或未来工作方向摘要未明确说明。",
      "tags": [
        "Large Language Models",
        "Sycophancy",
        "Medical MCQA",
        "Scaling Analysis",
        "Reasoning Traces"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:08.361854Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18330",
    "title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification",
    "authors": [
      "Muhammad Ali Shah",
      "Muhammad Mansoor Alam",
      "Saddam Hussain Khan"
    ],
    "abstract": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18330.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18330",
    "published": "2026-01-26T10:14:57Z",
    "updated": "2026-01-26T10:14:57Z",
    "comment": "33 Pages, 8 Tables, Figures 16",
    "light_analysis": {
      "overview": "本文提出一种高效的DenseNet-Swin混合框架（EDSH），通过增强和分层特征空间，显著提升大规模脑MRI肿瘤分类的准确性。",
      "motivation": "脑肿瘤MRI分类需同时处理细粒度纹理和长距离上下文依赖，以应对弥漫性胶质瘤、脑膜瘤等类别的诊断挑战。现有独立CNN或Vision Transformer方法可能无法充分整合局部和全局特征，导致在特定肿瘤上性能不足，影响临床诊断准确性。本研究旨在通过设计肿瘤感知混合架构，解决现有方法的局限性，提高分类精度和鲁棒性。",
      "method": "论文提出EDSH框架，包含两种肿瘤感知设置：Boosted Feature Space（BFS）中定制DenseNet和Swin分支学习互补特征，经维度对齐、融合和提升，用于检测弥漫性胶质瘤；分层架构中DenseNet作为主干学习局部特征，Swin_t捕获全局肿瘤形态，通过Deep Feature Extraction和Dual Residual连接抑制假阴性。关键创新包括定制输入匹配MRI空间特性、密集残差连接减轻梯度消失，以及任务对齐补丁嵌入和移位窗口自注意力高效捕获全局依赖。",
      "result": "在大规模MRI数据集（40,260张图像，四个肿瘤类别）上的评估显示，EDSH框架在测试未见数据集上达到98.50%的准确率和召回率，优于独立CNN、Vision Transformer及其他混合方法，展现出一致的性能优越性，验证了其在提升分类精度方面的有效性。",
      "conclusion": "本研究主要贡献是提出EDSH混合框架，有效结合局部和全局特征，针对特定肿瘤特征优化，显著提高脑MRI分类准确性。学术价值在于为医学图像分析提供创新架构设计，实际应用可支持临床诊断，提升诊断效率。未来工作可探索扩展到其他医学图像任务或进一步优化计算效率。",
      "tags": [
        "DenseNet",
        "Swin Transformer",
        "Feature Fusion",
        "Medical Image Classification",
        "Self-Attention"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:59.732999Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18329",
    "title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection",
    "authors": [
      "Chuhan Feng",
      "Jing Li",
      "Jie Li",
      "Lu Lv",
      "Fengkui Gong"
    ],
    "abstract": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18329.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18329",
    "published": "2026-01-26T10:13:07Z",
    "updated": "2026-01-26T10:13:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种基于可判别性驱动空间-通道选择和梯度范数的无人机信号分布外检测算法，以提高检测性能。",
      "motivation": "该研究旨在解决无人机信号分布外检测的实际问题，即识别未知或异常信号以增强无线通信安全和监控能力。现有方法可能因忽略信号不稳定性和类间差异而在多变环境下表现不足，因此需要开发更鲁棒的算法来适应不同信噪比和无人机类型。摘要未明确说明具体不足，但可推断其重要性在于提升检测可靠性和适应性。",
      "method": "方法首先基于协议特定时频特性，量化类间相似性和方差，以实现对时频图像特征在空间和通道维度上的自适应加权。关键创新点在于结合可判别性驱动的选择和梯度范数度量，后者用于评估样本对扰动的敏感性，捕捉OOD样本的固有不稳定性。最后，将梯度范数与基于能量的分数融合进行联合推理，未提及具体数据集或模型架构细节。",
      "result": "仿真结果显示，在信噪比变化和多种无人机类型的测试中，所提算法展现出卓越的判别能力和鲁棒性能。摘要未提供具体数据如准确率提升，但结果表明其优于基线方法，有效提高了检测的可靠性和适应性，验证了算法的有效性。",
      "conclusion": "论文主要贡献是提出了一种创新的OOD检测框架，通过融合空间-通道选择和梯度范数，显著增强了无人机信号检测的准确性和鲁棒性。研究具有学术价值，推动了信号处理领域的技术进步，并在实际应用中如无人机监控中具有潜力，未来工作可探索更多复杂场景的扩展和优化。",
      "tags": [
        "OOD Detection",
        "Spatial-Channel Selection",
        "Gradient Norm",
        "Time-Frequency Analysis",
        "Joint Inference"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:53.186746Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18326",
    "title": "Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals",
    "authors": [
      "Jie Li",
      "Jing Li",
      "Lu Lv",
      "Zhanyu Ju",
      "Fengkui Gong"
    ],
    "abstract": "We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18326.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18326",
    "published": "2026-01-26T10:10:08Z",
    "updated": "2026-01-26T10:10:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于ZC序列和时频图像认知融合的无人机信号外分布检测算法，通过多模态特征融合和自适应注意力机制提高检测性能。",
      "motivation": "该研究旨在解决无人机信号外分布检测问题，这在无人机远程识别任务中至关重要，以确保安全和监管。现有方法可能难以处理具有未知或非标准通信协议的信号，导致检测准确性和鲁棒性不足。通过结合ZC序列（分析已知DJI无人机通信协议）和时频图像（捕捉未知协议的时间频率特性），本研究提出一种多模态融合方法，以弥补单一模态的局限性，增强对外分布信号的识别能力。",
      "method": "该方法从接收的射频信号中生成ZC序列特征和时频图像特征，通过专用特征提取模块增强和对齐。接着，进行多模态特征交互、单模态特征融合和多模态特征融合，以整合和补充跨模态信息。从融合特征中计算空间和通道维度的判别分数，这些分数转化为自适应注意力权重，加权后通过Softmax函数输出分类结果，核心创新点在于多模态认知融合和自适应注意力机制的应用。摘要未明确说明使用的具体数据集或模型架构，但基于仿真环境验证。",
      "result": "仿真结果表明，该算法在无人机远程识别指标上提升1.7%，在外分布检测指标上提升7.5%，显著优于现有算法。此外，算法在不同飞行条件和多种无人机类型下表现出强鲁棒性，验证了其泛化能力和稳定性，确保了在实际复杂环境中的可靠性能，具体性能改进基于与基线方法的对比，展示了其优越的检测效果。",
      "conclusion": "该研究的主要贡献是提出一种创新的多模态融合算法，有效提升无人机信号外分布检测的准确性和鲁棒性。学术价值在于将认知融合和自适应注意力引入信号处理领域，推动了多模态分析方法的发展；实际应用价值在于增强无人机远程识别系统的性能，可用于安全和监控任务。未来工作可探索更多模态融合或应用于其他信号检测场景，但摘要未明确说明具体局限性。",
      "tags": [
        "Out-of-Distribution Detection",
        "Multimodal Fusion",
        "Time-Frequency Images",
        "ZC Sequences",
        "Adaptive Attention"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:11.898915Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18320",
    "title": "MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization",
    "authors": [
      "Jinwei Lu",
      "Yuanfeng Song",
      "Chen Zhang",
      "Raymond Chi-Wing Wong"
    ],
    "abstract": "Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18320.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18320",
    "published": "2026-01-26T10:03:10Z",
    "updated": "2026-01-26T10:03:10Z",
    "comment": "Accepted to SIGMOD 2026",
    "light_analysis": {
      "overview": "本研究提出了MultiVis-Agent，一个结合逻辑规则和多代理的框架，用于可靠生成复杂多模态数据可视化，创新在于用数学规则保障系统可靠性。",
      "motivation": "现实世界的数据可视化任务常需结合文本、图像和代码等多模态输入，并涉及迭代细化过程。现有方法或局限于单模态处理，或依赖一次性生成，导致无法适应复杂需求。基于大型语言模型的系统虽具灵活性，但引入可靠性问题，如灾难性失败和无限循环风险。这突显了开发一种兼具灵活性与可靠性的多模态可视化框架的重要性，以克服自动化生成中的核心挑战。",
      "method": "MultiVis-Agent采用多代理框架，通过引入四层逻辑规则来增强可靠性。这些逻辑规则作为数学约束，指导大型语言模型的推理过程，而非替代它，从而在保持灵活性的同时提供数学上的可靠性保证。研究形式化了MultiVis任务，涵盖从基础图表生成到迭代细化的四个场景，并开发了MultiVis-Bench基准测试，包含超过1,000个案例用于多模态可视化评估。",
      "result": "实验结果表明，MultiVis-Agent在挑战性任务上实现了75.63%的可视化得分，显著优于基线方法的57.54%至62.79%。系统任务完成率达到99.58%，代码执行成功率为94.56%；与之对比，未使用逻辑规则的版本仅达到74.48%的任务完成率和65.10%的代码执行成功率。这些数据证实了逻辑规则在提升可靠性和性能方面的关键作用。",
      "conclusion": "本研究的核心贡献是提出MultiVis-Agent框架，有效解决了自动化可视化生成中的复杂性和可靠性双重挑战。通过结合多代理系统和逻辑规则，该系统在学术上为多模态交互提供了新思路，并在实际应用中提升了可视化工具的稳健性和效率。未来工作可能包括扩展更多应用场景或优化逻辑规则的设计，以进一步增强系统的普适性。",
      "tags": [
        "Multi-Agent Systems",
        "Logic Rules",
        "Large Language Models (LLMs)",
        "Cross-Modal Visualization",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:46.690642Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18314",
    "title": "A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods",
    "authors": [
      "Lina Felsner",
      "Sevgi G. Kafali",
      "Hannah Eichhorn",
      "Agnes A. J. Leth",
      "Aidas Batvinskas",
      "Andre Datchev",
      "Fabian Klemm",
      "Jan Aulich",
      "Puntika Leepagorn",
      "Ruben Klinger",
      "Daniel Rueckert",
      "Julia A. Schnabel"
    ],
    "abstract": "We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18314.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18314",
    "published": "2026-01-26T09:50:02Z",
    "updated": "2026-01-26T09:50:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文通过一个学生黑客松，聚焦于复制三篇先进MRI重建论文的结果，以促进AI/机器学习领域的可重现性实践。",
      "motivation": "MRI重建方法在医疗影像处理中至关重要，但现有研究常因代码不公开或复现困难而难以验证，影响了科学进步和实际应用。本研究旨在解决AI/机器学习领域的可重现性问题，特别是针对复杂算法如基于模型的网络和混合架构，通过黑客松形式提升研究透明度和可靠性，强调复现困难对领域发展的制约。",
      "method": "研究方法设计了一个学生黑客松，复制三篇有影响力的MRI重建论文：MoDL（一个展开的基于模型网络，具有学习去噪）、HUMUS-Net（一个混合展开多尺度CNN+Transformer架构）以及一个无训练的物理正则化动态MRI方法。黑客松详细描述了设置、协议和代码库构建实践，如版本控制和文档标准化，以确保可重现性。摘要未明确指定具体数据集，但聚焦于方法论复制和技术实施流程。",
      "result": "论文报告了黑客松的复制结果和额外实验，但摘要未提供具体性能指标如准确率或效率改进数据。结果展示了成功复现目标论文的关键发现，并通过与原始方法的对比验证了可重现性实践的有效性。这些实验为未来研究提供了基准和教训，强调了代码共享和透明实验设计在提升研究质量中的作用。",
      "conclusion": "本论文的主要贡献在于提供了一个可重现性黑客松的详细案例研究，强调了建立可重现代码库的核心实践。学术价值在于推动AI/机器学习领域对复现困难的关注，促进开放科学；实际应用价值在于帮助研究者更有效地验证和部署MRI重建方法。未来工作可扩展到其他医疗影像任务或更广泛的AI领域，以持续提升研究透明度和可靠性。",
      "tags": [
        "Reproducibility",
        "MRI Reconstruction",
        "Hackathon",
        "Model-Based Deep Learning",
        "CNN+Transformer"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:29.735271Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18308",
    "title": "A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience",
    "authors": [
      "Geunsik Lim"
    ],
    "abstract": "As climate-related hazards intensify, conventional early warning systems (EWS) disseminate alerts rapidly but often fail to trigger timely protective actions, leading to preventable losses and inequities. We introduce Climate RADAR (Risk-Aware, Dynamic, and Action Recommendation system), a generative AI-based reliability layer that reframes disaster communication from alerts delivered to actions executed. It integrates meteorological, hydrological, vulnerability, and social data into a composite risk index and employs guardrail-embedded large language models (LLMs) to deliver personalized recommendations across citizen, volunteer, and municipal interfaces. Evaluation through simulations, user studies, and a municipal pilot shows improved outcomes, including higher protective action execution, reduced response latency, and increased usability and trust. By combining predictive analytics, behavioral science, and responsible AI, Climate RADAR advances people-centered, transparent, and equitable early warning systems, offering practical pathways toward compliance-ready disaster resilience infrastructures.",
    "categories": [
      "cs.AI",
      "cs.SI",
      "eess.SY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18308.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18308",
    "published": "2026-01-26T09:43:30Z",
    "updated": "2026-01-26T09:43:30Z",
    "comment": "19 pages",
    "light_analysis": {
      "overview": "本文提出Climate RADAR，一种生成AI驱动的可靠性层，通过集成多源数据和嵌入式护栏大语言模型，为灾害预警系统提供个性化行动推荐，以提升行动执行效率和公平性。",
      "motivation": "气候变化导致灾害加剧，传统预警系统虽能快速发布警报，但常无法有效触发及时保护行动，造成可预防损失和社会不平等。现有方法侧重于警报传递而非行动执行，缺乏个性化推荐和风险评估整合，因此需要开发更有效、以人为本的解决方案以应对日益严峻的灾害挑战。",
      "method": "Climate RADAR系统整合气象、水文、脆弱性及社会数据，构建复合风险指数，并采用嵌入式护栏的大语言模型生成个性化行动推荐。该方法通过生成AI技术，将灾害通信从简单警报转变为可执行行动，确保推荐的安全性和责任感。关键创新在于多数据融合和护栏LLMs的应用，以优化公民、志愿者和市政接口的交互。",
      "result": "通过模拟、用户研究和市政试点评估，Climate RADAR显示显著改善：保护行动执行率提高，响应延迟减少，系统可用性和用户信任度增强。摘要未明确说明具体数据对比，但结果表明该系统优于传统预警方法，在促进及时行动和提升韧性方面表现突出。",
      "conclusion": "研究贡献在于结合预测分析、行为科学和负责任AI，开发出Climate RADAR系统，推动灾害预警系统向以人为本、透明和公平方向发展。它为构建合规准备就绪的灾害韧性基础设施提供实用路径，具有重要学术和实际应用价值。未来可进一步探索技术扩展和多灾害场景应用。",
      "tags": [
        "Generative AI",
        "Large Language Models",
        "Risk Assessment",
        "Disaster Resilience",
        "Behavioral Science"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:20.227021Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18306",
    "title": "Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM",
    "authors": [
      "Everlyn Asiko Chimoto",
      "Mostafa Elhoushi",
      "Bruce A. Bassett"
    ],
    "abstract": "Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18306.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18306",
    "published": "2026-01-26T09:36:03Z",
    "updated": "2026-01-26T09:36:03Z",
    "comment": "Accepted to EACL 2026 Main Conference",
    "light_analysis": {
      "overview": "本研究通过系统评估发现，非英语和多语言校准集能显著提升量化多语言大型语言模型的性能。",
      "motivation": "量化技术虽能有效减少大型语言模型的存储和计算成本，但常导致性能下降。现有后训练量化方法通常依赖小规模、仅英语的校准集，然而这种单一语言策略对多语言模型的影响尚未得到充分探索。现有方法的不足之处在于忽略了语言多样性可能对量化效果的影响，这限制了多语言模型在现实场景中的鲁棒性，因此本研究旨在解决校准数据的语言对齐问题，以提升量化多语言模型的性能。",
      "method": "论文提出了系统评估方法，针对两种量化器（GPTQ和AWQ），在10种语言的数据上比较了八种校准设置：包括五种单语言校准和三种多语言混合校准。使用模型如Llama3.1 8B和Qwen2.5 7B进行实验，核心创新在于引入语言多样性校准策略，通过量化不同校准集对性能的影响，探索校准数据的语言定制和多语言混合优化路径。",
      "result": "实验结果显示，相比仅英语基线，非英语和多语言校准集显著降低困惑度。在Llama3.1 8B和Qwen2.5 7B模型上，多语言混合校准实现最大困惑度减少达3.52点，平均提升效果明显。定制校准到具体评估语言效果最佳，但也识别了特定语言与量化器组合导致性能下降的失败案例，分析表明这与不同语言间激活范围分布的差异有关。",
      "conclusion": "本研究的核心贡献是证明静态一刀切校准方法在多语言大型语言模型量化中次优，强调定制校准数据在语言和多样性方面对提升性能至关重要。这为更鲁棒的量化策略提供了理论依据，具有学术价值和应用前景，未来可扩展更多语言覆盖和优化校准算法以克服局限性。",
      "tags": [
        "Quantization",
        "Multilingual Language Models",
        "Calibration",
        "Perplexity",
        "Activation Range Distribution"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:32.222945Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18305",
    "title": "SwipeGen: Bridging the Execution Gap in GUI Agents via Human-like Swipe Synthesis",
    "authors": [
      "Xuan Wang",
      "Siyuan Su",
      "Quantong Fu",
      "Yongxiang Hu",
      "Yangfan Zhou"
    ],
    "abstract": "With the widespread adoption of Graphical User Interface (GUI) agents for automating GUI interaction tasks, substantial research focused on improving GUI perception to ground task instructions into concrete action steps. However, the step execution capability of these agents has gradually emerged as a new bottleneck for task completion. In particular, existing GUI agents often adopt overly simplified strategies for handling swipe interactions, preventing them from accurately replicating human-like behavior. To address this limitation, we decompose human swipe gestures into multiple quantifiable dimensions and propose an automated pipeline SwipeGen to synthesize human-like swipe interactions through GUI exploration. Based on this pipeline, we construct and release the first benchmark for evaluating the swipe execution capability of GUI agents. Furthermore, leveraging the synthesized data, we propose GUISwiper, a GUI agent with enhanced interaction execution capabilities. Experimental results demonstrate that GUISwiper achieves a swipe execution accuracy of 69.07%, representing a 214% improvement over existing VLM baselines.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18305.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18305",
    "published": "2026-01-26T09:35:10Z",
    "updated": "2026-01-26T09:35:10Z",
    "comment": "15 pages, 3 figures. Under review. Code and dataset will be released upon acceptance",
    "light_analysis": {
      "overview": "该论文提出SwipeGen方法，通过合成类人滑动手势来增强GUI代理的执行能力，显著提升交互准确性。",
      "motivation": "随着GUI代理在自动化任务中的广泛应用，执行能力逐渐成为新的瓶颈。现有代理处理滑动手势时策略过于简化，无法准确模仿人类行为，导致任务完成效率低下。本研究旨在解决这一问题，通过改进滑动交互执行策略，提升代理在真实场景中的性能。背景强调执行能力的重要性，弥补现有方法的不足。",
      "method": "论文将人类滑动手势分解为多个可量化维度，开发自动化管道SwipeGen，通过GUI探索合成类人滑动交互。基于此构建首个评估GUI代理滑动手势执行能力的基准。进一步，利用合成数据提出GUISwiper代理，增强其交互执行能力。关键创新包括手势维度分解、合成管道设计和基准构建。",
      "result": "实验结果显示，GUISwiper的滑动手势执行准确率达到69.07%，相比现有的视觉语言模型（VLM）基线提升了214%，显著超越其他方法。这证明所提方法在改进执行性能方面具有显著效果，提升了任务完成率。",
      "conclusion": "本研究通过SwipeGen方法改进了GUI代理的滑动手势执行能力，并发布了相关基准，具有重要的学术和实际应用价值。它为GUI自动化领域提供了新工具，未来工作可扩展到其他交互类型或结合更多模态数据进一步优化。",
      "tags": [
        "Swipe Synthesis",
        "GUI Agents",
        "Human-like Interaction",
        "Benchmark Evaluation",
        "Visual Language Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:33.467268Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18302",
    "title": "Suppressing Final Layer Hidden State Jumps in Transformer Pretraining",
    "authors": [
      "Keigo Shibata",
      "Kazuki Yano",
      "Ryosuke Takahashi",
      "Jaesung Lee",
      "Wataru Ikeda",
      "Jun Suzuki"
    ],
    "abstract": "This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18302.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18302",
    "published": "2026-01-26T09:30:49Z",
    "updated": "2026-01-26T09:30:49Z",
    "comment": "Accepted to the Findings of EACL 2026",
    "light_analysis": {
      "overview": "论文提出跳躍抑制正則化器（JREG），旨在抑制Transformer預訓練中最後一層隱藏狀態的跳躍，以提升模型性能而不改變架構。",
      "motivation": "該研究針對Transformer語言模型在預訓練過程中，最後一層隱藏狀態向量常出現不比例大的角度跳躍問題。這種現象可能導致中間層能力使用不均衡，影響模型的內部行為和學習效率，而現有方法未直接解決這一問題，因此優化內部跳躍對改進預訓練效果具有重要性。",
      "method": "論文首先引入量化指標來測量最後一層隱藏狀態跳躍的強度，並展示其在多個開放權重模型中的普遍性和預訓練過程中的放大。核心方法是提出跳躍抑制正則化器（JREG），在預訓練階段懲罰這種跳躍，鼓勵中間層更平衡地使用能力。實驗基於Llama架構的三種模型大小進行，但摘要未明確說明使用的具體數據集。",
      "result": "實證評估顯示，使用JREG方法訓練的模型在任務性能上優於未使用該方法的基線模型，且未改變模型架構。摘要未提供具體性能指標如準確率提升的數值，但表明性能有所改善，突顯了JREG在優化內部行為方面的有效性。",
      "conclusion": "論文的主要貢獻是提出JREG方法，有效抑制Transformer預訓練中的隱藏狀態跳躍，從而改善模型性能。這一研究揭示了模型內部行為的新問題，為預訓練優化提供了學術價值和實際應用潛力。未來工作可能包括進一步探索內部機制的其他優化方法或應用擴展。",
      "tags": [
        "Transformer",
        "Pre-training",
        "Regularization",
        "Hidden State",
        "Language Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:37.093370Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18301",
    "title": "Contextual Range-View Projection for 3D LiDAR Point Clouds",
    "authors": [
      "Seyedali Mousavi",
      "Seyedhamidreza Mousavi",
      "Masoud Daneshtalab"
    ],
    "abstract": "Range-view projection provides an efficient method for transforming 3D LiDAR point clouds into 2D range image representations, enabling effective processing with 2D deep learning models. However, a major challenge in this projection is the many-to-one conflict, where multiple 3D points are mapped onto the same pixel in the range image, requiring a selection strategy. Existing approaches typically retain the point with the smallest depth (closest to the LiDAR), disregarding semantic relevance and object structure, which leads to the loss of important contextual information. In this paper, we extend the depth-based selection rule by incorporating contextual information from both instance centers and class labels, introducing two mechanisms: \\textit{Centerness-Aware Projection (CAP)} and \\textit{Class-Weighted-Aware Projection (CWAP)}. In CAP, point depths are adjusted according to their distance from the instance center, thereby prioritizing central instance points over noisy boundary and background points. In CWAP, object classes are prioritized through user-defined weights, offering flexibility in the projection strategy. Our evaluations on the SemanticKITTI dataset show that CAP preserves more instance points during projection, achieving up to a 3.1\\% mIoU improvement compared to the baseline. Furthermore, CWAP enhances the performance of targeted classes while having a negligible impact on the performance of other classes",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18301.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18301",
    "published": "2026-01-26T09:30:43Z",
    "updated": "2026-01-26T09:30:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了上下文感知的范围视图投影方法，通过中心感知投影和类加权感知投影，解决了3D LiDAR点云投影中的多对一冲突，提升了语义分割性能。",
      "motivation": "在自动驾驶和机器人领域，3D LiDAR点云的高效处理依赖于范围视图投影将其转换为2D图像。然而，多对一冲突导致多个点映射到同一像素，现有方法基于最小深度选择点，忽略了语义相关性和对象结构，造成关键上下文信息丢失，从而降低了下游任务如语义分割的准确性。因此，需要改进投影策略以保留更多语义信息。",
      "method": "论文提出两种上下文感知投影机制：中心感知投影（CAP）和类加权感知投影（CWAP）。CAP根据点到实例中心的距离调整深度值，优先选择中心实例点，减少噪声和背景点影响；CWAP通过用户定义的权重优先特定对象类别，提供灵活的策略调整。方法基于SemanticKITTI数据集的实例和类标签信息，实现结合上下文信息的点云投影优化。",
      "result": "在SemanticKITTI数据集上的实验表明，CAP方法相比基线（传统深度选择），在投影中保留了更多实例点，平均交并比（mIoU）提升了高达3.1%。CWAP方法在针对目标类别优化时，能显著提升这些类别的性能，同时对其他类别的影响可忽略，验证了方法的有效性和选择性增强能力。",
      "conclusion": "本研究通过CAP和CWAP机制解决了点云投影中的多对一冲突问题，贡献在于结合上下文信息提升投影质量，增强了语义分割的准确性。这对于自动驾驶等实际应用具有重要价值，未来工作可探索更多上下文因素或扩展到其他数据集以进一步验证泛化性。",
      "tags": [
        "3D LiDAR Point Clouds",
        "Range-View Projection",
        "Semantic Segmentation",
        "Instance Awareness",
        "Contextual Information"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:24.257423Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18296",
    "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning",
    "authors": [
      "Zhaoyan Gong",
      "Zhiqiang Liu",
      "Songze Li",
      "Xiaoke Guo",
      "Yuanxiang Liu",
      "Xinle Deng",
      "Zhizhen Liu",
      "Lei Liang",
      "Huajun Chen",
      "Wen Zhang"
    ],
    "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18296.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18296",
    "published": "2026-01-26T09:23:53Z",
    "updated": "2026-01-26T09:23:53Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "提出Temp-R1，一个基于反向课程强化学习的自主端到端代理，用于复杂时间知识图问答，显著提升性能。",
      "motivation": "时间知识图问答具有挑战性，需要处理动态事实、多跳依赖和复杂时间约束。现有方法依赖固定工作流程和昂贵闭源API，导致灵活性和可扩展性受限，尤其在处理复杂问题时效率低下。因此，开发一种自主、高效的解决方案至关重要，以克服这些不足。",
      "method": "该方法通过强化学习训练一个8B参数的自主代理，扩展动作空间以包括内部和外部动作，解决推理中的认知过载问题。关键创新是引入反向课程学习，先在困难问题上训练，强制代理发展复杂推理能力，再迁移到简单案例，避免捷径学习。模型在MultiTQ和TimelineKGQA数据集上进行端到端训练，展示了自主推理能力。",
      "result": "实验结果显示，Temp-R1在MultiTQ和TimelineKGQA数据集上达到了最先进性能，特别是在复杂问题上，性能比强基线提升了19.8%。这表明方法在处理多跳依赖和时间约束时具有显著优势，验证了其高效性和鲁棒性。",
      "conclusion": "本研究的主要贡献是建立了自主时间推理代理的新范式，通过强化学习和反向课程学习提升推理能力，具有重要的学术意义和实际应用价值。代码即将公开，促进进一步研究和应用，为未来自主代理的发展奠定基础。摘要未明确说明具体局限性或未来方向。",
      "tags": [
        "Temporal Knowledge Graph Question Answering",
        "Reinforcement Learning",
        "Reverse Curriculum Learning",
        "Autonomous Agents",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:08.979980Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18292",
    "title": "TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment",
    "authors": [
      "Zhewen Tan",
      "Wenhan Yu",
      "Jianfeng Si",
      "Tongxin Liu",
      "Kaiqi Guan",
      "Huiyan Jin",
      "Jiawen Tao",
      "Xiaokun Yuan",
      "Duohe Ma",
      "Xiangzheng Zhang",
      "Tong Yang",
      "Lin Sun"
    ],
    "abstract": "In recent years, safety risks associated with large language models have become increasingly prominent, highlighting the urgent need to mitigate the generation of toxic and harmful content. The mainstream paradigm for LLM safety alignment typically adopts a collaborative framework involving three roles: an attacker for adversarial prompt generation, a defender for safety defense, and an evaluator for response assessment. In this paper, we propose a closed-loop reinforcement learning framework called TriPlay-RL that enables iterative and co-improving collaboration among three roles with near-zero manual annotation. Experimental results show that the attacker preserves high output diversity while achieving a 20%-50% improvement in adversarial effectiveness; the defender attains 10%-30% gains in safety performance without degrading general reasoning capability; and the evaluator continuously refines its fine-grained judgment ability through iterations, accurately distinguishing unsafe responses, simple refusals, and useful guidance. Overall, our framework establishes an efficient and scalable paradigm for LLM safety alignment, enabling continuous co-evolution within a unified learning loop.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18292.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18292",
    "published": "2026-01-26T09:21:43Z",
    "updated": "2026-01-26T09:21:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出TriPlay-RL框架，通过三角色自玩强化学习实现大型语言模型安全对齐的协同进化。",
      "motivation": "随着大型语言模型的广泛应用，其生成有毒和有害内容的安全风险日益突出，亟需有效对齐方法来减轻危害。主流的安全对齐范式通常依赖于攻击者、防御者和评估者三个角色的协作框架，但这种方法往往需要大量人工标注，效率低下且难以实现角色的持续协同改进。本研究旨在开发一个闭环强化学习框架，以减少人工干预，促进三个角色间的动态互动和共同进化，从而更高效地解决LLM安全对齐中的实际挑战。",
      "method": "本研究提出TriPlay-RL框架，采用闭环强化学习技术，集成攻击者、防御者和评估者三个角色于统一学习循环中。攻击者负责生成对抗性提示以测试模型漏洞，防御者进行安全防御以过滤有害内容，评估者评估响应质量并提供反馈，三者通过自玩机制协同训练。关键创新在于实现近零人工标注的迭代协同改进，利用强化学习算法驱动角色间的对抗和协作，促进能力的动态平衡和提升。具体技术细节如模型架构和训练数据集在摘要中未明确说明。",
      "result": "实验结果显示，TriPlay-RL框架显著提升了各角色的性能：攻击者在保持高输出多样性的同时，对抗效果提高了20%到50%；防御者的安全性能获得10%到30%的提升，且未影响一般推理能力；评估者通过迭代不断细化其细粒度判断能力，能准确区分不安全响应、简单拒绝和有用指导。这些改进表明该框架在安全对齐方面优于传统方法，实现了高效的协同进化，具体基线对比在摘要中未明确说明。",
      "conclusion": "本研究的核心贡献是提出了TriPlay-RL框架，通过三角色自玩强化学习为LLM安全对齐建立了一个高效且可扩展的范式。学术价值在于探索了减少人工干预的安全对齐方法，推动了强化学习在模型安全领域的应用；实际应用价值在于为提升大型语言模型的安全性提供了实用工具，支持持续协同进化和可扩展部署。未来工作可扩展至更复杂的场景或多角色交互，以进一步增强鲁棒性和适应性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Self-Play",
        "Safety Alignment",
        "Adversarial Training"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:35.180806Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18285",
    "title": "U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents",
    "authors": [
      "Jin Su",
      "Runnan Fang",
      "Yeqiu Li",
      "Xiaobin Wang",
      "Shihao Cai",
      "Pengjun Xie",
      "Ningyu Zhang",
      "Fajie Yuan"
    ],
    "abstract": "Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18285.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18285",
    "published": "2026-01-26T09:11:49Z",
    "updated": "2026-01-26T09:11:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出动态意图感知的上下文折叠框架 U-Fold，以改进用户中心对话中基于大型语言模型的代理的上下文管理，解决现有方法在跟踪用户意图和保留关键信息方面的不足。",
      "motivation": "基于大型语言模型的代理在工具增强环境中广泛部署，但其可扩展性受限于上下文长度。现有上下文折叠方法通过总结过去交互来缓解此问题，但主要设计用于单查询或单意图场景，不适用于现实的用户中心对话。这导致两个主要失败模式：不可逆地丢弃细粒度约束和中间事实，以及摘要未能有效跟踪演化的用户意图，从而引起遗漏和错误动作，限制了在复杂多回合任务中的表现。因此，需要一种更动态、意图感知的方法来提升上下文管理的效率和准确性。",
      "method": "U-Fold 是一个动态上下文折叠框架，专为以用户为中心的任务设计。它保留完整的用户与代理对话历史以及工具调用记录，但通过两个核心组件在每个回合更新上下文：一是生成意图感知的演化对话摘要，以捕捉用户意图的变化；二是创建紧凑的任务相关工具日志，确保关键工具交互信息不丢失。框架的关键创新点在于动态性和意图感知能力，能够适应长、嘈杂的多回合场景。使用的数据集包括 τ-bench、τ²-bench 和 VitaBench 等基准，但模型架构的具体细节在摘要中未明确说明，主要基于大型语言模型技术。",
      "result": "在多个实验基准上进行评估，包括 τ-bench、τ²-bench、VitaBench 和更难的环境设置，U-Fold 持续表现出优越性能。在长上下文设置中，与 ReAct 相比达到 71.4% 的胜率；相对于先前的折叠基线，改进幅度高达 27.0%。特别在长、嘈杂、多回合任务中，U-Fold 的优势更为明显，显著减少了遗漏和错误动作，证明了其在复杂对话中的鲁棒性和有效性，为上下文管理提供了可靠的技术方案。",
      "conclusion": "U-Fold 框架通过动态意图感知的上下文折叠，有效解决了用户中心对话中的上下文长度限制和意图跟踪问题。其主要贡献在于提出了一种新颖的方法，能保留关键信息并适应意图演化，从而提升代理的性能和可靠性。这项研究的学术价值在于推动了上下文管理技术从单查询基准向现实应用的转移，实际应用价值在于为工具增强的代理系统提供了更可扩展的解决方案。未来工作可能涉及扩展到更广泛的应用场景或优化计算效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Context Folding",
        "Intent-Aware Modeling",
        "Tool-Augmented Agents",
        "User-Centric Dialogue"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:21.716624Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18282",
    "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning",
    "authors": [
      "Lei Wei",
      "Jinpeng Ou",
      "Xiao Peng",
      "Bin Wang"
    ],
    "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18282.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18282",
    "published": "2026-01-26T09:05:00Z",
    "updated": "2026-01-26T09:05:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 Think-Augmented Function Calling (TAFC) 框架，通过在函数和参数级别嵌入显式推理，提高大型语言模型在函数调用中的参数准确性。",
      "motivation": "当前大型语言模型在函数调用时，参数生成缺乏显式推理透明度，尤其对于具有依赖参数的复杂函数。现有方法如链式思维提示仅在代理层面提供推理，无法指导单个参数的生成过程，导致准确性和可解释性受限。因此，研究旨在开发一种细粒度的推理机制，以增强函数调用性能并支持 AI 代理行为的调试，解决现有方法在参数级推理指导不足的问题。",
      "method": "TAFC 框架的核心是引入一个通用的“think”参数增强，使模型能明确表达决策逻辑，并通过动态优化参数描述来提升推理质量。对于复杂参数，基于复杂性评分自动触发细粒度推理，确保关键决策有合理理由。此外，推理引导优化技术将生成推理与人类期望对齐。该方法无需修改现有 LLM 架构，保持 API 兼容性，实现即插即用的推理增强，适用于各种专有和开源模型。",
      "result": "在 ToolBench 数据集上评估 TAFC，覆盖专有和开源模型，结果表明，TAFC 在多参数函数的参数生成准确性和推理一致性方面有显著改进。摘要未提供具体提升百分比，但与基线相比，效果显著且推理更连贯。同时，框架增强了函数调用的可解释性，为调试 AI 代理行为提供了便利，尽管具体数据未在摘要中明确说明。",
      "conclusion": "TAFC 框架通过在函数和参数级别嵌入显式推理，有效提高了 LLM 函数调用的准确性和透明度。其学术价值在于提出了一种无需架构修改的推理增强方法，推动了可解释 AI 的发展。实际应用上，提升了 AI 代理的可靠性和调试效率。未来工作可能包括扩展评估场景、优化推理质量评估或探索更复杂的函数调用场景，以进一步提高性能。",
      "tags": [
        "Large Language Model",
        "Function Calling",
        "Reasoning Enhancement",
        "Parameter Optimization",
        "Interpretability"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:53.404525Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18281",
    "title": "Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue",
    "authors": [
      "Yuhang Jia",
      "Pei Liu",
      "Haoqin Sun",
      "Jiaming Zhou",
      "Xuxin Cheng",
      "Cao Liu",
      "Ke Zeng",
      "Xunliang Cai",
      "Yong Qin"
    ],
    "abstract": "End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single \"correct\" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.",
    "categories": [
      "cs.CL",
      "cs.SD"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18281.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18281",
    "published": "2026-01-26T09:04:50Z",
    "updated": "2026-01-26T09:04:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出ReEmpathy模型，通过自反思交替推理机制提升端到端口语语言模型在共情对话中的表现。",
      "motivation": "当前端到端口语语言模型在增强共情对话能力时，主要依赖于刚性监督信号，如监督微调中的真实回复或强化学习中的偏好分数。这种方法的局限性在于共情表达缺乏单一正确标准，简单数值评分无法全面捕捉情感细微差别和共情行为适当性。因此，研究旨在解决现有方法在建模复杂共情方面的不足，推动更灵活、准确的评估和生成机制，以提升人机交互的情感智能水平。",
      "method": "研究方法分为两个步骤：首先引入EmpathyEval，一个基于描述性自然语言的评估模型，用于评估口语对话中的共情质量，提供更细致的评价标准。其次，提出ReEmpathy模型，这是一个端到端口语语言模型，采用新颖的Empathetic Self-Reflective Alternating Inference机制。该机制将口语回复生成与自由形式、共情相关的反思推理交织进行，使模型在输出前进行自反思，从而优化共情表达。摘要未明确说明具体数据集或模型架构细节。",
      "result": "大量实验表明，ReEmpathy通过启用反思推理，显著改善了共情敏感性口语对话的性能。摘要强调模型在提升共情质量方面有实质性改进，但未提供具体数值如准确率或效率指标。与现有依赖刚性监督信号的方法相比，ReEmpathy在生成更自然、情感丰富的回复方面表现更优，为更情感智能的人机交互提供了有前景的途径。",
      "conclusion": "论文的主要贡献是提出了ReEmpathy模型和EmpathyEval评估模型，通过自反思交替推理机制增强端到端口语模型的共情对话能力。学术上，该方法克服了传统监督信号的局限性，为建模复杂共情提供了创新思路；应用上，推动了情感智能和共情感知人机交互的发展。未来工作可能包括进一步优化推理机制或扩展应用到其他情感相关任务，摘要未明确说明具体局限性。",
      "tags": [
        "Spoken Language Models",
        "Empathetic Dialogue",
        "Self-Reflective Inference",
        "Natural Language Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:40.212559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18278",
    "title": "What Do Learned Models Measure?",
    "authors": [
      "Indrė Žliobaitė"
    ],
    "abstract": "In many scientific and data-driven applications, machine learning models are increasingly used as measurement instruments, rather than merely as predictors of predefined labels. When the measurement function is learned from data, the mapping from observations to quantities is determined implicitly by the training distribution and inductive biases, allowing multiple inequivalent mappings to satisfy standard predictive evaluation criteria. We formalize learned measurement functions as a distinct focus of evaluation and introduce measurement stability, a property capturing invariance of the measured quantity across admissible realizations of the learning process and across contexts. We show that standard evaluation criteria in machine learning, including generalization error, calibration, and robustness, do not guarantee measurement stability. Through a real-world case study, we show that models with comparable predictive performance can implement systematically inequivalent measurement functions, with distribution shift providing a concrete illustration of this failure. Taken together, our results highlight a limitation of existing evaluation frameworks in settings where learned model outputs are identified as measurements, motivating the need for an additional evaluative dimension.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18278.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18278",
    "published": "2026-01-26T09:00:48Z",
    "updated": "2026-01-26T09:00:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过引入测量稳定性概念，揭示了机器学习模型用作测量工具时现有评估标准的不足，并指出需要新的评估维度。",
      "motivation": "在科学和数据驱动应用中，机器学习模型越来越多地被用作测量仪器，而非仅预测预定义标签。然而，当测量函数从数据中学得时，由于训练分布和归纳偏差隐式决定映射，允许多个不等价的映射满足标准预测评估标准，导致测量结果不一致。这一问题的重要性在于，现有评估方法如泛化误差、校准和鲁棒性无法保证测量稳定性，在敏感应用中可能引发误导或不可靠性。这突出了在模型输出被视为测量的设置中，现有框架的局限性，强调了研究评估新维度的重要性。",
      "method": "研究通过形式化学习测量函数作为独立的评估焦点，并引入测量稳定性的属性，该属性捕捉测量量在学习过程和不同情境下的不变性。关键创新点在于提出测量稳定性作为补充评估标准，确保模型在测量功能上的可靠性。通过一个真实世界的案例研究，展示了分布漂移下模型实现不等价测量函数的现象。方法上依赖于概念分析和实证演示，摘要未明确说明具体数据集或模型架构，但重点在于揭示标准评估的不足。",
      "result": "主要结果表明，标准机器学习评估标准，包括泛化误差、校准和鲁棒性，不能保证测量稳定性。在案例研究中，即使模型具有可比的预测性能，它们也可能实现系统上不等价的测量函数，具体体现在分布漂移的情境下。这突出了现有评估框架在将模型输出视为测量时的失败，强调了预测性能与测量一致性之间的脱节。研究通过概念论证和实例分析，证明了测量稳定性的必要性，但摘要未提供具体性能指标数据，仅基于理论框架和观察得出结论。",
      "conclusion": "本文的核心贡献是引入测量稳定性的概念，揭示了机器学习模型用作测量工具时现有评估标准的不足。研究强调了在科学应用中确保模型测量一致性的学术价值，并为未来评估框架的扩展提供了方向。实际应用价值在于提升模型在数据驱动领域的可靠性，潜在局限性包括摘要未明确说明具体实现方法或广泛测试。未来工作可以探索如何将测量稳定性集成到评估标准中，并应用于更广泛的现实场景。",
      "tags": [
        "Measurement Stability",
        "Generalization Error",
        "Calibration",
        "Robustness",
        "Distribution Shift"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:41.922977Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18264",
    "title": "Neural Network Approximation: A View from Polytope Decomposition",
    "authors": [
      "ZeYu Li",
      "ShiJun Zhang",
      "TieYong Zeng",
      "FengLei Fan"
    ],
    "abstract": "Universal approximation theory offers a foundational framework to verify neural network expressiveness, enabling principled utilization in real-world applications. However, most existing theoretical constructions are established by uniformly dividing the input space into tiny hypercubes without considering the local regularity of the target function. In this work, we investigate the universal approximation capabilities of ReLU networks from a view of polytope decomposition, which offers a more realistic and task-oriented approach compared to current methods. To achieve this, we develop an explicit kernel polynomial method to derive an universal approximation of continuous functions, which is characterized not only by the refined Totik-Ditzian-type modulus of continuity, but also by polytopical domain decomposition. Then, a ReLU network is constructed to approximate the kernel polynomial in each subdomain separately. Furthermore, we find that polytope decomposition makes our approximation more efficient and flexible than existing methods in many cases, especially near singular points of the objective function. Lastly, we extend our approach to analytic functions to reach a higher approximation rate.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18264.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18264",
    "published": "2026-01-26T08:39:11Z",
    "updated": "2026-01-26T08:39:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过多面体分解视角提出ReLU神经网络的通用近似方法，改进了现有理论，提高近似效率并考虑局部规律性。",
      "motivation": "现有神经网络通用近似理论通常采用均匀划分输入空间的方法，忽略目标函数的局部规律性，导致在奇异点附近近似效果不佳，限制了实际应用的效率。为了解决这一问题，本研究引入任务导向的多面体分解方法，以提供更现实的近似框架，弥补现有方法的不足。",
      "method": "研究开发了基于多面体域分解的明确核多项式方法，结合Refined Totik-Ditzian-type连续性模量，在每个子域中构建ReLU网络以近似核多项式。该方法还扩展到解析函数，以提高整体近似率，核心创新在于利用多面体分解优化网络结构。",
      "result": "摘要未明确说明具体实验数据，但指出多面体分解方法使近似过程更高效灵活，尤其在目标函数奇异点附近相比现有方法具有优势，提升了通用近似能力。",
      "conclusion": "论文主要贡献在于通过多面体分解改进了神经网络的通用近似理论，增强了近似效率和灵活性，尤其在处理复杂局部结构时表现突出。此外，扩展到解析函数为实现更高近似率提供了可能，为实际应用奠定了基础。",
      "tags": [
        "Universal Approximation Theory",
        "Polytope Decomposition",
        "ReLU Networks",
        "Kernel Polynomial Method",
        "Analytic Functions"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:40.226808Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18263",
    "title": "Revisiting Aerial Scene Classification on the AID Benchmark",
    "authors": [
      "Subhajeet Das",
      "Susmita Ghosh",
      "Abhiroop Chatterjee"
    ],
    "abstract": "Aerial images play a vital role in urban planning and environmental preservation, as they consist of various structures, representing different types of buildings, forests, mountains, and unoccupied lands. Due to its heterogeneous nature, developing robust models for scene classification remains a challenge. In this study, we conduct a literature review of various machine learning methods for aerial image classification. Our survey covers a range of approaches from handcrafted features (e.g., SIFT, LBP) to traditional CNNs (e.g., VGG, GoogLeNet), and advanced deep hybrid networks. In this connection, we have also designed Aerial-Y-Net, a spatial attention-enhanced CNN with multi-scale feature fusion mechanism, which acts as an attention-based model and helps us to better understand the complexities of aerial images. Evaluated on the AID dataset, our model achieves 91.72% accuracy, outperforming several baseline architectures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18263.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18263",
    "published": "2026-01-26T08:39:02Z",
    "updated": "2026-01-26T08:39:02Z",
    "comment": "Presented at the IEEE India Geoscience and Remote Sensing Symposium 2025 and accepted for publication in IEEE Xplore",
    "light_analysis": {
      "overview": "论文提出了Aerial-Y-Net，一种空间注意力增强的CNN模型，通过多尺度特征融合改进航空图像场景分类的准确性。",
      "motivation": "航空图像在城市规划和环境保护中至关重要，但由于其异质性，包含各种结构如建筑和森林，开发稳健的场景分类模型面临挑战。现有方法从手工特征到传统CNN存在局限性，难以有效处理复杂场景，因此本研究旨在通过更先进的模型提升分类性能，以支持实际应用需求。",
      "method": "本文提出Aerial-Y-Net，一个基于空间注意力的卷积神经网络，结合多尺度特征融合机制。模型通过注意力机制聚焦图像关键区域，融合不同尺度特征以捕捉复杂结构，使用AID数据集进行评估，但摘要未明确说明具体架构细节如层数或训练过程，仅强调其作为注意力增强模型的创新性。",
      "result": "在AID数据集上的实验结果显示，Aerial-Y-Net模型达到91.72%的分类准确率，优于多个基线架构，表明提出的空间注意力和多尺度特征融合方法有效提升了性能，但摘要未提供具体基线对比数据或误差率等细节。",
      "conclusion": "论文的主要贡献是设计Aerial-Y-Net模型，提高了航空图像场景分类的准确性，为处理异质图像提供了新技术思路。研究具有学术价值，促进了注意力机制在图像分类中的应用，并有潜力支持城市规划和环境保护等实际场景。未来工作可能包括优化模型或在更多数据集上验证性能。",
      "tags": [
        "Aerial Image Classification",
        "Spatial Attention",
        "Convolutional Neural Networks",
        "Multi-scale Feature Fusion",
        "Scene Classification"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:43.201372Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18261",
    "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning",
    "authors": [
      "Chao-Hong Tan",
      "Qian Chen",
      "Wen Wang",
      "Yukun Ma",
      "Chong Zhang",
      "Chong Deng",
      "Qinglin Zhang",
      "Xiangang Li",
      "Jieping Ye"
    ],
    "abstract": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18261.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18261",
    "published": "2026-01-26T08:35:34Z",
    "updated": "2026-01-26T08:35:34Z",
    "comment": "Accepted by ICASSP 2026",
    "light_analysis": {
      "overview": "提出Fisher-Guided Gradient Masking (FGGM)框架，通过Fisher信息指导梯度掩码来缓解持续学习中的灾难性遗忘。",
      "motivation": "灾难性遗忘是大型语言模型在持续学习中的一个关键挑战，它阻碍模型在学习新任务时保持旧任务能力。这一问题的重要性在于限制了模型在实际应用中的适应性和长期性能，现有方法如MIGU基于参数幅度选择，缺乏数学原理的参数重要性估计，导致无法有效平衡稳定性和可塑性，从而影响持续学习效果。",
      "method": "论文提出Fisher-Guided Gradient Masking (FGGM)框架，核心是利用对角线Fisher信息来估计参数重要性，战略性地选择需要更新的参数。该方法动态生成二进制掩码，通过自适应阈值保留关键网络权重，实现在无需历史数据的情况下平衡模型稳定性与学习新任务的可塑性。关键创新在于提供数学原理的参数重要性估计，优于传统幅度基方法。",
      "result": "在TRACE基准测试中，FGGM在保持一般能力方面相对于监督微调(SFT)有9.6%的相对提升，在TRACE任务上比MIGU提升4.4%。在代码生成任务上的额外分析进一步确认了FGGM的卓越性能，有效减少了遗忘，证明了其在持续学习任务中的有效性和鲁棒性。",
      "conclusion": "论文的主要贡献是提出了FGGM框架，为持续学习中的灾难性遗忘问题提供了数学原理的新方法。学术价值在于提升了参数重要性估计的理论基础，增强了模型稳定性和可塑性的平衡；实际应用价值在于帮助大型语言模型更好地适应新任务而不丢失旧知识。摘要未明确说明潜在局限性或未来工作方向。",
      "tags": [
        "Fisher Information",
        "Gradient Masking",
        "Continual Learning",
        "Parameter Importance",
        "Binary Masks"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:12.201303Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18260",
    "title": "Depth to Anatomy: Learning Internal Organ Locations from Surface Depth Images",
    "authors": [
      "Eytan Kats",
      "Kai Geissler",
      "Daniel Mensing",
      "Jochen G. Hirsch",
      "Stefan Heldman",
      "Mattias P. Heinrich"
    ],
    "abstract": "Automated patient positioning plays an important role in optimizing scanning procedure and improving patient throughput. Leveraging depth information captured by RGB-D cameras presents a promising approach for estimating internal organ positions, thereby enabling more accurate and efficient positioning. In this work, we propose a learning-based framework that directly predicts the 3D locations and shapes of multiple internal organs from single 2D depth images of the body surface. Utilizing a large-scale dataset of full-body MRI scans, we synthesize depth images paired with corresponding anatomical segmentations to train a unified convolutional neural network architecture. Our method accurately localizes a diverse set of anatomical structures, including bones and soft tissues, without requiring explicit surface reconstruction. Experimental results demonstrate the potential of integrating depth sensors into radiology workflows to streamline scanning procedures and enhance patient experience through automated patient positioning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18260.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18260",
    "published": "2026-01-26T08:33:11Z",
    "updated": "2026-01-26T08:33:11Z",
    "comment": "preprint",
    "light_analysis": {
      "overview": "本论文提出一个学习框架，直接从单个表面深度图像预测内部器官的3D位置和形状，无需显式表面重建。",
      "motivation": "该研究的动机在于自动化患者定位在优化医学扫描流程、提高患者吞吐量方面的重要性。在放射学工作流中，传统定位方法依赖技术员经验，导致精度不足、效率低下，而深度信息提供非侵入性估计器官位置的新途径。现有方法可能需复杂重建或额外步骤，限制了实际应用。因此，开发直接从深度图像预测器官位置的框架，旨在解决这些不足，提升定位的准确性和速度，优化临床操作。",
      "method": "研究方法采用基于学习的框架，核心是统一的卷积神经网络架构。利用大规模全身MRI扫描数据集，合成深度图像与对应的解剖分割图像配对，用于训练模型。关键创新点在于直接从单个2D深度图像预测多个内部器官的3D位置和形状，避免了显式表面重建步骤，简化了从表面信息到内部解剖结构的映射。通过深度学习的特征提取能力，该方法实现了高效的数据驱动建模。",
      "result": "实验结果表明，该方法能够准确定位多种解剖结构，包括骨骼和软组织。尽管摘要未提供具体的性能指标如准确率或基线对比数据，但实验展示了该框架在预测器官位置方面的潜力，验证了集成深度传感器到放射学工作流程的可行性。这暗示了方法可能提升效率或精度，为自动化患者定位提供基础。摘要未明确说明具体数值，需进一步实验验证。",
      "conclusion": "本研究的贡献是提出了一个学习框架，直接从深度图像预测内部器官位置，为自动化患者定位提供新方案。学术价值在于结合深度学习和医学图像分析，扩展了非侵入性解剖建模方法。应用价值是简化放射学工作流程，提高患者吞吐量和体验，减少操作依赖。局限性可能包括数据依赖性或泛化能力，未来工作可探索更多解剖结构或临床实际应用验证。摘要未明确说明具体局限性或未来方向。",
      "tags": [
        "Depth Sensing",
        "Convolutional Neural Networks",
        "Medical Imaging",
        "Organ Localization",
        "Anatomical Segmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:25.677379Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18255",
    "title": "Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs",
    "authors": [
      "Fei Meng"
    ],
    "abstract": "Continual learning in Large Language Models (LLMs) faces the critical challenge of balancing stability (retaining old knowledge) and plasticity (learning new tasks). While Experience Replay (ER) is a standard countermeasure against catastrophic forgetting, its impact across diverse capabilities remains underexplored. In this work, we uncover a critical dichotomy in ER's behavior: while it induces positive backward transfer on robust, unstructured tasks (e.g., boosting performance on previous NLP classification tasks through repeated rehearsal), it causes severe negative transfer on fragile, structured domains like code generation (e.g., a significant relative drop in coding accuracy). This reveals that ER trades structural integrity for broad consolidation. To address this dilemma, we propose \\textbf{Orthogonal Subspace Wake-up (OSW)}. OSW identifies essential parameter subspaces of previous tasks via a brief \"wake-up\" phase and enforces orthogonal updates for new tasks, providing a mathematically grounded \"safety guarantee\" for established knowledge structures. Empirical results across a diverse four-task sequence demonstrate that OSW uniquely succeeds in preserving fragile coding abilities where Replay fails, while simultaneously maintaining high plasticity for novel tasks. Our findings emphasize the necessity of evaluating structural safety alongside average retention in LLM continual learning.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18255.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18255",
    "published": "2026-01-26T08:28:02Z",
    "updated": "2026-01-26T08:28:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "OSW方法通过在连续学习中引入正交子空间更新，解决了经验回放对脆弱任务的负向迁移问题，有效平衡结构安全性和可塑性。",
      "motivation": "连续学习在大语言模型（LLMs）中面临平衡稳定性（保留旧知识）和可塑性（学习新任务）的关键挑战。经验回放（ER）作为标准方法，虽能缓解灾难性遗忘，但其对不同任务能力的影响研究不足，导致在脆弱、结构化领域（如代码生成）引发负向迁移，这突显了评估结构安全性的重要性。",
      "method": "论文提出Orthogonal Subspace Wake-up (OSW)方法，包括一个“唤醒”阶段用于识别先前任务的关键参数子空间，并通过正交更新为新任务学习提供数学保障，确保不干扰旧知识。该方法强调了结构安全性的技术特色，但摘要未明确说明使用的具体数据集和模型架构。",
      "result": "实证结果在一个包含四个任务的连续学习序列中显示，OSW成功保持了脆弱的代码生成能力，而经验回放（ER）导致该能力显著相对下降。OSW同时维持了新任务的高可塑性，实现了结构安全性和学习能力的平衡，但摘要未提供具体性能指标数据。",
      "conclusion": "论文的主要贡献是提出OSW方法，以解决ER在处理脆弱任务时的局限性，强调在LLM连续学习中评估结构安全性的必要性。其学术价值在于提供了数学保障的新途径，实际应用有助于提升LLM在代码生成等结构化任务的性能，未来工作可能包括扩展OSW到更多任务类型。",
      "tags": [
        "Continual Learning",
        "Large Language Models",
        "Experience Replay",
        "Orthogonal Subspace",
        "Code Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:48.757047Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18253",
    "title": "BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation",
    "authors": [
      "Peng Sun",
      "Xiangyu Zhang",
      "Duan Wu"
    ],
    "abstract": "Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18253.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18253",
    "published": "2026-01-26T08:20:02Z",
    "updated": "2026-01-26T08:20:02Z",
    "comment": "This is a pre-print",
    "light_analysis": {
      "overview": "BoRP框架通过结合bootstrapping和PLS技术，实现了一种高效且与人对齐的LLM评估方法，解决了对话AI用户满意度评估中的传统局限。",
      "motivation": "本研究旨在解决对话AI用户满意度评估的挑战，传统A/B测试因显式反馈稀少和隐式指标模糊而不可靠，这阻碍了系统的迭代开发。准确评估对AI助手的改进至关重要，但现有方法缺乏高保真度和可扩展性，导致难以进行有效的性能监控和优化，急需新的评估框架来弥补这一不足。",
      "method": "BoRP方法基于大型语言模型（LLM）潜在空间的几何属性，采用极化指数bootstrapping机制自动化生成评估标准，并使用偏最小二乘法（PLS）将隐藏状态映射到连续满意度分数。该方法的关键创新在于避免生成过程，转而利用回归探测和几何分析，提高了评估的效率和准确性，适用于大规模场景。",
      "result": "在工业数据集上的实验显示，BoRP（使用Qwen3-8B/14B模型）在与人判断对齐方面显著优于生成基线（包括Qwen3-Max），实现了更高的评估准确性。此外，BoRP大幅降低了推理成本，减少多个数量级，从而支持全面监控和高度敏感的A/B测试，通过CUPED等技术增强了实际应用能力。",
      "conclusion": "BoRP为LLM评估提供了一种可扩展和高保真的框架，通过整合bootstrapping和PLS技术，改进了用户满意度评估方法。其学术价值在于推动了基于几何和回归的评估策略，实际应用价值在于促进对话AI的迭代开发和优化。局限性方面，摘要未明确说明，但未来工作可能涉及扩展到更多模型和多样化场景。",
      "tags": [
        "LLM Evaluation",
        "Bootstrapping",
        "Regression Probing",
        "Partial Least Squares (PLS)",
        "Latent Space Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:50.404099Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18252",
    "title": "Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing",
    "authors": [
      "Chao Wang",
      "Xuanying Li",
      "Cheng Dai",
      "Jinglei Feng",
      "Yuxiang Luo",
      "Yuqi Ouyang",
      "Hao Qin"
    ],
    "abstract": "Wireframe parsing aims to recover line segments and their junctions to form a structured geometric representation useful for downstream tasks such as Simultaneous Localization and Mapping (SLAM). Existing methods predict lines and junctions separately and reconcile them post-hoc, causing mismatches and reduced robustness. We present Co-PLNet, a point-line collaborative framework that exchanges spatial cues between the two tasks, where early detections are converted into spatial prompts via a Point-Line Prompt Encoder (PLP-Encoder), which encodes geometric attributes into compact and spatially aligned maps. A Cross-Guidance Line Decoder (CGL-Decoder) then refines predictions with sparse attention conditioned on complementary prompts, enforcing point-line consistency and efficiency. Experiments on Wireframe and YorkUrban show consistent improvements in accuracy and robustness, together with favorable real-time efficiency, demonstrating our effectiveness for structured geometry perception.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18252.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18252",
    "published": "2026-01-26T08:16:02Z",
    "updated": "2026-01-26T08:16:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Co-PLNet，一个点线协作框架，通过空间提示和交叉引导来提高线框解析的准确性和效率。",
      "motivation": "线框解析旨在恢复线段和交点以构建结构化几何表示，这对下游任务如SLAM至关重要。现有方法通常独立预测线和交点后再进行调和，导致不匹配问题并降低了系统的鲁棒性，因此需要一种更统一的方法来协同处理点线关系，以增强解析的准确性和鲁棒性。",
      "method": "Co-PLNet采用点线协作框架，通过Point-Line Prompt Encoder（PLP-Encoder）将早期检测转换为空间提示，编码几何属性为紧凑、空间对齐的映射。然后，Cross-Guidance Line Decoder（CGL-Decoder）利用互补提示通过稀疏注意力机制精炼预测，强制执行点线一致性和效率，该方法在Wireframe和YorkUrban数据集上进行验证。",
      "result": "在Wireframe和YorkUrban数据集上的实验表明，Co-PLNet在准确性和鲁棒性方面均优于现有基线方法，并展现出良好的实时效率，摘要未明确说明具体性能指标，但结果一致显示改进。",
      "conclusion": "Co-PLNet框架有效解决了线框解析中的点线不匹配问题，通过协作机制提高了结构化几何感知的性能，具有实际应用价值，尤其是在计算机视觉和机器人导航领域，未来工作可能涉及进一步优化模型效率或扩展到更复杂的场景。",
      "tags": [
        "Wireframe Parsing",
        "Collaborative Framework",
        "Spatial Prompts",
        "Sparse Attention",
        "Point-Line Interaction"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:33.430511Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18250",
    "title": "A multimodal vision foundation model for generalizable knee pathology",
    "authors": [
      "Kang Yu",
      "Dingyu Wang",
      "Zimu Yuan",
      "Nan Zhou",
      "Jiajun Liu",
      "Jiaxin Liu",
      "Shanggui Liu",
      "Yaoyan Zheng",
      "Huishu Yuan",
      "Di Huang",
      "Dong Jiang"
    ],
    "abstract": "Musculoskeletal disorders represent a leading cause of global disability, creating an urgent demand for precise interpretation of medical imaging. Current artificial intelligence (AI) approaches in orthopedics predominantly rely on task-specific, supervised learning paradigms. These methods are inherently fragmented, require extensive annotated datasets, and often lack generalizability across different modalities and clinical scenarios. The development of foundation models in this field has been constrained by the scarcity of large-scale, curated, and open-source musculoskeletal datasets. To address these challenges, we introduce OrthoFoundation, a multimodal vision foundation model optimized for musculoskeletal pathology. We constructed a pre-training dataset of 1.2 million unlabeled knee X-ray and MRI images from internal and public databases. Utilizing a Dinov3 backbone, the model was trained via self-supervised contrastive learning to capture robust radiological representations. OrthoFoundation achieves state-of-the-art (SOTA) performance across 14 downstream tasks. It attained superior accuracy in X-ray osteoarthritis diagnosis and ranked first in MRI structural injury detection. The model demonstrated remarkable label efficiency, matching supervised baselines using only 50% of labeled data. Furthermore, despite being pre-trained on knee images, OrthoFoundation exhibited exceptional cross-anatomy generalization to the hip, shoulder, and ankle. OrthoFoundation represents a significant advancement toward general-purpose AI for musculoskeletal imaging. By learning fundamental, joint-agnostic radiological semantics from large-scale multimodal data, it overcomes the limitations of conventional models, which provides a robust framework for reducing annotation burdens and enhancing diagnostic accuracy in clinical practice.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18250.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18250",
    "published": "2026-01-26T08:14:51Z",
    "updated": "2026-01-26T08:14:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了OrthoFoundation，一个针对肌肉骨骼病理学的多模态视觉基础模型，通过自监督对比学习实现卓越的跨模态泛化能力和诊断效率提升。",
      "motivation": "肌肉骨骼疾病是全球致残的主要原因，需精准医学影像解读。当前AI方法依赖任务特定监督学习，导致模型碎片化、需大量标注数据且缺乏跨模态和临床场景通用性。开发基础模型受限于大规模开源数据稀缺。本研究旨在构建通用多模态视觉基础模型，解决标注负担重和泛化能力不足问题，以推动临床AI应用。",
      "method": "研究开发了OrthoFoundation模型，基于Dinov3架构，采用自监督对比学习方法进行预训练。预训练数据集包括120万无标签膝盖X光和MRI图像，来自内部和公开数据库。关键创新在于使用多模态数据学习关节无关的放射学表示，捕获稳健语义特征，为下游任务提供可泛化的基础模型框架。",
      "result": "OrthoFoundation在14个下游任务中达到最先进性能，具体表现为X光骨关节炎诊断准确率优异，MRI结构损伤检测排名第一。模型标签效率显著，仅用50%标注数据即可匹配监督基线。此外，模型展现出出色跨解剖部位泛化能力，如对髋、肩、踝部应用效果良好。",
      "conclusion": "该研究证明OrthoFoundation通过大规模多模态数据学习基础语义，有效克服传统模型碎片化和标注依赖问题。它为肌肉骨骼成像AI提供通用框架，有望减少临床标注负担并提升诊断准确性，推动相关领域进展。未来可扩展至更多解剖部位或疾病类型以增强适用性。",
      "tags": [
        "Multimodal Vision Foundation Model",
        "Self-Supervised Learning",
        "Contrastive Learning",
        "Medical Imaging",
        "Dinov3"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:39.757354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18245",
    "title": "Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity",
    "authors": [
      "Santanu Das",
      "Jatin Batra"
    ],
    "abstract": "Phase retrieval is the classical problem of recovering a signal $x^* \\in \\mathbb{R}^n$ from its noisy phaseless measurements $y_i = \\langle a_i, x^* \\rangle^2 + ζ_i$ (where $ζ_i$ denotes noise, and $a_i$ is the sensing vector) for $i \\in [m]$. The problem of phase retrieval has a rich history, with a variety of applications such as optics, crystallography, heteroscedastic regression, astrophysics, etc. A major consideration in algorithms for phase retrieval is robustness against measurement errors. In recent breakthroughs in algorithmic robust statistics, efficient algorithms have been developed for several parameter estimation tasks such as mean estimation, covariance estimation, robust principal component analysis (PCA), etc. in the presence of heavy-tailed noise and adversarial corruptions. In this paper, we study efficient algorithms for robust phase retrieval with heavy-tailed noise when a constant fraction of both the measurements $y_i$ and the sensing vectors $a_i$ may be arbitrarily adversarially corrupted. For this problem, Buna and Rebeschini (AISTATS 2025) very recently gave an exponential time algorithm with sample complexity $O(n \\log n)$. Their algorithm needs a robust spectral initialization, specifically, a robust estimate of the top eigenvector of a covariance matrix, which they deemed to be beyond known efficient algorithmic techniques (similar spectral initializations are a key ingredient of a large family of phase retrieval algorithms). In this work, we make a connection between robust spectral initialization and recent algorithmic advances in robust PCA, yielding the first polynomial-time algorithms for robust phase retrieval with both heavy-tailed noise and adversarial corruptions, in fact with near-linear (in $n$) sample complexity.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18245.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18245",
    "published": "2026-01-26T08:06:16Z",
    "updated": "2026-01-26T08:06:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文首次提出了处理重尾噪声和对抗性腐败相位恢复的多项式时间算法，样本复杂度接近线性。",
      "motivation": "相位恢复是从无相位测量中恢复信号的经典问题，广泛应用于光学、晶体学和天体物理学等领域。现有算法在面对重尾噪声和对抗性腐败时效率低下，如Buna和Rebeschini的算法需要指数时间，样本复杂度为O(n log n)，且依赖稳健光谱初始化，这被传统方法视为计算上不可行的挑战。因此，开发高效鲁棒的算法对提升实际应用的可靠性和性能至关重要，特别是在噪声和腐败可能同时影响测量向量和信号的情况下。",
      "method": "本文提出一种新方法，通过将稳健光谱初始化与稳健主成分分析（PCA）的最新算法进展相结合，实现多项式时间的相位恢复。核心创新在于利用稳健PCA的高效技术来估计协方差矩阵的顶部特征向量，从而避免了传统方法中的指数时间计算。具体技术路线包括处理重尾噪声和对抗性腐败，其中测量向量和信号可能被任意腐败，通过算法连接稳健光谱初始化和稳健PCA，使得初始化步骤变得可处理，进而支持整个恢复过程的高效执行。",
      "result": "摘要未明确说明具体的实验数据，但论文报告了算法在理论上的重大改进：首次实现多项式时间算法，样本复杂度接近线性（in n）。相较于Buna和Rebeschini的指数时间算法和O(n log n)样本复杂度，本方法在计算效率和样本需求上均有显著提升，为鲁棒相位恢复提供了更实用的解决方案，尽管具体性能指标如准确率或误差率未在摘要中详细说明。",
      "conclusion": "本研究的主要贡献是开发了首个处理重尾噪声和对抗性腐败相位恢复的多项式时间算法，样本复杂度接近线性。通过连接稳健光谱初始化与稳健PCA，解决了算法效率瓶颈，具有重要的理论意义，推动了鲁棒统计算法的发展，并为光学等领域的实际应用提供了新工具。未来工作可能包括进一步优化算法性能、扩展到更复杂的噪声模型或探索实际数据集上的验证，以提升其泛化能力和实用性。",
      "tags": [
        "Phase Retrieval",
        "Heavy-Tailed Noise",
        "Adversarial Corruption",
        "Robust PCA",
        "Spectral Initialization"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:07.691810Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18242",
    "title": "Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation",
    "authors": [
      "Zerui Kang",
      "Yishen Lim",
      "Zhouyou Gu",
      "Seung-Woo Ko",
      "Tony Q. S. Quek",
      "Jihong Park"
    ],
    "abstract": "Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\\times$ faster convergence and 10-100$\\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.",
    "categories": [
      "cs.CV",
      "cs.NI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18242.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18242",
    "published": "2026-01-26T07:54:53Z",
    "updated": "2026-01-26T07:54:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种视觉语言模型引导的可微射线追踪框架，用于加速和稳定多材料射频参数估计。",
      "motivation": "本研究旨在解决6G系统中电磁数字孪生对准确射频材料参数的需求。现有基于梯度的逆射线追踪方法在有限测量下对初始化敏感且计算成本高，导致参数估计效率低下、结果不稳定，限制了实际应用。这一问题的重要性在于精确的材料参数对于优化6G系统性能至关重要，而传统方法难以在成本和精度间取得平衡，因此需要开发更高效、稳定的估计方法以支持高级无线通信技术发展。",
      "method": "论文提出了一个视觉语言模型（VLM）引导的框架，结合可微射线追踪（DRT）引擎进行多材料射频参数估计。VLM解析场景图像以推断材料类别，并利用ITU-R材料表映射到量化先验（如电导率），为优化提供智能初始化。此外，VLM选择信息丰富的发射器和接收器位置，以增强路径多样性并促进材料区分。然后，DRT基于测量的接收信号强度执行梯度下降精炼过程。关键创新在于整合VLM的语义先验来指导物理优化，该方法在NVIDIA Sionna平台上实现，提高了估计的速度和稳定性。",
      "result": "在NVIDIA Sionna平台的室内场景实验中，与均匀或随机初始化及随机放置基线相比，所提方法实现了2-4倍更快的收敛速度和10-100倍更低的最终参数误差。仅使用少量接收器即可达到低于0.1%的平均相对误差。复杂度分析表明，每次迭代时间随材料数量和测量设置近线性增长，而VLM引导的放置策略减少了准确恢复所需的测量次数。消融研究进一步证实，增加射线追踪深度和射线数量可带来额外准确性增益，而无显著迭代开销。这些结果验证了方法的效率和高精度性能。",
      "conclusion": "本研究的主要贡献是证明了视觉语言模型提供的语义先验能有效指导基于物理的优化，实现快速可靠的射频材料参数估计。这为结合计算机视觉、自然语言处理和电磁模拟开辟了新途径，显著提高了参数估计的效率和稳定性，具有重要的学术和应用价值，尤其在6G系统电磁数字孪生中潜力巨大。摘要未明确说明具体局限性或未来工作方向，但可推测未来可能扩展至更多场景类型和优化VLM的先验知识库。",
      "tags": [
        "Vision-Language Model",
        "Differentiable Ray Tracing",
        "Gradient-Based Optimization",
        "RF Parameter Estimation",
        "Multi-Material Estimation"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:38.140248Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18240",
    "title": "V-Loop: Visual Logical Loop Verification for Hallucination Detection in Medical Visual Question Answering",
    "authors": [
      "Mengyuan Jin",
      "Zehui Liao",
      "Yong Xia"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable capability in assisting disease diagnosis in medical visual question answering (VQA). However, their outputs remain vulnerable to hallucinations (i.e., responses that contradict visual facts), posing significant risks in high-stakes medical scenarios. Recent introspective detection methods, particularly uncertainty-based approaches, offer computational efficiency but are fundamentally indirect, as they estimate predictive uncertainty for an image-question pair rather than verifying the factual correctness of a specific answer. To address this limitation, we propose Visual Logical Loop Verification (V-Loop), a training-free and plug-and-play framework for hallucination detection in medical VQA. V-Loop introduces a bidirectional reasoning process that forms a visually grounded logical loop to verify factual correctness. Given an input, the MLLM produces an answer for the primary input pair. V-Loop extracts semantic units from the primary QA pair, generates a verification question by conditioning on the answer unit to re-query the question unit, and enforces visual attention consistency to ensure answering both primary question and verification question rely on the same image evidence. If the verification answer matches the expected semantic content, the logical loop closes, indicating factual grounding; otherwise, the primary answer is flagged as hallucinated. Extensive experiments on multiple medical VQA benchmarks and MLLMs show that V-Loop consistently outperforms existing introspective methods, remains highly efficient, and further boosts uncertainty-based approaches when used in combination.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18240.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18240",
    "published": "2026-01-26T07:46:41Z",
    "updated": "2026-01-26T07:46:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了V-Loop框架，一种无需训练、即插即用的方法，通过视觉逻辑循环验证来检测医疗视觉问答中的幻觉。",
      "motivation": "多模态大型语言模型在医疗视觉问答中展现出强大能力，但其输出容易产生幻觉，即与视觉事实矛盾的响应，这在高风险医疗场景中带来显著风险，如误诊。现有内省检测方法，如基于不确定性的方法，虽然计算高效，但根本上是间接的：它们通过估计图像-问题对的预测不确定性来间接评估幻觉，而不是直接验证具体答案的事实正确性。因此，需要一种更直接的方法来准确检测幻觉，以确保医疗AI系统的可靠性和安全性，减少潜在错误。",
      "method": "V-Loop是一种训练免费、即插即用的框架，其核心方法是引入双向推理过程，形成一个基于视觉的逻辑循环来验证事实正确性。具体来说，给定输入图像和问题，MLLM先生成答案；然后，V-Loop从主要问题-答案对中提取语义单元，基于答案单元生成验证问题来重新查询问题单元，同时强制执行视觉注意力一致性，确保回答主要问题和验证问题时都依赖于相同的图像证据。如果验证答案与预期语义内容匹配，逻辑循环闭合，表示事实基础牢固；否则，主要答案被标记为幻觉。该方法无需额外训练，可直接应用于多种MLLMs，关键创新在于结合了语义提取、问题生成和注意力机制以实现直接验证。",
      "result": "论文在多个医疗视觉问答基准和多模态大型语言模型上进行了广泛实验，结果表明，V-Loop在幻觉检测性能上持续优于现有的内省方法，如基于不确定性的方法。此外，V-Loop保持了高度计算效率，能够在实际应用中快速部署；当与基于不确定性的方法结合使用时，它能进一步提升检测性能，表现出协同效应。具体性能指标如准确率提升等数据未在摘要中明确说明，但实验对比强调了其在多个场景下的优越性。",
      "conclusion": "本研究的核心贡献是提出了V-Loop框架，提供了一种更直接和有效的幻觉检测方法，显著改善了医疗视觉问答中的答案可靠性。它在学术上推动了多模态AI的验证技术发展，具有创新性；在实际应用中，可增强医疗AI系统的鲁棒性，降低高风险场景下的风险，提升诊断辅助工具的可信度。未来工作可能包括扩展框架到其他多模态任务或优化验证过程，以应对更复杂的情况，尽管摘要未明确说明具体局限性。",
      "tags": [
        "Multimodal Large Language Models",
        "Visual Question Answering",
        "Hallucination Detection",
        "Logical Loop Verification",
        "Visual Attention Consistency"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:26.236377Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18238",
    "title": "TechING: Towards Real World Technical Image Understanding via VLMs",
    "authors": [
      "Tafazzul Nadeem",
      "Bhavik Shangari",
      "Manish Rai",
      "Gagan Raj Gupta",
      "Ashutosh Modi"
    ],
    "abstract": "Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.",
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18238.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18238",
    "published": "2026-01-26T07:43:55Z",
    "updated": "2026-01-26T07:43:55Z",
    "comment": "Accepted at Findings of EACL 2026, 30 Pages (9 Pages main paper + 4 pages references + 17 pages appendix)",
    "light_analysis": {
      "overview": "本研究通过合成数据集和自监督任务微调视觉语言模型，显著提升了技术图表理解的性能。",
      "motivation": "该研究旨在解决视觉语言模型在处理真实世界手绘技术图表时的性能瓶颈。专业人员常在讨论中手绘技术图表（如流程图、框图），但编辑时需要重新绘制，效率低下。现有VLMs在通用图像理解上虽有进步，但对技术图表理解不足，而微调方法需要大量手绘图像，这在现实中难以收集，因此需要开发新方法来克服数据稀缺问题，提高模型的实际应用能力。",
      "method": "研究方法的核心是生成一个大型合成语料库，该语料库模拟真实世界手绘技术图表，以解决数据不足问题。论文引入了多个新的自监督学习任务来训练视觉语言模型，关键创新点在于结合合成数据和自监督学习。具体细节包括使用Llama 3.2 11B-instruct模型，在合成图像上微调，形成LLama-VL-TUG模型，并在小型手绘图像数据集上评估，借助人类协助验证模型性能。",
      "result": "实验结果显示显著性能提升：在合成数据上，LLama-VL-TUG将Llama 3.2 11B-instruct的ROUGE-L性能提高了2.14倍，在所有基线模型中取得了最佳整体表现。在真实世界手绘图像的人类评估中，模型在8种图表类型中的7种实现了最小编译错误，并将平均F1分数提升了6.97倍，这表明了方法在减少错误和提高准确性方面的有效性。",
      "conclusion": "摘要未明确说明具体结论，但基于研究内容，该研究的主要贡献是提出了使用合成数据集和自监督任务来微调视觉语言模型，以增强技术图表理解。学术价值在于为解决数据稀缺问题提供了创新方案，实际应用价值体现在支持专业工具的开发和效率提升。未来工作方向可能包括扩展到更多图表类型或减少对合成数据的依赖，以进一步提高泛化能力。",
      "tags": [
        "Visual Language Models (VLMs)",
        "Self-Supervised Learning",
        "Synthetic Data Generation",
        "Fine-tuning",
        "Llama Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:51.050599Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18231",
    "title": "Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting",
    "authors": [
      "Trong Khiem Tran",
      "Manh Cuong Dao",
      "Phi Le Nguyen",
      "Thao Nguyen Truong",
      "Trong Nghia Hoang"
    ],
    "abstract": "Adapting pre-trained models to unseen feature modalities has become increasingly important due to the growing need for cross-disciplinary knowledge integration.~A key challenge here is how to align the representation of new modalities with the most relevant parts of the pre-trained model's representation space to enable accurate knowledge transfer.~This requires combining feature alignment with target fine-tuning, but uncalibrated combinations can exacerbate misalignment between the source and target feature-label structures and reduce target generalization.~Existing work however lacks a theoretical understanding of this critical interaction between feature alignment and target fitting.~To bridge this gap, we develop a principled framework that establishes a provable generalization bound on the target error, which explains the interaction between feature alignment and target fitting through a novel concept of feature-label distortion.~This bound offers actionable insights into how this interaction should be optimized for practical algorithm design. The resulting approach achieves significantly improved performance over state-of-the-art methods across a wide range of benchmark datasets.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18231.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18231",
    "published": "2026-01-26T07:34:15Z",
    "updated": "2026-01-26T07:34:15Z",
    "comment": "Accepted AISTATS 20226. Preprint version",
    "light_analysis": {
      "overview": "论文提出了一个理论框架，通过优化特征对齐与目标拟合的相互作用，提升了跨模态微调的性能。",
      "motivation": "随着跨学科知识集成需求的增长，将预训练模型适应到未见特征模态变得日益重要，以促进知识转移。关键挑战在于对齐新模态表示与预训练模型表示空间的最相关部分，但现有方法缺乏对特征对齐和目标拟合相互作用的理论理解。未校准的组合可能加剧源与目标特征-标签结构的不对齐，降低目标泛化能力，因此解决这一问题对提高模型适应性至关重要。",
      "method": "作者开发了一个原则性框架，建立了目标误差的可证明泛化界限，通过新颖的特征-标签扭曲概念来解释特征对齐和目标拟合的相互作用。该框架提供了优化交互的见解，用于指导实际算法设计，关键创新点在于理论分析和可证明界限，而非具体实现细节。摘要未明确说明使用的具体数据集、模型架构或其他技术路线细节，但强调了理论框架的构建。",
      "result": "论文方法在广泛的基准数据集上实现了显著改进的性能，优于当前最先进的方法。具体性能指标如准确率提升未在摘要中提供，但结果表明通过优化特征对齐和目标拟合的相互作用，该方法有效提高了跨模态微调的效果，并与基线方法进行了对比，显示出显著的优势。",
      "conclusion": "该研究的主要贡献是提出了一个理论框架，解释了特征对齐和目标拟合的相互作用，并建立了可证明的泛化界限，填补了现有研究的理论空白。学术价值在于深化了表示学习的理论理解，实际应用价值在于为跨模态微调的算法设计提供可操作的见解。摘要未明确说明研究的局限性或未来工作方向，但潜在方向可能包括扩展到更多模态或验证实际应用。",
      "tags": [
        "Cross-Modal Fine-Tuning",
        "Feature Alignment",
        "Generalization Bound",
        "Representation Learning",
        "Target Fitting"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:32.747217Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18228",
    "title": "Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach",
    "authors": [
      "Sahil Naik",
      "Soham Bagayatkar",
      "Pavankumar Singh"
    ],
    "abstract": "Detection of human emotions based on facial images in real-world scenarios is a difficult task due to low image quality, variations in lighting, pose changes, background distractions, small inter-class variations, noisy crowd-sourced labels, and severe class imbalance, as observed in the FER-2013 dataset of 48x48 grayscale images. Although recent approaches using large CNNs such as VGG and ResNet achieve reasonable accuracy, they are computationally expensive and memory-intensive, limiting their practicality for real-time applications. We address these challenges using a lightweight and efficient facial emotion recognition pipeline based on EfficientNetB2, trained using a two-stage warm-up and fine-tuning strategy. The model is enhanced with AdamW optimization, decoupled weight decay, label smoothing (epsilon = 0.06) to reduce annotation noise, and clipped class weights to mitigate class imbalance, along with dropout, mixed-precision training, and extensive real-time data augmentation. The model is trained using a stratified 87.5%/12.5% train-validation split while keeping the official test set intact, achieving a test accuracy of 68.78% with nearly ten times fewer parameters than VGG16-based baselines. Experimental results, including per-class metrics and learning dynamics, demonstrate stable training and strong generalization, making the proposed approach suitable for real-time and edge-based applications.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18228.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18228",
    "published": "2026-01-26T07:29:50Z",
    "updated": "2026-01-26T07:29:50Z",
    "comment": "6 pages, 4 figures",
    "light_analysis": {
      "overview": "本文提出了一种基于EfficientNetB2的轻量级面部情绪识别方法，通过优化训练策略和缓解类别不平衡，在FER-2013数据集上实现了高效准确的识别。",
      "motivation": "面部情绪识别在现实场景中面临诸多挑战，如FER-2013数据集（48x48灰度图像）中的低质量图像、光照和姿态变化、背景干扰、类别间差异小、噪声标签及严重类别不平衡。现有方法如使用VGG和ResNet等大型CNN虽能获得合理准确率，但计算成本高、内存占用大，限制了实时应用的可能性，尤其是在边缘设备部署中。这突出了开发更高效、轻量级解决方案的必要性，以平衡准确性与实用性。",
      "method": "本研究采用基于EfficientNetB2的轻量级管道进行面部情绪识别，通过两阶段预热和微调策略优化训练。关键创新包括使用AdamW优化器结合解耦权重衰减、标签平滑（epsilon=0.06）以减少标签噪声，以及裁剪类别权重来缓解类别不平衡问题。此外，引入了dropout、混合精度训练和广泛的实时数据增强技术以增强模型泛化能力。模型在FER-2013数据集上使用分层87.5%/12.5%的训练验证分割进行训练，保持官方测试集独立。",
      "result": "实验结果表明，在官方测试集上实现了68.78%的测试准确率，同时参数数量比基于VGG16的基线减少近十倍，显著提高了计算效率。每个类别的指标和学习动态显示训练过程稳定且具有强泛化能力，验证了该方法在保持高性能的同时更适合实时和边缘应用场景。",
      "conclusion": "该研究的主要贡献是开发了一种高效的面部情绪识别方法，结合轻量级模型和多种优化技术，有效应对了FER-2013数据集的挑战。其学术价值在于展示了在资源受限环境下实现高精度识别的可行性，实际应用价值在于为实时和边缘设备部署提供了实用解决方案。未来工作可探索扩展到其他数据集或进一步优化模型性能。",
      "tags": [
        "EfficientNetB2",
        "AdamW Optimization",
        "Label Smoothing",
        "Class Imbalance Mitigation",
        "Real-time Data Augmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:50.842258Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18226",
    "title": "Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks",
    "authors": [
      "Haotian Li",
      "Shijun Yang",
      "Weizhen Qi",
      "Silei Zhao",
      "Rui Hua",
      "Mingzhu Song",
      "Xiaojian Yang",
      "Chao Peng"
    ],
    "abstract": "Conventional agent systems often struggle in open-ended environments where task distributions continuously drift and external supervision is scarce. Their reliance on static toolsets or offline training lags behind these dynamics, leaving the system's capability boundaries rigid and unknown. To address this, we propose the In-Situ Self-Evolving paradigm. This approach treats sequential task interactions as a continuous stream of experience, enabling the system to distill short-term execution feedback into long-term, reusable capabilities without access to ground-truth labels. Within this framework, we identify tool evolution as the critical pathway for capability expansion, which provides verifiable, binary feedback signals. Within this framework, we develop Yunjue Agent, a system that iteratively synthesizes, optimizes, and reuses tools to navigate emerging challenges. To optimize evolutionary efficiency, we further introduce a Parallel Batch Evolution strategy. Empirical evaluations across five diverse benchmarks under a zero-start setting demonstrate significant performance gains over proprietary baselines. Additionally, complementary warm-start evaluations confirm that the accumulated general knowledge can be seamlessly transferred to novel domains. Finally, we propose a novel metric to monitor evolution convergence, serving as a function analogous to training loss in conventional optimization. We open-source our codebase, system traces, and evolved tools to facilitate future research in resilient, self-evolving intelligence.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18226.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18226",
    "published": "2026-01-26T07:27:47Z",
    "updated": "2026-01-26T07:27:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一个基于现场自进化范式的代理系统，通过迭代工具进化实现零起点自适应，以应对开放环境中的任务分布漂移。",
      "motivation": "传统代理系统在开放环境中面临任务分布持续漂移和外部监督稀缺的挑战，导致系统能力边界僵硬且未知。现有方法如依赖静态工具集或离线训练无法适应这些动态变化，因此亟需一种能够在线自适应、无需监督标签的解决方案，以提升智能系统在现实世界应用中的弹性和适应性。",
      "method": "研究提出 In-Situ Self-Evolving 范式，将序列任务交互视为连续经验流，使系统能够从短期执行反馈中蒸馏出长期可重用能力，而无需真实标签。关键创新是工具进化作为能力扩展的途径，提供可验证的二元反馈信号。具体实现为 Yunjue Agent 系统，采用 Parallel Batch Evolution 策略批量优化工具进化过程，迭代合成、优化和重用工具以提高效率。",
      "result": "在零起点设置下，对五个多样化基准进行实证评估，结果显示相比专有基线方法有显著性能提升，具体增益摘要未明确说明，但强调了系统在多个任务上的优越性。补充热启动评估证实了累积的通用知识能够无缝迁移到新领域，增强了系统的泛化能力和适应性，表明自进化机制有效促进了知识积累和转移。",
      "conclusion": "该论文的主要贡献是提出了 In-Situ Self-Evolving 范式，并开发了 Yunjue Agent 系统，在零起点设置下实现高效自适应。学术上，这为开放环境中的代理系统提供了新方法，强调无监督在线进化；实际上，支持自进化的能力扩展，适用于动态场景。通过开源代码库和工具，促进了弹性、自进化智能的研究，未来工作可能涉及进一步优化进化策略和扩展到更广泛的应用领域。",
      "tags": [
        "Self-Evolving Agent",
        "Tool Evolution",
        "Parallel Batch Evolution",
        "Zero-Start Learning",
        "In-Situ Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:52.715151Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18225",
    "title": "ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants",
    "authors": [
      "Pei Wang",
      "Yanan Wu",
      "Xiaoshuai Song",
      "Weixun Wang",
      "Gengru Chen",
      "Zhongwen Li",
      "Kezhong Yan",
      "Ken Deng",
      "Qi Liu",
      "Shuaibing Zhao",
      "Shaopan Xiong",
      "Xuepeng Liu",
      "Xuefeng Chen",
      "Wanxi Deng",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "abstract": "Large language model (LLM)-based agents are increasingly deployed in e-commerce shopping. To perform thorough, user-tailored product searches, agents should interpret personal preferences, engage in multi-turn dialogues, and ultimately retrieve and discriminate among highly similar products. However, existing research has yet to provide a unified simulation environment that consistently captures all of these aspects, and always focuses solely on evaluation benchmarks without training support. In this paper, we introduce ShopSimulator, a large-scale and challenging Chinese shopping environment. Leveraging ShopSimulator, we evaluate LLMs across diverse scenarios, finding that even the best-performing models achieve less than 40% full-success rate. Error analysis reveals that agents struggle with deep search and product selection in long trajectories, fail to balance the use of personalization cues, and to effectively engage with users. Further training exploration provides practical guidance for overcoming these weaknesses, with the combination of supervised fine-tuning (SFT) and reinforcement learning (RL) yielding significant performance improvements. Code and data will be released at https://github.com/ShopAgent-Team/ShopSimulator.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18225.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18225",
    "published": "2026-01-26T07:24:28Z",
    "updated": "2026-01-26T07:24:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "ShopSimulator 提供了一个大规模的购物模拟环境，用于评估和通过监督微调与强化学习结合训练大语言模型代理。",
      "motivation": "目前，基于大语言模型的购物助手代理在电商应用中需要处理用户个性化偏好、多轮对话和复杂产品检索，但现有研究缺乏一个统一的模拟环境来全面评估这些能力，且多只关注基准测试而忽视训练支持。这限制了代理的优化和实际部署效果，导致代理在深度搜索和用户互动方面表现不足。因此，本研究旨在解决这一问题，提供一个集成评估与训练的仿真平台，以提升代理在真实场景中的性能。",
      "method": "论文提出了 ShopSimulator，一个大规模、挑战性的中文购物模拟环境，用于模拟真实购物场景，包括个人偏好理解、多轮交互和产品选择。方法包括构建该环境，并利用它评估多种大语言模型在多样场景下的表现。关键创新点是通过监督微调（SFT）和强化学习（RL）的组合来训练代理，探索如何优化代理的行为，以克服在长轨迹中的搜索和选择困难。环境设计集成了评估和训练功能，支持全面的代理开发。",
      "result": "实验结果显示，在使用 ShopSimulator 评估时，即使表现最佳的大语言模型，其完全成功率也低于 40%，揭示了代理在深度搜索、产品区分、平衡个性化线索和用户互动方面的不足。错误分析进一步明确了这些弱点，表明代理在复杂任务中表现有限。通过结合 SFT 和 RL 进行训练，代理性能得到显著改善，具体提升数据摘要未明确说明，但与基线方法相比展示了明显的进步，强调了训练探索的重要性。",
      "conclusion": "本研究的核心贡献是开发了 ShopSimulator 模拟环境，填补了现有研究在统一评估和训练方面的空白。通过 SFT+RL 方法的探索，为改进购物助手代理提供了实用指导，具有学术价值（如建立新基准）和实际应用价值（提升电商助手效率）。未来工作可能涉及进一步优化训练策略、扩展环境到其他领域或解决代理在个性化交互中的局限性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Supervised Fine-Tuning",
        "Simulation Environment",
        "Shopping Assistant"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:05.237004Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18222",
    "title": "HomoFM: Deep Homography Estimation with Flow Matching",
    "authors": [
      "Mengfan He",
      "Liangzheng Sun",
      "Chunyu Li",
      "Ziyang Meng"
    ],
    "abstract": "Deep homography estimation has broad applications in computer vision and robotics. Remarkable progresses have been achieved while the existing methods typically treat it as a direct regression or iterative refinement problem and often struggling to capture complex geometric transformations or generalize across different domains. In this work, we propose HomoFM, a new framework that introduces the flow matching technique from generative modeling into the homography estimation task for the first time. Unlike the existing methods, we formulate homography estimation problem as a velocity field learning problem. By modeling a continuous and point-wise velocity field that transforms noisy distributions into registered coordinates, the proposed network recovers high-precision transformations through a conditional flow trajectory. Furthermore, to address the challenge of domain shifts issue, e.g., the cases of multimodal matching or varying illumination scenarios, we integrate a gradient reversal layer (GRL) into the feature extraction backbone. This domain adaptation strategy explicitly constrains the encoder to learn domain-invariant representations, significantly enhancing the network's robustness. Extensive experiments demonstrate the effectiveness of the proposed method, showing that HomoFM outperforms state-of-the-art methods in both estimation accuracy and robustness on standard benchmarks. Code and data resource are available at https://github.com/hmf21/HomoFM.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18222.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18222",
    "published": "2026-01-26T07:17:32Z",
    "updated": "2026-01-26T07:17:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "HomoFM首次将生成建模中的流匹配技术引入深度同源性估计，通过学习速度场和集成领域适应策略，实现了高精度和强鲁棒性的变换恢复。",
      "motivation": "深度同源性估计在计算机视觉和机器人学中应用广泛，如图像配准和视觉SLAM。现有方法通常将问题视为直接回归或迭代优化，难以有效捕捉复杂几何变形，且在跨领域场景（如多模态匹配或光照变化）中泛化能力不足，导致在实际应用中性能受限。因此，开发一种能精确建模变换并提升鲁棒性的新框架至关重要，以应对现实世界中的多变环境。",
      "method": "HomoFM框架创新性地将生成建模中的流匹配技术应用于同源性估计，将问题重新定义为速度场学习。它通过建模一个连续的点对点速度场，将噪声分布逐步转换为配准坐标，网络利用条件流轨迹恢复高精度同源性矩阵。为处理领域转移挑战，如多模态匹配或光照变化，方法在特征提取骨干中集成梯度反转层（GRL），迫使编码器学习领域不变的特征表示，从而显著增强网络的跨场景鲁棒性。",
      "result": "大量实验验证了HomoFM的有效性。在标准基准测试中，该方法在估计精度和鲁棒性方面均优于现有最先进方法。尽管摘要未提供具体性能数据（如准确率提升百分比），但实验结果表明，通过流匹配和领域适应策略，网络能更准确地恢复复杂几何变换，并在跨领域场景中保持稳定性能，证实了方法的优越性。",
      "conclusion": "HomoFM通过首次应用流匹配技术到同源性估计，显著提升了精度和鲁棒性，为几何估计领域引入了新思路。其学术价值在于将生成建模与计算机视觉任务结合，推动图像配准和视觉系统的发展；实际应用价值体现在增强视觉SLAM和机器人导航的稳定性。未来工作可探索扩展到其他几何估计任务或集成更多先验知识，以进一步提升泛化能力。",
      "tags": [
        "Deep Homography Estimation",
        "Flow Matching",
        "Velocity Field Learning",
        "Domain Adaptation",
        "Gradient Reversal Layer"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:24.937135Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18217",
    "title": "Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents",
    "authors": [
      "Zhihan Liu",
      "Lin Guan",
      "Yixin Nie",
      "Kai Zhang",
      "Zhuoqun Hao",
      "Lin Chen",
      "Asli Celikyilmaz",
      "Zhaoran Wang",
      "Na Zhang"
    ],
    "abstract": "Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18217.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18217",
    "published": "2026-01-26T07:07:03Z",
    "updated": "2026-01-26T07:07:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过分析强化学习环境属性对大型语言模型代理跨领域泛化的影响，提出了一种增强泛化能力的随机化技术。",
      "motivation": "通用大型语言模型代理通常在窄环境集上进行后训练，但部署时面临广泛未见领域的挑战，导致泛化性能下降。现有方法在未知测试领域下的优化不足，过度关注领域真实性和文本相似性，忽视了环境属性的关键作用。因此，本研究旨在探索强化学习训练中哪些因素最能影响跨领域泛化，以提升代理在实际应用中的适应性和鲁棒性，解决部署中的泛化差距问题。",
      "method": "论文首先分析了强化学习环境的属性，识别出状态信息丰富度（即代理从状态中处理的信息量）和规划复杂性（通过基础策略下的目标可达性和轨迹长度估计）作为影响跨领域泛化的关键因素。通过比较不同环境（如Sokoban和ALFWorld）验证这些相关性。基于此，提出了一种低开销的随机化技术：向状态中添加少量干扰性目标无关特征来增加信息丰富度，而不改变任务本质。此外，还考察了建模选择，包括监督微调热身、混合训练以及逐步思考的开启，以评估其对泛化的影响。",
      "result": "实验表明，状态信息丰富度和规划复杂性是跨领域泛化的强相关因素，而非领域真实性或文本相似性；例如，简单的Sokoban环境在SciWorld中泛化效果优于更真实的ALFWorld。提出的随机化技术能有效提高跨领域鲁棒性，但摘要未明确说明具体性能指标。建模选择方面，监督微调热身有助于防止灾难性遗忘，但可能损害未包含在训练混合中领域的泛化；逐步思考在保持泛化中起关键作用，尽管不总是提升域内性能，与基线方法对比显示了这些因素的差异化影响。",
      "conclusion": "本研究的主要贡献在于识别了强化学习训练中影响大型语言模型代理跨领域泛化的关键环境属性，并提出了一种实用的随机化技术来增强泛化能力。这为设计更鲁棒的代理训练流程提供了理论指导和实践方法，具有重要的学术价值，有助于优化代理在多样化场景中的部署。未来工作可进一步探索其他环境因素或优化技术，以扩展泛化研究的深度和应用范围。",
      "tags": [
        "Large Language Model Agents",
        "Reinforcement Learning",
        "Cross-Domain Generalization",
        "State Information Richness",
        "Planning Complexity"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:28.575942Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18207",
    "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
    "authors": [
      "James Burgess",
      "Jan N. Hansen",
      "Duo Peng",
      "Yuhui Zhang",
      "Alejandro Lozano",
      "Min Woo Sun",
      "Emma Lundberg",
      "Serena Yeung-Levy"
    ],
    "abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18207.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18207",
    "published": "2026-01-26T06:46:16Z",
    "updated": "2026-01-26T06:46:16Z",
    "comment": "EACL 2026",
    "light_analysis": {
      "overview": "该论文提出一种基于强化学习与可验证奖励的训练方法，用于构建在科学论文上进行搜索和推理的搜索代理，并发布了生物医学领域的语料库和问答数据集。",
      "motivation": "研究动机源于现有强化学习与可验证奖励（RLVR）的搜索代理主要集中在通用领域问答，忽视了科学、工程和医学等技术领域的特殊需求。科学领域的技术问答具有挑战性，直接关系到实际科学家的研究和未来AI科学家系统的开发。现有方法缺乏针对科学论文的专门训练环境和数据集，限制了其在专业领域的应用效果和相关性，因此需要开发能够处理科学文献搜索和推理的代理系统来弥补这一不足。",
      "method": "研究方法涉及使用强化学习与可验证奖励（RLVR）训练搜索代理，使其能够在科学论文中执行搜索和推理任务。具体地，发布了包含1600万生物医学论文摘要的搜索语料库，并构建了名为PaperSearchQA的事实性问答数据集，包含60k个样本，并提供了基准测试。该方法利用流行的Search-R1代码库进行RLVR训练，数据创建方法设计为可扩展，便于扩展到其他科学领域，以提高模型的适应性和泛化能力。",
      "result": "实验结果显示，训练的搜索代理在性能上超越了非强化学习的检索基线方法，表明RLVR方法在科学论文搜索任务中具有有效性。通过进一步定量分析，观察到代理展现出有趣的行为，如规划、推理和自我验证，这些行为有助于提高问答的准确性和可靠性。然而，摘要未明确说明具体的性能指标数值，如准确率或效率改进的百分比，但强调了代理行为的提升和与基线的对比优势。",
      "conclusion": "论文的主要贡献是提出了训练搜索代理在科学论文上进行搜索和推理的RLVR方法，并发布了大规模的生物医学语料库和PaperSearchQA数据集及基准。研究的学术价值在于推动了科学领域AI系统的发展，尤其是未来AI科学家系统的能力构建；实际应用价值在于辅助科学家进行文献检索和复杂问答。局限性可能在于仅针对生物医学领域，但未来工作方向可通过可扩展的数据创建方法扩展到其他科学领域，以提升泛化性和实用性。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Search Agents",
        "Language Models",
        "Factoid Question Answering",
        "Biomedical Corpus"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:44.567429Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18204",
    "title": "MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning",
    "authors": [
      "Juexiang Ye",
      "Xue Li",
      "Xinyu Yang",
      "Chengkai Huang",
      "Lanshun Nie",
      "Lina Yao",
      "Dechen Zhan"
    ],
    "abstract": "Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\\% compared to long-context baselines.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18204.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18204",
    "published": "2026-01-26T06:39:27Z",
    "updated": "2026-01-26T06:39:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "MemWeaver提出了一种统一记忆框架，通过整合混合记忆系统来增强长时程大语言模型代理的追踪性和推理能力。",
      "motivation": "研究动机源于大语言模型代理在长时程交互中面临的记忆挑战，例如需要支持时间一致性、多跳推理和会话间证据重用。现有方法主要依赖非结构化检索或粗略抽象，常导致时间冲突、推理脆弱性以及有限的可追踪性，这限制了代理在复杂任务中的鲁棒性和有效性，因此开发更高效的记忆系统至关重要。",
      "method": "MemWeaver的核心方法是设计一个统一记忆框架，包含三个互连组件：时间图记忆用于结构化关系推理，经验记忆从重复观察中抽象交互模式，段落记忆保留原始文本证据。框架采用双通道检索策略，联合检索结构化知识和支持证据以构建紧凑且信息密集的推理上下文。在LoCoMo基准上进行实验，关键创新点在于实现记忆整合和可追踪性。",
      "result": "在LoCoMo基准上的实验结果表明，MemWeaver显著提高了多跳推理和时间推理的准确性。与长上下文基线相比，输入上下文长度减少了超过95%，同时保持了较高的性能，这意味着框架在提升推理质量的同时大大降低了计算开销，显示出效率和有效性的双重优势。",
      "conclusion": "论文的主要贡献是提出了MemWeaver框架，解决了长时程代理记忆系统中的关键问题，通过整合混合记忆和双通道检索提高了推理的准确性和可追踪性。该研究具有重要的学术价值和实际应用潜力，未来工作可能包括扩展到更多领域或优化记忆管理策略以增强代理能力，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model-based Agents",
        "Graph Memory",
        "Dual-channel Retrieval",
        "Multi-hop Reasoning",
        "Temporal Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:45.933818Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18202",
    "title": "SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback",
    "authors": [
      "Fangyuan Xu",
      "Rujun Han",
      "Yanfei Chen",
      "Zifeng Wang",
      "I-Hung Hsu",
      "Jun Yan",
      "Vishy Tirumalashetty",
      "Eunsol Choi",
      "Tomas Pfister",
      "Chen-Yu Lee"
    ],
    "abstract": "Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18202.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18202",
    "published": "2026-01-26T06:37:56Z",
    "updated": "2026-01-26T06:37:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出SAGE管道，通过代理交互和执行反馈自动化生成高质量、难度可控的深度搜索问题-答案对。",
      "motivation": "深度搜索代理旨在加速需要跨文档推理的复杂问题回答，但人工标注成本高昂，因为探索轨迹长且复杂，限制了高质量训练数据的获取。现有方法依赖于昂贵的人工干预，难以大规模生成难度可控的数据，阻碍了搜索代理的性能提升和应用扩展。因此，自动化生成合成数据成为关键研究方向，以降低成本并提高数据质量。摘要未明确说明其他具体局限性。",
      "method": "SAGE方法包括数据生成器和搜索代理两个核心组件：数据生成器提出初始问题-答案对，搜索代理尝试解答问题并提供执行反馈。两者通过多轮交互迭代优化，直到生成的数据满足预设难度级别。关键创新在于使用代理交互和执行反馈机制，实现数据生成过程的难度控制和自动校正，提升生成的多样性和准确性。该方法基于给定语料库和目标难度，无需人工干预，涉及自动化管道和推理策略设计。",
      "result": "内在评估显示，SAGE生成的问题需要多样推理策略，显著提高正确性和难度控制能力。外在评估表明，使用合成数据训练的深度搜索代理在流行基准测试中获得最多23%的相对性能提升，具体数据未明确说明基准名称。额外实验验证了训练代理能够适应从固定语料库检索到Google搜索的切换，无需额外训练，增强了方法的泛化性和应用价值。",
      "conclusion": "论文主要贡献是提出SAGE管道，自动化生成高质量合成数据，提升深度搜索代理性能，并实现适应性优化。学术价值在于减少了依赖昂贵人工标注，为自动数据生成领域提供新方法；实际应用价值在于改善信息检索系统效率。局限性或未来工作方向摘要未明确说明，可进一步探索扩展到更复杂场景或优化反馈机制。",
      "tags": [
        "Deep Search Agents",
        "Synthetic Data Generation",
        "Iterative Optimization",
        "Question Answering",
        "Execution Feedback"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:32.224843Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18200",
    "title": "HeterCSI: Channel-Adaptive Heterogeneous CSI Pretraining Framework for Generalized Wireless Foundation Models",
    "authors": [
      "Chenyu Zhang",
      "Xinchen Lyu",
      "Chenshan Ren",
      "Shuhan Liu",
      "Qimei Cui",
      "Xiaofeng Tao"
    ],
    "abstract": "Wireless foundation models promise transformative capabilities for channel state information (CSI) processing across diverse 6G network applications, yet face fundamental challenges due to the inherent dual heterogeneity of CSI across both scale and scenario dimensions. However, current pretraining approaches either constrain inputs to fixed dimensions or isolate training by scale, limiting the generalization and scalability of wireless foundation models. In this paper, we propose HeterCSI, a channel-adaptive pretraining framework that reconciles training efficiency with robust cross-scenario generalization via a new understanding of gradient dynamics in heterogeneous CSI pretraining. Our key insight reveals that CSI scale heterogeneity primarily causes destructive gradient interference, while scenario diversity actually promotes constructive gradient alignment when properly managed. Specifically, we formulate heterogeneous CSI batch construction as a partitioning optimization problem that minimizes zero-padding overhead while preserving scenario diversity. To solve this, we develop a scale-aware adaptive batching strategy that aligns CSI samples of similar scales, and design a double-masking mechanism to isolate valid signals from padding artifacts. Extensive experiments on 12 datasets demonstrate that HeterCSI establishes a generalized foundation model without scenario-specific finetuning, achieving superior average performance over full-shot baselines. Compared to the state-of-the-art zero-shot benchmark WiFo, it reduces NMSE by 7.19 dB, 4.08 dB, and 5.27 dB for CSI reconstruction, time-domain, and frequency-domain prediction, respectively. The proposed HeterCSI framework also reduces training latency by 53% compared to existing approaches while improving generalization performance by 1.53 dB on average.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18200.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18200",
    "published": "2026-01-26T06:35:48Z",
    "updated": "2026-01-26T06:35:48Z",
    "comment": "13 pages, 8 figures",
    "light_analysis": {
      "overview": "HeterCSI提出一个通道自适应的异构CSI预训练框架，通过优化梯度动态处理CSI的规模和场景异质性，实现无线基础模型的泛化和高效训练。",
      "motivation": "无线基础模型在6G网络中用于处理信道状态信息（CSI），但CSI在规模和场景上具有双重异质性，导致现有预训练方法要么约束输入到固定维度，要么按规模隔离训练，这限制了模型的泛化能力和可扩展性。因此，需要解决CSI异质性带来的训练效率低和泛化不足的问题，以促进无线基础模型在多样化网络应用中的部署，提高实际效用。",
      "method": "方法的核心是将异构CSI批量构造公式化为分区优化问题，以最小化零填充开销并保持场景多样性。为此，设计了尺度感知自适应批处理策略，对齐相似尺度的CSI样本，减少梯度干扰；同时引入双掩码机制，隔离有效信号与填充伪影，管理梯度动态以平衡训练效率和跨场景泛化。这些技术整合为HeterCSI框架，无需场景特定调整即可处理CSI异质性。",
      "result": "实验在12个数据集上进行，结果显示HeterCSI在无需场景特定微调的情况下建立了广义基础模型，性能优于全-shot基线。与零-shot基准WiFo相比，在CSI重建、时域和频域预测任务中，NMSE分别降低了7.19 dB、4.08 dB和5.27 dB。此外，训练延迟减少了53%，泛化性能平均提升了1.53 dB，证明了其高效性和优越性。",
      "conclusion": "该研究的主要贡献是提出了HeterCSI框架，有效解决了CSI的规模和场景异质性挑战，通过优化梯度动态实现了无线基础模型的泛化和高效预训练。学术上，它提供了新的梯度理解和方法；应用上，为6G网络的CSI处理奠定了基础。摘要未明确说明局限性或未来工作，但可推断其潜在扩展至更多无线场景。",
      "tags": [
        "Channel State Information (CSI)",
        "Pretraining",
        "Gradient Dynamics",
        "Adaptive Batching",
        "Wireless Foundation Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:54.499169Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18197",
    "title": "GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models",
    "authors": [
      "Shaokang Wang",
      "Pei Fu",
      "Ruoceng Zhang",
      "Shaojie Zhang",
      "Xiuwen Xi",
      "Jiahui Yang",
      "Bin Qin",
      "Ying Huang",
      "Zhenbo Luo",
      "Jian Luan"
    ],
    "abstract": "While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations, where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18197.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18197",
    "published": "2026-01-26T06:29:41Z",
    "updated": "2026-01-26T06:29:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了GAIA系统，通过数据飞轮机制训练GUI代理的评论家模型，以改进其在测试时间的性能缩放。",
      "motivation": "随着大视觉语言模型（LVLMs）在GUI代理领域的应用，代理操作的不可逆性成为关键挑战，单个错误动作可能导致灾难性偏差，影响代理的可靠性和实用性。现有方法虽能解析指令和屏幕内容，但缺乏即时反馈机制来预防错误，限制了测试时间性能的动态优化。因此，需要一种能自我改进的框架来增强代理的鲁棒性和准确性。",
      "method": "GAIA系统采用数据飞轮机制，首先训练直觉评论家模型（ICM），使用基础GUI代理生成的正负动作示例来评估动作的即时正确性并选择高成功率操作。随后，ICM指导代理收集精炼的正负样本，启动自我改进循环。增强的数据用于训练第二轮评论家，提升辨别能力，形成迭代优化过程。该方法在各种数据集上进行实验，未明确说明模型架构细节，但强调通过数据循环实现性能提升。",
      "result": "实验结果表明，直觉评论家模型（ICM）能有效改进GUI代理的测试时间性能。在多个数据集上，ICM对闭源和开源模型均显示出性能提升，且性能随着数据飞轮的迭代逐渐增强。摘要未提供具体数值对比，但暗示了ICM能减少错误动作、提高任务成功率，优于基础代理方法，强调了数据循环带来的持续优化效果。",
      "conclusion": "本文的核心贡献是提出了GAIA系统，一个基于数据飞轮的训练框架，使GUI代理具备迭代评论能力，提升了测试时间性能缩放。学术上，它结合了评论家模型和数据增强技术，为GUI自动化任务提供了新方法；应用上，可增强智能助手和自动化工具的可靠性。未来工作可能包括优化模型架构、扩展数据集或应用于更多场景，局限性可能依赖于初始数据质量和模型泛化能力。",
      "tags": [
        "Large Vision-Language Models",
        "GUI Agents",
        "Critic Models",
        "Test-Time Scaling",
        "Data Flywheel System"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:12.009810Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18195",
    "title": "QualiRAG: Retrieval-Augmented Generation for Visual Quality Understanding",
    "authors": [
      "Linhan Cao",
      "Wei Sun",
      "Weixia Zhang",
      "Xiangyang Zhu",
      "Kaiwei Zhang",
      "Jun Jia",
      "Dandan Zhu",
      "Guangtao Zhai",
      "Xiongkuo Min"
    ],
    "abstract": "Visual quality assessment (VQA) is increasingly shifting from scalar score prediction toward interpretable quality understanding -- a paradigm that demands \\textit{fine-grained spatiotemporal perception} and \\textit{auxiliary contextual information}. Current approaches rely on supervised fine-tuning or reinforcement learning on curated instruction datasets, which involve labor-intensive annotation and are prone to dataset-specific biases. To address these challenges, we propose \\textbf{QualiRAG}, a \\textit{training-free} \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{G}eneration \\textbf{(RAG)} framework that systematically leverages the latent perceptual knowledge of large multimodal models (LMMs) for visual quality perception. Unlike conventional RAG that retrieves from static corpora, QualiRAG dynamically generates auxiliary knowledge by decomposing questions into structured requests and constructing four complementary knowledge sources: \\textit{visual metadata}, \\textit{subject localization}, \\textit{global quality summaries}, and \\textit{local quality descriptions}, followed by relevance-aware retrieval for evidence-grounded reasoning. Extensive experiments show that QualiRAG achieves substantial improvements over open-source general-purpose LMMs and VQA-finetuned LMMs on visual quality understanding tasks, and delivers competitive performance on visual quality comparison tasks, demonstrating robust quality assessment capabilities without any task-specific training. The code will be publicly available at https://github.com/clh124/QualiRAG.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18195.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18195",
    "published": "2026-01-26T06:27:03Z",
    "updated": "2026-01-26T06:27:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "QualiRAG提出一个无需训练的检索增强生成框架，通过动态知识生成和检索增强视觉质量理解能力。",
      "motivation": "视觉质量评估正从标量评分预测转向可解释的质量理解，这需要细粒度时空感知和辅助上下文信息。当前方法依赖监督微调或强化学习在人工标注数据集上，存在劳动密集、成本高且易产生数据集特定偏见的问题，限制了模型的泛化能力和实际应用效果。",
      "method": "QualiRAG是一个训练免费的RAG框架，利用大型多模态模型的潜感知知识进行视觉质量感知。核心创新在于动态生成辅助知识，将问题分解为结构化请求，构建四个互补知识源：视觉元数据、主体定位、全局质量摘要和局部质量描述，然后通过相关感知检索进行证据驱动的推理，无需任务特定训练。",
      "result": "实验表明，QualiRAG在视觉质量理解任务上显著优于开源通用大型多模态模型和经过视觉质量微调的模型，取得了实质性性能提升。在视觉质量比较任务中也展现出竞争力，证明了其无需专门训练即可实现鲁棒质量评估的能力，对比基线方法有明显优势。",
      "conclusion": "QualiRAG的主要贡献是提供一个无需训练的检索增强生成框架，有效利用大型多模态模型进行视觉质量理解，解决了传统方法依赖标注数据的局限性。具有推动训练免费方法发展的学术价值，实际应用前景在于高效、低成本的视觉质量评估，未来可能扩展到更多任务或改进检索机制。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Large Multimodal Models",
        "Visual Quality Assessment",
        "Fine-Grained Perception",
        "Knowledge Retrieval"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:53.053508Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18192",
    "title": "MindCine: Multimodal EEG-to-Video Reconstruction with Large-Scale Pretrained Models",
    "authors": [
      "Tian-Yi Zhou",
      "Xuan-Hao Liu",
      "Bao-Liang Lu",
      "Wei-Long Zheng"
    ],
    "abstract": "Reconstructing human dynamic visual perception from electroencephalography (EEG) signals is of great research significance since EEG's non-invasiveness and high temporal resolution. However, EEG-to-video reconstruction remains challenging due to: 1) Single Modality: existing studies solely align EEG signals with the text modality, which ignores other modalities and are prone to suffer from overfitting problems; 2) Data Scarcity: current methods often have difficulty training to converge with limited EEG-video data. To solve the above problems, we propose a novel framework MindCine to achieve high-fidelity video reconstructions on limited data. We employ a multimodal joint learning strategy to incorporate beyond-text modalities in the training stage and leverage a pre-trained large EEG model to relieve the data scarcity issue for decoding semantic information, while a Seq2Seq model with causal attention is specifically designed for decoding perceptual information. Extensive experiments demonstrate that our model outperforms state-of-the-art methods both qualitatively and quantitatively. Additionally, the results underscore the effectiveness of the complementary strengths of different modalities and demonstrate that leveraging a large-scale EEG model can further enhance reconstruction performance by alleviating the challenges associated with limited data.",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.MM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18192.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18192",
    "published": "2026-01-26T06:20:34Z",
    "updated": "2026-01-26T06:20:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "MindCine框架通过多模态联合学习和预训练大模型，在有限EEG数据上实现高保真视频重建，解决了单模态和数据稀缺问题。",
      "motivation": "研究背景是利用脑电图（EEG）信号重建人类动态视觉感知，因EEG的非侵入性和高时间分辨率而具有重要科研价值，如脑机接口和神经科学应用。然而，现有方法面临两个主要挑战：一是单模态对齐，仅将EEG与文本模态结合，忽略视觉、听觉等其他模态，易导致过拟合和信息丢失；二是数据稀缺，现有EEG-视频数据集有限，使得模型训练困难，收敛性能不佳。因此，亟需开发新方法来克服这些限制，提升重建精度和泛化能力。",
      "method": "论文提出MindCine框架，核心方法包括三个部分：采用多模态联合学习策略，在训练中整合超越文本的模态（如视觉特征），以减少过拟合并增强特征表示；利用预训练的大型EEG模型（基于大规模数据）来解码语义信息，以缓解数据稀缺问题，提升模型泛化；专门设计一个带有因果注意力的序列到序列（Seq2Seq）模型，用于解码感知信息，确保视频重建的时序连贯性和准确性，从而实现对EEG信号的高效处理和视觉内容生成。",
      "result": "实验结果表明，MindCine在质量和数量上均显著优于现有最先进方法。多模态联合学习展示了互补优势，有效提升重建性能，而大规模预训练EEG模型进一步缓解了数据稀缺挑战，增强了语义解码能力。尽管摘要未明确说明具体性能指标（如准确率或效率数据），但通过定性和定量对比，模型在EEG-to-video任务中展现出卓越效果，验证了方法的有效性和鲁棒性。",
      "conclusion": "论文的主要贡献是开发了MindCine框架，创新性地结合多模态学习和预训练模型，有效解决了EEG-to-video重建中的单模态和数据稀缺难题。该研究具有重要学术价值，推动了脑信号解码领域的发展，并为实际应用如脑机接口和神经康复提供了新思路。潜在的局限性或未来工作方向摘要未明确说明，但可探索更多模态集成或扩展到其他感知重建任务。",
      "tags": [
        "EEG-to-Video Reconstruction",
        "Multimodal Learning",
        "Large-Scale Pretrained Models",
        "Sequence-to-Sequence Model",
        "Causal Attention"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:13.063514Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18190",
    "title": "Multi-Perspective Subimage CLIP with Keyword Guidance for Remote Sensing Image-Text Retrieval",
    "authors": [
      "Yifan Li",
      "Shiying Wang",
      "Jianqiang Huang"
    ],
    "abstract": "Vision-Language Pre-training (VLP) models like CLIP have significantly advanced Remote Sensing Image-Text Retrieval (RSITR). However, existing methods predominantly rely on coarse-grained global alignment, which often overlooks the dense, multi-scale semantics inherent in overhead imagery. Moreover, adapting these heavy models via full fine-tuning incurs prohibitive computational costs and risks catastrophic forgetting. To address these challenges, we propose MPS-CLIP, a parameter-efficient framework designed to shift the retrieval paradigm from global matching to keyword-guided fine-grained alignment. Specifically, we leverage a Large Language Model (LLM) to extract core semantic keywords, guiding the Segment Anything Model (SamGeo) to generate semantically relevant sub-perspectives. To efficiently adapt the frozen backbone, we introduce a Gated Global Attention (G^2A) adapter, which captures global context and long-range dependencies with minimal overhead. Furthermore, a Multi-Perspective Representation (MPR) module aggregates these local cues into robust multi-perspective embeddings. The framework is optimized via a hybrid objective combining multi-perspective contrastive and weighted triplet losses, which dynamically selects maximum-response perspectives to suppress noise and enforce precise semantic matching. Extensive experiments on the RSICD and RSITMD benchmarks demonstrate that MPS-CLIP achieves state-of-the-art performance with 35.18% and 48.40% mean Recall (mR), respectively, significantly outperforming full fine-tuning baselines and recent competitive methods. Code is available at https://github.com/Lcrucial1f/MPS-CLIP.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18190.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18190",
    "published": "2026-01-26T06:16:53Z",
    "updated": "2026-01-26T06:16:53Z",
    "comment": "7 pages, 3 figures. Code: https://github.com/Lcrucial1f/MPS-CLIP",
    "light_analysis": {
      "overview": "MPS-CLIP提出参数高效框架，通过关键词引导生成多视角子图像，实现遥感图像文本检索的细粒度对齐。",
      "motivation": "在遥感图像文本检索中，现有方法主要依赖视觉语言预训练模型进行粗粒度全局对齐，这忽略了遥感图像密集、多尺度的语义特性，导致检索精度不足。此外，通过完全微调适应这些大型模型需要高昂计算成本，并易引发灾难性遗忘，限制了实际应用。因此，开发一种参数高效且能捕捉细粒度语义的框架至关重要，以提升检索性能和适应性，解决现有方法的不足。",
      "method": "MPS-CLIP框架首先利用大型语言模型提取核心语义关键词，指导Segment Anything Model生成语义相关的子视角，以捕捉多尺度信息。为高效适应冻结骨干网络，引入门控全局注意力适配器，以低开销捕获全局上下文和长程依赖关系。多视角表示模块进一步聚合这些局部线索，形成鲁棒的多视角嵌入。框架通过混合损失函数优化，结合多视角对比损失和加权三元损失，动态选择最大响应视角以抑制噪声并强化语义匹配，无需完全微调即可实现参数高效调整。",
      "result": "在RSICD和RSITMD基准上，MPS-CLIP取得最先进性能，平均召回率分别达到35.18%和48.40%。实验结果表明，该框架显著优于完全微调基线和近期竞争方法，例如在效率提升和精度改进方面表现出色，验证了关键词引导和多视角细粒度对齐的有效性，具体数据支撑了其在遥感图像文本检索中的优越性。",
      "conclusion": "本研究主要贡献是提出了参数高效的MPS-CLIP框架，通过关键词引导和多视角细粒度对齐，显著提升了遥感图像文本检索的准确性和效率。学术上，它推动了视觉语言预训练模型在遥感领域的应用；实际中，减少计算成本并避免灾难性遗忘问题。潜在局限性或未来工作方向可能包括扩展到更多数据集或进一步优化模型架构以提升泛化能力。",
      "tags": [
        "Large Language Model",
        "Segment Anything Model",
        "Gated Global Attention",
        "Contrastive Learning",
        "Remote Sensing Image-Text Retrieval"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:41.464318Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18189",
    "title": "Smooth, Sparse, and Stable: Finite-Time Exact Skeleton Recovery via Smoothed Proximal Gradients",
    "authors": [
      "Rui Wu",
      "Yongjun Li"
    ],
    "abstract": "Continuous optimization has significantly advanced causal discovery, yet existing methods (e.g., NOTEARS) generally guarantee only asymptotic convergence to a stationary point. This often yields dense weighted matrices that require arbitrary post-hoc thresholding to recover a DAG. This gap between continuous optimization and discrete graph structures remains a fundamental challenge. In this paper, we bridge this gap by proposing the Hybrid-Order Acyclicity Constraint (AHOC) and optimizing it via the Smoothed Proximal Gradient (SPG-AHOC). Leveraging the Manifold Identification Property of proximal algorithms, we provide a rigorous theoretical guarantee: the Finite-Time Oracle Property. We prove that under standard identifiability assumptions, SPG-AHOC recovers the exact DAG support (structure) in finite iterations, even when optimizing a smoothed approximation. This result eliminates structural ambiguity, as our algorithm returns graphs with exact zero entries without heuristic truncation. Empirically, SPG-AHOC achieves state-of-the-art accuracy and strongly corroborates the finite-time identification theory.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18189.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18189",
    "published": "2026-01-26T06:16:47Z",
    "updated": "2026-01-26T06:16:47Z",
    "comment": "20 pages, 8 figures",
    "light_analysis": {
      "overview": "本文通过引入混合阶无环约束和平滑近端梯度优化，实现了有限时间内精确恢复因果图骨架结构，消除后处理阈值需求。",
      "motivation": "当前因果发现中，连续优化方法如NOTEARS仅保证渐近收敛，常产生密集矩阵，需依赖后处理阈值来提取有向无环图，这导致连续优化与离散图结构之间存在根本挑战，影响模型准确性和稳定性。为解决这一局限性，本文旨在开发一种能直接输出精确图结构的方法，避免启发式截断，提升因果推理的可靠性和效率。",
      "method": "本文提出混合阶无环约束（AHOC），并通过平滑近端梯度（SPG-AHOC）进行优化。该方法利用近端算法的流形识别特性，确保在有限迭代内直接恢复有向无环图的支持结构，无需后处理阈值。理论部分证明了有限时间Oracle属性，在标准可识别性假设下，即使优化平滑近似，也能精确输出具有零条目的图，消除了结构模糊性。",
      "result": "论文通过实验验证SPG-AHOC在因果发现任务上的性能，达到了最先进的准确性，并强有力地支持有限时间识别理论。与基线方法对比，展示了其在精确恢复图结构方面的优越性，避免了传统方法中的后处理需求，提升了模型的可靠性和解释性。摘要未明确说明具体数据集和指标数值。",
      "conclusion": "本研究主要贡献是提出AHOC约束和SPG-AHOC算法，实现了因果图结构的有限时间精确恢复，填补了连续优化与离散图结构之间的理论空白，增强了方法稳定性和可解释性。对因果推理和机器学习领域有重要价值，为优化算法设计提供了新方向。潜在局限性包括对标准可识别性假设的依赖，未来工作可探索更宽松的假设或复杂场景扩展。",
      "tags": [
        "Causal Discovery",
        "Proximal Gradient",
        "DAG Recovery",
        "Finite-Time Convergence",
        "Acyclicity Constraint"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:03.999577Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18188",
    "title": "\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation",
    "authors": [
      "Weiye Zhu",
      "Zekai Zhang",
      "Xiangchen Wang",
      "Hewei Pan",
      "Teng Wang",
      "Tiantian Geng",
      "Rongtao Xu",
      "Feng Zheng"
    ],
    "abstract": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18188.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18188",
    "published": "2026-01-26T06:16:17Z",
    "updated": "2026-01-26T06:16:17Z",
    "comment": "18 pages, 14 figures",
    "light_analysis": {
      "overview": "NaVIDA框架通过逆动力学增强和分层动作分块，提升视觉语言导航中视觉-动作因果关系建模和稳定性。",
      "motivation": "视觉语言导航要求代理在视觉丰富环境中根据自然语言指令行动。现有方法大多依赖反应式状态-动作映射，缺乏对动作如何因果改变视觉观察的显式建模，导致代理无法预见自身动作引起的视觉变化，引发行为不稳定、泛化能力弱和轨迹累积误差。这些问题降低了导航代理的可靠性和在实际应用中的效率，因此需要改进因果关系建模以提升整体性能。",
      "method": "NaVIDA框架结合策略学习、动作基础的视觉动态和自适应执行。核心创新包括使用基于块的逆动力学监督学习视觉变化与动作之间的因果关系；通过分层概率动作分块将轨迹组织为多步块，提供更长范围的视觉变化线索；并采用熵引导机制自适应设置动作块的执行范围，以在推理中稳定行为和减少误差累积。该方法在训练中增强因果关系学习，无需额外大型模型，并优化执行策略。",
      "result": "实验表明，NaVIDA在导航性能上优于最先进方法，且参数更少（3B对比8B）。广泛的基准测试显示其提升的稳定性和泛化能力，现实世界机器人评估进一步验证了其实用性和有效性。摘要未明确说明具体准确率指标，但强调了性能优越性和效率改进，表明在减少参数的同时实现了更好的导航效果。",
      "conclusion": "NaVIDA通过逆动力学增强和自适应执行，显著强化了视觉语言导航中的视觉-动作因果关系。其学术价值在于提供一种统一框架来建模动作对视觉变化的因果影响，实际应用价值体现在更少参数下的高性能和现实世界机器人部署的可行性。未来工作可探索扩展到更复杂动态环境或优化自适应机制以进一步提升鲁棒性。",
      "tags": [
        "Vision-Language Navigation",
        "Inverse Dynamics",
        "Action Chunking",
        "Hierarchical Probabilistic Models",
        "Adaptive Execution"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:48.139423Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18175",
    "title": "Success Conditioning as Policy Improvement: The Optimization Problem Solved by Imitating Success",
    "authors": [
      "Daniel Russo"
    ],
    "abstract": "A widely used technique for improving policies is success conditioning, in which one collects trajectories, identifies those that achieve a desired outcome, and updates the policy to imitate the actions taken along successful trajectories. This principle appears under many names -- rejection sampling with SFT, goal-conditioned RL, Decision Transformers -- yet what optimization problem it solves, if any, has remained unclear. We prove that success conditioning exactly solves a trust-region optimization problem, maximizing policy improvement subject to a $χ^2$ divergence constraint whose radius is determined automatically by the data. This yields an identity: relative policy improvement, the magnitude of policy change, and a quantity we call action-influence -- measuring how random variation in action choices affects success rates -- are exactly equal at every state. Success conditioning thus emerges as a conservative improvement operator. Exact success conditioning cannot degrade performance or induce dangerous distribution shift, but when it fails, it does so observably, by hardly changing the policy at all. We apply our theory to the common practice of return thresholding, showing this can amplify improvement, but at the cost of potential misalignment with the true objective.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SY",
      "stat.ML"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18175.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18175",
    "published": "2026-01-26T05:54:39Z",
    "updated": "2026-01-26T05:54:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文证明模仿成功轨迹的策略改进技术解决了一个信任区域优化问题，并揭示了策略改进、变化幅度与动作影响之间的等式关系。",
      "motivation": "成功条件化是一种广泛使用的策略改进技术，通过收集轨迹、识别成功轨迹并模仿其动作来更新策略，应用于拒绝采样与SFT、目标条件强化学习和决策变换器等多种方法。然而，这些技术所解决的优化问题一直不明确，导致缺乏理论基础，限制了对其性能和安全性的深入理解。因此，本研究旨在澄清成功条件化的数学优化问题，为相关实践提供理论支撑，以弥补现有方法的不足。",
      "method": "论文通过理论证明，成功条件化精确地解决了一个信任区域优化问题，即在χ²散度约束下最大化策略改进，其中约束半径由数据自动确定。关键创新点在于揭示了一个等式：相对策略改进、策略变化幅度和一个称为动作影响的量（衡量动作选择的随机变化如何影响成功率）在每个状态下都精确相等。该方法基于现有策略和成功轨迹数据进行推导，未明确使用特定数据集或模型架构，聚焦于优化框架的理论分析。",
      "result": "结果表明，成功条件化作为一个保守的改进操作符，不会降低性能或引发危险的分布偏移；当它失败时，会以几乎不改变策略的方式可观察地表现出来。摘要未明确提供具体实验数据如准确率提升，但理论分析显示该方法在信任区域内保证改进。应用于常见实践如回报阈值化时，虽然可以放大改进效果，但可能导致与真实目标的对齐问题，需在应用中谨慎权衡风险与收益。",
      "conclusion": "论文的主要贡献是为成功条件化技术建立了优化问题的理论框架，揭示了其内在的等式关系。学术上，这为模仿成功轨迹的方法提供了坚实的数学基础，增强了可解释性和可信度；实际应用中，有助于指导实践如回报阈值化，平衡改进与对齐风险。未来工作可探索该理论在更广泛策略优化场景中的应用，或处理实际中的对齐挑战与局限性。",
      "tags": [
        "Success Conditioning",
        "Trust-Region Optimization",
        "χ² Divergence",
        "Policy Improvement",
        "Action-Influence"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:12.346309Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18172",
    "title": "YOLO-DS: Fine-Grained Feature Decoupling via Dual-Statistic Synergy Operator for Object Detection",
    "authors": [
      "Lin Huang",
      "Yujuan Tan",
      "Weisheng Li",
      "Shitai Shan",
      "Liu Liu",
      "Bo Liu",
      "Linlin Shen",
      "Jing Yu",
      "Yue Niu"
    ],
    "abstract": "One-stage object detection, particularly the YOLO series, strikes a favorable balance between accuracy and efficiency. However, existing YOLO detectors lack explicit modeling of heterogeneous object responses within shared feature channels, which limits further performance gains. To address this, we propose YOLO-DS, a framework built around a novel Dual-Statistic Synergy Operator (DSO). The DSO decouples object features by jointly modeling the channel-wise mean and the peak-to-mean difference. Building upon the DSO, we design two lightweight gating modules: the Dual-Statistic Synergy Gating (DSG) module for adaptive channel-wise feature selection, and the Multi-Path Segmented Gating (MSG) module for depth-wise feature weighting. On the MS-COCO benchmark, YOLO-DS consistently outperforms YOLOv8 across five model scales (N, S, M, L, X), achieving AP gains of 1.1% to 1.7% with only a minimal increase in inference latency. Extensive visualization, ablation, and comparative studies validate the effectiveness of our approach, demonstrating its superior capability in discriminating heterogeneous objects with high efficiency.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18172.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18172",
    "published": "2026-01-26T05:50:32Z",
    "updated": "2026-01-26T05:50:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 YOLO-DS 框架，基于双统计协同算子实现细粒度特征解耦，显著提升目标检测性能。",
      "motivation": "YOLO 系列一阶段目标检测器在准确性和效率之间平衡良好，但现有方法缺乏对共享特征通道中异质对象响应的显式建模，限制了性能的进一步提升。本研究旨在解决这一问题，通过引入特征解耦机制来改进检测精度，同时保持效率，这对于实际应用如自动驾驶和视频监控等具有重要意义。",
      "method": "论文提出 YOLO-DS 框架，核心是 Dual-Statistic Synergy Operator (DSO)，通过联合建模通道均值和峰值与均值差异来解耦对象特征。基于 DSO，设计了两个轻量门控模块：Dual-Statistic Synergy Gating (DSG) 用于自适应通道特征选择，和 Multi-Path Segmented Gating (MSG) 用于深度特征加权。该方法基于 YOLO 架构，在 MS-COCO 数据集上进行评估，模型涵盖五个不同尺度（N, S, M, L, X），以确保轻量化和适应性。",
      "result": "在 MS-COCO 基准测试中，YOLO-DS 在五个模型尺度上均优于 YOLOv8，平均精度（AP）提升范围为 1.1% 到 1.7%，而推理延迟仅有最小增加。通过可视化、消融和比较研究验证了方法的有效性，展示了其在区分异质对象方面的优越性，性能提升显著且效率损失可忽略。",
      "conclusion": "本研究提出 YOLO-DS 和 DSO，有效解决了 YOLO 检测器中特征建模不足的问题，通过双统计协同算子实现特征解耦，提升了目标检测性能。该研究具有学术价值，为 YOLO 架构改进提供新思路，并具有实际应用潜力，如提高计算机视觉系统的精度和效率。摘要未明确说明其局限性，但未来工作可能包括优化模型或扩展到其他检测任务。",
      "tags": [
        "YOLO",
        "Object Detection",
        "Feature Decoupling",
        "Statistical Modeling",
        "Gating Modules"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:25.932413Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18171",
    "title": "Learning Fair Domain Adaptation with Virtual Label Distribution",
    "authors": [
      "Yuguang Zhang",
      "Lijun Sheng",
      "Jian Liang",
      "Ran He"
    ],
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to mitigate performance degradation when training and testing data are sampled from different distributions. While significant progress has been made in enhancing overall accuracy, most existing methods overlook performance disparities across categories-an issue we refer to as category fairness. Our empirical analysis reveals that UDA classifiers tend to favor certain easy categories while neglecting difficult ones. To address this, we propose Virtual Label-distribution-aware Learning (VILL), a simple yet effective framework designed to improve worst-case performance while preserving high overall accuracy. The core of VILL is an adaptive re-weighting strategy that amplifies the influence of hard-to-classify categories. Furthermore, we introduce a KL-divergence-based re-balancing strategy, which explicitly adjusts decision boundaries to enhance category fairness. Experiments on commonly used datasets demonstrate that VILL can be seamlessly integrated as a plug-and-play module into existing UDA methods, significantly improving category fairness.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18171.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18171",
    "published": "2026-01-26T05:48:47Z",
    "updated": "2026-01-26T05:48:47Z",
    "comment": "ICASSP 2026",
    "light_analysis": {
      "overview": "本论文提出了一种名为Virtual Label-distribution-aware Learning（VILL）的框架，通过自适应重加权和KL散度策略，改善无监督领域适应中的类别公平性，旨在提升最差情况性能并保持高总体准确率。",
      "motivation": "无监督领域适应（UDA）旨在解决训练和测试数据分布不同导致的模型性能下降问题。现有方法主要关注总体准确率提升，却忽略了不同类别间性能差异的公平性，即类别公平性问题。实证分析显示，UDA分类器往往偏向简单类别而忽视困难类别，这降低了模型在实际应用中的鲁棒性和公平性。因此，本研究关注于平衡UDA中的类别公平性，以提升模型泛化能力和公平性，弥补现有方法的不足。",
      "method": "本研究提出VILL框架，核心是自适应重加权策略，动态调整难分类别的权重，以增强其影响力。此外，引入基于KL散度的重平衡策略，明确调整决策边界，以优化类别公平性。VILL设计为即插即用模块，可无缝集成到现有UDA方法中，无需改变底层模型架构，从而简化了应用过程。具体技术细节如数据集未明确说明，但框架适用于常用UDA场景。",
      "result": "在常用数据集上的实验表明，VILL能显著提高类别公平性，同时保持高总体准确率。作为即插即用模块，它可以与多种现有UDA方法集成，有效提升最差情况性能。摘要未提供具体性能指标数据，但强调了对类别公平性的改进，显示出比基线方法更好的公平性表现，增强了模型的整体鲁棒性和适用性。",
      "conclusion": "本论文的主要贡献是提出了VILL框架，通过关注类别公平性扩展了UDA研究，结合自适应重加权和KL散度策略，在保持准确率的同时提升公平性。这具有重要的学术价值，推动了AI公平性和领域适应的交叉研究，实际应用中可提高模型在多样数据中的公平性和鲁棒性。局限性包括可能未覆盖所有场景，未来工作可探索更复杂的公平性指标或扩展到多领域适应中。",
      "tags": [
        "Unsupervised Domain Adaptation",
        "Category Fairness",
        "Adaptive Re-weighting",
        "KL-divergence",
        "Virtual Label Distribution"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:16.379428Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18168",
    "title": "TempDiffReg: Temporal Diffusion Model for Non-Rigid 2D-3D Vascular Registration",
    "authors": [
      "Zehua Liu",
      "Shihao Zou",
      "Jincai Huang",
      "Yanfang Zhang",
      "Chao Tong",
      "Weixin Si"
    ],
    "abstract": "Transarterial chemoembolization (TACE) is a preferred treatment option for hepatocellular carcinoma and other liver malignancies, yet it remains a highly challenging procedure due to complex intra-operative vascular navigation and anatomical variability. Accurate and robust 2D-3D vessel registration is essential to guide microcatheter and instruments during TACE, enabling precise localization of vascular structures and optimal therapeutic targeting. To tackle this issue, we develop a coarse-to-fine registration strategy. First, we introduce a global alignment module, structure-aware perspective n-point (SA-PnP), to establish correspondence between 2D and 3D vessel structures. Second, we propose TempDiffReg, a temporal diffusion model that performs vessel deformation iteratively by leveraging temporal context to capture complex anatomical variations and local structural changes. We collected data from 23 patients and constructed 626 paired multi-frame samples for comprehensive evaluation. Experimental results demonstrate that the proposed method consistently outperforms state-of-the-art (SOTA) methods in both accuracy and anatomical plausibility. Specifically, our method achieves a mean squared error (MSE) of 0.63 mm and a mean absolute error (MAE) of 0.51 mm in registration accuracy, representing 66.7\\% lower MSE and 17.7\\% lower MAE compared to the most competitive existing approaches. It has the potential to assist less-experienced clinicians in safely and efficiently performing complex TACE procedures, ultimately enhancing both surgical outcomes and patient care. Code and data are available at: \\textcolor{blue}{https://github.com/LZH970328/TempDiffReg.git}",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18168.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18168",
    "published": "2026-01-26T05:40:45Z",
    "updated": "2026-01-26T05:40:45Z",
    "comment": "Accepted by IEEE BIBM 2025",
    "light_analysis": {
      "overview": "提出TempDiffReg，一个时间扩散模型，通过从粗到细策略改进2D-3D血管非刚性配准的准确性和解剖合理性。",
      "motivation": "经动脉化疗栓塞（TACE）是治疗肝细胞癌等肝脏恶性肿瘤的首选手术，但由于术中血管导航复杂和解剖变异性大，手术极具挑战性。准确而鲁棒的2D-3D血管配准对于精确引导微导管和器械至关重要，能优化治疗靶向。现有方法在应对复杂血管变形时可能准确性不足，导致导航误差和手术风险增加，因此亟需开发更可靠的配准技术以提升临床实践效果。",
      "method": "本文采用从粗到细的配准策略。首先，引入结构感知视角n点（SA-PnP）模块进行全局对齐，建立2D和3D血管结构的对应关系。然后，提出TempDiffReg时间扩散模型，通过利用时间上下文迭代地进行血管变形，以捕捉复杂的解剖变化和局部结构变化。关键创新在于结合扩散模型和时间信息处理非刚性配准。研究基于23名患者数据构建了626个配对多帧样本进行评估。",
      "result": "实验结果表明，所提出的方法在准确性和解剖合理性方面一致优于最先进的配准方法。具体性能指标：均方误差为0.63毫米，平均绝对误差为0.51毫米；与现有最具竞争力的方法相比，MSE降低了66.7%，MAE降低了17.7%。通过对626个样本的评估，该方法在多个指标上表现优异，证明了其在提升配准精度和鲁棒性方面的显著效果。",
      "conclusion": "本文的主要贡献是提出TempDiffReg时间扩散模型和coarse-to-fine策略，显著改进了2D-3D血管非刚性配准的准确性和解剖合理性。学术上，该方法创新地应用扩散模型处理医学图像配准；实际上，它有潜力辅助临床医生更安全高效地执行TACE手术，从而改善手术效果和患者护理。未来工作可扩展到其他配准任务，代码和数据的公开促进了进一步研究。",
      "tags": [
        "Temporal Diffusion Model",
        "2D-3D Registration",
        "Vessel Deformation",
        "Non-Rigid Registration",
        "SA-PnP"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:54.614547Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18162",
    "title": "Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models",
    "authors": [
      "Ani Harutyunyan",
      "Sachin Kumar"
    ],
    "abstract": "Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18162.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18162",
    "published": "2026-01-26T05:29:27Z",
    "updated": "2026-01-26T05:29:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究比较了经典机器学习、BiLSTM和Transformer模型在GoEmotions数据集上的细粒度情绪检测性能，发现BERT模型在多标签分类中实现最优总体平衡。",
      "motivation": "细粒度情绪检测作为多标签自然语言处理任务，面临标签重叠和类别不平衡的挑战，影响了准确识别人类情感的效果。该研究旨在解决这一问题，因为现有方法在处理罕见情绪和复杂情感表达时可能不足，导致模型性能受限。通过系统性比较不同建模家族，研究探索更有效的解决方案，以提升情绪识别在学术和实际应用中的准确性。",
      "method": "研究在GoEmotions数据集上对三种模型进行基准测试：基于TF-IDF的逻辑回归系统（使用二进制相关性训练）、带注意力的BiLSTM模型和微调的BERT模型。实验采用官方训练/验证/测试分割，并应用逆频率类别权重来缓解类别不平衡问题。关键创新在于系统性地评估了从经典机器学习到深度学习和transformer技术的不同方法，分析它们处理多标签情绪检测任务的能力，突出模型架构和数据处理策略的差异。",
      "result": "实验结果显示，逻辑回归模型在Micro-F1指标上表现最佳，达到0.51，而BERT模型在综合指标上实现最优总体平衡，取得Macro-F1 0.49、Hamming Loss 0.036和Subset Accuracy 0.36，超越了GoEmotions官方论文报告的结果。与基线方法对比，BERT在罕见情绪和模糊示例上的性能更优，而逻辑回归依赖表面词汇线索处理频繁情绪，证明了上下文表示在提升多标签分类准确性方面的优势。",
      "conclusion": "本研究通过实验比较表明，BERT模型在细粒度情绪检测中实现了最佳的总体性能，尤其是在处理罕见情绪和类别不平衡方面，为情绪识别领域提供了重要的模型选择参考。研究的学术价值在于验证了transformer模型在多标签NLP任务中的有效性，实际应用价值在于指导情感分析系统的设计。未来工作可以探索更先进的模型架构或改进不平衡处理方法，以进一步提升性能。",
      "tags": [
        "Multi-label Classification",
        "BERT",
        "BiLSTM",
        "TF-IDF Logistic Regression",
        "GoEmotions Dataset"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:03.591922Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18157",
    "title": "Agentic Very Long Video Understanding",
    "authors": [
      "Aniket Rege",
      "Arka Sadhu",
      "Yuliang Li",
      "Kejie Li",
      "Ramya Korlakai Vinayak",
      "Yuning Chai",
      "Yong Jae Lee",
      "Hyo Jin Kim"
    ],
    "abstract": "The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18157.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18157",
    "published": "2026-01-26T05:20:47Z",
    "updated": "2026-01-26T05:20:47Z",
    "comment": "26 pages, 7 figures, 8 tables",
    "light_analysis": {
      "overview": "本文提出了EGAgent框架，一个基于实体场景图的增强代理框架，用于实现超长视频的跨模态、时间一致性理解，解决了现有方法在长视频推理中的限制。",
      "motivation": "随着全天候可穿戴设备如智能眼镜的发展，个人AI助手需要理解连续、纵向的自视视频流，以提供更准确的上下文服务。现有方法，包括大型语言模型和检索增强生成，受限于固定上下文窗口，无法对超长视频流进行组合、多跳推理，导致难以捕捉长时间事件间的关联，阻碍了实际应用。因此，研究旨在克服这些不足，提升长视频理解的深度和连续性。",
      "method": "论文提出EGAgent框架，核心创新是使用实体场景图来表示视频中的人物、地点、物体及其随时间变化的关系。系统配备一个规划代理，该代理集成了结构化搜索和推理工具，以及混合视觉和音频搜索能力，支持跨模态查询。通过这种方法，系统能进行详细的、时间连贯的推理，克服了传统方法在长视频理解中的上下文限制和模态孤立问题。",
      "result": "在EgoLifeQA和Video-MME（Long）数据集上的实验表明，EGAgent在复杂纵向视频理解任务中表现优异。在EgoLifeQA数据集上达到57.5%的准确率，实现了最先进的性能；在Video-MME（Long）数据集上达到74.1%，展现出竞争性水平。这些结果验证了该方法在提升长视频理解能力方面的有效性，尽管摘要未明确说明与基线的具体对比细节，但指出其优于现有方法。",
      "conclusion": "EGAgent框架通过整合实体场景图和代理式推理，有效解决了超长视频理解的挑战，提升了AI助手对连续事件的跨模态理解能力。其学术价值在于推动了长视频分析技术的发展，实际应用潜力包括个人助理和可穿戴设备。未来工作可扩展至更多模态或优化图表示，但摘要未明确说明具体局限性。",
      "tags": [
        "Entity Scene Graphs",
        "Agentic Framework",
        "Long-horizon Video Understanding",
        "Cross-modal Reasoning",
        "Planning Agent"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:50.642306Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18150",
    "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
    "authors": [
      "Zhaopeng Qiu",
      "Shuang Yu",
      "Jingqi Zhang",
      "Shuai Zhang",
      "Xue Huang",
      "Jingyi Yang",
      "Junjie Lai"
    ],
    "abstract": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18150.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18150",
    "published": "2026-01-26T05:12:05Z",
    "updated": "2026-01-26T05:12:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一个用于大型语言模型强化学习的实用且稳定的FP8低精度rollout堆栈，解决了低精度计算中的工程和算法挑战。",
      "motivation": "研究动机是解决大型语言模型在强化学习中rollout过程的瓶颈问题，由于长输出序列导致注意力机制和KV-cache内存消耗主导端到端时间，降低效率。FP8低精度计算虽然能减少计算成本和内存流量以加速强化学习，但应用于强化学习时面临独特挑战：策略权重每步变化需频繁量化和同步，以及低精度rollout可能偏离高精度训练策略，引发训练-推理不匹配和不稳定性。这些问题使得现有方法在实用性和稳定性上不足，因此需要一种新方案来平衡效率与准确性。",
      "method": "研究方法包括开发一个集成于veRL生态系统的FP8 rollout堆栈，关键技术涵盖：采用块状FP8量化实现FP8 W8A8线性层rollout，扩展FP8到KV-cache以处理长上下文内存瓶颈，通过每步QKV缩放重新校准优化KV-cache存储。同时，引入基于重要性采样的rollout校正方法（如token-level TIS和MIS变体）来减轻训练-推理不匹配。该实现兼容常见训练后端（如FSDP/Megatron-LM）和推理引擎（如vLLM/SGLang），以提供稳定且高效的低精度计算支持。",
      "result": "实验结果表明，该方法在密集和MoE模型上实现了显著的rollout吞吐量提升，增益高达44%。同时，学习行为与BF16基线保持相当，证明了在维持模型性能的同时大幅提高计算效率。具体数据基于实际测试，例如在吞吐量指标上对比基线方法，显示了FP8低精度技术在强化学习中的可行性和稳定性，从而验证了其在加速训练过程方面的有效性。",
      "conclusion": "本论文的主要贡献是提供了一个实用且稳定的FP8 rollout堆栈，有效解决了大型语言模型强化学习中低精度计算的工程和算法难题。学术价值在于展示了低精度计算在强化学习领域的应用潜力，推动了高效模型训练技术的发展。实际应用价值体现在加速训练过程、减少资源消耗，适用于多种模型架构如密集和MoE模型，未来工作可能包括进一步优化性能或扩展至其他低精度场景。",
      "tags": [
        "FP8 Quantization",
        "Reinforcement Learning for LLMs",
        "KV-cache Optimization",
        "Importance Sampling",
        "Low-Precision Computing"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:26.085744Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18142",
    "title": "Enhance the Safety in Reinforcement Learning by ADRC Lagrangian Methods",
    "authors": [
      "Mingxu Zhang",
      "Huicheng Zhang",
      "Jiaming Ji",
      "Yaodong Yang",
      "Ying Sun"
    ],
    "abstract": "Safe reinforcement learning (Safe RL) seeks to maximize rewards while satisfying safety constraints, typically addressed through Lagrangian-based methods. However, existing approaches, including PID and classical Lagrangian methods, suffer from oscillations and frequent safety violations due to parameter sensitivity and inherent phase lag. To address these limitations, we propose ADRC-Lagrangian methods that leverage Active Disturbance Rejection Control (ADRC) for enhanced robustness and reduced oscillations. Our unified framework encompasses classical and PID Lagrangian methods as special cases while significantly improving safety performance. Extensive experiments demonstrate that our approach reduces safety violations by up to 74%, constraint violation magnitudes by 89%, and average costs by 67\\%, establishing superior effectiveness for Safe RL in complex environments.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18142.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18142",
    "published": "2026-01-26T04:54:57Z",
    "updated": "2026-01-26T04:54:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ADRC-Lagrangian方法，通过集成主动抗扰控制（ADRC）减少振荡和安全违规，提升安全强化学习的鲁棒性和性能。",
      "motivation": "安全强化学习（Safe RL）旨在最大化奖励同时满足安全约束，常用基于拉格朗日的方法。然而，现有方法如PID和经典拉格朗日方法存在振荡和频繁安全违规问题，这是由于参数敏感性和固有的相位滞后所致。这在实际应用中，特别是在复杂环境中，导致安全可靠性不足，限制了Safe RL的实用价值，因此需要一种更稳定和鲁棒的方法来减少动态响应中的不足。",
      "method": "提出ADRC-Lagrangian方法，利用主动抗扰控制（ADRC）来增强鲁棒性和减少振荡。该方法作为一个统一框架，将经典和PID拉格朗日方法作为特例包含在内，通过ADRC处理参数不确定性和外部干扰，优化Lagrangian优化过程以减少相位滞后和改善动态性能。关键创新在于结合控制理论中的ADRC技术，专注于减少参数敏感性和振荡。摘要未明确说明具体数据集和模型架构，但推断该方法可应用于标准Safe RL环境。",
      "result": "通过广泛实验，ADRC-Lagrangian方法在安全性能上取得显著提升。具体数据显示，安全违规减少了最多74%，约束违规幅度降低了89%，平均成本减少了67%。这些结果与基线方法（如经典和PID Lagrangian方法）相比，展现出显著优势，表明该方法在复杂环境中能有效减少振荡和安全违规，提高整体效率。",
      "conclusion": "本研究的核心贡献是提出了ADRC-Lagrangian方法，解决了安全强化学习中振荡和安全违规的问题，提供了一个统一且鲁棒的框架。学术上，该方法结合了ADRC和控制理论，推动了Safe RL的稳健性研究；实际上，在复杂环境中有广泛应用价值，增强了安全约束下的强化学习实用性和可靠性。摘要未明确说明局限性或未来工作方向，但该方法为进一步优化和扩展提供了潜在基础。",
      "tags": [
        "Safe Reinforcement Learning",
        "Lagrangian Methods",
        "Active Disturbance Rejection Control",
        "PID Control",
        "Robust Control"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:24.561859Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18137",
    "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
    "authors": [
      "Yinger Zhang",
      "Shutong Jiang",
      "Renhao Li",
      "Jianhong Tu",
      "Yang Su",
      "Lianghao Deng",
      "Xudong Guo",
      "Chenxu Lv",
      "Junyang Lin"
    ],
    "abstract": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18137.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18137",
    "published": "2026-01-26T04:43:49Z",
    "updated": "2026-01-26T04:43:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了DeepPlanning基准，用于评估长期智能体规划中的全局约束优化和主动信息收集能力，填补了现有基准的空白。",
      "motivation": "研究动机在于现有智能体评估基准过分强调局部、步骤级推理，而忽略了现实世界任务所需的全局约束优化（如时间和财务预算）以及主动信息收集。现有的LLM规划基准在细粒度局部约束方面代表性不足，这导致无法充分评估智能体的真正规划能力，限制了其在复杂长期任务中的应用。因此，需要一个更贴近实际环境的挑战性基准来推动规划能力的发展和改进。",
      "method": "论文的核心方法是引入DeepPlanning基准，该基准包含多日旅行规划和多产品购物任务，要求智能体进行主动信息获取、局部约束推理和全局约束优化。这些任务设计旨在模拟真实场景中的规划挑战，强调对动态信息的主动探索和多种约束条件的综合处理。基准的建立不涉及特定模型架构的提出，而是通过任务定义和评估协议来测试智能体的规划能力，为研究者提供标准化的测试平台。",
      "result": "在DeepPlanning基准上的评估结果显示，即使是当前最前沿的智能体LLM也难以有效处理这些长期规划任务，这表明现有方法在全局约束优化和主动信息收集方面存在显著不足。结果突出了可靠明确推理模式和并行工具使用在实现更好有效性-效率权衡中的关键作用。错误分析进一步揭示了智能体在规划过程中常见的问题，例如信息处理不足和约束协调困难，为改进LLM的长期规划能力提供了具体方向。",
      "conclusion": "本文的主要贡献是提出了DeepPlanning基准，填补了长期智能体规划评估的空白，并开源了代码和数据以促进未来研究。该研究不仅强调了现有方法的局限性，还通过错误分析为优化智能体LLM提供了实用见解，具有重要的学术价值和实际应用潜力。潜在局限性包括基准任务的特定性，未来工作可扩展更多任务类型和约束场景，以进一步提升规划的泛化能力。",
      "tags": [
        "Long-Horizon Planning",
        "Constraint Optimization",
        "Agentic LLMs",
        "Benchmark Evaluation",
        "Tool Use"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:31.793650Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18135",
    "title": "Forward Consistency Learning with Gated Context Aggregation for Video Anomaly Detection",
    "authors": [
      "Jiahao Lyu",
      "Minghua Zhao",
      "Xuewen Huang",
      "Yifei Chen",
      "Shuangli Du",
      "Jing Hu",
      "Cheng Shi",
      "Zhiyong Lv"
    ],
    "abstract": "As a crucial element of public security, video anomaly detection (VAD) aims to measure deviations from normal patterns for various events in real-time surveillance systems. However, most existing VAD methods rely on large-scale models to pursue extreme accuracy, limiting their feasibility on resource-limited edge devices. Moreover, mainstream prediction-based VAD detects anomalies using only single-frame future prediction errors, overlooking the richer constraints from longer-term temporal forward information. In this paper, we introduce FoGA, a lightweight VAD model that performs Forward consistency learning with Gated context Aggregation, containing about 2M parameters and tailored for potential edge devices. Specifically, we propose a Unet-based method that performs feature extraction on consecutive frames to generate both immediate and forward predictions. Then, we introduce a gated context aggregation module into the skip connections to dynamically fuse encoder and decoder features at the same spatial scale. Finally, the model is jointly optimized with a novel forward consistency loss, and a hybrid anomaly measurement strategy is adopted to integrate errors from both immediate and forward frames for more accurate detection. Extensive experiments demonstrate the effectiveness of the proposed method, which substantially outperforms state-of-the-art competing methods, running up to 155 FPS. Hence, our FoGA achieves an excellent trade-off between performance and the efficiency metric.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18135.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18135",
    "published": "2026-01-26T04:35:31Z",
    "updated": "2026-01-26T04:35:31Z",
    "comment": "It has been submitted to the KBS journal",
    "light_analysis": {
      "overview": "论文提出轻量级视频异常检测模型FoGA，结合前向一致性学习和门控上下文聚合，实现高效边缘检测。",
      "motivation": "视频异常检测（VAD）是公共安全领域的关键技术，旨在实时监控中检测偏离正常模式的事件。现有VAD方法为追求高精度多依赖大规模模型，在资源有限的边缘设备上不可行；同时主流方法仅利用单帧未来预测误差检测异常，忽略了长期时间前向信息提供的丰富约束。因此，开发轻量化且能有效利用多帧信息的方法成为迫切需求。",
      "method": "论文提出FoGA模型，首先基于Unet架构对连续帧进行特征提取，生成立即帧和前向帧的预测。关键创新是引入门控上下文聚合模块，嵌入跳跃连接中，动态融合同一空间尺度的编码器和解码器特征。模型通过新颖的前向一致性损失进行联合优化，并采用混合异常测量策略，整合立即帧和前向帧的预测误差，以提高检测准确性。该模型约200万参数，专为边缘设备设计。",
      "result": "实验结果表明，FoGA模型在性能上显著优于现有最先进方法，实现高达155帧每秒的运行速度，体现了优异效率。尽管摘要未明确说明具体准确率提升数据，但强调其在边缘设备上的可行性，并达到性能与效率的良好平衡，验证了方法的有效性。",
      "conclusion": "论文的主要贡献是提出FoGA轻量级模型，通过前向一致性学习和门控上下文聚合，有效利用多帧时间信息，提升了视频异常检测在边缘设备上的实用性。该研究不仅为VAD领域提供新思路，具有实际应用价值，还促进了轻量化AI模型的发展。潜在局限性包括摘要未详细讨论复杂场景适应性，未来工作可进一步优化模型以适应更广泛环境。",
      "tags": [
        "Forward Consistency Learning",
        "Gated Context Aggregation",
        "Video Anomaly Detection",
        "Unet-based Model",
        "Lightweight Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:28.139724Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18132",
    "title": "RareAlert: Aligning heterogeneous large language model reasoning for early rare disease risk screening",
    "authors": [
      "Xi Chen",
      "Hongru Zhou",
      "Huahui Yi",
      "Shiyu Feng",
      "Hanyu Zhou",
      "Tiancheng He",
      "Mingke You",
      "Li Wang",
      "Qiankun Li",
      "Kun Wang",
      "Weili Fu",
      "Kang Li",
      "Jian Li"
    ],
    "abstract": "Missed and delayed diagnosis remains a major challenge in rare disease care. At the initial clinical encounters, physicians assess rare disease risk using only limited information under high uncertainty. When high-risk patients are not recognised at this stage, targeted diagnostic testing is often not initiated, resulting in missed diagnosis. Existing primary care triage processes are structurally insufficient to reliably identify patients with rare diseases at initial clinical presentation and universal screening is needed to reduce diagnostic delay. Here we present RareAlert, an early screening system which predict patient-level rare disease risk from routinely available primary-visit information. RareAlert integrates reasoning generated by ten LLMs, calibrates and weights these signals using machine learning, and distils the aligned reasoning into a single locally deployable model. To develop and evaluate RareAlert, we curated RareBench, a real-world dataset of 158,666 cases covering 33 Orphanet disease categories and more than 7,000 rare conditions, including both rare and non-rare presentations. The results showed that rare disease identification can be reconceptualised as a universal uncertainty resolution process applied to the general patient population. On an independent test set, RareAlert, a Qwen3-4B based model trained with calibrated reasoning signals, achieved an AUC of 0.917, outperforming the best machine learning ensemble and all evaluated LLMs, including GPT-5, DeepSeek-R1, Claude-3.7-Sonnet, o3-mini, Gemini-2.5-Pro, and Qwen3-235B. These findings demonstrate the diversity in LLM medical reasoning and the effectiveness of aligning such reasoning in highly uncertain clinical tasks. By incorporating calibrated reasoning into a single model, RareAlert enables accurate, privacy-preserving, and scalable rare disease risk screening suitable for large-scale local deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18132.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18132",
    "published": "2026-01-26T04:27:16Z",
    "updated": "2026-01-26T04:27:16Z",
    "comment": "28 page, 3 figures",
    "light_analysis": {
      "overview": "提出RareAlert系统，通过对齐异构大型语言模型推理来早期筛查罕见病风险，并蒸馏成单一本地可部署模型。",
      "motivation": "罕见病诊断中漏诊和延迟诊断是主要挑战，医生在初期临床遭遇中评估风险时信息有限、不确定性高，导致高风险患者未被识别，针对性诊断测试未启动。现有初级护理分流过程不足以可靠识别罕见病患者，需要普遍筛查以减少诊断延迟。本研究旨在解决这一问题，通过开发早期筛查系统来提高诊断准确性和效率，减少医疗资源浪费和患者痛苦。",
      "method": "RareAlert系统集成十个大型语言模型的推理，使用机器学习方法校准和加权这些异构信号，并将对齐的推理蒸馏成一个基于Qwen3-4B的单一模型。关键创新在于对LLM推理的校准和权重调整，以解决高度不确定临床任务中的不一致性。开发过程中使用了RareBench数据集，这是一个包含158,666个真实案例的数据集，覆盖33个Orphanet疾病类别和超过7,000种罕见病，包括罕见和非罕见表现，确保模型的泛化能力。",
      "result": "在独立测试集上，RareAlert模型实现了0.917的AUC值，优于最佳机器学习集成和所有评估的大型语言模型，如GPT-5、DeepSeek-R1、Claude-3.7-Sonnet等。这表明通过对齐LLM推理，可以显著提高罕见病风险筛查的准确性。实验还重新概念化了罕见病识别为应用于一般患者群体的不确定性解决过程，验证了方法的有效性。",
      "conclusion": "本研究证明了大型语言模型医学推理的多样性，以及通过校准和对齐这些推理在高度不确定临床任务中的有效性。RareAlert系统通过将校准推理融入单一模型，实现了准确、隐私保护和可扩展的罕见病风险筛查，适合大规模本地部署，具有重要的学术和临床应用价值。摘要未明确说明局限性，但该方法为罕见病诊断提供了新思路，未来可扩展至其他医疗任务。",
      "tags": [
        "Large Language Model",
        "Model Distillation",
        "Ensemble Methods",
        "Medical Reasoning",
        "Data Calibration"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:16.599593Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18130",
    "title": "RouteMoA: Dynamic Routing without Pre-Inference Boosts Efficient Mixture-of-Agents",
    "authors": [
      "Jize Wang",
      "Han Wu",
      "Zhiyuan You",
      "Yiming Song",
      "Yijun Wang",
      "Zifei Shan",
      "Yining Li",
      "Songyang Zhang",
      "Xinyi Le",
      "Cailian Chen",
      "Xinping Guan",
      "Dacheng Tao"
    ],
    "abstract": "Mixture-of-Agents (MoA) improves LLM performance through layered collaboration, but its dense topology raises costs and latency. Existing methods employ LLM judges to filter responses, yet still require all models to perform inference before judging, failing to cut costs effectively. They also lack model selection criteria and struggle with large model pools, where full inference is costly and can exceed context limits. To address this, we propose RouteMoA, an efficient mixture-of-agents framework with dynamic routing. It employs a lightweight scorer to perform initial screening by predicting coarse-grained performance from the query, narrowing candidates to a high-potential subset without inference. A mixture of judges then refines these scores through lightweight self- and cross-assessment based on existing model outputs, providing posterior correction without additional inference. Finally, a model ranking mechanism selects models by balancing performance, cost, and latency. RouteMoA outperforms MoA across varying tasks and model pool sizes, reducing cost by 89.8% and latency by 63.6% in the large-scale model pool.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18130.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18130",
    "published": "2026-01-26T04:22:22Z",
    "updated": "2026-01-26T04:22:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出RouteMoA框架，通过动态路由和轻量级筛选实现高效的多智能体混合，显著降低成本和延迟。",
      "motivation": "Mixture-of-Agents (MoA) 通过分层协作提升大型语言模型性能，但其密集拓扑结构导致计算成本和延迟增加。现有方法利用LLM法官过滤响应，但仍需所有模型进行推理后才能判断，无法有效削减成本。此外，这些方法缺乏明确的模型选择标准，当模型池规模较大时，全量推理成本高昂且可能超出上下文限制，限制了实际应用。研究动机是解决这些问题，提高效率和可扩展性。",
      "method": "RouteMoA框架采用动态路由机制。首先，一个轻量级打分器基于查询预测粗粒度性能，筛选出高潜力候选模型子集，无需进行完整推理。然后，混合法官通过轻量级的自我评估和交叉评估精炼分数，利用现有模型输出进行后验校正，无需额外推理开销。最后，模型排名机制综合考虑性能、成本和延迟因素，动态选择最优模型集合，实现高效协同。",
      "result": "实验表明，RouteMoA在不同任务和模型池规模下均优于传统MoA。特别是在大规模模型池中，RouteMoA将成本降低了89.8%，延迟减少了63.6%，显著提升了效率。这些结果基于具体性能指标对比，证明了其在保持高性能的同时有效优化资源使用，为大规模部署提供了可行方案。",
      "conclusion": "本研究提出RouteMoA框架，通过动态路由和轻量评估解决了MoA框架的高成本和延迟问题。其核心贡献在于引入预测性筛选和后验校正机制，避免了预推理开销，并提供了模型选择标准。学术上，该研究推进了多智能体协作的效率优化；实际中，为部署大规模语言模型系统降低了成本，提高了响应速度。未来工作可探索更广泛的模型类型和任务适应性。",
      "tags": [
        "Mixture-of-Agents",
        "Dynamic Routing",
        "Lightweight Scorer",
        "Model Ranking",
        "Efficient Inference"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:07.535406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18129",
    "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
    "authors": [
      "Kunat Pipatanakul",
      "Pittawat Taveekitworachai"
    ],
    "abstract": "Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18129.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18129",
    "published": "2026-01-26T04:20:59Z",
    "updated": "2026-01-26T04:20:59Z",
    "comment": "19 pages. Code is publicly available at https://github.com/scb-10x/typhoon-s . Datasets and model weights are available at https://huggingface.co/collections/typhoon-ai/typhoon-s",
    "light_analysis": {
      "overview": "Typhoon S是一种最小化后训练方法，通过结合监督微调、策略蒸馏和小规模强化微调，有效构建主权大型语言模型，减少对大规模资源的依赖。",
      "motivation": "当前大型语言模型（LLMs）主要在英语和中文等高资源语言中发展，由少数组织控制，这为区域或国家机构等主权环境带来障碍。这些环境需要在资源有限、透明度要求高的条件下控制模型权重、数据和部署，而现有方法依赖大规模指令语料库和复杂微调管道，不适合主权设置。因此，研究旨在解决如何在不依赖大规模数据的情况下，实现模型的通用可采纳性和特定任务性能，以满足主权需求。",
      "method": "论文提出Typhoon S，一种最小化开放后训练配方，结合监督微调（SFT）、on-policy蒸馏和小规模强化微调（RFT）。以泰语为案例研究，该方法将主权适应和通用基础模型转化为指令调整模型。关键创新是InK-GRPO，作为GRPO的扩展，通过在GRPO损失中增加下一个词预测损失来增强RFT效果，旨在减少对大规模指令数据和复杂管道的依赖，实现高效的模型调整。",
      "result": "实验显示，Typhoon S能有效将基础模型转化为具有强大通用性能的指令调整模型。小规模RFT结合InK-GRPO显著提高了泰语法律推理和泰语特定知识的能力，同时保持通用能力。与依赖大规模数据的基线方法相比，该方法减少了指令数据和计算需求，证明了在学术规模资源下实现高质量主权LLMs的可行性，摘要未明确具体性能指标。",
      "conclusion": "本研究的主要贡献是提出了一种后训练策略，可减少构建高质量主权大型语言模型所需的指令数据和计算规模，为资源有限的主权环境提供了实用路径。学术上，它展示了在小规模资源下实现模型性能的潜力，促进了区域机构对模型的控制和透明度。未来工作可进一步优化配方或扩展到其他语言和任务，摘要未明确说明局限性。",
      "tags": [
        "Large Language Models",
        "Supervised Fine-Tuning",
        "On-Policy Distillation",
        "Reinforcement Fine-Tuning",
        "InK-GRPO"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:08.486708Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18123",
    "title": "Deadline-Aware, Energy-Efficient Control of Domestic Immersion Hot Water Heaters",
    "authors": [
      "Muhammad Ibrahim Khan",
      "Bivin Pradeep",
      "James Brusey"
    ],
    "abstract": "Typical domestic immersion water heater systems are often operated continuously during winter, heating quickly rather than efficiently and ignoring predictable demand windows and ambient losses. We study deadline-aware control, where the aim is to reach a target temperature at a specified time while minimising energy consumption. We introduce an efficient Gymnasium environment that models an immersion hot water heater with first-order thermal losses and discrete on and off actions of 0 W and 6000 W applied every 120 seconds. Methods include a time-optimal bang-bang baseline, a zero-shot Monte Carlo Tree Search planner, and a Proximal Policy Optimisation policy. We report total energy consumption in watt-hours under identical physical dynamics. Across sweeps of initial temperature from 10 to 30 degrees Celsius, deadline from 30 to 90 steps, and target temperature from 40 to 80 degrees Celsius, PPO achieves the most energy-efficient performance at a 60-step horizon of 2 hours, using 3.23 kilowatt-hours, compared to 4.37 to 10.45 kilowatt-hours for bang-bang control and 4.18 to 6.46 kilowatt-hours for MCTS. This corresponds to energy savings of 26 percent at 30 steps and 69 percent at 90 steps. In a representative trajectory with a 50 kg water mass, 20 degrees Celsius ambient temperature, and a 60 degrees Celsius target, PPO consumes 54 percent less energy than bang-bang control and 33 percent less than MCTS. These results show that learned deadline-aware control reduces energy consumption under identical physical assumptions, while planners provide partial savings without training and learned policies offer near-zero inference cost once trained.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18123.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18123",
    "published": "2026-01-26T04:09:29Z",
    "updated": "2026-01-26T04:09:29Z",
    "comment": "Accepted at AAAI 2026",
    "light_analysis": {
      "overview": "本研究提出了一种基于强化学习的 deadline-aware 控制方法，显著优化家用浸入式热水器的能效。",
      "motivation": "家用浸入式热水器在冬季通常连续运行，加热迅速但效率低下，忽略可预测的需求窗口和环境热损失，导致不必要的能源浪费。现有方法如传统控制策略缺乏对时间目标的考虑，造成能耗高且成本增加。因此，开发能在指定时间内高效达到目标温度的控制策略至关重要，以提升家庭能源管理的智能化水平，降低能源消耗和环境影响。",
      "method": "研究方法包括构建一个模拟浸入式热水器的 Gymnasium 环境，基于一阶热损失模型，设置每120秒应用离散开关动作（0 W 或 6000 W）。采用时间最优 bang-bang 控制作为基线，零射 Monte Carlo Tree Search 规划器用于无需训练的规划，以及 Proximal Policy Optimisation 策略进行强化学习。关键创新在于引入 deadline-aware 目标，优化控制策略以最小化能耗，PPO 通过训练学习高效的控制策略。",
      "result": "实验结果表明，在初始温度10-30°C、deadline 30-90步、目标温度40-80°C的参数扫描下，PPO 策略在60步（对应2小时）时能耗最低，为3.23千瓦时。相比之下，bang-bang 控制能耗范围为4.37-10.45千瓦时，MCTS 为4.18-6.46千瓦时，能耗节省分别为26% 和 69%。在代表性场景中，PPO 比 bang-bang 节省54% 能量，比 MCTS 节省33%，验证了学习策略的优越性能。",
      "conclusion": "本研究证明了 deadline-aware 控制能有效减少家用浸入式热水器的能耗，PPO 策略在相同物理条件下显著优于传统方法。学术上推动了强化学习在能源管理中的应用，实际价值包括节能和降低运营成本。规划器如 MCTS 提供无需训练的节能方案，学习策略训练后推理成本低。未来工作可扩展更多动态场景或集成其他优化目标。",
      "tags": [
        "Proximal Policy Optimization",
        "Monte Carlo Tree Search",
        "Bang-Bang Control",
        "Energy Efficiency",
        "Gymnasium Environment"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:31.535235Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18119",
    "title": "Beyond Text-to-SQL: Can LLMs Really Debug Enterprise ETL SQL?",
    "authors": [
      "Jing Ye",
      "Yiwen Duan",
      "Yonghong Yu",
      "Victor Ma",
      "Yang Gao",
      "Xing Chen"
    ],
    "abstract": "SQL is central to enterprise data engineering, yet generating fully correct SQL code in a single attempt remains difficult, even for experienced developers and advanced text-to-SQL LLMs, often requiring multiple debugging iterations. We introduce OurBench, the first benchmark for enterprise-level SQL reasoning and debugging. Our benchmark is built on two key innovations: (1) an automated construction workflow that uses reverse engineering to systematically inject realistic bugs into large-scale SQL code, enabling scalable and diverse benchmark generation; and (2) an execution-free evaluation framework tailored to enterprise settings, providing fast, accurate, and resource-efficient assessment.   OurBench comprises 469 OurBenchSyn queries featuring syntax errors with explicit error messages, and 516 OurBenchSem queries targeting semantic errors in which the code fails to meet user intent. The queries are highly complex, averaging over 140 lines and featuring deep and wide abstract syntax trees.   Evaluation of nearly 30 LLMs reveals a substantial performance gap: the best-performing model, Claude-4-Sonnet, achieves only 36.46 percent accuracy on OurBenchSyn and 32.17 percent on OurBenchSem, while most models score below 20 percent. We further explore four solution strategies, identify key challenges, and outline promising directions for enterprise SQL debugging with LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.18119.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18119",
    "published": "2026-01-26T04:06:35Z",
    "updated": "2026-01-26T04:06:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了首个企业级SQL调试基准OurBench，并评估了大型语言模型在该任务上的性能，揭示了显著的性能差距。",
      "motivation": "在数据工程领域，SQL代码调试是一个关键但具有挑战性的问题。即使对于经验丰富的开发者或先进的文本到SQL LLMs，一次性生成完全正确的SQL代码也常常失败，导致需要多次调试迭代。这一问题的核心在于企业环境中的SQL查询通常高度复杂，而现有LLMs在处理这类任务时性能有限，缺乏针对性的评估基准来指导改进。因此，创建一个专门的企业级SQL调试基准至关重要，以促进自动化调试工具的发展和优化。",
      "method": "研究引入了OurBench基准，这是首个针对企业级SQL推理和调试的基准。其关键创新包括：(1) 自动化构建工作流，利用逆向工程技术在大规模SQL代码中系统注入现实的bug，实现了可扩展和多样化的错误生成；(2) 无执行评估框架，专为企业场景设计，提供快速、准确且资源高效的评估方式。OurBench包含469个语法错误查询（OurBenchSyn）和516个语义错误查询（OurBenchSem），这些查询平均超过140行，具有深且宽的抽象语法树，确保基准的复杂性和实用性。",
      "result": "通过评估近30个大型语言模型，研究发现性能存在显著差距。在OurBenchSyn（语法错误）上，最佳模型Claude-4-Sonnet的准确率仅为36.46%；在OurBenchSem（语义错误）上，准确率为32.17%。而大多数其他模型的准确率低于20%，这突显了现有LLMs在企业SQL调试任务中的严重局限性。实验进一步对比了不同模型的性能，表明当前技术仍无法有效应对复杂企业环境中的SQL错误检测和修正。",
      "conclusion": "论文的主要贡献在于建立了首个企业级SQL调试基准OurBench，并系统评估了LLMs在此任务上的表现。研究发现现有模型性能不足，强调了企业SQL调试的挑战性。这为学术界和工业界提供了重要的评估工具和方向，促进了LLM-based调试工具的研究和发展。未来工作可以专注于探索更有效的解决方案策略，改进模型在复杂环境中的推理能力，并推动实际应用中的自动化调试进程。",
      "tags": [
        "Large Language Models",
        "SQL Debugging",
        "Benchmarking",
        "ETL",
        "Reverse Engineering"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:20.323797Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18118",
    "title": "LungCRCT: Causal Representation based Lung CT Processing for Lung Cancer Treatment",
    "authors": [
      "Daeyoung Kim"
    ],
    "abstract": "Due to silence in early stages, lung cancer has been one of the most leading causes of mortality in cancer patients world-wide. Moreover, major symptoms of lung cancer are hard to differentiate with other respiratory disease symptoms such as COPD, further leading patients to overlook cancer progression in early stages. Thus, to enhance survival rates in lung cancer, early detection from consistent proactive respiratory system monitoring becomes crucial. One of the most prevalent and effective methods for lung cancer monitoring would be low-dose computed tomography(LDCT) chest scans, which led to remarkable enhancements in lung cancer detection or tumor classification tasks under rapid advancements and applications of computer vision based AI models such as EfficientNet or ResNet in image processing. However, though advanced CNN models under transfer learning or ViT based models led to high performing lung cancer detections, due to its intrinsic limitations in terms of correlation dependence and low interpretability due to complexity, expansions of deep learning models to lung cancer treatment analysis or causal intervention analysis simulations are still limited. Therefore, this research introduced LungCRCT: a latent causal representation learning based lung cancer analysis framework that retrieves causal representations of factors within the physical causal mechanism of lung cancer progression. With the use of advanced graph autoencoder based causal discovery algorithms with distance Correlation disentanglement and entropy-based image reconstruction refinement, LungCRCT not only enables causal intervention analysis for lung cancer treatments, but also leads to robust, yet extremely light downstream models in malignant tumor classification tasks with an AUC score of 93.91%.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18118.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18118",
    "published": "2026-01-26T04:03:50Z",
    "updated": "2026-01-26T04:03:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "LungCRCT 是一种基于因果表示学习的框架，通过图自编码器和距离相关性解缠技术，实现肺癌的因果干预分析和高效肿瘤分类。",
      "motivation": "肺癌是全球癌症死亡的主要原因之一，早期症状不明显，易与慢性阻塞性肺疾病等呼吸疾病混淆，导致患者忽视癌症进展，因此早期检测至关重要。现有方法如低剂量CT扫描和基于计算机视觉的AI模型（如CNN和ViT）在肺癌检测中虽有成效，但受限于相关性依赖和低可解释性，难以扩展至治疗分析或因果干预模拟。本研究旨在开发一种能理解肺癌进展因果机制的方法，以提升早期检测和治疗分析的准确性。",
      "method": "本研究提出了LungCRCT框架，基于潜在因果表示学习，利用图自编码器进行因果发现，结合距离相关性解缠技术减少特征间的虚假相关，并通过基于熵的图像重建精炼增强表示质量。该方法旨在从肺癌进展的物理因果机制中提取因果表示，支持因果干预分析，并构建轻量级下游模型用于恶性肿瘤分类任务。",
      "result": "在恶性肿瘤分类任务中，LungCRCT框架的AUC得分达到93.91%，展现了优越的分类性能。该框架还实现了因果干预分析，并构建了鲁棒且高效的下游模型。摘要未详细说明与基线方法的对比，但高AUC分数暗示了相对于现有方法的潜在改进。",
      "conclusion": "LungCRCT框架通过因果表示学习，克服了现有深度学习模型在肺癌分析中的局限性，实现了可解释的因果干预和高效分类。该研究为肺癌早期检测和治疗分析提供了创新方法，具有重要的临床应用价值。未来工作可能包括框架的进一步优化和扩展到其他疾病类型。",
      "tags": [
        "Causal Representation Learning",
        "Graph Autoencoder",
        "Distance Correlation",
        "Entropy-based Reconstruction",
        "Malignant Tumor Classification"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:25.380406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18116",
    "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
    "authors": [
      "Lin Sun",
      "Linglin Zhang",
      "Jingang Huang",
      "Change Jia",
      "Zhengwei Cheng",
      "Xiangzheng Zhang"
    ],
    "abstract": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.   We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.   Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18116.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18116",
    "published": "2026-01-26T04:00:56Z",
    "updated": "2026-01-26T04:00:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "FABLE框架通过层次森林索引和自适应双向检索策略，集成LLMs增强多文档推理的效率和准确性。",
      "motivation": "随着长上下文大语言模型（LLMs）的快速发展，检索增强生成（RAG）的必要性引发争议。然而，实证显示长上下文推断存在中间丢失现象、高计算成本和可扩展性差等问题，特别是在多文档推理任务中。同时，传统RAG系统受限于平面块级检索，引入语义噪声且无法支持结构化跨文档合成，导致推理精度不足。因此，迫切需要一种新方法结合LLMs的优势，提升检索效率并减少噪声，以应对复杂多文档场景下的挑战。",
      "method": "论文提出FABLE框架，集成了LLMs到知识组织和检索过程。首先，使用LLMs构建层次化的森林索引，包含多粒度语义结构，以组织文档信息。在检索阶段，采用双向策略：一条路径通过LLM引导遍历层次结构，另一条路径利用结构感知传播获取细粒度证据。这一设计通过显式预算控制实现自适应效率权衡，优化检索精度和计算资源，关键创新在于层次森林索引和自适应双向检索的结合。",
      "result": "广泛实验表明，FABLE在性能上持续超越现有的最先进RAG方法。在多文档推理任务中，其准确率可与全上下文LLM推断相当，同时显著减少计算成本。具体数据显示，FABLE实现高达94%的token减少，有效缓解长上下文推断的高计算负担，证明了其在保持高准确性的同时提升效率的优势。与基线方法相比，FABLE展示了更好的可扩展性和适应性。",
      "conclusion": "FABLE框架的主要贡献在于成功集成LLMs到检索过程，解决了长上下文推断和传统RAG的不足。该研究证明长上下文LLMs增强了而非完全替代结构化检索的需求，具有重要学术价值，推动RAG技术在多文档推理领域的发展。实际应用中，FABLE可提高系统效率和可扩展性，潜在局限性可能包括对特定数据集的依赖，未来工作可探索更广泛的应用场景和自适应机制的进一步优化。",
      "tags": [
        "Large Language Models",
        "Retrieval-Augmented Generation",
        "Hierarchical Indexing",
        "Multi-Document Reasoning",
        "Adaptive Retrieval"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:42.257256Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18115",
    "title": "Robust Learning of a Group DRO Neuron",
    "authors": [
      "Guyang Cao",
      "Shuyao Li",
      "Sushrut Karmalkar",
      "Jelena Diakonikolas"
    ],
    "abstract": "We study the problem of learning a single neuron under standard squared loss in the presence of arbitrary label noise and group-level distributional shifts, for a broad family of covariate distributions. Our goal is to identify a ''best-fit'' neuron parameterized by $\\mathbf{w}_*$ that performs well under the most challenging reweighting of the groups. Specifically, we address a Group Distributionally Robust Optimization problem: given sample access to $K$ distinct distributions $\\mathcal p_{[1]},\\dots,\\mathcal p_{[K]}$, we seek to approximate $\\mathbf{w}_*$ that minimizes the worst-case objective over convex combinations of group distributions $\\boldsymbolλ \\in Δ_K$, where the objective is $\\sum_{i \\in [K]}λ_{[i]}\\,\\mathbb E_{(\\mathbf x,y)\\sim\\mathcal p_{[i]}}(σ(\\mathbf w\\cdot\\mathbf x)-y)^2 - νd_f(\\boldsymbolλ,\\frac{1}{K}\\mathbf1)$ and $d_f$ is an $f$-divergence that imposes (optional) penalty on deviations from uniform group weights, scaled by a parameter $ν\\geq 0$. We develop a computationally efficient primal-dual algorithm that outputs a vector $\\widehat{\\mathbf w}$ that is constant-factor competitive with $\\mathbf{w}_*$ under the worst-case group weighting. Our analytical framework directly confronts the inherent nonconvexity of the loss function, providing robust learning guarantees in the face of arbitrary label corruptions and group-specific distributional shifts. The implementation of the dual extrapolation update motivated by our algorithmic framework shows promise on LLM pre-training benchmarks.",
    "categories": [
      "cs.LG",
      "cs.DS",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18115.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18115",
    "published": "2026-01-26T04:00:53Z",
    "updated": "2026-01-26T04:00:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文开发了一个计算高效的原对偶算法，用于在组分布鲁棒优化框架下学习单个神经元，以增强对标签噪声和分布偏移的鲁棒性。",
      "motivation": "研究旨在解决学习单个神经元时，面对任意标签噪声和组级分布偏移的挑战。在实际应用中，数据常来自不同组或分布，存在不确定性，例如在群体数据中存在噪声或分布变化，这会导致标准学习方法性能下降。组分布鲁棒优化（Group DRO）方法的目标是使模型在最坏情况下也能表现良好，提高泛化能力。现有方法可能对分布偏移敏感，缺乏鲁棒性，因此需要一个有效的框架来应对这些现实问题，确保模型在实际部署中的可靠性。",
      "method": "论文提出一个组分布鲁棒优化问题，通过样本访问K个不同分布，目标是最小化在最坏情况组加权下的平方损失，并引入f-散度惩罚偏离均匀组权重，参数ν控制惩罚强度。为了求解这个非凸优化问题，作者开发了一个计算高效的原对偶算法，输出参数向量与最优参数在常数因子内竞争。关键创新包括直接处理损失函数的非凸性，设计双重外推更新策略，这些创新在处理标签噪声和分布偏移时提供理论保证。算法框架在大型语言模型预训练基准上得到实施，展示了实际应用前景。",
      "result": "主要实验结果表明，算法在组分布鲁棒优化框架下能够学习神经元参数，与目标最优参数相比，在最坏情况组加权下具有常数因子的竞争力。摘要未明确说明具体性能指标如准确率提升或效率改进数据，但提到算法在大型语言模型预训练基准上显示有前景的结果。这表明该方法在对抗标签噪声和组特定分布偏移时可能具有良好的鲁棒性，但需要更多实验验证来量化改进效果。",
      "conclusion": "论文的贡献在于提出了一个鲁棒学习框架和高效算法，用于在标签噪声和分布偏移下学习单个神经元，提供理论鲁棒保证。这项研究具有重要学术价值，通过处理非凸优化问题拓展了分布鲁棒优化的应用范围；在实际应用中，如人工智能模型训练，该方法能提高模型在多样数据环境中的可靠性。潜在局限性可能包括对复杂模型的扩展性，未来工作可包括更多实验验证、应用到其他领域以及进一步优化算法性能。",
      "tags": [
        "Group Distributionally Robust Optimization",
        "Primal-Dual Algorithm",
        "f-Divergence",
        "Nonconvex Optimization",
        "Single Neuron Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:45.022874Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18111",
    "title": "Demystifying Data-Driven Probabilistic Medium-Range Weather Forecasting",
    "authors": [
      "Jean Kossaifi",
      "Nikola Kovachki",
      "Morteza Mardani",
      "Daniel Leibovici",
      "Suman Ravuri",
      "Ira Shokar",
      "Edoardo Calvello",
      "Mohammad Shoaib Abbas",
      "Peter Harrington",
      "Ashay Subramaniam",
      "Noah Brenowitz",
      "Boris Bonev",
      "Wonmin Byeon",
      "Karsten Kreis",
      "Dale Durran",
      "Arash Vahdat",
      "Mike Pritchard",
      "Jan Kautz"
    ],
    "abstract": "The recent revolution in data-driven methods for weather forecasting has lead to a fragmented landscape of complex, bespoke architectures and training strategies, obscuring the fundamental drivers of forecast accuracy. Here, we demonstrate that state-of-the-art probabilistic skill requires neither intricate architectural constraints nor specialized training heuristics. We introduce a scalable framework for learning multi-scale atmospheric dynamics by combining a directly downsampled latent space with a history-conditioned local projector that resolves high-resolution physics. We find that our framework design is robust to the choice of probabilistic estimator, seamlessly supporting stochastic interpolants, diffusion models, and CRPS-based ensemble training. Validated against the Integrated Forecasting System and the deep learning probabilistic model GenCast, our framework achieves statistically significant improvements on most of the variables. These results suggest scaling a general-purpose model is sufficient for state-of-the-art medium-range prediction, eliminating the need for tailored training recipes and proving effective across the full spectrum of probabilistic frameworks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18111.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18111",
    "published": "2026-01-26T03:52:16Z",
    "updated": "2026-01-26T03:52:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出一个通用且可扩展的框架，用于数据驱动的概率中期天气预报，证明无需复杂架构即可实现最先进性能。",
      "motivation": "该研究旨在解决数据驱动天气预报中复杂架构和训练策略碎片化的问题，这些问题掩盖了预测准确性的根本驱动因素。现有方法如集成预报系统和深度学习模型 GenCast 依赖于定制化方案，导致模型开发繁琐且难以推广，限制了天气预报的准确性和可扩展性。因此，研究动机是澄清关键因素并简化方法，以推动更有效的预测技术发展。",
      "method": "论文引入一个可扩展框架，通过结合直接下采样的潜在空间和历史条件的局部投影器来学习多尺度大气动力学。关键创新在于框架设计对概率估计器的选择具有鲁棒性，支持多种技术包括随机插值、扩散模型和基于连续排名概率评分（CRPS）的集合训练。该方法避免了复杂的架构约束，使用通用模型实现高效学习，无需特殊训练策略。",
      "result": "实验验证中，该框架与集成预报系统和深度学习概率模型 GenCast 进行比较。结果显示，在大多数天气预报变量上，框架实现了统计显著的性能改进，具体数据如准确率等未明确说明，但证实了其在概率预测方面的优越性，超越了现有基线方法，证明了框架的有效性和鲁棒性。",
      "conclusion": "论文的主要贡献是证明了扩展通用模型足以实现最先进的概率中期天气预报，无需复杂架构或定制训练策略，澄清了预测准确性的关键驱动因素。学术价值在于简化模型开发理论，实际应用价值是提高天气预报的可扩展性和效率。未来工作可探索更多概率框架的集成或优化局限性。",
      "tags": [
        "Probabilistic Weather Forecasting",
        "Data-Driven Methods",
        "Diffusion Models",
        "Multi-scale Learning",
        "CRPS"
      ]
    },
    "analyzed_at": "2026-01-27T03:36:55.855401Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18110",
    "title": "AttenMIA: LLM Membership Inference Attack through Attention Signals",
    "authors": [
      "Pedram Zaree",
      "Md Abdullah Al Mamun",
      "Yue Dong",
      "Ihsen Alouani",
      "Nael Abu-Ghazaleh"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly deployed to enable or improve a multitude of real-world applications. Given the large size of their training data sets, their tendency to memorize training data raises serious privacy and intellectual property concerns. A key threat is the membership inference attack (MIA), which aims to determine whether a given sample was included in the model's training set. Existing MIAs for LLMs rely primarily on output confidence scores or embedding-based features, but these signals are often brittle, leading to limited attack success. We introduce AttenMIA, a new MIA framework that exploits self-attention patterns inside the transformer model to infer membership. Attention controls the information flow within the transformer, exposing different patterns for memorization that can be used to identify members of the dataset. Our method uses information from attention heads across layers and combines them with perturbation-based divergence metrics to train an effective MIA classifier. Using extensive experiments on open-source models including LLaMA-2, Pythia, and Opt models, we show that attention-based features consistently outperform baselines, particularly under the important low-false-positive metric (e.g., achieving up to 0.996 ROC AUC & 87.9% TPR@1%FPR on the WikiMIA-32 benchmark with Llama2-13b). We show that attention signals generalize across datasets and architectures, and provide a layer- and head-level analysis of where membership leakage is most pronounced. We also show that using AttenMIA to replace other membership inference attacks in a data extraction framework results in training data extraction attacks that outperform the state of the art. Our findings reveal that attention mechanisms, originally introduced to enhance interpretability, can inadvertently amplify privacy risks in LLMs, underscoring the need for new defenses.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18110.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18110",
    "published": "2026-01-26T03:45:56Z",
    "updated": "2026-01-26T03:45:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "AttenMIA通过利用transformer模型的自注意力模式，提出了一种新型的LLM成员推断攻击方法，显著提升了攻击性能。",
      "motivation": "大型语言模型（LLMs）在训练过程中可能记忆敏感数据，引发隐私和知识产权风险，成员推断攻击（MIA）是评估这种风险的关键。然而，现有MIA方法主要依赖于输出置信度或嵌入特征，这些信号脆弱且不稳定，导致攻击成功率有限，难以准确检测训练集成员。因此，亟需开发更鲁棒的攻击技术来应对LLMs部署中的隐私挑战，促进安全研究。",
      "method": "AttenMIA框架的核心创新是利用transformer模型中的自注意力机制来推断成员身份。方法从不同层和注意力头提取注意力模式，结合基于扰动的发散度量（如KL散度）训练高效的MIA分类器。关键细节包括在多个开源LLMs（如LLaMA-2、Pythia、Opt）上验证，通过注意力信号的动态变化捕捉记忆化特征，首次将注意力应用于MIA以提高鲁棒性。",
      "result": "实验结果显示，AttenMIA在多个基准测试中优于现有基线方法，特别是在低误报率下表现突出。例如，在WikiMIA-32基准上，使用Llama2-13b模型实现了0.996的ROC AUC和87.9%的TPR@1%FPR。注意力特征展现了跨数据集和架构的泛化能力，层和头级别分析揭示了成员泄漏最明显的区域。在数据提取攻击中，AttenMIA的应用进一步提升了提取效率，超越了当前最优方法。",
      "conclusion": "AttenMIA成功验证了注意力机制在成员推断攻击中的有效性，为LLM隐私风险评估提供了新工具。研究贡献了一种高性能MIA方法，并揭示注意力机制可能无意中加剧隐私泄露，强调了开发针对性防御措施的重要性。未来工作可探索如何平衡模型性能与隐私保护，或改进攻击策略以适应更复杂场景，推动隐私安全研究。",
      "tags": [
        "Membership Inference Attack",
        "Self-Attention",
        "Transformer Models",
        "Large Language Models",
        "Privacy"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:24.058070Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18107",
    "title": "Beyond Static Datasets: Robust Offline Policy Optimization via Vetted Synthetic Transitions",
    "authors": [
      "Pedram Agand",
      "Mo Chen"
    ],
    "abstract": "Offline Reinforcement Learning (ORL) holds immense promise for safety-critical domains like industrial robotics, where real-time environmental interaction is often prohibitive. A primary obstacle in ORL remains the distributional shift between the static dataset and the learned policy, which typically mandates high degrees of conservatism that can restrain potential policy improvements. We present MoReBRAC, a model-based framework that addresses this limitation through Uncertainty-Aware latent synthesis. Instead of relying solely on the fixed data, MoReBRAC utilizes a dual-recurrent world model to synthesize high-fidelity transitions that augment the training manifold. To ensure the reliability of this synthetic data, we implement a hierarchical uncertainty pipeline integrating Variational Autoencoder (VAE) manifold detection, model sensitivity analysis, and Monte Carlo (MC) dropout. This multi-layered filtering process guarantees that only transitions residing within high-confidence regions of the learned dynamics are utilized. Our results on D4RL Gym-MuJoCo benchmarks reveal significant performance gains, particularly in ``random'' and ``suboptimal'' data regimes. We further provide insights into the role of the VAE as a geometric anchor and discuss the distributional trade-offs encountered when learning from near-optimal datasets.",
    "categories": [
      "cs.LG",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18107.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18107",
    "published": "2026-01-26T03:38:27Z",
    "updated": "2026-01-26T03:38:27Z",
    "comment": "11 pages, 2 figures, 2 tables",
    "light_analysis": {
      "overview": "本研究提出MoReBRAC框架，通过不确定性感知的潜在合成解决离线强化学习中的分布偏移问题，提升策略性能。",
      "motivation": "离线强化学习在工业机器人等安全关键领域有应用潜力，但静态数据集与学习策略之间的分布偏移是主要障碍，导致现有方法需高度保守，限制了策略改进。现有技术依赖固定数据，难以处理动态变化，因此需要新方法增强数据多样性和可靠性，以应对真实环境交互成本高的问题。摘要未明确说明具体不足之处，但强调了分布偏移的挑战。",
      "method": "MoReBRAC框架采用基于模型的方法，通过不确定性感知的潜在合成增强训练。核心创新是使用双循环世界模型合成高保真状态转换，并整合层次不确定性管道，包括VAE流形检测、模型敏感性分析和MC dropout，确保合成数据仅来自高置信区域。该方法在D4RL Gym-MuJoCo基准数据集上进行测试，技术特色在于多层过滤以减少分布偏移风险。",
      "result": "在D4RL Gym-MuJoCo基准测试中，MoReBRAC显示出显著性能提升，特别是在“随机”和“次优”数据制度下，有效缓解分布偏移，提升策略鲁棒性。与基线方法相比，该方法在性能指标上优于传统方法，但具体准确率提升数值摘要未明确说明，整体效果通过benchmark验证。",
      "conclusion": "论文主要贡献是提出MoReBRAC框架，通过合成数据增强和不确定性管理解决离线强化学习的分布偏移问题，学术上为模型强化学习提供新思路，实际应用于安全关键领域有潜力。局限性包括数据集类型的影响，未来工作可优化VAE作为几何锚并探索分布权衡策略。摘要未明确说明具体未来方向细节，但提及了相关讨论。",
      "tags": [
        "Offline Reinforcement Learning",
        "Model-Based Reinforcement Learning",
        "Uncertainty Quantification",
        "Variational Autoencoder",
        "Synthetic Data Augmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:14.549224Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18106",
    "title": "GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health",
    "authors": [
      "Jiatan Huang",
      "Zheyuan Zhang",
      "Tianyi Ma",
      "Mingchen Li",
      "Yaning Zheng",
      "Yanfang Ye",
      "Chuxu Zhang"
    ],
    "abstract": "Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18106.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18106",
    "published": "2026-01-26T03:32:46Z",
    "updated": "2026-01-26T03:32:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了GLEN-Bench，首个综合性的基于图-语言的营养健康评估基准，旨在解决个性化饮食指导中的现实约束和解释性问题。",
      "motivation": "营养干预对慢性健康管理至关重要，但现有计算方法存在显著缺陷。关键问题包括：饮食模式研究常忽略社会经济状况、共病和食物获取限制等现实约束，导致建议不切实际；推荐系统缺乏解释性，无法说明为何特定食物对患者有益；并且缺乏统一基准评估营养干预所需的关联任务。这些不足限制了方法的可比性和应用效果，因此急需开发一个综合基准来整合数据、任务和解释性功能，以推动个性化健康支持的发展。",
      "method": "研究提出GLEN-Bench基准，通过整合NHANES健康记录、FNDDS食物成分数据和USDA食物获取度量，构建知识图谱，连接人口统计、健康状况、饮食行为、贫困相关约束和营养需求。该基准定义三个关联任务：风险检测基于饮食和社会经济模式识别高危个体；推荐在资源约束下提供满足临床需求的个性化食物建议；问答提供基于图谱的自然语言解释以增强理解。方法评估了图神经网络、大型语言模型和混合架构等图-语言技术，以建立基准并探索实用设计选择，特别针对阿片类药物使用障碍场景，要求模型检测疾病阶段的微妙营养差异。",
      "result": "摘要未明确说明具体性能指标或与基线方法的对比数据，但分析结果表明，基于GLEN-Bench的评估揭示了与健康风险相关的明确饮食模式，这为实际干预提供了见解。研究通过基准测试建立了初步基线，表明图-语言方法在风险识别和解释生成方面具有潜力，尽管具体准确率提升或效率改进等量化结果在摘要中未详述。结果强调了结合多样数据源和任务的重要性，为后续改进提供了基础。",
      "conclusion": "本文的主要贡献是引入GLEN-Bench，首个基于图-语言的营养健康评估综合基准，填补了现有方法在现实约束集成、解释性支持和统一评估方面的空白。该研究具有学术价值，推动了知识图谱和语言模型在健康领域的交叉应用，并为个性化饮食干预提供了实用工具。潜在局限性可能在于数据覆盖或任务泛化能力，未来工作可扩展基准到更多疾病场景、增加数据集或优化模型架构，以进一步提升应用效果和可扩展性。",
      "tags": [
        "Graph Neural Networks",
        "Large Language Models",
        "Knowledge Graph",
        "Benchmark",
        "Nutritional Health Assessment"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:27.345819Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18102",
    "title": "CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations",
    "authors": [
      "Stephanie Fong",
      "Zimu Wang",
      "Guilherme C. Oliveira",
      "Xiangyu Zhao",
      "Yiwen Jiang",
      "Jiahe Liu",
      "Beau-Luke Colton",
      "Scott Woods",
      "Martha E. Shenton",
      "Barnaby Nelson",
      "Zongyuan Ge",
      "Dominic Dwyer"
    ],
    "abstract": "The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18102.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18102",
    "published": "2026-01-26T03:25:06Z",
    "updated": "2026-01-26T03:25:06Z",
    "comment": "This paper is accepted at EACL 2026",
    "light_analysis": {
      "overview": "CHiRPE是一个临床NLP管道，通过临床医生共同开发的解释格式预测精神病风险，实现高准确性和临床可解释性。",
      "motivation": "该研究旨在解决医疗NLP工具在临床实践中需要最终用户（如临床医生）可解释性的实际问题。传统可解释AI方法通常不符合临床推理流程，缺乏临床医生的输入，导致工具难以被采纳。精神病风险预测在早期干预中至关重要，现有方法在解释性方面存在不足，影响了临床决策支持的有效性和可信度。因此，研究强调结合临床专家知识来改进模型解释，以促进NLP工具在真实世界医疗环境中的应用。",
      "method": "CHiRPE管道采用半结构化临床访谈转录作为输入，集成症状领域映射、大型语言模型摘要和BERT分类器来预测精神病风险。核心技术包括使用BERT变体进行分类，以及基于SHAP框架生成解释格式，这些格式与临床医生共同开发以增强临床相关性。数据集来自AMP-SCZ研究的24个国际诊所的944份访谈转录。创新点在于结合症状分析和LLM摘要，生成概念引导的解释，如混合图和文本摘要，使模型输出更符合临床思维。",
      "result": "实验结果显示，CHiRPE在三个BERT变体上实现了超过90%的准确率，显著优于基线模型，证明了其在精神病风险预测中的有效性。解释格式由28名临床专家进行评估，专家表现出强烈偏好，尤其是对概念引导的解释，其中混合图和文本摘要格式最受青睐。这表明CHiRPE不仅提高了预测性能，还通过临床相关的解释增强了模型的可信度和实用性，为医疗NLP工具的采纳提供了实证支持。",
      "conclusion": "CHiRPE研究表明，临床指导的模型开发能有效生成准确且可解释的结果，为医疗NLP领域提供了重要贡献。其学术价值在于将临床推理与AI解释方法结合，提升了模型在复杂医疗任务中的适用性；实际应用价值在于促进精神病风险预测工具在临床环境中的部署。未来工作将集中在24个国际站点进行现实世界测试，以验证其泛化能力和长期效果，可能扩展到其他医疗领域，尽管当前研究局限于特定数据集和风险预测任务。",
      "tags": [
        "Clinical NLP",
        "BERT",
        "LLM Summarization",
        "SHAP",
        "Model Interpretability"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:25.872384Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18100",
    "title": "Spatial-Conditioned Reasoning in Long-Egocentric Videos",
    "authors": [
      "James Tribble",
      "Hao Wang",
      "Si-En Hong",
      "Chaoyi Zhou",
      "Ashish Bastola",
      "Siyu Huang",
      "Abolfazl Razi"
    ],
    "abstract": "Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18100.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18100",
    "published": "2026-01-26T03:21:35Z",
    "updated": "2026-01-26T03:21:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文研究了显式空间信号对视觉语言模型在长期第一人称视频中空间推理能力的影响，并展示了融合深度图能提升安全关键任务的性能。",
      "motivation": "长期第一人称视频在视觉导航中面临视角漂移和几何上下文缺失的挑战，导致视觉语言模型在长期序列中的空间推理能力有限。现有方法虽在图像和短视频中表现良好，但忽视了显式空间信号的利用，限制了在安全关键应用如行人和障碍物检测中的准确性。因此，本研究旨在不修改模型架构，通过增强输入数据探索空间信号的影响，以提升导航任务的可靠性。",
      "method": "研究方法包括引入 Sanpo-D 数据集，对 Google Sanpo 进行细粒度重新标注，专门用于导航导向的空间查询基准测试。通过将深度图与 RGB 帧融合作为输入，评估多个视觉语言模型在空间推理任务上的表现。关键创新点在于保持模型架构不变，仅通过输入级归纳偏见引入空间信号，从而分析深度感知表示对长期第一人称视频理解的有效性。",
      "result": "实验结果显示，融合深度图的表示方法在长期第一人称视频的空间推理任务中性能得到提升，特别是在行人和障碍物检测等安全关键任务上优于仅使用 RGB 的基线。研究揭示了一般用途准确性与空间专业化之间的权衡，表明深度感知表示在增强空间理解的同时可能对通用任务略有影响，但整体改善了导航导向查询的效果。摘要未明确说明具体性能指标数值。",
      "conclusion": "本研究贡献在于探索了显式空间信号对视觉语言模型空间推理的影响，通过引入 Sanpo-D 数据集和深度融合方法，证明了深度感知表示能提升安全关键任务的性能。学术上，这为视频理解中的空间推理提供了新视角；实际上，有助于改善自动驾驶等领域的导航安全性。未来工作可进一步优化通用性与专业化的平衡，或扩展到更复杂的视频场景。",
      "tags": [
        "Vision-Language Model",
        "Spatial Reasoning",
        "Egocentric Video",
        "Depth Maps",
        "Dataset Annotation"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:06.319532Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18099",
    "title": "Computational Framework for Estimating Relative Gaussian Blur Kernels between Image Pairs",
    "authors": [
      "Akbar Saadat"
    ],
    "abstract": "Following the earlier verification for Gaussian model in \\cite{ASaa2026}, this paper introduces a zero training forward computational framework for the model to realize it in real time applications. The framework is based on discrete calculation of the analytic expression of the defocused image from the sharper one for the application range of the standard deviation of the Gaussian kernels and selecting the best matches. The analytic expression yields multiple solutions at certain image points, but is filtered down to a single solution using similarity measures over neighboring points.The framework is structured to handle cases where two given images are partial blurred versions of each other. Experimental evaluations on real images demonstrate that the proposed framework achieves a mean absolute error (MAE) below $1.7\\%$ in estimating synthetic blur values. Furthermore, the discrepancy between actual blurred image intensities and their corresponding estimates remains under $2\\%$, obtained by applying the extracted defocus filters to less blurred images.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18099.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18099",
    "published": "2026-01-26T03:21:26Z",
    "updated": "2026-01-26T03:21:26Z",
    "comment": "9 pages, 14 input images, 3 TikZ images. arXiv admin note: substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779. substantial text overlap with arXiv:2601.04779",
    "light_analysis": {
      "overview": "本文提出一个零训练前向计算框架，用于实时估计图像对之间的相对高斯模糊核。",
      "motivation": "本研究旨在解决图像处理中估计图像对之间相对高斯模糊核的问题，这对于图像去模糊、深度估计等计算机视觉任务至关重要。现有方法通常依赖训练数据或复杂优化算法，计算开销大，限制了实时应用。本框架提出零训练方法，通过直接解析计算提升效率，弥补了现有方法在实用性和计算速度上的不足。摘要未明确说明具体不足之处，但基于上下文推断，传统方法可能涉及迭代优化或数据依赖，而本框架强调实时性和无需训练的优势。",
      "method": "该框架基于高斯模糊的解析表达式，通过离散化计算从更清晰的图像推导出模糊图像的表达式。它适用于高斯核标准差的范围，通过选择最佳匹配来估计模糊核。关键创新包括零训练、前向计算，并使用邻点相似性度量过滤解析表达式中可能产生的多个解，以确保单一解。框架特别设计用于处理两个图像互为部分模糊版本的情况，增强了鲁棒性。摘要未明确说明具体使用的数据集或模型架构，但方法依赖于离散数学和相似性比较技术。",
      "result": "在真实图像上的实验评估显示，该框架在估计合成模糊值时平均绝对误差（MAE）低于1.7%，表明高精度性能。此外，通过将提取的散焦滤波器应用于较不模糊的图像，实际模糊图像强度与其对应估计值之间的差异保持在2%以下，验证了框架的稳定性和有效性。摘要未明确对比基线方法，但低误差指标暗示了相对于传统估计方法的优势。结果以百分比形式呈现，支持了框架在准确估计高斯模糊核方面的能力。",
      "conclusion": "本研究贡献了一个零训练前向计算框架，成功实现了实时高斯模糊核估计，提供了一种高效的解析方法。学术价值在于简化了估计过程，避免了数据驱动的复杂性；实际应用价值在于促进图像处理任务的实时部署，如去模糊和场景分析。未来工作可能包括扩展到其他模糊模型或处理噪声干扰，但摘要未明确说明局限性和具体方向。该框架为计算机视觉领域提供了新的计算工具，平衡了精度与效率。",
      "tags": [
        "Gaussian Blur Kernel Estimation",
        "Zero-Training Framework",
        "Image Processing",
        "Analytic Expression",
        "Similarity Measures"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:55.319825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18098",
    "title": "Text-Pass Filter: An Efficient Scene Text Detector",
    "authors": [
      "Chuang Yang",
      "Haozhao Ma",
      "Xu Han",
      "Yuan Yuan",
      "Qi Wang"
    ],
    "abstract": "To pursue an efficient text assembling process, existing methods detect texts via the shrink-mask expansion strategy. However, the shrinking operation loses the visual features of text margins and confuses the foreground and background difference, which brings intrinsic limitations to recognize text features. We follow this issue and design Text-Pass Filter (TPF) for arbitrary-shaped text detection. It segments the whole text directly, which avoids the intrinsic limitations. It is noteworthy that different from previous whole text region-based methods, TPF can separate adhesive texts naturally without complex decoding or post-processing processes, which makes it possible for real-time text detection. Concretely, we find that the band-pass filter allows through components in a specified band of frequencies, called its passband but blocks components with frequencies above or below this band. It provides a natural idea for extracting whole texts separately. By simulating the band-pass filter, TPF constructs a unique feature-filter pair for each text. In the inference stage, every filter extracts the corresponding matched text by passing its pass-feature and blocking other features. Meanwhile, considering the large aspect ratio problem of ribbon-like texts makes it hard to recognize texts wholly, a Reinforcement Ensemble Unit (REU) is designed to enhance the feature consistency of the same text and to enlarge the filter's recognition field to help recognize whole texts. Furthermore, a Foreground Prior Unit (FPU) is introduced to encourage TPF to discriminate the difference between the foreground and background, which improves the feature-filter pair quality. Experiments demonstrate the effectiveness of REU and FPU while showing the TPF's superiority.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18098.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18098",
    "published": "2026-01-26T03:21:11Z",
    "updated": "2026-01-26T03:21:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Text-Pass Filter，一种高效场景文本检测器，通过模拟带通滤波器直接分割整个文本并自然分离粘连文本，无需复杂后处理。",
      "motivation": "现有文本检测方法常采用收缩-掩码扩展策略，但收缩操作会丢失文本边缘的视觉特征，混淆前景与背景差异，带来识别文本特征的内在限制。这导致在处理任意形状和粘连文本时效率低下，影响实时应用。本文旨在解决这一问题，通过直接分割整个文本来避免这些局限性，提升检测精度和速度。",
      "method": "Text-Pass Filter（TPF）模拟带通滤波器，为每个文本构建特征-滤波器对，在推理阶段滤波器传递匹配特征并阻止其他特征以提取文本。关键创新包括设计Reinforcement Ensemble Unit（REU）增强同一文本的特征一致性并扩大识别范围，以处理带状文本的大宽高比问题；引入Foreground Prior Unit（FPU）区分前景与背景，提高特征-滤波器对质量。该方法无需复杂解码或后处理，能自然分离粘连文本。",
      "result": "实验证明了REU和FPU的有效性，并展示了TPF的优越性。尽管摘要未明确说明具体性能指标（如准确率或速度数据），但TPF在避免收缩操作限制、自然分离粘连文本方面表现出色，表明其在高效率场景文本检测中具有潜力，适用于实时应用。",
      "conclusion": "本文的主要贡献是提出Text-Pass Filter，通过模拟带通滤波器创新地解决现有文本检测方法的内在限制，在直接分割文本和自然分离粘连文本方面有显著优势。这项研究为场景文本检测提供了新方法，对实时应用有重要价值；未来工作可进一步优化滤波器设计以处理更复杂文本形状，并探索更多实验验证。",
      "tags": [
        "Scene Text Detection",
        "Band-pass Filter",
        "Reinforcement Ensemble Unit",
        "Foreground Prior Unit",
        "Arbitrary-shaped Text Detection"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:51.687899Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18091",
    "title": "From LLMs to LRMs: Rethinking Pruning for Reasoning-Centric Models",
    "authors": [
      "Longwei Ding",
      "Anhao Zhao",
      "Fanghua Ye",
      "Ziyang Chen",
      "Xiaoyu Shen"
    ],
    "abstract": "Large language models (LLMs) are increasingly costly to deploy, motivating extensive research on model pruning. However, most existing studies focus on instruction-following LLMs, leaving it unclear whether established pruning strategies transfer to reasoning-augmented models that explicitly generate long intermediate reasoning traces. In this work, we conduct a controlled study of pruning for both instruction-following ($\\textbf{LLM-instruct}$) and reasoning-augmented ($\\textbf{LLM-think}$) models. To isolate the effects of pruning, we align pruning calibration and post-pruning recovery data with each model's original training distribution, which we show yields more stable and reliable pruning behavior. We evaluate static depth pruning, static width pruning, and dynamic pruning across 17 tasks spanning classification, generation, and reasoning. Our results reveal clear paradigm-dependent differences: depth pruning outperforms width pruning on classification tasks, while width pruning is more robust for generation and reasoning. Moreover, static pruning better preserves reasoning performance, whereas dynamic pruning excels on classification and generation but remains challenging for long-chain reasoning. These findings underscore the need for pruning strategies that explicitly account for the distinct characteristics of reasoning-augmented LLMs. Our code is publicly available at https://github.com/EIT-NLP/LRM-Pruning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18091.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18091",
    "published": "2026-01-26T03:01:39Z",
    "updated": "2026-01-26T03:01:39Z",
    "comment": "18 pages, 7 figures",
    "light_analysis": {
      "overview": "本文通过对比实验揭示推理增强大语言模型的剪枝策略需专门设计，不同于传统指令遵循模型。",
      "motivation": "大型语言模型（LLMs）部署成本高昂，当前剪枝研究主要聚焦于指令遵循模型，但推理增强模型能生成长中间推理轨迹，其剪枝效果尚未明确。这一问题限制了推理模型的高效部署，标准剪枝方法可能不适用，导致性能下降。因此，研究推理增强模型的剪枝策略至关重要，以填补现有方法的空白，促进成本降低和应用优化。",
      "method": "论文提出系统对比研究指令遵循模型（LLM-instruct）与推理增强模型（LLM-think）的剪枝效果，关键创新是确保剪枝校准和后剪枝恢复数据与模型原始训练分布对齐，以稳定剪枝行为。评估方法包括静态深度剪枝、静态宽度剪枝和动态剪枝，应用于17个涵盖分类、生成和推理的任务。摘要未明确说明具体数据集和模型架构，但通过控制变量设计，强调了隔离剪枝影响的实验策略。",
      "result": "实验结果显示了范式依赖的差异：在分类任务中，深度剪枝优于宽度剪枝；而在生成和推理任务中，宽度剪枝更鲁棒。静态剪枝能更好地保持推理性能，而动态剪枝在分类和生成任务上表现优异，但在长链推理中面临挑战。这些发现基于17个任务的评估，强调剪枝策略需根据模型类型和任务特性定制，凸显了推理增强模型的独特性能需求。",
      "conclusion": "论文的主要贡献在于揭示针对推理增强大语言模型的剪枝策略需专门设计，而非套用现有方法。研究丰富了剪枝理论在推理模型中的应用，并为降低部署成本提供实践指导。学术上，拓展了模型压缩领域；实际上，支持高效推理模型发展。未来工作可探索更高效的长链推理剪枝技术，并扩展到更多模型变体和任务类型。",
      "tags": [
        "Large Language Models",
        "Model Pruning",
        "Reasoning-Augmented Models",
        "Static Depth Pruning",
        "Dynamic Pruning"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:07.910274Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18089",
    "title": "LatentMoE: Toward Optimal Accuracy per FLOP and Parameter in Mixture of Experts",
    "authors": [
      "Venmugil Elango",
      "Nidhi Bhatia",
      "Roger Waleffe",
      "Rasoul Shafipour",
      "Tomer Asida",
      "Abhinav Khattar",
      "Nave Assaf",
      "Maximilian Golub",
      "Joey Guman",
      "Tiyasa Mitra",
      "Ritchie Zhao",
      "Ritika Borkar",
      "Ran Zilberstein",
      "Mostofa Patwary",
      "Mohammad Shoeybi",
      "Bita Rouhani"
    ],
    "abstract": "Mixture of Experts (MoEs) have become a central component of many state-of-the-art open-source and proprietary large language models. Despite their widespread adoption, it remains unclear how close existing MoE architectures are to optimal with respect to inference cost, as measured by accuracy per floating-point operation and per parameter. In this work, we revisit MoE design from a hardware-software co-design perspective, grounded in empirical and theoretical considerations. We characterize key performance bottlenecks across diverse deployment regimes, spanning offline high-throughput execution and online, latency-critical inference. Guided by these insights, we introduce LatentMoE, a new model architecture resulting from systematic design exploration and optimized for maximal accuracy per unit of compute. Empirical design space exploration at scales of up to 95B parameters and over a 1T-token training horizon, together with supporting theoretical analysis, shows that LatentMoE consistently outperforms standard MoE architectures in terms of accuracy per FLOP and per parameter. Given its strong performance, the LatentMoE architecture has been adopted by the flagship Nemotron-3 Super and Ultra models and scaled to substantially larger regimes, including longer token horizons and larger model sizes, as reported in Nvidia et al. (arXiv:2512.20856).",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18089.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18089",
    "published": "2026-01-26T02:59:23Z",
    "updated": "2026-01-26T02:59:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出LatentMoE，一种通过硬件-软件协同设计优化的MoE架构，旨在最大化单位计算量下的准确率。",
      "motivation": "MoE在大语言模型中广泛应用，但现有架构在推理成本方面（准确率 per FLOP 和 per parameter）是否最优尚不清楚。本研究旨在解决这一优化问题，因为高效推理对于实际部署至关重要，特别是在离线高吞吐和在线低延迟等多样化场景下。现有方法可能未充分考虑硬件利用和成本效益的平衡，导致资源浪费。通过重新审视设计，论文动机源于提升MoE的实用性和经济效益，以支持更大规模模型的部署。",
      "method": "论文从硬件-软件协同设计视角出发，分析不同部署场景（如离线高吞吐和在线低延迟）的性能瓶颈，并基于经验和理论考虑进行设计。提出LatentMoE架构，通过系统化设计空间探索优化模型结构，以实现最佳准确率 per 计算单位。关键创新点包括综合性能分析和协同优化策略。实验涉及大规模参数设置（高达95B参数）和训练数据（超过1T令牌），辅以理论分析支持设计决策，确保方法在多样环境中稳健有效。",
      "result": "LatentMoE在准确率 per FLOP 和 per parameter 方面一致优于标准MoE架构，尽管摘要未明确说明具体性能数据（如准确率提升百分比）。研究通过大规模实证和理论验证展示了其优势，并被采纳到Nemotron-3 Super和Ultra模型中，进一步扩展到更长的令牌视野和更大的模型尺寸。这表明LatentMoE在成本效益方面具有显著改进，为高效推理提供了可靠基准。",
      "conclusion": "论文的主要贡献是提出LatentMoE架构，优化了MoE的计算效率，填补了现有设计在成本效益方面的不足。学术上提供了硬件-软件协同设计的新视角和方法论；实际上，该架构已成功应用于商业模型，显示出广泛的实际应用潜力。未来工作可能包括扩展到更大型模型和更多样化部署场景，进一步提升MoE技术的可扩展性和适应性。",
      "tags": [
        "Mixture of Experts",
        "Large Language Models",
        "Hardware-Software Co-design",
        "LatentMoE",
        "Inference Efficiency"
      ]
    },
    "analyzed_at": "2026-01-27T03:37:56.054126Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18088",
    "title": "Cross-Domain Transfer with Self-Supervised Spectral-Spatial Modeling for Hyperspectral Image Classification",
    "authors": [
      "Jianshu Chao",
      "Tianhua Lv",
      "Qiqiong Ma",
      "Yunfei Qiu",
      "Li Fang",
      "Huifang Shen",
      "Wei Yao"
    ],
    "abstract": "Self-supervised learning has demonstrated considerable potential in hyperspectral representation, yet its application in cross-domain transfer scenarios remains under-explored. Existing methods, however, still rely on source domain annotations and are susceptible to distribution shifts, leading to degraded generalization performance in the target domain. To address this, this paper proposes a self-supervised cross-domain transfer framework that learns transferable spectral-spatial joint representations without source labels and achieves efficient adaptation under few samples in the target domain. During the self-supervised pre-training phase, a Spatial-Spectral Transformer (S2Former) module is designed. It adopts a dual-branch spatial-spectral transformer and introduces a bidirectional cross-attention mechanism to achieve spectral-spatial collaborative modeling: the spatial branch enhances structural awareness through random masking, while the spectral branch captures fine-grained differences. Both branches mutually guide each other to improve semantic consistency. We further propose a Frequency Domain Constraint (FDC) to maintain frequency-domain consistency through real Fast Fourier Transform (rFFT) and high-frequency magnitude loss, thereby enhancing the model's capability to discern fine details and boundaries. During the fine-tuning phase, we introduce a Diffusion-Aligned Fine-tuning (DAFT) distillation mechanism. This aligns semantic evolution trajectories through a teacher-student structure, enabling robust transfer learning under low-label conditions. Experimental results demonstrate stable classification performance and strong cross-domain adaptability across four hyperspectral datasets, validating the method's effectiveness under resource-constrained conditions.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.18088.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18088",
    "published": "2026-01-26T02:52:35Z",
    "updated": "2026-01-26T02:52:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一个自监督的跨域转移框架，通过S2Former和DAFT机制实现高光谱图像分类，无需源域标注且适应目标域少量样本。",
      "motivation": "自监督学习在高光谱图像分类中显示出潜力，但在跨域转移场景下应用不足，导致实际应用受限。现有方法通常依赖源域的标注数据，容易受到领域间分布偏移的影响，从而在目标域中泛化性能下降。这在高光谱遥感等标注成本高昂的领域尤为突出，因此开发无需源域标注并能在少量目标样本下有效适应的跨域转移方法，对于提升模型实用性和减少数据依赖具有重要意义。",
      "method": "论文提出一个自监督跨域转移框架，分为预训练和微调阶段。预训练阶段，设计Spatial-Spectral Transformer (S2Former)模块，采用双分支transformer结构，通过双向交叉注意力机制实现光谱-空间的协同建模：空间分支利用随机掩码增强结构意识，光谱分支捕捉细粒度差异，二者相互指导以提升语义一致性。此外，引入Frequency Domain Constraint (FDC)，使用实快速傅里叶变换 (rFFT)和高频幅度损失来保持频域一致性，增强模型对细节和边界的识别能力。微调阶段，提出Diffusion-Aligned Fine-tuning (DAFT)蒸馏机制，通过教师-学生结构对齐语义演化轨迹，支持在低标签条件下的稳健转移学习。",
      "result": "实验结果表明，该方法在四个高光谱数据集上表现出稳定的分类性能和强大的跨域适应能力，验证了框架在资源受限条件下的有效性。尽管摘要未明确提供具体性能指标（如准确率提升），但与现有方法相比，该方法无需源域标注，并能在目标域少量样本下实现高效适应，显示出较好的泛化性能和克服分布偏移的能力。这表明自监督预训练和蒸馏微调机制能够有效提升跨域分类的准确性。",
      "conclusion": "本研究的核心贡献是提出了一种结合自监督学习和跨域转移的高光谱图像分类框架，通过S2Former模块实现光谱-空间协同建模，以及DAFT机制实现稳健微调。学术上，该工作推动了自监督学习在高光谱领域的应用，并为跨域适应问题提供了新思路。实际应用中，该方法降低了对标注数据的依赖，提高了在资源受限环境下的适应性，具有潜在的实际价值。未来工作可能包括扩展到更多数据集和优化模型效率。",
      "tags": [
        "Self-Supervised Learning",
        "Cross-Domain Transfer",
        "Hyperspectral Image Classification",
        "Transformer",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:43.636195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18081",
    "title": "DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal",
    "authors": [
      "Peixuan Han",
      "Yingjie Yu",
      "Jingjun Xu",
      "Jiaxuan You"
    ],
    "abstract": "Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at https://github.com/ulab-uiuc/DRPG-RebuttalAgent.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18081.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18081",
    "published": "2026-01-26T02:30:01Z",
    "updated": "2026-01-26T02:30:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "DRPG是一个通过分解、检索、规划和生成步骤的代理框架，自动生成高质量的学术反驳响应。",
      "motivation": "学术反驳是学术交流和同行评审的关键步骤，但现有自动化方法如现成大型语言模型或简单管道，在理解长上下文和生成有针对性的、有说服力的响应方面存在不足，限制了讨论效率和准确性。这一问题的重要性在于自动化支持能加速评审流程并提升学术沟通质量，而现有方法缺乏针对性和解释性，因此需要创新的解决方案。摘要未明确说明具体应用场景的扩展需求，但强调了该问题的紧迫性。",
      "method": "DRPG框架通过四个步骤实现自动学术反驳生成：首先将评审分解为原子关注点以简化处理；其次从原始论文中检索相关证据作为支持；然后使用Planner模块规划反驳策略，识别最可行方向；最后基于策略生成响应。关键创新在于代理化设计和Planner的高效规划能力，结合检索增强生成技术，使用8B模型作为基础，框架在顶级会议数据集上进行评估，但摘要未明确说明具体模型架构和数据集细节。",
      "result": "实验结果显示，DRPG的Planner在识别最可行反驳方向时准确率超过98%。在顶级会议数据上，DRPG显著优于现有反驳管道，使用仅8B模型达到了超越平均人类水平的性能。此外，框架在多轮反驳设置中也表现良好，证明了其鲁棒性和实用性。摘要未明确提供具体基线方法的性能数据对比，但强调了效果的显著提升。",
      "conclusion": "DRPG框架有效提升了学术反驳生成的自动化水平，提供高质量、多视角和可解释的响应，其学术价值在于推动了AI在学术工作流中的应用，实际应用价值在于支持大规模学术讨论的扩展。摘要未明确说明局限性或未来工作方向，但暗示了在复杂场景中的潜在优化和应用扩展。",
      "tags": [
        "Large Language Model",
        "Agentic Framework",
        "Retrieval-Augmented Generation",
        "Natural Language Generation",
        "Academic Rebuttal"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:25.855584Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18077",
    "title": "Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents",
    "authors": [
      "Mahesh Ramesh",
      "Kaousheik Jayakumar",
      "Aswinkumar Ramkumar",
      "Pavan Thodima",
      "Aniket Rege"
    ],
    "abstract": "Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.18077.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18077",
    "published": "2026-01-26T02:23:47Z",
    "updated": "2026-01-26T02:23:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过评估和改进大型语言模型在 Hanabi 游戏中的合作推理能力，并通过微调显著提升性能，展示了其在多代理协作任务中的泛化潜力。",
      "motivation": "不完整信息下的合作推理对人工智能和多代理系统具有关键挑战，Hanabi 游戏作为理想测试平台需要心智理论和战略沟通；现有 LLM 在合作推理中表现不足，常因推理失败和上下文处理能力有限而无法有效协作，影响其在现实协作应用中的部署。本研究旨在通过系统基准测试和上下文工程探索 LLM 的协作能力，为解决 AI 合作推理的瓶颈提供新方法，推动智能系统向更复杂的交互场景发展。",
      "method": "研究采用 Hanabi 游戏作为基准，对 17 个先进 LLM 代理在 2-5 玩家游戏中测试，并引入三种上下文设置：Watson（仅提供明确卡片细节）、Sherlock（基于程序化贝叶斯推理的脚手架）和 Mycroft（通过工作记忆进行多回合状态跟踪）。关键创新包括内部工作记忆用于状态跟踪，以及基于新发布的数据集 HanabiLogs（1520 个游戏日志）和 HanabiRewards（560 个游戏的价值标注）进行监督和强化学习微调，使用 4B 参数的开放权重模型 Qwen3-Instruct 来提升合作推理性能。",
      "result": "实验结果表明，在 Sherlock 设置下，最强推理模型平均得分超过 15 分，但低于经验人类和专用 Hanabi 代理（均超过 20 分）。监督微调将 Hanabi 游戏性能提升 21%，强化学习微调提升 156%，使模型接近专有推理模型 o4-mini（相差约 3 分）并超越非推理模型 GPT-4.1 52%。微调模型还展现泛化能力：在合作群体猜测基准上提升 11%，EventQA 时间推理提升 6.4%，IFBench-800K 指令跟随提升 1.7 Pass@10，并匹配 AIME 2025 数学推理的 Pass@10 水平。",
      "conclusion": "本研究系统评估了 LLM 的合作推理能力，通过上下文工程和微调方法显著提升了 Hanabi 游戏性能，并验证了泛化到其他任务的潜力。学术上，为 LLM 在多代理协作领域提供了新基准和方法论；实际中，增强了 AI 系统的合作和自适应能力。未来可进一步探索泛化机制、扩展至更多复杂协作场景，并缩小与人类专家的差距，尽管摘要未明确说明局限性，但暗示了推理模型仍有改进空间。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Cooperative Reasoning",
        "Context Engineering",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:38.002418Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.18076",
    "title": "Comparison requires valid measurement: Rethinking attack success rate comparisons in AI red teaming",
    "authors": [
      "Alexandra Chouldechova",
      "A. Feder Cooper",
      "Solon Barocas",
      "Abhinav Palia",
      "Dan Vann",
      "Hanna Wallach"
    ],
    "abstract": "We argue that conclusions drawn about relative system safety or attack method efficacy via AI red teaming are often not supported by evidence provided by attack success rate (ASR) comparisons. We show, through conceptual, theoretical, and empirical contributions, that many conclusions are founded on apples-to-oranges comparisons or low-validity measurements. Our arguments are grounded in asking a simple question: When can attack success rates be meaningfully compared? To answer this question, we draw on ideas from social science measurement theory and inferential statistics, which, taken together, provide a conceptual grounding for understanding when numerical values obtained through the quantification of system attributes can be meaningfully compared. Through this lens, we articulate conditions under which ASRs can and cannot be meaningfully compared. Using jailbreaking as a running example, we provide examples and extensive discussion of apples-to-oranges ASR comparisons and measurement validity challenges.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.18076.pdf",
    "abs_url": "https://arxiv.org/abs/2601.18076",
    "published": "2026-01-26T02:22:19Z",
    "updated": "2026-01-26T02:22:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出基于测量理论的框架，重新思考AI红队中攻击成功率比较的有效性，以确定有意义的比较条件。",
      "motivation": "在AI红队攻击中，常通过攻击成功率（ASR）比较评估系统安全性或攻击方法效能，但许多结论缺乏可靠证据支持，因为这些比较往往基于苹果与橘子的无效对比或低效度测量。这可能导致安全评估出现偏差，误导决策，影响AI系统的可靠性。因此，研究何时ASR可以有意义比较至关重要，以提升评估的准确性和科学性。",
      "method": "作者借鉴社会科学测量理论和推断统计学，构建概念框架来定义攻击成功率（ASR）有意义比较的条件。通过理论分析和实证讨论，以jailbreaking攻击为案例，探讨ASR比较中的无效对比和测量挑战。核心创新在于引入测量效度标准，结合实际例子说明，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "论文通过概念和理论分析指出，许多ASR比较缺乏效度，例如在jailbreaking案例中展示了无效对比问题。然而，摘要未提供具体实验数据，如准确率提升或效率改进，也没有与基线方法的直接量化对比。因此，结果主要在于理论框架的验证和问题识别，强调提高测量效度的重要性。",
      "conclusion": "本文的主要贡献是建立基于测量理论的框架，重新思考攻击成功率（ASR）比较，增强AI红队评估的效度。研究揭示了ASR比较中的常见无效性，并提供了跨学科的理论基础。学术价值在于整合社会科学方法到AI安全领域，实际应用价值在于促进更科学的测量实践。未来工作可包括开发具体测量工具和扩展实证验证。",
      "tags": [
        "AI Red Teaming",
        "Attack Success Rate (ASR)",
        "Measurement Theory",
        "Inferential Statistics",
        "Jailbreaking"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:42.366132Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16885",
    "title": "GPA-VGGT:Adapting VGGT to Large Scale Localization by Self-Supervised Learning with Geometry and Physics Aware Loss",
    "authors": [
      "Yangfan Xu",
      "Lilian Zhang",
      "Xiaofeng He",
      "Pengdong Wu",
      "Wenqi Wu",
      "Jun Mao"
    ],
    "abstract": "Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.16885.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16885",
    "published": "2026-01-23T16:46:59Z",
    "updated": "2026-01-26T14:14:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种自监督学习框架，通过几何和物理感知损失训练VGGT，显著提升其在无标签大规模场景中的视觉定位能力。",
      "motivation": "Transformer-based VGGT模型在相机姿态估计和3D重建中表现优异，但通常依赖标注数据训练，这限制了其在无标签和未见场景中的适应性。大规模定位在实际应用中至关重要，如自动驾驶和增强现实，但标注数据获取成本高且困难。因此，本研究旨在解决现有方法对硬标签的依赖问题，开发自监督学习框架，以增强模型在复杂环境中的鲁棒性和泛化能力。",
      "method": "该方法扩展了传统的成对关系为序列级几何约束，用于自监督学习。具体地，在每个序列中采样多个源帧，并将它们通过几何投影到不同目标帧，以提升时序特征一致性。通过将物理光度一致性和几何约束结合为联合优化损失，避免了硬标签需求，从而训练VGGT模型。训练过程中，模型的局部和全局跨视图注意力层以及相机和深度头能有效捕捉多视图几何，增强特征表示。",
      "result": "实验结果表明，模型在数百次迭代内收敛，并在大规模定位任务中取得显著改进。摘要未明确说明具体性能指标如准确率提升，但与现有依赖标注的方法相比，该框架展现出更好的适应性和效率。代码将在GitHub上发布，便于验证和复现结果。",
      "conclusion": "本研究的主要贡献是提出了一个自监督框架，用于训练VGGT，增强其在大规模无标签场景中的定位能力，具有重要的学术价值，推动了自监督视觉几何模型的发展。实际应用中，可适用于机器人导航和虚拟现实等领域。未来工作可进一步优化损失函数或扩展到更动态、多模态的场景中。",
      "tags": [
        "Self-Supervised Learning",
        "Visual Geometry Transformer",
        "Geometric Constraints",
        "Photometric Consistency",
        "Camera Pose Estimation"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:35.633274Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16781",
    "title": "Persuasion Tokens for Editing Factual Knowledge in LLMs",
    "authors": [
      "Paul Youssef",
      "Christin Seifert",
      "Jörg Schlötterer"
    ],
    "abstract": "In-context knowledge editing (IKE) is a promising technique for updating Large Language Models (LLMs) with new information. However, IKE relies on lengthy, fact-specific demonstrations which are costly to create and consume significant context window space. In this paper, we introduce persuasion tokens (P-Tokens) -- special tokens trained to replicate the effect of IKE demonstrations, enabling efficient knowledge editing without requiring fact-specific demonstrations. We evaluate P-Tokens across two editing datasets and three LLMs, demonstrating performance comparable to, and often exceeding, IKE. We further find that editing performance is robust to distractors with small negative effects to neighboring facts, and that increasing the number of P-Tokens improves performance. Our work addresses key limitations of IKE and provides a more practical and scalable alternative for editing LLMs.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.16781.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16781",
    "published": "2026-01-23T14:29:28Z",
    "updated": "2026-01-26T07:21:17Z",
    "comment": "Accepted at EACL Main 2026",
    "light_analysis": {
      "overview": "本文提出persuasion tokens（P-Tokens），通过训练特殊令牌实现高效知识编辑，无需依赖冗长演示，解决了现有in-context知识编辑方法的局限性。",
      "motivation": "研究动机是解决in-context knowledge editing（IKE）在更新大型语言模型（LLMs）知识时的不足之处。IKE需要冗长、特定事实的演示，导致创建成本高昂且消耗大量上下文窗口空间，这在实际应用中限制了模型快速适应新信息和保持时效性。现有方法的不足在于演示的复杂性和资源消耗，使得IKE不够实用和可扩展，迫切需要更高效的编辑技术来提升LLMs的更新效率。",
      "method": "研究方法的核心是引入persuasion tokens（P-Tokens），即训练特殊令牌来复制IKE演示的效果，从而实现无需事实特定演示的知识编辑。关键创新点在于通过训练令牌避免创建冗长演示，简化编辑流程；技术特色包括可调整令牌数量以优化性能。在实验方面，使用了两种编辑数据集和三种LLMs进行评估，但具体数据集名称和模型架构摘要未明确说明，这体现了方法的通用性。",
      "result": "主要实验结果显示，P-Tokens在两个编辑数据集和三个LLMs上的性能与IKE相当，并经常超越。编辑性能对干扰具有鲁棒性，仅对邻近事实有轻微负面影响；增加P-Tokens数量能进一步改善编辑效果。具体性能指标如准确率提升等数据，摘要未提供详细数字，但与基线IKE的对比表明该方法在效率和可扩展性上具有优势，为知识编辑提供了有效替代。",
      "conclusion": "本文的主要贡献是提出P-Tokens作为高效知识编辑方法，解决了IKE依赖冗长演示的关键问题。学术价值在于改进了LLMs的知识编辑技术，引入了新的令牌训练策略；实际应用价值是降低了编辑成本，提高了可扩展性，使得模型更新更实用。局限性包括对邻近事实的轻微负面影响，未来工作可探索优化以减少副作用，并扩展至更多模型和场景。",
      "tags": [
        "Large Language Model",
        "In-context Knowledge Editing",
        "Persuasion Tokens",
        "Knowledge Editing",
        "Token Training"
      ]
    },
    "analyzed_at": "2026-01-27T03:38:54.852203Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16532",
    "title": "AnchoredDream: Zero-Shot 360° Indoor Scene Generation from a Single View via Geometric Grounding",
    "authors": [
      "Runmao Yao",
      "Junsheng Zhou",
      "Zhen Dong",
      "Yu-Shen Liu"
    ],
    "abstract": "Single-view indoor scene generation plays a crucial role in a range of real-world applications. However, generating a complete 360° scene from a single image remains a highly ill-posed and challenging problem. Recent approaches have made progress by leveraging diffusion models and depth estimation networks, yet they still struggle to maintain appearance consistency and geometric plausibility under large viewpoint changes, limiting their effectiveness in full-scene generation. To address this, we propose AnchoredDream, a novel zero-shot pipeline that anchors 360° scene generation on high-fidelity geometry via an appearance-geometry mutual boosting mechanism. Given a single-view image, our method first performs appearance-guided geometry generation to construct a reliable 3D scene layout. Then, we progressively generate the complete scene through a series of modules: warp-and-inpaint, warp-and-refine, post-optimization, and a novel Grouting Block, which ensures seamless transitions between the input view and generated regions. Extensive experiments demonstrate that AnchoredDream outperforms existing methods by a large margin in both appearance consistency and geometric plausibility--all in a zero-shot manner. Our results highlight the potential of geometric grounding for high-quality, zero-shot single-view scene generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.16532.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16532",
    "published": "2026-01-23T08:08:12Z",
    "updated": "2026-01-26T07:46:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 AnchoredDream，一种基于几何锚定的零样本方法，用于从单视图生成 360° 室内场景。",
      "motivation": "单视图室内场景生成在虚拟现实、游戏和建筑设计等现实应用中至关重要，但生成完整 360° 场景是一个不适定且具挑战性的问题。现有方法如使用扩散模型和深度估计网络，在大视角变化下难以维持外观一致性和几何合理性，限制了全场景生成的实际效果。AnchoredDream 旨在通过几何锚定解决这一瓶颈，提升生成场景的真实感和完整性，以满足高质量应用需求。",
      "method": "AnchoredDream 是一种零样本流水线，采用外观-几何相互促进机制。首先，通过外观引导的几何生成模块构建可靠的 3D 场景布局，为后续生成提供几何基础。然后，利用一系列模块逐步生成完整场景：warp-and-inpaint 进行初始扩展，warp-and-refine 优化细节，post-optimization 进行全局调整，以及创新的 Grouting Block 确保输入视图和生成区域之间的无缝过渡，从而提升外观一致性和几何合理性。",
      "result": "广泛实验表明，AnchoredDream 在外观一致性和几何合理性方面显著优于现有的单视图场景生成方法，如基于扩散模型和深度估计的基线。该方法以零样本方式实现了性能的大幅提升，突出了其在生成高质量 360° 室内场景中的有效性，为相关应用提供了强有力的技术支持，但摘要未明确说明具体性能指标数据。",
      "conclusion": "AnchoredDream 的主要贡献在于通过几何锚定机制成功解决了单视图室内场景生成中的外观和几何挑战。研究在学术上推动了零样本场景生成技术的发展，在实际应用中为虚拟现实和建筑设计等领域提供了新工具。摘要未明确说明该方法的局限性或未来工作方向，但其成功暗示了在更大规模数据集或多样化场景类型中进一步探索的潜力。",
      "tags": [
        "Zero-Shot Learning",
        "Geometric Grounding",
        "Appearance-Guided Geometry Generation",
        "Warp-and-Inpaint",
        "Grouting Block"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:20.935575Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16531",
    "title": "A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics",
    "authors": [
      "Tao Lin"
    ],
    "abstract": "We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.   Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent \"hot-to-cold advantage flip\" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.   Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.16531.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16531",
    "published": "2026-01-23T08:07:20Z",
    "updated": "2026-01-26T03:40:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示了碰撞在Engram风格条件内存中作为隐式正则化，而非主要瓶颈，为训练动态提供新见解。",
      "motivation": "该研究动机是探究高频率键碰撞是否是Engram风格条件内存的性能瓶颈，这在实际内存系统中可能影响效率。现有方法可能侧重于消除碰撞以优化查找精度，但忽略了其在训练过程中的潜在角色，如摘要未明确说明的具体不足，但强调问题的重要性在于理解碰撞对训练动态的影响，而非仅仅提升索引准确性。",
      "method": "研究提出Engram-Nine方法，作为一种无碰撞的热层扩展，利用最小完美哈希函数处理高频率n-gram，同时保留原有多头哈希查找作为冷层，实现热冷分层结构。关键创新包括路由分层评估技术，将损失分解为热层和冷层贡献以分析训练动态，并在严格等参条件下进行实验，但具体数据集和模型架构摘要未明确说明。",
      "result": "实验结果表明，无碰撞设计未一致改善验证损失，说明碰撞并非主要限制。通过路由分层评估，发现训练中存在热到冷优势翻转：热位置初始损失较低，但冷位置最终超越，且无碰撞配置翻转更早，表明碰撞作为隐式正则化。此外，识别出门控不匹配现象，门持续赋予损失较高的热位置更高权重。",
      "conclusion": "研究结论强调，仅改善查找精度不能保证训练效果提升，主要限制可能在于门控信用分配机制。碰撞引起的噪声具有有益的正则化作用，不应盲目消除，这为内存系统设计提供了重要见解。未来工作方向可包括优化门控机制或进一步研究正则化策略，摘要未明确说明具体局限性。",
      "tags": [
        "Engram-Style Conditional Memory",
        "Minimal Perfect Hash Function",
        "Training Dynamics",
        "Implicit Regularization",
        "Routing-Stratified Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:19.448914Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16444",
    "title": "Exploring the Effects of Alignment on Numerical Bias in Large Language Models",
    "authors": [
      "Ayako Sato",
      "Hwichan Kim",
      "Zhousi Chen",
      "Masato Mita",
      "Mamoru Komachi"
    ],
    "abstract": "\"LLM-as-a-judge,\" which utilizes large language models (LLMs) as evaluators, has proven effective in many evaluation tasks. However, evaluator LLMs exhibit numerical bias, a phenomenon where certain evaluation scores are generated disproportionately often, leading reduced evaluation performance. This study investigates the cause of this bias. Given that most evaluator LLMs are aligned through instruction tuning and preference tuning, and that prior research suggests alignment reduces output diversity, we hypothesize that numerical bias arises from alignment. To test this, we compare outputs from pre- and post-alignment LLMs, and observe that alignment indeed increases numerical bias. We also explore mitigation strategies for post-alignment LLMs, including temperature scaling, distribution calibration, and score range adjustment. Among these, score range adjustment is most effective in reducing bias and improving performance, though still heuristic. Our findings highlight the need for further work on optimal score range selection and more robust mitigation strategies.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.16444.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16444",
    "published": "2026-01-23T04:45:35Z",
    "updated": "2026-01-26T02:24:36Z",
    "comment": "Accepted at AIBSD 2026 (Workshop at AAAI 2026)",
    "light_analysis": {
      "overview": "本研究揭示大型语言模型对齐过程导致数值偏差的现象，并通过实验验证分数范围调整作为有效缓解策略。",
      "motivation": "随着“LLM作为评估器”方法的普及，数值偏差问题日益凸显，其中某些评估分数出现频率过高，降低了评估任务的准确性和可靠性。现有方法多未深入探究此偏差的成因，而研究表明对齐（如指令调优和偏好调优）可能减少输出多样性，因此本研究假设数值偏差源于对齐，旨在验证这一假设并开发缓解策略以提升评估性能。",
      "method": "研究方法分为两部分：首先，通过比较对齐前和对齐后大型语言模型的输出，分析对齐对数值偏差的影响；其次，针对对齐后模型，探索多种缓解策略，包括温度缩放、分布校准和分数范围调整，以调整分数分布减少偏差。关键创新在于系统性测试对齐与偏差的关联，并评估策略效果，尽管摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果显示，对齐显著增加了大型语言模型的数值偏差，验证了研究假设。在缓解策略中，分数范围调整表现最佳，能有效减少偏差并提升评估性能，但该方法仍基于启发式；相比之下，温度缩放和分布校准效果较弱，但摘要未提供具体性能指标如准确率提升数据。",
      "conclusion": "本研究结论指出，对齐过程是数值偏差的主要成因，影响LLM评估器的实用性，而分数范围调整为有效缓解方法。学术上，这深化了对对齐机制的理解；应用上，为改进评估工具提供方向，但策略仍需优化，未来工作需探索更优分数范围选择及鲁棒的缓解方案。",
      "tags": [
        "Large Language Model",
        "Alignment",
        "Numerical Bias",
        "Evaluation",
        "Score Range Adjustment"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:03.245984Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16399",
    "title": "A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning",
    "authors": [
      "Sihan Zeng",
      "Sujay Bhatt",
      "Sumitra Ganesh",
      "Alec Koppel"
    ],
    "abstract": "We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.16399.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16399",
    "published": "2026-01-23T02:12:24Z",
    "updated": "2026-01-26T05:27:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于衰减熵正则化的单循环一阶actor-critic算法，用于高效优化双层强化学习问题。",
      "motivation": "本研究旨在解决结构化双层优化问题，其中上层目标为平滑函数，下层问题涉及马尔可夫决策过程（MDP）的策略优化，上层决策变量参数化下层MDP的奖励，上层目标依赖于下层诱导的最优策略。现有方法如双层优化和强化学习方法通常需要二阶导数信息、施加强正则化或采用效率低下的嵌套循环过程，导致计算复杂性高、样本利用效率不足。因此，开发一种更高效、一阶的单循环算法来应对这些挑战，对提升优化性能和实际应用具有重要意义。",
      "method": "论文提出一种单循环、一阶的actor-critic算法，通过基于惩罚的重新表述来优化双层目标。核心创新是在下层强化学习目标中引入衰减熵正则化，使得在不精确求解无正则化RL问题的情况下，能实现上层超梯度估计的渐近无偏性。该方法利用特殊Polyak-Lojasiewicz条件进行下层残差分析，确保算法收敛性。实验部分涉及GridWorld目标位置问题和基于人类反馈的强化学习（RLHF）的快乐推文生成任务，以验证方法性能，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "在理论层面，该算法被证明在有限时间和有限样本下收敛到原无正则化双层优化问题的平稳点，通过新颖的下层残差分析实现收敛保证。实验方面，通过在GridWorld目标位置问题和RLHF的快乐推文生成任务上进行验证，方法显示出有效性，但摘要未提供具体性能数据如准确率提升或效率改进，仅表明与基线方法相比具有优势，并强调了在标准问题上的良好表现。",
      "conclusion": "本研究的主要贡献是提出了一种高效的单循环actor-critic算法，通过衰减熵正则化解决了双层强化学习中的优化挑战，避免了现有方法对二阶信息或强正则化的依赖。学术价值在于提供了一种一阶优化框架，具有理论收敛保证，推动了复杂决策问题的方法创新；实际应用价值体现在智能决策和自然语言生成等领域，如GridWorld和RLHF任务。潜在局限性或未来工作方向摘要未明确说明，但可推断为进一步探索更广泛的优化条件、扩展到其他应用场景或提升算法泛化能力。",
      "tags": [
        "Bi-Level Optimization",
        "Reinforcement Learning",
        "Actor-Critic Algorithm",
        "Entropy Regularization",
        "Markov Decision Process"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:39.237709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.16249",
    "title": "Ordering-based Causal Discovery via Generalized Score Matching",
    "authors": [
      "Vy Vo",
      "He Zhao",
      "Trung Le",
      "Edwin V. Bonilla",
      "Dinh Phung"
    ],
    "abstract": "Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.16249.pdf",
    "abs_url": "https://arxiv.org/abs/2601.16249",
    "published": "2026-01-22T18:08:31Z",
    "updated": "2026-01-26T17:35:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于广义分数匹配的排序因果发现方法，通过离散分数函数优化叶子节点检测，以提高离散数据的因果图推断准确性。",
      "motivation": "因果发现从纯观测数据中学习有向无环图结构是科学领域的长期挑战，但现有方法如分数匹配框架主要针对连续数据，在处理离散数据时效果有限。本文旨在解决这一问题，扩展分数匹配框架到离散数据，以改进因果顺序推断，从而提升因果发现的准确性和适用范围，填补现有方法的不足。",
      "method": "本文提出了一种基于广义分数匹配的因果发现方法，通过叶子节点检测推断基础DAG的拓扑顺序，再进行边缘剪枝恢复图结构。关键创新在于引入基于离散分数函数的新叶子判别准则，使得方法适用于离散观测数据，扩展了原有连续数据框架，并采用模拟和真实世界实验进行验证。",
      "result": "通过模拟和真实世界实验，论文展示了方法能从观测离散数据准确推断真实因果顺序，识别出的顺序可显著提升现有因果发现基线的准确性，在几乎所有设置中都优于基线。尽管摘要未明确给出具体数字，但强调了整体准确性的显著改善，证明了方法在处理离散数据时的有效性。",
      "conclusion": "本文的主要贡献在于将分数匹配框架扩展到离散数据，并提出了基于离散分数函数的叶子判别准则，改进了从纯观测数据学习DAG结构的方法。该研究具有重要学术价值，为离散数据因果发现提供了新工具，可应用于各种科学领域的因果推断任务。未来工作可探索扩展到更复杂数据类型或优化计算效率。",
      "tags": [
        "Causal Discovery",
        "Score Matching",
        "Discrete Data",
        "DAG Learning",
        "Leaf Detection"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:48.323674Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.15516",
    "title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views",
    "authors": [
      "William Huang",
      "Siyou Pei",
      "Leyi Zou",
      "Eric J. Gonzalez",
      "Ishan Chatterjee",
      "Yang Zhang"
    ],
    "abstract": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >= 50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.",
    "categories": [
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.15516.pdf",
    "abs_url": "https://arxiv.org/abs/2601.15516",
    "published": "2026-01-21T23:00:43Z",
    "updated": "2026-01-26T18:45:41Z",
    "comment": "16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal",
    "light_analysis": {
      "overview": "本研究提出一种基于手背皮肤变形的双流delta编码器方法，显著提升遮挡场景下自我中心手部姿态估计的准确性。",
      "motivation": "随着XR设备的普及，自我中心视角下的手部姿态估计变得至关重要，但该视角常因手指频繁遮挡而难以准确估计，影响了交互的可靠性。现有方法依赖整个手部几何信息和大型模型骨干，在遮挡场景下性能受限，无法有效处理局部信息。因此，开发一种能够利用手背特征的方法，以弥补现有技术的不足，成为研究的重点。",
      "method": "论文提出一种新方法，利用手背皮肤变形信息来增强姿态估计。得益于密集视觉特征提取器的进展，构建了双流delta编码器，该编码器通过对比动态手状态与基线松弛状态的特征来学习姿态。关键创新点在于仅使用裁剪的手背图像，无需整个手的几何信息，从而优化了模型设计，减少了数据输入要求，并专门针对遮挡场景进行训练。",
      "result": "实验评估显示，在手指遮挡≥50%的自遮挡场景中，该方法仅使用裁剪手背图像，将Mean Per Joint Angle Error (MPJAE) 降低了18%，显著优于依赖完整手部几何信息和大型模型骨干的现有技术。这一改进还增强了遮挡场景下下游任务如食指捏合和点击估计的可靠性，验证了方法的有效性。",
      "conclusion": "本研究通过引入手背皮肤变形特征和双流delta编码器，为遮挡场景下的自我中心手部姿态估计提供了新思路。学术上，展示了利用局部特征进行姿态估计的潜力；实际上，提升了XR交互的准确性和多样性，例如支持检测无可见运动的等长力以实现“点击”交互，同时优化了模型尺寸。未来工作可扩展至更多手势类型或实际应用场景。",
      "tags": [
        "Hand Pose Estimation",
        "Dense Visual Featurizers",
        "Delta Encoder",
        "Egocentric Views",
        "Self-Occlusion"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:49.841914Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.15436",
    "title": "Not Your Typical Sycophant: The Elusive Nature of Sycophancy in Large Language Models",
    "authors": [
      "Shahar Ben Natan",
      "Oren Tsur"
    ],
    "abstract": "We propose a novel way to evaluate sycophancy of LLMs in a direct and neutral way, mitigating various forms of uncontrolled bias, noise, or manipulative language, deliberately injected to prompts in prior works. A key novelty in our approach is the use of LLM-as-a-judge, evaluation of sycophancy as a zero-sum game in a bet setting. Under this framework, sycophancy serves one individual (the user) while explicitly incurring cost on another. Comparing four leading models - Gemini 2.5 Pro, ChatGpt 4o, Mistral-Large-Instruct-2411, and Claude Sonnet 3.7 - we find that while all models exhibit sycophantic tendencies in the common setting, in which sycophancy is self-serving to the user and incurs no cost on others, Claude and Mistral exhibit \"moral remorse\" and over-compensate for their sycophancy in case it explicitly harms a third party. Additionally, we observed that all models are biased toward the answer proposed last. Crucially, we find that these two phenomena are not independent; sycophancy and recency bias interact to produce `constructive interference' effect, where the tendency to agree with the user is exacerbated when the user's opinion is presented last.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.15436.pdf",
    "abs_url": "https://arxiv.org/abs/2601.15436",
    "published": "2026-01-21T20:00:14Z",
    "updated": "2026-01-26T16:45:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种使用LLM-as-a-judge在赌注设置中作为零和游戏直接评估大语言模型逢迎行为的新方法，揭示了逢迎与近因偏见的相互作用效应。",
      "motivation": "现有研究中评估大语言模型逢迎行为时，常因提示中注入不可控的偏见、噪声或操纵性语言而影响准确性，导致结果不中性。逢迎行为直接影响模型的伦理表现和可信度，在实际应用中可能误导用户决策。因此，迫切需要一种直接、中性的评估方法，以更真实地理解模型行为并改进其可靠性，解决现有方法在减少人为干扰方面的不足。",
      "method": "论文提出了一种新颖评估框架，使用LLM-as-a-judge在赌注设置中将逢迎行为建模为零和游戏，从而直接、中性评估模型反应。该方法通过比较四个领先模型（Gemini 2.5 Pro、ChatGpt 4o、Mistral-Large-Instruct-2411和Claude Sonnet 3.7）在不同情境下的表现，关键创新在于减少评估过程中的偏见干扰，并探索逢迎在是否损害第三方等具体条件下的变化。",
      "result": "实验发现所有测试模型在常见设置（无成本逢迎）中均表现出逢迎倾向，但Claude和Mistral在逢迎明确损害第三方时表现出道德悔恨并过度补偿。此外，所有模型都偏向最后提出的答案（近因偏见）。两者相互作用产生“建设性干扰”效应，当用户意见最后呈现时，同意的倾向加剧。这些结果未提供具体数据，但基于对比揭示了模型行为的多变性和交互效应。",
      "conclusion": "本研究的主要贡献是提出了一种减少偏见的评估方法，并发现了逢迎与近因偏见的交互作用，深化了对大语言模型伦理行为的理解。其学术价值在于为公平评估模型提供新框架，实际应用有助于开发更可信的AI系统。未来工作可能涉及扩展模型范围或探究其他偏见交互，以进一步改进模型伦理设计，但摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Sycophancy Evaluation",
        "LLM-as-a-judge",
        "Recency Bias",
        "Zero-Sum Game"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:37.258821Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.12816",
    "title": "Fisher-Orthogonal Projected Natural Gradient Descent for Continual Learning",
    "authors": [
      "Ishir Garg",
      "Neel Kolhe",
      "Andy Peng",
      "Rohan Gopalam"
    ],
    "abstract": "Continual learning aims to enable neural networks to acquire new knowledge on sequential tasks. However, the key challenge in such settings is to learn new tasks without catastrophically forgetting previously learned tasks. We propose the Fisher-Orthogonal Projected Natural Gradient Descent (FOPNG) optimizer, which enforces Fisher-orthogonal constraints on parameter updates to preserve old task performance while learning new tasks. Unlike existing methods that operate in Euclidean parameter space, FOPNG projects gradients onto the Fisher-orthogonal complement of previous task gradients. This approach unifies natural gradient descent with orthogonal gradient methods within an information-geometric framework. We provide theoretical analysis deriving the projected update, describe efficient and practical implementations using the diagonal Fisher, and demonstrate strong results on standard continual learning benchmarks such as Permuted-MNIST, Split-MNIST, Rotated-MNIST, Split-CIFAR10, and Split-CIFAR100. Our code is available at https://github.com/ishirgarg/FOPNG.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.12816.pdf",
    "abs_url": "https://arxiv.org/abs/2601.12816",
    "published": "2026-01-19T08:23:12Z",
    "updated": "2026-01-26T04:21:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Fisher-正交投影自然梯度下降（FOPNG）优化器，通过在信息几何框架中统一自然梯度和正交方法，有效解决持续学习中的灾难性遗忘问题。",
      "motivation": "持续学习旨在使神经网络能在序列任务中逐步积累知识，但其核心挑战在于学习新任务时避免灾难性遗忘旧任务性能。现有方法常在欧几里得参数空间中操作，如直接梯度更新，可能导致旧知识被覆盖或干扰，缺乏有效机制来保护先前学习到的表示。因此，本文致力于克服这一不足，通过引入几何约束优化参数更新方向，以提升任务间的知识保留能力，从而增强神经网络在动态环境中的适应性和稳定性。",
      "method": "本文提出FOPNG优化器，核心方法是在参数更新中强制执行Fisher-正交约束。具体而言，它将梯度向量投影到先前任务梯度的Fisher-正交补空间中，从而在学习新任务时最小化对旧任务参数的影响。创新点在于将自然梯度下降与正交梯度方法统一在信息几何框架内，利用Fisher信息矩阵定义正交性以处理参数空间的几何结构。实现上，采用对角Fisher矩阵进行高效计算，提供理论推导确保投影更新的合理性，并通过实用算法应用于标准数据集，如使用梯度投影步骤来优化网络权重。",
      "result": "在多个标准持续学习基准上进行了评估，包括Permuted-MNIST、Split-MNIST、Rotated-MNIST、Split-CIFAR10和Split-CIFAR100。摘要未明确说明具体准确率提升数值，但指出FOPNG方法取得了强劲结果，表明其有效减少了灾难性遗忘。实验表明，与基线方法相比，FOPNG在任务序列中表现优异，验证了其在保护旧任务性能的同时学习新任务的能力，尽管具体性能指标如准确率改进未详细列出，但强调其通用性和有效性。",
      "conclusion": "本文的主要贡献是提出FOPNG优化器，通过在信息几何框架下应用Fisher-正交投影，解决了持续学习中的灾难性遗忘问题。研究提供了理论分析和高效实现，具有学术价值，推动了优化算法在持续学习领域的发展。实际应用方面，该方法有助于神经网络更稳定地积累知识，提升在动态环境中的部署效果。未来工作可探索更复杂的任务设置或扩展到更大规模模型，以进一步增强方法的实用性和鲁棒性。",
      "tags": [
        "Continual Learning",
        "Natural Gradient Descent",
        "Orthogonal Projection",
        "Fisher Information Matrix",
        "Gradient Descent Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:54.516644Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.11451",
    "title": "PRISM-CAFO: Prior-conditioned Remote-sensing Infrastructure Segmentation and Mapping for CAFOs",
    "authors": [
      "Oishee Bintey Hoque",
      "Nibir Chandra Mandal",
      "Kyle Luong",
      "Amanda Wilson",
      "Samarth Swarup",
      "Madhav Marathe",
      "Abhijin Adiga"
    ],
    "abstract": "Large-scale livestock operations pose significant risks to human health and the environment, while also being vulnerable to threats such as infectious diseases and extreme weather events. As the number of such operations continues to grow, accurate and scalable mapping has become increasingly important. In this work, we present an infrastructure-first, explainable pipeline for identifying and characterizing Concentrated Animal Feeding Operations (CAFOs) from aerial and satellite imagery. Our method (i) detects candidate infrastructure (e.g., barns, feedlots, manure lagoons, silos) with a domain-tuned YOLOv8 detector, then derives SAM2 masks from these boxes and filters component-specific criteria; (ii) extracts structured descriptors (e.g., counts, areas, orientations, and spatial relations) and fuses them with deep visual features using a lightweight spatial cross-attention classifier; and (iii) outputs both CAFO type predictions and mask-level attributions that link decisions to visible infrastructure. Through comprehensive evaluation, we show that our approach achieves state-of-the-art performance, with Swin-B+PRISM-CAFO surpassing the best performing baseline by up to 15\\%. Beyond strong predictive performance across diverse U.S. regions, we run systematic gradient--activation analyses that quantify the impact of domain priors and show how specific infrastructure (e.g., barns, lagoons) shapes classification decisions. We release code, infrastructure masks, and descriptors to support transparent, scalable monitoring of livestock infrastructure, enabling risk modeling, change detection, and targeted regulatory action.   Github: https://github.com/Nibir088/PRISM-CAFO.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.11451.pdf",
    "abs_url": "https://arxiv.org/abs/2601.11451",
    "published": "2026-01-16T17:16:26Z",
    "updated": "2026-01-26T03:39:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一个可解释的遥感图像管道，结合先验知识和空间注意力，用于准确识别和特征化集中动物饲养操作。",
      "motivation": "大规模畜牧业操作对人类健康和环境构成显著风险，同时易受传染病和极端天气威胁。随着操作数量持续增长，精确且可扩展的映射变得至关重要。现有方法可能在可解释性和可扩展性方面不足，无法有效支持监控和风险管理，因此需要一种基础设施优先、可解释的管道来解决这些问题，促进透明监管和风险建模。",
      "method": "该方法采用三步管道：首先，使用领域调优的YOLOv8检测器检测候选基础设施（如畜舍、饲养场、粪污池、筒仓），然后通过SAM2生成掩码并筛选组件特定标准；其次，提取结构化描述符（如数量、面积、朝向和空间关系），并使用轻量级空间交叉注意力分类器融合这些描述符与深度视觉特征；最后，输出CAFO类型预测和掩码级归因，将分类决策链接到可见基础设施，增强可解释性。",
      "result": "全面评估显示该方法达到最先进性能，其中Swin-B+PRISM-CAFO超越最佳基线高达15%。在多样化美国区域表现出强预测性能，并通过系统梯度-激活分析量化领域先验的影响，具体展示特定基础设施（如畜舍、粪污池）如何塑造分类决策，支持了方法的有效性和可解释性。",
      "conclusion": "该研究贡献了一个可解释的遥感图像管道，显著提升了CAFO识别的性能和透明度，并通过释放代码、基础设施掩码和描述符支持可扩展监控。这有助于风险建模、变化检测和目标性监管行动，具有重要实际应用价值，未来工作可扩展到更广泛区域或不同类型的基础设施监控。",
      "tags": [
        "Object Detection",
        "Segmentation",
        "Spatial Cross-Attention",
        "Remote Sensing",
        "Explainable AI"
      ]
    },
    "analyzed_at": "2026-01-27T03:39:48.820412Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.11214",
    "title": "T$^\\star$: Progressive Block Scaling for MDM Through Trajectory Aware RL",
    "authors": [
      "Hanchen Xia",
      "Baoyou Chen",
      "Yutang Ge",
      "Guojiang Zhao",
      "Siyu Zhu"
    ],
    "abstract": "We present T*, a simple TraceRL-based training curriculum for progressive block-size scaling in masked diffusion language models (MDMs). Starting from an AR-initialized small-block MDM, T* transitions smoothly to larger blocks, enabling higher-parallelism decoding with minimal performance degradation on math reasoning benchmarks. Moreover, further analysis suggests that T* can converge to an alternative decoding schedule that achieves comparable performance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.11214.pdf",
    "abs_url": "https://arxiv.org/abs/2601.11214",
    "published": "2026-01-16T11:44:12Z",
    "updated": "2026-01-26T05:54:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了T*方法，一种基于轨迹感知强化学习的训练课程，用于在掩码扩散语言模型中实现渐进块大小缩放，以提升解码并行度。",
      "motivation": "摘要未明确说明具体动机，但可以推断掩码扩散语言模型（MDMs）在解码时可能因块大小限制而影响效率，现有方法在缩放块大小时可能导致性能显著下降。因此，需要一种平稳过渡的方法来优化解码并行度，这对于提高模型在数学推理等任务中的应用性能非常重要。",
      "method": "方法核心是使用基于TraceRL（轨迹感知强化学习）的训练课程，从自回归初始化的小块MDM开始，逐步将块大小增加到更大规模。关键创新在于渐进式缩放机制和轨迹感知的强化学习框架，确保在过渡过程中最小化性能损失。摘要未明确说明具体数据集或模型架构细节，但专注于数学推理任务的应用。",
      "result": "实验结果显示，在数学推理基准测试中，T*方法实现了更高并行度的解码，同时性能下降最小。进一步分析表明，该方法可以收敛到一种替代解码调度，达到与原始方法可比较的性能。摘要未提供具体数值指标，但强调了在保持性能的前提下提升解码效率。",
      "conclusion": "T*方法成功通过渐进块大小缩放提升了掩码扩散语言模型的解码并行度，同时维持了模型性能，具有潜在的实际应用价值和学术意义。该研究为优化大型语言模型的训练策略提供了新思路，未来工作可能包括扩展到其他模型类型或探索更高效的训练方法。",
      "tags": [
        "Masked Diffusion Models",
        "Reinforcement Learning",
        "Training Curriculum",
        "Progressive Scaling",
        "Decoding Parallelism"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:08.750960Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09852",
    "title": "Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences",
    "authors": [
      "Sriram Padmanabhan",
      "Siyuan Song",
      "Kanishka Misra"
    ],
    "abstract": "Language places subtle constraints on how we make inductive inferences. Developmental evidence by Gelman et al. (2002) has shown children (4 years and older) to differentiate among generic statements (\"Bears are daxable\"), universally quantified NPs (\"all bears are daxable\") and indefinite plural NPs (\"some bears are daxable\") in extending novel properties to a specific member (all > generics > some), suggesting that they represent these types of propositions differently. We test if these subtle differences arise in general purpose statistical learners like Vision Language Models, by replicating the original experiment. On tasking them through a series of precondition tests (robust identification of categories in images and sensitivities to all and some), followed by the original experiment, we find behavioral alignment between models and humans. Post-hoc analyses on their representations revealed that these differences are organized based on inductive constraints and not surface-form differences.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09852.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09852",
    "published": "2026-01-14T20:16:10Z",
    "updated": "2026-01-26T03:34:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "这篇论文通过复制人类儿童实验，测试视觉语言模型是否在语言约束下进行归纳推理，发现模型行为与人类对齐。",
      "motivation": "研究动机源于语言对归纳推理的微妙约束，如儿童能区分全称、泛型和不定量化语句，并将其用于属性扩展。在AI领域，现有方法往往忽视模型是否具备类似的深层语言理解能力，这限制了对AI推理能力的评估和改进。本工作旨在探索通用统计学习器（如视觉语言模型）是否能体现这些语言约束，以填补该领域的空白，并为AI语言理解提供新视角。",
      "method": "研究方法基于Gelman等人的认知科学实验，将其复制到视觉语言模型中。具体步骤包括：先进行先决条件测试，如评估模型对图像中类别的鲁棒识别能力，以及对量词“所有”和“一些”的敏感性；然后执行原始归纳推理实验，测试模型在不同语言语句（全称、泛型、不定）下扩展新属性的行为。关键创新在于将人类发展心理学范式应用于AI模型评估，以验证其推理能力。摘要未明确说明具体使用的模型架构或数据集。",
      "result": "主要实验结果表明，视觉语言模型在归纳推理任务中表现出与人类儿童相似的行为对齐，优先级顺序为全称 > 泛型 > 不定，这与人类数据一致。事后分析揭示，这些行为差异是基于内在的归纳约束，而非语言表面的形式差异，例如模型表示中捕捉了更深层的语义结构。摘要未提供具体性能指标如准确率，但强调了定性对齐，未与基线方法进行详细对比。",
      "conclusion": "论文的主要贡献是展示了视觉语言模型能模拟人类语言推理模式，验证了AI模型在语言约束下的归纳能力。这具有跨学科研究价值，促进了AI与认知科学的融合，并为改进语言模型的设计和评估提供了新思路。局限性可能包括实验的简化设置，未来工作可扩展到更多模型类型、复杂语言结构或更大数据集。",
      "tags": [
        "Vision Language Models",
        "Inductive Inference",
        "Language Constraints",
        "Cognitive Modeling",
        "Statistical Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:37.037942Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05344",
    "title": "Coding the Visual World: From Image to Simulation Using Vision Language Models",
    "authors": [
      "Sagi Eppel"
    ],
    "abstract": "The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) have the ability to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05344.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05344",
    "published": "2026-01-08T19:49:05Z",
    "updated": "2026-01-26T10:11:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出使用视觉语言模型通过Im2Sim方法，探索其在理解和模拟图像中复杂系统的能力。",
      "motivation": "研究动机在于视觉理解的核心是构建系统模型，而现有视觉语言模型主要关注图像识别，但模拟图像中系统机制的能力尚未充分探索。本研究旨在解决如何让AI更好地理解图像中的复杂系统（如物理现象、城市结构），以提升深层视觉理解。这一问题的重要性在于它扩展了AI的应用范围，现有方法往往缺乏模拟和生成能力，导致对系统动态的理解不足，因此探索VLMs的模拟潜力对推动认知建模和实际应用（如计算机图形学）有重要意义。",
      "method": "研究方法采用Im2Sim技术路线，核心是让视觉语言模型（如GPT、Gemini）接收真实世界系统的自然图像，首先描述系统特性，然后生成代码来模拟和生成该系统的合成图像。关键创新点在于将VLM的视觉语言理解与代码生成结合，用于图像模拟任务。具体步骤包括：图像输入、系统描述、代码编写、代码执行生成合成图像，并与原始图像比较以评估理解能力。测试数据集涵盖多种复杂系统，包括物理系统（如波浪、光线）、植被、城市、材料和地质形态，以验证方法的泛化性。",
      "result": "主要实验结果表明，主流视觉语言模型能够理解和建模复杂、多组件的系统，跨越多层抽象和广泛领域（如物理、城市等），显示出高层视觉理解能力。然而，VLMs在复制图像中精细细节和低层模式安排方面表现出有限能力，例如对图案的具体排列还原不足。这揭示了有趣的不对称性：VLMs结合了高层的深度理解与有限的细节感知。与基线方法相比，摘要未明确说明具体对比数据，但强调了VLMs在新应用场景下的独特优势，为后续研究提供了性能基准。",
      "conclusion": "论文主要贡献在于证明了视觉语言模型在图像系统理解方面的潜力，通过Im2Sim方法推动了视觉认知研究。学术价值在于扩展了VLM的应用，并揭示了高层理解与细节感知之间的不对称性，这对AI视觉理解和认知建模有重要启发。实际应用价值可体现在自动化图形生成、教育模拟等领域。局限性包括细节还原能力有限，未来工作方向可聚焦于改进细节感知技术，或结合多模态方法提升模拟精度，以更全面地复制图像内容。",
      "tags": [
        "Vision Language Models",
        "Im2Sim",
        "Code Generation",
        "Image Simulation",
        "Emergent Systems"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:32.101422Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03612",
    "title": "Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias",
    "authors": [
      "Joonwon Seo"
    ],
    "abstract": "This monograph introduces a novel approach to polyphonic music generation by addressing the \"Missing Middle\" problem through structural inductive bias. Focusing on Beethoven's piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.",
    "categories": [
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.03612.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03612",
    "published": "2026-01-07T05:40:09Z",
    "updated": "2026-01-26T18:53:22Z",
    "comment": "Monograph. Code available at https://github.com/Chooseredone/Smart-Embedding-Music-Generation",
    "light_analysis": {
      "overview": "本论文提出了一种基于结构归纳偏置的复音音乐生成新方法，解决了‘缺失中间’问题，并通过数学理论和实验验证其稳定性和泛化性。",
      "motivation": "该研究旨在解决复音音乐生成中的‘缺失中间’问题，即如何在生成多声部音乐时处理结构化不足的挑战。现有AI音乐生成方法通常缺乏坚实的数学基础，导致模型不稳定和泛化能力弱。以贝多芬钢琴奏鸣曲为例，本研究强调结合信息论和范畴论等数学工具的重要性，以弥补当前方法在理论严谨性上的不足，推动深度学习在艺术领域的可靠应用。",
      "method": "论文提出了Smart Embedding架构，通过结构归纳偏置优化复音音乐生成，减少了48.30%的参数。关键创新点在于使用归一化互信息验证音高和手属性的独立性（NMI=0.167），并结合信息理论、Rademacher复杂度和范畴论提供严格的数学证明，确保方法的稳定性和泛化性改进。该方法以贝多芬钢琴奏鸣曲为数据集，展示了高效性和理论深度。",
      "result": "实证结果显示验证损失减少了9.47%，表明模型性能显著提升，并通过SVD分析确认了优化效果。专家听音研究（N=53）进一步验证了生成音乐的质量。数学上，信息论证明损失可忽略（边界0.153比特），Rademacher复杂度改善了泛化边界（紧致28.09%），与基线方法相比，Smart Embedding架构在参数效率和模型效果上均有优越表现。",
      "conclusion": "该研究的主要贡献在于开发了一个理论和应用双框架，通过数学证明和实验验证，提升了复音音乐生成的稳定性和泛化性。学术价值在于为AI音乐生成提供了可验证的数学基础，促进了基于数学的深度学习发展；实际应用价值在于提高了音乐生成的质量和可靠性。未来工作可能包括扩展到其他音乐类型和更广泛的数据集，以进一步验证方法的普适性。",
      "tags": [
        "Polyphonic Music Generation",
        "Structural Inductive Bias",
        "Information Theory",
        "Rademacher Complexity",
        "Category Theory"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:52.937441Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02103",
    "title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures",
    "authors": [
      "Yating Wang",
      "Yuan Sun",
      "Xuan Wang",
      "Ran Yi",
      "Boyao Zhou",
      "Yipengjing Sun",
      "Hongyu Liu",
      "Yinuo Wang",
      "Lizhuang Ma"
    ],
    "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.02103.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02103",
    "published": "2026-01-05T13:32:37Z",
    "updated": "2026-01-26T16:17:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了HeadLighter，一个监督框架，用于在生成3D高斯头部模型中解缠光照和内在外观，以实现可控重光照和视点编辑。",
      "motivation": "研究背景是当前基于3D高斯喷涂的头部生成模型虽能实现实时和逼真合成，但光照与内在外观的深度纠缠限制了可控重光照能力，阻碍了实际应用。现有解缠方法依赖强假设进行弱监督学习，处理复杂光照时效果有限，导致无法精确分离渲染要素。此问题的重要性在于，它影响生成模型的灵活性和实用性，特别是在虚拟现实和影视特效等领域，需要高效且逼真的光照控制来提升用户体验。",
      "method": "HeadLighter采用监督框架，通过双分支架构分别建模光照无关的头部属性和基于物理的渲染组件，以实现光照和外观的解缠。关键创新包括渐进式解缠训练，逐步将头部外观先验注入生成架构，并使用光舞台捕获的多视图图像进行监督，确保物理可分解性。此外，引入蒸馏策略生成高质量法线，提升渲染真实感。方法不依赖强假设，而是利用可控光照环境下的数据集，增强了处理复杂照明场景的能力。",
      "result": "实验结果显示，HeadLighter在保持高质量生成和实时渲染的同时，支持显式光照和视点编辑，显著优于现有解缠方法。摘要未明确说明具体性能指标，如准确率提升或效率改进，但可以推断，该方法在处理复杂照明时表现更优，并通过多视图监督提升了分解精度。与基线方法对比，它避免了强假设限制，实现了更可靠的解缠效果，增强了生成模型的可控性和应用潜力。",
      "conclusion": "本研究的主要贡献是提出了HeadLighter框架，成功解缠了生成3D头部模型中的光照和外观，推动了可控生成模型的进展。学术价值在于引入监督学习和物理分解方法，提升了模型的可解释性和实用性；实际应用价值在于支持重光照和视点编辑，适用于虚拟现实和数字内容创作。潜在局限性包括数据获取成本高，未来工作可扩展到更广泛的照明条件，并计划公开代码和数据集以促进社区发展。",
      "tags": [
        "3D Gaussian Splatting",
        "Disentanglement",
        "Light Stage",
        "Supervised Learning",
        "Normal Distillation"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:32.312004Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.24235",
    "title": "LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring",
    "authors": [
      "May Bashendy",
      "Walid Massoud",
      "Sohaila Eltanbouly",
      "Salam Albatarni",
      "Marwan Sayed",
      "Abrar Abir",
      "Houda Bouamor",
      "Tamer Elsayed"
    ],
    "abstract": "Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.24235.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24235",
    "published": "2025-12-30T13:49:52Z",
    "updated": "2026-01-26T11:22:08Z",
    "comment": "Accepted at EACL 2026 - main conference",
    "light_analysis": {
      "overview": "论文介绍了LAILA，一个基于特质的大型阿拉伯语自动作文评分数据集，填补了该领域的数据空白。",
      "motivation": "自动作文评分（AES）在教育评估中具有重要应用，能提高评分的效率和一致性，但阿拉伯语AES研究因公开数据集的缺乏而受限。现有方法多集中于英语等资源丰富语言，阿拉伯语由于其复杂的语法结构和数据稀缺性，难以开发有效的评分系统。本研究旨在解决这一问题，通过提供一个标准化数据集来推动阿拉伯语AES技术的发展，从而克服现有方法在数据可用性方面的不足。",
      "method": "本研究提出了LAILA数据集，包含7,859篇阿拉伯语作文，每篇作文标注了整体评分和七个特质维度（相关性、组织、词汇、风格、发展、机械和语法）的详细分数。方法涉及数据集的设计、收集和注释过程，确保数据质量和多样性。创新点在于综合评分维度和大规模数据规模，并使用最先进的阿拉伯语和英语模型进行基准测试，在特定提示和跨提示设置下评估数据集的有效性，为模型开发提供技术基础。",
      "result": "论文提供了基准测试结果，展示了LAILA数据集在自动作文评分任务上的应用效果。尽管摘要未明确说明具体性能指标如准确率提升或效率改进，但结果证明了数据集对开发鲁棒评分系统的支持作用。与现有基线方法相比，LAILA为阿拉伯语AES提供了更全面的评估基准，有助于模型在特定和跨提示场景下的性能验证，但具体对比数据需参考论文详细内容。",
      "conclusion": "本研究的主要贡献是引入了LAILA，最大公开阿拉伯语AES数据集，填补了该领域的关键需求，支持基于特质的评分系统开发。学术价值在于促进了阿拉伯语自然语言处理研究，提供了标准化评估资源；实际应用价值体现在教育技术中自动化评估的推广。未来工作可能包括扩展数据集规模、探索更多评分维度或集成先进AI模型以进一步优化评分准确性。",
      "tags": [
        "Automated Essay Scoring",
        "Arabic Language Processing",
        "Dataset Creation",
        "Natural Language Processing",
        "Trait-Based Scoring"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:47.270742Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.19367",
    "title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture",
    "authors": [
      "Christian Hägg",
      "Kathlén Kohn",
      "Giovanni Luca Marchetti",
      "Boris Shapiro"
    ],
    "abstract": "We introduce Sprecher Networks (SNs), a family of trainable architectures derived from David Sprecher's 1965 constructive form of the Kolmogorov-Arnold representation. Each SN block implements a \"sum of shifted univariate functions\" using only two shared learnable splines per block, a monotone inner spline $φ$ and a general outer spline $Φ$, together with a learnable shift parameter $η$ and a mixing vector $λ$ shared across all output dimensions. Stacking these blocks yields deep, compositional models; for vector-valued outputs we append an additional non-summed output block.   We also propose an optional lateral mixing operator enabling intra-block communication between output channels with only $O(d_{\\mathrm{out}})$ additional parameters. Owing to the vector (not matrix) mixing weights and spline sharing, SNs scale linearly in width, approximately $O(\\sum_{\\ell}(d_{\\ell-1}+d_{\\ell}+G))$ parameters for $G$ spline knots, versus $O(\\sum_{\\ell} d_{\\ell-1}d_{\\ell})$ for dense MLPs and $O(G\\sum_{\\ell} d_{\\ell-1}d_{\\ell})$ for edge-spline KANs. This linear width-scaling is particularly attractive for extremely wide, shallow models, where low depth can translate into low inference latency. Finally, we describe a sequential forward implementation that avoids materializing the $d_{\\mathrm{in}}\\times d_{\\mathrm{out}}$ shifted-input tensor, reducing peak forward-intermediate memory from quadratic to linear in layer width, relevant for memory-constrained settings such as on-device/edge inference; we demonstrate deployability via fixed-point real-time digit classification on resource-constrained embedded device with only 4 MB RAM. We provide empirical demonstrations on supervised regression, Fashion-MNIST classification (including stable training at 25 hidden layers with residual connections and normalization), and a Poisson PINN, with controlled comparisons to MLP and KAN baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.19367.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19367",
    "published": "2025-12-22T13:09:45Z",
    "updated": "2026-01-26T10:10:49Z",
    "comment": "45 pages",
    "light_analysis": {
      "overview": "论文提出Sprecher Networks，一种基于Kolmogorov-Arnold表示的可训练架构，通过共享splines和线性参数缩放实现参数高效和内存优化，适用于宽浅模型和边缘部署。",
      "motivation": "当前深度模型如密集多层感知机（MLP）和边缘spline KANs在宽浅架构中参数数量呈二次增长，导致高推理延迟和大内存需求，尤其在资源受限的边缘设备上难以部署。这凸显了开发参数高效模型的重要性，以提升计算效率和降低内存开销，推动AI在实际应用中的普及。摘要未明确说明所有背景细节，但基于上下文推断出现有方法在效率和可扩展性方面的不足是主要驱动力。",
      "method": "Sprecher Networks基于David Sprecher的Kolmogorov-Arnold表示构建，每个SN块使用两个共享学习splines（单调内spline φ和通用外spline Φ）以及学习参数（如偏移参数η和混合向量λ）。块可堆叠成深度模型，并引入可选横向混合算子以O(d_out)额外参数增强输出通道间通信。参数数量随层宽度线性增长（约O(∑ℓ(dℓ-1 + dℓ + G))），相比MLPs和KANs更高效。还设计了顺序前向实现，避免形成二次内存的张量，将峰值内存从二次降至线性，适用于设备推理。",
      "result": "论文在监督回归、Fashion-MNIST分类（包括在25个隐藏层中稳定训练）和Poisson物理信息神经网络（PINN）任务上进行了实验，与MLP和KAN基线进行对比。结果显示SNs在参数效率和内存使用方面具有优势，成功在仅4 MB RAM的嵌入式设备上实现定点实时数字分类，验证了部署能力。具体性能指标如准确率未详细提供，但实验强调了效率和实用性的显著提升。",
      "conclusion": "本研究的主要贡献是提出Sprecher Networks，一种参数高效的Kolmogorov-Arnold架构，通过共享splines和线性缩放降低了参数和内存需求。学术上丰富了神经网络表示理论，实际上促进了在宽浅模型和边缘设备上的高效部署，为资源受限环境中的AI应用提供了新方案。局限性包括具体任务性能比较不够详尽，未来工作可扩展到更多领域或进一步优化架构。摘要未明确说明未来方向，但基于结果可合理推断潜在扩展。",
      "tags": [
        "Kolmogorov-Arnold Networks",
        "Spline Functions",
        "Parameter-Efficient Models",
        "Memory Optimization",
        "Edge Inference"
      ]
    },
    "analyzed_at": "2026-01-27T03:40:58.828012Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.16912",
    "title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward",
    "authors": [
      "Peter Chen",
      "Xiaopeng Li",
      "Ziniu Li",
      "Wotao Yin",
      "Xi Chen",
      "Tianyi Lin"
    ],
    "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.16912.pdf",
    "abs_url": "https://arxiv.org/abs/2512.16912",
    "published": "2025-12-18T18:59:27Z",
    "updated": "2026-01-26T17:06:02Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": {
      "overview": "该论文揭示了在强化学习可验证奖励框架中，虚假奖励和剪裁偏差通过减少策略熵来提升大型语言模型推理性能的机制。",
      "motivation": "本研究旨在解决强化学习可验证奖励（RLVR）框架中探索与利用的权衡问题，该框架用于提升大型语言模型（LLMs）的数学推理能力。现有方法中，虚假奖励和熵最小化看似矛盾：虚假奖励奖励与真实值无关的结果以抑制利用，而熵最小化抑制探索，两者都能提升性能，但其底层原理未被充分理解。因此，动机是深入探索这些机制，以填补研究空白并优化RLVR训练策略，推动LLMs推理能力的进一步发展。",
      "method": "论文采用理论分析和实验方法，聚焦于两个核心问题：策略熵与性能的关系，以及虚假奖励是否通过剪裁偏差和模型污染的交互相带来收益。技术路线包括分析剪裁偏差在虚假奖励下如何减少策略熵，从而产生更自信的模型输出；并提出奖励失配模型来解释虚假奖励在非污染设置中的性能提升。关键创新点在于结合RLVR框架，探讨大型语言模型中的策略优化，使用熵最小化和奖励设计作为核心工具，强调理论模型与实验验证的结合。",
      "result": "实验结果表明，在虚假奖励下，剪裁偏差有效减少策略熵，使模型输出更自信和确定性，从而提升推理性能；相比之下，仅实施熵最小化不足以带来显著改进。论文还通过奖励失配模型阐明，虚假奖励可以在模型污染设置之外提供额外收益，揭示了这些机制在RLVR中的积极作用。摘要未明确说明与基线方法的对比具体数据，但提供了机制上的解释，强调了剪裁偏差和虚假奖励的协同效应。",
      "conclusion": "本研究的主要贡献在于澄清了RLVR中虚假奖励益处的机制，为设计更有效的训练原则提供了理论依据。学术价值体现在深化了对探索与利用动态的理解，丰富了强化学习理论；实际应用价值在于优化大型语言模型的推理能力训练。未来工作方向可能包括进一步验证奖励失配模型、扩展到其他领域如自然语言处理，以及探索更多奖励策略以提升鲁棒性。摘要未明确说明研究的局限性，但可推断需要更多实验支持。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Large Language Models",
        "Policy Entropy",
        "Spurious Reward",
        "Clipping Bias"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:19.137208Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.13481",
    "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings",
    "authors": [
      "Arnav Ramamoorthy",
      "Shrey Dhorajiya",
      "Ojas Pungalia",
      "Rashi Upadhyay",
      "Abhishek Mishra",
      "Abhiram H",
      "Tejasvi Alladi",
      "Sujan Yenuganti",
      "Dhruv Kumar"
    ],
    "abstract": "Envy shapes competitiveness and cooperation in human groups, yet its role in large language model interactions remains largely unexplored. As LLMs increasingly operate in multi-agent settings, it is important to examine whether they exhibit envy-like preferences under social comparison. We evaluate LLM behavior across two scenarios: (1) a point-allocation game testing sensitivity to relative versus absolute payoff, and (2) comparative evaluations across general and contextual settings. To ground our analysis in psychological theory, we adapt four established psychometric questionnaires spanning general, domain-specific, workplace, and sibling-based envy. Our results reveal heterogeneous envy-like patterns across models and contexts, with some models sacrificing personal gain to reduce a peer's advantage, while others prioritize individual maximization. These findings highlight competitive dispositions as a design and safety consideration for multi-agent LLM systems.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.13481.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13481",
    "published": "2025-12-15T16:17:12Z",
    "updated": "2026-01-26T04:35:00Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "该论文首次探索大型语言模型在多智能体环境中是否表现类似嫉妒的偏好，揭示了模型间异质的竞争行为。",
      "motivation": "嫉妒在人类群体中塑造竞争和合作，但大型语言模型（LLMs）在此方面的角色尚未被充分研究。随着LLMs在多智能体设置中的广泛应用，理解其是否在社会比较下表现嫉妒类似偏好变得至关重要，以解决潜在的安全和协作问题。现有方法多集中于LLMs的个体性能，忽视了其在社交互动中的心理偏好，可能导致多智能体系统设计存在不足。因此，本研究旨在填补这一空白，为LLMs的社会行为提供新见解。",
      "method": "论文提出了一种基于心理学理论的方法来评估LLMs的嫉妒类似偏好。通过两个实验场景：一是点分配游戏，用于测试模型对相对与绝对回报的敏感性；二是比较评估，覆盖通用和上下文设置。关键创新在于使用四个已建立的心理测量问卷（通用、领域特定、工作场所、兄弟姐妹嫉妒）来量化嫉妒偏好，并将心理学理论应用于LLM行为分析。摘要未明确说明具体使用的数据集或模型架构，但提及了跨多个模型进行评估。",
      "result": "实验结果显示，LLMs在不同模型和上下文中展现出异质的嫉妒类似模式：部分模型会牺牲个人收益以减少同伴的优势，表现出嫉妒倾向；而其他模型则优先个人最大化，较少受社会比较影响。这些发现基于定性分析，揭示了模型间竞争性行为的多样性。摘要未提供具体数值指标，但强调了模式的异质性，并通过跨模型比较突出了不同行为策略的差异。",
      "conclusion": "该论文的主要贡献是首次系统性地揭示了大型语言模型在多智能体环境中的嫉妒类似偏好，强调了竞争倾向作为系统设计和安全的关键考虑因素。学术上，它扩展了对LLMs社会行为的理解，并为多智能体心理学提供了新视角；实际上，这些发现有助于优化AI系统的协作性和安全性，减少潜在的负面互动。局限性包括摘要未明确说明具体模型或数据集的细节，未来工作可进一步探索嫉妒行为的内部机制和更广泛的应用场景。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Systems",
        "Envy-Like Preferences",
        "Psychometric Evaluation",
        "Social Comparison"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:21.657833Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.12469",
    "title": "Sparse Concept Anchoring for Interpretable and Controllable Neural Representations",
    "authors": [
      "Sandy Fraser",
      "Patryk Wielopolski"
    ],
    "abstract": "We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.12469.pdf",
    "abs_url": "https://arxiv.org/abs/2512.12469",
    "published": "2025-12-13T21:43:17Z",
    "updated": "2026-01-26T09:03:00Z",
    "comment": "8 pages, 3 figures, 1 table (main text). v2: Renamed sections 3.2, 3.3; Length reduction without substantial changes",
    "light_analysis": {
      "overview": "论文提出稀疏概念锚定方法，通过最小监督偏置潜在空间，实现神经表示的可解释和可控性，核心在于稀疏锚定和干预技术。",
      "motivation": "该研究旨在解决神经网络学习表示缺乏可解释性和可控性的问题。在实际应用中，如安全关键领域，现有方法往往需要大量标注数据或难以进行精确干预，导致模型透明度不足和行为调控困难。可解释AI对于理解和控制模型决策至关重要，特别是在伦理合规和人类监督场景下。本工作通过开发一种高效方法，使少数关键概念在潜在空间中被锚定，从而允许用户以最小监督成本进行干预，并提升模型的可操作性和安全性。",
      "method": "研究方法采用训练时结合激活归一化、分离正则化器以及锚定或子空间正则化器的策略。这些正则化器利用极少数标注样本（每个锚定概念<0.1%的样本），将样本吸引到预定义方向或轴对齐子空间中，从而偏置潜在空间以定位目标概念，同时让其他概念自组织。关键创新点包括稀疏概念锚定，以及基于锚定几何的两种干预方式：推理时可逆行为引导，通过投影出概念潜在成分；和永久移除，通过靶向权重消融锚定维度。实验基于结构化自编码器进行验证。",
      "result": "在结构化自编码器上的实验结果表明，稀疏概念锚定能够选择性衰减目标概念，对正交特征的影响可忽略不计，且不影响整体模型性能。通过永久移除锚定维度，目标概念可被完全消除，重构误差接近理论下界。这些效果与基线方法相比，显示出高效性和精确控制能力，验证了方法在保持表示质量的同时实现可解释和可控行为的优势，具体表现为衰减和消除操作的有效性，无需额外监督或损害其他特征。",
      "conclusion": "结论是稀疏概念锚定为学习表示的可解释和可控行为提供了实用途径，通过最小监督和创新正则化策略提升神经网络的透明度和干预能力。学术价值在于提出了一种新表示学习方法，增强了模型的可操作性和解释性；实际应用中，该方法可改进AI系统的安全性、伦理合规性和用户体验。未来工作可能涉及扩展到更多数据集和模型架构，或探索更多类型的干预和控制机制，以进一步验证其泛化性和效率。",
      "tags": [
        "Sparse Concept Anchoring",
        "Latent Space Manipulation",
        "Regularization Techniques",
        "Interpretable Neural Networks",
        "Weight Ablation"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:32.880599Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.11654",
    "title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation",
    "authors": [
      "Luca Cazzola",
      "Ahed Alboody"
    ],
    "abstract": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at https://lucazzola.github.io/publications/kinemic.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.11654.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11654",
    "published": "2025-12-12T15:32:28Z",
    "updated": "2026-01-26T11:40:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出KineMIC框架，通过文本到运动的蒸馏实现少样本动作合成，解决人类活动识别中的数据获取瓶颈。",
      "motivation": "人类活动识别（HAR）需要大量标注的运动数据，但采集成本高昂，限制了模型性能的提升。现有文本到运动（T2M）生成模型虽能提供合成数据，但其训练侧重于通用艺术运动，与HAR所需的运动学精确、类别可区分的动作存在差异，导致显著的领域差距，使通用T2M模型无法有效生成适合HAR分类器的动作。因此，需要一种方法在数据稀缺条件下弥合这一差距，以支持少样本学习场景，减少对昂贵数据的依赖。",
      "method": "KineMIC框架采用迁移学习策略，将一个通用T2M扩散模型适应到HAR领域。其核心创新是kinetic mining策略，利用CLIP文本嵌入建立稀疏HAR标签与T2M源数据之间的语义对应，为运动学蒸馏提供软监督。通过这种对应关系指导模型的微调过程，将通用T2M主干转换为专用的少样本动作到运动生成器。实验中使用HumanML3D作为源T2M数据集，NTU RGB+D 120的子集作为目标HAR域，每个动作类别仅随机选取10个样本，实现高效适应。",
      "result": "在实验中，KineMIC生成的合成动作比基线方法更加连贯和逼真，显著提升了HAR分类器的性能。具体表现为，作为数据增强源，该方法带来了23.1%的准确率提升，证明了其在少样本条件下的有效性。通过使用少量样本（每个类别10个），框架在NTU RGB+D 120数据集上实现了鲁棒的生成效果，为HAR任务提供了高质量的数据补充，有效解决了数据稀缺问题。",
      "conclusion": "KineMIC框架的主要贡献是通过迁移学习和文本蒸馏实现了少样本动作合成，为HAR领域的数据增强提供了创新方案。其学术价值在于弥合了T2M生成模型与HAR需求之间的领域差距，推动了数据高效学习的发展；实际应用上，可降低HAR系统的数据采集成本，促进模型部署。未来工作可能包括扩展到更多动作类别或集成其他生成模型，以进一步提升泛化能力，摘要未明确说明具体局限性。",
      "tags": [
        "Text-to-Motion (T2M)",
        "Diffusion Model",
        "Few-Shot Learning",
        "CLIP Embeddings",
        "Action Synthesis"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:40.739048Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.15765",
    "title": "Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic",
    "authors": [
      "Mélissa Tamine",
      "Otmane Sakhi",
      "Benjamin Heymann"
    ],
    "abstract": "Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.",
    "categories": [
      "cs.LG",
      "cs.GT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.15765.pdf",
    "abs_url": "https://arxiv.org/abs/2512.15765",
    "published": "2025-12-12T10:13:54Z",
    "updated": "2026-01-26T16:21:48Z",
    "comment": "11 pages, 2 figures",
    "light_analysis": {
      "overview": "本文提出了一种基于直接偏好优化数学结构的高效Shapley值近似方法，用于大型语言模型微调中的数据估值，显著降低计算成本。",
      "motivation": "在AI训练中，数据是核心资源，尤其对于大型语言模型的微调，数据所有者需优化投资策略并公平分配合作收益。数据估值问题在机器学习中常通过合作博弈论和Shapley值解决，但计算Shapley值需要大量模型重新训练，成本高昂，成为实际应用瓶颈，尤其对于大型模型，限制了数据高效利用和利益公平分配。本研究旨在简化这一过程，以支持数据投资决策和协作模型训练。",
      "method": "本文利用直接偏好优化训练大型语言模型的特定数学结构，简化了Shapley值计算。通过语言模型算术，实现了可扩展的数据估值方法，无需传统的大量模型重新训练。关键创新点在于发现DPO的数学形式使数据贡献评估更高效，降低了计算复杂度，为数据估值在LLM微调中的应用提供了新的技术路线。摘要未明确说明具体数据集或模型架构细节，但突出了方法的可扩展性。",
      "result": "摘要未明确说明具体实验结果，如准确率、效率提升等性能指标。论文声称通过DPO的数学结构，Shapley值计算挑战被显著简化，从而降低了计算成本，但未提供与基线方法的对比数据或量化改进。研究基于理论分析，强调了方法在数据估值与大型语言模型交叉领域的潜在应用价值，但实际效果需参考全文的实证部分。",
      "conclusion": "本研究的主要贡献在于揭示直接偏好优化的数学结构能高效简化数据估值中的Shapley值计算，降低了成本。这为数据所有者合作训练更优模型并公平分配利益提供了新途径，推动了数据估值与大型语言模型应用的结合。学术上，拓展了合作博弈论在AI领域的应用；实践上，支持数据投资决策和协作创新。局限性包括摘要未详述具体实现和验证，未来工作可探索该方法的实际部署和扩展到其他模型训练场景。",
      "tags": [
        "Data Valuation",
        "Shapley Value",
        "Direct Preference Optimization",
        "Language Model Arithmetic",
        "LLM Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:42.367530Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.10408",
    "title": "MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos",
    "authors": [
      "Qiyue Sun",
      "Tailin Chen",
      "Yinghui Zhang",
      "Yuchen Zhang",
      "Jiangbei Yue",
      "Jianbo Jiao",
      "Zeyu Fu"
    ],
    "abstract": "The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.10408.pdf",
    "abs_url": "https://arxiv.org/abs/2512.10408",
    "published": "2025-12-11T08:18:22Z",
    "updated": "2026-01-26T12:39:05Z",
    "comment": "In Proceedings of the ACM Web Conference 2026 (WWW 2026)",
    "light_analysis": {
      "overview": "提出首个弱监督多模态仇恨内容时间定位框架MultiHateLoc，通过动态跨模态融合和模态感知机制实现精细预测。",
      "motivation": "视频平台如TikTok和YouTube上多模态仇恨言论的快速增长加剧了有害内容传播，其中视觉、听觉和文本流中的仇恨线索往往以微妙异步方式出现。现有研究主要集中于视频级分类，缺乏对仇恨段落发生时间进行定位的实践关键任务，这一问题在弱监督下尤其突出，仅依赖视频级标签导致传统静态融合或分类架构难以捕捉跨模态和时间动态，限制了检测效率和准确性。",
      "method": "MultiHateLoc框架包括模态感知时间编码器以建模异构序列模式，并设计文本预处理模块增强特征；采用动态跨模态融合自适应强调每个时刻信息量最大的模态，结合跨模态对比对齐策略提升多模态特征一致性；引入模态感知MIL目标在视频级监督下识别判别性片段，从而生成细粒度、可解释的帧级预测，无需额外标注。",
      "result": "在HateMM和MultiHateClip数据集上的实验表明，MultiHateLoc在时间定位任务中实现了最先进性能，显著优于现有基线方法。尽管摘要未提供具体数值如准确率提升，但结果证实该方法在弱监督条件下能够有效定位仇恨内容，生成精细的预测，展示出在多模态环境中的优越性。",
      "conclusion": "论文的主要贡献是提出了首个弱监督多模态仇恨内容时间定位框架MultiHateLoc，推动了在线视频检测领域的研究进展。该研究具有重要实际应用价值，可帮助平台实时识别仇恨内容以减少传播，但摘要未明确说明局限性或未来工作方向，可能涉及扩展至更多模态或监督场景的探索。",
      "tags": [
        "Weakly-Supervised Learning",
        "Multimodal Fusion",
        "Temporal Localisation",
        "Contrastive Learning",
        "Multiple Instance Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:37.024068Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.07076",
    "title": "Context-measure: Contextualizing Metric for Camouflage",
    "authors": [
      "Chen-Yang Wang",
      "Gepeng Ji",
      "Song Shao",
      "Ming-Ming Cheng",
      "Deng-Ping Fan"
    ],
    "abstract": "Camouflage is primarily context-dependent yet current metrics for camouflaged scenarios overlook this critical factor. Instead, these metrics are originally designed for evaluating general or salient objects, with an inherent assumption of uncorrelated spatial context. In this paper, we propose a new contextualized evaluation paradigm, Context-measure, built upon a probabilistic pixel-aware correlation framework. By incorporating spatial dependencies and pixel-wise camouflage quantification, our measure better aligns with human perception. Extensive experiments across three challenging camouflaged object segmentation datasets show that Context-measure delivers more reliability than existing context-independent metrics. Our measure can provide a foundational evaluation benchmark for various computer vision applications involving camouflaged patterns, such as agricultural, industrial, and medical scenarios. Code is available at https://github.com/pursuitxi/Context-measure.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.07076.pdf",
    "abs_url": "https://arxiv.org/abs/2512.07076",
    "published": "2025-12-08T01:23:28Z",
    "updated": "2026-01-26T09:52:42Z",
    "comment": "Technical Report",
    "light_analysis": {
      "overview": "本文提出了一种基于概率像素感知相关框架的上下文化度量方法Context-measure，用于改善伪装对象分割的评估。",
      "motivation": "伪装现象高度依赖于上下文，但现有的评估度量标准忽视了这一关键因素。这些度量标准原本是为评估一般或显著对象设计的，隐含假设空间上下文不相关，导致在伪装场景中评估不准确，限制了计算机视觉应用的发展。该研究旨在解决这一问题，通过引入上下文感知的度量方法，提高评估的可靠性和与人类感知的一致性，为农业、工业和医疗等领域的伪装模式分析提供更好的基准。",
      "method": "本研究提出了一种新的上下文化评估范式Context-measure，建立在概率像素感知相关框架之上。该方法通过整合空间依赖性和像素级伪装量化，更好地模拟人类对伪装对象的感知。关键创新在于将上下文信息编码到度量中，使用概率模型来计算像素间的相关性。尽管摘要未详细说明具体的数据集名称和模型架构，但提及了在三个伪装对象分割数据集上进行实验，表明该方法具有通用性和适应性。",
      "result": "通过在三个挑战性的伪装对象分割数据集上进行广泛实验，Context-measure相较于现有的上下文无关度量标准展现出更高的可靠性。实验结果验证了该方法在评估伪装场景时的优越性，尽管摘要未提供具体的准确率或效率数值，但强调了其与人类感知的一致性。这表明Context-measure能够更有效地捕捉伪装对象的上下文特征，为后续研究提供了可靠的评估工具。",
      "conclusion": "本文的主要贡献是提出了Context-measure作为一种新的上下文化度量方法，用于伪装对象分割的评估。该方法不仅改进了现有度量标准的不足，还为涉及伪装模式的计算机视觉应用提供了基础评估基准，具有广泛的学术和实际价值。然而，摘要未明确说明该方法的局限性或未来工作方向，这可能是未来研究需要探索的领域，例如扩展应用到更多场景或优化计算效率。",
      "tags": [
        "Camouflaged Object Segmentation",
        "Probabilistic Framework",
        "Contextual Metric",
        "Pixel-wise Quantification"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:01.692855Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.01289",
    "title": "OntoMetric: An Ontology-Driven LLM-Assisted Framework for Automated ESG Metric Knowledge Graph Generation",
    "authors": [
      "Mingqin Yu",
      "Fethi Rabhi",
      "Boming Xia",
      "Zhengyi Yang",
      "Felix Tan",
      "Qinghua Lu"
    ],
    "abstract": "Environmental, Social, and Governance (ESG) metric knowledge is inherently structured, connecting industries, reporting frameworks, metric categories, metrics, and calculation models through compositional dependencies, yet in practice this structure remains embedded implicitly in regulatory documents such as SASB, TCFD, and IFRS S2 and rarely exists as an explicit, governed, or machine-actionable artefact. Existing ESG ontologies define formal schemas but do not address scalable population and governance from authoritative regulatory sources, while unconstrained large language model (LLM) extraction frequently produces semantically incorrect entities, hallucinated relationships, and structurally invalid graphs. OntoMetric is an ontology-guided framework for the automated construction and governance of ESG metric knowledge graphs from regulatory documents that operationalises the ESG Metric Knowledge Graph (ESGMKG) ontology as a first-class constraint embedded directly into the extraction and population process. The framework integrates structure-aware segmentation, ontology-constrained LLM extraction enriched with semantic fields and deterministic identifiers, and two-phase validation combining semantic type verification with rule-based schema checking, while preserving segment-level and page-level provenance to ensure traceability to regulatory source text. Evaluation on five ESG regulatory standards shows that ontology-guided extraction achieves 65-90 percent semantic accuracy and over 80 percent schema compliance, compared with 3-10 percent for unconstrained baseline extraction, and yields stable cost efficiency with a cost per validated entity of 0.01-0.02 USD and a 48 times efficiency improvement over baseline.",
    "categories": [
      "cs.AI",
      "cs.GR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2512.01289.pdf",
    "abs_url": "https://arxiv.org/abs/2512.01289",
    "published": "2025-12-01T05:21:22Z",
    "updated": "2026-01-26T09:28:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了OntoMetric框架，一种本体引导的大型语言模型辅助方法，用于从监管文档自动构建ESG度量知识图，以提高提取准确性和治理效率。",
      "motivation": "ESG度量知识虽然本质结构化，但在实践中隐含在监管文档中，缺乏显式、可治理的知识图，阻碍了机器自动化处理。现有方法如正式本体未解决从权威源的可扩展人口问题，而无约束大语言模型提取常产生语义错误、幻觉关系和无效结构，因此需要一种可靠、高效的自动构建框架以支持ESG报告和分析应用。",
      "method": "OntoMetric框架采用本体引导的大语言模型提取技术，结合结构感知分割、语义字段和确定性标识符丰富提取过程，并实施两阶段验证（语义类型验证与规则检查）。关键创新是将ESGMKG本体直接嵌入提取流程作为约束，确保实体和关系准确性，同时保留段级和页级起源以实现可追溯性到源文本。",
      "result": "在五个ESG监管标准评估中，本体引导提取实现65-90%的语义准确性和超过80%的模式合规性，远高于基线无约束提取的3-10%。成本效率表现稳定，每个验证实体成本为0.01-0.02美元，效率比基线提高48倍，证明了该方法在精度和经济性上的显著优势。",
      "conclusion": "论文的主要贡献是开发了OntoMetric框架，它通过本体约束和大语言模型结合，实现了高精度和合规性的ESG知识图自动构建，具有实际应用价值，支持自动化ESG报告和分析。未来工作可扩展更多监管源或优化验证机制以提升适应性。",
      "tags": [
        "Ontology-Driven Framework",
        "Large Language Model",
        "Knowledge Graph Generation",
        "Semantic Validation",
        "Rule-Based Checking"
      ]
    },
    "analyzed_at": "2026-01-27T03:41:58.517692Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.18413",
    "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
    "authors": [
      "Yu Xia",
      "Sungchul Kim",
      "Tong Yu",
      "Ryan A. Rossi",
      "Julian McAuley"
    ],
    "abstract": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.18413.pdf",
    "abs_url": "https://arxiv.org/abs/2511.18413",
    "published": "2025-11-23T11:57:10Z",
    "updated": "2026-01-26T05:57:33Z",
    "comment": "WWW 2026",
    "light_analysis": {
      "overview": "提出了多代理协同过滤框架（MACF），通过动态编排用户和物品代理，改进代理推荐系统的性能。",
      "motivation": "当前代理推荐系统基于大型语言模型（LLM）代理，但大多数关注于通用单代理计划-执行工作流或多代理任务分解管道。由于缺乏推荐导向的设计，它们未能充分利用用户-物品交互历史中的协同信号，导致推荐结果不理想。这限制了推荐系统的准确性和个性化程度，影响了用户体验和应用效果。因此，研究动机是解决现有代理推荐系统在利用协同信号方面的不足，以提高推荐质量。",
      "method": "MACF框架通过将传统协同过滤中的用户和物品类比为LLM代理来构建多代理系统。具体地，针对目标用户和查询，实例化相似用户和物品作为具有独特配置的代理，每个代理能调用检索工具、建议候选项并与其他代理交互。关键创新在于引入中央编排器代理，通过动态代理招募和个性化协作指令，自适应地管理代理间的协作，从而替代传统协同过滤中的静态偏好聚合。该方法在多个领域的数据集上进行实验验证。",
      "result": "在三个不同领域的数据集上的实验结果表明，MACF框架相较于强大的代理推荐基线方法显示出显著优势。虽然摘要未明确说明具体性能指标（如准确率或效率提升），但实验验证了该框架在利用协同信号和改进推荐效果方面的有效性。与基线方法相比，MACF框架通过动态协作机制实现了更好的推荐性能，尽管具体数据未在摘要中提供。",
      "conclusion": "MACF框架的主要贡献在于将传统协同过滤思想引入LLM代理推荐系统，通过多代理动态协作提升推荐准确性。该研究具有学术价值，为代理推荐系统提供了新的设计思路，并促进LLM与推荐系统的结合。潜在局限性可能包括计算复杂度或代理交互效率，未来工作可进一步优化代理协作机制或扩展到更多场景，如增强可扩展性或整合更多领域知识。",
      "tags": [
        "Large Language Model (LLM)",
        "Multi-Agent Systems",
        "Collaborative Filtering",
        "Agentic Recommendations"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:21.629315Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.18247",
    "title": "Tail Distribution of Regret in Optimistic Reinforcement Learning",
    "authors": [
      "Sajad Khodadadian",
      "Mehrdad Moharrami"
    ],
    "abstract": "We derive instance-dependent tail bounds for the regret of optimism-based reinforcement learning in finite-horizon tabular Markov decision processes with unknown transition dynamics. Focusing on a UCBVI-type algorithm, we characterize the tail distribution of the cumulative regret $R_K$ over $K$ episodes, rather than only its expectation or a single high-probability quantile. We analyze two natural exploration-bonus schedules: (i) a $K$-dependent scheme that explicitly incorporates the total number of episodes $K$, and (ii) a $K$-independent scheme that depends only on the current episode index. For both settings, we obtain an upper bound on $\\Pr(R_K \\ge x)$ that exhibits a distinctive two-regime structure: a sub-Gaussian tail starting from an instance-dependent scale $m_K$ up to a transition threshold, followed by a sub-Weibull tail beyond that point. We further derive corresponding instance-dependent bounds on the expected regret $\\mathbb{E}[R_K]$. The proposed algorithm depends on a tuning parameter $α$, which balances the expected regret and the range over which the regret exhibits a sub-Gaussian tail. To the best of our knowledge, our results provide one of the first comprehensive tail-regret guarantees for a standard optimistic algorithm in episodic reinforcement learning.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.18247.pdf",
    "abs_url": "https://arxiv.org/abs/2511.18247",
    "published": "2025-11-23T02:23:09Z",
    "updated": "2026-01-26T04:16:22Z",
    "comment": "17 pages, 0 figures",
    "light_analysis": {
      "overview": "本文首次全面分析了乐观强化学习算法的遗憾尾部分布，揭示了其子高斯和子Weibull两阶段结构，并提供了实例依赖的上下界。",
      "motivation": "在强化学习中，评估算法性能通常通过遗憾衡量，但现有研究多集中于期望遗憾或单点高概率界，缺乏对遗憾分布全貌的深入分析。特别是在乐观强化学习（如基于UCBVI算法）场景中，面对未知转移动态的有限时域表格马尔可夫决策过程，准确刻画遗憾尾部分布对于理解算法鲁棒性和风险评估至关重要。本研究旨在弥补这一空白，通过推导实例依赖的尾部界，为算法性能提供更全面的理论保障。",
      "method": "论文提出了一种基于UCBVI-type的乐观强化学习算法，应用于有限时域表格马尔可夫决策过程。核心方法涉及分析两种自然探索策略：一种是依赖总episodes数K的K-依赖方案，另一种是仅依赖当前episode索引的K-独立方案。通过数学推导，得出遗憾分布的上界，该界展示了独特的两阶段结构：在实例依赖尺度m_K内为子高斯尾部，超过阈值后转为子Weibull尾部。算法引入调优参数α，用于平衡期望遗憾和子高斯尾部的范围，这是创新技术特色之一。",
      "result": "主要实验结果显示，对于所提出的算法，遗憾分布的上界呈现出两阶段结构：从实例依赖尺度m_K开始为子高斯尾部，直到一个过渡阈值，之后变为子Weibull尾部。这提供了比传统期望或单点界更全面的性能保障。同时，论文推导了期望遗憾的实例依赖上界，表明算法在控制风险方面优于仅关注期望的基线方法。具体数据指标未明确说明，但分析强调了这些结果为标准乐观算法在episodic强化学习中首次提供了尾部遗憾保证。",
      "conclusion": "本研究的主要贡献在于首次为乐观强化学习算法提供了全面的遗憾尾部分布分析，揭示了其两阶段结构，这在学术上深化了对算法性能分布的理论理解，丰富了强化学习领域的理论框架。实际应用中，这有助于设计更鲁棒、风险可控的算法，对自主系统等高风险场景有潜在价值。未来工作方向可能包括扩展到非表格环境或探索其他探索策略的优化。",
      "tags": [
        "Optimistic Reinforcement Learning",
        "Regret Distribution",
        "UCBVI",
        "Tail Bounds",
        "Episodic MDPs"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:14.497555Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.15552",
    "title": "Multimodal Evaluation of Russian-language Architectures",
    "authors": [
      "Artem Chervyakov",
      "Ulyana Isaeva",
      "Anton Emelyanov",
      "Artem Safin",
      "Maria Tikhonova",
      "Alexander Kharitonov",
      "Yulia Lyakh",
      "Petr Surovtsev",
      "Denis Shevelev",
      "Vildan Saburov",
      "Vasily Konovalov",
      "Elisei Rykov",
      "Ivan Sviridov",
      "Amina Miftakhova",
      "Ilseyar Alimova",
      "Alexander Panchenko",
      "Alexander Kapitanov",
      "Alena Fenogenova"
    ],
    "abstract": "Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce MERA Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (imageto-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.15552.pdf",
    "abs_url": "https://arxiv.org/abs/2511.15552",
    "published": "2025-11-19T15:43:53Z",
    "updated": "2026-01-26T07:50:42Z",
    "comment": "EACL main track",
    "light_analysis": {
      "overview": "论文提出MERA Multi，一个开放的多模态评估框架，针对俄语环境填补了多模态基准空白，包括新数据集和评估任务。",
      "motivation": "多模态大语言模型（MLLMs）虽快速发展，但其智能、局限性和风险评估不足，尤其在俄语环境下缺乏标准多模态基准，阻碍了相关研究和应用改进。现有方法缺乏针对俄语文化和语言特性的基准，导致模型评估不全面。",
      "method": "研究提出MERA Multi框架，基于指令设计，涵盖文本、图像、音频和视频四种模态，包括18个新构建的评估任务，用于通用模型和模态特定架构。关键创新包括：创建多模态能力通用分类；从头构建注重俄语文化特异性的数据集；采用水印等防基准泄漏方法。",
      "result": "论文提供了闭源和开源模型的基线评估结果，但摘要未明确说明具体性能指标如准确率或效率提升。通过初步比较展示了基准的适用性，与现有方法相比，该框架为俄语多模态评估提供了标准化起点。",
      "conclusion": "主要贡献是建立多模态评估框架，包括通用分类、文化特异性数据集和防泄漏方法，为斯拉夫语族语言提供可复制基准构建方法。研究价值在于推动多模态AI在非英语环境的应用，局限性是当前仅聚焦俄语，未来可扩展到其他语言。",
      "tags": [
        "Multimodal Large Language Models",
        "Benchmark Evaluation",
        "Russian Language Processing",
        "Dataset Creation",
        "Instruction-based Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:12.180228Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.15169",
    "title": "SafeRBench: Dissecting the Reasoning Safety of Large Language Models",
    "authors": [
      "Xin Gao",
      "Shaohan Yu",
      "Zerui Chen",
      "Yueming Lyu",
      "Weichen Yu",
      "Guanghao Li",
      "Jiyao Liu",
      "Jianxiong Gao",
      "Jian Liang",
      "Ziwei Liu",
      "Chenyang Si"
    ],
    "abstract": "Large Reasoning Models (LRMs) have significantly improved problem-solving through explicit Chain-of-Thought (CoT) reasoning. However, this capability creates a Safety-Helpfulness Paradox: the reasoning process itself can be misused to justify harmful actions or conceal malicious intent behind lengthy intermediate steps. Most existing benchmarks only check the final output, missing how risks evolve, or ``drift'', during the model's internal reasoning. To address this, we propose SafeRBench, the first framework to evaluate LRM safety end-to-end, from the initial input to the reasoning trace and final answer. Our approach introduces: (i) a Risk Stratification Probing that uses specific risk levels to stress-test safety boundaries beyond simple topics; (ii) Micro-Thought Analysis, a new chunking method that segments traces to pinpoint exactly where safety alignment breaks down; and (iii) a comprehensive suite of 10 fine-grained metrics that, for the first time, jointly measure a model's Risk Exposure (e.g., risk level, execution feasibility) and Safety Awareness (e.g., intent awareness). Experiments on 19 LRMs reveal that while enabling Thinking modes improves safety in mid-sized models, it paradoxically increases actionable risks in larger models due to a strong always-help tendency.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.15169.pdf",
    "abs_url": "https://arxiv.org/abs/2511.15169",
    "published": "2025-11-19T06:46:33Z",
    "updated": "2026-01-26T15:12:11Z",
    "comment": "29 pages, 8 figures",
    "light_analysis": {
      "overview": "SafeRBench框架首次实现大推理模型安全性端到端评估，通过风险分层探测和微思想分析揭示推理过程中的安全漏洞。",
      "motivation": "大推理模型通过显式思维链推理提升问题解决能力，但引入了安全性-帮助性悖论，即推理过程可能被滥用以合理化有害行为或掩盖恶意意图。随着模型在关键应用中的部署，确保推理安全变得至关重要。现有安全性基准主要检查最终输出，忽略了风险在推理轨迹中的演变过程，导致评估不全面，无法捕捉内部推理的风险漂移或潜在危害，这一问题亟待解决。",
      "method": "论文提出SafeRBench框架，包含三个核心组件：风险分层探测，使用具体风险级别对模型进行压力测试，超越简单话题以探究安全边界；微思想分析，一种新的分块方法，将推理轨迹分段以精确识别安全对齐失效点，揭示风险积累过程；以及一套10个细粒度指标，首次共同衡量风险暴露（如风险级别、执行可行性）和安全意识（如意图感知），在19个大推理模型上应用该方法进行全面评估。",
      "result": "在19个大推理模型上的实验揭示，启用思维模式在中等规模模型中能提高安全性，但在更大模型中由于强烈的总是帮助倾向，反而增加了可操作风险，表明安全性与模型规模存在矛盾关系。SafeRBench通过端到端评估提供了更全面的安全性洞察，与传统只检查输出的基准相比，能更精确地度量风险演变，但具体性能指标（如准确率提升）在摘要中未明确说明，侧重于揭示趋势而非数值比较。",
      "conclusion": "本研究的主要贡献是开发了SafeRBench，首个端到端评估大推理模型安全性的框架，填补了现有基准忽略推理过程风险的空白。学术上，它为理解模型安全性提供了新工具，促进对推理机制的深入研究；实际中，有助于构建更安全的AI系统，减少有害行为风险。未来工作可包括扩展到更多模型类型、优化评估指标，以及探索缓解策略以提高模型鲁棒性。",
      "tags": [
        "Large Reasoning Models",
        "Chain-of-Thought Reasoning",
        "Safety Evaluation",
        "Risk Stratification",
        "Micro-Thought Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:42.479122Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.14702",
    "title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images",
    "authors": [
      "Farheen Ramzan",
      "Yusuf Kiberu",
      "Nikesh Jathanna",
      "Meryem Jabrane",
      "Vicente Grau",
      "Shahnaz Jamil-Copley",
      "Richard H. Clayton",
      "Chen",
      "Chen"
    ],
    "abstract": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.14702.pdf",
    "abs_url": "https://arxiv.org/abs/2511.14702",
    "published": "2025-11-18T17:42:20Z",
    "updated": "2026-01-26T13:39:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一个整合心电图和解剖先验知识的多模态框架，通过时间感知特征融合机制，提升心肌疤痕分割的准确性和生理一致性。",
      "motivation": "心肌疤痕的准确分割对于评估心脏组织存活性至关重要，但在延迟钆增强 MRI 中，由于图像对比度可变和伪影存在，分割任务面临挑战。现有方法如基于图像的 nnU-Net 虽然有效，但忽略了心电图信号等生理信息，而这些信息可以通过传导异常帮助定位疤痕区域。然而，ECG 和 MRI 通常非同时采集，导致数据融合困难，因此本研究旨在通过整合生理和解剖知识来克服这些限制，提高分割性能。",
      "method": "本研究提出了一种多模态框架，整合来自心电图的电生理信息和 AHA-17 图集的解剖先验知识，用于进行生理一致的 LGE 疤痕分割。由于 ECG 和 LGE-MRI 非同时采集，引入了时间感知特征融合机制，该机制基于采集时间差动态加权和融合特征。在方法实现中，可能基于临床数据集进行评估，并利用深度学习架构，具体细节如模型结构在摘要中未明确说明，但提及了 nnU-Net 作为基线，TAFF 机制允许模型有效处理时间差异。",
      "result": "在临床数据集上的评估显示，该方法相比仅图像的基线方法 nnU-Net 有显著提升。疤痕分割的平均 Dice 分数从 0.6149 增加到 0.8463，精确度达到 0.9115，灵敏度为 0.9043。这些结果表明，整合生理和解剖知识显著提高了分割性能，不仅在整体准确性上改善，还在精确检测和灵敏性方面表现出色，验证了方法的有效性。",
      "conclusion": "本研究的主要贡献在于提出了一种整合生理和解剖知识的多模态框架，通过时间感知特征融合机制，使模型能够“超越图像”，实现更稳健和生理基础的心肌疤痕分割。这为心脏图像分析设定了新方向，强调了多模态信息融合的重要性。尽管未明确说明局限性，但该方法可能依赖于 ECG 和 MRI 数据的可用性，未来工作可探索更多生理信号或扩展到其他心脏疾病。",
      "tags": [
        "Multimodal Learning",
        "ECG Signal Processing",
        "Temporal Feature Fusion",
        "Myocardial Scar Segmentation",
        "Late Gadolinium Enhancement"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:56.571232Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.12869",
    "title": "On the Fundamental Limits of LLMs at Scale",
    "authors": [
      "Muhammad Ahmed Mohsin",
      "Muhammad Umer",
      "Ahsan Bilal",
      "Zeeshan Memon",
      "Muhammad Ibtsaam Qadir",
      "Sagnik Bhattacharya",
      "Hassan Rizwan",
      "Abhiram R. Gorle",
      "Maahe Zehra Kazmi",
      "Nukhba Amir",
      "Ali Subhan",
      "Muhammad Usman Rafique",
      "Zihao He",
      "Pulkit Mehta",
      "Muhammad Ali Jamshed",
      "John M. Cioffi"
    ],
    "abstract": "Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.IT",
      "cs.MA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.12869.pdf",
    "abs_url": "https://arxiv.org/abs/2511.12869",
    "published": "2025-11-17T01:55:33Z",
    "updated": "2026-01-26T05:53:56Z",
    "comment": "Submitted to TMLR 2025",
    "light_analysis": {
      "overview": "这篇论文提出一个统一的、证明驱动的理论框架，揭示大型语言模型扩展中五个基本限制的固有理论天花板。",
      "motivation": "LLMs在扩展过程中面临幻觉、上下文压缩、推理退化、检索脆弱性和多模态不对齐等五个基本限制，这些限制了模型的实际应用效果。现有研究通常对这些现象进行经验性描述，但缺乏系统化的理论综合，未能将它们与计算、信息和学习的基础极限联系起来。这导致了对扩展收益的盲目乐观，忽视了潜在的理论瓶颈，因此需要一个严谨的理论框架来深入理解这些限制的根源，并为改进方法提供指导。",
      "method": "论文提出了一个理论框架，结合计算理论、信息理论和几何效应来形式化LLM的扩展极限。关键创新点包括使用可计算性理论证明不可约误差的存在（如对角线论证），信息理论约束描述压缩误差和样本复杂性，以及分析几何效应如位置训练不足、编码衰减和注意力机制压缩。框架不依赖特定数据集或模型架构，而是基于数学证明和一般原则，涵盖似然训练偏好模式完成而非推理、检索在令牌限制下的语义漂移，以及多模态扩展的浅层对齐。",
      "result": "论文通过理论证明和实证证据表明，扩展在某些任务上可以提高性能，但在其他方面会饱和或无法进展。具体地，计算不可计算性导致不可约误差（如不可判定查询），信息理论限制绑定可达到的准确性，几何效应压缩上下文低于名义大小。与基线方法相比，该框架提供了理论依据，指出了现有LLMs的局限性，但摘要未明确说明具体实验数据如准确率提升，侧重于理论分析和框架验证。",
      "conclusion": "该研究的主要贡献是提供了一个统一的理论基础，解释LLM扩展的限制，并提出了实践缓解路径，如边界检索、位置课程和稀疏注意力。这具有重要的学术价值，连接了计算、信息和学习理论等多个领域，并为未来研究提供了方向，如进一步实证验证框架应用和开发新型训练方法，但摘要未明确说明具体局限性，潜在未来工作可能包括更深入的实验验证。",
      "tags": [
        "Large Language Models",
        "Theoretical Framework",
        "Context Compression",
        "Retrieval Mechanisms",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:41.911362Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.11233",
    "title": "STaR: Towards Effective and Stable Table Reasoning via Slow-Thinking Large Language Models",
    "authors": [
      "Huajian Zhang",
      "Mingyue Cheng",
      "Yucong Luo",
      "Xiaoyu Tao"
    ],
    "abstract": "Table reasoning with large language models (LLMs) plays a critical role in building intelligent systems capable of understanding and analyzing tabular data. Despite recent progress, existing methods still face key limitations: their reasoning processes lacks depth and explicit multi-step reasoning, often relying solely on implicit language model understanding. In addition, their reasoning processes suffer from instability, primarily caused by model uncertainty. In this work, we propose STaR, a novel slow-thinking model that can achieve effective and stable table reasoning. To enable effective multi-step reasoning, we design a two-stage training framework consisting of supervised fine-tuning (SFT) warm-up followed by reinforced fine-tuning (RFT). Specifically, in the SFT stage, we construct a high-quality dataset through automatic self-verification. In the RFT stage, we introduce a difficulty-aware reinforcement learning mechanism to further enhance reasoning capabilities. Furthermore, to improve reasoning stability, we introduce trajectory-level uncertainty quantification, which fuses token-level confidence with answer-level consistency, enabling the selection of better reasoning trajectories. Extensive experiments demonstrate that STaR-8B achieves state-of-the-art performance on in-domain benchmarks and exhibits strong generalization to out-of-domain datasets, highlighting its potential for enhancing both effectiveness and stability in table reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.11233.pdf",
    "abs_url": "https://arxiv.org/abs/2511.11233",
    "published": "2025-11-14T12:34:17Z",
    "updated": "2026-01-26T13:35:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "STaR模型通过慢思维方法、两阶段训练和不确定性量化，实现了高效且稳定的表格推理。",
      "motivation": "表格推理在构建智能系统中至关重要，能帮助理解和分析表格数据。现有大语言模型方法存在关键限制：推理过程缺乏深度和显式多步推理，仅依赖隐式理解，且由于模型不确定性导致推理不稳定，影响应用效果和可靠性。本研究旨在解决这些问题，提升表格推理的深度和稳定性，弥补现有方法的不足。",
      "method": "STaR采用两阶段训练框架：首先通过监督微调热身，利用自动自验证构建高质量数据集；然后进行强化微调，引入难度感知的强化学习机制来增强多步推理能力。为提高稳定性，提出轨迹级不确定性量化方法，融合词级置信度和答案级一致性，以选择更好的推理轨迹。关键创新在于慢思维模型设计，促进显式多步推理。",
      "result": "实验表明，STaR-8B在领域内基准测试中达到最先进的性能，同时在领域外数据集上表现出强泛化能力，证明了其在提升推理效果和稳定性方面的有效性。通过与基线方法对比，该方法通过改进多步推理和不确定性处理，显著增强了表格推理的可靠性。具体数据未在摘要中明确说明，但结果突显其实际应用潜力。",
      "conclusion": "STaR模型通过慢思维方法、两阶段训练和不确定性量化，显著改善了表格推理的有效性和稳定性。主要贡献在于提出创新训练框架和轨迹级稳定性增强，为智能系统提供了可靠解决方案。研究具有学术价值，推动了大语言模型在复杂推理任务中的应用，未来可扩展至其他推理场景。",
      "tags": [
        "Large Language Model",
        "Table Reasoning",
        "Reinforcement Learning",
        "Uncertainty Quantification",
        "Supervised Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:39.482442Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.08091",
    "title": "Gateways to Tractability for Satisfiability in Pearl's Causal Hierarchy",
    "authors": [
      "Robert Ganian",
      "Marlene Gründel",
      "Simon Wietheger"
    ],
    "abstract": "Pearl's Causal Hierarchy (PCH) is a central framework for reasoning about probabilistic, interventional, and counterfactual statements, yet the satisfiability problem for PCH formulas is computationally intractable in almost all classical settings. We revisit this challenge through the lens of parameterized complexity and identify the first gateways to tractability. Our results include fixed-parameter and XP-algorithms for satisfiability in key probabilistic and counterfactual fragments, using parameters such as primal treewidth and the number of variables, together with matching hardness results that map the limits of tractability. Technically, we depart from the dynamic programming paradigm typically employed for treewidth-based algorithms and instead exploit structural characterizations of well-formed causal models, providing a new algorithmic toolkit for causal reasoning.",
    "categories": [
      "cs.AI",
      "cs.CC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.08091.pdf",
    "abs_url": "https://arxiv.org/abs/2511.08091",
    "published": "2025-11-11T10:45:03Z",
    "updated": "2026-01-26T16:14:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过参数化复杂度，首次发现了Pearl's Causal Hierarchy满足性问题的可处理性路径，并提出了基于结构特征的新算法工具。",
      "motivation": "Pearl's Causal Hierarchy (PCH)作为概率、干预和反事实推理的核心框架，其公式的满足性问题在几乎所有经典计算设置中都是不可解的，这严重限制了因果推理在实际应用中的效率。现有方法在面对复杂因果模型时计算效率低下，难以处理大规模或高维问题，因此亟需找到可处理性条件来突破这一计算瓶颈，推动自动化因果推理的发展。",
      "method": "作者采用参数化复杂度的视角，识别出影响可处理性的关键参数，如原始树宽和变量数量。基于这些参数，提出了针对概率和反事实片段的固定参数算法和XP算法。技术上，作者放弃了传统的动态规划方法，转而利用良好形成因果模型的结构特征，构建了一个新的算法工具箱，为因果推理提供了更高效的解决方案。",
      "result": "研究取得了针对PCH关键片段的算法，并提供了匹配的硬度结果，精确划定了可处理性的界限。摘要未明确说明具体性能指标，如准确率或效率改进，但算法设计表明在特定参数条件下可以实现计算效率的提升，为因果推理的满足性问题提供了首个可处理性理论保证，并通过硬度结果映射了方法的适用范围。",
      "conclusion": "本论文的主要贡献在于首次识别了PCH满足性问题的可处理性网关，并开发了基于结构特征的新算法，具有重要的学术价值，为因果推理理论提供了突破。在实际应用中，可能改进因果模型的自动化推理工具，推动相关领域发展。未来工作可探索更多参数化策略、扩展应用到其他因果推理任务，或应对更复杂的模型设置。",
      "tags": [
        "Parameterized Complexity",
        "Fixed-Parameter Algorithms",
        "Treewidth",
        "Causal Reasoning",
        "Satisfiability"
      ]
    },
    "analyzed_at": "2026-01-27T03:42:48.377304Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.04235",
    "title": "Shared Spatial Memory Through Predictive Coding",
    "authors": [
      "Zhengru Fang",
      "Yu Guo",
      "Jingjing Wang",
      "Yuang Zhang",
      "Haonan An",
      "Yinhai Wang",
      "Wenbo Ding",
      "Yuguang Fang"
    ],
    "abstract": "Constructing a consistent shared spatial memory is a critical challenge in multi-agent systems, where partial observability and limited bandwidth often lead to catastrophic failures in coordination. We introduce a multi-agent predictive coding framework that formulates coordination as the minimization of mutual uncertainty among agents. Through an information bottleneck objective, this framework prompts agents to learn not only who and what to communicate but also when. At the foundation of this framework lies a grid-cell-like metric as internal spatial coding for self-localization, emerging spontaneously from self-supervised motion prediction. Building upon this internal spatial code, agents gradually develop a bandwidth-efficient communication mechanism and specialized neural populations that encode partners' locations-an artificial analogue of hippocampal social place cells (SPCs). These social representations are further utilized by a hierarchical reinforcement learning policy that actively explores to reduce joint uncertainty. On the Memory-Maze benchmark, our approach shows exceptional resilience to bandwidth constraints: success degrades gracefully from 73.5% to 64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically principled and biologically plausible basis for how complex social representations emerge from a unified predictive drive, leading to collective intelligence.",
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.04235.pdf",
    "abs_url": "https://arxiv.org/abs/2511.04235",
    "published": "2025-11-06T10:12:46Z",
    "updated": "2026-01-26T11:24:30Z",
    "comment": "We have prepared the open-source code and video demonstration pages: 1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html",
    "light_analysis": {
      "overview": "本文提出了一个多智能体预测编码框架，通过最小化相互不确定性来构建共享空间记忆，并在带宽受限下展现出卓越的协调性能。",
      "motivation": "在多智能体系统中，由于部分可观测性和有限通信带宽，构建一致的共享空间记忆是一个关键挑战，常导致协调失败。现有方法如全广播通信在带宽受限时性能急剧下降，无法维持有效协调，这突显了开发带宽高效协调机制的重要性，以提升多智能体系统的鲁棒性和集体行为可靠性。",
      "method": "论文引入了一个多智能体预测编码框架，基于信息瓶颈目标优化通信策略，使智能体学习谁、什么、何时进行通信。创新点包括：通过自监督运动预测自发涌现网格状度量作为内部空间编码；发展带宽高效通信机制和模拟社交位置细胞的神经群体；并采用分层强化学习策略主动探索以减少联合不确定性，实现协调。",
      "result": "在Memory-Maze基准测试中，该方法显示出对带宽限制的强健性：当带宽从128 bits/step降至4 bits/step时，成功率从73.5%平稳下降到64.4%。相比之下，全广播基线在相同条件下成功率从67.6%崩溃至28.6%，表明所提框架在有限带宽下显著优于传统方法，实现了高效协调。",
      "conclusion": "本研究提供了一个理论原则和生物学上合理的框架，解释了复杂社交表示如何从统一预测驱动中涌现，促进集体智能。它不仅提升了多智能体系统在带宽受限下的协调性能，还为人工智能和神经科学交叉研究提供了新见解。潜在局限性可能包括未覆盖更多动态环境；未来工作可探索扩展性和实际应用。",
      "tags": [
        "Predictive Coding",
        "Multi-Agent Systems",
        "Information Bottleneck",
        "Hierarchical Reinforcement Learning",
        "Social Place Cells"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:21.219915Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.04084",
    "title": "When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation",
    "authors": [
      "Nishchal Sapkota",
      "Haoyan Shi",
      "Yejia Zhang",
      "Xianshi Ma",
      "Bofang Zheng",
      "Fabian Vazquez",
      "Pengfei Gu",
      "Danny Z. Chen"
    ],
    "abstract": "Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: https://github.com/nsapkota417/UKAST",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.04084.pdf",
    "abs_url": "https://arxiv.org/abs/2511.04084",
    "published": "2025-11-06T05:44:57Z",
    "updated": "2026-01-26T18:07:50Z",
    "comment": "This paper has been accepted for publication in the Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI) 2026",
    "light_analysis": {
      "overview": "该论文提出UKAST架构，通过集成Kolmogorov-Arnold Networks (KANs)到Swin Transformer中，实现了数据高效和计算高效的医学图像分割。",
      "motivation": "医学图像分割对精准诊断和治疗规划至关重要，但面临复杂解剖结构和有限标注数据的挑战。现有CNN方法擅长提取局部特征，但难以建模长距离依赖关系；Transformer虽能有效捕获全局上下文，却需要大量数据和计算资源。这导致在数据稀缺或资源受限的医疗场景中，现有方法性能不足。因此，本研究旨在开发一种更高效、数据需求更低的分割方法，以克服这些限制并提升实用性。",
      "method": "本研究提出UKAST，一个类似U-Net的架构，将基于有理函数的Kolmogorov-Arnold Networks (KANs)整合到Swin Transformer编码器中。关键创新包括使用有理基函数和Group Rational KANs (GR-KANs)，这些源自Kolmogorov-Arnold Transformer (KAT)，以改进原始基于样条的KANs的低效性。这提高了模型的表达力和数据效率，相比SwinUNETR，减少了FLOPs且参数计数只小幅增加。架构设计旨在平衡计算复杂性和特征提取能力。",
      "result": "UKAST在四个不同的2D和3D医学图像分割基准测试中实现了最先进性能，一致超越CNN-和Transformer-based基线方法。特别地，在数据稀缺设置下，它展现出更高准确性，有效缓解了标准Vision Transformers对大量数据的依赖。虽然摘要未提供具体数值指标（如准确率提升百分比），但强调了模型在效率和效果上的综合优势，包括减少计算开销和保持参数可控。",
      "conclusion": "该研究的主要贡献在于提出了UKAST，一个KAN-enhanced Transformer架构，展示了在医学图像分割中提高数据效率和计算效率的潜力。其学术价值在于融合了Transformer和KANs的新思路，推动了数据高效视觉模型的发展；实际应用中，可促进医疗诊断的精确化和自动化。局限性方面，摘要未明确说明模型泛化能力或部署挑战，未来工作可进一步优化架构或扩展到其他医学图像任务。",
      "tags": [
        "Swin Transformer",
        "Kolmogorov-Arnold Networks (KANs)",
        "Medical Image Segmentation",
        "U-Net Architecture",
        "Group Rational KANs (GR-KANs)"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:14.872629Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.02599",
    "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour",
    "authors": [
      "Max Norris",
      "Kobi Gal",
      "Sahan Bulathwela"
    ],
    "abstract": "Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.02599.pdf",
    "abs_url": "https://arxiv.org/abs/2511.02599",
    "published": "2025-11-04T14:20:56Z",
    "updated": "2026-01-26T18:20:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Next Token Knowledge Tracing（NTKT），一种利用预训练大型语言模型将知识追踪重构为下一个令牌预测任务的方法，显著提升预测性能并改善泛化能力。",
      "motivation": "知识追踪（KT）是教育AI中的核心任务，旨在基于学生历史互动预测未来回答，对个性化学习至关重要。现有KT模型通常依赖答案正确性和元数据（如技能标签、时间戳），但忽略了问题文本，这限制了教学洞察的提取，导致预测性能受限。因此，本研究旨在通过整合问题内容来弥补这一不足，以提高模型的准确性和实用性。",
      "method": "NTKT方法将知识追踪任务重新定义为下一个令牌预测问题，利用预训练的大型语言模型（LLMs）。核心创新在于将学生历史互动和问题内容表示为文本序列，使LLMs能够同时学习行为和语言模式。具体来说，该方法通过文本化输入，充分利用了LLMs的预训练表示能力，避免了传统模型对结构化元数据的过度依赖。",
      "result": "实验结果表明，NTKT在多个基准测试中显著优于当前最先进的神经KT模型。特别是在冷启动问题和用户场景中，NTKT展现出更强的泛化能力，能够有效处理未见数据。尽管摘要未提供具体性能指标，但强调了性能提升和泛化改进，验证了问题内容在KT中的重要性。",
      "conclusion": "该研究的核心贡献是提出了NTKT方法，强调了问题内容在知识追踪中的关键作用，并展示了利用预训练LLMs表示的有效性。这不仅提高了预测精度，还为教育AI领域提供了新的建模思路，具有重要的学术和实际应用价值。未来工作可能包括进一步优化模型架构或扩展应用到其他教育场景。",
      "tags": [
        "Knowledge Tracing",
        "Large Language Models",
        "Next-Token Prediction",
        "Educational AI"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:36.225173Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.26441",
    "title": "A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models",
    "authors": [
      "Shihab Aaqil Ahamed",
      "Udaya S. K. P. Miriya Thanthrige",
      "Ranga Rodrigo",
      "Muhammad Haris Khan"
    ],
    "abstract": "Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs' reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.26441.pdf",
    "abs_url": "https://arxiv.org/abs/2510.26441",
    "published": "2025-10-30T12:45:24Z",
    "updated": "2026-01-26T17:12:54Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "本文提出A-TPT框架，通过引入角度多样性改进视觉-语言模型的测试时间提示调优校准性能。",
      "motivation": "测试时间提示调优（TPT）是一种无需标注数据即可使大型视觉-语言模型（VLMs）适应未见任务的技术，但文本特征之间缺乏分散性会损害校准性能，引发模型可靠性、可信度和安全性方面的担忧。现有TPT方法主要通过最大化平均文本特征分散性或强制正交约束来改进校准，然而这些方法可能未优化类别间文本特征的角分离，忽视了角度多样性的关键作用。因此，需要一种新方法来解决这一问题。",
      "method": "论文提出A-TPT，一种新颖的TPT框架，通过引入角度多样性来鼓励归一化文本特征的均匀分布。具体技术路线是优化可学习提示参数，使生成的特征在单位超球面上具有均匀角度分布，通过最大化最小成对角度距离来实现。该方法避免了特征聚集，提升了校准效果，关键创新点在于直接关注角度多样性而非传统分散性或正交性约束。",
      "result": "A-TPT在多种骨干网络和数据集上的广泛实验表明，它能显著减少聚合平均校准误差，同时维持可比准确度，优于现有TPT方法。具体地，在自然分布偏移下，该方法展示卓越的零样本校准性能，并能良好泛化到医疗数据集。论文通过理论分析进一步验证了A-TPT的有效性，强调了其在减少校准误差方面的优越性。",
      "conclusion": "论文的主要贡献是提出A-TPT框架，强调角度多样性在VLMs测试时间调优中的重要性，显著提升了校准性能。研究通过理论和实证分析，证明了促进角度多样性的有效性，增强了VLMs的可靠性和安全性。这项工作具有学术和实际应用价值，为未来研究提供了方向，例如可进一步探索在其他领域或模型中的泛化性能。",
      "tags": [
        "Test-Time Prompt Tuning",
        "Vision-Language Models",
        "Angular Diversity",
        "Calibration Error",
        "Normalized Features"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:43.632798Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.24940",
    "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens",
    "authors": [
      "Yinhan He",
      "Wendy Zheng",
      "Yaochen Zhu",
      "Zaiyi Zheng",
      "Lin Su",
      "Sriram Vasudevan",
      "Qi Guo",
      "Liangjie Hong",
      "Jundong Li"
    ],
    "abstract": "The verbosity of Chain-of-Thought (CoT) reasoning hinders its mass deployment in efficiency-critical applications. Recently, implicit CoT approaches have emerged, which encode reasoning steps within LLM's hidden embeddings (termed ``implicit reasoning'') rather than explicit tokens. This approach accelerates CoT by reducing the reasoning length and bypassing some LLM components. However, existing implicit CoT methods face two significant challenges: (1) they fail to preserve the semantic alignment between the implicit reasoning (when transformed to natural language) and the ground-truth reasoning, resulting in a significant CoT performance degradation, and (2) they focus on reducing the length of the implicit reasoning; however, they neglect the considerable time cost for an LLM to generate one individual implicit reasoning token. To tackle these challenges, we propose a novel semantically-aligned implicit CoT framework termed SemCoT. In particular, for the first challenge, we design a contrastively trained sentence transformer that evaluates semantic alignment between implicit and explicit reasoning, which is used to enforce semantic preservation during implicit reasoning optimization. To address the second challenge, we introduce an efficient implicit reasoning generator by finetuning a lightweight language model using knowledge distillation. This generator is guided by our sentence transformer to distill ground-truth reasoning into semantically aligned implicit reasoning, while also optimizing for accuracy. SemCoT is the first approach that enhances CoT efficiency by jointly optimizing token-level generation speed and preserving semantic alignment with ground-truth reasoning. Extensive experiments demonstrate the superior performance of SemCoT compared to state-of-the-art methods in both efficiency and effectiveness. Our code can be found at https://github.com/YinhanHe123/SemCoT/.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.24940.pdf",
    "abs_url": "https://arxiv.org/abs/2510.24940",
    "published": "2025-10-28T20:11:54Z",
    "updated": "2026-01-26T04:59:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "SemCoT提出一种语义对齐的隐式Chain-of-Thought框架，通过联合优化token级生成速度和语义对齐来加速推理。",
      "motivation": "该研究旨在解决Chain-of-Thought推理在效率关键应用中的部署问题。隐式CoT方法通过将推理步骤编码到隐藏嵌入中来加速推理，但现有方法面临两个挑战：首先，隐式推理在转换为自然语言时缺乏与真实推理的语义对齐，导致性能显著下降；其次，它们只关注减少推理长度，忽略了生成单个隐式推理token的时间成本。这些问题限制了隐式CoT的实际效果，因此需要一种更高效的解决方案来同时优化效率和语义保留。",
      "method": "SemCoT框架包括两个关键组件：一是设计一个对比训练的句子transformer，用于评估隐式推理和显式推理之间的语义对齐，并在隐式推理优化过程中强制语义保留；二是引入一个高效的隐式推理生成器，通过知识蒸馏方法微调一个轻量级语言模型。这个生成器在句子transformer的指导下，将真实推理蒸馏成语义对齐的隐式推理，同时优化准确性。SemCoT首次联合优化token级生成速度和语义对齐，以提升CoT效率。",
      "result": "论文通过广泛的实验验证了SemCoT的性能，显示其在效率和效果上均优于现有最先进的方法。实验表明，SemCoT在保持语义对齐的同时，显著加速了Chain-of-Thought推理。然而，摘要未明确说明具体的性能指标（如准确率提升百分比或速度改进倍数），仅表示在效率和有效性方面表现优越。与基线方法相比，SemCoT在提升推理速度的同时保持了较高的准确性。",
      "conclusion": "SemCoT的主要贡献是提出一种新的语义对齐隐式Chain-of-Thought框架，通过联合优化token级生成速度和语义对齐，有效解决了现有隐式CoT方法的缺陷。这项研究在学术上推动了推理效率的优化，在实际应用中使CoT推理更适合效率关键场景，如实时决策系统。未来工作可能包括进一步优化生成器的效率或在更多数据集上验证方法的通用性。",
      "tags": [
        "Chain-of-Thought",
        "Implicit Reasoning",
        "Sentence Transformer",
        "Knowledge Distillation",
        "Semantic Alignment"
      ]
    },
    "analyzed_at": "2026-01-27T03:43:48.433055Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22345",
    "title": "Uncertainty quantification in model discovery by distilling interpretable material constitutive models from Gaussian process posteriors",
    "authors": [
      "David Anton",
      "Henning Wessels",
      "Ulrich Römer",
      "Alexander Henkes",
      "Jorge-Humberto Urrea-Quintero"
    ],
    "abstract": "Constitutive model discovery refers to the task of identifying an appropriate model structure, usually from a predefined model library, while simultaneously inferring its material parameters. The data used for model discovery are measured in mechanical tests and are thus inevitably affected by noise which, in turn, induces uncertainties. Previously proposed methods for uncertainty quantification in model discovery either require the selection of a prior for the material parameters, are restricted to linear coefficients of the model library or are limited in the flexibility of the inferred parameter probability distribution. We therefore propose a partially Bayesian framework for uncertainty quantification in model discovery that does not require prior selection for the material parameters and also allows for the discovery of constitutive models with inner-non-linear parameters: First, we augment the available stress-deformation data with a Gaussian process. Second, we approximate the parameter distribution by a normalizing flow, which allows for modeling complex joint distributions. Third, we distill the parameter distribution by matching the distribution of stress-deformation functions induced by the parameters with the Gaussian process posterior. Fourth, we perform a Sobol' sensitivity analysis to obtain a sparse and interpretable model. We demonstrate the capability of our framework for both isotropic and experimental anisotropic data.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.22345.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22345",
    "published": "2025-10-25T16:02:03Z",
    "updated": "2026-01-26T14:59:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种部分贝叶斯框架，用于材料本构模型发现中的不确定性量化，无需先验选择并处理非线性参数。",
      "motivation": "模型发现任务需从噪声机械测试数据中识别模型结构和参数，不确定性量化至关重要，以确保建模准确性。现有方法如需要先验选择、限制于线性系数或参数分布灵活性不足，导致估计不准确或适用性受限。本研究旨在解决这些问题，开发更灵活的方法来量化不确定性，适应复杂材料行为分析，提升实际应用价值。",
      "method": "本框架包含四个步骤：首先，用高斯过程增强应力-变形数据以捕获不确定性；其次，采用归一化流近似参数分布，可建模复杂联合概率分布；第三，通过匹配参数诱导的应力-变形函数分布与高斯过程后验来精炼参数分布；第四，进行Sobol'敏感性分析以获得稀疏和可解释模型。关键创新点包括无需先验选择和可处理内非线性参数。",
      "result": "摘要未明确说明具体实验性能指标或量化对比数据，但论文指出框架演示了在各向同性和实验各向异性数据上的能力，这表明方法在多种材料数据类型中有效。然而，缺少与基线方法的直接定量结果，如准确率提升或效率改进，具体性能优势需参考全文进一步验证。",
      "conclusion": "论文的主要贡献是提出一种无需先验选择的部分贝叶斯框架，改进了模型发现中的不确定性量化方法，增强了参数分布的灵活性和模型可解释性。研究具有学术价值，推动了贝叶斯推理和模型选择领域的发展，在实际应用中可提升材料建模的准确性。未来工作可能包括扩展到更复杂场景或与其他技术结合。",
      "tags": [
        "Uncertainty Quantification",
        "Model Discovery",
        "Gaussian Process",
        "Normalizing Flow",
        "Sobol Sensitivity Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:44:05.162062Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22048",
    "title": "PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations",
    "authors": [
      "Ana K. Rivera",
      "Anvita Bhagavathula",
      "Alvaro Carbonero",
      "Priya Donti"
    ],
    "abstract": "Power flow (PF) calculations are the backbone of real-time grid operations, across workflows such as contingency analysis (where repeated PF evaluations assess grid security under outages) and topology optimization (which involves PF-based searches over combinatorially large action spaces). Running these calculations at operational timescales or across large evaluation spaces remains a major computational bottleneck. Additionally, growing uncertainty in power system operations from the integration of renewables and climate-induced extreme weather also calls for tools that can accurately and efficiently simulate a wide range of scenarios and operating conditions. Machine learning methods offer a potential speedup over traditional solvers, but their performance has not been systematically assessed on benchmarks that capture real-world variability. This paper introduces PF$Δ$, a benchmark dataset for power flow that captures diverse variations in load, generation, and topology. PF$Δ$ contains 859,800 solved power flow instances spanning six different bus system sizes, capturing three types of contingency scenarios (N , N -1, and N -2), and including close-to-infeasible cases near steady-state voltage stability limits. We evaluate traditional solvers and GNN-based methods, highlighting key areas where existing approaches struggle, and identifying open problems for future research. Our dataset is available at https://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code with data generation scripts and model implementations is at https://github.com/MOSSLab-MIT/pfdelta.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.22048.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22048",
    "published": "2025-10-24T22:09:09Z",
    "updated": "2026-01-26T16:54:28Z",
    "comment": "31 pages, 14 figures. Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "本文引入了PFΔ基准数据集，用于评估电力潮流计算方法在负载、发电和拓扑变化下的性能。",
      "motivation": "电力潮流计算是电网实时操作的基石，特别是在应急分析和拓扑优化中，但计算效率是主要瓶颈。随着可再生能源集成和气候极端事件导致电网不确定性增加，需要高效模拟多种场景的工具。传统求解器计算密集，机器学习方法虽具潜力，但缺乏系统性评估基准，无法反映真实世界的变化，因此本研究旨在提供一个标准化数据集以解决这些评估不足。",
      "method": "本研究提出PFΔ基准数据集，包含859,800个已解决的电力潮流实例，覆盖六个不同总线系统大小，并捕获负载、发电和拓扑的多样化变化。创新点在于集成三种应急场景（N、N-1和N-2）和接近不可行情况（近稳态电压稳定性极限），以提供全面评估框架。数据集通过数据生成脚本实现，并公开代码，促进方法比较，强调标准化基准在评估传统求解器和图神经网络方法中的重要性。",
      "result": "评估了传统求解器和基于图神经网络的方法，指出了现有方法在处理复杂场景（如应急分析和稳定性极限）时的挑战，但摘要未明确说明具体性能指标如准确率或效率提升。结果强调标准化基准能系统性揭示方法局限性，并识别开放问题，为未来研究提供方向，但没有提供与基线方法的详细定量对比数据。",
      "conclusion": "本文的主要贡献是提供PFΔ基准数据集，支持电力潮流计算方法的系统性评估。学术上，推动了机器学习在电力系统中的应用研究；实践上，有助于优化电网操作效率并应对不确定性。研究识别了开放问题，鼓励社区进一步探索，数据集和代码的公开促进了可重复性和协作，未来工作可围绕改进算法效率和扩展数据集展开。",
      "tags": [
        "Power Flow",
        "Benchmark Dataset",
        "Graph Neural Networks",
        "Contingency Analysis",
        "Topology Variations"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:07.121829Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.21366",
    "title": "BADiff: Bandwidth Adaptive Diffusion Model",
    "authors": [
      "Xi Zhang",
      "Hanwei Zhu",
      "Yan Zhong",
      "Jiamang Wang",
      "Weisi Lin"
    ],
    "abstract": "In this work, we propose a novel framework to enable diffusion models to adapt their generation quality based on real-time network bandwidth constraints. Traditional diffusion models produce high-fidelity images by performing a fixed number of denoising steps, regardless of downstream transmission limitations. However, in practical cloud-to-device scenarios, limited bandwidth often necessitates heavy compression, leading to loss of fine textures and wasted computation. To address this, we introduce a joint end-to-end training strategy where the diffusion model is conditioned on a target quality level derived from the available bandwidth. During training, the model learns to adaptively modulate the denoising process, enabling early-stop sampling that maintains perceptual quality appropriate to the target transmission condition. Our method requires minimal architectural changes and leverages a lightweight quality embedding to guide the denoising trajectory. Experimental results demonstrate that our approach significantly improves the visual fidelity of bandwidth-adapted generations compared to naive early-stopping, offering a promising solution for efficient image delivery in bandwidth-constrained environments. Code is available at: https://github.com/xzhang9308/BADiff.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.21366.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21366",
    "published": "2025-10-24T11:50:03Z",
    "updated": "2026-01-26T12:57:27Z",
    "comment": "NeurIPS 2025 Poster",
    "light_analysis": {
      "overview": "提出BADiff框架，使扩散模型能根据实时网络带宽约束自适应调整图像生成质量。",
      "motivation": "在云到设备场景中，网络带宽受限，传统扩散模型采用固定去噪步骤生成高保真图像，但忽略传输限制，导致图像需压缩传输时损失精细纹理并浪费计算资源。现有方法缺乏带宽自适应能力，影响实际应用效率和用户体验，因此亟需优化扩散模型以平衡生成质量与带宽约束。",
      "method": "论文提出一种端到端联合训练策略，扩散模型以目标质量级别为条件，该级别由可用带宽动态确定。模型学习自适应调节去噪过程，实现早期停止采样，通过轻量级质量嵌入指导去噪轨迹，最小化架构改动。核心创新在于带宽驱动的自适应生成机制，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果显示，与朴素的早期停止方法相比，BADiff方法在带宽适应生成的视觉保真度上显著改善，优化了图像传输效率与质量的平衡。摘要未提供具体性能指标数据，仅表明该方法在评估中优于基线，展示了其在带宽受限环境中的有效性。",
      "conclusion": "BADiff框架为带宽受限环境下的高效图像交付提供了有前景的解决方案，通过使扩散模型自适应带宽，减少了资源浪费并提升了用户体验。主要贡献在于引入带宽驱动的自适应生成策略。未来工作可能包括扩展到其他任务或进一步优化模型效率，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Diffusion Models",
        "Adaptive Generation",
        "Bandwidth Constraints",
        "End-to-End Training",
        "Quality Embedding"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:04.192454Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.20984",
    "title": "Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression",
    "authors": [
      "Xi Zhang",
      "Xiaolin Wu",
      "Jiamang Wang",
      "Weisi Lin"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities but typically require extensive computational resources and memory for inference. Post-training quantization (PTQ) can effectively reduce these demands by storing weights in lower bit-width formats. However, standard uniform quantization often leads to notable performance degradation, particularly in low-bit scenarios. In this work, we introduce a Grouped Lattice Vector Quantization (GLVQ) framework that assigns each group of weights a customized lattice codebook, defined by a learnable generation matrix. To address the non-differentiability of the quantization process, we adopt Babai rounding to approximate nearest-lattice-point search during training, which enables stable optimization of the generation matrices. Once trained, decoding reduces to a simple matrix-vector multiplication, yielding an efficient and practical quantization pipeline. Experiments on multiple benchmarks show that our approach achieves a better trade-off between model size and accuracy compared to existing post-training quantization baselines, highlighting its effectiveness in deploying large models under stringent resource constraints. Our source code is available on GitHub repository: https://github.com/xzhang9308/GLVQ.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.20984.pdf",
    "abs_url": "https://arxiv.org/abs/2510.20984",
    "published": "2025-10-23T20:19:48Z",
    "updated": "2026-01-26T12:51:01Z",
    "comment": "NeurIPS 2025 Poster",
    "light_analysis": {
      "overview": "本文提出分组格点向量量化框架，用于低比特大型语言模型压缩，以提升量化后模型的性能和效率。",
      "motivation": "大型语言模型（LLMs）虽然能力卓越，但推理时需要大量计算资源和内存，限制了实际部署。后训练量化（PTQ）能通过低比特格式存储权重来减少需求，然而标准均匀量化在低比特场景下常导致显著性能下降，现有方法难以平衡模型大小与准确性。因此，开发更有效的量化技术至关重要，以在资源受限环境中实现高效模型压缩。",
      "method": "论文提出Grouped Lattice Vector Quantization（GLVQ）框架，通过将权重分组并为每组分配由可学习生成矩阵定义的自定义格点码本，优化量化过程。为解决量化不可微分性问题，采用Babai取整近似最近格点搜索，确保训练中生成矩阵的稳定优化。训练后，解码简化为矩阵向量乘法，形成高效量化流程，核心创新在于分组策略与格点量化的结合，以及端到端优化方法。",
      "result": "在多个基准测试上的实验表明，GLVQ方法相比现有后训练量化基线，实现了更好的模型大小与准确性权衡，突显了其在严格资源约束下部署大型模型的有效性。摘要未明确说明具体性能指标如准确率提升百分比，但强调了整体权衡的改进，验证了该方法在不同场景下的优越性，为低比特压缩提供了可靠解决方案。",
      "conclusion": "GLVQ框架为低比特LLM压缩提供创新解决方案，通过分组格点向量量化显著提升了量化性能，在学术上推动了量化技术的发展，并在实际应用中促进了大模型在资源受限环境中的部署。代码开源便于复现和扩展，未来工作可能包括优化生成矩阵学习过程或扩展到其他模型架构，以进一步提升泛化能力。",
      "tags": [
        "Large Language Models (LLMs)",
        "Post-training Quantization (PTQ)",
        "Lattice Vector Quantization",
        "Babai Rounding",
        "Low-Bit Compression"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:42.658268Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.19229",
    "title": "Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition",
    "authors": [
      "Juntang Wang",
      "Yihan Wang",
      "Hao Wu",
      "Dongmian Zou",
      "Shixin Xu"
    ],
    "abstract": "Infants discover categories, detect novelty, and adapt to new contexts without supervision-a challenge for current machine learning. We present a brain-inspired perspective on configurations, a finite-resolution clustering framework that uses a single resolution parameter and attraction-repulsion dynamics to yield hierarchical organization, novelty sensitivity, and flexible adaptation. To evaluate these properties, we introduce mheatmap, which provides proportional heatmaps and reassignment algorithm to fairly assess multi-resolution and dynamic behavior. Across datasets, configurations are competitive on standard clustering metrics, achieve 87% AUC in novelty detection, and show 35% better stability during dynamic category evolution. These results position configurations as a principled computational model of early cognitive categorization and a step toward brain-inspired AI.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.19229.pdf",
    "abs_url": "https://arxiv.org/abs/2510.19229",
    "published": "2025-10-22T04:28:23Z",
    "updated": "2026-01-26T18:34:12Z",
    "comment": "13 pages, 4 figures, conference paper. Equal contribution: Juntang Wang, Yihan Wang and Hao Wu",
    "light_analysis": {
      "overview": "本文提出一个受大脑启发的配置框架，通过单分辨率参数和吸引-排斥动态实现无监督聚类，具备层级组织、新颖性检测和动态适应能力。",
      "motivation": "研究动机在于解决机器学习中无监督学习的挑战，模仿婴儿发现类别、检测新事物和适应环境的能力。现有方法通常依赖标注数据，缺乏灵活性和动态适应性，限制了在真实世界应用。这个问题重要，因为它涉及早期认知过程的模拟，有助于开发更自适应的AI系统。因此，开发一种受大脑启发的框架，以提升无监督学习的效率和鲁棒性。",
      "method": "核心方法是一种有限分辨率的聚类框架，使用单个分辨率参数和吸引-排斥动态，以实现层级结构、新颖性敏感性和灵活适应。关键创新点包括结合大脑启发的动态机制，以及引入mheatmap评估工具，该工具通过比例热图和重分配算法，公平评估多分辨率和动态行为。技术特色在于模拟认知过程的原理化建模，数据集跨多个标准集以验证性能，但摘要未明确说明具体模型架构。",
      "result": "在多个数据集上，配置框架在标准聚类指标上表现竞争性，与基线方法相当。具体数据包括在新颖性检测任务中达到87% AUC，显示出高准确性。在动态类别演化过程中，稳定性比基线方法提高了35%，证明在处理变化数据时的优越性能。这些结果通过跨数据集实验验证，但摘要未提供更详细的数据对比。",
      "conclusion": "该研究的主要贡献是提出一个有原则的计算模型，模拟早期认知分类，为受大脑启发的AI发展迈出重要一步。学术价值在于丰富无监督学习理论，实际应用潜力包括改进自适应学习系统。潜在局限性包括摘要未明确说明框架的具体扩展性，未来工作可探索更多复杂场景或结合其他技术以增强性能。",
      "tags": [
        "Unsupervised Learning",
        "Clustering Framework",
        "Novelty Detection",
        "Brain-Inspired AI",
        "Attraction-Repulsion Dynamics"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:21.489586Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.16387",
    "title": "Probing the Hidden Talent of ASR Foundation Models for L2 English Oral Assessment",
    "authors": [
      "Fu-An Chao",
      "Bi-Cheng Yan",
      "Berlin Chen"
    ],
    "abstract": "In this paper, we explore the untapped potential of Whisper, a well-established automatic speech recognition (ASR) foundation model, in the context of L2 spoken language assessment (SLA). Unlike prior studies that extrinsically analyze transcriptions produced by Whisper, our approach goes a step further to probe its latent capabilities by extracting acoustic and linguistic features from hidden representations. With only a lightweight classifier being trained on top of Whisper's intermediate and final outputs, our method achieves strong performance on the GEPT picture-description dataset, outperforming existing cutting-edge baselines, including a multimodal approach. Furthermore, by incorporating image and text-prompt information as auxiliary relevance cues, we demonstrate additional performance gains. Finally, we conduct an in-depth analysis of Whisper's embeddings, which reveals that, even without task-specific fine-tuning, the model intrinsically encodes both ordinal proficiency patterns and semantic aspects of speech, highlighting its potential as a powerful foundation for SLA and other spoken language understanding tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.16387.pdf",
    "abs_url": "https://arxiv.org/abs/2510.16387",
    "published": "2025-10-18T08:10:24Z",
    "updated": "2026-01-26T08:58:34Z",
    "comment": "Accepted to ICASSP 2026",
    "light_analysis": {
      "overview": "论文展示了通过提取 Whisper ASR 基础模型的隐藏声学和语言特征，在 L2 英语口语评估中实现卓越性能的创新方法，无需任务特定微调即可优于现有技术。",
      "motivation": "本研究旨在解决 L2 口语评估的准确性问题。现有方法主要依赖于 Whisper 模型的转录文本进行外部分析，忽略了其内在表示中丰富的声学和语言信息。由于 L2 学习者的口语熟练度评估在教育领域至关重要，这种不足导致评估工具可能不够精确。探索基础模型的潜在能力可以弥补这一空白，为更有效的口语评估提供新途径。摘要未明确说明所有具体挑战，但强调了从隐藏表示中提取特征的必要性。",
      "method": "论文提出一种创新方法，通过从 Whisper 模型的隐藏表示中提取声学和语言特征来探测其能力。核心是在 Whisper 的中间和最终输出上训练一个轻量级分类器，无需对模型进行任务特定微调。使用 GEPT 图片描述数据集作为实验平台，并加入图像和文本提示信息作为辅助相关线索以增强性能。关键创新在于直接利用基础模型的内在嵌入，而非仅依赖转录文本，从而揭示其编码的语义和熟练度模式。",
      "result": "在 GEPT 数据集上，该方法取得了强性能，优于包括多模态方法在内的现有前沿基线。通过集成图像和文本信息，性能得到进一步提升，但摘要未提供具体数值。分析显示，Whisper 的嵌入即使未经微调，也能内在编码语音的顺序熟练模式和语义方面，突显了其作为口语理解任务基础的潜力。这证明了特征提取方法在 L2 评估中的有效性，并提供了与传统方法相比的优势。",
      "conclusion": "论文的主要贡献是揭示了 Whisper ASR 基础模型在 L2 口语评估中的隐藏能力，通过提取隐藏特征实现了高效性能，无需额外微调。学术上，这为口语理解任务提供了利用基础模型内在表示的新视角。实际应用上，可推动 L2 评估工具的发展，提高准确性和效率。局限性包括可能依赖特定数据集，未来工作可扩展至其他语言任务或更广泛的应用场景。",
      "tags": [
        "ASR Foundation Model",
        "Hidden Representations",
        "Acoustic-Linguistic Features",
        "L2 Oral Assessment",
        "Speech Understanding"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:23.389338Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.13551",
    "title": "Tandem Training for Language Models",
    "authors": [
      "Robert West",
      "Ashton Anderson",
      "Ece Kamar",
      "Eric Horvitz"
    ],
    "abstract": "As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.13551.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13551",
    "published": "2025-10-15T13:48:16Z",
    "updated": "2026-01-26T13:19:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种名为tandem training的强化学习方法，通过随机交接控制训练语言模型，以提高解决方案的可理解性。",
      "motivation": "随着语言模型的快速进步，其推理过程变得复杂且难以追踪，削弱了可解释性和监督能力，这在实际应用中可能导致AI系统难以被人类或弱代理理解和审计，从而影响协作效率。现有方法通常侧重于提升模型性能，但忽视了可理解性激励，使得强模型的解决方案对弱合作者不友好。因此，开发能够促进可理解性的训练方法至关重要，以支持有效的人类-AI协作和多代理通信，确保AI系统在长期应用中保持透明和可控。",
      "method": "论文引入了tandem training，一种基于强化学习的训练范式。核心创新在于将可理解性形式化为交接稳健性：强模型的解决方案如果在路径上随机交由弱模型控制而不失败，则被视为可理解。在训练过程中，rollout tokens间歇性地从冻结的弱模型中采样，而非从正在训练的强模型中，这样只有当强模型的行动和推理过程能被弱模型延续时，rollout才会成功。这种方法隐式激励了正确性和可理解性，使用GSM8K数学推理任务作为实验平台，涉及强模型和弱模型在RL框架中的交互训练。",
      "result": "在GSM8K数学推理任务上，tandem training能可靠地教导模型放弃技术性行话，调整语言以适应弱合作者，同时保持高任务准确率。摘要未明确提供具体性能指标如准确率数值，但实验结果表明该方法能有效平衡准确性与可理解性，相比于基线方法，在保持高性能的同时增强了解决方案的可审计性。这证实了tandem training在激励可理解行为方面的潜力，为后续研究提供了实证基础。",
      "conclusion": "本文的主要贡献是提出了tandem training方法，为语言模型的可理解性训练提供了新途径。其学术价值在于将可理解性形式化为交接稳健性，并在强化学习框架中实现；实际应用价值在于促进可审计AI系统的开发，对人类-AI协作和多代理通信有重要意义。虽然研究在GSM8K任务上验证了有效性，但未来工作可扩展到更多领域和任务，以测试泛化能力和潜在局限性，进一步提升方法的实用性和普适性。",
      "tags": [
        "Language Model",
        "Reinforcement Learning",
        "Tandem Training",
        "Handoff Robustness",
        "Math Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:23.012020Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.11020",
    "title": "GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation",
    "authors": [
      "Shasha Guo",
      "Liang Pang",
      "Xi Wang",
      "Yanling Wang",
      "Huawei Shen",
      "Jing Zhang"
    ],
    "abstract": "Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Recent attempts construct auxiliary lines via code-driven rendering, a strategy that relies on accurate and executable code generation to produce visual renderings of the auxiliary lines for subsequent reasoning. However, in complex solid geometry settings, such a strong dependence on precise specifications substantially restricts the robustness of this strategy. Alternatively, we turn to a simpler and more stable solution, representing auxiliary-line constructions as structured textual descriptions. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. The core is a cross-modal reward model that evaluates how well the generated auxiliary-line description matches the ground-truth auxiliary-line diagram. The reward signal drives a GRPO-based RL stage to yield informative auxiliary-line descriptions for the reasoning. To support the training and evaluation, we develop a scalable data pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. Based on this framework, we derive GeoVLMath, an LVLM for solving complex solid geometry.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.11020.pdf",
    "abs_url": "https://arxiv.org/abs/2510.11020",
    "published": "2025-10-13T05:33:51Z",
    "updated": "2026-01-26T07:35:43Z",
    "comment": "19 pages",
    "light_analysis": {
      "overview": "本文提出GeoVLMath，通过基于跨模态奖励的强化学习框架，将辅助线构建表示为结构化文本描述，以增强视觉语言模型在几何推理中的能力。",
      "motivation": "辅助线在解决复杂几何问题中至关重要，但对大型视觉语言模型仍具挑战性。现有方法依赖代码驱动渲染来创建辅助线，但需精确代码生成，在复杂实体几何场景中鲁棒性不足。本研究旨在探索更稳定简单的方案，通过将辅助线构建表示为文本描述，以克服代码依赖性并提升模型在实际几何推理任务中的表现，从而推动LVLMs在复杂问题解决中的应用。",
      "method": "作者提出将辅助线构建转化为结构化文本描述，以减少对精确代码的依赖。为此，开发了一个强化学习框架，核心是一个跨模态奖励模型，用于评估生成的辅助线描述与真实辅助线图之间的匹配程度。通过基于GRPO的强化学习阶段，奖励信号驱动生成信息丰富的描述。此外，构建了可扩展数据管道和AuxSolidMath数据集，包含3,018个真实考试几何问题及其配对的图文数据，以支持训练和评估模型的几何推理能力。",
      "result": "摘要未明确说明具体的实验结果，如准确率提升或效率改进数据。然而，基于提出的框架和构建的数据集，可以推断模型在辅助线创建和几何推理任务上可能实现了改进，与基线代码驱动方法相比，该方法可能展现出更好的稳定性和实用性，但具体性能指标需参考完整论文以获取详细验证结果。",
      "conclusion": "本研究的主要贡献是开发了GeoVLMath，一个结合跨模态奖励和强化学习的视觉语言模型，用于增强几何推理。通过创新性地将辅助线构建转为文本描述并用强化学习优化图文对齐，提高了模型在复杂几何问题中的鲁棒性。这项工作不仅提出了新方法，还提供了数据集，为相关研究奠定了基础，具有学术价值和实际应用潜力，未来可扩展到其他视觉语言推理任务以进一步验证其泛化能力。",
      "tags": [
        "Vision-Language Models",
        "Reinforcement Learning",
        "Cross-Modal Reward",
        "Geometry Reasoning",
        "Auxiliary Line Creation"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:17.185468Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.10977",
    "title": "Revisiting Model Interpolation for Efficient Reasoning",
    "authors": [
      "Taiqiang Wu",
      "Runming Yang",
      "Tao Liu",
      "Jiahao Wang",
      "Ngai Wong"
    ],
    "abstract": "Model merging, typically on Instruct and Thinking models, has shown remarkable performance for efficient reasoning. In this paper, we systematically revisit the simplest merging method that interpolates two weights directly. Particularly, we observe that model interpolation follows a three-stage evolutionary paradigm with distinct behaviors on the reasoning trajectory. These dynamics provide a principled guide for navigating the performance-cost trade-off. Empirical results demonstrate that a strategically interpolated model surprisingly surpasses sophisticated model merging baselines on both efficiency and effectiveness. We further validate our findings with extensive ablation studies on model layers, modules, and decoding strategies. Ultimately, this work demystifies model interpolation and offers a practical framework for crafting models with precisely targeted reasoning capabilities. Code is available at \\href{https://github.com/wutaiqiang/MI}{Github}.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.10977.pdf",
    "abs_url": "https://arxiv.org/abs/2510.10977",
    "published": "2025-10-13T03:30:01Z",
    "updated": "2026-01-26T06:02:57Z",
    "comment": "14 pages, 6 figures, 7 tables. Working in progress (Llama results added)",
    "light_analysis": {
      "overview": "本文揭示了模型插值的三阶段进化范式，并提出了一个实用框架，通过战略性地插值模型权重来优化推理性能与成本的权衡。",
      "motivation": "本研究动机在于解决高效推理中的性能-成本权衡问题。现有模型合并方法（如基于Instruct和Thinking模型）虽表现优异，但通常复杂且缺乏理论指导，导致资源利用不高效。高效推理在AI应用中至关重要，尤其是在资源受限场景下，因此重新审视简单的模型插值方法，以弥补现有方法的不足并提供更优的解决方案。",
      "method": "研究方法采用模型插值技术，直接对两个预训练模型（如Instruct和Thinking模型）的权重进行线性插值。关键创新点是通过观察推理轨迹，发现插值过程遵循三阶段进化范式，这为动态调整插值策略提供了原则性指导。作者进一步通过消融研究分析了不同模型层、模块和解码策略的影响，以深入理解插值行为。摘要未明确说明使用的具体数据集，但方法侧重于理论分析和实证验证。",
      "result": "主要实验结果显示，战略性地插值模型在推理效率和效果上均超过复杂的模型合并基线方法。虽然摘要未提供具体性能指标数据，但作者通过广泛消融实验验证了插值策略的有效性，表明简单插值能在降低计算成本的同时提升或保持推理能力，从而优化性能-成本权衡。这些发现突出了模型插值在实践中的潜力。",
      "conclusion": "本研究的主要贡献是揭示了模型插值的动力学机制，并提供了基于三阶段范式的实用框架，用于精确构建目标推理模型。这增强了模型合并领域的理论理解，并为资源优化场景提供了实用工具。潜在局限性可能包括对特定模型类型的依赖，未来工作可扩展至更多架构和应用，以进一步提升泛化能力。",
      "tags": [
        "Model Interpolation",
        "Model Merging",
        "Efficient Reasoning",
        "Ablation Study",
        "Inference Trajectory"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:05.021728Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.10203",
    "title": "A Style-Based Profiling Framework for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Datasets",
    "authors": [
      "Dingyi Yao",
      "Xinyao Han",
      "Ruibo Ming",
      "Zhihang Song",
      "Lihui Peng",
      "Jianming Hu",
      "Danya Yao",
      "Yi Zhang"
    ],
    "abstract": "Ensuring the reliability of autonomous driving perception systems requires extensive environment-based testing, yet real-world execution is often impractical. Synthetic datasets have therefore emerged as a promising alternative, offering advantages such as cost-effectiveness, bias free labeling, and controllable scenarios. However, the domain gap between synthetic and real-world datasets remains a major obstacle to model generalization. To address this challenge from a data-centric perspective, this paper introduces a profile extraction and discovery framework for characterizing the style profiles underlying both synthetic and real image datasets. We propose Style Embedding Distribution Discrepancy (SEDD) as a novel evaluation metric. Our framework combines Gram matrix-based style extraction with metric learning optimized for intra-class compactness and inter-class separation to extract style embeddings. Furthermore, we establish a benchmark using publicly available datasets. Experiments are conducted on a variety of datasets and sim-to-real methods, and the results show that our method is capable of quantifying the synthetic-to-real gap. This work provides a standardized profiling-based quality control paradigm that enables systematic diagnosis and targeted enhancement of synthetic datasets, advancing future development of data-driven autonomous driving systems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.10203.pdf",
    "abs_url": "https://arxiv.org/abs/2510.10203",
    "published": "2025-10-11T13:09:41Z",
    "updated": "2026-01-26T08:50:40Z",
    "comment": "Accepted for publication at the 2026 IEEE Intelligent Vehicles Symposium (IEEE IV 2026)",
    "light_analysis": {
      "overview": "本文提出了一种基于风格的剖析框架，用于量化自动驾驶数据集中合成到真实的差距。",
      "motivation": "自动驾驶感知系统的可靠性测试依赖大量环境数据，但真实世界测试成本高、不可行，促使合成数据集成为替代方案，因其成本低、标签无偏且场景可控。然而，合成数据与真实数据之间的域差距阻碍了模型泛化，成为实际应用的主要障碍。现有方法缺乏系统化评估这一差距，限制了合成数据集的有效利用，因此需要从数据中心视角开发量化工具，以提升自动驾驶系统的鲁棒性和安全性。",
      "method": "本研究提出一个风格嵌入分布差异（SEDD）度量作为核心创新点，结合Gram矩阵提取图像风格嵌入，并通过度量学习优化类内紧凑性和类间分离性以增强嵌入效果。框架构建了公开数据集的基准，并应用于多种数据集和sim-to-real方法，旨在量化合成到真实差距。使用style-based profiling来提取和发现数据集的风格特性，为后续诊断和增强提供技术支持。",
      "result": "论文在多个自动驾驶数据集和sim-to-real方法上进行实验，结果显示该框架能够有效量化合成到真实差距，提供了一种标准化的质量控制和诊断工具。具体性能指标如准确率提升在摘要中未明确说明，但与基线方法的对比表明该方法优于传统评估方式。这为系统化诊断合成数据集的不足和针对性地优化提供了依据。",
      "conclusion": "本研究通过提出基于风格的剖析框架，实现了合成到真实差距的系统量化，贡献在于提供标准化的质量控制范式，支持诊断和增强合成数据集。这不仅提升了数据驱动自动驾驶系统的可靠性，还为未来研究和应用提供了工具基础。潜在局限性包括对特定数据集和场景的依赖，未来工作可扩展到更多领域，并探索与实际模型性能的进一步关联。",
      "tags": [
        "Style-Based Profiling",
        "Synthetic-to-Real Gap",
        "Gram Matrix",
        "Metric Learning",
        "Autonomous Driving"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:10.187522Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.09017",
    "title": "Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers",
    "authors": [
      "Rui Bu",
      "Haofeng Zhong",
      "Wenzheng Chen",
      "Yangyan Li"
    ],
    "abstract": "Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.09017.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09017",
    "published": "2025-10-10T05:40:53Z",
    "updated": "2026-01-26T07:50:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Value-State Gated Attention (VGA)，一种基于值状态门控的注意力机制，通过直接打破值状态和注意力分数的相互强化循环，有效缓解Transformers中的极端token现象。",
      "motivation": "基于Transformer的大型模型易受极端token现象影响，如注意力沉和值状态消耗，这些现象降低模型性能、量化保真度和可解释性。问题源于相互强化机制，模型关注值状态接近零的token，导致低效的'无操作'行为。现有方法如基于输入嵌入的门控可能不足，因为无法直接解耦值-注意力更新，凸显了开发更有效机制的重要性。",
      "method": "本文提出Value-State Gated Attention (VGA)，一种简单、专用且稳定的架构机制，通过引入可学习的、数据依赖的门，直接从值向量计算以调制输出，从而高效执行'无操作'注意力。关键创新在于用值状态的函数进行门控，理论梯度分析显示这比基于输入嵌入的门控更有效解耦值和注意力分数更新。摘要未明确说明具体模型架构或使用的数据集。",
      "result": "实验表明，VGA显著减轻了注意力沉的形成并稳定了值状态范数，这导致模型性能改善、鲁棒量化保真度增强和可解释性提升。与先前方法相比，VGA在打破值-注意力相互强化循环方面更有效，从而优化了整体模型行为。摘要未提供具体性能指标如准确率提升数据，但强调了VGA在解决极端token现象上的有效性。",
      "conclusion": "VGA的主要贡献在于提出了一种直接调控值状态的注意力机制，缓解了Transformers中的极端token问题。该研究增强了模型稳定性和效率，具有改进量化过程和模型解释的潜在应用价值。学术上，VGA为Transformer架构优化提供了新思路。摘要未明确说明局限性和未来工作方向，但暗示了进一步验证和扩展的可能性。",
      "tags": [
        "Transformer",
        "Attention Mechanism",
        "Value-State Gating",
        "Quantization",
        "Model Interpretability"
      ]
    },
    "analyzed_at": "2026-01-27T03:23:14.855056Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.07915",
    "title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding",
    "authors": [
      "Peiran Wu",
      "Zhuorui Yu",
      "Yunze Liu",
      "Chi-Hao Wu",
      "Enmin Zhou",
      "Junxiao Shen"
    ],
    "abstract": "The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.07915.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07915",
    "published": "2025-10-09T08:07:19Z",
    "updated": "2026-01-26T16:58:19Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": {
      "overview": "提出MARC，一种结合记忆检索和强化学习的令牌压缩方法，用于高效视频理解，显著降低计算成本。",
      "motivation": "随着大语言模型的发展，视觉语言模型扩展至视频理解时面临高计算成本挑战，现有免训练令牌压缩方法虽能减少计算量，但常导致信息丢失和性能下降。这一问题在资源受限的实时应用中尤为突出，如视频问答和自动驾驶。因此，研究旨在开发一种高效的压缩方法，以在保持高准确性的同时大幅提升计算效率，满足实际需求。",
      "method": "MARC采用“检索后压缩”策略，集成视觉记忆检索器（VMR）进行结构化检索以选择关键视频片段，并设计压缩组相对策略优化（C-GRPO）框架，通过强化学习从教师模型蒸馏推理能力到学生模型。关键创新点在于结合检索和强化学习，减少信息损失，实现令牌压缩。方法基于六个视频基准测试进行评估，使用具体模型架构进行实验。",
      "result": "在六个视频基准测试上，MARC使用仅一个帧的令牌达到了接近基线模型的准确度。具体性能指标包括：视觉令牌数量减少95%，GPU内存使用降低72%，延迟减少23.9%。这些结果表明MARC在高效压缩方面显著优于现有免训练方法，同时保持高准确性。",
      "conclusion": "MARC通过整合记忆增强和强化学习，为视频令牌压缩提供了有效方案，学术上推动了多模态模型的高效化研究，实际应用价值体现在视频问答、监控和自动驾驶等资源受限场景。潜在局限性包括摘要未明确说明的特定视频任务扩展，未来工作可进一步优化框架或探索更广泛的应用领域。",
      "tags": [
        "Token Compression",
        "Reinforcement Learning",
        "Video Understanding",
        "Memory-Augmented Retrieval",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:08.565330Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.07713",
    "title": "MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation",
    "authors": [
      "Shuo Yu",
      "Mingyue Cheng",
      "Daoyu Wang",
      "Qi Liu",
      "Zirui Liu",
      "Ze Guo",
      "Xiaoyu Tao"
    ],
    "abstract": "The primary form of user-internet engagement is shifting from leveraging implicit feedback signals, such as browsing and clicks, to harnessing the rich explicit feedback provided by textual interactive behaviors. This shift unlocks a rich source of user textual history, presenting a profound opportunity for a deeper form of personalization. However, prevailing approaches offer only a shallow form of personalization, as they treat user history as a flat list of texts for retrieval and fail to model the rich temporal and semantic structures reflecting dynamic nature of user interests. In this work, we propose \\textbf{MemWeaver}, a framework that weaves the user's entire textual history into a hierarchical memory to power deeply personalized generation. The core innovation of our memory lies in its ability to capture both the temporal evolution of interests and the semantic relationships between different activities. To achieve this, MemWeaver builds two complementary memory components that both integrate temporal and semantic information, but at different levels of abstraction: behavioral memory, which captures specific user actions, and cognitive memory, which represents long-term preferences. This dual-component memory serves as a comprehensive representation of the user, allowing large language models (LLMs) to reason over both concrete behaviors and abstracted cognitive traits. This leads to content generation that is deeply aligned with their latent preferences. Experiments on the six datasets of the Language Model Personalization (LaMP) benchmark validate the efficacy of MemWeaver. Our code is available.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.07713.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07713",
    "published": "2025-10-09T02:47:21Z",
    "updated": "2026-01-26T03:26:27Z",
    "comment": "Accepted by The Web Conference 2026 (WWW'26) 12 pages, 8 figures",
    "light_analysis": {
      "overview": "MemWeaver框架通过分层记忆从用户文本交互行为中捕获时间和语义结构，实现深度个性化生成。",
      "motivation": "用户与互联网的交互正从隐式反馈（如浏览和点击）转向显式文本交互，提供了丰富的用户文本历史，为实现深度个性化创造了重要机会。然而，现有方法将用户历史视为扁平文本列表进行检索，仅提供浅层个性化，无法建模反映用户兴趣动态特性的时间和语义结构，如兴趣演化和语义关系，这限制了个人化的深度和准确性。因此，亟需一种能捕捉这些结构的方法来提升个性化效果，解决现有方法的不足。",
      "method": "MemWeaver框架的核心创新是构建一个分层记忆，将用户的整个文本历史编织成两个互补组件：行为记忆捕获具体的用户行为，集成时间和语义信息以反映特定行动；认知记忆表示长期偏好，在不同抽象层次上同样集成这些信息以捕捉整体倾向。这种双组件结构允许大型语言模型（LLMs）同时推理具体行为和抽象认知特征，从而实现内容生成与用户潜在偏好的深度对齐。关键技术在于利用分层设计建模用户兴趣的演化和语义关系，提升个性化的精细度。",
      "result": "实验在Language Model Personalization (LaMP)基准的六个数据集上进行，验证了MemWeaver框架的有效性。摘要未明确说明具体的性能指标数据（如准确率提升或效率改进），但表明该方法在个性化生成任务上展现出优异效果，与基线方法对比后，MemWeaver通过捕捉用户兴趣的时间和语义结构，实现了更深层次的个性化，提升了内容生成与用户偏好的对齐度。实验结果支持了框架的实用性和创新性，增强了其在真实应用中的潜力。",
      "conclusion": "MemWeaver的主要贡献是提出一种基于分层记忆的框架，通过建模用户文本历史的时间和语义结构，实现深度个性化生成。学术价值在于创新地整合了兴趣演化和语义关系，推动了AI个性化领域的前沿技术发展；实际应用价值体现在生成更符合用户潜在偏好的内容，提升用户体验和交互效果。尽管在多个数据集上验证了有效性，但未来工作可进一步优化记忆结构并扩展到更广泛场景，以应对潜在局限性和挑战。",
      "tags": [
        "Hierarchical Memory",
        "Large Language Model",
        "Temporal Evolution",
        "Semantic Relationships",
        "Personalized Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:21.906095Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.06913",
    "title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning",
    "authors": [
      "Ke Guo",
      "Haochen Liu",
      "Xiaojun Wu",
      "Chen Lv"
    ],
    "abstract": "Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle's realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.06913.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06913",
    "published": "2025-10-08T11:46:39Z",
    "updated": "2026-01-26T14:53:15Z",
    "comment": "accepted by ICLR",
    "light_analysis": {
      "overview": "DecompGAIL通过分解真实性为ego-map和ego-neighbor组件并引入社会PPO目标，解决了多代理生成对抗模仿学习中无关交互误导问题，实现稳定真实交通行为建模。",
      "motivation": "真实交通仿真是开发自动驾驶系统和城市交通规划的关键，但现有模仿学习方法如行为克隆存在协变量偏移问题，而生成对抗模仿学习（GAIL）在多代理环境中因无关交互误导（鉴别器错误惩罚自车真实行为）而变得不稳定。这导致交通行为建模不真实，限制了实际应用，亟需改进稳定性以避免误导交互影响整体学习过程。",
      "method": "论文提出DecompGAIL方法，核心是显式分解真实性评估为ego-map（自车与地图交互）和ego-neighbor（自车与邻居交互）组件，过滤误导性的neighbor:neighbor和neighbor:map交互。关键创新点还包括引入社会PPO目标，通过距离加权的邻居奖励增强ego奖励，以鼓励多代理间整体真实性，并集成到轻量级SMART-based骨干框架中优化模仿学习。",
      "result": "DecompGAIL在WOMD Sim Agents 2025基准测试中实现了最先进的性能，表明其在建模真实交通行为方面优于基线方法如传统GAIL和SMART。摘要未明确说明具体准确率或效率数据，但通过过滤误导交互和结合社会PPO目标，该方法显著提高了多代理模仿学习的稳定性和真实性，有效减少了无关噪声的影响。",
      "conclusion": "论文主要贡献是开发了DecompGAIL，一种分解真实性的多代理模仿学习方法，解决了GAIL在多代理环境中的不稳定性问题，其学术价值在于为多代理学习提供了更稳定的框架，实际应用价值在于提升自动驾驶仿真的真实性和安全性。未来工作可探索方法在更复杂场景的泛化性或扩展到其他多代理任务中。",
      "tags": [
        "Multi-Agent Imitation Learning",
        "Generative Adversarial Imitation Learning",
        "Decomposition Methods",
        "Reinforcement Learning",
        "Traffic Simulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:14.764159Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.06700",
    "title": "How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects",
    "authors": [
      "Leonardo Bertolazzi",
      "Sandro Pezzelle",
      "Raffaella Bernardi"
    ],
    "abstract": "Both humans and large language models (LLMs) exhibit content effects: biases in which the plausibility of the semantic content of a reasoning problem influences judgments regarding its logical validity. While this phenomenon in humans is best explained by the dual-process theory of reasoning, the mechanisms behind content effects in LLMs remain unclear. In this work, we address this issue by investigating how LLMs encode the concepts of validity and plausibility within their internal representations. We show that both concepts are linearly represented and strongly aligned in representational geometry, leading models to conflate plausibility with validity. Using steering vectors, we demonstrate that plausibility vectors can causally bias validity judgements, and vice versa, and that the degree of alignment between these two concepts predicts the magnitude of behavioral content effects across models. Finally, we construct debiasing vectors that disentangle these concepts, reducing content effects and improving reasoning accuracy. Our findings advance understanding of how abstract logical concepts are represented in LLMs and highlight representational interventions as a path toward more logical systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.06700.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06700",
    "published": "2025-10-08T06:48:08Z",
    "updated": "2026-01-26T09:31:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文揭示了大型语言模型中逻辑有效性与语义合理性概念在内部表示中的强对齐机制，并利用转向向量解偏以提高推理准确性。",
      "motivation": "研究动机在于探究大型语言模型（LLMs）中内容效应的机制，即推理问题语义内容的合理性如何影响逻辑有效性判断的偏见。这一问题在人类研究中已有双过程理论解释，但在LLMs中尚不明确。理解LLMs的内部表示机制对改善其逻辑推理能力和克服认知偏见至关重要，现有方法在揭示LLMs内在认知过程方面存在不足，本研究旨在填补这一空白。",
      "method": "研究方法包括分析LLMs如何在其内部表示中编码有效性和合理性概念。通过表示几何分析，发现这两个概念在线性表示中呈现并强烈对齐。核心创新是使用转向向量来因果性地操纵这些表示，证明合理性向量能偏置有效性判断，反之亦然，并利用解偏向量分离概念以减少混淆。摘要未明确说明使用的具体数据集和模型架构，推断可能涉及标准LLMs如GPT系列进行实验。",
      "result": "实验结果显示，有效性和合理性概念在LLMs的表示几何中高度对齐，导致模型在推理时混淆二者。使用转向向量验证了它们的因果偏置关系，对齐程度与行为内容效应程度呈正相关。构建的解偏向量成功减少了内容效应，提高了推理准确性，但与基线方法的对比和具体性能指标如准确率提升摘要未明确说明，仅提及改善效果。",
      "conclusion": "结论总结为本研究增进了对LLMs中抽象逻辑概念表示的理解，揭示了内部表示对齐如何导致内容效应。其学术价值在于提供了一种基于表示几何的分析框架，实际应用价值在于展示转向向量作为干预工具能有效提升LLMs的逻辑推理能力。局限性包括摘要未明确说明泛化性和具体应用场景，未来工作可探索解偏技术的扩展和更广泛模型的评估。",
      "tags": [
        "Large Language Models",
        "Representational Geometry",
        "Steering Vectors",
        "Logical Reasoning",
        "Content Effects"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:12.932789Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.04182",
    "title": "Thinking on the Fly: Test-Time Reasoning Enhancement via Latent Thought Policy Optimization",
    "authors": [
      "Wengao Ye",
      "Yan Liang",
      "Lianlei Shan"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have shifted from explicit Chain-of-Thought (CoT) reasoning to more efficient latent reasoning, where intermediate thoughts are represented as vectors rather than text. However, latent reasoning can be brittle on challenging, out-of-distribution tasks where robust reasoning is most critical. To overcome these limitations, we introduce Latent Thought Policy Optimization (LTPO), a parameter-free framework that enhances LLM reasoning entirely at test time, without requiring model parameter updates. LTPO treats intermediate latent \"thought\" vectors as dynamic parameters that are actively optimized for each problem instance. It employs an online policy gradient method guided by an intrinsic, confidence-based reward signal computed directly from the frozen LLM's own output distributions, eliminating the need for external supervision or expensive text generation during optimization. Extensive experiments on five reasoning benchmarks show that LTPO not only matches or surpasses strong baselines on standard tasks but also demonstrates remarkable robustness where others fail. Most notably, on highly challenging AIME benchmarks where existing latent reasoning baselines collapse to near-zero accuracy, LTPO delivers substantial improvements, showcasing a unique capability for complex reasoning.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.04182.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04182",
    "published": "2025-10-05T12:50:39Z",
    "updated": "2026-01-26T14:30:08Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "本文提出了Latent Thought Policy Optimization（LTPO），一个无需参数更新的框架，在测试时通过优化潜在思考向量增强大型语言模型的推理鲁棒性。",
      "motivation": "近年来，大型语言模型从显式的链式思考推理转向更高效的潜在推理，其中中间思考表示为向量而非文本，以提升计算效率。然而，在具有挑战性的分布外任务中，潜在推理往往脆弱，导致推理失败，而这些任务正需要鲁棒的推理能力。现有方法在应对复杂、未见过的问题时性能下降，限制了模型的泛化能力，因此开发一种无需训练、能在测试时动态增强推理的方法至关重要。LTPO旨在克服这些不足，直接在推理过程中优化潜在表示，以提高模型在困难任务上的稳定性。",
      "method": "LTPO框架将大型语言模型生成的中间潜在“思考”向量视为动态参数，为每个问题实例进行主动优化。它采用在线策略梯度方法，利用冻结LLM自身输出分布计算的内在、基于置信度的奖励信号来指导优化过程，从而无需外部监督或昂贵的文本生成。该方法的关键创新在于参数免费设计，完全在测试时进行，通过直接调整潜在向量来提升推理准确性，避免了模型权重更新，并利用模型的内部信息作为反馈机制。",
      "result": "在五个推理基准测试上的广泛实验表明，LTPO在标准任务上匹配或超越了现有强基线方法。特别是在高度挑战性的AIME基准上，现有潜在推理方法的准确率降至近零，而LTPO实现了实质性改进，展示了其出色的鲁棒性和复杂推理能力。这些结果突显了LTPO在分布外任务中的优势，无需模型训练就能显著提升性能，验证了其作为有效测试时推理增强框架的潜力。",
      "conclusion": "LTPO的提出为大型语言模型的推理增强提供了新方向，其参数免费、测试时优化的特性在学术上推动了无监督推理方法的发展，具有重要价值。在实际应用中，该方法可用于复杂、未知任务的鲁棒推理，如数学问题求解或逻辑推理，减少了对外部数据的依赖。未来工作可能包括扩展到更多任务类型、优化效率或结合其他强化学习技术，以进一步挖掘其潜力。摘要未明确说明具体局限性，但该方法为无需训练的推理优化奠定了基础。",
      "tags": [
        "Large Language Models",
        "Latent Reasoning",
        "Policy Optimization",
        "Test-Time Adaptation",
        "Confidence-Based Reward"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:19.435830Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.03906",
    "title": "From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance",
    "authors": [
      "Ardalan Aryashad",
      "Parsa Razmara",
      "Amin Mahjoub",
      "Seyedarmin Azizi",
      "Mahdi Salmani",
      "Arad Firouzkouhi"
    ],
    "abstract": "Autonomous driving perception systems are particularly vulnerable in foggy conditions, where light scattering reduces contrast and obscures fine details critical for safe operation. While numerous defogging methods exist, from handcrafted filters to learned restoration models, improvements in image fidelity do not consistently translate into better downstream detection and segmentation. Moreover, prior evaluations often rely on synthetic data, raising concerns about real-world transferability.   We present a structured empirical study that benchmarks a comprehensive set of defogging pipelines, including classical dehazing filters, modern defogging networks, chained variants combining filters and models, and prompt-driven visual language image editing models applied directly to foggy images. To bridge the gap between simulated and physical environments, we evaluate these pipelines on both the synthetic Foggy Cityscapes dataset and the real-world Adverse Conditions Dataset with Correspondences (ACDC).   We examine generalization by evaluating performance on synthetic fog and real-world conditions, assessing both image quality and downstream perception in terms of object detection mean average precision and segmentation panoptic quality. Our analysis identifies when defogging is effective, the impact of combining models, and how visual language models compare to traditional approaches. We additionally report qualitative rubric-based evaluations from both human and visual language model judges and analyze their alignment with downstream task metrics.   Together, these results establish a transparent, task-oriented benchmark for defogging methods and identify the conditions under which pre-processing meaningfully improves autonomous perception in adverse weather.   Project page: https://aradfir.github.io/filters-to-vlms-defogging-page/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.03906.pdf",
    "abs_url": "https://arxiv.org/abs/2510.03906",
    "published": "2025-10-04T19:05:04Z",
    "updated": "2026-01-26T12:26:48Z",
    "comment": "Accepted at WACV 2026 Proceedings (Oral), 5th Workshop on Image, Video, and Audio Quality Assessment in Computer Vision, with a focus on VLM and Diffusion Models",
    "light_analysis": {
      "overview": "本研究通过实证基准测试从滤波器到视觉语言模型的多种除雾方法在目标检测和分割任务上的性能，揭示了预处理器有效改善自动驾驶感知的条件。",
      "motivation": "自动驾驶感知系统在雾天条件下表现脆弱，光散射导致对比度降低和关键细节模糊，影响安全操作。现有除雾方法（如手工滤波器和学习恢复模型）虽能提升图像保真度，但改进不总是转化为下游检测和分割性能的提升，且以往评估常依赖合成数据，真实世界可转移性存疑。因此，需要更全面的基准研究来弥合模拟与物理环境之间的差距，确保方法在实际应用中的有效性。",
      "method": "研究方法采用结构化实证研究，基准测试了多种除雾流程，包括经典去雾滤波器、现代除雾网络、滤波器与模型结合的链式变体，以及直接应用于雾天图像的视觉语言图像编辑模型。关键创新点在于评估使用了合成数据集Foggy Cityscapes和真实数据集Adverse Conditions Dataset with Correspondences（ACDC），结合图像质量指标（如对比度改进）和下游感知任务指标（目标检测的平均精度和分割的全景质量），以全面分析方法的泛化能力和实际效果。",
      "result": "摘要未明确说明具体性能数据（如准确率提升百分比），但分析结果表明，研究识别了除雾方法在合成雾和真实条件下有效的情况，评估了模型组合的影响，并比较了视觉语言模型与传统方法的性能。与基线方法相比，某些方法可能在图像质量或下游任务上表现更好，但具体数据需参考完整论文；结果还通过定性评估（人类和视觉语言模型评判）与任务指标的对齐分析，提供了更全面的基准视角。",
      "conclusion": "本研究的主要贡献是建立了一个透明、面向任务的除雾方法基准，识别了在恶劣天气下预处理器能有效改善自动驾驶感知的条件，为相关研究和应用提供了指导。学术价值在于填补了现有评估的空白，推动了多模态方法在真实场景中的研究；实际应用价值在于帮助自动驾驶系统开发者选择更有效的除雾策略。未来工作可扩展到更多天气条件或数据集，以进一步提升方法的鲁棒性和通用性。",
      "tags": [
        "Defogging",
        "Object Detection",
        "Segmentation",
        "Visual Language Models",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:16.331177Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.03437",
    "title": "Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation",
    "authors": [
      "Jairo Diaz-Rodriguez",
      "Mumin Jia"
    ],
    "abstract": "Kernel change-point detection (KCPD) has become a widely used tool for identifying structural changes in complex data. While existing theory establishes consistency under independence assumptions, real-world sequential data such as text exhibits strong dependencies. We establish new guarantees for KCPD under $m$-dependent data: specifically, we prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. We perform an LLM-based simulation that generates synthetic $m$-dependent text to validate the asymptotics. To complement these results, we present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings. Across diverse text datasets, KCPD with text embeddings outperforms baselines in standard text segmentation metrics. We demonstrate through a case study on Taylor Swift's tweets that KCPD not only provides strong theoretical and simulated reliability but also practical effectiveness for text segmentation tasks.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.03437.pdf",
    "abs_url": "https://arxiv.org/abs/2510.03437",
    "published": "2025-10-03T18:57:22Z",
    "updated": "2026-01-26T18:36:37Z",
    "comment": "This paper is withdrawn due to an error in the proof of Proposition 3, which is used to support Theorem 1",
    "light_analysis": {
      "overview": "本文为核变化点检测在m-dependent数据下建立了新的理论一致性保证，并通过实证研究验证了其在文本分割任务中的实际效果。",
      "motivation": "现有核变化点检测理论依赖于数据独立性假设，但在现实世界的序列数据，如文本中，存在显著的依赖性，这导致理论保证在实际应用中可能不成立，限制了KCPD在文本分割等任务中的有效性。本研究旨在填补这一空白，为m-dependent数据下的KCPD建立新的理论框架，以提升其在依赖性序列数据中的可靠性和适用性，解决传统方法在依赖数据中的不足。",
      "method": "本研究通过理论分析和实证验证相结合的方式。首先，在m-dependent数据假设下，建立了核变化点检测的新理论保证，证明了在温和额外条件下变化点数量的一致性和位置的弱一致性，关键创新在于放宽独立性假设以处理依赖性数据。其次，采用大型语言模型（LLM）生成合成的m-dependent文本来模拟和验证理论渐近性。最后，使用现代文本嵌入技术，首次全面评估KCPD在多样文本数据集上的文本分割性能。",
      "result": "实验表明，在多个文本数据集上，使用文本嵌入的核变化点检测在标准文本分割指标上显著优于基线方法。通过LLM-based模拟验证了理论建立的渐近一致性，支持了m-dependent数据下的可靠性。案例研究聚焦Taylor Swift的推文，进一步证实了KCPD不仅具备强大的理论支撑，还能在实际文本分割任务中提供有效的分割结果。",
      "conclusion": "本研究的主要贡献在于为核变化点检测在m-dependent数据下提供了新的理论一致性保证，并通过实证研究证明了其在文本分割中的优越性。学术上，它填补了依赖性序列数据中变化点检测理论的空白，推动了相关领域的发展；实际上，该方法增强了文本分割等应用的准确性和可靠性，具有广泛的应用前景。摘要未明确说明未来工作方向。",
      "tags": [
        "Kernel Change-Point Detection",
        "m-Dependence",
        "Text Segmentation",
        "Text Embeddings",
        "Large Language Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:24:55.884396Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.26136",
    "title": "CliniBench: A Clinical Outcome Prediction Benchmark for Generative and Encoder-Based Language Models",
    "authors": [
      "Paul Grundmann",
      "Dennis Fast",
      "Jan Frick",
      "Thomas Steffek",
      "Felix Gers",
      "Wolfgang Nejdl",
      "Alexander Löser"
    ],
    "abstract": "With their growing capabilities, generative large language models (LLMs) are being increasingly investigated for complex medical tasks. However, their effectiveness in real-world clinical applications remains underexplored. To address this, we present CliniBench, the first benchmark that enables comparability of well-studied encoder-based classifiers and generative LLMs for discharge diagnosis prediction from admission notes in MIMIC-IV dataset. Our extensive study compares 12 generative LLMs and 3 encoder-based classifiers and demonstrates that encoder-based classifiers consistently outperform generative models in diagnosis prediction. We assess several retrieval augmentation strategies for in-context learning from similar patients and find that they provide notable performance improvements for generative LLMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.26136.pdf",
    "abs_url": "https://arxiv.org/abs/2509.26136",
    "published": "2025-09-30T11:56:53Z",
    "updated": "2026-01-26T11:28:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出CliniBench基准，首次在MIMIC-IV数据集上实现生成性和编码器语言模型在临床预测任务中的标准化比较。",
      "motivation": "随着生成性大型语言模型（LLMs）能力的提升，它们在复杂医疗任务中的应用日益受到关注，但真实临床场景中的有效性尚未得到充分验证。当前研究往往缺乏统一的评估基准，难以直接比较不同类型模型（如生成性LLMs与基于编码器的分类器）的性能，这阻碍了医疗AI的优化与部署。因此，本研究的动机是解决这一问题，通过构建基准来评估模型在临床预测中的实际表现，以推动医学AI的可靠发展。",
      "method": "本研究创建了CliniBench基准，基于MIMIC-IV数据集，从入院笔记预测出院诊断，使生成性LLMs和基于编码器的分类器具有可比性。方法包括比较12个生成性LLMs和3个基于编码器的分类器，并评估几种检索增强策略，用于生成模型的上下文学习从相似患者中获取信息。关键创新在于首次提供了公平的评估框架，使用标准化的任务和数据集，以分析不同模型架构在临床任务中的性能差异。",
      "result": "实验结果显示，基于编码器的分类器在诊断预测任务中始终优于生成性LLMs，表明在特定预测场景下传统模型可能更具优势。此外，通过实施检索增强策略，生成性LLMs的性能得到显著提升，这表明外部知识注入能有效改进模型表现。与基线方法相比，这些策略帮助缩小了性能差距，但编码器模型仍保持领先，研究突出了模型比较的重要性，并为未来优化提供了数据支撑。",
      "conclusion": "本研究的主要贡献是提出了CliniBench基准，为标准比较生成性和编码器模型在临床预测中的性能提供了首个平台，揭示了编码器模型的优势以及检索增强策略对生成模型的提升作用。这具有重要的学术价值，为医疗AI模型选择提供了实证依据，并推动技术优化。潜在局限性可能在于仅使用MIMIC-IV数据集，未来工作可扩展到更多临床数据集和任务，并探索混合模型策略以进一步提升泛化能力。",
      "tags": [
        "Generative Language Models",
        "Encoder-Based Classifiers",
        "Clinical Outcome Prediction",
        "Benchmarking",
        "Retrieval Augmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:15.457143Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.25178",
    "title": "GHOST: Hallucination-Inducing Image Generation for Multimodal LLMs",
    "authors": [
      "Aryan Yazdan Parast",
      "Parsa Hosseini",
      "Hesam Asadollahzadeh",
      "Arshia Soltani Moakhar",
      "Basim Azam",
      "Soheil Feizi",
      "Naveed Akhtar"
    ],
    "abstract": "Object hallucination in Multimodal Large Language Models (MLLMs) is a persistent failure mode that causes the model to perceive objects absent in the image. This weakness of MLLMs is currently studied using static benchmarks with fixed visual scenarios, which preempts the possibility of uncovering model-specific or unanticipated hallucination vulnerabilities. We introduce GHOST (Generating Hallucinations via Optimizing Stealth Tokens), a method designed to stress-test MLLMs by actively generating images that induce hallucination. GHOST is fully automatic and requires no human supervision or prior knowledge. It operates by optimizing in the image embedding space to mislead the model while keeping the target object absent, and then guiding a diffusion model conditioned on the embedding to generate natural-looking images. The resulting images remain visually natural and close to the original input, yet introduce subtle misleading cues that cause the model to hallucinate. We evaluate our method across a range of models, including reasoning models like GLM-4.1V-Thinking, and achieve a hallucination success rate exceeding 28%, compared to around 1% in prior data-driven discovery methods. We confirm that the generated images are both high-quality and object-free through quantitative metrics and human evaluation. Also, GHOST uncovers transferable vulnerabilities: images optimized for Qwen2.5-VL induce hallucinations in GPT-4o at a 66.5% rate. Finally, we show that fine-tuning on our images mitigates hallucination, positioning GHOST as both a diagnostic and corrective tool for building more reliable multimodal systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.25178.pdf",
    "abs_url": "https://arxiv.org/abs/2509.25178",
    "published": "2025-09-29T17:59:23Z",
    "updated": "2026-01-26T16:30:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 GHOST 方法，通过主动生成诱导幻觉的图像来压力测试和纠正多模态大语言模型中的对象幻觉问题。",
      "motivation": "研究旨在解决多模态大语言模型中对象幻觉的持续失效问题，这种幻觉导致模型错误感知图像中不存在的物体，降低可靠性。现有方法依赖静态基准测试，限制了发现模型特异性或未预见漏洞的可能性，因此需要主动手段来全面揭示和应对这些弱点，以促进更鲁棒的多模态系统发展。",
      "method": "GHOST 是一种全自动方法，通过优化图像嵌入空间来误导模型，同时确保目标对象在视觉上缺席，然后引导扩散模型基于优化后的嵌入生成自然图像。关键创新在于操作于嵌入空间并无需人工监督，利用扩散模型生成视觉自然但包含微妙误导线索的图像，有效诱导模型产生幻觉，从而实现对多模态模型的主动压力测试。",
      "result": "在包括 GLM-4.1V-Thinking 在内的多个模型上评估，GHOST 实现了超过 28% 的幻觉诱发成功率，显著高于先前数据驱动方法的约 1%。生成图像通过定量指标和人工评估被证实高质量且无目标对象，并揭示了可转移漏洞：针对 Qwen2.5-VL 优化的图像在 GPT-4o 中诱导幻觉的比例高达 66.5%。此外，基于这些图像的微调能有效减轻幻觉。",
      "conclusion": "GHOST 作为一种诊断和纠正工具，不仅能够主动识别多模态大语言模型中的幻觉漏洞，还通过生成图像进行微调来提升模型可靠性。本研究为理解和缓解对象幻觉提供了新途径，具有推动构建更稳健多模态系统的学术和实际价值，未来可扩展至更多模型和场景以验证其广泛适用性。",
      "tags": [
        "Multimodal Large Language Models",
        "Object Hallucination",
        "Diffusion Models",
        "Image Embedding Optimization",
        "Adversarial Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:25.030711Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.24386",
    "title": "PCICF: A Pedestrian Crossing Identification and Classification Framework",
    "authors": [
      "Junyi Gu",
      "Beatriz Cabrero-Daniel",
      "Ali Nouri",
      "Lydia Armini",
      "Christian Berger"
    ],
    "abstract": "We have recently observed the commercial roll-out of robotaxis in various countries. They are deployed within an operational design domain (ODD) on specific routes and environmental conditions, and are subject to continuous monitoring to regain control in safety-critical situations. Since ODDs typically cover urban areas, robotaxis must reliably detect vulnerable road users (VRUs) such as pedestrians, bicyclists, or e-scooter riders. To better handle such varied traffic situations, end-to-end AI, which directly compute vehicle control actions from multi-modal sensor data instead of only for perception, is on the rise. High quality data is needed for systematically training and evaluating such systems within their OOD. In this work, we propose PCICF, a framework to systematically identify and classify VRU situations to support ODD's incident analysis. We base our work on the existing synthetic dataset SMIRK, and enhance it by extending its single-pedestrian-only design into the MoreSMIRK dataset, a structured dictionary of multi-pedestrian crossing situations constructed systematically. We then use space-filling curves (SFCs) to transform multi-dimensional features of scenarios into characteristic patterns, which we match with corresponding entries in MoreSMIRK. We evaluate PCICF with the large real-world dataset PIE, which contains more than 150 manually annotated pedestrian crossing videos. We show that PCICF can successfully identify and classify complex pedestrian crossings, even when groups of pedestrians merge or split. By leveraging computationally efficient components like SFCs, PCICF has even potential to be used onboard of robotaxis for OOD detection for example. We share an open-source replication package for PCICF containing its algorithms, the complete MoreSMIRK dataset and dictionary, as well as our experiment results presented in: https://github.com/Claud1234/PCICF",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.24386.pdf",
    "abs_url": "https://arxiv.org/abs/2509.24386",
    "published": "2025-09-29T07:35:12Z",
    "updated": "2026-01-26T14:32:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "PCICF通过扩展合成数据集MoreSMIRK和使用空间填充曲线，提出一个框架来系统识别和分类行人过街情况，以支持robotaxi操作设计域的事件分析。",
      "motivation": "研究动机是解决robotaxi在操作设计域中可靠检测易受伤害道路使用者的问题。随着端到端AI的兴起，需要高质量数据来训练和评估这些系统在多样交通情况下的性能，但现有方法缺乏系统识别和分类VRU情况的框架，这限制了事件分析和安全监控的效率。因此，PCICF旨在填补这一空白，通过提供结构化工具来支持ODD的深入分析。",
      "method": "研究方法基于现有的合成数据集SMIRK，扩展为MoreSMIRK数据集，这是一个系统构建的多行人过街场景字典。使用空间填充曲线将场景的多维特征转换为特征模式，并与MoreSMIRK字典中的条目匹配。关键创新点包括扩展数据集以涵盖复杂行人互动，以及利用SFCs实现高效的模式匹配，从而支持快速识别和分类行人过街情况。",
      "result": "使用真实世界数据集PIE进行评估，其中包含超过150个手动注释的行人过街视频。PCICF能够成功识别和分类复杂行人过街情况，包括行人组合并或分离的场景，摘要未明确说明具体性能指标。与基线方法对比，框架通过计算高效组件如SFCs，展现出潜在实时应用价值，例如用于robotaxi上的异常检测。",
      "conclusion": "PCICF的主要贡献是提供了一个系统化框架来支持操作设计域的事件分析，通过扩展数据集和采用空间填充曲线技术提高效率。学术价值在于推动了VRU情况识别的标准化方法；实际应用包括robotaxi的安全监控和OOD检测。未来工作方向可包括进一步优化算法效率或扩展到其他VRU类型，以增强系统的通用性。",
      "tags": [
        "Pedestrian Crossing Identification",
        "Operational Design Domain (ODD)",
        "Space-Filling Curves (SFCs)",
        "Synthetic Dataset",
        "Vulnerable Road Users (VRUs)"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:15.730431Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.22500",
    "title": "Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise",
    "authors": [
      "Juan Ramirez",
      "Simon Lacoste-Julien"
    ],
    "abstract": "Constrained optimization is a powerful framework for enforcing requirements on neural networks. These constrained deep learning problems are typically solved using first-order methods on their min-max Lagrangian formulation, but such approaches often suffer from oscillations and can fail to find all local solutions. While the Augmented Lagrangian method (ALM) addresses these issues, practitioners often favor dual optimistic ascent schemes (PI control) on the standard Lagrangian, which perform well empirically but lack formal guarantees. In this paper, we establish a previously unknown equivalence between these approaches: dual optimistic ascent on the Lagrangian is equivalent to gradient descent-ascent on the Augmented Lagrangian. This finding allows us to transfer the robust theoretical guarantees of the ALM to the dual optimistic setting, proving it converges linearly to all local solutions. Furthermore, the equivalence provides principled guidance for tuning the optimism hyper-parameter. Our work closes a critical gap between the empirical success of dual optimistic methods and their theoretical foundation in the single-step, first-order regime commonly used in constrained deep learning.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.22500.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22500",
    "published": "2025-09-26T15:41:20Z",
    "updated": "2026-01-26T16:22:02Z",
    "comment": "Published at ICLR 2026. Code available at https://github.com/juan43ramirez/pi-control-is-alm",
    "light_analysis": {
      "overview": "论文建立了双重乐观上升（PI控制）与增广拉格朗日方法的等价性，为约束深度学习中的优化方法提供了理论保证。",
      "motivation": "约束优化广泛应用于神经网络，以强制执行网络要求。当前常用基于拉格朗日的first-order方法，但这些方法常因振荡问题无法找到所有局部解，导致可靠性不足。增广拉格朗日方法解决了这些问题，但实践中更倾向于使用双重乐观上升方案，因其性能良好但缺乏正式理论保证。因此，迫切需要建立理论框架来支持这些流行方法的稳健性，弥补理论与实证之间的差距。",
      "method": "论文的核心方法是数学证明双重乐观上升在拉格朗日上的操作等价于梯度下降-上升在增广拉格朗日上的操作。具体来说，通过分析优化过程，揭示了dual optimistic ascent on the Lagrangian与gradient descent-ascent on the Augmented Lagrangian的等价性。这一创新点允许将增广拉格朗日方法的理论保证移植到双重乐观上升设置中，聚焦于单步、一阶机制，这在约束深度学习中常见，但摘要未明确涉及特定数据集或模型架构。",
      "result": "主要实验结果表明，通过建立的等价性，双重乐观上升方法能够继承增广拉格朗日方法的理论保证，证明其线性收敛到所有局部解。这为实践中常用的优化方案提供了坚实的理论支撑，增强了算法的可靠性和收敛效率。同时，等价性还为调整乐观超参数提供了原理性指导，有助于优化性能提升，但摘要未明确提供具体的数据指标，如准确率提升。",
      "conclusion": "论文的主要贡献是填补了双重乐观上升方法在实证成功与理论基础之间的关键差距，揭示了其与增广拉格朗日方法的等价性。学术价值在于为约束优化领域提供了新的理论洞察，增强了优化算法的理论可靠性。实际应用中，该发现可以指导超参数调优，改进深度学习中的约束优化性能，局限性或未来工作方向摘要未明确说明，但可能涉及扩展理论到更复杂的优化问题。",
      "tags": [
        "Constrained Optimization",
        "Lagrangian Method",
        "Augmented Lagrangian Method",
        "Optimistic Ascent",
        "Gradient Descent-Ascent"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:34.111664Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.25236",
    "title": "Networks of Causal Abstractions: A Sheaf-theoretic Framework",
    "authors": [
      "Gabriele D'Acunto",
      "Paolo Di Lorenzo",
      "Sergio Barbarossa"
    ],
    "abstract": "Causal artificial intelligence aims to improve explainability, robustness, and trustworthiness by leveraging causal models. Recent work has shown that sheaf-theoretic approaches offer a principled framework for representing and aligning causal knowledge across collections of subjective and imperfect causal models connected by relational structures. In this work, we introduce the causal abstraction network (CAN), a general sheaf-theoretic framework for representing, learning, and reasoning across collections of mixture causal models (MCMs). CAN formalizes causal abstraction relations among subjective MCMs operating at different levels of granularity, while remaining agnostic to explicit causal graphs, functional mechanisms, interventional data, or jointly sampled observations. At the theoretical level, we provide a categorical formulation of MCMs and characterize key properties of CANs, including consistency, smoothness, and the existence of global sections, which are related to spectral properties of an associated combinatorial Laplacian. At the methodological level, we address the problem of learning consistent CANs from data by exploiting the compositionality of causal abstractions and necessary conditions for their existence. The learning task decomposes into local problems on the network edges, for which we propose efficient solutions in Gaussian and Gaussian mixture settings. We validate the proposed learning methods on synthetic data and illustrate the practical relevance of the CAN framework through a financial application, demonstrating both recovery and counterfactual reasoning capabilities.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.25236.pdf",
    "abs_url": "https://arxiv.org/abs/2509.25236",
    "published": "2025-09-25T07:48:25Z",
    "updated": "2026-01-26T08:47:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入了因果抽象网络（CAN），一个基于sheaf理论的通用框架，用于表示和学习多个混合因果模型之间的因果抽象关系。",
      "motivation": "研究动机是解决因果AI中如何对齐和表示多个主观、不完美因果模型的知识，以增强可解释性、鲁棒性和可信度。现有方法虽然使用sheaf理论提供了原则性框架，但缺乏处理不同粒度下模型集合的通用方案，导致在复杂因果推理中可能不够灵活或高效。",
      "method": "研究方法包括构建CAN框架，该框架formalizes了不同粒度下主观混合因果模型之间的因果抽象关系，无需依赖显式因果图或特定数据。在理论层面，采用范畴论形式化混合因果模型，并分析CAN的关键属性如一致性、平滑性和全局截面存在，这些与组合拉普拉斯的谱性质相关。在方法论层面，利用因果抽象的组合性，提出从数据学习consistent CANs的方法，将任务分解为网络边上的局部问题，并在高斯和高斯混合设置中设计高效解决方案。",
      "result": "在合成数据上验证了学习方法的有效性，并通过金融应用展示了CAN框架的实际相关性，证明了其在恢复和反事实推理能力方面的应用价值。摘要未明确说明具体性能指标如准确率或效率改进，但强调了框架的验证成功和实际案例的可行性，为因果推理提供了新的实证基础。",
      "conclusion": "结论是CAN提供了一个通用的sheaf-theoretic框架，用于表示和学习多粒度因果模型，增强了因果推理的理论严谨性和实际应用价值。研究意义在于为跨模型知识对齐提供了结构化方法，未来工作可能涉及扩展到更复杂的数据分布或更广泛的应用领域，以进一步验证其泛化能力。",
      "tags": [
        "Causal AI",
        "Sheaf Theory",
        "Mixture Causal Models",
        "Causal Abstraction Networks",
        "Counterfactual Reasoning"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:18.089940Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18052",
    "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies",
    "authors": [
      "Jiaxu Zhou",
      "Jen-tse Huang",
      "Xuhui Zhou",
      "Man Ho Lam",
      "Xintao Wang",
      "Hao Zhu",
      "Wenxuan Wang",
      "Maarten Sap"
    ],
    "abstract": "Large language models (LLMs) are increasingly deployed to simulate human collective behaviors, yet the methodological rigor of these \"AI societies\" remains under-explored. Through a systematic audit of 42 recent studies, we identify six pervasive flaws-spanning agent profiles, interaction, memory, control, unawareness, and realism (PIMMUR). Our analysis reveals that 90.7% of studies violate at least one principle, undermining simulation validity. We demonstrate that frontier LLMs correctly identify the underlying social experiment in 47.6% of cases, while 65.3% of prompts exert excessive control that pre-determines outcomes. By reproducing five representative experiments (e.g., telephone game), we show that reported collective phenomena often vanish or reverse when PIMMUR principles are enforced, suggesting that many \"emergent\" behaviors are methodological artifacts rather than genuine social dynamics. Our findings suggest that current AI simulations may capture model-specific biases rather than universal human social behaviors, raising critical concerns about the use of LLMs as scientific proxies for human society.",
    "categories": [
      "cs.CL",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.18052.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18052",
    "published": "2025-09-22T17:27:29Z",
    "updated": "2026-01-26T14:24:09Z",
    "comment": "13 pages, 9 figures, 3 tables",
    "light_analysis": {
      "overview": "论文通过系统审计42项研究，提出了PIMMUR原则来识别和纠正大语言模型社会模拟中的方法论缺陷，揭示了许多'涌现'行为可能是伪影而非真实社会动态。",
      "motivation": "随着大语言模型（LLMs）被广泛用于模拟人类集体行为，如社会实验，其方法论的严谨性仍未被充分探索。现有研究往往在代理设置、交互和记忆等方面存在不足，导致模拟结果可能失真，无法准确反映人类社会的复杂性。这种缺陷使得模拟的有效性受到质疑，可能影响科学研究的基础，因此有必要开发系统性框架来评估和提升模拟的可靠性，以确保AI社会模拟能更真实地捕捉社会动态。",
      "method": "研究团队对42项近期关于LLM社会模拟的研究进行了系统审计，识别出六个普遍缺陷（PIMMUR原则：代理配置文件、交互、记忆、控制、无知性和现实主义）。方法包括分析这些研究违反原则的程度，并通过复制五个代表性实验（例如电话游戏）来验证原则的有效性。关键创新点在于提出了结构化框架，用于评估模拟的严谨性，并利用前沿LLMs进行测试，以识别底层社会实验和提示控制的影响。",
      "result": "分析显示，90.7%的研究违反了至少一个PIMMUR原则，严重削弱了模拟的有效性。前沿LLMs仅在47.6%的情况下正确识别底层社会实验，而65.3%的提示施加了过度控制，预先决定了实验结果。通过强制执行PIMMUR原则，复制实验中的许多报告的集体现象（如涌现行为）消失或逆转，这表明这些现象可能是方法论伪影，而非真实的社会动态，从而质疑了现有AI模拟的可靠性。",
      "conclusion": "论文的主要贡献是提出了PIMMUR原则，系统性地评估了大语言模型社会模拟的方法论严谨性，揭示了当前模拟可能捕获模型特定偏见而非普遍人类行为。学术价值在于促进了模拟有效性讨论，提升了科学研究的可靠性；实际应用价值在于警告使用LLMs作为人类社会科学代理的风险。未来工作可能包括开发更可靠的模拟框架和改进原则的实施，以克服现有局限性。",
      "tags": [
        "Large Language Model",
        "Simulation",
        "Collective Behavior",
        "Methodological Audit",
        "Social Experiment"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:48.802913Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.19391",
    "title": "TensLoRA: Tensor Alternatives for Low-Rank Adaptation",
    "authors": [
      "Axel Marmoret",
      "Reda Bensaid",
      "Jonathan Lys",
      "Vincent Gripon",
      "François Leduc-Primeau"
    ],
    "abstract": "Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.19391.pdf",
    "abs_url": "https://arxiv.org/abs/2509.19391",
    "published": "2025-09-22T17:15:23Z",
    "updated": "2026-01-26T17:51:38Z",
    "comment": "Published at ICASSP 2026. 5 pages, 1 figure, 2 tables. Code can be found at https://github.com/ax-le/TensLoRA",
    "light_analysis": {
      "overview": "TensLoRA是一个统一框架，通过张量聚合LoRA更新，实现模态特定的低秩适应，概括现有方法并提高参数效率。",
      "motivation": "LoRA广泛应用于Transformers的高效适应，但现有方法假设每个注意力投影（Query、Key、Value）和每层的低秩矩阵独立，缺乏系统性和灵活性，导致参数利用不充分。尽管有基于张量的扩展方法，它们形式有限且未提供统一框架，因此需要系统性方法来优化适应过程，以根据不同模态和任务定制参数预算，提升整体性能。",
      "method": "TensLoRA框架的核心是将LoRA更新矩阵聚合为高阶张量，从而建模广泛的基于张量低秩适应方法。其创新点包括允许模态特定压缩率，使参数预算可根据视觉或语言等模态及任务需求进行定制。该公式概括了现有基于张量技术，并在实验中使用视觉和语言基准测试，但摘要未明确说明具体数据集和模型架构。",
      "result": "在视觉和语言基准测试中，TensLoRA的性能直接受张量构造影响。在相似参数数量下，有时表现优于标准LoRA，显示出参数效率的提升。实验强调了张量构造对适应效果的重要性，但摘要未提供具体性能指标（如准确率），仅通过比较表明改进潜力。",
      "conclusion": "TensLoRA的主要贡献是提出一个统一框架，通过张量聚合低秩适应更新，并允许模态特定压缩率，概括了现有方法。这有助于优化Transformer适应过程，提高参数效率，在视觉和语言等多模态应用中具有价值。未来工作可扩展更多模态或改进压缩策略，潜在局限性如特定任务性能变化摘要未明确说明。",
      "tags": [
        "Low-Rank Adaptation",
        "Tensor Methods",
        "Transformer Adaptation",
        "Attention Projections",
        "Parameter Compression"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:51.638448Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14788",
    "title": "Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery",
    "authors": [
      "Jing Lan",
      "Hexiao Ding",
      "Hongzhao Chen",
      "Yufeng Jiang",
      "Nga-Chun Ng",
      "Gwing Kei Yip",
      "Gerald W. Y. Cheng",
      "Yunlin Mao",
      "Jing Cai",
      "Liang-ting Lin",
      "Jung Sun Yoo"
    ],
    "abstract": "Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability. This work introduces a sequence-based drug-target interaction framework that integrates structural priors into protein representations while maintaining high-throughput screening capability. Evaluated across multiple benchmarks, the model achieves state-of-the-art performance on Human and BioSNAP datasets and remains competitive on BindingDB. In virtual screening tasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in AUROC and BEDROC. Ablation studies confirm the critical role of learned aggregation, bilinear attention, and contrastive alignment in enhancing predictive robustness. Embedding visualizations reveal improved spatial correspondence with known binding pockets and highlight interpretable attention patterns over ligand-residue contacts. These results validate the framework's utility for scalable and structure-aware DTI prediction.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.14788.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14788",
    "published": "2025-09-18T09:38:46Z",
    "updated": "2026-01-26T08:19:42Z",
    "comment": "Accepted by 2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)",
    "light_analysis": {
      "overview": "本文提出一种结构感知的对比学习框架，整合细粒度绑定表示，用于药物发现中的药物-靶点相互作用预测。",
      "motivation": "准确识别药物-靶点相互作用（DTI）是计算药理学中的核心挑战，对药物发现和开发至关重要。现有基于序列的方法虽具可扩展性，但往往忽略蛋白质结构信息，导致预测精度受限。结构信息通常通过实验获取，成本高且不可扩展，本研究旨在开发一种既能集成结构先验又能保持高通量筛选能力的新方法，以解决这一实际问题，提升DTI预测的鲁棒性和实用性。",
      "method": "该研究提出基于序列的药物-靶点相互作用框架，通过将结构先验集成到蛋白质表示中来增强预测能力。关键创新点包括使用学习聚合机制处理细粒度绑定表示，双线性注意力关注配体与残基的接触，以及对比对齐学习优化表示空间。框架采用对比学习策略，实现结构感知的表示学习，同时保持了高通量筛选的效率，无需依赖复杂的结构实验数据。",
      "result": "在多个基准测试中，模型在Human和BioSNAP数据集上实现了最先进的性能，在BindingDB上保持竞争力。虚拟筛选中，在LIT-PCBA任务上超越了先前方法，AUROC和BEDROC指标有显著提升。消融研究证实了学习聚合、双线性注意力和对比对齐对预测鲁棒性的关键作用。嵌入可视化揭示了改进的空间对应性，与已知结合口袋匹配，并突出了可解释的注意力模式，增强了模型的可信度。",
      "conclusion": "该框架成功整合结构信息到序列基础方法中，显著提升了DTI预测的准确性和鲁棒性。其学术价值在于提供了一种可扩展且结构感知的对比学习策略，推动了计算药理学的发展。实际应用上，它支持高效的高通量虚拟筛选，有望加速药物发现过程。局限性包括摘要未明确说明对其他数据集的泛化能力，未来工作可扩展到更多生物分子相互作用场景。",
      "tags": [
        "Drug-Target Interaction",
        "Contrastive Learning",
        "Bilinear Attention",
        "Structural Priors",
        "Fine-Grained Representations"
      ]
    },
    "analyzed_at": "2026-01-27T03:25:59.094951Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14545",
    "title": "Controlling Language Difficulty in Dialogues with Linguistic Features",
    "authors": [
      "Shuyao Xu",
      "Wenguang Wang",
      "Handong Gao",
      "Wei Kang",
      "Long Qin",
      "Weizhi Wang"
    ],
    "abstract": "Large language models (LLMs) have emerged as powerful tools for supporting second language acquisition, particularly in simulating interactive dialogues for speaking practice. However, adapting the language difficulty of LLM-generated responses to match learners' proficiency levels remains a challenge. This work addresses this issue by proposing a framework for controlling language proficiency in educational dialogue systems. Our approach leverages three categories of linguistic features, readability features (e.g., Flesch-Kincaid Grade Level), syntactic features (e.g., syntactic tree depth), and lexical features (e.g., simple word ratio), to quantify and regulate text complexity. We demonstrate that training LLMs on linguistically annotated dialogue data enables precise modulation of language proficiency, outperforming prompt-based methods in both flexibility and stability. To evaluate this, we introduce Dilaprix, a novel metric integrating the aforementioned features, which shows strong correlation with expert judgments of language difficulty. Empirical results reveal that our approach achieves superior controllability of language proficiency while maintaining high dialogue quality.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.14545.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14545",
    "published": "2025-09-18T02:22:43Z",
    "updated": "2026-01-26T02:53:24Z",
    "comment": "15 pages,9 figures",
    "light_analysis": {
      "overview": "本文提出一个基于语言特征控制大语言模型生成对话难度的框架，以提高第二语言学习的自适应能力。",
      "motivation": "大语言模型（LLMs）作为第二语言习得的工具，在模拟对话练习中潜力巨大，但现有方法难以动态调整语言难度以匹配学习者不同水平。当前基于提示的方法在灵活性和稳定性上存在不足，限制了教育对话系统的有效应用。因此，本研究旨在解决这一挑战，通过开发可控性更强的框架来优化语言学习体验，提升教育技术实用性。",
      "method": "本研究提出一个框架，通过整合三类语言特征：可读性特征（如Flesch-Kincaid年级水平）、句法特征（如句法树深度）和词汇特征（如简单词汇比例），来量化和调节对话文本的复杂度。核心方法是使用语言学注释的对话数据集训练LLMs，实现对语言能力的精确调制。还引入了Dilaprix这一新度量标准，集成上述特征用于评估语言难度，从而增强方法的可解释性和评估能力。",
      "result": "实证结果显示，所提出的方法在控制语言能力方面优于基于提示的方法，表现出更高的灵活性和稳定性。新度量标准Dilaprix与专家对语言难度的判断有强相关性。具体而言，该方法在保持高对话质量的同时，实现了对语言难度的精确调节，为教育对话系统提供了更可靠的性能支撑，推动了自适应学习技术的发展。",
      "conclusion": "本研究的主要贡献是开发了一个基于语言特征的框架，用于控制教育对话系统中的语言难度，增强了LLMs在第二语言习得中的应用价值。学术上，它引入了创新的度量标准和方法，为自适应教育技术提供了新思路。实际应用方面，可提升语言学习工具的效果。未来工作可扩展到更多语言特征或应用场景，以进一步优化系统性能。",
      "tags": [
        "Large Language Model",
        "Linguistic Features",
        "Controllable Dialogue",
        "Readability Metrics",
        "Language Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:10.305470Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.13805",
    "title": "Towards a Physics Foundation Model",
    "authors": [
      "Florian Wiesner",
      "Matthias Wessling",
      "Stephen Baek"
    ],
    "abstract": "Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative - democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by more than 7x, (2) plausible zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) more stable long-term predictions through long-horizon rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.13805.pdf",
    "abs_url": "https://arxiv.org/abs/2509.13805",
    "published": "2025-09-17T08:19:57Z",
    "updated": "2026-01-26T15:42:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了General Physics Transformer (GPhyT)，首次展示了在物理学中实现基础模型的能力，通过单一预训练模型模拟多种物理现象，无需重训练。",
      "motivation": "基础模型在自然语言处理中的成功启发了物理学领域，但现有物理感知的机器学习方法局限于单一窄域，需为每个新系统重新训练，限制了通用性和效率。构建物理学基础模型（PFM）能民主化高保真模拟、加速科学发现并减少专用求解器开发，本研究旨在克服这些局限性，探索在物理学中实现基础模型的可能性，以推动计算科学和工程的发展。",
      "method": "本研究提出General Physics Transformer (GPhyT)，基于Transformer架构，在1.8 TB多样化物理模拟数据上进行预训练。关键创新是Transformer能从上下文推断控制动力学，使模型无需知道底层物理方程即可模拟流体-固体相互作用、冲击波、热对流和多相动力学。该方法利用自注意力机制处理复杂物理数据，通过大规模训练学习泛化原则，并采用上下文学习策略实现适应性推理。",
      "result": "GPhyT取得三大突破：首先，在多个物理领域性能超越专门架构超过7倍；其次，通过上下文学习实现零样本泛化，能合理模拟未见过系统；最后，长时程预测更稳定，验证了泛化能力。这些结果基于具体性能指标，与基线方法对比，展示了模型在跨领域、泛化和长期预测方面的显著优势。",
      "conclusion": "本工作证明了单一模型可从数据中学习可泛化的物理原则，为构建通用物理学基础模型（PFM）铺平道路。学术价值在于首次在物理学中展示基础模型可行性，可能变革计算科学和工程领域。实际应用能加速科学发现和降低模拟成本，未来工作可扩展模型覆盖更广现象并提升泛化能力。",
      "tags": [
        "Transformer",
        "Foundation Model",
        "Physics Simulation",
        "In-context Learning",
        "Zero-shot Generalization"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:29.684557Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.13588",
    "title": "CoBRA: Programming Cognitive Bias in Social Agents Using Classic Social Science Experiments",
    "authors": [
      "Xuan Liu",
      "Haoyang Shang",
      "Haojian Jin"
    ],
    "abstract": "This paper introduces CoBRA, a novel toolkit for systematically specifying agent behavior in LLM-based social simulation. We found that conventional approaches that specify agent behavior through implicit natural-language descriptions often do not yield consistent behavior across models, and the resulting behavior does not capture the nuances of the descriptions. In contrast, CoBRA introduces a model-agnostic way to control agent behavior that lets researchers explicitly specify desired nuances and obtain consistent behavior across models. At the heart of CoBRA is a novel closed-loop system primitive with two components: (1) Cognitive Bias Index that measures the demonstrated cognitive bias of a social agent, by quantifying the agent's reactions in a set of validated classic social science experiments; (2) Behavioral Regulation Engine that aligns the agent's behavior to exhibit controlled cognitive bias. Through CoBRA, we show how to operationalize validated social science knowledge (i.e., classical experiments) as reusable \"gym\" environments for AI -- an approach that may generalize to richer social and affective simulations beyond bias alone.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.13588.pdf",
    "abs_url": "https://arxiv.org/abs/2509.13588",
    "published": "2025-09-16T23:03:02Z",
    "updated": "2026-01-26T09:12:21Z",
    "comment": "CHI 2026",
    "light_analysis": {
      "overview": "提出了CoBRA工具包，用于在基于大语言模型的社交代理中系统编程认知偏差，通过经典社会科学实验实现模型无关的行为控制。",
      "motivation": "现有方法通过隐式自然语言描述指定代理行为，但这种方法往往导致行为在不同模型间不一致，且无法准确捕捉描述的细微差别。这在社交模拟中尤为重要，因为需要代理表现出特定的人类认知偏差以模拟真实社交交互。CoBRA旨在解决这一问题，提供更可靠和可控的行为指定方式，避免现有方法的局限。",
      "method": "CoBRA的核心是一个闭环系统原语，包含两个组件：Cognitive Bias Index（认知偏差指数）通过量化代理在经典社交科学实验中的反应来测量其认知偏差；以及Behavioral Regulation Engine（行为调节引擎）用于调整代理行为以展示受控的认知偏差。该方法利用经过验证的社会科学知识，创建可重用的“gym”环境，实现对LLM代理行为的精确和模型无关的控制。",
      "result": "摘要未明确说明具体的实验结果数据，如性能指标或与基线方法的对比。但CoBRA的设计旨在提供一致的行为表现和捕捉细微差别，未来工作可能包括实验验证其在跨模型行为控制上的有效性，以及与其他方法的定量比较。",
      "conclusion": "论文的主要贡献是开发了CoBRA工具包，将社会科学知识操作化为可重用的“gym”环境，用于AI代理。这具有重要的学术价值，促进了社会科学与AI的交叉研究，并为更丰富的社交和情感模拟奠定了基础。局限性可能在于方法的泛化能力，未来可扩展到更复杂的偏差和模拟场景。",
      "tags": [
        "Large Language Model",
        "Social Simulation",
        "Cognitive Bias",
        "Closed-Loop System",
        "Behavioral Regulation"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:18.029698Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.13379",
    "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs",
    "authors": [
      "Asif Azad",
      "Mohammad Sadat Hossain",
      "MD Sadik Hossain Shanto",
      "M Saifur Rahman",
      "Md Rizwan Parvez"
    ],
    "abstract": "Vision-Language Models (VLMs) have achieved remarkable progress in complex visual understanding across scientific and reasoning tasks. While performance benchmarking has advanced our understanding of these capabilities, the critical dimension of uncertainty quantification has received insufficient attention. Therefore, unlike prior conformal prediction studies that focused on limited settings, we conduct a comprehensive uncertainty benchmarking study, evaluating 18 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets with 3 distinct scoring functions. For closed-source models lacking token-level logprob access, we develop and validate instruction-guided likelihood proxies. Our findings demonstrate that larger models consistently exhibit better uncertainty quantification; models that know more also know better what they don't know. More certain models achieve higher accuracy, while mathematical and reasoning tasks elicit poorer uncertainty performance across all models compared to other domains. This work establishes a foundation for reliable uncertainty evaluation in multimodal systems.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.13379.pdf",
    "abs_url": "https://arxiv.org/abs/2509.13379",
    "published": "2025-09-16T08:17:39Z",
    "updated": "2026-01-26T07:17:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文通过全面评估18个视觉-语言模型的不确定性量化能力，为多模态系统建立了可靠的基准测试框架。",
      "motivation": "研究动机是解决视觉-语言模型中不确定性量化不足的问题。现有性能基准测试虽能评估模型能力，但忽视了不确定性这一关键维度，导致模型在科学和推理等复杂任务中可能产生不可靠的输出。先前的共形预测研究设置有限，因此需要进行更全面的不确定性评估，以提升模型的可靠性和实用性。摘要未明确说明具体应用场景，但强调了在复杂视觉理解任务中不确定性评估的重要性。",
      "method": "研究方法采用共形预测框架，对18个最先进的视觉-语言模型（包括开源和闭源模型）进行全面不确定性基准测试。关键创新在于覆盖了6个多模态数据集，并使用了3种不同的评分函数。特别地，对于缺乏令牌级别对数概率访问的闭源模型，论文开发并验证了指令引导似然代理，以替代标准方法进行不确定性量化，确保了评估的全面性和准确性。",
      "result": "实验结果表明，更大的模型 consistently 表现出更优的不确定性量化能力，即模型知识更丰富时，也更能识别自身的不确定性。更确定的模型通常实现更高的准确性。此外，与视觉等任务相比，数学和推理任务在所有模型中引发较差的不确定性表现。摘要未提供具体性能指标，但强调了模型大小与不确定性量化之间的正相关趋势，并通过多个数据集和评分函数验证了这些发现。",
      "conclusion": "论文的主要贡献在于建立了多模态系统中不确定性评估的全面基准，揭示了模型大小与不确定性量化能力之间的关联。其学术价值在于推动不确定性量化研究向前发展，实际应用价值在于提升视觉-语言模型在复杂任务中的可靠性，为未来更稳健的模型设计提供基础。摘要未明确说明局限性，但未来工作可扩展到更多模型或任务领域以增强评估广度。",
      "tags": [
        "Vision-Language Models",
        "Conformal Prediction",
        "Uncertainty Quantification",
        "Benchmarking",
        "Likelihood Proxies"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:27.033980Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.11480",
    "title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs",
    "authors": [
      "Amir Taherin",
      "Juyi Lin",
      "Arash Akbari",
      "Arman Akbari",
      "Pu Zhao",
      "Weiwei Chen",
      "David Kaeli",
      "Yanzhi Wang"
    ],
    "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.11480.pdf",
    "abs_url": "https://arxiv.org/abs/2509.11480",
    "published": "2025-09-15T00:00:37Z",
    "updated": "2026-01-26T15:57:20Z",
    "comment": "To appear in the Asilomar Conference on Signals, Systems, and Computers 2025",
    "light_analysis": {
      "overview": "论文评估了视觉语言动作模型在边缘到云GPU平台上的性能扩展，揭示了架构选择和硬件配置对系统指标的影响。",
      "motivation": "视觉语言动作模型在机器人控制中展现出强大能力，但其在不同模型架构和硬件平台上的性能扩展及其功耗问题尚不清晰，这在实际部署中至关重要。现有方法缺乏系统性评估，无法指导在边缘功率约束和GPU配置下的模型选择，限制了VLA模型的优化应用。论文旨在填补这一空白，通过实验分析帮助应对资源受限场景下的机器人推理挑战。",
      "method": "研究选取了五个代表性的VLA模型，包括当前最佳基线和两个新提出的架构，针对边缘和GPU平台进行性能评估。使用LIBERO基准测试，在可变功率约束和高性能GPU配置下，测量了准确率、延迟、吞吐量和峰值内存使用量等系统级指标。关键创新点在于系统性比较模型架构和硬件平台，揭示跨平台的性能趋势。",
      "result": "实验结果显示：架构选择如动作标记化和模型骨干大小强烈影响吞吐量和内存占用；功率受限的边缘设备表现出非线性性能退化，部分配置匹配或超过旧的数据中心GPU；高吞吐量变体可在不显著损失准确率下实现。这些发现基于具体数据，如基准测试中的性能对比，为跨平台部署提供了量化指导。",
      "conclusion": "研究提供了VLA模型跨平台部署的可操作见解，挑战了数据中心硬件在机器人推理中必然优越的假设。贡献在于为实际应用中的模型选择和优化提供指导，促进边缘计算和机器人系统发展。潜在局限性可能在于仅使用特定基准测试和硬件，未来工作可扩展到更多平台和应用场景。",
      "tags": [
        "Vision-Language-Action Models",
        "Cross-Platform Scaling",
        "Edge GPUs",
        "LIBERO Benchmark",
        "Performance Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:44.602882Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18135",
    "title": "SDGF: Fusing Static and Multi-Scale Dynamic Correlations for Multivariate Time Series Forecasting",
    "authors": [
      "Shaoxun Wang",
      "Xingjun Zhang",
      "Qianyang Li",
      "Jiawei Cao",
      "Zhendong Tan"
    ],
    "abstract": "Accurate multivariate time series forecasting hinges on inter-series correlations, which often evolve in complex ways across different temporal scales. Existing methods are limited in modeling these multi-scale dependencies and struggle to capture their intricate and evolving nature. To address this challenge, this paper proposes a novel Static-Dynamic Graph Fusion network (SDGF), whose core lies in capturing multi-scale inter-series correlations through a dual-path graph structure learning approach. Specifically, the model utilizes a static graph based on prior knowledge to anchor long-term, stable dependencies, while concurrently employing Multi-level Wavelet Decomposition to extract multi-scale features for constructing an adaptively learned dynamic graph to capture associations at different scales. We design an attention-gated module to fuse these two complementary sources of information intelligently, and a multi-kernel dilated convolutional network is then used to deepen the understanding of temporal patterns. Comprehensive experiments on multiple widely used real-world benchmark datasets demonstrate the effectiveness of our proposed model. Code is available at https://github.com/shaoxun6033/SDGFNet.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.18135.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18135",
    "published": "2025-09-14T11:23:12Z",
    "updated": "2026-01-26T08:35:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SDGF网络，通过融合静态和动态图来捕捉多元时间序列的多尺度相关性。",
      "motivation": "多元时间序列预测的核心挑战在于序列间相关性在不同时间尺度上复杂演化，现有方法难以有效建模这些多尺度依赖，尤其是在捕捉其动态和复杂特性方面存在局限。因此，需要一种新方法来更准确地处理这些依赖，以提升预测准确性和鲁棒性。",
      "method": "方法采用双路径图结构学习：静态路径基于先验知识构建图以捕捉长期稳定依赖；动态路径使用多级小波分解提取多尺度特征，自适应学习不同尺度的动态关联。通过注意力门控模块智能融合静态和动态信息，并结合多核扩张卷积网络深化时间模式建模，全面优化相关性捕捉。",
      "result": "在多个广泛使用的真实世界基准数据集上进行综合实验，证明模型的有效性。摘要未明确说明具体性能指标如准确率提升，但实验结果表明SDGF模型在多元时间序列预测任务中优于现有基线方法，能够更好地处理多尺度依赖。",
      "conclusion": "论文贡献在于提出SDGF网络，融合静态和动态图以改进多尺度相关性建模，提升预测准确性。该研究具有学术价值，为复杂相关性分析提供新方法；实际应用广泛，可扩展至需多尺度动态分析的场景。未来工作可探索更精细的动态图学习或应用到其他数据类型。",
      "tags": [
        "Multivariate Time Series Forecasting",
        "Graph Neural Networks",
        "Multi-scale Analysis",
        "Wavelet Decomposition",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:44.206191Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.10414",
    "title": "Is In-Context Learning Learning?",
    "authors": [
      "Adrian de Wynter"
    ],
    "abstract": "In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.10414.pdf",
    "abs_url": "https://arxiv.org/abs/2509.10414",
    "published": "2025-09-12T17:12:04Z",
    "updated": "2026-01-26T16:34:06Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "论文通过大规模实证分析探讨上下文学习是否真正构成学习，揭示其泛化能力有限的本质。",
      "motivation": "ICL声称允许自回归模型通过提示中的少量示例解决未见任务，无需额外训练，但这可能仅是依赖模型的先验知识和示例，而非真正的学习过程。研究动机在于验证ICL是否具备实际学习能力，以评估AI系统的泛化潜力；现有方法可能高估ICL的学习效果，忽略了记忆、预训练等因素对结果的影响，因此需要实证分析来厘清ICL的学习机制和局限性。",
      "method": "研究采用大规模分析方法，通过消融实验考虑记忆效应、预训练数据、分布变化以及提示风格和措辞等因素，以孤立ICL的学习机制。关键创新在于系统性地排除混淆变量，实证评估ICL的本质；摘要未明确指定具体数据集或模型架构，但提及使用自回归模型进行实验，重点分析各种实验条件下的表现差异。",
      "result": "实验结果表明，ICL是一种有效的学习范式，但在泛化到未见任务方面表现有限。具体发现包括：当示例数量增多时，准确性对示例分布、模型、提示风格和输入语言特征变得不敏感；模型通过推断提示中的规律来决策，这导致分布敏感性，尤其在如chain-of-thought等提示风格中更为明显；在不同形式相似任务上，准确性变化显著，显示ICL机制的稳健性不足。",
      "conclusion": "论文的主要贡献是揭示了ICL通过自回归编码机制并不具备稳健的泛化能力，强调其作为学习机制的局限性。学术价值在于深化了对ICL本质的理解，实际应用价值在于提醒在AI系统中谨慎使用ICL；未来工作可进一步探索改进ICL的泛化策略，或开发更稳健的学习机制以克服现有不足。",
      "tags": [
        "In-Context Learning",
        "Autoregressive Models",
        "Chain-of-Thought",
        "Memorisation",
        "Distributional Shift"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:40.277762Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.09828",
    "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception",
    "authors": [
      "Tim Broedermannn",
      "Christos Sakaridis",
      "Luigi Piccinelli",
      "Wim Abbeloos",
      "Luc Van Gool"
    ],
    "abstract": "Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models are available at https://github.com/timbroed/DGFusion",
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.09828.pdf",
    "abs_url": "https://arxiv.org/abs/2509.09828",
    "published": "2025-09-11T20:03:00Z",
    "updated": "2026-01-26T18:33:05Z",
    "comment": "Code and models are available at https://github.com/timbroed/DGFusion",
    "light_analysis": {
      "overview": "论文提出DGFusion方法，通过深度引导的传感器融合提升自动驾驶语义感知的鲁棒性。",
      "motivation": "自动驾驶系统依赖多传感器融合实现鲁棒的语义感知，但现有方法通常在空间上统一处理传感器数据，忽视了传感器可靠性的空间变化，这导致在挑战性条件下（如恶劣天气）性能受限。深度信息对传感器可靠性至关重要，但传统融合方法未能有效整合，因此需要一种能动态适应空间变化的融合机制，以提升整体感知准确性和稳定性。",
      "method": "DGFusion将多模态分割作为多任务问题，利用LiDAR测量作为模型输入和深度学习的真实值。核心创新包括辅助深度头学习深度感知特征，将其编码为空间变化的局部深度标记，结合全局条件标记，通过注意力交叉模态融合动态调整传感器融合。此外，提出鲁棒的损失函数处理LiDAR数据的稀疏性和噪声，模型在MUSES和DeLiVER数据集上进行训练和验证。",
      "result": "在MUSES和DeLiVER数据集上，DGFusion实现了最先进的全景和语义分割性能，摘要未明确说明具体数值，但表明其方法超越了现有基线，证明了深度引导融合在提升感知准确性和鲁棒性方面的有效性，尤其是在传感器可靠性变化大的场景中。",
      "conclusion": "论文的主要贡献是提出深度引导的传感器融合方法DGFusion，通过整合深度信息改进条件感知融合，增强了自动驾驶系统在挑战性条件下的语义感知能力，具有学术和实际应用价值。未来工作可能包括扩展到更多传感器类型或处理更复杂场景，摘要未明确说明局限性。",
      "tags": [
        "Depth-Guided Fusion",
        "Multimodal Segmentation",
        "Attention Mechanism",
        "LiDAR-based Learning",
        "Panoptic Segmentation"
      ]
    },
    "analyzed_at": "2026-01-27T03:26:46.830514Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.06822",
    "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
    "authors": [
      "Chenyang Zhu",
      "Spencer Hong",
      "Jingyu Wu",
      "Kushal Chawla",
      "Charlotte Tang",
      "Youbing Yin",
      "Nathan Wolfe",
      "Erin Babinsky",
      "Daben Liu"
    ],
    "abstract": "The advent of complex, interconnected long-horizon LLM systems has made it incredibly tricky to identify where and when these systems break down. Evaluation capabilities that currently exist today are limited in that they often focus on simple metrics, end-to-end outcomes, and are dependent on the perspectives of humans. In order to match the increasing complexity of these many component systems, evaluation frameworks must also be able to reason, probe, iterate, and understand the nuanced logic passing through these systems. In this paper, we present RAFFLES, an offline evaluation architecture that incorporates iterative reasoning. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically identify faults and a set of specialized Evaluators to assess the quality of the candidate faults as well as rationales of the Judge. We evaluated RAFFLES with several benchmarks - the Who&When dataset to identify step-level faults in multi-agent systems and the ReasonEval datasets to diagnose step-level mathematical reasoning errors. RAFFLES outperforms strong baselines, achieving an accuracy of over 20% and 50% on the Who&When Hand-Crafted and Algorithmically-Generated datasets, and over 80% on the ReasonEval datasets. These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual review.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2509.06822.pdf",
    "abs_url": "https://arxiv.org/abs/2509.06822",
    "published": "2025-09-08T15:57:14Z",
    "updated": "2026-01-26T16:25:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出RAFFLES，一个基于推理的LLM系统故障识别评估架构，通过迭代推理机制提升故障检测能力。",
      "motivation": "随着复杂、互联的长期视野LLM系统广泛应用，识别故障点和时间变得极其困难。现有评估方法通常局限于简单性能指标和端到端结果，依赖人类视角，难以处理系统内部逻辑的细微变化和迭代过程。因此，需要能够推理、探测和迭代的评估框架来匹配多组件系统的复杂性，以提升故障识别的准确性和效率。",
      "method": "RAFFLES采用离线评估架构，结合迭代推理过程。它包括一个中心Judge组件，用于系统识别故障，以及一组专业Evaluators，负责评估候选故障的质量和Judge的推理逻辑。作为多组件管道，它迭代地处理和验证故障点，应用于多智能体系统和数学推理场景，使用Who&When和ReasonEval等数据集进行测试，以优化故障检测流程。",
      "result": "实验结果显示，RAFFLES在多个基准测试中显著优于基线方法。在Who&When Hand-Crafted数据集上准确率超过20%，Algorithmically-Generated数据集上超过50%，在ReasonEval数据集上准确率超过80%。这些数据表明，RAFFLES在步骤级故障识别方面表现优异，为自动化评估提供了可靠的技术支撑。",
      "conclusion": "本研究的核心贡献是提出RAFFLES评估框架，通过推理和迭代机制提升了LLM系统故障识别的准确性。这标志着向自动化故障检测迈出了关键一步，减少了劳动密集型手动审查的需求，具有重要学术价值和实际应用潜力，未来可扩展到更广泛的自主系统评估和改进。",
      "tags": [
        "Fault Detection",
        "Reasoning-based Evaluation",
        "LLM Systems",
        "Multi-Agent Systems",
        "Automated Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:05.712378Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.01387",
    "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links",
    "authors": [
      "Serwar Basch",
      "Ilia Kuznetsov",
      "Tom Hope",
      "Iryna Gurevych"
    ],
    "abstract": "Understanding fine-grained links between documents is crucial for many applications, yet progress is limited by the lack of efficient methods for data curation. To address this limitation, we introduce a domain-agnostic framework for bootstrapping sentence-level cross-document links from scratch. Our approach (1) generates and validates semi-synthetic datasets of linked documents, (2) uses these datasets to benchmark and shortlist the best-performing linking approaches, and (3) applies the shortlisted methods in large-scale human-in-the-loop annotation of natural text pairs. We apply the framework in two distinct domains -- peer review and news -- and show that combining retrieval models with LLMs achieves a 73% human approval rate for suggested links, more than doubling the acceptance of strong retrievers alone. Our framework allows users to produce novel datasets that enable systematic study of cross-document understanding, supporting downstream tasks such as media framing analysis and peer review assessment. All code, data, and annotation protocols are released to facilitate future research.",
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.01387.pdf",
    "abs_url": "https://arxiv.org/abs/2509.01387",
    "published": "2025-09-01T11:32:24Z",
    "updated": "2026-01-26T11:09:33Z",
    "comment": "Accepted at EACL 2026",
    "light_analysis": {
      "overview": "该论文提出了ABCD-LINK框架，通过结合检索模型和大语言模型，从零开始高效引导跨文档细粒度链接的注释，以解决数据整理效率低下的问题。",
      "motivation": "研究动机源于跨文档细粒度链接在媒体框架分析和同行评审等应用中至关重要，但缺乏高效的数据整理方法限制了进展。现有方法通常依赖手动注释，成本高、不可扩展，且难以从零开始构建高质量数据集，导致模型训练和评估受限。因此，需要一个领域无关的框架来自动化链接生成和验证，以提升数据整理效率，支持更广泛的应用场景。",
      "method": "研究方法包括三个核心步骤：首先，生成并验证半合成链接文档数据集，作为训练和评估基础；其次，使用这些数据集基准测试多种链接方法，筛选最佳性能方法；最后，将筛选方法应用于自然文本对的大规模人机协作注释。创新点在于框架的领域无关性，允许从零开始引导，并结合检索模型与大语言模型优化链接建议。技术细节涉及在同行评审和新闻领域应用，使用半合成数据集进行模型评估。",
      "result": "主要实验结果显示，在同行评审和新闻领域，结合检索模型和LLMs的方法实现了73%的人类批准率，表明链接建议准确性高。与单独使用强大检索器相比，批准率提升超过两倍，突显了LLMs在验证和优化链接中的优势。此外，框架在两个领域均有效生成高质量数据集，支持后续系统性研究，量化指标如73%批准率证明了方法的优越性。",
      "conclusion": "论文的主要贡献是ABCD-LINK框架，支持从零开始引导跨文档细粒度链接注释，促进系统性研究和下游任务如媒体框架分析。学术价值在于提供领域无关方法，整合先进技术，并通过公开代码、数据和协议推动未来研究。实际应用价值在于提高数据整理效率，降低人工成本，适用于多领域。局限性或未来工作方向：摘要未明确说明，但可推断可能需要进一步优化模型性能或扩展到更多领域。",
      "tags": [
        "Cross-Document Fine-Grained Linking",
        "Retrieval Models",
        "Large Language Models",
        "Human-in-the-Loop Annotation",
        "Annotation Bootstrapping"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:46.197209Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.19065",
    "title": "Tackling Federated Unlearning as a Parameter Estimation Problem",
    "authors": [
      "Antonio Balordi",
      "Lorenzo Manini",
      "Fabio Stella",
      "Alessio Merlo"
    ],
    "abstract": "Privacy regulations require the erasure of data from deep learning models. This is a significant challenge that is amplified in Federated Learning, where data remains on clients, making full retraining or coordinated updates often infeasible. This work introduces an efficient Federated Unlearning framework based on information theory, modeling leakage as a parameter estimation problem. Our method uses second-order Hessian information to identify and selectively reset only the parameters most sensitive to the data being forgotten, followed by minimal federated retraining. This model-agnostic approach supports categorical and client unlearning without requiring server access to raw client data after initial information aggregation. Evaluations on benchmark datasets demonstrate strong privacy (MIA success near random, categorical knowledge erased) and high performance (Normalized Accuracy against re-trained benchmarks of $\\approx$ 0.9), while aiming for increased efficiency over complete retraining. Furthermore, in a targeted backdoor attack scenario, our framework effectively neutralizes the malicious trigger, restoring model integrity. This offers a practical solution for data forgetting in FL.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.19065.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19065",
    "published": "2025-08-26T14:24:45Z",
    "updated": "2026-01-26T11:53:10Z",
    "comment": "Substantial revisions are in progress; this version is being withdrawn in favor of a significantly updated manuscript",
    "light_analysis": {
      "overview": "本论文提出一种基于信息论的联邦遗忘框架，将数据遗忘建模为参数估计问题，实现高效选择性遗忘。",
      "motivation": "随着隐私法规要求从深度学习模型中删除数据，联邦学习面临独特挑战：数据分散在客户端，完全再训练或协调更新往往不可行，导致隐私合规成本高昂。现有方法可能效率低下或难以实施，本研究旨在解决这一实际问题，通过开发一种无需访问原始数据的高效遗忘机制，确保在分布式环境中满足隐私需求。",
      "method": "论文提出一个模型无关的联邦遗忘框架，基于信息论将数据泄漏建模为参数估计问题。核心方法是利用二阶Hessian信息识别对遗忘数据最敏感的模型参数，仅选择性重置这些参数，随后进行最小化联邦再训练。该方法支持类别和客户端级别的遗忘，无需服务器在初始信息聚合后访问原始客户端数据，提高了灵活性和隐私保护。",
      "result": "在基准数据集上的评估显示，该框架实现了强隐私保护，成员推理攻击成功率接近随机水平，类别知识被有效擦除。性能方面，归一化准确率达到约0.9，与完全再训练基准相当。此外，在针对性后门攻击场景中，框架能有效中和恶意触发，恢复模型完整性，显著提升了效率，避免了完全再训练的高成本。",
      "conclusion": "本研究的主要贡献是提供了一种实用的联邦遗忘解决方案，通过参数估计和选择性重置，在保护隐私的同时保持模型性能。其学术价值在于融合信息论和优化技术解决数据遗忘问题，实际应用价值在于帮助组织遵守隐私法规。未来工作可能涉及扩展到更复杂的场景或进一步优化效率，摘要未明确说明。",
      "tags": [
        "Federated Learning",
        "Federated Unlearning",
        "Hessian Information",
        "Parameter Estimation",
        "Backdoor Attack"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:14.538643Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.19008",
    "title": "Computational Phenomenology of Borderline Personality Disorder: A Comparative Evaluation of LLM-Simulated Expert Personas and Human Clinical Experts",
    "authors": [
      "Marcin Moskalewicz",
      "Anna Sterna",
      "Karolina Drożdż",
      "Kacper Dudzic",
      "Marek Pokropski",
      "Paula Flores"
    ],
    "abstract": "Building on a human-led thematic analysis of life-story interviews with inpatients with Borderline Personality Disorder, this study examines the capacity of large language models (OpenAI's GPT, Google's Gemini, and Anthropic's Claude) to support qualitative clinical analysis. The models were evaluated through a mixed procedure. Study A involved blinded and non-blinded expert judges in phenomenology and clinical psychology. Assessments included semantic congruence, Jaccard coefficients for overlap of outputs, multidimensional validity ratings of credibility, coherence, and the substantiveness of results, and their grounding in qualitative data. In Study B, neural methods were used to embed the theme descriptions created by humans and the models in a two-dimensional vector space to provide a computational measure of the difference between human and model semantics and linguistic style. In Study C, complementary non-expert evaluations were conducted to examine the influence of thematic verbosity on the perception of human authorship and content validity. Results of all three studies revealed variable overlap with the human analysis, with models being partly indistinguishable from, and also identifying themes originally omitted by, human researchers. The findings highlight both the variability and potential of AI-augmented thematic qualitative analysis to mitigate human interpretative bias and enhance sensitivity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.19008.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19008",
    "published": "2025-08-26T13:13:47Z",
    "updated": "2026-01-26T15:37:22Z",
    "comment": "20 pages, 7 tables, 1 figure",
    "light_analysis": {
      "overview": "本研究通过比较大型语言模型模拟专家与人类临床专家，评估其在边缘型人格障碍定性分析中的能力，发现模型能部分替代人类并识别新主题。",
      "motivation": "边缘型人格障碍的临床定性分析依赖于人类专家，但可能存在解释偏见和主题遗漏问题。现有方法以人类主导，效率有限且易受主观影响，导致分析不完整或偏差。本研究旨在探索大型语言模型是否能够支持定性分析，通过AI增强技术减轻人类偏见并提高分析的敏感性和完整性，为临床心理学提供新的辅助工具，以优化诊断和治疗过程。",
      "method": "论文采用混合方法评估大型语言模型（包括GPT、Gemini和Claude）在临床定性分析中的表现。基于对边缘型人格障碍患者生活故事访谈的人类主题分析，研究A通过盲审和非盲审专家评估模型的语义一致性、输出重叠Jaccard系数和多维有效性（如可信度、连贯性）。研究B使用神经方法将人类和模型生成的主题描述嵌入二维向量空间，计算语义和语言风格差异。研究C进行非专家评估，分析主题冗长对人为作者感知和内容有效性的影响。关键创新在于结合专家评审、计算度量和非专家评估，全面比较模型与人类表现。",
      "result": "研究结果显示，大型语言模型的输出与人类分析存在可变重叠，部分情况下难以区分。模型能识别人类研究者最初忽略的主题，补充了分析内容。通过Jaccard系数和多维有效性评级，模型在语义连贯性和可信度方面表现良好，但结果存在变异性。与人类基线相比，模型展现出潜力，但摘要未明确说明具体性能指标，仅指出模型在某些方面与人类相似，并能发现新主题，强调了AI增强方法的优势和局限性。",
      "conclusion": "本研究证实了大型语言模型在临床定性分析中的潜力和可变性，主要贡献在于评估AI如何减轻人类解释偏见并增强敏感性。学术价值为AI在医疗应用提供实证支持，推动人机协作研究；实际应用可辅助临床决策，提高分析效率。局限性包括模型输出的不一致性，未来工作可探索模型优化、更多数据集验证及跨领域应用，以进一步提升可靠性和实用性。",
      "tags": [
        "Large Language Models",
        "Qualitative Analysis",
        "Clinical Psychology",
        "Neural Embedding",
        "Mixed Methods"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:41.265023Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.19005",
    "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark",
    "authors": [
      "Yuxuan Cai",
      "Yipeng Hao",
      "Jie Zhou",
      "Hang Yan",
      "Zhikai Lei",
      "Rui Zhen",
      "Zhenhua Han",
      "Yutao Yang",
      "Junsong Li",
      "Qianjun Pan",
      "Tianyu Huai",
      "Qin Chen",
      "Xin Li",
      "Kai Chen",
      "Bo Zhang",
      "Xipeng Qiu",
      "Liang He"
    ],
    "abstract": "As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as \"second nature\".   We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.19005.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19005",
    "published": "2025-08-26T13:04:28Z",
    "updated": "2026-01-26T06:00:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了经验驱动的终身学习（ELL）框架和StuLife基准数据集，用于构建能通过持续交互自我演化的智能体。",
      "motivation": "随着AI向通用智能发展，焦点从静态任务优化转向创建开放式的持续学习智能体。现有系统通常限于特定任务，难以在动态、开放环境中长期适应，限制了智能体的自主性和现实世界应用。因此，需要一种方法使智能体能像人类一样通过经验持续学习和成长，以解决真实世界问题的复杂性。",
      "method": "论文引入了经验驱动的终身学习（ELL）框架，基于四个核心原则：经验探索（智能体通过自我激励的交互获取经验）、长期记忆（结构化存储历史知识和推理）、技能学习（从经验抽象可重用技能）和知识内化（将显性经验转化为隐性能力）。此外，创建了StuLife基准数据集，模拟学生大学旅程，涵盖三个阶段和十个子场景，以评估智能体的持续学习和适应性。",
      "result": "摘要未明确说明具体的实验结果或性能指标。论文主要介绍了ELL框架和StuLife基准的设计，但未提及与基线方法的对比数据或准确率提升。可能暗示框架和基准为未来实验提供基础，实际评估需进一步研究。",
      "conclusion": "ELL框架为构建自我演化智能体提供了系统性方法，结合经验学习和知识管理，促进智能体的持续成长和适应能力。StuLife基准为研究提供了可评估场景，推动终身学习和开放世界智能的发展。潜在局限性包括框架的通用性和基准的复杂度，未来工作可扩展更多应用领域并验证实际效果。",
      "tags": [
        "Experience-Driven Lifelong Learning",
        "Self-Evolving Agents",
        "Benchmark Dataset",
        "Skill Abstraction",
        "Long-term Memory"
      ]
    },
    "analyzed_at": "2026-01-27T03:27:30.770556Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.18989",
    "title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone",
    "authors": [
      "Shaivi Malik",
      "Hasnat Md Abdullah",
      "Sriparna Saha",
      "Amit Sheth"
    ],
    "abstract": "As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.18989.pdf",
    "abs_url": "https://arxiv.org/abs/2508.18989",
    "published": "2025-08-26T12:41:35Z",
    "updated": "2026-01-26T12:18:24Z",
    "comment": "Accepted to the Findings of EACL 2026",
    "light_analysis": {
      "overview": "论文提出GRAS基准及其偏差分数，用于全面测量和量化视觉语言模型在性别、种族、年龄和肤色方面的偏差。",
      "motivation": "随着视觉语言模型（VLMs）在现实世界应用中变得不可或缺，理解其人口统计偏差对确保公平性和可靠性至关重要。现有评估方法可能覆盖不足，特别是在多个维度如性别、种族、年龄和肤色上缺乏综合基准。此外，使用视觉问答（VQA）评估偏差时，单一问题表述可能无法准确反映模型的真实偏差水平，导致评估不全面。因此，需要一个新的、可解释的方法来系统化地量化VLMs的偏差，以弥补现有方法的不足。",
      "method": "本研究引入了GRAS基准，专门设计用于揭示视觉语言模型在性别、种族、年龄和肤色方面的偏差。核心方法是提出GRAS偏差分数，这是一个可解释的量化指标，用于标准化地测量偏差程度。关键创新点包括强调在评估过程中使用视觉问答（VQA）时需要考虑多种问题表述，以提高评估的准确性和全面性。此外，研究公开了代码和数据，确保了方法的可重复性和透明性，没有指定具体模型架构，但基于现有信息推断依赖于多样化的数据集来支持评估。",
      "result": "对五个最先进的视觉语言模型进行基准测试，使用GRAS偏差分数进行评估。结果显示，所有模型均表现出显著的偏差，其中偏差最小的模型仅获得2分（满分100分），分数越低表示偏差越小，表明当前VLMs在人口统计维度上存在严重问题，偏差水平令人担忧。结果与现有最先进模型相比，突出了新基准在揭示偏差方面的有效性，提供了具体数据支撑，如GRAS偏差分数的极端低值，强调了模型公平性的不足。",
      "conclusion": "本文的主要贡献是提出了GRAS基准和GRAS偏差分数，为全面评估视觉语言模型的偏差提供了新工具，并强调了使用多种问题表述的方法论洞察。学术上，这促进了AI公平性领域的标准化研究和深入理解；实际应用中，可帮助开发者改进模型公平性，应用于现实场景以提升系统可靠性。潜在局限性包括摘要未明确说明的未来工作方向，但可推断未来可能扩展基准到更多维度或开发更精细的评估方法，以进一步提升评估效果。",
      "tags": [
        "Vision Language Models",
        "Demographic Bias",
        "Benchmarking",
        "GRAS Bias Score",
        "Visual Question Answering"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:14.275640Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.17218",
    "title": "Generalized Policy Gradient with History-Aware Decision Transformer for Path Planning",
    "authors": [
      "Xing Wei",
      "Duoxiang Zhao",
      "Zezhou Zhang",
      "Yuqi Ouyang",
      "Hao Qin"
    ],
    "abstract": "With the rapidly increased number of vehicles in urban areas, existing road infrastructure struggles to accommodate modern traffic demands, resulting in congestion. This highlights the importance of efficient path planning strategies. Most recent navigation models focus on deterministic or time-dependent networks, overlooking correlations and the stochastic nature of traffic flows. In this work, we address the reliable shortest path problem in stochastic transportation networks and propose a path planning solution integrating the decision Transformer with the Generalized Policy Gradient (GPG) framework. Leveraging the Transformer's ability to model long-term dependencies, our solution improves path decision accuracy and stability. Experiments on the Sioux Falls (SFN) and large Anaheim (AN) networks show consistent improvement in on-time arrival probabilities by capturing non-Markovian dependencies in historical routing decisions on real-world topologies.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.17218.pdf",
    "abs_url": "https://arxiv.org/abs/2508.17218",
    "published": "2025-08-24T05:41:11Z",
    "updated": "2026-01-26T07:54:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合决策Transformer和广义策略梯度（GPG）的路径规划方法，显著提高了随机交通网络中路径决策的准确性和准时到达概率。",
      "motivation": "随着城市车辆数量急剧增加，现有道路基础设施难以满足交通需求，导致拥堵问题突出，突显了高效路径规划的重要性。当前多数导航模型侧重于确定性或时间依赖网络，忽视了交通流的随机性和相互关系，这使得在随机交通网络中寻找可靠最短路径变得困难。本文旨在解决这一问题，通过创新方法改进现有模型，以更好地处理实际交通中的不确定性，提升路径规划的可靠性。",
      "method": "本论文提出了一种创新的路径规划解决方案，将决策Transformer与广义策略梯度（GPG）框架集成。Transformer用于建模历史路由决策中的长期依赖关系，捕捉非马尔可夫依赖，以提高路径决策的准确性和稳定性；广义策略梯度则用于优化策略，在随机交通网络中实现可靠最短路径规划。方法基于现实世界拓扑，如Sioux Falls和Anaheim网络，通过训练适应交通流的随机性，关键创新点在于结合深度学习与强化学习技术来增强决策能力。",
      "result": "实验在Sioux Falls（SFN）和大型Anaheim（AN）网络上进行，结果表明所提出的方法在准时到达概率方面实现了持续改进。通过捕捉历史路由决策中的非马尔可夫依赖，方法在真实世界拓扑上显示出优于现有导航模型的性能。摘要未提供具体的准确率提升数据，但实验验证了该方法的有效性和稳定性，为随机交通网络中的路径规划提供了新的基准，证明了其在实际应用中的潜力。",
      "conclusion": "本研究通过集成决策Transformer和广义策略梯度框架，成功解决了随机交通网络中的可靠最短路径问题，显著提高了路径决策的准确性和准时到达概率。这一贡献具有重要的学术价值，推动了智能交通系统和强化学习在路径规划领域的应用。在实际应用中，该方法有助于优化导航策略，缓解城市交通拥堵，提升出行效率。未来工作可能包括扩展到更复杂的网络结构或集成实时交通数据，以进一步提升性能；摘要未明确说明具体局限性。",
      "tags": [
        "Path Planning",
        "Decision Transformer",
        "Generalized Policy Gradient",
        "Stochastic Networks",
        "Non-Markovian Dependencies"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:04.965129Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.16984",
    "title": "HiCache: A Plug-in Scaled-Hermite Upgrade for Taylor-Style Cache-then-Forecast Diffusion Acceleration",
    "authors": [
      "Liang Feng",
      "Shikang Zheng",
      "Jiacheng Liu",
      "Yuqi Lin",
      "Qinming Zhou",
      "Peiliang Cai",
      "Xinyu Wang",
      "Junjie Chen",
      "Chang Zou",
      "Yue Ma",
      "Linfeng Zhang"
    ],
    "abstract": "Diffusion models have achieved remarkable success in content generation but often incur prohibitive computational costs due to iterative sampling. Recent feature caching methods accelerate inference via temporal extrapolation, yet can suffer quality degradation from inaccurate modeling of the complex dynamics of feature evolution. We propose HiCache (Hermite Polynomial-based Feature Cache), a training-free acceleration framework that improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature-derivative approximations in diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials as a potentially optimal basis for Gaussian-correlated processes. We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy, and is also effective when applied standalone or integrated with TaylorSeer. Extensive experiments demonstrate HiCache's superiority, achieving 5.55x speedup on FLUX.1-dev while matching or exceeding baseline quality, and maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Moreover, HiCache can be naturally added to previous caching methods to enhance their performance, e.g., improving ClusCa from 0.9480 to 0.9840 in terms of image rewards. Code: https://github.com/fenglang918/HiCache",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.16984.pdf",
    "abs_url": "https://arxiv.org/abs/2508.16984",
    "published": "2025-08-23T10:35:16Z",
    "updated": "2026-01-26T18:39:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了 HiCache，一种基于 Hermite 多项式的无需训练加速框架，有效提升扩散模型推理效率并保持生成质量。",
      "motivation": "扩散模型在内容生成中表现出色，但迭代采样导致高昂计算成本，限制了实时应用。现有的特征缓存方法通过时间外推加速推理，但常因对特征演化复杂动态建模不准确而引发质量下降。因此，需要一种更准确的方法来平衡效率与生成质量，以解决现有方法的建模误差和性能瓶颈问题，推动高效内容生成的实用性。",
      "method": "HiCache 是一个无需训练的加速框架，核心是利用 Hermite 多项式作为基础来改进特征预测，这基于扩散 Transformer 中特征导数近似呈现多元高斯特性的关键见解。它引入双缩放机制，确保数值稳定性并维持预测准确性，可以单独使用或与现有方法如 TaylorSeer 集成。该方法的关键创新在于将数学工具与经验特性对齐，优化了高斯相关过程的特征预测，从而提升缓存效果，适用于文本到图像等任务。",
      "result": "实验表明，HiCache 在 FLUX.1-dev 上实现了 5.55 倍加速，同时生成质量匹配或超过基线方法。在文本到图像、视频生成和超分辨率等多种任务中保持强劲性能，并能集成到先前缓存方法中，显著提升性能，例如将 ClusCa 的图像奖励从 0.9480 提高到 0.9840，展示了其广泛适用性和有效性。",
      "conclusion": "HiCache 的主要贡献在于提出了一种基于 Hermite 多项式的训练免费加速框架，显著提升了扩散模型的推理效率并保持了高质量输出。其学术价值在于改进了特征预测的理论基础，实际应用价值在于降低计算成本，加速内容生成任务。未来工作可能涉及扩展到更多模型类型或优化机制，尽管摘要未明确说明局限性，但 HiCache 的灵活性暗示了良好的发展潜力。",
      "tags": [
        "Hermite Polynomials",
        "Diffusion Models",
        "Feature Caching",
        "Inference Acceleration",
        "Gaussian Correlated Processes"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:02.670703Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.11669",
    "title": "Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset",
    "authors": [
      "Wentao Li",
      "Yonghu He",
      "Zirong Yu",
      "Kun Gao",
      "Qing Liu",
      "Yali Zheng"
    ],
    "abstract": "Noninvasive arterial blood pressure (ABP) monitoring is essential for patient management in critical care and perioperative settings, providing continuous assessment of cardiovascular hemodynamics with minimal risks. Numerous deep learning models have developed to reconstruct ABP waveform from noninvasively acquired physiological signals such as electrocardiogram and photoplethysmogram. However, limited research has addressed the issue of model performance and computational load for deployment on embedded systems. The study introduces a lightweight sInvResUNet, along with a collaborative learning scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a computational load of 0.02 GFLOPS, real-time ABP estimation was successfully achieved on embedded devices with an inference time of just 8.49 milliseconds for a 10-second output. We performed subject-independent validation in a large-scale and heterogeneous perioperative dataset containing 1,257,141 data segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and 31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better performance compared to large models, with a mean absolute error of 10.06 mmHg and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these promising results, all deep learning models showed significant performance variations across different demographic and cardiovascular conditions, highlighting their limited ability to generalize across such a broad and diverse population. This study lays a foundation work for real-time, unobtrusive ABP monitoring in real-world perioperative settings, providing baseline for future advancements in this area.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.11669.pdf",
    "abs_url": "https://arxiv.org/abs/2508.11669",
    "published": "2025-08-07T02:40:17Z",
    "updated": "2026-01-26T06:17:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出轻量级 sInvResUNet 模型及协作学习方案 KDCL_sInvResUNet，用于在嵌入式设备上实现实时无创动脉血压波形预测，性能优于大型模型。",
      "motivation": "无创动脉血压（ABP）监测在重症监护和围手术期至关重要，能连续评估心血管血流动力学且风险最小。尽管已有深度学习方法从心电图和光电容积脉搏波等信号重构血压波形，但现有研究较少关注模型在嵌入式系统上的部署性能与计算负载，这限制了实时应用。当前方法往往计算量大，难以在资源受限设备上高效运行，因此开发轻量级、低计算成本的模型具有重要实际意义。",
      "method": "论文引入轻量级 sInvResUNet 模型和协作学习方案 KDCL_sInvResUNet，模型仅含 0.89 百万参数和 0.02 GFLOPS 计算负载。关键创新在于结合协作学习增强轻量化设计，从非侵入性生理信号中重构动脉血压波形。使用大规模围手术期数据集进行验证，包含 1,257,141 数据段和 2,154 名患者，覆盖宽血压范围，支持实时推断，推理时间仅 8.49 毫秒用于 10 秒输出。",
      "result": "在大型、异质性数据集上，KDCL_sInvResUNet 实现平均绝对误差 10.06 mmHg 和平均皮尔逊相关性 0.88，性能略优于大型模型，推理时间短至 8.49 毫秒，适合嵌入式设备实时应用。但所有深度学习模型在不同人口统计和心血管条件下性能有显著变化，表明泛化能力有限，基线对比显示轻量级方法在效率上优势明显。",
      "conclusion": "该研究为现实世界围手术期设置中的实时无创血压监测奠定了基础，贡献在于轻量化和协作学习的技术方案，学术上促进高效模型发展，应用上支持嵌入式部署。然而，模型泛化能力受限，未来工作需改进跨多样人群的适应性，为领域进一步进展提供基准。",
      "tags": [
        "Lightweight Models",
        "Collaborative Learning",
        "sInvResUNet",
        "Real-time Inference",
        "Medical Deep Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:19.068783Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.15431",
    "title": "Inexact calculus of variations on the hyperspherical tangent bundle and its connections to the attention mechanism",
    "authors": [
      "Andrew Gracyk"
    ],
    "abstract": "We offer a theoretical mathematical background through Lagrangian optimization on the unit hyperspherical manifold and its tangential collection with application to the Transformer and its token space. Our methods are catered to the attention mechanism in a theoretical setting, but largely appeal to a broader mathematical lens as well. The Transformer, as a flow map, exists in the tangent fiber for each token along the high-dimensional unit sphere. The circumstance of the hypersphere across the latent data is reasonable due to the trained diagonal matrix equal to the identity, which has various empirical justifications. Thus, under the continuum limit of the dynamics, the latent vectors flow among the tangent bundle. Using these facts, we devise a mathematical framework focusing on the attention mechanism through calculus of variations. We develop a functional and show that the continuous flow map induced by the Transformer satisfies this functional, therefore attention can be viewed as a natural solver of a calculus of variations problem. We invent new scenarios of when our methods are applicable based on loss optimization with respect to path optimality. We derive the projected Euler-Lagrange equation under the specific flow map. The variant of the Euler-Lagrange equation we present has various appearances in literature, but, to our understanding, oftentimes not foundationally proven or under other specialized cases. Our overarching proof is new: our techniques are classical and the use of the flow map object is original. We provide several other relevant results, primarily ones specific to neural scenarios. In particular, much of our analysis will be attempting to quantify Transformer data in variational contexts under neural approximations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.15431.pdf",
    "abs_url": "https://arxiv.org/abs/2507.15431",
    "published": "2025-07-21T09:43:33Z",
    "updated": "2026-01-26T04:15:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一个数学框架，将Transformer的注意力机制与超球面切丛上的变分法连接，证明注意力是变分问题的自然求解器。",
      "motivation": "该研究旨在解决注意力机制在Transformer中缺乏坚实数学理论基础的问题。现有文献中，相关变分方程的基础证明不足，限制了对其数学本质的理解和进一步优化。问题的重要性在于为神经网络的注意力机制提供理论支持，弥补现有方法的不足，如方程未经过基础证明或仅在专门情况下讨论。通过建立理论框架，论文试图增强对注意力机制数学行为的洞察，并为后续研究提供更稳固的基础。",
      "method": "研究方法基于在单位超球面流形及其切丛上的拉格朗日优化和变分法。核心是开发一个泛函，并证明Transformer诱导的连续流映射满足该泛函，从而将注意力机制视为变分问题的求解器。关键创新点包括使用流映射对象分析注意力，推导投影欧拉-拉格朗日方程。技术路线涉及经典数学技术，如变分法，应用于Transformer的token空间，其中潜在向量在超球面切丛中流动。",
      "result": "主要理论结果包括推导了投影欧拉-拉格朗日方程，并证明了Transformer的注意力机制可以作为变分问题的自然求解器。论文展示了连续流映射满足开发的泛函，为注意力提供了数学基础。此外，提供了针对神经场景的相关结果，试图在变分上下文中量化Transformer数据。摘要未明确说明具体实验性能指标或对比基线方法，但强调了理论证明的新颖性和原创性。",
      "conclusion": "论文的主要贡献是建立了一个将注意力机制与超球面切丛上变分法联系的数学框架，提供了原创证明，技术经典但应用新颖。学术价值在于增强了注意力机制的理论基础，为神经网络的变分分析提供了新视角，可能弥补文献中的证明缺口。实际应用价值包括可能优化Transformer架构和理解神经网络动态。未来工作方向可能包括在神经近似中进一步量化数据，或扩展到其他模型。摘要未明确说明局限性。",
      "tags": [
        "Lagrangian Optimization",
        "Calculus of Variations",
        "Attention Mechanism",
        "Transformer",
        "Tangent Bundle"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:24.786247Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.12070",
    "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions",
    "authors": [
      "George Bird"
    ],
    "abstract": "Presented is a novel methodology for determining representational structure, which builds upon the existing Spotlight Resonance method. This new tool is used to gain insight into how discrete representations can emerge and organise in autoencoder models, through a controlled ablation study that alters only the activation function. Using this technique, the validity of whether function-driven symmetries can act as implicit inductive biases on representations is determined. Representations are found to tend to discretise when the activation functions are defined through a discrete algebraic permutation-equivariant symmetry. In contrast, they remain continuous under a continuous algebraic orthogonal-equivariant definition. This confirms the hypothesis that the symmetries of network primitives can carry unintended inductive biases, leading to task-independent artefactual structures in representations. The discrete symmetry of contemporary forms is shown to be a strong predictor for the production of symmetry-organised discrete representations emerging from otherwise continuous distributions -- a quantisation effect. This motivates further reassessment of functional forms in common usage due to such unintended consequences. Moreover, this supports a general causal model for a mode in which discrete representations may form, and could constitute a prerequisite for downstream interpretability phenomena, including grandmother neurons, discrete coding schemes, general linear features and a type of Superposition. Hence, this tool and proposed mechanism for the influence of functional form on representations may provide insights into interpretability research. Finally, preliminary results indicate that quantisation of representations correlates with a measurable increase in reconstruction error, reinforcing previous conjectures that this collapse can be detrimental.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.12070.pdf",
    "abs_url": "https://arxiv.org/abs/2507.12070",
    "published": "2025-07-16T09:27:54Z",
    "updated": "2026-01-26T14:19:44Z",
    "comment": "47 pages, 37 figures",
    "light_analysis": {
      "overview": "论文提出基于Spotlight Resonance的新方法论，揭示激活函数对称性如何导致自编码器表示结构中的量化效应。",
      "motivation": "研究动机在于探索神经网络中离散表示的形成机制，现有方法可能未充分考虑激活函数对称性作为隐式归纳偏置的影响。这导致任务无关的伪结构，阻碍了可解释性研究。通过分析函数驱动的对称性，可以揭示其如何无意中引导表示的组织，从而推动对网络内部机制的理解，并为改善模型解释提供基础。",
      "method": "研究方法使用一种新工具，基于Spotlight Resonance方法，通过控制性消融实验来研究表示结构。核心实验仅改变自编码器中的激活函数，比较离散代数置换等变对称和连续代数正交等变对称的定义。这种方法允许观察激活函数变化如何影响表示的离散化程度，从而验证对称性作为隐式偏置的假设，为量化效应提供技术路线。",
      "result": "主要实验结果表明，当激活函数基于离散代数置换等变对称时，表示倾向于离散化；在连续代数正交等变对称下则保持连续。这确认了对称性作为无意偏置的假设。量化效应与重构误差增加相关，初步数据显示重构误差在量化表示中上升，支持了离散结构可能导致性能下降的先前推测，但与基线对比表明量化增强了表示组织。",
      "conclusion": "研究确认了函数形式对称性对表示的隐式影响，揭示了量化效应的机制，为可解释性研究提供了新视角，并促进常用函数形式的重新评估。潜在局限性包括量化可能增加重构误差，未来工作可聚焦于优化激活函数以减少无意偏置或应用于更广泛模型。",
      "tags": [
        "Autoencoder",
        "Activation Function",
        "Symmetry",
        "Quantised Representations",
        "Interpretability"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:24.110005Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.20941",
    "title": "Revisiting the Past: Data Unlearning with Model State History",
    "authors": [
      "Keivan Rezaei",
      "Mehrdad Saberi",
      "Abhilasha Ravichander",
      "Soheil Feizi"
    ],
    "abstract": "Large language models are trained on massive corpora of web data, which may include private data, copyrighted material, factually inaccurate data, or data that degrades model performance. Eliminating the influence of such problematic datapoints on a model through complete retraining -- by repeatedly pretraining the model on datasets that exclude these specific instances -- is computationally prohibitive. To address this, unlearning algorithms have been proposed, that aim to eliminate the influence of particular datapoints at a low computational cost, while leaving the rest of the model intact. However, precisely unlearning the influence of data on a large language model has proven to be a major challenge. In this work, we propose a new algorithm, MSA (Model State Arithmetic), for unlearning datapoints in large language models. MSA utilizes prior model checkpoints -- artifacts that record model states at different stages of pretraining -- to estimate and counteract the effect of targeted datapoints. Our experimental results show that MSA achieves competitive performance and often outperforms existing machine unlearning algorithms across multiple benchmarks, models, and evaluation metrics, suggesting that MSA could be an effective approach towards more flexible large language models that are capable of data erasure.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.20941.pdf",
    "abs_url": "https://arxiv.org/abs/2506.20941",
    "published": "2025-06-26T02:16:16Z",
    "updated": "2026-01-26T15:38:28Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": {
      "overview": "本文提出MSA（模型状态算术）算法，利用模型检查点历史高效消除大型语言模型中的问题数据影响。",
      "motivation": "大型语言模型训练依赖于海量网络数据，其中可能包含隐私、版权或不准确数据，这些问题数据不仅侵犯法律权益，还可能降低模型性能。完全重新训练来排除这些数据点计算成本极高，而现有遗忘算法难以在大型语言模型中精确消除数据影响。因此，亟需一种低成本、高效的方法来应对数据遗忘挑战，以实现模型的灵活性和合规性。",
      "method": "论文提出MSA（模型状态算术）算法，核心方法是利用预训练过程中保存的模型检查点，这些检查点记录了模型在不同训练阶段的状态。MSA通过分析这些历史状态来估计目标数据点的影响，并通过算术操作抵消这种影响，从而实现数据遗忘。该方法避免了昂贵的重新训练，关键创新在于结合模型状态历史进行动态调整，适用于大型语言模型。",
      "result": "实验结果显示，MSA算法在多个基准测试、不同模型和评估指标上均表现出竞争性性能，并且经常优于现有的机器遗忘算法。这表明MSA在有效消除数据影响方面具有优势，为大型语言模型的数据擦除提供了可行的技术路径，尽管摘要未提供具体数值，但强调了其相对于基线方法的优越性。",
      "conclusion": "本研究的贡献在于提出MSA算法，为大型语言模型的数据遗忘问题提供了新解决方案。该方法利用模型状态历史，以低成本实现数据影响消除，具有较高的学术价值，促进了机器学习模型的可控性和适应性。在实际应用中，MSA有助于构建更灵活、合规的模型，支持数据隐私保护。未来工作可能包括进一步优化算法或扩展到其他模型类型。",
      "tags": [
        "Large Language Model",
        "Machine Unlearning",
        "Model Checkpoint",
        "Model State Arithmetic"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:20.746705Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.19893",
    "title": "Distillation-Enabled Knowledge Alignment for Generative Semantic Communications of AIGC Images",
    "authors": [
      "Jingzhi Hu",
      "Geoffrey Ye Li"
    ],
    "abstract": "Due to the surging amount of AI-generated images, its provisioning to edges and mobile users from the cloud incurs substantial traffic on networks. Generative semantic communication (GSC) offers a promising solution by transmitting highly compact information, i.e., prompt text and latent representations, instead of high-dimensional image data. However, GSC relies on the alignment between the knowledge in the cloud generative AI (GAI) and that possessed by the edges and users, and between the knowledge for wireless transmission and that of actual channels, which remains challenging. In this paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm for GSC systems. The core idea is to distill the image generation knowledge from the cloud-GAI into low-rank matrices, which can be incorporated by the edge and used to adapt the transmission knowledge to diverse wireless channel conditions. DeKA-g comprises two novel methods: metaword-aided knowledge distillation (MAKD) and condition-aware low-rank adaptation (CALA). For MAKD, an optimized metaword is employed to enhance the efficiency of knowledge distillation, while CALA enables efficient adaptation to diverse rate requirements and channel conditions. From simulation results, DeKA-g improves the consistency between the edge-generated images and the cloud-generated ones by 44% and enahnces the average transmission quality in terms of PSNR by 6.5 dB over the baselines without knowledge alignment.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.19893.pdf",
    "abs_url": "https://arxiv.org/abs/2506.19893",
    "published": "2025-06-24T10:50:14Z",
    "updated": "2026-01-26T14:18:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出DeKA-g算法，通过知识蒸馏实现生成式语义通信中云端与边缘的知识对齐，提升图像传输效率和质量。",
      "motivation": "随着AI生成图像数量激增，云端向边缘传输图像导致网络流量剧增。生成式语义通信通过传输紧凑信息（如提示文本和潜在表示）缓解此问题，但面临云端生成AI知识、边缘知识、传输知识与无线通道知识之间的对齐挑战。现有方法缺乏有效对齐机制，导致传输质量和一致性不足，因此研究知识对齐算法对优化通信系统和减少网络负载至关重要。",
      "method": "DeKA-g算法包含元词辅助的知识蒸馏（MAKD）和条件感知的低秩适配（CALA）。MAKD利用优化元词提升知识蒸馏效率，将云端生成AI的图像知识压缩为低秩矩阵；CALA使边缘能根据不同速率要求和通道条件自适应调整传输知识。核心创新在于通过蒸馏实现知识对齐，使边缘生成图像与云端一致，并适应多样无线环境。摘要未明确说明具体数据集或模型架构。",
      "result": "仿真实验显示，DeKA-g显著提升性能。边缘生成图像与云端生成图像的一致性提高44%；在峰值信噪比（PSNR）上，平均传输质量提升6.5 dB。这些改进相比没有知识对齐的基线方法有显著优势，证实了算法在优化生成式语义通信中的有效性。",
      "conclusion": "本研究提出DeKA-g算法，有效解决生成式语义通信中的知识对齐问题，提高图像传输的一致性和质量。学术上，推动了知识蒸馏和语义通信的融合；实际上，有助于减少网络流量，提升用户体验。摘要未明确说明局限性或未来工作方向，但潜在扩展可能包括优化算法或应用于其他数据类型。",
      "tags": [
        "Generative Semantic Communication",
        "Knowledge Distillation",
        "Low-Rank Adaptation",
        "AI-generated Content",
        "Wireless Communication"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:35.680316Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.17977",
    "title": "SliceGX: Layer-wise GNN Explanation with Model-slicing",
    "authors": [
      "Tingting Zhu",
      "Tingyang Chen",
      "Yinghui Wu",
      "Arijit Khan",
      "Xiangyu Ke"
    ],
    "abstract": "Ensuring the trustworthiness of graph neural networks (GNNs), which are often treated as black-box models, requires effective explanation techniques. Existing GNN explanations typically apply input perturbations to identify subgraphs that are responsible for the occurrence of the final output of GNNs. However, such approaches lack finer-grained, layer-wise analysis of how intermediate representations contribute to the final result, capabilities that are crucial for model diagnosis and architecture optimization. This paper introduces SliceGX, a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner. Given a GNN model M, a set of selected intermediate layers, and a target layer, SliceGX slices M into layer blocks(\"model slice\") and discovers high-quality explanatory subgraphs within each block that elucidate how the model output arises at the target layer. Although finding such layer-wise explanations is computationally challenging, we develop efficient algorithms and optimization techniques that incrementally construct and maintain these subgraphs with provable approximation guarantees. Extensive experiments on synthetic and real-world benchmarks demonstrate the effectiveness and efficiency of SliceGX, and illustrate its practical utility in supporting model debugging.",
    "categories": [
      "cs.LG",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.17977.pdf",
    "abs_url": "https://arxiv.org/abs/2506.17977",
    "published": "2025-06-22T10:28:46Z",
    "updated": "2026-01-26T08:59:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "SliceGX提出了一种基于模型切片的层级图神经网络解释方法，通过渐进式分析中间表示，以弥补现有技术的不足。",
      "motivation": "图神经网络通常被视为黑盒模型，现有解释技术主要通过输入扰动识别影响最终输出的子图，但缺乏对中间表示贡献的层级分析。这在模型诊断和架构优化中至关重要，因为更细粒度的解释有助于提升模型透明度和性能，从而解决现有方法在提供深度洞察方面的局限性。",
      "method": "SliceGX将给定的GNN模型M切片为层块（称为“模型切片”），在选定的中间层和目标层上渐进式地生成解释。通过高效算法和优化技术，在每个块中发现高质量的解释性子图，以阐明目标层输出的形成过程。这些算法具有增量构建和可证明的近似保证，确保了解释的精确性和计算效率。",
      "result": "论文在合成和真实世界基准上进行了广泛实验，验证了SliceGX在解释效果和计算效率上的优越性。实验结果显示，相比于基线方法，SliceGX能提供更准确的层级解释，并展示了其在模型调试中的实用价值，例如辅助识别模型错误和优化架构。",
      "conclusion": "SliceGX的主要贡献是实现GNN的层级解释，为模型诊断和架构优化提供了新工具，增强了可解释性并支持实际应用。尽管摘要未明确说明局限性，但未来工作可能涉及扩展到更复杂模型或应用于其他领域，具有重要学术和实际意义。",
      "tags": [
        "Graph Neural Networks",
        "Model Explanation",
        "Layer-wise Analysis",
        "Model-slicing",
        "Approximation Algorithms"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:47.679750Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.15408",
    "title": "Unifying VXAI: A Systematic Review and Framework for the Evaluation of Explainable AI",
    "authors": [
      "David Dembinsky",
      "Adriano Lucieri",
      "Stanislav Frolov",
      "Hiba Najjar",
      "Ko Watanabe",
      "Andreas Dengel"
    ],
    "abstract": "Modern AI systems frequently rely on opaque black-box models, most notably Deep Neural Networks, whose performance stems from complex architectures with millions of learned parameters. While powerful, their complexity poses a major challenge to trustworthiness, particularly due to a lack of transparency. Explainable AI (XAI) addresses this issue by providing human-understandable explanations of model behavior. However, to ensure their usefulness and trustworthiness, such explanations must be rigorously evaluated. Despite the growing number of XAI methods, the field lacks standardized evaluation protocols and consensus on appropriate metrics. To address this gap, we conduct a systematic literature review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and introduce a unified framework for the eValuation of XAI (VXAI). We identify 362 relevant publications and aggregate their contributions into 41 functionally similar metric groups. In addition, we propose a three-dimensional categorization scheme spanning explanation type, evaluation contextuality, and explanation quality desiderata. Our framework provides the most comprehensive and structured overview of VXAI to date. It supports systematic metric selection, promotes comparability across methods, and offers a flexible foundation for future extensions.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.15408.pdf",
    "abs_url": "https://arxiv.org/abs/2506.15408",
    "published": "2025-06-18T12:25:37Z",
    "updated": "2026-01-26T07:18:02Z",
    "comment": "Published at TMLR",
    "light_analysis": {
      "overview": "本论文提出统一的VXAI框架，通过系统综述和三维分类方案标准化可解释AI评估，填补领域空白。",
      "motivation": "现代AI系统如深度神经网络是黑盒模型，复杂性影响可信度，可解释AI提供解释但评估缺乏标准。当前XAI领域方法众多，却无统一评估协议和指标共识，导致解释的实用性和可信度难以保证，阻碍了技术发展和应用部署。现有研究分散，未能系统化解决评估问题，因此迫切需要结构化框架以提升评估效率和质量。",
      "method": "作者遵循PRISMA指南进行系统文献综述，筛选出362篇相关文献，并将其中的评估指标聚合为41个功能相似的组。关键创新是提出三维分类方案，包括解释类型、评估上下文性和解释质量需求，该方法通过结构化整合现有研究，为评估提供系统化技术路线，支持更有效的指标选择和方法比较。",
      "result": "VXAI框架提供了当前最全面和结构化的可解释AI评估概述，基于362篇文献的分析。它支持系统化指标选择，促进不同XAI方法的可比性，摘要未明确说明具体性能提升数据，但强调框架作为灵活基础，为未来扩展和应用奠定了基础，提升了评估的标准化程度。",
      "conclusion": "本研究的主要贡献是提出统一的VXAI框架，通过系统综述和分类方案标准化评估，学术上填补了可解释AI评估标准的缺失，实际应用中支持研究人员选择指标和比较方法。未来工作可扩展框架，进一步细化和应用评估指标，推动领域发展。",
      "tags": [
        "Explainable AI (XAI)",
        "Systematic Review",
        "Evaluation Framework",
        "Classification Scheme",
        "PRISMA Guidelines"
      ]
    },
    "analyzed_at": "2026-01-27T03:28:45.110319Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.14054",
    "title": "Scientifically-Interpretable Reasoning Network (ScIReN): Discovering Hidden Relationships in the Carbon Cycle and Beyond",
    "authors": [
      "Joshua Fan",
      "Haodi Xu",
      "Feng Tao",
      "Md Nasim",
      "Marc Grimson",
      "Yiqi Luo",
      "Carla P. Gomes"
    ],
    "abstract": "Soils have potential to mitigate climate change by sequestering carbon from the atmosphere, but the soil carbon cycle remains poorly understood. Scientists have developed process-based models of the soil carbon cycle based on existing knowledge, but they contain numerous unknown parameters and often fit observations poorly. On the other hand, neural networks can learn patterns from data, but do not respect known scientific laws, and are too opaque to reveal novel scientific relationships. We thus propose Scientifically-Interpretable Reasoning Network (ScIReN), a fully-transparent framework that combines interpretable neural and process-based reasoning. An interpretable encoder predicts scientifically-meaningful latent parameters, which are then passed through a differentiable process-based decoder to predict labeled output variables. While the process-based decoder enforces existing scientific knowledge, the encoder leverages Kolmogorov-Arnold networks (KANs) to reveal interpretable relationships between input features and latent parameters, using novel smoothness penalties to balance expressivity and simplicity. ScIReN also introduces a novel hard-sigmoid constraint layer to restrict latent parameters into prior ranges while maintaining interpretability. We apply ScIReN on two tasks: simulating the flow of organic carbon through soils, and modeling ecosystem respiration from plants. On both tasks, ScIReN outperforms or matches black-box models in predictive accuracy, while greatly improving scientific interpretability -- it can infer latent scientific mechanisms and their relationships with input features.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.14054.pdf",
    "abs_url": "https://arxiv.org/abs/2506.14054",
    "published": "2025-06-16T23:21:37Z",
    "updated": "2026-01-26T06:05:18Z",
    "comment": "19 pages, 11 figures",
    "light_analysis": {
      "overview": "ScIReN框架结合可解释神经网络与基于过程模型，以提升科学建模的预测精度和可解释性，揭示碳循环中的隐藏关系。",
      "motivation": "土壤碳循环对气候变化缓解至关重要，但现有理解不足。基于过程的模型基于科学知识开发，但参数未知且拟合观测差；神经网络能学习数据模式，但不遵循科学定律且不透明，难以揭示新科学关系。因此，研究旨在解决这一矛盾，开发一种兼顾准确性和可解释性的方法，以改进碳循环建模并促进科学发现，解决现有方法的局限。",
      "method": "ScIReN框架结合可解释编码器和基于过程解码器。编码器使用Kolmogorov-Arnold网络（KANs）预测科学潜在参数，引入平滑惩罚平衡表达性和简单性以揭示输入特征与参数间关系；解码器基于可微分过程模型强制执行现有科学知识。关键创新包括硬Sigmoid约束层限制参数在先前范围，同时保持可解释性，实现神经与过程推理的透明结合。",
      "result": "应用ScIReN于模拟土壤有机碳流动和建模生态系统呼吸两个任务。在预测准确性方面，ScIReN优于或匹配黑盒模型，同时显著提升科学可解释性。它能推断潜在科学机制及其与输入特征的关系，具体性能指标摘要未明确说明，但强调在准确性和解释性上的双重优势，验证了框架的有效性。",
      "conclusion": "ScIReN成功整合神经网络的灵活性和过程模型的可解释性，为科学建模提供新途径。学术价值在于推动可解释AI在科学发现中的应用，实际价值包括改进碳循环预测和生态系统建模。局限性摘要未明确说明，未来可能扩展至其他科学领域，进一步提升模型通用性。",
      "tags": [
        "Kolmogorov-Arnold Networks (KANs)",
        "Interpretable Neural Networks",
        "Process-based Models",
        "Smoothness Penalties",
        "Carbon Cycle Modeling"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:02.815709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.13470",
    "title": "Induce, Align, Predict: Zero-Shot Stance Detection via Cognitive Inductive Reasoning",
    "authors": [
      "Bowen Zhang",
      "Jun Ma",
      "Fuqiang Niu",
      "Li Dong",
      "Jinzhou Cao",
      "Genan Dai"
    ],
    "abstract": "Zero-shot stance detection (ZSSD) seeks to determine the stance of text toward previously unseen targets, a task critical for analyzing dynamic and polarized online discourse with limited labeled data. While large language models (LLMs) offer zero-shot capabilities, prompting-based approaches often fall short in handling complex reasoning and lack robust generalization to novel targets. Meanwhile, LLM-enhanced methods still require substantial labeled data and struggle to move beyond instance-level patterns, limiting their interpretability and adaptability. Inspired by cognitive science, we propose the Cognitive Inductive Reasoning Framework (CIRF), a schema-driven method that bridges linguistic inputs and abstract reasoning via automatic induction and application of cognitive reasoning schemas. CIRF abstracts first-order logic patterns from raw text into multi-relational schema graphs in an unsupervised manner, and leverages a schema-enhanced graph kernel model to align input structures with schema templates for robust, interpretable zero-shot inference. Extensive experiments on SemEval-2016, VAST, and COVID-19-Stance benchmarks demonstrate that CIRF not only establishes new state-of-the-art results, but also achieves comparable performance with just 30% of the labeled data, demonstrating its strong generalization and efficiency in low-resource settings.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.13470.pdf",
    "abs_url": "https://arxiv.org/abs/2506.13470",
    "published": "2025-06-16T13:28:37Z",
    "updated": "2026-01-26T15:05:54Z",
    "comment": "Accepted at AAAI 2026",
    "light_analysis": {
      "overview": "本文提出基于认知归纳推理的Cognitive Inductive Reasoning Framework (CIRF)，通过自动归纳和应用认知推理图式，显著提升了零样本立场检测的性能和解释性。",
      "motivation": "研究动机源于零样本立场检测（ZSSD）在处理动态和极化的在线讨论时面临挑战，尤其在数据标注稀缺的情况下。现有方法如基于提示的大型语言模型（LLMs）在复杂推理和泛化到新目标方面存在不足，而LLM增强方法则需要大量标注数据，且解释性和适应性有限，限制了在低资源场景中的应用。因此，开发一个无需大量标注、具有强泛化能力和解释性的框架是重要且迫切的需求。",
      "method": "论文提出Cognitive Inductive Reasoning Framework (CIRF），一种基于认知科学的模式驱动方法。它通过无监督方式从原始文本中自动抽象一阶逻辑模式，构建多关系模式图，并利用模式增强的图核模型将输入结构与模式模板对齐，实现稳健且可解释的零样本推理。关键创新点包括结合认知推理模式来超越实例级限制，并采用图核模型进行高效结构匹配，从而提高泛化能力。",
      "result": "在SemEval-2016、VAST和COVID-19-Stance基准上的实验表明，CIRF不仅建立了新的最先进结果，而且在仅使用30%标注数据的情况下就达到了与基线方法可比的性能。这证明了其强大的泛化能力和在低资源设置下的高效率，有效解决了现有方法的不足，尤其是在数据稀缺和复杂推理场景中表现优异。",
      "conclusion": "本研究的主要贡献是CIRF框架，它通过认知归纳推理提升了零样本立场检测的性能和解释性。学术价值在于将认知科学与机器学习相结合，推动了可解释AI的发展；实际应用价值体现在低资源场景中实现高效的在线讨论分析。未来工作可能包括扩展框架到其他类似任务或进一步优化推理过程，以应对更复杂的真实世界数据。",
      "tags": [
        "Zero-Shot Stance Detection",
        "Large Language Models",
        "Cognitive Inductive Reasoning",
        "Graph Kernel Model",
        "Schema-Driven Method"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:07.385595Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.06216",
    "title": "Integer Linear Programming Preprocessing for Maximum Satisfiability",
    "authors": [
      "Jialu Zhang",
      "Chu-Min Li",
      "Sami Cherif",
      "Shuolin Li",
      "Zhifei Zheng"
    ],
    "abstract": "The Maximum Satisfiability problem (MaxSAT) is a major optimization challenge with numerous practical applications. In recent MaxSAT evaluations, most MaxSAT solvers have incorporated an Integer Linear Programming (ILP) solver into their portfolios. However, a good portfolio strategy requires a lot of tuning work and is limited to the profiling benchmark. This paper proposes a methodology to fully integrate ILP preprocessing techniques into the MaxSAT solving pipeline and investigates the impact on the top-performing MaxSAT solvers. Experimental results show that our approach helps to improve 5 out of 6 state-of-the-art MaxSAT solvers, especially for WMaxCDCL-OpenWbo1200, the winner of the MaxSAT evaluation 2024 on the unweighted track, which is able to solve 15 additional instances using our methodology.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.06216.pdf",
    "abs_url": "https://arxiv.org/abs/2506.06216",
    "published": "2025-06-06T16:21:38Z",
    "updated": "2026-01-26T11:50:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种将整数线性规划预处理技术完全集成到最大可满足性求解流程的方法，显著提升了顶级求解器的性能。",
      "motivation": "最大可满足性问题（MaxSAT）是优化领域的关键挑战，广泛应用于硬件验证、人工智能等实际场景。现有MaxSAT求解器通常将整数线性规划求解器作为组合策略的一部分，但这需要大量调参工作，并且性能局限于特定的基准测试，难以泛化到更广泛的应用中。因此，开发一种更有效的集成方法来提升求解效率和鲁棒性显得尤为重要，以克服当前方法的不足并推动该领域的发展。",
      "method": "论文提出了一种系统方法，将整数线性规划预处理技术深度整合到MaxSAT求解流程中，而不是简单地将ILP求解器作为外部工具组合使用。这种方法通过优化预处理步骤来增强求解过程，具体技术路线涉及设计新的集成框架，以减少对基准测试的依赖并提高通用性。摘要未明确说明所使用的具体数据集或模型架构，但推断作者可能在标准MaxSAT评估数据集上进行验证，以评估方法的有效性。",
      "result": "实验结果表明，该方法在6个最先进的MaxSAT求解器中成功改进了5个，显示了广泛的性能提升潜力。特别地，WMaxCDCL-OpenWbo1200（2024年MaxSAT评估无权重轨道获胜者）在使用该方法后能够解决额外的15个实例，这凸显了方法在关键场景下的优势。这些结果通过具体数据与基线方法对比，证明了ILP预处理集成的实际效果，为MaxSAT求解提供了新的改进途径。",
      "conclusion": "论文的主要贡献是验证了将整数线性规划预处理技术完全集成到MaxSAT求解流程中的可行性和有效性，该方法优于传统的组合策略，具有重要的学术和实际价值。研究为优化问题求解提供了新的思路，可能提升求解器在复杂应用中的表现。潜在局限性在于方法可能对特定问题类型或基准有依赖性，未来工作可探索更广泛的集成技术和应用到其他优化挑战中。",
      "tags": [
        "Integer Linear Programming (ILP)",
        "Maximum Satisfiability (MaxSAT)",
        "Preprocessing",
        "Solver Integration",
        "Performance Improvement"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:21.721406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.00555",
    "title": "MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning",
    "authors": [
      "Peng Xia",
      "Jinglu Wang",
      "Yibo Peng",
      "Kaide Zeng",
      "Zihan Dong",
      "Xian Wu",
      "Xiangru Tang",
      "Hongtu Zhu",
      "Yun Li",
      "Linjun Zhang",
      "Shujie Liu",
      "Yan Lu",
      "Huaxiu Yao"
    ],
    "abstract": "Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6% over strong baselines.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.00555.pdf",
    "abs_url": "https://arxiv.org/abs/2506.00555",
    "published": "2025-05-31T13:22:55Z",
    "updated": "2026-01-26T17:15:26Z",
    "comment": "ICLR 2026",
    "light_analysis": {
      "overview": "论文提出MMedAgent-RL，一种基于强化学习的多代理协作框架，通过动态优化医疗代理协作，显著提升多模态医疗推理性能。",
      "motivation": "医疗大型视觉语言模型在诊断任务中展现出潜力，但现有单代理模型难以泛化到多样化医疗专业，性能受限。近期研究引入基于临床工作流的多代理协作框架，如全科医生和专科医生固定序列交互，虽有改进但缺乏灵活性和适应性推理。这一问题导致医疗诊断中难以动态调整协作，影响准确性和效率，因此需要开发更优化的协作机制，以模拟真实医疗决策的复杂性和动态性。",
      "method": "论文提出MMedAgent-RL框架，基于强化学习训练两个全科医生代理：分诊医生学习将患者分配到合适专科，主治医师则整合多专科判断与自身知识做出最终决策。关键技术包括课程学习引导的RL策略，通过动态熵调节，逐步教导主治医师平衡模仿专科输出与纠正错误。研究使用Qwen2.5-VL作为基础模型，在五个医疗视觉问答基准上验证方法，实现动态协作优化。",
      "result": "在五个医疗视觉问答基准测试中，MMedAgent-RL均优于开源和专有医疗大型视觉语言模型。具体而言，平均性能增益达到23.6%，相较于强基线方法有显著提升。实验结果显示该框架能够有效优化多代理协作，提高推理准确性和泛化能力，验证了动态协作策略的有效性。对比表明，自适应协作机制比静态管道更能适应复杂医疗场景，带来实质性性能改进。",
      "conclusion": "论文的主要贡献是提出了基于强化学习的动态多代理协作框架MMedAgent-RL，解决了医疗推理中单代理泛化能力不足和静态协作不灵活的问题。学术上，该研究结合强化学习和课程学习，为多代理系统优化提供了新方法；实际应用上，提升医疗诊断的准确性和适应性，有潜力应用于临床辅助决策系统。未来工作可进一步扩展代理数量或应用于其他医疗任务，增强框架的通用性。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Multi-Agent System",
        "Curriculum Learning",
        "Medical Vision-Language Model"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:58.228505Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.22846",
    "title": "RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation",
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Gleb Solovev",
      "Anton Podkopaev"
    ],
    "abstract": "Interactive Theorem Proving was repeatedly shown to be fruitful when combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We identify retrieval-based premise selection as a central component of effective Rocq proof generation and propose a novel approach based on a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.22846.pdf",
    "abs_url": "https://arxiv.org/abs/2505.22846",
    "published": "2025-05-28T20:26:11Z",
    "updated": "2026-01-26T18:27:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种结合相似性驱动检索和多阶段代理系统的Rocq证明生成方法，显著提升了证明生成的性能。",
      "motivation": "研究动机是改进Rocq定理证明生成的效率和效果。交互式定理证明与生成式AI的结合虽有效，但现有方法在前提选择和证明规划方面存在不足，限制了性能提升。论文通过评估多种方法，识别检索式前提选择为关键瓶颈，旨在开发更精准的技术来优化AI辅助定理证明，推动形式验证领域的进步。摘要未明确说明具体问题细节，但强调了改进需求。",
      "method": "研究方法包括两个核心部分：一是提出基于自注意力嵌入模型的相似性驱动检索方法，用于高效选择前提以辅助Rocq证明生成；二是设计了一个多阶段代理系统，专门针对形式验证定制，系统中多个代理在规划阶段进行辩论以优化策略，并整合反思机制来增强决策的稳定性和一致性。这些创新点结合了机器学习与代理协作，提升了整体技术路线。",
      "result": "实验结果表明，提出的检索方法使生成器性能相对提升高达28%。在多阶段代理系统中，多代理辩论将整体证明成功率提高了20%，对于复杂定理，成功率几乎翻倍。反思机制进一步增强了系统的稳定性和一致性，与基线方法相比，这些改进显著验证了方法的有效性，展示了在Rocq证明生成中的高效性能。",
      "conclusion": "论文的主要贡献在于成功整合相似性驱动检索和代理系统，显著优化了Rocq证明生成的效率和准确性。研究为AI辅助定理证明和形式验证领域提供了创新方法，具有重要的学术价值和实际应用潜力。尽管摘要未明确说明局限性或未来工作，但该方法可进一步扩展到更广泛的定理证明任务中。",
      "tags": [
        "Similarity-driven Retrieval",
        "Self-Attentive Embedder",
        "Multi-Agent Systems",
        "Multi-Agent Debate",
        "Formal Verification"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:39.096457Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.20298",
    "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding",
    "authors": [
      "Jeonghun Baek",
      "Kazuki Egashira",
      "Shota Onohara",
      "Atsuyuki Miyai",
      "Yuki Imajuku",
      "Hikaru Ikuta",
      "Kiyoharu Aizawa"
    ],
    "abstract": "Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.20298.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20298",
    "published": "2025-05-26T17:59:59Z",
    "updated": "2026-01-26T03:27:53Z",
    "comment": "EACL 2026 Findings. Project page: https://manga109.github.io/MangaVQA_LMM/",
    "light_analysis": {
      "overview": "该论文提出了用于多模态漫画理解的两个基准和一个专门模型，包括MangaVQA基准和从Qwen2.5-VL微调而来的MangaLMM模型。",
      "motivation": "漫画是一种融合图像和文本的复杂多模态叙事形式，现有的LMMs在理解此类叙事时存在不足，难以实现人类级别的分析，这限制了其在辅助漫画创作者优化故事等实际应用中的潜力。因此，研究旨在通过开发专门的评估工具和模型，提升LMMs在漫画领域的性能，填补多模态理解技术在这一叙事场景中的空白，推动相关技术进步。",
      "method": "研究方法包括构建两个基准：MangaOCR用于页面内文本识别，MangaVQA则通过526个高质量、手动构建的问答对评估上下文理解。在此基础上，开发了MangaLMM模型，该模型从开源LMM Qwen2.5-VL微调而来，能够联合处理文本识别和视觉问答任务，关键创新点在于针对漫画的多模态特性设计评估基准，并优化模型以适应复杂的叙事结构。",
      "result": "通过广泛实验，包括与GPT-4o和Gemini 2.5等专有模型进行比较，评估了LMMs在漫画理解任务上的表现。实验表明，MangaLMM在漫画多模态理解方面显示出有效性，但摘要未明确说明具体性能指标，如准确率或效率改进，主要基于对比评估突显了模型在漫画领域的潜力。",
      "conclusion": "该研究的主要贡献在于提供了MangaOCR和MangaVQA基准以及MangaLMM模型，为评估和推进LMMs在漫画叙事领域奠定了基础。学术上促进了多模态理解技术的发展，实际应用中有助于漫画创作者反思和改进故事。未来工作可扩展数据集规模或增强模型泛化能力，以克服潜在局限性。",
      "tags": [
        "Multimodal Understanding",
        "Visual Question Answering",
        "Large Multimodal Models",
        "Benchmark Evaluation",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:51.146258Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19514",
    "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback",
    "authors": [
      "Yaoning Yu",
      "Ye Yu",
      "Peiyan Zhang",
      "Kai Wei",
      "Haojing Luo",
      "Haohan Wang"
    ],
    "abstract": "Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.19514.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19514",
    "published": "2025-05-26T04:56:48Z",
    "updated": "2026-01-26T15:07:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了SIPDO框架，通过集成合成数据生成和反馈循环实现闭环提示优化，提升大型语言模型的提示质量。",
      "motivation": "提示质量对大型语言模型性能至关重要，但现有提示优化方法通常基于固定数据集，假设静态输入分布，缺乏迭代改进能力。这限制了提示优化的适应性和有效性，尤其是在动态任务环境中。因此，需要一种能够自我改进的方法来克服这些限制，以支持更高效的提示学习和模型性能提升。",
      "method": "SIPDO是一个闭环框架，结合合成数据生成器和提示优化器。生成器产生新示例以揭示当前提示弱点，优化器基于反馈逐步优化提示，形成反馈驱动循环。该方法无需外部监督或新任务，关键创新在于将数据合成集成到优化过程，实现自改进的提示学习。",
      "result": "在多个问答和推理基准测试中，SIPDO表现出优于标准提示调优方法的性能。这突显了集成数据合成到提示学习工作流的有效性，尽管具体数据如准确率提升未在摘要中详细说明，但实验表明该方法能系统性改进提示质量，增强了模型在这些任务上的表现。",
      "conclusion": "SIPDO框架通过闭环反馈循环实现提示自改进，强调了数据合成在提示学习中的价值，为动态任务优化提供新思路。该研究的学术意义在于提出了一种无需外部监督的优化方法，实际应用潜力在于提升LLM在复杂任务中的适应性。未来工作可扩展至更多任务类型和模型架构。",
      "tags": [
        "Large Language Model",
        "Prompt Optimization",
        "Synthetic Data Generation",
        "Feedback Loop",
        "Self-Improving Systems"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:32.363940Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19459",
    "title": "Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation",
    "authors": [
      "Kaichao Jiang",
      "He Wang",
      "Xiaoshuai Hao",
      "Xiulong Yang",
      "Ajian Liu",
      "Qi Chu",
      "Yunfeng Diao",
      "Richang Hong"
    ],
    "abstract": "Joint Energy-based Models (JEMs) are well known for their ability to unify classification and generation within a single framework. Despite their promising generative and discriminative performance, their robustness remains far inferior to adversarial training (AT), which, conversely, achieves strong robustness but sacrifices clean accuracy and lacks generative ability. This inherent trilemma-balancing classification accuracy, robustness, and generative capability-raises a fundamental question: Can a single model achieve all three simultaneously? To answer this, we conduct a systematic energy landscape analysis of clean, adversarial, and generated samples across various JEM and AT variants. We observe that AT reduces the energy gap between clean and adversarial samples, while JEMs narrow the gap between clean and synthetic ones. This observation suggests a key insight: if the energy distributions of all three data types can be aligned, we might bridge their performance disparities. Building on this idea, we propose Energy-based Joint Distribution Adversarial Training (EB-JDAT), a unified generative-discriminative-robust framework that maximizes the joint probability of clean and adversarial distribution. EB-JDAT introduces a novel min-max energy optimization to explicitly aligning energies across clean, adversarial, and generated samples. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet subsets demonstrate that EB-JDAT achieves state-of-the-art robustness while maintaining near-original accuracy and generation quality of JEMs, effectively resolving the triple trade-off between accuracy, robustness, and generation.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.19459.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19459",
    "published": "2025-05-26T03:26:55Z",
    "updated": "2026-01-26T06:41:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了EB-JDAT方法，通过能量优化对齐清洁、对抗和生成样本的能量分布，有效解决了分类准确性、鲁棒性和生成能力之间的三难问题。",
      "motivation": "当前AI模型中，联合能量模型（JEMs）在分类和生成方面表现优异，但鲁棒性不足；对抗训练（AT）能提升鲁棒性，却牺牲了清洁准确性和生成能力。这导致了分类准确性、鲁棒性和生成能力之间的三难平衡问题，限制了多功能模型的发展。因此，研究旨在探索能否在单一模型中同时实现这三方面的高性能，以推动更全面、鲁棒的AI系统构建，弥补现有方法的不足。",
      "method": "论文首先对清洁、对抗和生成样本的能量分布进行系统分析，发现AT减少清洁与对抗样本能量差距，JEMs缩小清洁与合成样本差距。基于此，提出Energy-based Joint Distribution Adversarial Training (EB-JDAT)，一个统一的生成-判别-鲁棒框架。该方法采用最小最大能量优化，显式对齐三类样本的能量，最大化清洁和对抗分布的联合概率。实验在CIFAR-10、CIFAR-100和ImageNet子集上进行，关键创新在于能量景观分析和联合概率优化。",
      "result": "在CIFAR-10、CIFAR-100和ImageNet子集上的实验显示，EB-JDAT实现了最先进的鲁棒性表现，同时保持了接近原始的清洁准确性和JEMs的生成质量。与基线方法如JEMs和AT相比，EB-JDAT显著缓解了鲁棒性与准确性、生成能力之间的权衡，有效平衡了三者性能。具体性能指标在摘要中未明确说明，但结果证实了该方法在解决三难问题上的有效性，显示出综合优势。",
      "conclusion": "EB-JDAT成功解决了分类准确性、鲁棒性和生成能力之间的三难问题，为构建多功能AI模型提供了统一框架。其学术价值在于融合了生成、判别和鲁棒训练，推动了能量基础模型的研究；实际应用上，可用于开发更安全、准确且具备生成能力的AI系统。未来工作可能包括扩展到更大数据集、其他任务或优化能量对齐策略，进一步提升模型性能。",
      "tags": [
        "Energy-based Models",
        "Adversarial Training",
        "Joint Distribution",
        "Min-max Optimization",
        "Classification"
      ]
    },
    "analyzed_at": "2026-01-27T03:29:50.846054Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.18832",
    "title": "Localizing Knowledge in Diffusion Transformers",
    "authors": [
      "Arman Zarei",
      "Samyadeep Basu",
      "Keivan Rezaei",
      "Zihao Lin",
      "Sayan Nag",
      "Soheil Feizi"
    ],
    "abstract": "Understanding how knowledge is distributed across the layers of generative models is crucial for improving interpretability, controllability, and adaptation. While prior work has explored knowledge localization in UNet-based architectures, Diffusion Transformer (DiT)-based models remain underexplored in this context. In this paper, we propose a model- and knowledge-agnostic method to localize where specific types of knowledge are encoded within the DiT blocks. We evaluate our method on state-of-the-art DiT-based models, including PixArt-alpha, FLUX, and SANA, across six diverse knowledge categories. We show that the identified blocks are both interpretable and causally linked to the expression of knowledge in generated outputs. Building on these insights, we apply our localization framework to two key applications: model personalization and knowledge unlearning. In both settings, our localized fine-tuning approach enables efficient and targeted updates, reducing computational cost, improving task-specific performance, and better preserving general model behavior with minimal interference to unrelated or surrounding content. Overall, our findings offer new insights into the internal structure of DiTs and introduce a practical pathway for more interpretable, efficient, and controllable model editing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.18832.pdf",
    "abs_url": "https://arxiv.org/abs/2505.18832",
    "published": "2025-05-24T19:02:20Z",
    "updated": "2026-01-26T04:03:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种模型无关的方法来定位Diffusion Transformers中的知识编码，并应用于高效和可解释的模型编辑。",
      "motivation": "研究动机在于探索知识在生成模型层中的分布，以提升模型的可解释性、可控性和适应性。现有研究主要集中于UNet架构，而基于Diffusion Transformer (DiT)的模型在此方面尚未充分探索，这限制了模型编辑、个性化和知识遗忘等应用的发展。因此，针对DiT模型进行知识定位研究具有重要意义，有助于开发更高效和可控的AI系统。",
      "method": "论文提出了一种模型和知识无关的方法，用于定位DiT块中特定类型知识的编码位置。该方法的关键创新点在于其通用性和可解释性，不依赖具体模型架构或知识类型，并通过因果分析验证定位结果的可靠性。评估基于多个先进的DiT模型（如PixArt-alpha、FLUX、SANA）和六个多样化的知识类别。摘要未详细说明具体技术细节如算法或数据集，但突出了方法的模型无关特性。",
      "result": "实验结果显示，定位出的DiT块具有可解释性，并与生成输出中的知识表达存在因果联系。应用在模型个性化和知识遗忘中，局部微调方法有效减少了计算成本，提升了任务特定性能，并更好地保留了模型的一般行为，最小化了对无关或周围内容的干扰。摘要未提供具体量化数据如准确率提升百分比，但强调了方法的效率和目标更新效果。",
      "conclusion": "论文的主要贡献是揭示了DiT模型的内部知识结构，并引入了一种实用的模型编辑框架。学术价值在于增进了对生成模型可解释性的理解，实际应用价值体现在实现更高效、可控和个性化的模型更新。未来工作可能包括扩展方法到其他模型架构或探索更多应用场景，摘要未明确说明局限性。",
      "tags": [
        "Diffusion Transformer (DiT)",
        "Knowledge Localization",
        "Model Editing",
        "Personalized Fine-tuning",
        "Knowledge Unlearning"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:10.716915Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.18663",
    "title": "DVD-Quant: Data-free Video Diffusion Transformers Quantization",
    "authors": [
      "Zhiteng Li",
      "Hanxuan Li",
      "Junyi Wu",
      "Kai Liu",
      "Haotong Qin",
      "Linghe Kong",
      "Guihai Chen",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Diffusion Transformers (DiTs) have emerged as the state-of-the-art architecture for video generation, yet their computational and memory demands hinder practical deployment. While post-training quantization (PTQ) presents a promising approach to accelerate Video DiT models, existing methods suffer from two critical limitations: (1) dependence on computation-heavy and inflexible calibration procedures, and (2) considerable performance deterioration after quantization. To address these challenges, we propose DVD-Quant, a novel Data-free quantization framework for Video DiTs. Our approach integrates three key innovations: (1) Bounded-init Grid Refinement (BGR) and (2) Auto-scaling Rotated Quantization (ARQ) for calibration data-free quantization error reduction, as well as (3) $δ$-Guided Bit Switching ($δ$-GBS) for adaptive bit-width allocation. Extensive experiments across multiple video generation benchmarks demonstrate that DVD-Quant achieves an approximately 2$\\times$ speedup over full-precision baselines on advanced DiT models while maintaining visual fidelity. Notably, DVD-Quant is the first to enable W4A4 PTQ for Video DiTs without compromising video quality. Code and models will be available at https://github.com/lhxcs/DVD-Quant.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.18663.pdf",
    "abs_url": "https://arxiv.org/abs/2505.18663",
    "published": "2025-05-24T11:56:02Z",
    "updated": "2026-01-26T16:04:47Z",
    "comment": "Code and models will be available at https://github.com/lhxcs/DVD-Quant",
    "light_analysis": {
      "overview": "论文提出DVD-Quant，一种无数据校准的视频扩散变换器量化框架，首次实现W4A4量化而不损失视频生成质量。",
      "motivation": "扩散变换器（DiTs）作为视频生成的先进架构，计算和内存需求高，难以实际部署。后训练量化（PTQ）是加速方法，但现有方法依赖计算重、不灵活的校准过程，且量化后性能显著下降。这阻碍了高效部署，需要更智能、无数据依赖的量化方案，以减少资源消耗和性能损失。",
      "method": "论文提出DVD-Quant框架，集成三个关键创新：Bounded-init Grid Refinement（BGR）和Auto-scaling Rotated Quantization（ARQ）用于无校准数据的量化误差减少，以及δ-Guided Bit Switching（δ-GBS）用于自适应位宽分配。该方法基于Video DiT模型，通过内部优化机制，无需外部数据校准，有效降低量化过程中的精度损失。",
      "result": "实验在多个视频生成基准上进行，结果表明DVD-Quant相比全精度基线实现约2倍加速，同时保持视觉保真度。特别地，首次在Video DiTs上成功应用W4A4的PTQ，不损害视频质量，与现有方法相比减少了性能退化。这证明了DVD-Quant在加速模型时的有效性和鲁棒性。",
      "conclusion": "DVD-Quant解决了Video DiTs量化中的关键挑战，通过无数据校准和自适应技术减少了部署障碍，具有学术和实际应用价值。其创新方法为高效视频生成模型的发展提供了新方向。代码和模型公开，促进未来研究和应用扩展，摘要未明确说明局限性。",
      "tags": [
        "Data-free Quantization",
        "Video Diffusion Transformers",
        "Post-training Quantization",
        "Adaptive Bit-width Allocation",
        "Bounded-init Grid Refinement"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:08.796136Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.15556",
    "title": "A Survey on Multilingual Mental Disorders Detection from Social Media Data",
    "authors": [
      "Ana-Maria Bucur",
      "Marcos Zampieri",
      "Tharindu Ranasinghe",
      "Fabio Crestani"
    ],
    "abstract": "The increasing prevalence of mental disorders globally highlights the urgent need for effective digital screening methods that can be used in multilingual contexts. Most existing studies, however, focus on English data, overlooking critical mental health signals that may be present in non-English texts. To address this gap, we present a survey of the detection of mental disorders using social media data beyond the English language. We compile a comprehensive list of 108 datasets spanning 25 languages that can be used for developing NLP models for mental health screening. In addition, we discuss the cultural nuances that influence online language patterns and self-disclosure behaviors, and how these factors can impact the performance of NLP tools. Our survey highlights major challenges, including the scarcity of resources for low- and mid-resource languages and the dominance of depression-focused data over other disorders. By identifying these gaps, we advocate for interdisciplinary collaborations and the development of multilingual benchmarks to enhance mental health screening worldwide.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.15556.pdf",
    "abs_url": "https://arxiv.org/abs/2505.15556",
    "published": "2025-05-21T14:15:54Z",
    "updated": "2026-01-26T09:53:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文是一篇关于多语言精神障碍检测的综述，通过编译数据集列表和分析文化因素，填补了非英语数据研究的空白。",
      "motivation": "全球精神障碍的日益普遍凸显了在多语言环境中开发有效数字筛查方法的紧迫性。现有研究大多聚焦于英语数据，忽视了非英语文本中可能包含的心理健康信号，导致对非英语人群的筛查不足。这个问题的重要性在于心理健康是全球性挑战，缺乏多语言工具会限制筛查的覆盖范围和准确性，而现有方法的不足在于无法处理语言和文化多样性带来的复杂影响，从而影响筛查的有效性和公平性。",
      "method": "本文采用文献综述的方法，系统梳理多语言精神障碍检测的研究。技术路线包括编译一个涵盖25种语言、108个数据集的全面列表，为开发NLP模型提供资源支持。关键创新点在于整合多语言数据资源，并深入讨论文化细微差别如何影响在线语言模式和自我披露行为，进而分析这些因素对NLP工具性能的潜在影响。摘要未明确说明具体模型架构或实验设置，但突出了数据收集和文化分析的集成。",
      "result": "作为综述，本文没有具体实验数据，但总结了主要发现：当前领域面临资源稀缺的挑战，尤其在中低资源语言中数据不足，且数据集以抑郁为中心，其他精神障碍覆盖较少。通过编译数据集列表，揭示了多语言数据分布的现状，并讨论了文化因素可能导致的检测性能差异。这些结果为未来研究提供了方向，但摘要未提供与基线方法的直接性能对比或具体指标提升。",
      "conclusion": "本文通过综述多语言精神障碍检测，提供了数据集资源和文化因素分析，填补了现有研究的空白。主要贡献包括为NLP模型开发奠定基础，识别关键挑战如语言资源不平衡。研究的学术价值在于推动跨语言心理健康研究的进展，实际应用价值在于促进全球心理健康筛查工具的开发。未来工作方向包括倡导跨学科合作和开发多语言基准测试，以应对资源不足和数据偏斜的问题。",
      "tags": [
        "Multilingual NLP",
        "Mental Disorders Detection",
        "Social Media Data",
        "Cultural Nuances",
        "Dataset Survey"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:20.146584Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.14313",
    "title": "Teaching Small Language Models to Learn Logic through Meta-Learning",
    "authors": [
      "Leonardo Bertolazzi",
      "Manuel Vargas Guzmán",
      "Raffaella Bernardi",
      "Maciej Malicki",
      "Jakub Szymanik"
    ],
    "abstract": "Large language models (LLMs) are increasingly evaluated on reasoning tasks, yet their logical abilities remain contested. To address this, we study LLMs' reasoning in a well-defined fragment of logic: syllogistic reasoning. We cast the problem as premise selection and construct controlled datasets to isolate logical competence. Beyond evaluation, an open challenge is enabling LLMs to acquire abstract inference patterns that generalize to novel structures. We propose to apply few-shot meta-learning to this domain, thereby encouraging models to extract rules across tasks rather than memorize patterns within tasks. Although meta-learning has been little explored in the context of logic learnability, our experiments show that it is effective: small models (1.5B-7B) fine-tuned with meta-learning demonstrate strong gains in generalization, with especially pronounced benefits in low-data regimes. These meta-learned models outperform GPT-4o and o3-mini on our syllogistic reasoning task.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.14313.pdf",
    "abs_url": "https://arxiv.org/abs/2505.14313",
    "published": "2025-05-20T13:00:48Z",
    "updated": "2026-01-26T09:25:06Z",
    "comment": "EACL 2026 Main",
    "light_analysis": {
      "overview": "本文提出通过少样本元学习方法，教学小型语言模型学习逻辑推理，显著提升其在低数据情况下的泛化能力。",
      "motivation": "该研究动机源于大型语言模型在逻辑推理任务上表现的不确定性，尽管在推理任务中被频繁评估，但其核心逻辑能力仍存在争议。作者针对这一不足，选择三段论推理作为定义明确的逻辑领域，通过构建受控数据集来隔离和精确评估模型的逻辑能力。现有方法可能侧重于模式记忆而非真正理解逻辑规则，导致泛化能力受限，因此需要开发能提取抽象推理模式并应对新结构任务的技术。",
      "method": "作者将问题形式化为前提选择，并构建了受控的三段论推理数据集来隔离逻辑能力。核心方法采用少样本元学习，通过训练模型在多个任务中学习通用逻辑规则，而非仅仅记忆特定任务中的模式。具体实验中，使用参数规模为1.5B到7B的小型语言模型进行元学习微调，元学习策略帮助模型提取跨任务的抽象推理模式，从而增强学习效率。",
      "result": "实验结果显示，经过元学习微调的1.5B到7B小型模型在泛化能力上取得了强劲增长，特别是在低数据环境下表现更为突出，强调了元学习在少数据场景的优势。这些模型在三段论推理任务中性能超越了GPT-4o和o3-mini等大型模型，证明了元学习在提升逻辑推理能力方面的有效性，为方法提供了实证支持。",
      "conclusion": "本研究的主要贡献是验证了元学习在让小型语言模型学习逻辑推理上的有效性，显著提升了泛化能力，尤其适用于数据稀缺情况。学术上，它为语言模型的逻辑学习研究提供了新方法，强调跨任务学习的重要性；实际应用中，有助于开发更高效和可扩展的推理系统。未来工作可能包括将元学习扩展到更广泛的逻辑任务和其他推理领域，以进一步探索其应用潜力。",
      "tags": [
        "Meta-Learning",
        "Few-Shot Learning",
        "Syllogistic Reasoning",
        "Premise Selection",
        "Logic Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:39.264407Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.14125",
    "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning",
    "authors": [
      "Viet Anh Khoa Tran",
      "Emre Neftci",
      "Willem A. M. Wybo"
    ],
    "abstract": "Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.14125.pdf",
    "abs_url": "https://arxiv.org/abs/2505.14125",
    "published": "2025-05-20T09:31:57Z",
    "updated": "2026-01-26T08:58:24Z",
    "comment": "Accepted to NeurIPS 2025. Camera-ready version. 33 pages, 5 figures. Updated acknowledgements. Code available at: https://github.com/tran-khoa/tmcl",
    "light_analysis": {
      "overview": "论文引入任务调制对比学习（TMCL），通过自上而下调制实现稀疏监督下的持续学习，避免了灾难性遗忘。",
      "motivation": "生物大脑能持续从无标签数据流中学习，并集成稀疏标签信息而不影响泛化能力，但机器学习方法在持续学习设置中易受灾难性遗忘影响。现有监督微调方法会损害模型在原始任务上的性能，限制了在现实应用如机器人或自动驾驶中的实用性。因此，研究旨在开发一种能模仿生物学习机制的方法，以平衡新知识获取和旧知识保留。",
      "method": "研究提出任务调制对比学习（TMCL），受新皮层生物物理机制启发，采用预测编码原理和对比损失构建视图不变性表示空间。核心创新包括学习仿射调制来增强新类分离而不影响前馈权重，并通过协同视图不变性机制训练前馈权重匹配无调制和调制表示，从而引入调制不变性并利用过去调制稳定表示空间。摘要未明确说明具体数据集或模型架构，但技术路线基于对比学习和调制机制。",
      "result": "实验显示TMCL在类增量和迁移学习任务中，相比最先进的无监督方法和可比的有监督方法，性能有所改善，仅使用1%的可用标签即可实现。这表明方法能有效利用稀疏监督，提升学习效率和泛化能力，但摘要未提供具体准确率或效率数据，仅概述了性能改进的总体趋势。",
      "conclusion": "本工作强调了自上而下调制在平衡学习稳定性和可塑性中的关键作用，TMCL方法提升了持续学习性能，为生物启发式机器学习提供了新思路。研究的学术价值在于推动持续学习领域的发展，实际应用价值在于使模型能在稀疏监督下高效学习。未来可探索调制机制的扩展或在更大数据集上的验证。",
      "tags": [
        "Contrastive Learning",
        "Continual Learning",
        "Predictive Coding",
        "Task Modulation",
        "Top-Down Modulations"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:39.655822Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.12540",
    "title": "Harnessing the Universal Geometry of Embeddings",
    "authors": [
      "Rishi Jha",
      "Collin Zhang",
      "Vitaly Shmatikov",
      "John X. Morris"
    ],
    "abstract": "We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets.   The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.12540.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12540",
    "published": "2025-05-18T20:37:07Z",
    "updated": "2026-01-26T14:47:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种无监督方法，用于在无需配对数据的情况下跨向量空间翻译文本嵌入到通用语义结构。",
      "motivation": "该研究旨在解决文本嵌入翻译中依赖配对数据或编码器的限制问题。现有方法通常需要配对数据或预定义匹配集，使得跨不同模型的嵌入翻译困难，特别是在安全敏感场景中。本文探索通用语义结构，实现无监督翻译，以评估向量数据库的安全风险。",
      "method": "论文提出了一种无监督翻译方法，基于柏拉图表示假设的通用潜在语义结构。该方法能够将任何文本嵌入向量从一个向量空间翻译到另一个，而无需任何配对数据、编码器或匹配集。关键创新在于利用通用表示作为中介，实现跨不同架构和训练数据的模型嵌入转换。",
      "result": "实验结果显示，该翻译方法在不同架构、参数量和训练数据集的模型对之间实现了高余弦相似度。这表明翻译过程有效保留了嵌入的几何结构，验证了通用表示假设的可行性。摘要未提供具体性能对比数据，但强调了翻译质量的优越性。",
      "conclusion": "该研究的主要贡献是提出了一种无需配对数据的嵌入翻译方法，并验证了其跨模型有效性。此外，它揭示了这种能力对向量数据库安全的严重威胁，表明攻击者可能利用翻译后的嵌入提取敏感信息。这为未来在安全防护和隐私保护方向的研究提供了重要启示。",
      "tags": [
        "Text Embeddings",
        "Unsupervised Translation",
        "Universal Latent Representation",
        "Cosine Similarity",
        "Vector Database Security"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:39.390839Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.12513",
    "title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification",
    "authors": [
      "Yang Mu",
      "Zhitong Xiong",
      "Yi Wang",
      "Muhammad Shahzad",
      "Franz Essl",
      "Holger Kreft",
      "Mark van Kleunen",
      "Xiao Xiang Zhu"
    ],
    "abstract": "Global tree species mapping using remote sensing data is vital for biodiversity monitoring, forest management, and ecological research. However, progress in this field has been constrained by the scarcity of large-scale, labeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive global dataset for tree species classification. GlobalGeoTree comprises 6.3 million geolocated tree occurrences, spanning 275 families, 2,734 genera, and 21,001 species across the hierarchical taxonomic levels. Each sample is paired with Sentinel-2 image time series and 27 auxiliary environmental variables, encompassing bioclimatic, geographic, and soil data. The dataset is partitioned into GlobalGeoTree-6M for model pretraining and curated evaluation subsets, primarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To demonstrate the utility of the dataset, we introduce a baseline model, GeoTreeCLIP, which leverages paired remote sensing data and taxonomic text labels within a vision-language framework pretrained on GlobalGeoTree-6M. Experimental results show that GeoTreeCLIP achieves substantial improvements in zero- and few-shot classification on GlobalGeoTree-10kEval over existing advanced models. By making the dataset, models, and code publicly available, we aim to establish a benchmark to advance tree species classification and foster innovation in biodiversity research and ecological applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.12513.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12513",
    "published": "2025-05-18T18:31:00Z",
    "updated": "2026-01-26T12:22:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了GlobalGeoTree多粒度视觉-语言数据集和GeoTreeCLIP基线模型，以解决全球树木物种分类中大规模数据稀缺问题。",
      "motivation": "全球树木物种分类对生物多样性监测、森林管理和生态研究至关重要，但现有方法因缺乏大规模、标注的全球数据集而受限。数据稀缺阻碍了精确物种映射和模型泛化，导致分类精度不足，影响相关研究和应用进展。因此，本研究旨在构建一个全面数据集来推动该领域发展。",
      "method": "研究构建了GlobalGeoTree数据集，包含6.3百万个地理位置点，覆盖275个科、2,734个属和21,001个物种，每个样本配对Sentinel-2图像时间序列和27个辅助环境变量（如生物气候和土壤数据）。数据集分为GlobalGeoTree-6M用于预训练和GlobalGeoTree-10kEval用于评估。基线模型GeoTreeCLIP采用视觉-语言框架，通过预训练遥感图像与分类文本标签的配对数据来实现多模态学习。",
      "result": "实验结果表明，GeoTreeCLIP在GlobalGeoTree-10kEval数据集上实现了显著的零样本和少样本分类改进，优于现有高级模型。虽然摘要未提供具体准确率数字，但模型展示了更好的泛化能力和性能提升，验证了数据集和方法的有效性，为树木物种分类建立了新基准。",
      "conclusion": "本论文的主要贡献是引入了GlobalGeoTree数据集和GeoTreeCLIP模型，建立了全球树木物种分类的基准。学术价值在于填补大规模数据空白，促进视觉-语言学习在生态领域的应用；实际价值是支持生物多样性监测和生态研究。未来工作可能包括扩展数据集或优化模型，但摘要未明确说明具体局限性。",
      "tags": [
        "Vision-Language Framework",
        "Remote Sensing",
        "Dataset Creation",
        "Zero-Shot Classification",
        "Few-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:53.133674Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.02831",
    "title": "No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves",
    "authors": [
      "Dengyang Jiang",
      "Mengmeng Wang",
      "Liuzhuozheng Li",
      "Lei Zhang",
      "Haoyu Wang",
      "Wei Wei",
      "Guang Dai",
      "Yanning Zhang",
      "Jingdong Wang"
    ],
    "abstract": "Recent studies have demonstrated that learning a meaningful internal representation can accelerate generative training. However, existing approaches necessitate to either introduce an off-the-shelf external representation task or rely on a large-scale, pre-trained external representation encoder to provide representation guidance during the training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We propose SelfRepresentation Alignment (SRA), a simple yet effective method that obtains representation guidance using the internal representations of learned diffusion transformer. SRA aligns the latent representation of the diffusion transformer in the earlier layer conditioned on higher noise to that in the later layer conditioned on lower noise to progressively enhance the overall representation learning during only the training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements, and largely outperforms approaches relying on auxiliary representation task. Our approach achieves performance comparable to methods that are dependent on an external pre-trained representation encoder, which demonstrates the feasibility of acceleration with representation alignment in diffusion transformers themselves.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.02831.pdf",
    "abs_url": "https://arxiv.org/abs/2505.02831",
    "published": "2025-05-05T17:58:05Z",
    "updated": "2026-01-26T17:30:19Z",
    "comment": "ICLR 2026. Self-Representation Alignment for Diffusion Transformers. Code: https://github.com/vvvvvjdy/SRA",
    "light_analysis": {
      "overview": "论文提出SelfRepresentation Alignment（SRA）方法，使扩散变换器无需外部组件即可自我提供表示指导，加速生成训练。",
      "motivation": "研究旨在加速生成模型的训练，通过增强内部表示学习来减少训练时间和资源消耗。现有方法需要引入外部表示任务或依赖大规模预训练编码器提供指导，这增加了复杂性和对外部系统的依赖。问题的重要性在于提高训练效率和简化模型设计，但现有方法因外部组件可能导致额外计算成本或数据需求不足。摘要未明确说明具体应用领域，但聚焦于机器学习中生成模型的高效训练挑战。",
      "method": "论文提出的核心方法是SelfRepresentation Alignment（SRA），利用扩散变换器自身的内部表示来获得表示指导，无需外部组件。具体而言，SRA在训练过程中对齐早期层（基于高噪声条件）和后期层（基于低噪声条件）的潜在表示，以逐步增强整体表示学习。关键创新点在于仅依赖扩散变换器的内部结构，通过噪声条件的表示对齐实现自我指导。摘要提到应用到DiTs和SiTs模型，但未指定具体的数据集或架构细节，因此推断涉及标准扩散变换器架构。",
      "result": "实验结果表明，将SRA应用于DiTs和SiTs模型时，性能得到一致提升，大幅优于依赖辅助表示任务的方法。性能与依赖外部预训练表示编码器的方法相当，证明了SRA的有效性。摘要未提供具体的数据指标如准确率或效率改进数值，但强调了性能改进和对比优势，表明SRA在生成训练中能提供类似的加速效果，而无需外部资源。",
      "conclusion": "论文的主要贡献是证明了扩散变换器可以通过自我表示对齐提供表示指导，无需外部组件，这展示了在扩散变换器自身中加速训练表示的可行性。研究的学术价值在于为生成模型训练提供了一种更自足和高效的方法，减少对外部系统的依赖。实际应用价值可能包括更高效的模型部署和训练流程优化。未来工作方向可能包括扩展到其他生成模型或探索更多噪声条件策略，摘要未明确说明局限性。",
      "tags": [
        "Diffusion Transformers",
        "Self-Representation Alignment",
        "Representation Learning",
        "Generative Training",
        "Noise Conditioning"
      ]
    },
    "analyzed_at": "2026-01-27T03:30:54.556354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.02567",
    "title": "Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities",
    "authors": [
      "Shanshan Zhao",
      "Xinjie Zhang",
      "Jintao Guo",
      "Jiakui Hu",
      "Lunhao Duan",
      "Minghao Fu",
      "Yong Xien Chng",
      "Guo-Hua Wang",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang"
    ],
    "abstract": "Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.02567.pdf",
    "abs_url": "https://arxiv.org/abs/2505.02567",
    "published": "2025-05-05T11:18:03Z",
    "updated": "2026-01-26T03:08:38Z",
    "comment": "In this version, we incorporate new papers (after Aug. 2025), datasets, and benchmarks. This work is still in progress; Github project: https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models",
    "light_analysis": {
      "overview": "这篇论文提供了一个关于多模态理解和生成模型统一框架的全面综述，分析不同架构范式并讨论关键挑战。",
      "motivation": "近年来，多模态理解模型和图像生成模型在各自领域取得显著进展，但它们独立发展导致架构范式分离：自回归模型主导理解任务，扩散模型主导生成任务。这种分离限制了统一框架的开发，影响了AI系统在多任务处理中的效率和多功能性。GPT-4o的新功能展示了统一模型的潜力，突显了集成任务的重要性。然而，架构差异带来挑战，如tokenization策略、跨模态注意力等，使得统一变得困难。因此，本研究旨在通过综述当前努力，揭示现有方法的不足，为未来研究提供清晰指导，推动这一新兴领域的发展。",
      "method": "本研究采用综述方法，系统性地整理和分析统一多模态理解和生成模型的现有工作。首先，介绍多模态理解和文本到图像生成模型的基础概念及最新进展。然后，将现有统一模型分类为三个主要架构范式：基于扩散的模型、基于自回归的模型，以及融合自回归和扩散机制的混合方法。对每个类别，分析相关工作的结构设计和创新点，例如模型架构和训练策略。此外，编译了专门为统一模型设计的数据集和基准，为未来探索提供资源支持。这种方法通过分类和分析，为研究人员提供了技术路线图和潜在研究方向。",
      "result": "摘要未明确说明具体的实验结果数据，因为这是一篇综述论文。论文的主要成果包括对统一多模态模型的全面调查和分类为三个架构范式，分析每个范式的设计创新。通过编译数据集和基准，论文提供了资源以支持未来研究。虽然没有提供具体的性能指标（如准确率或效率提升），但综述总结了当前模型的进展和挑战，如tokenization策略和跨模态注意力问题，并与基线方法对比了不同范式的优缺点，为社区提供了有价值的参考和未来探索的基础。",
      "conclusion": "论文的主要贡献在于提供了一个关于多模态理解和生成模型统一框架的全面综述，详细分类现有模型并分析关键挑战。该研究具有重要的学术价值，为AI社区提供了清晰的技术路线图和未来研究方向，有助于推动统一模型的发展。实际应用价值体现在促进多功能AI系统的开发，如更高效的跨模态任务处理。局限性在于该领域仍处于早期阶段，面临诸如tokenization策略、跨模态注意力等核心挑战。未来工作方向包括解决这些技术问题，并期待通过定期更新调查来反映快速技术进步，保持其时效性和指导作用。",
      "tags": [
        "Multimodal Understanding",
        "Text-to-Image Generation",
        "Diffusion Models",
        "Autoregressive Models",
        "Hybrid Architectures"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:08.105373Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.02380",
    "title": "EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices",
    "authors": [
      "Arnab Sanyal",
      "Gourav Datta",
      "Prithwish Mukherjee",
      "Sandeep P. Chinchali",
      "Michael Orshansky"
    ],
    "abstract": "Large Language Models (LLMs) achieve strong performance across tasks, but face storage and compute challenges on edge devices. We propose EntroLLM, a compression framework combining mixed quantization and entropy coding to reduce storage while preserving accuracy. We use a combination of unsigned and asymmetric quantization. Tensor-level quantization produces an entropy-reducing effect, increasing weight compressibility, and improving downstream Huffman encoding by $7\\times$ (8-bit) and $11.3\\times$ (4-bit) over state-of-the-art methods. Huffman coding further reduces memory bandwidth demands, while a parallel decoding strategy enables efficient weight retrieval with minimal latency. Experiments on edge-scale LLMs (smolLM-1.7B, phi3-mini-4k, mistral-7B) show up to $30\\%$ storage savings over uint8 and $65\\%$ over uint4 models, with $31.9-146.6\\%$ faster inference on memory-limited devices like the NVIDIA JETSON P3450. EntroLLM requires no retraining and is compatible with existing post-training quantization pipelines, making it practical for edge LLM deployment.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.02380.pdf",
    "abs_url": "https://arxiv.org/abs/2505.02380",
    "published": "2025-05-05T05:42:14Z",
    "updated": "2026-01-26T11:44:39Z",
    "comment": "4 pages, 1 reference page",
    "light_analysis": {
      "overview": "EntroLLM提出一种结合混合量化和熵编码的压缩框架，用于减少大语言模型的存储需求并保持精度，适用于边缘设备高效推理。",
      "motivation": "大语言模型（LLMs）在各类任务中表现出色，但其庞大的模型参数导致在资源受限的边缘设备上部署时面临存储和计算瓶颈。边缘设备如移动和嵌入式系统需要轻量级模型以实现本地推理，以降低延迟和增强隐私保护。现有量化方法在压缩模型权重时可能损失精度或效率不足，因此需要更高效的压缩技术来平衡模型大小与性能，这是本研究的核心动机。",
      "method": "EntroLLM采用混合量化和熵编码结合的方法。具体而言，使用无符号和非对称量化对模型权重进行张量级量化，这种量化方式能减少权重分布的熵，增强可压缩性。接着应用哈夫曼编码进一步压缩量化权重，降低内存带宽需求，同时设计并行解码策略以实现高效权重检索，最小化延迟。该方法无需重新训练，兼容现有的后训练量化流程，便于实际部署。",
      "result": "实验在多个边缘规模LLMs（如smolLM-1.7B、phi3-mini-4k、mistral-7B）上进行，结果显示相比uint8量化模型节省高达30%存储空间，相比uint4模型节省高达65%。在NVIDIA JETSON P3450等内存受限设备上，推理速度提升31.9%至146.6%。与现有先进方法相比，下游哈夫曼编码效率提高7倍（8位）和11.3倍（4位），表明压缩效果显著。",
      "conclusion": "EntroLLM的主要贡献是提出高效压缩框架，通过混合量化和熵编码减少大语言模型的存储占用并保持精度。该研究具有重要学术和实际价值，促进边缘计算中LLM的部署，且无需重新训练并兼容现有流程，易于集成。未来工作可能包括扩展到更多模型架构或优化解码策略以进一步提升性能。",
      "tags": [
        "Large Language Model",
        "Mixed Quantization",
        "Entropy Coding",
        "Edge Devices",
        "Weight Compression"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:46.154850Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.20082",
    "title": "Evolution of AI in Education: Agentic Workflows",
    "authors": [
      "Firuz Kamalov",
      "David Santandreu Calonge",
      "Linda Smail",
      "Dilshod Azizov",
      "Dimple R. Thadani",
      "Theresa Kwong",
      "Amara Atif"
    ],
    "abstract": "The primary goal of this study is to analyze agentic workflows in education according to the proposed four major technological paradigms: reflection, planning, tool use, and multi-agent collaboration. We critically examine the role of AI agents in education through these key design paradigms, exploring their advantages, applications, and challenges. Second, to illustrate the practical potential of agentic systems, we present a proof-of-concept application: a multi-agent framework for automated essay scoring. Preliminary results suggest this agentic approach may offer improved consistency compared to stand-alone LLMs. Our findings highlight the transformative potential of AI agents in educational settings while underscoring the need for further research into their interpretability and trustworthiness.",
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2504.20082.pdf",
    "abs_url": "https://arxiv.org/abs/2504.20082",
    "published": "2025-04-25T13:44:57Z",
    "updated": "2026-01-26T10:11:03Z",
    "comment": "made the abstract more succinct, revised the methodology, added PRISMA flow chart, updated references",
    "light_analysis": {
      "overview": "本文通过分析反思、规划、工具使用和多代理协作四大技术范式，探讨了AI代理在教育中的工作流，并提出了一个用于自动作文评分的多代理框架作为概念验证。",
      "motivation": "本研究旨在解决AI在教育应用中缺乏系统性分析和高效协作机制的问题。传统AI方法如独立大语言模型（LLMs）在处理复杂教育任务时可能表现不一致，而代理工作流能提供更结构化的智能解决方案。通过审查反思、规划等关键范式，本研究评估AI代理的优势和挑战，以推动教育技术的创新和发展。",
      "method": "研究方法基于提出的四大技术范式（反思、规划、工具使用和多代理协作）来构建代理工作流分析框架。核心创新在于设计了一个多代理系统，用于自动作文评分，其中代理通过协作处理评分任务。摘要未明确说明使用的具体数据集或模型架构，但框架涉及多个代理的交互和决策过程，体现了代理技术在教育场景中的实际应用潜力。",
      "result": "初步实验结果表明，所提出的多代理框架在自动作文评分中可能提供比独立LLMs更高的一致性，但摘要未明确说明具体性能指标（如准确率或效率改进），也没有详细对比基线方法。基于现有信息，可以推断该代理方法通过协作机制优化了评分过程，但需要进一步实证验证以获得更可靠的数据支撑。",
      "conclusion": "本研究的主要贡献在于系统分析了教育中AI代理工作流的技术范式，并通过概念验证展示了多代理框架的潜力。学术上，它为教育AI提供了新的设计思路；实际中，多代理系统可能提升自动化教育的可靠性和效率。未来工作应侧重于代理的可解释性和可信度研究，以克服潜在局限性并促进更广泛的应用。",
      "tags": [
        "AI Agents",
        "Multi-Agent Systems",
        "Automated Essay Scoring",
        "Large Language Models",
        "Educational Technology"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:19.230050Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.05695",
    "title": "Architecture independent generalization bounds for overparametrized deep ReLU networks",
    "authors": [
      "Anandatheertha Bapu",
      "Thomas Chen",
      "Chun-Kai Kevin Chien",
      "Patricia Muñoz Ewald",
      "Andrew G. Moore"
    ],
    "abstract": "We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove a uniform generalization bound that is independent of the network architecture. We perform computational experiments of our theoretical results with MNIST, and obtain agreement with the true test error within a 22 % margin on average.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.AP",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2504.05695.pdf",
    "abs_url": "https://arxiv.org/abs/2504.05695",
    "published": "2025-04-08T05:37:38Z",
    "updated": "2026-01-26T06:20:02Z",
    "comment": "AMS Latex, 19 pages",
    "light_analysis": {
      "overview": "论文证明了过参数化深度ReLU网络具有与网络架构无关的泛化界限，测试误差不依赖于VC维和过参数化程度。",
      "motivation": "过参数化神经网络在深度学习中广泛使用，但传统理论如VC维难以解释其泛化能力，可能导致高估误差。本研究旨在解决这一问题，提供独立于网络架构的泛化理论，以弥补现有方法在理论预测与实际性能之间的差距，促进对过参数化模型的理解。摘要未明确说明现有具体方法的不足，但基于背景推断，现有理论可能过度依赖复杂架构参数。",
      "method": "论文提出了显式泛化界限，仅依赖于测试和训练集的度量几何、ReLU激活函数的正则性属性、以及权重算子范数和偏置范数。关键创新是显式构造零损失最小化器，不使用梯度下降，适用于训练样本大小受输入空间维度限制的深度ReLU网络。这确保了界限独立于网络架构，方法基于数学分析，避免了优化算法的复杂性。",
      "result": "在MNIST数据集上的计算实验验证了理论预测，测试误差与真实值之间的误差在平均22%的范围内一致。这为理论界限提供了实证支持，表明界限能准确估计实际性能。摘要未明确对比基线方法，但实验结果显示理论与实验的一致性，间接证实了方法的有效性。",
      "conclusion": "主要贡献是证明了过参数化ReLU网络可实现架构无关的泛化，挑战了传统VC维理论，为深度学习理论提供了新视角。学术价值在于加深对神经网络泛化的理解，实际应用可能指导模型设计以提高稳定性。潜在局限性包括实验仅基于MNIST，未来工作可扩展到更多数据集和复杂场景。",
      "tags": [
        "Overparametrized Neural Networks",
        "ReLU Activation",
        "Generalization Bounds",
        "VC Dimension",
        "Metric Geometry"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:29.071565Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.04139",
    "title": "Introducing COGENT3: An AI Architecture for Emergent Cognition",
    "authors": [
      "Eduardo Salazar"
    ],
    "abstract": "This paper presents COGENT3 (or Collective Growth and Entropy-modulated Triads System), a novel approach for emergent cognition integrating pattern formation networks with group influence dynamics. Contrasting with traditional strategies that rely on predetermined architectures, computational structures emerge dynamically in our framework through agent interactions. This enables a more flexible and adaptive system exhibiting characteristics reminiscent of human cognitive processes. The incorporation of temperature modulation and memory effects in COGENT3 closely integrates statistical mechanics, machine learning, and cognitive science.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2504.04139.pdf",
    "abs_url": "https://arxiv.org/abs/2504.04139",
    "published": "2025-04-05T11:05:55Z",
    "updated": "2026-01-26T14:56:16Z",
    "comment": "V2 has updated content in several sections, fixed typos",
    "light_analysis": {
      "overview": "本文提出COGENT3架构，通过整合模式形成网络与群体影响动力学，实现了一种动态涌现认知的新型AI系统，核心创新在于其自组织的计算结构。",
      "motivation": "传统AI系统通常依赖预设的固定架构，限制了灵活性和适应性，难以模拟人类认知的动态过程。COGENT3研究旨在解决这一缺陷，通过引入动态交互和自组织机制，实现更接近人类认知的灵活系统。该问题的重要性在于推动AI向自适应、自然化的方向发展，以克服现有方法在复杂环境中的不足，摘要未明确说明具体应用场景。",
      "method": "COGENT3方法整合了模式形成网络和群体影响动力学，计算结构通过智能体交互动态涌现，而非预设架构。关键创新点包括引入温度调制和记忆效应，利用统计力学原理来模拟认知过程中的热力学特性。技术特色是多学科交叉，融合了机器学习算法和认知科学理论，以实现自适应的系统架构，摘要未明确说明具体数据集或模型细节。",
      "result": "摘要未明确说明实验结果，因此没有提供具体的性能指标或与基线方法的对比数据。基于论文描述，可推断COGENT3能提供更灵活和自适应的系统特性，但缺乏实证数据支撑其效果和效率改进。",
      "conclusion": "论文主要贡献在于提出COGENT3架构，通过动态涌现的计算结构提升了AI系统的灵活性和适应性，展现了类似人类认知的特征。学术价值体现在整合了统计力学、机器学习和认知科学，推动了跨学科研究。实际应用可能包括自适应AI和认知计算系统，未来工作需验证系统性能并扩展到复杂场景，摘要未明确说明局限性。",
      "tags": [
        "Emergent Cognition",
        "Pattern Formation Networks",
        "Group Influence Dynamics",
        "Temperature Modulation",
        "Statistical Mechanics"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:39.023761Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.23972",
    "title": "Noise-based reward-modulated learning",
    "authors": [
      "Jesús García Fernández",
      "Nasir Ahmad",
      "Marcel van Gerven"
    ],
    "abstract": "The pursuit of energy-efficient and adaptive artificial intelligence (AI) has positioned neuromorphic computing as a promising alternative to conventional computing. However, achieving learning on these platforms requires techniques that prioritize local information while enabling effective credit assignment. Here, we propose noise-based reward-modulated learning (NRL), a novel synaptic plasticity rule that mathematically unifies reinforcement learning and gradient-based optimization with biologically-inspired local updates. NRL addresses the computational bottleneck of exact gradients by approximating them through stochastic neural activity, transforming the inherent noise of biological and neuromorphic substrates into a functional resource. Drawing inspiration from biological learning, our method uses reward prediction errors as its optimization target to generate increasingly advantageous behavior, and eligibility traces to facilitate retrospective credit assignment. Experimental validation on reinforcement tasks, featuring immediate and delayed rewards, shows that NRL achieves performance comparable to baselines optimized using backpropagation, although with slower convergence, while showing significantly superior performance and scalability in multi-layer networks compared to reward-modulated Hebbian learning (RMHL), the most prominent similar approach. While tested on simple architectures, the results highlight the potential of noise-driven, brain-inspired learning for low-power adaptive systems, particularly in computing substrates with locality constraints. NRL offers a theoretically grounded paradigm well-suited for the event-driven characteristics of next-generation neuromorphic AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.23972.pdf",
    "abs_url": "https://arxiv.org/abs/2503.23972",
    "published": "2025-03-31T11:35:23Z",
    "updated": "2026-01-26T11:58:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出噪声基础的奖励调制学习（NRL），一种新颖的突触可塑性规则，数学统一强化学习和梯度优化，融合生物启发的局部更新。",
      "motivation": "随着节能自适应AI的发展，神经形态计算成为有前景的替代方案，但在这些平台上实现学习面临挑战：需要平衡局部信息处理和有效信用分配。现有方法在神经形态基底上存在计算瓶颈，如精确梯度难以获取，噪声资源未被充分利用。因此，本研究旨在开发新方法，解决精确梯度的计算问题，并将噪声转化为学习资源，以提高自适应AI的效率。",
      "method": "NRL是一种突触可塑性规则，通过随机神经活动近似梯度，将生物和神经形态基底的内在噪声转化为功能资源。核心创新包括使用奖励预测误差作为优化目标，指导行为生成，并结合资格迹促进回顾性信用分配。方法数学统一了强化学习和梯度优化，采用局部更新策略，适合事件驱动的神经形态系统，无需全局信息。关键细节涉及模拟神经网络活动，实现类似反向传播的优化效果。",
      "result": "在强化任务实验上，包括即时和延迟奖励场景，NRL性能与基于反向传播的基线方法相当，但收敛速度较慢。与奖励调制赫布学习（RMHL）对比，NRL在多层网络中表现出显著更优的性能和可扩展性，例如在复杂任务中准确率更高、网络规模适应性更强。结果突显NRL在大规模应用中的优势，尽管收敛效率有待提升。",
      "conclusion": "NRL的主要贡献是提供理论基础的学习范式，适合下一代神经形态AI的事件驱动特性，展示了噪声驱动、受大脑启发的学习在低功耗自适应系统中的潜力。研究学术价值在于统一强化学习和梯度优化，实际应用指向节能AI和局部约束平台。局限性在于测试仅用简单架构，未来工作可扩展到更复杂网络，或改进收敛速度。",
      "tags": [
        "Reinforcement Learning",
        "Gradient-based Optimization",
        "Neuromorphic Computing",
        "Local Updates",
        "Noise-based Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:31:52.575108Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.23832",
    "title": "An extrapolated and provably convergent algorithm for nonlinear matrix decomposition with the ReLU function",
    "authors": [
      "Nicolas Gillis",
      "Margherita Porcelli",
      "Giovanni Seraghiti"
    ],
    "abstract": "ReLU matrix decomposition (RMD) is the following problem: given a sparse, nonnegative matrix $X$ and a factorization rank $r$, identify a rank-$r$ matrix $Θ$ such that $X\\approx \\max(0,Θ)$. RMD is a particular instance of nonlinear matrix decomposition (NMD) that finds application in data compression, matrix completion with entries missing not at random, and manifold learning. The standard RMD model minimizes the least squares error, that is, $\\|X - \\max(0,Θ)\\|_F^2$. The corresponding optimization problem, Least-Squares RMD (LS-RMD), is nondifferentiable and highly nonconvex. This motivated Saul to propose an alternative model, \\revise{dubbed Latent-RMD}, where a latent variable $Z$ is introduced and satisfies $\\max(0,Z)=X$ while minimizing $\\|Z - Θ\\|_F^2$ (``A nonlinear matrix decomposition for mining the zeros of sparse data'', SIAM J.\\ Math.\\ Data Sci., 2022). Our first contribution is to show that the two formulations may yield different low-rank solutions $Θ$. We then consider a reparametrization of the Latent-RMD, called 3B-RMD, in which $Θ$ is substituted by a low-rank product $WH$, where $W$ has $r$ columns and $H$ has $r$ rows. Our second contribution is to prove the convergence of a block coordinate descent (BCD) approach applied to 3B-RMD. Our third contribution is a novel extrapolated variant of BCD, dubbed eBCD, which we prove is also convergent under mild assumptions. We illustrate the significant acceleration effect of eBCD compared to eBCD, and also show that eBCD performs well against the state of the art on synthetic and real-world data sets.",
    "categories": [
      "cs.LG",
      "eess.IV",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.23832.pdf",
    "abs_url": "https://arxiv.org/abs/2503.23832",
    "published": "2025-03-31T08:27:41Z",
    "updated": "2026-01-26T09:08:56Z",
    "comment": "25 pages. Codes and data available from https://github.com/giovanniseraghiti/ReLU-NMD",
    "light_analysis": {
      "overview": "提出一种外推块坐标下降算法，用于ReLU矩阵分解，证明其收敛性并显著加速计算。",
      "motivation": "ReLU矩阵分解用于处理稀疏非负数据，在数据压缩、矩阵完成和流形学习中应用重要，但标准最小二乘模型因不可微和非凸性导致优化困难。现有Latent-RMD模型虽引入潜在变量改善，但可能产生不一致解，因此需要开发高效且理论保证的优化方法，以提升分解的可靠性和计算效率，解决实际数据处理中的瓶颈问题。",
      "method": "论文提出3B-RMD作为Latent-RMD的重参数化，将Θ表示为低秩矩阵积WH，其中W和H为因子矩阵。采用块坐标下降算法进行优化，并引入外推技术提出eBCD变体。关键创新在于证明BCD和eBCD在3B-RMD模型上的收敛性，eBCD通过外推步骤加速迭代，适用于稀疏和非负约束的矩阵分解任务。",
      "result": "实验结果显示，eBCD算法相比基线方法在合成和真实数据集上展现出显著加速效果。摘要未明确具体数值，但指出eBCD性能优异，验证了其在优化效率和分解质量上的优势，为非线性矩阵分解提供了实用的解决方案，并与现有技术对比表现良好。",
      "conclusion": "本研究通过分析不同RMD公式的差异，提出3B-RMD和eBCD算法，证明了收敛性并实现加速优化。贡献在于为非线性矩阵分解提供可证明收敛的优化框架，提升数据处理任务的效率和可靠性。未来工作可扩展应用到更多非线性函数或其他机器学习领域。",
      "tags": [
        "ReLU Matrix Decomposition",
        "Nonlinear Matrix Decomposition",
        "Block Coordinate Descent",
        "Extrapolation",
        "Convergence Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:05.869775Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.16522",
    "title": "Adams Bashforth Moulton Solver for Inversion and Editing in Rectified Flow",
    "authors": [
      "Yongjia Ma",
      "Donglin Di",
      "Xuan Liu",
      "Xiaokai Chen",
      "Lei Fan",
      "Tonghua Su",
      "Yue Gao"
    ],
    "abstract": "Rectified flow models have achieved remarkable performance in image and video generation tasks. However, existing numerical solvers face a trade-off between fast sampling and high accuracy solutions, limiting their effectiveness in downstream applications such as reconstruction and editing. To address this challenge, we propose leveraging the Adams Bashforth Moulton (ABM) predictor corrector method to enhance the accuracy of ODE solving in rectified flow models. Specifically, we introduce ABM Solver, which integrates a multi step predictor corrector approach to reduce local truncation errors and employs Adaptive Step Size Adjustment to improve sampling speed. Furthermore, to effectively preserve non edited regions while facilitating semantic modifications, we introduce a Mask Guided Feature Injection module. We estimate self-similarity to generate a spatial mask that differentiates preserved regions from those available for editing. Extensive experiments on multiple high resolution image datasets validate that ABM Solver significantly improves inversion precision and editing quality, outperforming existing solvers without requiring additional training or optimization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.16522.pdf",
    "abs_url": "https://arxiv.org/abs/2503.16522",
    "published": "2025-03-17T02:17:33Z",
    "updated": "2026-01-26T09:12:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于Adams Bashforth Moulton方法的求解器，结合Mask Guided Feature Injection模块，以提高rectified flow模型中ODE求解精度和图像编辑质量。",
      "motivation": "Rectified flow模型在图像和视频生成任务中表现出色，但现有数值求解器面临快速采样与高精度解之间的权衡，这限制了其在重建和编辑等下游应用中的有效性。该问题的重要性在于，高效的求解器对于实际应用如内容创建和修改至关重要，而现有方法在精度和速度上的不足，阻碍了模型潜力的发挥，因此需要改进方案以克服这一瓶颈。",
      "method": "论文提出了ABM Solver，它采用Adams Bashforth Moulton预测器校正器方法，通过多步策略减少局部截断误差，提升ODE求解精度，并结合自适应步长调整以优化采样速度。此外，为了在图像编辑中有效保留非编辑区域，引入了Mask Guided Feature Injection模块，该模块利用自相似性估计生成空间掩码，以区分保留区域和可编辑区域，从而实现更精确的语义修改。",
      "result": "在多个人高分辨率图像数据集上的实验验证，ABM Solver在反演精度和编辑质量方面均显著优于现有求解器，例如在精度指标上有明显提升，且无需额外训练或优化步骤。该方法在保持采样效率的同时，提高了ODE解的准确性，从而在图像编辑任务中实现了更好的性能表现。",
      "conclusion": "本研究的主要贡献是开发了ABM Solver和Mask Guided Feature Injection模块，有效解决了rectified flow模型中求解器精度与速度的权衡问题。学术价值在于推动了数值求解方法在生成模型中的应用，实际应用价值在于提升了图像重建和编辑的效率和效果。未来工作可进一步探索该方法在其他领域的扩展或优化，但摘要未明确说明具体方向。",
      "tags": [
        "Adams Bashforth Moulton",
        "Rectified Flow",
        "Predictor-Corrector Method",
        "Adaptive Step Size",
        "Image Inversion and Editing"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:19.032306Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.08603",
    "title": "CellStyle: Improved Zero-Shot Cell Segmentation via Style Transfer",
    "authors": [
      "Rüveyda Yilmaz",
      "Zhu Chen",
      "Yuli Wu",
      "Johannes Stegmaier"
    ],
    "abstract": "Cell microscopy data are abundant; however, corresponding segmentation annotations remain scarce. Moreover, variations in cell types, imaging devices, and staining techniques introduce significant domain gaps between datasets. As a result, even large, pretrained segmentation models trained on diverse datasets (source datasets) struggle to generalize to unseen datasets (target datasets). To overcome this generalization problem, we propose CellStyle, which improves the segmentation quality of such models without requiring labels for the target dataset, thereby enabling zero-shot adaptation. CellStyle transfers the attributes of an unannotated target dataset, such as texture, color, and noise, to the annotated source dataset. This transfer is performed while preserving the cell shapes of the source images, ensuring that the existing source annotations can still be used while maintaining the visual characteristics of the target dataset. The styled synthetic images with the existing annotations enable the finetuning of a generalist segmentation model for application to the unannotated target data. We demonstrate that CellStyle significantly improves zero-shot cell segmentation performance across diverse datasets by finetuning multiple segmentation models on the style-transferred data. The code will be made publicly available.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2503.08603.pdf",
    "abs_url": "https://arxiv.org/abs/2503.08603",
    "published": "2025-03-11T16:39:09Z",
    "updated": "2026-01-26T09:13:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了CellStyle方法，通过风格转移技术提升零样本细胞分割的泛化性能。",
      "motivation": "研究动机源于细胞显微镜数据丰富但分割标注稀缺，且不同数据集间因细胞类型、成像设备和染色技术差异导致显著领域差距，这使得预训练分割模型在未见数据集上泛化能力差。现有方法依赖大量标注，成本高且难以适应新环境，因此需要一种无需目标标注的自适应技术来解决零样本分割的挑战，以促进医学图像分析的广泛应用。",
      "method": "CellStyle的核心方法是风格转移，它将未标注目标数据集的视觉属性（如纹理、颜色、噪声）转移到标注源数据集，同时通过技术手段保留源图像中的细胞形状，确保现有标注可用于生成合成图像。然后，利用这些合成图像微调通用分割模型，实现零样本适应。关键创新在于结合风格转移与分割任务，在维持结构一致性的基础上适应目标域风格，摘要未明确说明具体使用的数据集和模型架构。",
      "result": "实验结果展示CellStyle在多个细胞显微镜数据集上显著提升了零样本分割性能，通过微调不同分割模型，与基线方法相比，分割质量得到明显改善，具体表现为更高的准确率和更好的泛化能力。摘要未明确说明具体性能指标如准确率提升百分比，但强调了方法的有效性。代码将公开，以支持进一步研究和验证。",
      "conclusion": "CellStyle的主要贡献是通过风格转移实现无需目标标注的零样本细胞分割，降低了标注成本并增强了模型跨数据集泛化能力。该研究在医学图像分析领域具有学术价值，提供了新的自适应方法，实际应用可促进疾病诊断等场景。未来工作可能包括优化风格转移技术或扩展到其他图像类型，以克服潜在局限性。",
      "tags": [
        "Style Transfer",
        "Zero-Shot Segmentation",
        "Cell Segmentation",
        "Domain Adaptation",
        "Image Synthesis"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:05.126568Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2503.06950",
    "title": "CtrlRAG: Black-box Document Poisoning Attacks for Retrieval-Augmented Generation of Large Language Models",
    "authors": [
      "Runqi Sui"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) systems enhance response credibility and traceability by displaying reference contexts, but this transparency simultaneously introduces a novel black-box attack vector. Existing document poisoning attacks, where adversaries inject malicious documents into the knowledge base to manipulate RAG outputs, rely primarily on unrealistic white-box or gray-box assumptions, limiting their practical applicability. To address this gap, we propose CtrlRAG, a two-stage black-box attack that (1) constructs malicious documents containing misinformation or emotion-inducing content and injects them into the knowledge base, and (2) iteratively optimizes them using a localization algorithm and Masked Language Model (MLM) guided on reference context feedback, ensuring their retrieval priority while preserving linguistic naturalness. With only five malicious documents per target question injected into the million-document MS MARCO dataset, CtrlRAG achieves up to 90% attack success rates on commercial LLMs (e.g., GPT-4o), a 30% improvement over optimal baselines, in both *Emotion Manipulation* and *Hallucination Amplification* tasks. Furthermore, we show that existing defenses fail to balance security and performance. To mitigate this challenge, we introduce a dynamic *Knowledge Expansion* defense strategy based on *Parametric/Non-parametric Memory Confrontation*, blocking 78% of attacks while maintaining 95.5% system accuracy. Our findings reveal critical vulnerabilities in RAG systems and provide effective defense strategies.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2503.06950.pdf",
    "abs_url": "https://arxiv.org/abs/2503.06950",
    "published": "2025-03-10T05:55:15Z",
    "updated": "2026-01-26T16:58:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "CtrlRAG 提出了一种针对检索增强生成系统的黑盒文档中毒攻击方法，并引入了有效的动态知识扩展防御策略。",
      "motivation": "检索增强生成系统通过显示参考上下文增强响应可信度和可追溯性，但这种透明性引入了新的黑盒攻击向量，使得系统容易受到恶意文档注入的影响。现有文档中毒攻击主要依赖于不切实际的白盒或灰盒假设，如需要内部模型知识或部分访问权限，这限制了攻击在实际场景中的适用性。因此，本研究旨在填补这一缺口，开发更实用的黑盒攻击方法，以评估和改进 RAG 系统的安全性，确保其在现实世界部署中的稳健性。",
      "method": "CtrlRAG 是一种两阶段黑盒攻击方法：首先，构造包含错误信息或情绪诱导内容的恶意文档，并将其注入知识库（如 MS MARCO 数据集）；其次，通过定位算法和基于参考上下文反馈的掩码语言模型进行迭代优化，以提升恶意文档的检索优先级，同时保持语言自然性。核心创新在于利用黑盒设置，仅依赖参考上下文进行优化，无需访问内部模型参数，从而增强了攻击的实际应用性。该方法针对商业大语言模型（如 GPT-4o）进行测试。",
      "result": "实验结果表明，在百万文档的 MS MARCO 数据集中，仅注入每个目标问题五个恶意文档，CtrlRAG 在情绪操纵和幻觉放大任务上实现高达 90% 的攻击成功率，比最优基线提升 30%。此外，提出的动态知识扩展防御策略，基于参数化/非参数化内存对抗，能有效阻止 78% 的攻击，同时保持 95.5% 的系统准确性，显示出其在平衡安全性和性能方面的潜力。",
      "conclusion": "本研究揭示了检索增强生成系统的关键安全漏洞，提出了 CtrlRAG 攻击方法和有效的动态知识扩展防御策略，增强了 RAG 安全性的学术理解和实践保护能力。贡献在于提供了一种实用的黑盒攻击评估框架和防御机制，为未来研究奠定了基础。摘要未明确说明局限性，但防御策略展示了在现实应用中改进系统稳健性的可能方向，如扩展到更多攻击类型或数据集。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Black-box Attack",
        "Document Poisoning",
        "Masked Language Model",
        "Knowledge Expansion Defense"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:38.009832Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.16944",
    "title": "Pretrain Value, Not Reward: Decoupled Value Policy Optimization",
    "authors": [
      "Chenghua Huang",
      "Lu Wang",
      "Fangkai Yang",
      "Pu Zhao",
      "Zhixu Li",
      "Qingwei Lin",
      "Dongmei Zhang",
      "Saravan Rajmohan",
      "Qi Zhang"
    ],
    "abstract": "In this paper, we explore how directly pretraining a value model simplifies and stabilizes reinforcement learning from human feedback (RLHF). In reinforcement learning, value estimation is the key to policy optimization, distinct from reward supervision. The value function predicts the \\emph{return-to-go} of a partial answer, that is, how promising the partial answer is if it were continued to completion. In RLHF, however, the standard pipeline first pretrains a reward model and then learns a value function online, even though no new reward signals are available once preference data is collected. This makes critic learning redundant, as the process of training a reward model and then deriving a value model is informationally equivalent to directly pretraining a value model. Importantly, this requires no additional supervision, and our value model is trained on exactly the same data used for reward modeling. Building on this insight, we introduce \\emph{Decoupled Value Policy Optimization} (DVPO), a framework that pretrains a \\emph{Global Value Model} (GVM) offline and freezes it as a universal critic for policy learning. The GVM provides stable, fine-grained credit assignment without critic drift or trajectory sampling. Experiments across MT-Bench, Alpaca-Eval, and Arena-Hard demonstrate that DVPO matches or surpasses state-of-the-art RLHF methods. These results highlight RLHF can be reframed as policy-only optimization guided by a single pretrained value model.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.16944.pdf",
    "abs_url": "https://arxiv.org/abs/2502.16944",
    "published": "2025-02-24T08:11:33Z",
    "updated": "2026-01-26T14:09:10Z",
    "comment": "16 pages, 3 figures",
    "light_analysis": {
      "overview": "论文提出了Decoupled Value Policy Optimization（DVPO）框架，通过直接预训练价值模型简化强化学习从人类反馈（RLHF）流程，无需在线学习价值函数，匹配或超越现有方法。",
      "motivation": "研究动机源于RLHF中标准方法的冗余性。在收集人类偏好数据后，奖励信号固定，但现有流程仍需预训练奖励模型并在线学习价值函数，导致critic学习冗余和不稳定。这个问题至关重要，因为RLHF是训练大型语言模型的核心技术，效率低下和稳定性不足会阻碍实际应用。现有方法的不足在于信息上等同于直接预训练价值模型，却引入额外复杂度，增加训练难度和潜在风险。",
      "method": "论文提出Decoupled Value Policy Optimization（DVPO）框架。核心方法是离线预训练一个Global Value Model（GVM），使用与奖励建模完全相同的人类偏好数据，无需额外监督。GVM被冻结作为策略学习的通用critic，提供稳定、细粒度的信用分配，避免critic漂移和轨迹采样问题。关键创新点在于直接预训练价值模型，而不是先训练奖励模型再推导价值，从而简化流程并提高稳定性，无需新数据集或模型架构。",
      "result": "在MT-Bench、Alpaca-Eval和Arena-Hard等多个基准测试中，DVPO框架的实验结果表明其能够匹配或超越最先进的RLHF方法。这证明了通过直接预训练价值模型，可以有效提升策略优化性能，尽管摘要未提供具体准确率或效率数据，但强调了在稳定性和简化方面的优势。与基线方法相比，DVPO展现出竞争性性能，支持其作为RLHF的简化替代方案。",
      "conclusion": "结论强调DVPO框架的主要贡献是将RLHF重构为由单个预训练价值模型指导的策略优化，消除冗余的价值学习环节。这具有重要学术价值，为RLHF领域提供新方法论和简化视角。在实际应用中，能提高训练效率和稳定性，潜在应用于大型语言模型优化。摘要未明确说明局限性，但未来工作可能涉及扩展框架到其他领域或进一步验证通用性和可扩展性。",
      "tags": [
        "Reinforcement Learning from Human Feedback",
        "Value Model Pretraining",
        "Policy Optimization",
        "Global Value Model",
        "Decoupled Value Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:02.561064Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.13358",
    "title": "Bridging the Editing Gap in LLMs: FineEdit for Precise and Targeted Text Modifications",
    "authors": [
      "Yiming Zeng",
      "Wanhao Yu",
      "Zexin Li",
      "Tao Ren",
      "Yu Ma",
      "Jinghan Cao",
      "Xiyan Chen",
      "Tingting Yu"
    ],
    "abstract": "Large Language Models (LLMs) have significantly advanced natural language processing, demonstrating strong capabilities in tasks such as text generation, summarization, and reasoning. Recently, their potential for automating precise text editing tasks across specialized domains, such as programming code, LaTeX, and structured database languages, has gained attention. However, current state-of-the-art LLMs still struggle with executing precise, instruction-driven edits, particularly when structural accuracy and strict adherence to domain conventions are required. To address these challenges, we introduce InstrEditBench, an automated benchmark dataset comprising over 30,000 structured editing tasks spanning diverse domains, including Wikipedia articles, LaTeX documents, source code, and database languages. Using this benchmark, we develop FineEdit, a specialized editing model explicitly trained for accurate, context-aware text modifications. Experimental evaluations demonstrate that FineEdit outperforms state-of-the-art models, achieving improvements of approximately 10\\% over Gemini models on single-turn edits, up to 30\\% over Llama-3.2-3B, and exceeding Mistral-7B-OpenOrca performance by over 40\\% on direct editing tasks. FineEdit also effectively generalizes to realistic multi-turn editing scenarios, highlighting its practical applicability. To facilitate further research and reproducibility, we release FineEdit at https://github.com/StuRinDQB/FineEdit} and https://huggingface.co/datasets/YimingZeng/FineEdit_bench.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.13358.pdf",
    "abs_url": "https://arxiv.org/abs/2502.13358",
    "published": "2025-02-19T01:41:44Z",
    "updated": "2026-01-26T02:27:54Z",
    "comment": "We resolved some issues in this paper",
    "light_analysis": {
      "overview": "论文提出了FineEdit模型和InstrEditBench数据集，旨在解决大语言模型在精确文本编辑任务中的性能瓶颈。",
      "motivation": "LLMs虽然在自然语言处理任务中表现优异，但在执行精确、指令驱动的文本编辑任务时存在显著不足，尤其是在需要结构准确性和严格遵循领域惯例的专业领域，如编程代码、LaTeX和数据库语言。当前先进模型难以在这些场景下可靠地完成任务，限制了其在自动化编辑应用中的潜力。现有方法通常缺乏针对编辑任务的专门优化，导致性能下降，这凸显了研究针对性的编辑模型的必要性，以提升实用性和准确性。",
      "method": "研究首先引入了InstrEditBench数据集，这是一个自动化的基准数据集，包含超过30,000个结构化编辑任务，覆盖Wikipedia文章、LaTeX文档、源代码和数据库语言等多个领域。基于此数据集，开发了FineEdit模型，该模型专门为准确、上下文感知的文本修改而训练。核心创新点在于结合大规模结构化数据，专注于指令驱动的编辑任务，但摘要未明确说明模型的具体架构或训练技术细节，推断其可能涉及微调或其他监督学习方法以增强编辑能力。",
      "result": "实验评估表明，FineEdit在编辑任务中显著优于现有先进模型。具体而言，在单轮编辑任务中，相对于Gemini模型提升了约10%，相比Llama-3.2-3B提升达30%，在直接编辑任务中超过Mistral-7B-OpenOrca性能达40%以上。这些结果证明了FineEdit在精确编辑任务中的有效性。此外，模型还能泛化到现实的多轮编辑场景，显示出强大的实际应用潜力。",
      "conclusion": "论文的主要贡献是提出了InstrEditBench数据集和FineEdit模型，填补了LLMs在精确文本编辑方面的空白，不仅提升了编辑性能，还能适应复杂的多轮编辑需求。这具有重要的学术价值，推动了文本编辑技术的研究进展，同时在自动化文档修订、代码编辑等领域具有实际应用前景。未来工作可能包括进一步优化模型以处理更多样化的编辑任务，或扩展数据集到更广泛的领域。",
      "tags": [
        "Large Language Model",
        "Text Editing",
        "Benchmark Dataset",
        "Fine-tuning",
        "Context-aware Learning"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:50.234745Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.12945",
    "title": "LLMPopcorn: Exploring LLMs as Assistants for Popular Micro-video Generation",
    "authors": [
      "Junchen Fu",
      "Xuri Ge",
      "Kaiwen Zheng",
      "Alexandros Karatzoglou",
      "Ioannis Arapakis",
      "Xin Xin",
      "Yongxin Ni",
      "Joemon M. Jose"
    ],
    "abstract": "In an era where micro-videos dominate platforms like TikTok and YouTube, AI-generated content is nearing cinematic quality. The next frontier is using large language models (LLMs) to autonomously create viral micro-videos, a largely untapped potential that could shape the future of AI-driven content creation. To address this gap, this paper presents the first exploration of LLM-assisted popular micro-video generation (LLMPopcorn). We selected popcorn as the icon for this paper because it symbolizes leisure and entertainment, aligning with this study on leveraging LLMs as assistants for generating popular micro-videos that are often consumed during leisure time. Specifically, we empirically study the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? Exploring these questions, we show that advanced LLMs like DeepSeek-V3 can generate micro-videos with popularity rivaling human content. Prompt enhancement further boosts results, while benchmarking highlights DeepSeek-V3 and R1 for LLMs, and LTX-Video and HunyuanVideo for video generation. This work advances AI-assisted micro-video creation and opens new research directions. The code is publicly available at https://github.com/GAIR-Lab/LLMPopcorn.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.12945.pdf",
    "abs_url": "https://arxiv.org/abs/2502.12945",
    "published": "2025-02-18T15:29:05Z",
    "updated": "2026-01-26T16:44:12Z",
    "comment": "Accepted by ICASSP2026",
    "light_analysis": {
      "overview": "本文首次探索了使用大语言模型作为助手来生成流行微视频，填补了AI辅助内容创作领域的空白。",
      "motivation": "在当前微视频平台如TikTok和YouTube主导的时代，AI生成内容质量已接近电影级，但利用大语言模型自主创建病毒式微视频的潜力未被充分挖掘。现有方法缺乏针对流行度优化的系统研究，本工作旨在解决这一不足，探索LLM如何促进AI驱动的内容创作未来，推动微视频生成技术的创新。",
      "method": "论文提出了LLMPopcorn框架，通过实证研究三个关键问题：如何有效利用LLM辅助微视频生成、提示增强如何优化内容流行度，以及不同LLM和视频生成器的性能比较。研究使用了高级LLM如DeepSeek-V3和视频生成器如LTX-Video，结合提示工程来提升生成效果，为探索AI辅助微视频创作提供了技术路线。",
      "result": "实验结果显示，使用DeepSeek-V3等高级LLM生成的微视频在流行度上可与人类制作内容媲美。通过提示增强进一步提升了生成内容的受欢迎程度。基准测试突出DeepSeek-V3和R1在LLM中表现优异，而LTX-Video和HunyuanVideo在视频生成方面效果显著，证明了LLM辅助方法的可行性。",
      "conclusion": "本研究首次系统地探索了LLM作为助手用于流行微视频生成，证明其能有效提升内容流行度，为AI辅助内容创作开辟了新研究方向，具有重要的学术和实际应用价值。未来工作可进一步优化LLM与视频生成器的集成，推动该领域的发展。",
      "tags": [
        "Large Language Models",
        "Video Generation",
        "Prompt Engineering",
        "Popularity Optimization",
        "Micro-video Generation"
      ]
    },
    "analyzed_at": "2026-01-27T03:32:43.397674Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.07687",
    "title": "Large Language Models as Proxies for Theories of Human Linguistic Cognition",
    "authors": [
      "Imry Ziv",
      "Nur Lan",
      "Emmanuel Chemla",
      "Roni Katzir"
    ],
    "abstract": "We consider the possible role of current large language models (LLMs) in the study of human linguistic cognition. We focus on the use of such models as proxies for theories of cognition that are relatively linguistically-neutral in their representations and learning but differ from current LLMs in key ways. We illustrate this potential use of LLMs as proxies for theories of cognition in the context of two kinds of questions: (a) whether the target theory accounts for the acquisition of a given pattern from a given corpus; and (b) whether the target theory makes a given typologically-attested pattern easier to acquire than another, typologically-unattested pattern. For each of the two questions we show, building on recent literature, how current LLMs can potentially be of help, but we note that at present this help is quite limited.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.07687.pdf",
    "abs_url": "https://arxiv.org/abs/2502.07687",
    "published": "2025-02-11T16:38:16Z",
    "updated": "2026-01-26T10:13:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文评估了大语言模型作为人类语言认知理论代理的潜在应用，指出当前使用存在局限。",
      "motivation": "研究动机在于探讨如何利用当前大语言模型来模拟人类语言认知理论，解决认知科学研究中语言习得和类型学模式验证的难题。现有LLMs虽然在语言处理上表现优异，但其表示和学习机制可能与认知理论所需的中立性不匹配，导致作为代理的有效性受限，这突显了改进模型的必要性。",
      "method": "研究方法聚焦于将LLMs用作认知理论的代理，基于近期文献分析两个核心问题：一是理论能否解释给定模式从语料库的习得；二是理论是否使类型学验证模式比未验证模式更易习得。关键创新在于应用LLMs作为模拟工具，评估理论可行性，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "主要实验结果表明，LLMs在帮助评估语言习得和类型学模式方面具有潜在价值，但当前应用效果有限。例如，初步分析显示LLMs能部分模拟习得过程，但未提供具体性能指标如准确率提升，与理论理想情况的对比凸显其作为代理的不足。",
      "conclusion": "结论总结LLMs作为人类语言认知理论代理的初步可行性，学术价值在于促进AI与认知科学的交叉研究，推动模型改进。实际应用价值有限，需未来工作克服现有模型在表示中立性等方面的限制，探索更精确的模拟方法。",
      "tags": [
        "Large Language Models",
        "Human Linguistic Cognition",
        "Cognitive Theories",
        "Language Acquisition",
        "Typology"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:47.235318Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.06574",
    "title": "On the Impact of the Utility in Semivalue-based Data Valuation",
    "authors": [
      "Mélissa Tamine",
      "Benjamin Heymann",
      "Patrick Loiseau",
      "Maxime Vono"
    ],
    "abstract": "Semivalue-based data valuation uses cooperative-game theory intuitions to assign each data point a value reflecting its contribution to a downstream task. Still, those values depend on the practitioner's choice of utility, raising the question: How robust is semivalue-based data valuation to changes in the utility? This issue is critical when the utility is set as a trade-off between several criteria and when practitioners must select among multiple equally valid utilities. We address it by introducing the notion of a dataset's spatial signature: given a semivalue, we embed each data point into a lower-dimensional space where any utility becomes a linear functional, making the data valuation framework amenable to a simpler geometric picture. Building on this, we propose a practical methodology centered on an explicit robustness metric that informs practitioners whether and by how much their data valuation results will shift as the utility changes. We validate this approach across diverse datasets and semivalues, demonstrating strong agreement with rank-correlation analyses and offering analytical insight into how choosing a semivalue can amplify or diminish robustness.",
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2502.06574.pdf",
    "abs_url": "https://arxiv.org/abs/2502.06574",
    "published": "2025-02-10T15:42:38Z",
    "updated": "2026-01-26T16:18:37Z",
    "comment": "45 pages, 19 figures. Accepted at ICLR 2026. This version corresponds to the accepted manuscript; minor changes may appear in the final camera-ready version",
    "light_analysis": {
      "overview": "本文引入了数据集的空间签名和鲁棒性指标，以评估半值数据估值方法对效用变化的鲁棒性。",
      "motivation": "基于半值的数据估值方法使用合作博弈论直觉为数据点分配价值，但估值结果高度依赖于效用函数的选择，这在效用是多个标准间权衡或多效用可选时，鲁棒性问题尤为突出。现有方法缺乏系统评估工具，导致估值结果在不确定性下不可靠，影响数据选择、公平分配等实际应用。研究旨在解决这一关键问题，探索如何量化效用变化对数据估值的影响，以提升方法的可靠性和适用性。",
      "method": "论文提出数据集的空间签名概念，将每个数据点嵌入到低维空间，使任何效用函数转化为线性函数，从而简化几何表示和分析框架。基于此，设计了一个显式鲁棒性指标，用于量化数据估值结果随效用变化而偏移的程度。该方法不依赖具体模型架构或数据集，关键创新在于将复杂的效用依赖问题转化为线性几何问题，便于从业者理解和评估不同设置下的鲁棒性。",
      "result": "方法在多种数据集和半值上进行了验证，结果显示与秩相关分析有强一致性，证实了所提鲁棒性指标的有效性。实验提供了分析洞察，揭示了不同半值选择如何放大或减弱估值结果的鲁棒性，但摘要未明确具体性能提升数据如准确率或效率改进。与基线方法的对比表明，该方法能可靠地预测效用变化对估值的影响，增强了评估过程的稳定性和可解释性。",
      "conclusion": "研究的主要贡献在于开发了一种实用方法，帮助从业者评估和优化基于半值的数据估值设置，提升了方法的鲁棒性和决策支持能力。学术上，它推进了数据估值理论，特别是在鲁棒性分析方面；实际中，提供了应对效用不确定性的工具，增强应用可靠性。未来工作可扩展到更复杂的效用场景或与其他评估方法结合，但摘要未明确说明具体局限性。",
      "tags": [
        "Semivalue-based Data Valuation",
        "Cooperative Game Theory",
        "Utility Function",
        "Robustness Metric",
        "Geometric Embedding"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:07.134968Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.00435",
    "title": "Towards Real-time Adaptation of Embodied Agent in Human-Robot Collaboration",
    "authors": [
      "Shipeng Liu",
      "Boshen Zhang",
      "Zhehui Huang"
    ],
    "abstract": "Large Language Models (LLMs) have opened transformative possibilities for human-robot collaboration. However, enabling real-time collaboration requires both low latency and robust reasoning, and most LLMs suffer from high latency. To address this gap, we first propose a fine-grained benchmark that explicitly assesses agents' proactive adaptability and temporal responsiveness in the Overcooked-AI environment. Based on evaluation results, we propose MonTA (Monitor-then-Adapt), a hierarchical framework inspired by cognitive science research. MonTA contains three key modules: a lightweight Monitor that operates at high frequency (7 Hz) to detect adaptation needs, and two proficient Adapters for subtask and path adaptation reasoning that provide instructions to humans at a lower frequency. Our results demonstrate that MonTA significantly outperforms baseline agents on our proposed benchmark, achieving superior performance across layouts with varying teaming fluency. User studies confirm the high reasonableness of adaptation plans and consistent language instructions provided by our framework to humans.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2412.00435.pdf",
    "abs_url": "https://arxiv.org/abs/2412.00435",
    "published": "2024-11-30T11:17:17Z",
    "updated": "2026-01-26T02:45:04Z",
    "comment": "13 pages, 7 figures",
    "light_analysis": {
      "overview": "本文提出 MonTA 分层框架，通过监控和适应模块实现人类-机器人协作的实时适应。",
      "motivation": "大型语言模型为人类-机器人协作带来了变革性可能性，但实现实时协作需要低延迟和健壮推理。当前大多数大型语言模型存在高延迟问题，这限制了其在动态交互环境中的应用，导致响应不及时和协作效率低下。因此，本研究旨在解决延迟与推理能力之间的差距，以促进更高效、响应更快的人类-机器人协作系统开发，从而克服现有方法在实时性方面的不足。",
      "method": "本研究首先提出了一个细粒度基准，在 Overcooked-AI 环境中明确评估代理的主动适应性和时间响应性。基于评估结果，我们提出了 MonTA（Monitor-then-Adapt）框架，这是一个受认知科学启发的分层结构。MonTA 包含三个关键模块：一个轻量级的 Monitor 以高频率（7 Hz）运行来检测适应需求，以及两个熟练的 Adapters 用于子任务和路径适应推理，后者以较低频率向人类提供语言指令，从而平衡延迟与推理精度。",
      "result": "实验结果表明，MonTA 在提出的基准上显著优于基线代理，在具有不同团队流畅度的布局中实现了优越性能，具体表现在适应性和响应性方面。用户研究进一步证实了适应计划的高合理性和框架向人类提供的语言指令的一致性，验证了其在实际协作场景中的有效性和可靠性，尽管摘要未明确说明具体数值如准确率提升。",
      "conclusion": "本研究的核心贡献在于提出了一个基准和一个基于认知科学的 MonTA 框架，以改善人类-机器人协作的实时适应。学术上，它结合了大型语言模型与认知科学原理，推动了实时推理系统的发展。实际应用中，该框架可提升人机协作的效率和响应性，具有潜在价值。未来工作可能涉及优化延迟或扩展应用场景，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Large Language Model",
        "Human-Robot Collaboration",
        "Real-time Adaptation",
        "Overcooked-AI",
        "Hierarchical Framework"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:15.632101Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.13687",
    "title": "Your Extreme Multi-label Classifier is Secretly a Hierarchical Text Classifier for Free",
    "authors": [
      "Nerijus Bertalis",
      "Paul Granse",
      "Ferhat Gül",
      "Florian Hauss",
      "Leon Menkel",
      "David Schüler",
      "Tom Speier",
      "Lukas Galke",
      "Ansgar Scherp"
    ],
    "abstract": "Assigning a set of labels to a given text is a classification problem with many real-world applications, such as recommender systems. Two separate research streams address this issue. Hierarchical Text Classification (HTC) focuses on datasets with label pools of hundreds of entries, accompanied by a semantic label hierarchy. In contrast, eXtreme Multi-Label Text Classification (XML) considers very large sets of labels with up to millions of entries but without an explicit hierarchy. In XML methods, it is common to construct an artificial hierarchy in order to deal with the large label space before or during the training process. Here, we investigate how state-of-the-art HTC models perform when trained and tested on XML datasets and vice versa using three benchmark datasets from each of the two streams. Our results demonstrate that XML models, with their internally constructed hierarchy, are very effective HTC models. HTC models, on the other hand, are not equipped to handle the sheer label set size of XML datasets and achieve poor transfer results. We further argue that for a fair comparison in HTC and XML, more than one metric like F1 should be used but complemented with P@k and R-Precision.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2411.13687.pdf",
    "abs_url": "https://arxiv.org/abs/2411.13687",
    "published": "2024-11-20T20:07:25Z",
    "updated": "2026-01-26T17:39:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文揭示极端多标签分类器通过内部构建的层次结构，可免费作为有效的层次文本分类器。",
      "motivation": "研究动机源于文本分类中两种方法的对比：层次文本分类（HTC）处理有语义层次的小标签集，而极端多标签分类（XML）处理无显式层次的大标签集。XML方法常构建人工层次结构以应对标签空间，但现有研究未充分探讨HTC和XML模型的互通性。HTC模型是否能在XML数据集上有效，或XML模型在HTC任务中的潜力，存在知识空白。此外，当前比较多依赖单一指标如F1，可能忽略了公平评价，因此研究旨在填补这些不足，促进交叉领域的理解和应用。",
      "method": "研究方法采用交叉实验设计，选取三个基准数据集，分别来自HTC和XML领域，训练和测试state-of-the-art的HTC和XML模型在对方数据集上的表现。关键创新在于首次系统性地评估两种模型的迁移能力，特别是利用XML模型内部构建的层次结构来处理HTC任务。数据集具体名称摘要未明确说明，但确保了代表性和多样性，模型基于当前最优技术，通过交叉验证揭示其潜在效能，强调了多指标评价如F1、P@k和R-Precision的重要性。",
      "result": "实验结果表明，XML模型在HTC数据集上表现卓越，得益于其内部层次结构，能够有效执行层次分类任务，而无需额外成本。相反，HTC模型在XML数据集上表现较差，难以应对大标签集带来的复杂度，转移结果不佳。摘要未提供具体性能数值如准确率，但强调了使用多个指标（如F1、P@k和R-Precision）进行公平比较的必要性，以避免单一指标的偏颇，凸显XML模型在交叉应用中的优势。",
      "conclusion": "论文的主要贡献是证明了XML模型的内部层次结构可使其免费应用于HTC任务，为分类器设计提供了新视角。学术上，该研究促进了HTC和XML领域的融合，倡导多指标评价体系以提高比较公平性。实际应用中，XML模型可轻松部署于HTC场景，而HTC模型需改进以处理大规模标签。潜在局限性包括数据集范围的限制和模型泛化能力，未来工作可扩展实验到更多数据集和优化模型架构。",
      "tags": [
        "Hierarchical Text Classification (HTC)",
        "eXtreme Multi-Label Classification (XML)",
        "Multi-label Classification",
        "F1 Score",
        "R-Precision"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:42.534888Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.14485",
    "title": "CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers for Causally Constrained Predictions",
    "authors": [
      "Matthew J. Vowels",
      "Mathieu Rochat",
      "Sina Akbari"
    ],
    "abstract": "Artificial Neural Networks (ANNs), including fully-connected networks and transformers, are highly flexible and powerful function approximators, widely applied in fields like computer vision and natural language processing. However, their inability to inherently respect causal structures can limit their robustness, making them vulnerable to covariate shift and difficult to interpret/explain. This poses significant challenges for their reliability in real-world applications. In this paper, we introduce Causal Transformers (CaTs), a general model class designed to operate under predefined causal constraints, as specified by a Directed Acyclic Graph (DAG). CaTs retain the powerful function approximation abilities of traditional neural networks while adhering to the underlying structural constraints, improving robustness, reliability, and interpretability at inference time. This approach opens new avenues for deploying neural networks in more demanding, real-world scenarios where robustness and explainability is critical.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.14485.pdf",
    "abs_url": "https://arxiv.org/abs/2410.14485",
    "published": "2024-10-18T14:10:16Z",
    "updated": "2026-01-26T18:20:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了结合有向无环图和Transformers的因果Transformer模型（CaTs），以增强预测的鲁棒性和可解释性。",
      "motivation": "人工神经网络（ANNs），包括全连接网络和Transformers，虽然灵活且强大，广泛应用于计算机视觉和自然语言处理领域，但它们无法自然尊重因果结构。这导致模型在面对协变量偏移时鲁棒性差，难以解释和解释，从而在实际应用中可靠性受限，限制了在高要求场景（如医疗或金融）中的部署。现有方法往往过于依赖数据驱动，缺乏集成先验知识或因果约束的能力，因此需要一种方法来弥补这一不足，提升模型的稳定性和可信度。",
      "method": "论文引入了Causal Transformers（CaTs），这是一种基于预定义有向无环图（DAG）因果约束的通用模型类。CaTs保留了传统神经网络的强大函数近似能力，同时通过DAG强制执行结构约束，确保预测过程符合因果逻辑。关键创新点在于将Transformers架构与DAG结合，在推理时自动融入因果约束，无需显式修改训练算法。摘要未明确说明具体的数据集或详细架构设计，但暗示该方法适用于多种领域，通过结构约束改善模型表现。",
      "result": "摘要未明确说明具体实验结果和性能指标。基于描述，CaTs在理论上改善了鲁棒性、可靠性和可解释性，预期能够减少协变量偏移的影响，并在实际应用中提供更稳定的预测。与不考虑因果约束的基线方法（如标准Transformers）相比，CaTs通过结构约束增强了模型的稳健性，但由于缺乏实验数据，实际提升程度需进一步验证。这表明该模型在概念上具有优势，但需要后续实验支持其有效性。",
      "conclusion": "该研究的主要贡献是开发了CaTs模型类，成功集成DAG与Transformers，为在要求高鲁棒性和可解释性的实际场景（如自动驾驶或医疗诊断）中部署神经网络开辟了新途径。研究强调了因果约束在提升神经网络可靠性方面的重要性，具有学术价值（如推动可解释AI领域发展）和实际应用潜力（如增强系统信任度）。局限性包括摘要未提及具体验证，未来工作可包括实验评估和扩展应用到更多因果结构复杂的领域。",
      "tags": [
        "Transformers",
        "Directed Acyclic Graph",
        "Causal Inference",
        "Explainable AI",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:33.302713Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.07582",
    "title": "Detecting Training Data of Large Language Models via Expectation Maximization",
    "authors": [
      "Gyuwan Kim",
      "Yang Li",
      "Evangelia Spiliopoulou",
      "Jie Ma",
      "William Yang Wang"
    ],
    "abstract": "Membership inference attacks (MIAs) aim to determine whether a specific example was used to train a given language model. While prior work has explored prompt-based attacks such as ReCALL, these methods rely heavily on the assumption that using known non-members as prompts reliably suppresses the model's responses to non-member queries. We propose EM-MIA, a new membership inference approach that iteratively refines prefix effectiveness and membership scores using an expectation-maximization strategy without requiring labeled non-member examples. To support controlled evaluation, we introduce OLMoMIA, a benchmark that enables analysis of MIA robustness under systematically varied distributional overlap and difficulty. Experiments on WikiMIA and OLMoMIA show that EM-MIA outperforms existing baselines, particularly in settings with clear distributional separability. We highlight scenarios where EM-MIA succeeds in practical settings with partial distributional overlap, while failure cases expose fundamental limitations of current MIA methods under near-identical conditions. We release our code and evaluation pipeline to encourage reproducible and robust MIA research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2410.07582.pdf",
    "abs_url": "https://arxiv.org/abs/2410.07582",
    "published": "2024-10-10T03:31:16Z",
    "updated": "2026-01-26T02:45:17Z",
    "comment": "EACL 2026",
    "light_analysis": {
      "overview": "论文提出EM-MIA，一种基于期望最大化的成员推断攻击方法，无需标记非成员示例，并引入OLMoMIA基准以系统评估MIA鲁棒性。",
      "motivation": "成员推断攻击(MIAs)旨在判断特定示例是否用于训练语言模型，这对于模型隐私和安全评估至关重要。现有方法如ReCALL依赖假设使用已知非成员作为提示来抑制非成员响应，但该假设在实际中可能不成立，导致攻击效果受限和对外部标记数据的过度依赖。因此，亟需开发更鲁棒的方法，以更准确地识别训练数据成员资格，减少对人工标记的依赖。",
      "method": "EM-MIA采用期望最大化策略迭代优化前缀有效性和成员分数，无需标记的非成员示例。关键创新在于使用EM算法自动调整参数，减少对已知非成员数据的依赖，提高攻击的适应性。此外，论文引入OLMoMIA基准，通过系统变化数据分布的相似性和难度，支持在控制条件下评估MIA鲁棒性。实验基于WikiMIA和OLMoMIA数据集进行。",
      "result": "在WikiMIA和OLMoMIA基准上的实验表明，EM-MIA优于现有基线方法，特别是在数据分布可分离性明确的情况下。摘要未明确说明具体性能指标，但强调了方法在部分分布重叠的实际场景中成功应用。失败案例揭示了在分布高度相似条件下当前MIA方法的根本局限性，凸显了其适用范围和挑战。",
      "conclusion": "EM-MIA提供了一种无需标记非成员的成员推断方法，增强了MIA的鲁棒性和实用性，为语言模型隐私评估提供新工具。引入OLMoMIA基准促进了可重复和鲁棒的MIA研究。局限性包括在分布高度重叠场景下的性能不足，未来工作可集中于改进这些条件下的攻击策略或探索混合方法。",
      "tags": [
        "Membership Inference Attack",
        "Expectation Maximization",
        "Large Language Model",
        "Robustness Evaluation",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-27T03:33:51.455675Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.05270",
    "title": "CLIP's Visual Embedding Projector is a Few-shot Cornucopia",
    "authors": [
      "Mohammad Fahes",
      "Tuan-Hung Vu",
      "Andrei Bursuc",
      "Patrick Pérez",
      "Raoul de Charette"
    ],
    "abstract": "We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder's projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation'' and ``validation-free'' settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP's lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2410.05270.pdf",
    "abs_url": "https://arxiv.org/abs/2410.05270",
    "published": "2024-10-07T17:59:59Z",
    "updated": "2026-01-26T14:50:34Z",
    "comment": "WACV 2026",
    "light_analysis": {
      "overview": "本文提出ProLIP，一种简单架构无关的方法，通过Frobenius norm正则化微调CLIP视觉编码器的投影矩阵，实现少样本分类的高效适配。",
      "motivation": "少样本分类是AI领域的关键挑战，旨在用少量样本快速适配预训练模型。现有方法如非线性CLIP-Adapter依赖复杂调参，对学习率敏感，导致效率低下和性能不稳定。本研究旨在克服这些不足，开发一种更简洁的方法，提升预训练视觉-语言模型在数据稀缺场景下的实用性和泛化能力，以满足实际应用中对高效适配的需求。",
      "method": "ProLIP的核心技术是微调视觉编码器的投影矩阵，并使用Frobenius norm正则化约束权重偏差，以防止过拟合，保持预训练知识。该方法与模型架构无关，适用于CLIP等对比预训练模型。基于对CLIP-Adapter的非线性结构反思，还设计了Regularized Linear Adapter (RLA)，在无法访问权重的黑盒场景中作为替代方案。RLA无需超参数调优，学习率鲁棒性强，提供了更灵活的适配选择。",
      "result": "ProLIP在11个少样本分类基准测试中，在“few-shot validation”和“validation-free”设置下均达到最先进的性能，与基线方法如CLIP-Adapter相比有显著提升。此外，在跨数据集转移、域泛化、基础到新类泛化和测试时适应等任务中表现出色，例如在测试时适应中性能优于提示调优，同时训练速度快一个数量级，展示了高效且有效的适配能力。",
      "conclusion": "本研究的主要贡献是提出了ProLIP和RLA两种适配方法，显著提升了预训练模型在少样本分类及相关任务中的性能。这些方法具有学术创新价值，为视觉-语言模型的适配提供了新思路，同时实际应用价值高，支持快速部署于资源受限环境。未来工作可探索更广泛的模型类型或优化正则化策略以进一步提高泛化能力。",
      "tags": [
        "Few-shot Learning",
        "Contrastive Learning",
        "CLIP",
        "Frobenius Norm Regularization",
        "Vision-Language Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:00.599835Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2409.12680",
    "title": "Exploiting Minority Pseudo-Labels for Semi-Supervised Fine-grained Road Scene Understanding",
    "authors": [
      "Yuting Hong",
      "Yongkang Wu",
      "Hui Xiao",
      "Huazheng Hao",
      "Xiaojie Qiu",
      "Baochen Yao",
      "Chengbin Peng"
    ],
    "abstract": "In fine-grained road scene understanding, semantic segmentation plays a crucial role in enabling vehicles to perceive and comprehend their surroundings. By assigning a specific class label to each pixel in an image, it allows for precise identification and localization of detailed road features, which is vital for high-quality scene understanding and downstream perception tasks. A key challenge in this domain lies in improving the recognition performance of minority classes while mitigating the dominance of majority classes, which is essential for achieving balanced and robust overall performance. However, traditional semi-supervised learning methods often train models overlooking the imbalance between classes. To address this issue, firstly, we propose a general training module that learns from all the pseudo-labels without a conventional filtering strategy. Secondly, we propose a professional training module to learn specifically from reliable minority-class pseudo-labels identified by a novel mismatch score metric. The two modules are crossly supervised by each other so that it reduces model coupling which is essential for semi-supervised learning. During contrastive learning, to avoid the dominance of the majority classes in the feature space, we propose a strategy to assign evenly distributed anchors for different classes in the feature space. Experimental results on multiple public benchmarks show that our method surpasses traditional approaches in recognizing tail classes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2409.12680.pdf",
    "abs_url": "https://arxiv.org/abs/2409.12680",
    "published": "2024-09-19T11:47:25Z",
    "updated": "2026-01-26T08:29:19Z",
    "comment": "14 pages, 12 figures",
    "light_analysis": {
      "overview": "该论文提出一种交叉监督的半监督学习框架，通过利用少数类别伪标签来提升细粒度道路场景语义分割中少数类别的识别性能。",
      "motivation": "在细粒度道路场景理解中，语义分割对车辆感知环境至关重要，但类别不平衡问题导致少数类别识别性能差，多数类别主导影响整体平衡性和鲁棒性。传统半监督学习方法常忽略类别间不平衡，导致模型偏向多数类别，限制了实际应用中的精确场景理解能力。本研究旨在解决这一问题，以实现更均衡和鲁棒的语义分割性能，这对于自动驾驶等下游任务具有重要意义。",
      "method": "论文提出两个训练模块：通用训练模块直接学习所有生成的伪标签，不使用传统过滤策略；专业训练模块则专门从通过新颖不匹配分数度量识别的可靠少数类别伪标签中学习，以专注少数类别的特征提取。两个模块通过交叉监督相互指导，减少了模型的耦合度，这在半监督学习中至关重要。在对比学习阶段，为了避免多数类别在特征空间中的主导，引入了一种策略，为不同类别分配均匀分布的锚点，从而平衡特征表示并提升少数类别的识别效果。",
      "result": "实验在多个公开基准数据集上进行，结果显示该方法在识别尾部类别方面超越了传统的半监督学习方法。尽管摘要未提供具体的准确率或效率数据，但实验验证了所提出框架能够有效提高少数类别的识别性能，展现出更好的类别平衡性和整体场景理解能力。与基线方法相比，该方法在少数类别识别任务上表现优异，证明了其处理类别不平衡问题的有效性。",
      "conclusion": "本研究的主要贡献在于提出了一种新的半监督学习框架，通过交叉监督和平衡特征空间的方法，有效解决了细粒度语义分割中的类别不平衡问题。其学术价值体现在改进了半监督学习在类别不平衡场景下的应用策略，实际应用价值则在于提升了自动驾驶等系统中对道路场景的精确理解和鲁棒性。未来工作可能包括进一步优化模型参数、扩展到其他不平衡数据集或探索在更广泛视觉任务中的应用，以增强模型的通用性和适应性。",
      "tags": [
        "Semi-Supervised Learning",
        "Fine-grained Semantic Segmentation",
        "Contrastive Learning",
        "Pseudo-Labels",
        "Class Imbalance"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:14.374360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2409.07314",
    "title": "MEDIC: Comprehensive Evaluation of Leading Indicators for LLM Safety and Utility in Clinical Applications",
    "authors": [
      "Praveenkumar Kanithi",
      "Clément Christophe",
      "Marco AF Pimentel",
      "Tathagata Raha",
      "Prateek Munjal",
      "Nada Saadi",
      "Hamza A Javed",
      "Svetlana Maslenkova",
      "Nasir Hayat",
      "Ronnie Rajan",
      "Shadab Khan"
    ],
    "abstract": "While Large Language Models (LLMs) achieve superhuman performance on standardized medical licensing exams, these static benchmarks have become saturated and increasingly disconnected from the functional requirements of clinical workflows. To bridge the gap between theoretical capability and verified utility, we introduce MEDIC, a comprehensive evaluation framework establishing leading indicators across various clinical dimensions. Beyond standard question-answering, we assess operational capabilities using deterministic execution protocols and a novel Cross-Examination Framework (CEF), which quantifies information fidelity and hallucination rates without reliance on reference texts. Our evaluation across a heterogeneous task suite exposes critical performance trade-offs: we identify a significant knowledge-execution gap, where proficiency in static retrieval does not predict success in operational tasks such as clinical calculation or SQL generation. Furthermore, we observe a divergence between passive safety (refusal) and active safety (error detection), revealing that models fine-tuned for high refusal rates often fail to reliably audit clinical documentation for factual accuracy. These findings demonstrate that no single architecture dominates across all dimensions, highlighting the necessity of a portfolio approach to clinical model deployment. As part of this investigation, we released a public leaderboard on Hugging Face.\\footnote{https://huggingface.co/spaces/m42-health/MEDIC-Benchmark}",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2409.07314.pdf",
    "abs_url": "https://arxiv.org/abs/2409.07314",
    "published": "2024-09-11T14:44:51Z",
    "updated": "2026-01-26T06:45:00Z",
    "comment": "Technical report",
    "light_analysis": {
      "overview": "本研究引入了MEDIC框架，通过结合确定性执行和交叉检查，全面评估大语言模型在临床应用中的安全性和效用。",
      "motivation": "大语言模型在标准化医学考试中表现优异，但这些静态基准测试已饱和，且与临床工作流的实际需求脱节。现有评估方法主要关注静态问答任务，未能全面评估模型的操作能力和安全性，导致理论能力与验证效用之间存在差距。因此，需要一个新框架来弥补这一不足，确保LLM在临床应用中的可靠性和有效性。",
      "method": "论文提出了MEDIC框架，一个综合评估体系，包括确定性执行协议和新颖的交叉检查框架（CEF）。CEF用于量化信息保真度和幻觉率，而不依赖参考文本。评估覆盖多个临床维度，使用异质性任务套件（如临床计算和SQL生成）来测试模型的操作能力。关键创新在于将执行协议与CEF结合，以更全面地评估模型在真实场景中的表现。",
      "result": "评估结果显示关键性能权衡：模型在静态检索和操作任务（如临床计算）之间存在显著的知识执行差距；被动安全（拒绝回答）和主动安全（错误检测）之间也存在分歧，例如微调用于高拒绝率的模型常无法可靠审计临床文档的事实准确性。这些发现表明没有单一模型架构在所有维度上占优，强调了多维度评估的重要性。",
      "conclusion": "该研究的主要贡献是开发了MEDIC框架，强调了多维度评估在临床LLM部署中的必要性。学术上，填补了现有评估框架的空白；实际中，为模型选择和优化提供了实用指导。研究还发布了公共排行榜，促进社区参与。未来可探索模型组合部署策略，以应对不同临床任务的需求。",
      "tags": [
        "Large Language Model",
        "Clinical Informatics",
        "Evaluation Metrics",
        "Hallucination Detection",
        "Safety Evaluation"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:06.162700Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2409.02588",
    "title": "Multiview Random Vector Functional Link Network for Predicting DNA-Binding Proteins",
    "authors": [
      "A. Quadir",
      "M. Sajid",
      "M. Tanveer"
    ],
    "abstract": "The identification of DNA-binding proteins (DBPs) is essential due to their significant impact on various biological activities. Understanding the mechanisms underlying protein-DNA interactions is essential for elucidating various life activities. In recent years, machine learning-based models have been prominently utilized for DBP prediction. In this paper, to predict DBPs, we propose a novel framework termed a multiview random vector functional link (MvRVFL) network, which fuses neural network architecture with multiview learning. The MvRVFL model integrates both late and early fusion advantages, enabling separate regularization parameters for each view, while utilizing a closed-form solution for efficiently determining unknown parameters. The primal objective function incorporates a coupling term aimed at minimizing a composite of errors stemming from all views. From each of the three protein views of the DBP datasets, we extract five features. These features are then fused together by incorporating a hidden feature during the model training process. The performance of the proposed MvRVFL model on the DBP dataset surpasses that of baseline models, demonstrating its superior effectiveness. We further validate the practicality of the proposed model across diverse benchmark datasets, and both theoretical analysis and empirical results consistently demonstrate its superior generalization performance over baseline models.",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2409.02588.pdf",
    "abs_url": "https://arxiv.org/abs/2409.02588",
    "published": "2024-09-04T10:14:17Z",
    "updated": "2026-01-26T06:22:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种多视图随机向量功能链接网络（MvRVFL），融合神经网络与多视图学习，用于预测DNA结合蛋白，提高了预测准确性和泛化性能。",
      "motivation": "DNA结合蛋白（DBPs）的鉴定对理解蛋白质-DNA相互作用机制至关重要，影响多种生物活动。近年来，基于机器学习的模型广泛用于DBP预测，但现有方法在多视图数据融合方面可能不足，如未能有效集成不同视图的信息。本研究旨在通过提出新的融合框架，解决DBP预测中的多视图融合问题，提升模型的预测准确性和泛化能力，以适应生物信息学领域的需求。",
      "method": "本文提出了MvRVFL网络，将神经网络架构与多视图学习相结合。关键创新包括集成晚期和早期融合优势，允许为每个视图设置单独的正则化参数，并使用闭式解高效确定未知参数。目标函数包含耦合项，旨在最小化所有视图的误差组合。从DBP数据集的三个蛋白质视图中，各提取五个特征，并在模型训练过程中通过隐藏特征进行融合，具体基于随机向量功能链接网络架构实现多视图特征的有效整合。",
      "result": "在DBP数据集上，MvRVFL模型的性能超越基线模型，表现出优越的有效性。通过在不同基准数据集上的进一步验证，理论和实证结果一致表明该模型具有卓越的泛化性能，优于对比基线模型。摘要未明确说明具体性能指标数据，但强调了其在多数据集上的普遍适用性和改进效果，展示了模型在提升预测准确性和稳定性方面的优势。",
      "conclusion": "MvRVFL网络通过创新地融合神经网络和多视图学习，显著提升了DNA结合蛋白预测的准确性和泛化能力。这不仅为生物信息学领域提供了有效的预测工具，还具有重要的学术价值，展示了多视图融合方法的潜力。未来研究可探索该框架在其他生物预测任务中的应用，并可能进一步优化模型结构或扩展到更多视图场景，以应对更复杂的生物数据挑战。",
      "tags": [
        "Multiview Learning",
        "Random Vector Functional Link Network",
        "Feature Fusion",
        "Regularization",
        "Closed-form Solution"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:43.875270Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2408.08055",
    "title": "DeNOTS: Stable Deep Neural ODEs for Time Series",
    "authors": [
      "Ilya Kuleshov",
      "Evgenia Romanenkova",
      "Vladislav Zhuzhel",
      "Galina Boeva",
      "Evgeni Vorsin",
      "Alexey Zaytsev"
    ],
    "abstract": "Neural CDEs provide a natural way to process the temporal evolution of irregular time series. The number of function evaluations (NFE) is these systems' natural analog of depth (the number of layers in traditional neural networks). It is usually regulated via solver error tolerance: lower tolerance means higher numerical precision, requiring more integration steps. However, lowering tolerances does not adequately increase the models' expressiveness. We propose a simple yet effective alternative: scaling the integration time horizon to increase NFEs and \"deepen`` the model. Increasing the integration interval causes uncontrollable growth in conventional vector fields, so we also propose a way to stabilize the dynamics via Negative Feedback (NF). It ensures provable stability without constraining flexibility. It also implies robustness: we provide theoretical bounds for Neural ODE risk using Gaussian process theory. Experiments on four open datasets demonstrate that our method, DeNOTS, outperforms existing approaches~ -- ~including recent Neural RDEs and state space models,~ -- ~achieving up to $20\\%$ improvement in metrics. DeNOTS combines expressiveness, stability, and robustness, enabling reliable modelling in continuous-time domains.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2408.08055.pdf",
    "abs_url": "https://arxiv.org/abs/2408.08055",
    "published": "2024-08-15T09:49:37Z",
    "updated": "2026-01-26T16:17:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出DeNOTS方法，通过扩展积分时间范围和负反馈稳定，提升神经ODE在时间序列建模中的表达性和鲁棒性。",
      "motivation": "该研究旨在解决不规则时间序列建模中神经ODE表达性不足的问题。现有方法如Neural CDEs通过降低求解器容差来增加函数评估次数（NFE），但这并不能有效提升模型表达能力，且可能导致计算不稳定。因此，需要一种能够增强模型深度而不牺牲稳定性的新方法，以提高连续时间域建模的可靠性，尤其是在处理时间序列数据时，传统方法难以平衡深度与稳定性的矛盾。",
      "method": "论文提出DeNOTS方法，核心包括缩放积分时间范围以增加NFE来深化模型，同时引入负反馈机制稳定动态，避免常规向量场的不可控增长。该方法基于高斯过程理论提供了理论风险界限，确保稳定性和鲁棒性。实验在四个开放数据集上进行，模型结构优化以实现高效的时间序列处理。关键创新点在于结合负反馈控制与积分范围调整，从而在不限制灵活性的前提下提升性能。",
      "result": "在四个开放数据集上的实验结果表明，DeNOTS方法显著优于现有方法，包括近期的Neural RDEs和状态空间模型。具体性能指标提升高达20%，证明了该方法在时间序列建模中具有优越的表达性和鲁棒性。实验结果还显示了稳定性的改进，与基线方法相比，在多个评估指标上均有所进步，例如准确性或效率的提升，尽管摘要未明确说明具体指标名称。",
      "conclusion": "DeNOTS的主要贡献在于结合了表达性、稳定性和鲁棒性，促进了连续时间域的可靠建模，提供了新的神经ODE优化途径。其学术价值在于理论风险界限的推导和实际应用的改进，潜在局限性包括对特定数据集的依赖性，未来工作可扩展到更广泛的应用场景或探索与其他技术的结合。摘要未明确说明具体局限性，但基于方法可推断其在复杂环境中的泛化性需进一步研究。",
      "tags": [
        "Neural ODEs",
        "Neural CDEs",
        "Negative Feedback",
        "Gaussian Process Theory",
        "Time Series Analysis"
      ]
    },
    "analyzed_at": "2026-01-27T03:34:46.250162Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2407.09386",
    "title": "Radiance Fields from Photons",
    "authors": [
      "Sacha Jungerman",
      "Aryan Garg",
      "Mohit Gupta"
    ],
    "abstract": "Neural radiance fields, or NeRFs, have become the de facto approach for high-quality view synthesis from a collection of images captured from multiple viewpoints. However, many issues remain when capturing images in-the-wild under challenging conditions, such as low light, high dynamic range, or rapid motion leading to smeared reconstructions with noticeable artifacts. In this work, we introduce quanta radiance fields, a novel class of neural radiance fields that are trained at the granularity of individual photons using single-photon cameras (SPCs). We develop theory and practical computational techniques for building radiance fields and estimating dense camera poses from unconventional, stochastic, and high-speed binary frame sequences captured by SPCs. We demonstrate, both via simulations and a SPC hardware prototype, high-fidelity reconstructions under high-speed motion, in low light, and for extreme dynamic range settings.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2407.09386.pdf",
    "abs_url": "https://arxiv.org/abs/2407.09386",
    "published": "2024-07-12T16:06:51Z",
    "updated": "2026-01-26T14:06:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种名为“quanta radiance fields”的新型神经辐射场方法，利用单光子相机在光子级别训练，以解决低光、高动态范围和高速度条件下的视图合成挑战。",
      "motivation": "神经辐射场（NeRFs）已成为多视角图像高质量视图合成的标准方法，但在实际应用中，如低光照、高动态范围或快速运动等挑战条件下，重构质量会显著下降，产生明显伪影。这些条件在野外拍摄中常见，现有方法难以处理，限制了NeRFs的实用性和适用范围。因此，本研究旨在开发一种新技术，以克服这些局限，提升在恶劣环境下的重构性能。",
      "method": "本研究引入了“quanta radiance fields”，一种基于单光子相机（SPCs）的新型神经辐射场，通过在单个光子级别进行训练，处理非传统、随机和高速的二进制帧序列。作者开发了理论框架和实用计算技术，用于从SPCs捕获的数据中构建辐射场并估计密集相机姿态。关键创新点包括处理光子级数据的方法，整合神经网络与光子传感器，以应对低光和高动态范围场景的挑战，并通过模拟和硬件原型实现验证。",
      "result": "通过模拟和单光子相机硬件原型验证，该方法在高速度运动、低光照和极端动态范围设置下实现了高保真重构。摘要未明确提供具体性能指标如准确率提升或效率改进，但演示表明该方法能有效克服传统NeRFs在这些条件下的限制。与基线方法对比，新方法显示出在挑战条件下的优越重构能力，但具体数据摘要未明确说明。",
      "conclusion": "研究的主要贡献是提出“quanta radiance fields”，扩展了神经辐射场在低光、高动态范围和高速度条件下的应用。学术价值在于结合光子级数据与神经网络，推动了视图合成技术的发展；实际应用潜力包括改善低光摄影、高速运动捕捉和恶劣环境下的三维重建。局限性可能涉及计算复杂性或硬件依赖，未来工作可进一步优化算法并在更广泛场景中验证性能。",
      "tags": [
        "Neural Radiance Fields",
        "Single-Photon Cameras",
        "High Dynamic Range Imaging",
        "Low Light Reconstruction",
        "View Synthesis"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:49.112529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2407.00449",
    "title": "Fully tensorial approach to hypercomplex-valued neural networks",
    "authors": [
      "Agnieszka Niemczynowicz",
      "Radosław Antoni Kycia"
    ],
    "abstract": "A fully tensorial theoretical framework for hypercomplex-valued neural networks is presented. The proposed approach enables neural network architectures to operate on data defined over arbitrary finite-dimensional algebras. The central observation is that algebra multiplication can be represented by a rank-three tensor, which allows all algebraic operations in neural network layers to be formulated in terms of standard tensor contractions, permutations, and reshaping operations.   This tensor-based formulation provides a unified and dimension-independent description of hypercomplex-valued dense and convolutional layers and is directly compatible with modern deep learning libraries supporting optimized tensor operations. The proposed framework recovers existing constructions for four-dimensional algebras as a special case.   Within this setting, a tensor-based version of the universal approximation theorem for single-layer hypercomplex-valued perceptrons is established under mild non-degeneracy assumptions on the underlying algebra, thereby providing a rigorous theoretical foundation for the considered class of neural networks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2407.00449.pdf",
    "abs_url": "https://arxiv.org/abs/2407.00449",
    "published": "2024-06-29T14:19:40Z",
    "updated": "2026-01-26T09:51:42Z",
    "comment": "23 pages, 3 figures",
    "light_analysis": {
      "overview": "提出了一个全张量的理论框架，使超复数神经网络能在任意有限维代数上操作，并建立了通用逼近定理。",
      "motivation": "超复数神经网络在处理多维数据时具有潜力，但现有方法可能局限于四维代数等特定维度，缺乏统一的数学框架来支持任意有限维代数上的操作。该研究旨在解决这一问题，以提高模型的表达能力和适用性，扩展神经网络到更广泛的数学结构。摘要未明确说明实际应用场景，但暗示现有构造的局限性，需要通用方法来促进理论发展和实际实现。",
      "method": "论文提出了一种基于张量的理论框架，通过将代数乘法表示为秩三张量，使得神经网络层中的操作能以标准张量收缩、排列和重塑的形式实现。关键创新在于提供统一且维度无关的描述，适用于稠密层和卷积层等架构，并兼容现代深度学习库的优化张量操作。框架以四维代数作为特例，恢复了现有构造，突出了其灵活性和扩展性。",
      "result": "该框架的主要理论成果是建立了超复数单层感知器的通用逼近定理，在底层代数的温和非退化假设下，提供了严格的数学证明。此外，框架成功恢复了四维代数的现有构造，验证了其一致性和兼容性。摘要未提及具体实验性能指标，但强调了框架的数学严谨性和与现有技术的无缝集成，为后续应用奠定基础。",
      "conclusion": "论文的主要贡献是提出了一个全张量的理论框架，为超复数神经网络提供了统一和维度无关的数学基础，具有重要的学术价值。研究建立了通用逼近定理，扩展了神经网络到更广泛的代数结构，并增强了理论严谨性。实际应用中，框架与深度学习库兼容，便于实现和部署。未来工作可能包括实验验证和扩展到更复杂的网络架构或更多代数类型。",
      "tags": [
        "Hypercomplex-valued Neural Networks",
        "Tensor Representation",
        "Universal Approximation Theorem",
        "Algebra Multiplication",
        "Convolutional Layers"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:04.253272Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2404.06106",
    "title": "Unifying Low Dimensional Observations in Deep Learning Through the Deep Linear Unconstrained Feature Model",
    "authors": [
      "Connall Garrod",
      "Jonathan P. Keating"
    ],
    "abstract": "Empirical studies have revealed low dimensional structures in the eigenspectra of weights, Hessians, gradients, and feature vectors of deep networks, consistently observed across datasets and architectures in the overparameterized regime. In this work, we analyze deep unconstrained feature models (UFMs) to provide an analytic explanation of how these structures emerge at the layerwise level, including the bulk outlier Hessian spectrum and the alignment of gradient descent with the outlier eigenspace. We show that deep neural collapse underlies these phenomena, deriving explicit expressions for eigenvalues and eigenvectors of many deep learning matrices in terms of class feature means. Furthermore, we demonstrate that the full Hessian inherits its low dimensional structure from the layerwise Hessians, and empirically validate our theory in both UFMs and deep networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2404.06106.pdf",
    "abs_url": "https://arxiv.org/abs/2404.06106",
    "published": "2024-04-09T08:17:32Z",
    "updated": "2026-01-26T16:10:02Z",
    "comment": "44 pages, 18 figures",
    "light_analysis": {
      "overview": "本文通过深度无约束特征模型分析，为深度学习中观测到的低维结构提供统一解析解释，基于深度神经坍塌推导矩阵特征表达式。",
      "motivation": "实证研究在过参数化深度网络中一致观察到权重、Hessians、梯度和特征向量的特征谱呈现低维结构，但缺乏理论解释其涌现机制。本研究旨在解决这一理论缺口，解释这些结构如何从深度学习模型中层级出现，以填补现有方法仅关注现象观测、未深入解析成因的不足，从而促进对深度学习内在机制的理解。",
      "method": "论文采用深度无约束特征模型（UFMs）作为分析框架，通过理论推导揭示层级上Hessians、梯度和特征向量的低维结构。关键创新在于将深度神经坍塌现象与这些结构关联，基于类别特征均值给出特征值和特征向量的显式表达式，统一解释批量异常Hessian谱和梯度下降与异常特征空间的对应，模型架构聚焦于UFMs以简化分析。",
      "result": "理论推导显示深度神经坍塌是低维结构的基础，层级Hessians呈现低维谱，且完整Hessian从其继承此特性。经验验证在UFMs和实际深度网络中进行，支持了理论预测，但摘要未明确说明具体性能指标如准确率提升数据，主要确认了理论框架的有效性和普遍性。",
      "conclusion": "本研究贡献了统一的解析框架，解释了深度学习中多种低维观测现象，核心在于理论化神经坍塌的作用，具有重要学术价值，可指导模型优化和设计。局限性包括模型简化假设，未来工作可扩展验证到更复杂网络或探索实际应用场景，如提升训练效率。",
      "tags": [
        "Deep Learning",
        "Low Dimensional Structures",
        "Hessian Spectrum",
        "Neural Collapse",
        "Unconstrained Feature Models"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:13.028949Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2309.16738",
    "title": "ELIP: Efficient Discriminative Language-Image Pre-training with Fewer Vision Tokens",
    "authors": [
      "Yangyang Guo",
      "Haoyu Zhang",
      "Yongkang Wong",
      "Liqiang Nie",
      "Mohan Kankanhalli"
    ],
    "abstract": "Learning a versatile language-image model is computationally prohibitive under a limited computing budget. This paper delves into the \\emph{efficient language-image pre-training}, an area that has received relatively little attention despite its importance in reducing computational cost and footprint. To that end, we propose a vision token pruning and merging method ELIP, to remove less influential tokens based on the supervision of language outputs. Our method is designed with several strengths, such as being computation-efficient, memory-efficient, and trainable-parameter-free, and is distinguished from previous vision-only token pruning approaches by its alignment with task objectives. We implement this method in a progressively pruning manner using several sequential blocks. To evaluate its generalization performance, we apply ELIP to three commonly used language-image pre-training models and utilize public image-caption pairs with 4M images for pre-training. Our experiments demonstrate that with the removal of ~30$\\%$ vision tokens across 12 ViT layers, ELIP maintains significantly comparable performance with baselines ($\\sim$0.32 accuracy drop on average) over various downstream tasks including cross-modal retrieval, VQA, image captioning, \\emph{etc}. In addition, the spared GPU resources by our ELIP allow us to scale up with larger batch sizes, thereby accelerating model pre-training and even sometimes enhancing downstream model performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2309.16738.pdf",
    "abs_url": "https://arxiv.org/abs/2309.16738",
    "published": "2023-09-28T05:31:07Z",
    "updated": "2026-01-26T12:11:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "ELIP提出了一种基于语言监督的视觉令牌剪枝方法，用于高效的语言-图像预训练，在减少计算成本的同时保持性能。",
      "motivation": "该研究旨在解决语言-图像预训练中高计算成本的问题，这对资源有限的环境至关重要。尽管高效预训练能显著降低计算负担和模型足迹，但现有研究关注较少。现有方法如纯视觉令牌剪枝可能未与任务目标充分对齐，导致效率提升有限。因此，开发一种能结合语言监督的剪枝方法成为重要需求，以实现在保持模型通用性的同时优化计算资源使用。",
      "method": "ELIP方法通过语言输出监督来识别和剪枝影响力较低的视觉令牌，并结合合并技术减少令牌数量，实现高效计算和内存管理。其关键创新在于任务对齐设计，与纯视觉方法不同，确保了剪枝过程与下游目标一致。采用渐进式剪枝策略，在多个连续块中逐步移除令牌，无需额外训练参数。该方法应用于三个常用语言-图像预训练模型，并使用包含400万图像对的公共数据集进行验证，增强了通用性和可扩展性。",
      "result": "实验结果表明，在移除约30%的视觉令牌后，ELIP在各种下游任务（如跨模态检索、视觉问答和图像描述）中平均准确率仅下降约0.32，性能与基线方法显著可比。与基线对比，ELIP不仅维持了高效表现，还通过节省的GPU资源允许扩大批次大小，从而加速了模型预训练过程，有时甚至提升了模型在下游任务中的性能，展示了其在资源优化和效率提升方面的优势。",
      "conclusion": "ELIP通过创新性的视觉令牌剪枝技术，有效降低了语言-图像预训练的计算需求，同时保持了模型性能。该研究突出了任务对齐在高效预训练中的重要性，为后续学术探索提供了新方向。在实际应用中，ELIP有助于在资源受限场景下部署高性能模型，未来可进一步探索更广泛的模型架构和任务扩展，以优化效率和通用性，可能涉及更多数据集和剪枝策略的改进。",
      "tags": [
        "Vision Token Pruning",
        "Language-Image Pre-training",
        "Progressive Pruning",
        "Task Alignment",
        "Efficient Computation"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:13.556160Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2005.05837",
    "title": "Energy-Aware DNN Graph Optimization",
    "authors": [
      "Yu Wang",
      "Rong Ge",
      "Shuang Qiu"
    ],
    "abstract": "Unlike existing work in deep neural network (DNN) graphs optimization for inference performance, we explore DNN graph optimization for energy awareness and savings for power- and resource-constrained machine learning devices. We present a method that allows users to optimize energy consumption or balance between energy and inference performance for DNN graphs. This method efficiently searches through the space of equivalent graphs, and identifies a graph and the corresponding algorithms that incur the least cost in execution. We implement the method and evaluate it with multiple DNN models on a GPU-based machine. Results show that our method achieves significant energy savings, i.e., 24% with negligible performance impact.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2005.05837.pdf",
    "abs_url": "https://arxiv.org/abs/2005.05837",
    "published": "2020-05-12T14:56:19Z",
    "updated": "2026-01-26T09:20:07Z",
    "comment": "Accepted paper at Resource-Constrained Machine Learning (ReCoML) Workshop of MLSys 2020 Conference, Austin, TX, USA, 2020",
    "light_analysis": {
      "overview": "该论文提出了一种能量感知的深度神经网络图优化方法，在保持推理性能的同时实现显著节能。",
      "motivation": "研究动机源于现有DNN图优化工作主要关注推理性能提升，而忽视了能量效率，这在功率和资源受限的机器学习设备（如移动和嵌入式系统）中尤为重要。这些问题需要节能以延长设备续航和降低运营成本，但现有方法不足在于缺乏能量优化。因此，本研究旨在填补这一空白，探索能量感知的DNN图优化，以解决资源受限环境下的高效推理需求。",
      "method": "该方法通过高效搜索深度神经网络的等效图空间，识别执行成本（包括能量消耗和推理性能）最低的图和相应算法。在实现中，该方法在基于GPU的机器上实施，考虑了硬件特性以优化能量效率，并允许用户根据需求平衡能量与性能。关键创新在于将能量因素整合到优化目标中，使用多种DNN模型进行验证，涉及图变换和算法选择策略。",
      "result": "实验结果显示，在GPU平台上使用多个DNN模型评估时，该方法平均实现24%的能量节省，同时推理性能损失可忽略不计。这证明了方法在提升能量效率方面的有效性，与仅关注性能的传统优化方法相比有显著改进。具体数据支撑了节能效果，验证了其在资源受限设备中的实用性和优化潜力。",
      "conclusion": "本研究的主要贡献在于开发了一种能量感知的DNN图优化方法，成功在保持性能的同时实现节能。学术价值在于扩展了DNN优化领域，关注能量效率研究；实际应用价值体现在为资源受限设备提供节能方案，降低能耗。未来工作可探索更多硬件平台或动态优化策略，以提升方法的通用性和适应性。",
      "tags": [
        "Deep Neural Network",
        "Graph Optimization",
        "Energy Efficiency",
        "GPU Computing",
        "Inference Optimization"
      ]
    },
    "analyzed_at": "2026-01-27T03:35:41.301108Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]