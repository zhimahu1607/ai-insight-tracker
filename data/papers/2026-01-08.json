[
  {
    "id": "2601.04170",
    "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
    "authors": [
      "Abhishek Rath"
    ],
    "abstract": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).   We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.   We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04170.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04170",
    "published": "2026-01-07T18:37:26Z",
    "updated": "2026-01-07T18:37:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究首次提出并量化了多智能体LLM系统中的agent drift现象，开发了代理稳定性指数框架，并提出了缓解策略。",
      "motivation": "多智能体大型语言模型系统在复杂任务处理中展现出强大能力，但其长期交互中的行为稳定性问题鲜有研究。随着这些系统在企业应用中的广泛部署，行为退化可能导致任务完成质量下降并增加人类干预需求，从而威胁系统可靠性和安全性。现有研究多关注短期性能，对长期稳定性的系统性分析不足，因此本研究旨在填补这一空白，为AI系统的长期运行提供理论支撑。",
      "method": "论文提出了一个理论框架来理解agent drift现象，将其分为语义漂移、协调漂移和行为漂移三种表现形式。关键创新是引入了代理稳定性指数，一个复合度量框架，用于量化12个维度如响应一致性、工具使用模式、推理路径稳定性和智能体间一致性率。通过基于仿真的分析和理论建模，验证了漂移的发生及其机制，并提出了缓解策略如情景记忆巩固、漂移感知路由协议和自适应行为锚定。",
      "result": "通过理论建模和仿真分析，研究展示了未受控制的agent drift会导致任务完成准确率显著降低和人类干预需求增加。摘要未明确说明具体性能指标数据如准确率数字，但强调了漂移的负面影响。与基线方法的直接对比未在摘要中详细说明，但暗示了漂移问题的严重性，突出了研究的重要性和缓解策略的潜在效果。",
      "conclusion": "本论文为生产性AI系统中监控、测量和缓解agent drift建立了基础方法论，具有重要的学术和实际价值。它不仅推动了多智能体系统长期稳定性的研究，还对企业部署可靠性和AI安全研究有直接启示。未来的工作可进一步实证验证缓解策略的有效性，并扩展应用到更广泛的实际场景中，以提升AI系统的长期性能。",
      "tags": [
        "Multi-Agent Systems",
        "Large Language Models",
        "Agent Drift",
        "Simulation-based Analysis",
        "Behavioral Stability"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:57.900024Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04164",
    "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
    "authors": [
      "Alberto Marfoglia",
      "Jong Ho Jhee",
      "Adrien Coulet"
    ],
    "abstract": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04164.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04164",
    "published": "2026-01-07T18:25:02Z",
    "updated": "2026-01-07T18:25:02Z",
    "comment": "12 pages, 5 tables, 4 figures",
    "light_analysis": {
      "overview": "本文提出MEDS-OWL本体和meds2rdf转换库，将医疗事件数据标准与语义网络整合，为事件临床数据提供可重用的语义表示层。",
      "motivation": "机器学习在医疗数据应用常受限于缺乏标准化和语义明确的表示，导致数据集间互操作性差和实验可重复性不足。MEDS虽为可重复机器学习工作流设计了事件中心数据模型，但仅作为格式规范，未原生集成语义网络，限制了数据的深入分析和共享，因此需解决数据表示不统一和语义缺失的问题。",
      "method": "论文提出MEDS-OWL轻量级OWL本体，定义了13个类、10个对象属性、20个数据属性和24个OWL公理，形式化表示MEDS数据集。同时开发meds2rdf Python库，将MEDS事件转换为RDF图，确保与本体一致。在一个合成临床数据集上演示该方法，描述破裂颅内动脉瘤患者护理路径，并使用SHACL约束验证结果图，实现数据标准化转换。",
      "result": "研究发布了MEDS-OWL第一版，包含13个类、10个对象属性、20个数据属性和24个OWL公理。结合meds2rdf转换库，成功将MEDS数据转化为RDF图，实现数据向FAIR原则对齐、支持溯源感知发布，并增强事件临床数据的互操作性。在合成数据集上的演示验证了方法的可行性，通过SHACL约束确保数据完整性，摘要未明确说明与基线方法的直接性能对比。",
      "conclusion": "本文通过桥接MEDS与语义网络，贡献了事件临床数据的可重用语义层，为后续基于图的医疗数据分析奠定基础，促进数据标准化和语义化，增强共享和分析能力，推动精准医疗和机器学习应用。摘要未明确说明局限性，未来工作可扩展本体以覆盖更多医疗场景或集成更多数据源。",
      "tags": [
        "OWL Ontology",
        "RDF Graphs",
        "Semantic Web",
        "SHACL Constraints",
        "Data Conversion"
      ]
    },
    "analyzed_at": "2026-01-08T17:35:21.649723Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04151",
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "authors": [
      "Jun Wang",
      "Chunyu Qiang",
      "Yuxin Guo",
      "Yiran Wang",
      "Xijuan Zeng",
      "Chen Zhang",
      "Pengfei Wan"
    ],
    "abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04151.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04151",
    "published": "2026-01-07T18:03:45Z",
    "updated": "2026-01-07T18:03:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "Klear 通过统一的模型架构、训练策略和数据整理，解决了音频-视频联合生成的同步、对齐和退化问题，提供了可扩展的高性能生成方法。",
      "motivation": "音频-视频联合生成在多媒体应用中具有重要性，但现有非商业方法常面临音频-视频不同步、唇语对齐差和单模态退化的挑战，这些问题源于弱音频-视觉对应建模、有限泛化能力和高质量密集字幕数据稀缺，导致生成质量低下，影响实际部署如虚拟现实和视频合成，因此亟需综合解决方案来提升对齐精度和鲁棒性。",
      "method": "模型架构上采用单一塔设计和统一的 Diffusion Transformer (DiT) 块，结合 Omni-Full Attention 机制以增强音频-视频对齐和可扩展性；训练策略上实施渐进式多任务方法，包括随机模态掩码促进联合优化和多阶段课程学习，以提升表示能力、加强对齐知识并防止单模态崩溃；数据方面构建首个大规模音频-视频密集字幕数据集，并通过自动化管道生成高质量、严格对齐的音频-视频-字幕三元组，确保数据多样性和对齐精度。",
      "result": "Klear 在大规模数据集上实现高保真、语义和时间对齐的音频-视频生成，支持联合和单模态设置，并能稳健泛化到分布外场景。在多个任务中，其性能显著优于先前方法，摘要未明确说明具体准确率提升数据，但与 Veo 3 性能相当，验证了其在音频-视频合成领域的有效性和先进性，突出了统一方法的优势。",
      "conclusion": "Klear 通过从模型架构、训练策略和数据整理三个方面的创新，有效解决了音频-视频联合生成的挑战，提供了一条统一且可扩展的技术路径。这不仅提升了生成质量和实用性，为学术研究贡献了新方法，还具有在增强现实、虚拟会议等领域的应用潜力；未来工作可进一步探索更复杂场景的适应性和数据效率优化。",
      "tags": [
        "Audio-Video Joint Generation",
        "Diffusion Transformer (DiT)",
        "Omni-Full Attention",
        "Multitask Learning",
        "Data Curation"
      ]
    },
    "analyzed_at": "2026-01-08T17:35:10.499991Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04131",
    "title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models",
    "authors": [
      "Nikhil Anand",
      "Shwetha Somasundaram",
      "Anirudh Phukan",
      "Apoorv Saxena",
      "Koyel Mukherjee"
    ],
    "abstract": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04131.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04131",
    "published": "2026-01-07T17:45:20Z",
    "updated": "2026-01-07T17:45:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "ContextFocus是一种无需模型微调的轻量级激活引导方法，用于提升大型语言模型在知识冲突下的上下文忠实度。",
      "motivation": "随着世界知识的演变，大型语言模型（LLMs）在实际部署中越来越依赖其忠实地遵循外部检索上下文的能力，但当外部证据与模型内部知识冲突时，LLMs往往默认使用记忆的事实，导致不忠实的输出。这个问题限制了LLMs在动态环境中的有效性，现有方法如微调可能效率不高或灵活性不足，因此需要更高效的技术来解决知识冲突的挑战。",
      "method": "论文提出ContextFocus方法，这是一种基于激活引导的技术，通过调整LLM的内部激活来增强上下文忠实度，无需额外的模型微调，并最小化推理时的计算开销。方法在ConFiQA基准上进行评估，使用了该基准的数据集，但摘要未明确说明具体模型架构或技术实现细节。",
      "result": "在ConFiQA基准上的实验中，ContextFocus显著提高了上下文忠实度，优于基线方法如ContextDPO、COIECD和基于提示的策略。此外，该方法与提示策略互补，并在更大的LLMs模型上保持有效性，但摘要未提供具体的性能指标数据如准确率提升。",
      "conclusion": "ContextFocus有效提升了LLM输出的上下文忠实度，具有高效、鲁棒的特点，增强了模型处理动态信息的能力，具备实际应用价值。摘要未明确说明局限性，未来工作可能包括优化技术细节或扩展到其他领域。",
      "tags": [
        "Large Language Models",
        "Activation Steering",
        "Contextual Faithfulness",
        "Knowledge Conflict",
        "ConFiQA Benchmark"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:41.878645Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04127",
    "title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images",
    "authors": [
      "Leandro Stival",
      "Ricardo da Silva Torres",
      "Helio Pedrini"
    ],
    "abstract": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04127.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04127",
    "published": "2026-01-07T17:41:11Z",
    "updated": "2026-01-07T17:41:11Z",
    "comment": "21 pages, 9 Figures",
    "light_analysis": {
      "overview": "本文提出一种像素级多模态对比学习方法，通过二维表示改进遥感图像时间序列的自监督特征编码，以提升地球观测任务的性能。",
      "motivation": "卫星持续生成大量数据，特别是地球观测中的卫星图像时间序列（SITS）。现有深度学习模型通常设计为处理整个图像或完整序列来提取特征用于下游任务，但可能无法有效捕捉像素级的局部变化。这一问题的重要性在于，缺乏高效的特征表示方法会限制预测和分类等任务的准确性。因此，本研究旨在解决像素级特征提取的挑战，通过提出更有效的表示方法来编码SITS的视觉属性变化，从而提高下游任务的性能。",
      "method": "论文提出了PIxel-wise Multimodal Contrastive（PIMC）方法，这是一种多模态自监督学习框架。核心创新在于使用重绘图从基于像素的植被指数时间序列（如NDVI、EVI和SAVI）生成二维表示，替代原始像素值，以提供更有信息的表示。该方法结合了遥感图像（RSI）和SITS，通过对比学习来训练有效编码器。实验在PASTIS和EuroSAT数据集上进行，验证了像素级预测、分类和土地覆盖分类等下游任务。",
      "result": "实验结果显示，二维表示显著增强了从SITS中提取特征的能力，对比学习提高了像素时间序列和遥感图像的表示质量。在PASTIS数据集的像素级预测和分类任务中，以及EuroSAT数据集的土地覆盖分类任务中，该方法与现有最先进模型相比，表现出更好的性能。这些结果证明了该多模态方法的有效性和鲁棒性，优于现有模型，建立了一个稳健的自监督框架。",
      "conclusion": "本研究的主要贡献是提出了一种像素级多模态对比学习方法，通过结合二维表示和对比学习，实现了遥感图像时间序列和图像的有效自监督编码。学术价值在于提供了一个新的特征提取框架，改进了地球观测任务的性能。实际应用价值高，可支持卫星数据分析和预测任务。摘要未明确说明局限性，但未来工作可能涉及扩展到其他遥感数据集或任务。",
      "tags": [
        "Contrastive Learning",
        "Multimodal Learning",
        "Pixel-Wise Learning",
        "Remote Sensing Images",
        "Self-Supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:38.110594Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04126",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04126.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04126",
    "published": "2026-01-07T17:40:08Z",
    "updated": "2026-01-07T17:40:08Z",
    "comment": "Work In Progress",
    "light_analysis": {
      "overview": "提出InfiniteWeb系统，自动生成大规模功能性web环境，以解决GUI代理训练中环境稀缺的问题，并通过集成任务评估器为强化学习提供密集奖励信号。",
      "motivation": "GUI代理作为实用AI助手的有前景方向，其训练因合适环境稀缺而受阻。现有方法如大型语言模型在生成单个网页时表现良好，但难以构建现实、功能性强且页面互联的网站，限制了代理的训练效率和泛化能力。这突显了开发可扩展环境生成系统的重要性，以克服现有技术的不足，促进AI助手的发展。",
      "method": "InfiniteWeb系统采用统一规范定义网站结构，结合任务为中心的测试驱动开发来验证功能性。通过网站种子与参考设计图像的组合，确保生成环境的多样性和真实性。关键创新在于自动生成可验证的任务评估器，为强化学习训练GUI代理提供密集奖励信号，解决了多页面网站合成和训练反馈的集成挑战。",
      "result": "实验表明，InfiniteWeb在构建现实网站方面超越了商业编码代理，展示了环境生成技术的优势。在OSWorld和Online-Mind2Web基准测试中，使用生成环境训练的GUI代理实现了显著的性能提升，相比基线方法有明显改进，验证了系统对代理训练的有效性和实用性。",
      "conclusion": "本研究的主要贡献是开发了InfiniteWeb系统，解决了GUI代理训练的环境稀缺问题，推动了环境合成与代理训练方法的结合。学术上，它提供了密集奖励信号以优化强化学习；实际上，为构建智能AI助手奠定了基础。未来可探索更多环境类型或优化生成算法，以进一步提升系统性能。",
      "tags": [
        "GUI Agent Training",
        "Web Environment Synthesis",
        "Reinforcement Learning",
        "Large Language Models",
        "Test-Driven Development"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:07.237559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04098",
    "title": "Layer-wise Positional Bias in Short-Context Language Modeling",
    "authors": [
      "Maryam Rahimi",
      "Mahdi Nouri",
      "Yadollah Yaghoobzadeh"
    ],
    "abstract": "Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04098.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04098",
    "published": "2026-01-07T17:04:30Z",
    "updated": "2026-01-07T17:04:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入基于归因的框架，分析短上下文语言模型中层级位置偏差，揭示偏差随层演化、架构特定且独立于任务复杂性。",
      "motivation": "语言模型常表现出对输入特定位置的偏好，而不考虑语义相关性，这可能影响任务性能，尤其在短上下文场景中。先前研究关注了位置偏差在长上下文等环境中的影响，但未深入探讨偏差如何在模型不同层间演化，或如何独立于任务复杂性而变化。这一研究空白对理解模型内部机制和提升模型鲁棒性至关重要，因为偏差的演化可能揭示模型处理信息的内在规律。",
      "method": "论文提出一个基于归因的分析框架，使用层传导力和滑动窗口方法，量化语言模型各层在输入位置上的重要性分布，生成层级位置重要性剖面。该框架的关键创新在于通过归因技术来表征位置效应，独立于任务复杂性，能够稳定地捕捉偏差模式。摘要未明确说明使用的具体数据集或模型架构，但提及分析针对短上下文语言建模，并假设了不同架构的对比研究。",
      "result": "研究发现层级位置重要性剖面具有架构特异性，在不同输入中稳定且对词汇混乱不变。具体表现为显著的近因偏差随着模型深度增加而增强，而细微的首要偏差随深度减弱。此外，早期层在所有位置优先加权内容词而非功能词，后期层则失去这种词类型区分。这些发现填补了先前研究对位置偏差层级演化分析的空白，揭示了偏差的动态变化规律。",
      "conclusion": "论文主要贡献在于揭示了语言模型中层级位置偏差的特性，表明偏差是架构依赖且稳定的，对理解模型内部表示机制有重要学术价值。该研究为改进模型设计和减少偏差提供了理论基础，具有潜在的实际应用价值。未来工作可能包括扩展到长上下文设置或探索更多模型架构的偏差模式，以验证其通用性。摘要未明确说明具体局限性，但可推断需进一步实验验证。",
      "tags": [
        "Layer-wise Analysis",
        "Positional Bias",
        "Short-Context Language Modeling",
        "Attribution Framework",
        "Conductance Analysis"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:47.590142Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04073",
    "title": "Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts",
    "authors": [
      "Zhihao Zhu",
      "Jiafeng Liang",
      "Shixin Jiang",
      "Jinlan Fu",
      "Ming Liu",
      "Guanglu Sun",
      "See-Kiong Ng",
      "Bing Qin"
    ],
    "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04073.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04073",
    "published": "2026-01-07T16:39:34Z",
    "updated": "2026-01-07T16:39:34Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出了主动视觉上下文精炼方法，以抑制大型多模态模型在跨模态冲突下的文本幻觉传播，从而增强推理鲁棒性。",
      "motivation": "大型多模态模型（LMMs）在视频推理中通过思维链展现出强大能力，但其推理链的鲁棒性存在问题。具体来说，当出现文本幻觉时，模型会盲目遵循错误文本，忽视视觉证据，这种现象称为文本惯性。现有方法缺乏有效的自我纠正机制，导致错误传播，影响推理的准确性和可靠性。因此，需要研究如何提高LMMs在跨模态冲突下的推理一致性，以确保其在复杂场景中的稳定表现。",
      "method": "论文提出逻辑图扰动协议，通过结构性地向不同LMMs的推理链注入扰动，评估模型的自我反思能力。关键创新是引入了主动视觉上下文精炼，这是一种无需训练的推理范式。它包括主动视觉重接地机制，用于细粒度验证视觉证据，以及自适应上下文精炼策略，用于总结和去噪推理历史。这种方法旨在强制模型在推理过程中重新审视视觉信息，减少对错误文本的依赖，从而增强推理的鲁棒性。",
      "result": "实验结果表明，基线LMMs在出现文本幻觉时，成功自我纠正的情况少于10%，显示出明显的文本惯性问题。而提出的主动视觉上下文精炼方法能显著抑制幻觉传播，提高推理的鲁棒性。摘要未明确说明具体对比数据，但实验证实了该方法在减少错误传播方面的有效性，表明其在实际应用中具有潜力，尤其是在需要高可靠性的多模态推理任务中。",
      "conclusion": "本文的主要贡献是识别了大型多模态模型中的文本惯性问题，并提出了一种无需训练的推理范式来缓解这一问题。该方法通过主动视觉重接地和上下文精炼，增强了模型在跨模态冲突下的推理一致性。学术上，这深化了对LMMs推理机制的理解，并提供了改进鲁棒性的新途径。实际应用中，可提升多模态系统的可靠性。未来工作可能包括扩展到更多模态或复杂推理场景，以及探索训练方法以进一步提高性能。",
      "tags": [
        "Large Multimodal Models",
        "Chain-of-Thought",
        "Self-Reflection",
        "Visual Grounding",
        "Hallucination Mitigation"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:55.618256Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04068",
    "title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models",
    "authors": [
      "Zitong Huang",
      "Kaidong Zhang",
      "Yukang Ding",
      "Chao Gao",
      "Rui Ding",
      "Ying Chen",
      "Wangmeng Zuo"
    ],
    "abstract": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04068.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04068",
    "published": "2026-01-07T16:32:17Z",
    "updated": "2026-01-07T16:32:17Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "本研究提出了LocalDPO，一种基于局部偏好对的后训练框架，旨在高效优化视频扩散模型与人类偏好的对齐，通过细粒度时空区域学习提升生成质量。",
      "motivation": "视频生成中，将文本到视频扩散模型与人类偏好对齐对生成高质量视频至关重要。现有直接偏好优化（DPO）方法依赖多样本排名和任务特定评论家模型，效率低下且常产生模糊的全局监督，难以精确处理时空细节。这导致视频真实性和连贯性不足，因此需要一种更高效、细粒度的对齐方法来解决这些问题。",
      "method": "论文提出LocalDPO框架，包括自动化收集偏好对的管道：以高质量真实视频为正样本，通过应用随机时空掩码局部损坏生成负样本，仅使用冻结的基础模型恢复掩码区域。核心创新是区域感知的DPO损失，将偏好学习限制在损坏的时空区域，加速收敛，无需外部评论家模型或手动标注，从而在单次推理中完成偏好对构建。",
      "result": "在Wan2.1和CogVideoX数据集上的实验显示，LocalDPO相比其他后训练方法，一致提高了视频保真度、时间连贯性和人类偏好分数，证明了其在视频生成对齐中的有效性。尽管摘要未提供具体数值，但强调了性能改进，验证了该方法的效率和质量优势。",
      "conclusion": "LocalDPO的主要贡献是建立了更高效和细粒度的视频生成器对齐范式，通过局部偏好优化提高了模型性能，为视频扩散模型的后训练提供了新思路。其学术价值在于引入了区域级优化技术，实际应用可提升视频生成质量。未来工作可能包括扩展到其他生成任务或处理更复杂场景，摘要未明确说明局限性。",
      "tags": [
        "Video Diffusion Models",
        "Direct Preference Optimization",
        "Spatio-temporal Localization",
        "Region-aware Loss",
        "Post-training Alignment"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:53.819866Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04060",
    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
    "authors": [
      "Jinwei Su",
      "Qizhen Lan",
      "Zeyu Wang",
      "Yinghui Xia",
      "Hairu Wen",
      "Yiqun Duan",
      "Xi Xiao",
      "Tianyu Shi",
      "Yang Jingsong",
      "Lewei He"
    ],
    "abstract": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04060.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04060",
    "published": "2026-01-07T16:24:01Z",
    "updated": "2026-01-07T16:24:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了ComfySearch代理框架，通过验证引导自主构建ComfyUI工作流，以解决模块化工作流生成中的低通过率和质量限制问题。",
      "motivation": "随着AI生成内容从单体模型转向模块化工作流，ComfyUI等平台允许定制复杂创意管道，但组件数量众多且在严格图约束下保持长视野结构一致性困难，这导致工作流通过率低、质量有限。现有方法在应对这些挑战时表现不足，尤其是在复杂和创意任务上，因此迫切需要开发更有效的工作流生成框架来提升可执行性和泛化能力。",
      "method": "ComfySearch是一个代理框架，通过有效探索ComfyUI的组件空间，结合验证引导的工作流构建方法生成功能性的管道。其核心创新在于自主探索机制和验证步骤，利用代理技术优化结构一致性和可执行性。摘要未明确说明具体数据集和模型架构，但框架侧重于组件搜索和实时验证，以在严格图约束下确保工作流质量。",
      "result": "实验显示，ComfySearch在复杂和创意任务上大幅优于现有方法，实现了更高的可执行性（通过）率、更高的解决方案率和更强的泛化能力。摘要未提供具体数值，但与基线方法相比，其在提升工作流质量和效率方面表现显著，验证了框架的优越性。",
      "conclusion": "本研究的主要贡献是提出了ComfySearch，一个能自主探索和构建ComfyUI工作流的代理框架，学术上为模块化AI内容生成提供了新思路，实际应用中可改善创意内容生成的效率和质量。局限性或未来工作方向摘要未明确说明，但可能涉及扩展框架到其他平台或优化探索算法以进一步提升性能。",
      "tags": [
        "ComfyUI",
        "Workflow Generation",
        "Agentic Framework",
        "Validation-Guided Construction",
        "Autonomous Exploration"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:51.211546Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04160",
    "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
    "authors": [
      "Yuechen Jiang",
      "Zhiwei Liu",
      "Yupeng Cao",
      "Yueru He",
      "Ziyang Xu",
      "Chen Xu",
      "Zhiyang Deng",
      "Prayag Tiwari",
      "Xi Chen",
      "Alejandro Lopez-Lira",
      "Jimin Huang",
      "Junichi Tsujii",
      "Sophia Ananiadou"
    ],
    "abstract": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
    "categories": [
      "cs.CL",
      "cs.CE",
      "q-fin.CP"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04160.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04160",
    "published": "2026-01-07T18:18:28Z",
    "updated": "2026-01-07T18:18:28Z",
    "comment": "39 pages; 24 figures",
    "light_analysis": {
      "overview": "本研究提出了RFC Bench基准，用于评估大语言模型在金融虚假信息检测中的性能，特别关注无参考推理的挑战。",
      "motivation": "金融虚假信息在现实新闻环境中具有高度上下文复杂性，通常需要从分散的线索中提取含义。当前的大语言模型在无参考设置下可能难以维持一致信念状态，导致检测不准确，现有方法未充分评估这种局限性。因此，需要一个专门基准来系统识别和解决这些问题，以提升金融信息处理的可靠性。",
      "method": "论文提出了RFC Bench基准，该基准在段落级别操作，专门捕捉金融新闻的上下文复杂性。它定义了两个互补任务：无参考虚假信息检测，要求模型独立判断段落真实性；以及比较诊断，通过配对原始和扰动输入来评估模型。基准设计强调分散线索的整合，提供了一个结构化框架来测试大语言模型在金融领域的推理能力。",
      "result": "实验结果显示了一个一致模式：当模型可以访问比较上下文时，性能显著更强；而在无参考设置下，模型暴露了显著弱点，包括预测不稳定和无效输出增多。这表明当前模型在没有外部基础的情况下，难以维持一致信念状态，基准测试凸显了这些局限性，为后续改进提供了数据支撑。",
      "conclusion": "RFC Bench基准的主要贡献是提供了一个结构化测试床，用于研究无参考推理和大语言模型在金融虚假信息检测中的表现。它揭示了模型在无参考环境中的不足，推动学术社区开发更可靠的检测方法。未来工作可能包括扩展基准到更多领域或优化模型架构，以提升实际应用价值。",
      "tags": [
        "Large Language Model",
        "Benchmark",
        "Financial Misinformation Detection",
        "Reference-Free Reasoning",
        "Contextual Analysis"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:35.654703Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04157",
    "title": "FLEx: Language Modeling with Few-shot Language Explanations",
    "authors": [
      "Adar Avsian",
      "Christopher Richardson",
      "Anirudh Sundar",
      "Larry Heck"
    ],
    "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04157.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04157",
    "published": "2026-01-07T18:12:05Z",
    "updated": "2026-01-07T18:12:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出FLEx方法，利用少量语言解释示例通过嵌入聚类选择错误并总结为提示前缀，在推理时引导语言模型避免类似错误，无需修改模型权重。",
      "motivation": "语言模型在数学问题求解和开放域问答等任务中表现良好，但仍会犯错误，且这些错误在相关查询中重复出现。自然语言解释可帮助纠正错误，但大规模收集困难，尤其在需要专家注释的领域，如专业学科任务，导致现有方法难以高效利用少量解释资源。本研究旨在解决如何通过少量解释性示例有效改进模型行为，避免重复错误，提升模型可靠性和实用性。",
      "method": "FLEx方法核心包括三个步骤：首先使用嵌入聚类技术选择语言模型产生的代表性错误；其次验证相关自然语言解释是否能纠正这些错误；最后将这些解释总结成一个提示前缀，在推理时添加到新输入前。关键创新在于利用少量解释示例，不修改模型权重，通过提示工程引导模型行为。该方法在CounterBench、GSM8K和ReasonIF等数据集上进行评估，结合了聚类算法和提示生成技术，以实现错误纠正和性能提升。",
      "result": "在CounterBench、GSM8K和ReasonIF三个数据集上，FLEx方法一致优于链式思维（CoT）提示，展现了更好的模型性能。具体来说，FLEx能减少高达83%的CoT方法所剩余的误差，显著提升了任务准确率。与基线CoT方法相比，FLEx通过少量解释示例有效降低了错误发生率，证明了其在改善语言模型行为方面的有效性，尤其是在处理重复错误和复杂查询时表现突出。",
      "conclusion": "本研究主要贡献是提出FLEx方法，通过少量语言解释改进语言模型行为，解决了大规模收集解释的挑战，具有实际应用价值，特别是在需要专家输入的领域。学术上，FLEx扩展了提示工程和少量学习在语言模型中的应用，为模型纠错提供了新思路。未来工作可探索该方法在其他数据集或任务中的泛化性，以及结合更多解释类型以进一步优化性能。摘要未明确说明具体局限性，但可能涉及对解释质量或聚类方法的依赖性。",
      "tags": [
        "Few-shot Learning",
        "Language Modeling",
        "Prompt Engineering",
        "Embedding Clustering",
        "Chain-of-Thought Prompting"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:40.445558Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04135",
    "title": "LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation",
    "authors": [
      "Leonardo Bottona",
      "Nicolò Penzo",
      "Bruno Lepri",
      "Marco Guerini",
      "Sara Tonelli"
    ],
    "abstract": "We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04135.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04135",
    "published": "2026-01-07T17:49:17Z",
    "updated": "2026-01-07T17:49:17Z",
    "comment": "9 pages, 3 figures",
    "light_analysis": {
      "overview": "LLMberjack 是一个结合交互式树可视化和大型语言模型辅助的平台，用于从辩论树生成连贯的多方对话。",
      "motivation": "研究动机是应对创建多方对话资源的缺乏，尤其是在自然语言处理和社会科学领域，现有方法通常依赖人工构建，效率低下且难以保持参与者身份和话语关系的连贯性。LLMberjack 旨在通过提供可视化工具和自动化支持，简化对话生成流程，弥补资源缺口，促进对话数据的有效创建和应用。摘要指出缺乏这类资源，这凸显了开发新工具的重要性。",
      "method": "研究方法基于 LLMberjack 平台，它提供交互式界面可视化辩论树作为回复树结构，让用户选择和修剪分支以构建线性对话序列，同时保留身份和关系。核心创新是集成可选的大型语言模型（LLM），用于自动编辑消息内容和说话者描述，结合人工交互实现半自动化对话生成。该方法利用树可视化促进用户理解，并通过 LLM 辅助减少手动编辑负担，强调透明和可复制的工作流程。",
      "result": "主要实验结果显示，树可视化有助于用户创建连贯、有意义的对话线程，LLM 支持则提高了输出质量并减少了人力投入。摘要未明确提供具体性能指标，如准确率或效率数据，但基于平台演示强调了实用性和改进效果，表明该工具在生成高质量对话数据方面具有潜力，与基线方法相比，通过自动化辅助优化了工作流程。",
      "conclusion": "论文的主要贡献是开发了开源的 LLMberjack 平台，它提供了一种透明、可复制的方法来创建多方对话，解决了资源缺乏问题。学术价值在于推动了对话生成和可视化交互技术的研究，实际应用可扩展到 AI 训练数据、社交媒体分析等领域。局限性可能包括对 LLM 的依赖性和可扩展性，未来工作可进一步优化算法或评估框架以增强功能。",
      "tags": [
        "Large Language Model",
        "Multi-Party Conversation",
        "Tree Visualization",
        "Interactive Interface",
        "Conversation Generation"
      ]
    },
    "analyzed_at": "2026-01-08T17:35:57.371497Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04093",
    "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Mingfeng Li",
      "Zheming Yang",
      "Chiwei Zhu",
      "Fei Ma",
      "Benfeng Xu",
      "Min Liu"
    ],
    "abstract": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \\textbf{\\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04093.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04093",
    "published": "2026-01-07T16:59:34Z",
    "updated": "2026-01-07T16:59:34Z",
    "comment": "We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources",
    "light_analysis": {
      "overview": "提出SearchAttack方法，通过将有害语义外包给网络搜索，有效攻击搜索增强的大型语言模型，暴露其安全漏洞。",
      "motivation": "近年来，大型语言模型在开放和知识密集型任务中的不可靠性差距引发关注，搜索增强的LLMs被引入以提升可靠性。然而，当搜索引擎用于有害任务时，返回内容可能直接包含恶意信息，而LLM的安全防护无法阻止，这揭示了集成外部搜索时的安全风险。现有方法往往忽视这一结合点，使得搜索功能成为潜在攻击面，因此本研究旨在探索和评估这种漏洞，促进更安全的系统设计。",
      "method": "SearchAttack方法的核心是识别网络搜索为关键攻击面，设计攻击策略将有害语义外包给搜索引擎。它仅保留查询的骨架和碎片化线索，避免直接触发LLM防护，然后通过结构化准则（如特定提示模板）引导LLM重构检索到的内容以实现恶意目标。该方法利用现有的搜索增强LLM架构进行测试，关键创新在于通过查询重构间接引入有害信息，测试系统在真实世界威胁场景中的脆弱性。",
      "result": "论文通过广泛实验对搜索增强的LLMs进行红队测试，经验结果表明SearchAttack在攻击这些系统时表现出强大有效性，能够成功诱导有害内容生成。尽管摘要未明确提供具体性能指标如攻击成功率或基线对比数据，但实验证实了该方法在模拟真实威胁时的实用价值，为负责任的安全漏洞评估提供了实证基础，突显了搜索增强系统在当前防护下的不足。",
      "conclusion": "本研究的主要贡献是提出SearchAttack方法，用于红队测试搜索增强LLMs，揭示了网络搜索作为关键攻击面的安全漏洞。这具有重要学术价值，推动了AI安全领域对混合系统风险的研究，并具有实际应用价值，可用于系统安全评估和加固。未来工作可能包括扩展攻击场景、改进防护机制或探索更多攻击向量，以进一步提升系统安全性。",
      "tags": [
        "Large Language Models",
        "Web Search",
        "Red-Teaming",
        "Security Vulnerability",
        "Attack Surface"
      ]
    },
    "analyzed_at": "2026-01-08T17:37:01.325877Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04086",
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "authors": [
      "Jinbo Hao",
      "Kai Yang",
      "Qingzhen Su",
      "Yifan Li",
      "Chao Jiang"
    ],
    "abstract": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04086.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04086",
    "published": "2026-01-07T16:54:20Z",
    "updated": "2026-01-07T16:54:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出KDCM框架，通过代码引导的显式推理结构有效减少大语言模型中的幻觉。",
      "motivation": "大语言模型在处理复杂提示时经常产生幻觉，尤其是当推理过程缺乏外部知识引导时，导致输出错误或不可靠信息。这严重影响了模型在实际应用中的可信度，例如在问答或决策支持系统中。现有方法往往依赖于模型内部参数，忽视了显式推理结构的整合，使得幻觉问题难以有效控制。因此，本研究旨在开发一种新方法，专注于减少由提示引起的幻觉，通过引入结构化知识探索来增强推理的可靠性。",
      "method": "本研究提出KDCM框架，扩展了链式知识蒸馏方法，融入可编程模块以指导知识图谱探索。关键创新在于将可执行代码嵌入推理提示中，使模型在推理过程中能动态访问外部结构化知识。这一设计形成了增强的蒸馏推理框架，明确规范中间步骤，确保逻辑一致性和透明性。该方法利用代码的灵活性引导模型思考，减少了因提示不当导致的错误推理。实验中使用GPT-4和LLaMA-3.3模型进行评估，以验证其在不同基准上的性能。",
      "result": "实验在多个公共基准上进行，结果表明代码引导的推理显著改善了上下文建模，并减少了提示引起的幻觉。具体数据方面，HIT@1、HIT@3和HIT@5的得分分别提升了15.64%、13.38%和13.28%，在多个评估设置中均超过95%。与基线方法相比，该方法在准确性和可靠性方面有显著提高，显示了显式推理结构在约束错误推理方面的有效性。这些结果证实了所提框架能同时提升模型的准确性和可解释性，为减少幻觉问题提供了实证支持。",
      "conclusion": "本研究的主要贡献是提出了KDCM框架，通过代码引导的显式推理结构有效减少大语言模型的幻觉，提高了准确性和可解释性。学术上，该方法为控制模型推理过程提供了新思路；应用上，增强了LLM在现实任务中的可靠性。摘要未明确说明局限性，但未来工作可扩展到更多模型或任务，进一步优化推理模块。总体而言，该研究为大语言模型的可靠部署提供了有价值的工具。",
      "tags": [
        "Large Language Models",
        "Knowledge Distillation",
        "Reasoning Structures",
        "Hallucination Reduction",
        "Code-Guided Reasoning"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:59.462529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04056",
    "title": "Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion",
    "authors": [
      "Yuanfeng Xu",
      "Yuhao Chen",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "abstract": "The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \\textbf{CoM-DAD} (\\textbf{Co}upled \\textbf{M}anifold \\textbf{D}iscrete \\textbf{A}bsorbing \\textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \\textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \\textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04056.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04056",
    "published": "2026-01-07T16:21:19Z",
    "updated": "2026-01-07T16:21:19Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "CoM-DAD 通过耦合连续语义扩散和离散标记吸收扩散，解决了多模态生成的统一性问题，建立新的文本图像生成范式。",
      "motivation": "研究动机在于生成建模中离散数据的自回归方法与连续数据的扩散方法之间存在鸿沟，阻碍了统一多模态系统的发展。现有掩码语言模型（MLMs）虽高效，但缺乏自回归模型的生成保真度和扩散模型的语义连续性。扩展到多模态设置时，会引入严重对齐挑战和训练不稳定性，因此急需一个稳定、统一的框架来弥合这一差距，推动AI系统进步。",
      "method": "CoM-DAD 是一个层次化双过程框架，将多模态生成分为高级语义规划和低级标记合成。首先，通过连续潜在扩散过程建模语义流形；其次，使用离散吸收扩散过程生成标记，受变量率噪声调度调节，并以演化语义先验为条件。关键创新是引入随机混合模态传输策略，对齐不同模态，无需繁重的对比双编码器，数据集和模型架构摘要未明确说明。",
      "result": "论文表明，CoM-DAD 在稳定性上优于标准掩码建模，为可扩展的统一文本图像生成建立了新范式。具体性能指标摘要未明确说明，但强调方法提高了训练稳定性和生成效果，与基线方法相比展现更优潜力，为实际应用提供基础。",
      "conclusion": "CoM-DAD 的主要贡献是提出了一个统一概率框架，有效弥合离散与连续数据的生成鸿沟。该研究提升了多模态生成的稳定性和质量，具有重要学术价值，为可扩展统一生成系统的发展开辟新途径。未来工作可包括扩展到更多模态或优化具体应用，摘要未明确说明局限性。",
      "tags": [
        "Multimodal Generation",
        "Diffusion Models",
        "Discrete Modeling",
        "Continuous Latent Diffusion",
        "Stochastic Mixed-Modal Transport"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:45.848028Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04055",
    "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
    "authors": [
      "Prith Sharma",
      "Austin Z. Henley"
    ],
    "abstract": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04055.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04055",
    "published": "2026-01-07T16:20:08Z",
    "updated": "2026-01-07T16:20:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了模块化提示优化（MPO）框架，通过固定提示模式和局部文本梯度优化，提升小规模开源语言模型的推理性能。",
      "motivation": "研究动机在于提示质量对大型语言模型（LLMs）的行为和推理性能至关重要，特别是对于依赖显式结构的小规模开源指令调优模型。现有自动提示优化方法常将提示视为整体文本块，导致难以定位错误、保留关键指令或控制提示增长，限制了优化效果和可解释性。因此，开发一种能够保持提示结构同时进行局部优化的方法具有重要实用价值，以解决现有方法在错误定位和可控性方面的不足。",
      "method": "论文提出模块化提示优化（MPO）框架，将提示视为结构化对象，包含固定语义部分如系统角色、上下文、任务描述、约束和输出格式。MPO使用批评语言模型生成局部文本梯度，独立优化每个部分，同时保持整体提示模式固定。通过去重整合更新来减少冗余和组件间干扰，实现可解释且健壮的优化过程。该方法在ARC-Challenge和MMLU推理基准上评估，使用LLaMA-3 8B-Instruct和Mistral-7B-Instruct作为求解模型。",
      "result": "实验结果显示，在ARC-Challenge和MMLU基准测试中，MPO一致优于未调整的结构化提示和TextGrad基线方法，实现了显著的准确率提升。具体地，使用两个不同模型（LLaMA-3 8B-Instruct和Mistral-7B-Instruct）时，MPO均展现出更好的性能，且未修改模型参数或改变提示结构。这些结果表明局部优化策略在提升推理准确性方面是有效的。",
      "conclusion": "结论表明，保持固定提示模式并应用局部、部分优化的方法是提升小规模开源语言模型推理性能的有效途径。MPO框架提供了可解释和健壮的优化过程，具有学术价值，如改进模型可靠性和用户控制，以及实际应用价值，如优化小规模模型的推理任务。未来工作可能包括扩展该框架到更多模型类型和任务，或探索其他优化技术，进一步验证其通用性。",
      "tags": [
        "Large Language Models",
        "Prompt Optimization",
        "Textual Gradients",
        "Structured Prompts",
        "Critic Model"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:52.477242Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04194",
    "title": "Choreographing a World of Dynamic Objects",
    "authors": [
      "Yanzhe Lyu",
      "Chen Geng",
      "Karthik Dharmarajan",
      "Yunzhi Zhang",
      "Hadi Alzayer",
      "Shangzhe Wu",
      "Jiajun Wu"
    ],
    "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04194.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04194",
    "published": "2026-01-07T18:59:40Z",
    "updated": "2026-01-07T18:59:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出CHORD，一个通用生成管道，通过蒸馏从2D视频提取拉格朗日运动信息，实现动态对象和场景的4D生成。",
      "motivation": "该研究旨在解决4D（3D+时间）世界中动态对象的生成问题，这些对象不断变化、变形和交互，形成复杂场景。现有传统基于规则的方法依赖类别特定启发式规则，劳动密集型且不可扩展；而基于学习的方法通常需要大规模数据集，可能无法覆盖所有感兴趣的对象类别，限制了通用性。生成动态场景在计算机图形学、机器人学等领域有重要应用价值。",
      "method": "CHORD方法是一个基于蒸馏的生成管道，从2D视频的欧拉表示中提取隐藏的拉格朗日运动信息，从而继承视频生成模型的通用性。该方法的核心创新在于将欧拉视角（如像素变化）转化为拉格朗日视角（如对象运动），实现类别无关的动态场景合成，无需大规模特定数据集。摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验展示了CHORD在生成多种多体4D动态场景方面的有效性，相比现有方法显示出优势，如提高了生成多样性和准确性，但摘要未明确说明具体性能指标如准确率。此外，方法被应用于生成机器人操作策略，证明了其实用性和泛化能力。",
      "conclusion": "CHORD提供了一种通用、多用途的动态生成方法，克服了传统规则和基于学习方法的局限性，具有学术价值，推动生成建模在图形学和机器人学中的应用。潜在局限性可能包括对视频质量的依赖或处理更复杂交互的挑战，未来工作可扩展至更广泛场景。",
      "tags": [
        "Video Generation Models",
        "Distillation-based Pipeline",
        "Lagrangian Motion",
        "4D Dynamics",
        "Generative Pipeline"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:39.041905Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04185",
    "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
    "authors": [
      "Xudong Jiang",
      "Fangjinhua Wang",
      "Silvano Galliani",
      "Christoph Vogel",
      "Marc Pollefeys"
    ],
    "abstract": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04185.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04185",
    "published": "2026-01-07T18:51:51Z",
    "updated": "2026-01-07T18:51:51Z",
    "comment": "Code will be available at https://github.com/cvg/Hierarchical-Localization",
    "light_analysis": {
      "overview": "该论文提出ImLoc方法，通过增强2D图像的深度估计和密集匹配，实现视觉定位的高准确度与高效存储计算，兼顾几何推理与更新便利性。",
      "motivation": "视觉定位在机器人导航和增强现实等领域应用广泛，但现有方法存在局限：2D图像基础方法易于构建和维护，但几何推理能力不足；3D结构基础方法虽准确，却需集中重建且难以更新。这种矛盾导致实际应用中难以平衡准确性与灵活性。因此，该研究旨在开发一种新方法，结合两者优势，解决几何推理和可维护性问题，提升在动态环境中的实用性。",
      "method": "研究方法采用2D图像基础表示，为每个图像估计深度图以捕获几何结构，关键创新是结合密集匹配器进行高效匹配。通过紧凑压缩技术和GPU加速的LO-RANSAC实现，整个流水线在存储和计算上高效，允许在准确度和内存效率间灵活权衡。此外，该方法避免了复杂3D重建，简化构建和维护，支持从图像到姿态的完整流程优化。",
      "result": "在标准基准测试中，ImLoc方法达到新的state-of-the-art准确度，在挑战性条件下表现出色。与现有内存高效方法相比，在可比地图大小下，ImLoc显著优于对手，验证了其性能优势。摘要未提供具体数据如准确率百分比，但强调了在多个基准上的领先地位，证明该方法在准确性和效率方面的突破。",
      "conclusion": "该论文的主要贡献是提出ImLoc方法，融合2D图像的易维护性和深度图的几何准确性，实现高准确度与高效存储计算的平衡。这在学术上推进了视觉定位技术，具有实际应用价值，如增强现实和机器人导航。未来工作可能涉及深度估计优化或场景扩展，但摘要未明确说明具体局限性。",
      "tags": [
        "Visual Localization",
        "Image-based Representation",
        "Depth Estimation",
        "Dense Matching",
        "GPU Acceleration"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:56.315251Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04159",
    "title": "ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography",
    "authors": [
      "Vladimir Frants",
      "Sos Agaian",
      "Karen Panetta"
    ],
    "abstract": "Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04159.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04159",
    "published": "2026-01-07T18:15:09Z",
    "updated": "2026-01-07T18:15:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ToTMNet，一种轻量级远程光电容积描记架构，通过FFT加速的Toeplitz时间混合层替代时间注意力，实现高效时间建模。",
      "motivation": "远程光电容积描记（rPPG）用于从面部视频估计心率，在资源受限设备上应用时需平衡准确性和效率。现有深度模型虽提高鲁棒性，但计算成本和参数数量增加，基于注意力的时间建模引入时间长度二次缩放，导致效率低下。这限制了rPPG在实时或边缘设备上的部署，因此需开发轻量级替代方案以解决这些瓶颈，提升实用性和可扩展性。",
      "method": "论文提出ToTMNet架构，核心方法是使用FFT加速的Toeplitz时间混合层替代传统注意力机制。Toeplitz运算符通过循环嵌入和基于FFT的卷积实现，以线性参数数量提供全序列时间感受野，并在近线性时间内计算，提高效率。架构整合局部深度时间卷积分支和门控全局Toeplitz混合，构成紧凑门控时间混合器，总参数仅63k，支持高效长距离时间滤波，优化资源使用。",
      "result": "实验在UBFC-rPPG（真实视频）和SCAMPS（合成视频）数据集上进行，显示ToTMNet在轻量设计下达到高精度心率估计。UBFC-rPPG数据集内评估中，平均绝对误差为1.055 bpm，皮尔逊相关性为0.996；合成到真实设置下，平均绝对误差为1.582 bpm，皮尔逊相关性为0.994。消融实验确认门控机制对全局Toeplitz混合的有效性至关重要，尤其在领域转移场景中，表明模型在紧凑性下保持了强性能。",
      "conclusion": "本研究证明Toeplitz结构化时间混合是注意力机制在rPPG中的实用高效替代品，主要贡献在于提出ToTMNet轻量级架构，平衡了准确性和计算成本。学术价值在于为时间建模提供了新方法，实际应用价值在于促进rPPG在资源受限环境下的部署。局限性是仅使用两个数据集，未来工作可扩展到更多数据集验证泛化能力，并探索在其他时序任务中的应用潜力。",
      "tags": [
        "Remote Photoplethysmography",
        "Toeplitz Operator",
        "FFT-based Convolution",
        "Temporal Mixing",
        "Gating Mechanism"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:40.364252Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04153",
    "title": "Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning",
    "authors": [
      "Yifan Wang",
      "Yanyu Li",
      "Sergey Tulyakov",
      "Yun Fu",
      "Anil Kag"
    ],
    "abstract": "Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04153.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04153",
    "published": "2026-01-07T18:05:08Z",
    "updated": "2026-01-07T18:05:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Diffusion-DRF，一种使用冻结视觉语言模型作为无训练批评器的可微分奖励流，用于微调视频扩散模型，以提升生成质量和训练稳定性。",
      "motivation": "该研究旨在解决文本到视频生成中直接偏好优化方法依赖不可微分偏好信号的问题。现有方法依赖于人工标注或学习到的奖励模型，导致训练过程标签密集、易偏置和易受攻击，常触发奖励破解和不稳定训练。这个问题的重要性在于影响生成模型的效率和鲁棒性，阻碍了高质量视频生成的进一步发展。因此，需要一种更稳定、高效的微调方法来解决这些不足。",
      "method": "Diffusion-DRF采用冻结的现成视觉语言模型作为无训练批评器，通过扩散去噪链直接反向传播其反馈，将逻辑级响应转换为令牌感知梯度进行优化。核心创新包括设计自动化、方面结构化的提示管道，以获取可靠的多维VLM反馈；同时利用梯度检查点技术，实现通过最终去噪步骤的高效更新。该方法无需额外奖励模型或偏好数据集，具有模型无关性，易于推广到其他基于扩散的生成任务，突出了可微分奖励流的技术特色。",
      "result": "Diffusion-DRF显著提高了视频生成的质量和语义对齐，有效缓解了奖励破解和训练崩溃问题。与依赖非可微分偏好信号的基线方法相比，该方法无需额外的奖励模型或偏好数据集，减少了训练成本和复杂性。摘要未明确说明具体性能指标如准确率或效率数值，但基于描述推断，该方法在优化过程中表现出更稳定的训练行为和改进的生成效果。这突显了其在实际应用中的优势。",
      "conclusion": "Diffusion-DRF的主要贡献是提出了一种可微分奖励流，用于视频扩散模型的微调，在不依赖额外资源的情况下提升生成质量并缓解训练不稳定。学术价值在于引入了基于VLM的无训练批评机制，推动了可微分优化在生成模型中的应用；实际应用价值在于其模型无关性和通用性，可推广到其他扩散生成任务。摘要未明确说明局限性或未来工作方向，但潜在方向可能包括进一步扩展应用到更多生成场景或优化提示管道。",
      "tags": [
        "Diffusion Models",
        "Text-to-Video Generation",
        "Vision-Language Models",
        "Differentiable Reward Flow",
        "Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:43.699167Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04121",
    "title": "MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis",
    "authors": [
      "Gabriel Ansah",
      "Eden Ruffell",
      "Delmiro Fernandez-Reyes",
      "Petru Manescu"
    ],
    "abstract": "Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning framework for white blood cell morphology analysis that enables collaborative training across institutions without exchanging training data. Using blood films from multiple clinical sites, our federated models learn robust, domain-invariant representations while preserving complete data privacy. Evaluations across convolutional and transformer-based architectures show that federated training achieves strong cross-site performance and improved generalization to unseen institutions compared to centralized training. These findings highlight federated learning as a practical and privacy-preserving approach for developing equitable, scalable, and generalizable medical imaging AI in resource-limited healthcare environments.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04121.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04121",
    "published": "2026-01-07T17:32:24Z",
    "updated": "2026-01-07T17:32:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于联邦学习的跨机构血液形态分析框架，实现隐私保护下的强泛化性能。",
      "motivation": "研究动机在于，自动血液形态分析在低收入和中等收入国家（LMICs）中对于血液学诊断至关重要，但常因染色变异、成像差异和罕见形态导致数据集偏移，影响模型可靠性。由于严格的隐私法规和数据共享限制，构建集中式数据集以捕获这种多样性变得不可行，现有方法无法有效处理跨机构数据协同训练问题。因此，需要开发一种隐私保护的分布式学习方案，以解决数据孤岛和泛化挑战，支持资源有限医疗环境中的公平AI应用。",
      "method": "论文提出了名为MORPHFED的联邦学习框架，专门用于白细胞形态分析，使多个临床机构能在不交换原始训练数据的情况下进行协作训练。核心创新在于利用血液涂片数据学习鲁棒且域不变的表示，同时保持完全数据隐私。方法通过结合联邦学习机制，集成多个临床站点数据，并采用卷积神经网络和transformer-based架构进行技术验证，以评估框架的通用性和适应性，从而在医学图像分析中实现隐私保护下的模型优化。",
      "result": "实验结果表明，联邦学习框架在血液形态分析中展现出显著优势。与集中式训练相比，联邦训练在卷积和transformer-based架构上均实现了更强的跨站点性能和更好的泛化能力，能够适应未见过的临床机构。摘要未明确说明具体的准确率或效率改进数值，但通过跨机构评估，证实了联邦模型在保持数据隐私的同时，提高了模型的鲁棒性和泛化效果，优于传统集中式方法，为医疗AI提供了实用解决方案。",
      "conclusion": "论文的主要贡献是提出并验证了联邦学习框架，用于跨机构血液形态分析，确保数据隐私的同时增强模型泛化能力，学术上展示了联邦学习在医学图像分析中的潜力，为解决数据隐私和泛化问题提供了新途径。实践上，它为资源有限医疗环境中的公平、可扩展和可泛化AI开发提供了可行方案。未来工作方向可能涉及进一步优化联邦学习策略以处理更复杂的数据集偏移，摘要未明确说明局限性，但强调了该方法在资源受限场景中的实用性。",
      "tags": [
        "Federated Learning",
        "Medical Imaging AI",
        "Domain-invariant Learning",
        "Convolutional Neural Networks",
        "Transformers"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:49.484226Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04118",
    "title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning",
    "authors": [
      "Wenshuai Li",
      "Xiantai Xiang",
      "Zixiao Wen",
      "Guangyao Zhou",
      "Ben Niu",
      "Feng Wang",
      "Lijia Huang",
      "Qiantong Wang",
      "Yuxin Hu"
    ],
    "abstract": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04118.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04118",
    "published": "2026-01-07T17:26:41Z",
    "updated": "2026-01-07T17:26:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出GeoReason框架，通过逻辑一致性强化学习解决远程感知视觉-语言模型中的逻辑幻觉问题，提升推理可靠性和可解释性。",
      "motivation": "当前远程感知视觉-语言模型在复杂空间任务中存在逻辑幻觉问题，即正确答案源自错误的推理链或依赖位置捷径而非空间逻辑，这削弱了模型在战略决策中的可靠性。研究旨在推动模型从感知为中心转向高级演绎推理，以增强认知可靠性，现有方法在推理链可靠性方面不足，限制了实际应用。",
      "method": "GeoReason框架采用两阶段训练策略：首先通过监督知识初始化，使用GeoReason-Bench数据集（包含4,000个基于几何原语和专家知识的推理轨迹）教模型推理语法和领域知识；其次通过一致性感知强化学习，引入逻辑一致性奖励，该奖励采用选项排列策略惩罚逻辑漂移，优化演绎推理的可靠性，确保内部思考与最终决策同步。",
      "result": "实验结果显示，GeoReason框架显著提升了远程感知视觉-语言模型的认知可靠性和可解释性，相比其他先进方法实现了最先进的性能。摘要未明确说明具体性能指标如准确率，但强调了在复杂推理任务中的有效性提升。",
      "conclusion": "GeoReason框架通过逻辑一致性强化学习同步思考和决策，主要贡献是提高了远程感知视觉-语言模型的推理可靠性。研究意义在于增强了模型在复杂空间任务中的可解释性和可靠性，为后续相关应用提供了基础，未来工作可能聚焦于扩展数据集或更广泛的评估，但摘要未明确说明局限性。",
      "tags": [
        "Remote Sensing Vision-Language Models",
        "Reinforcement Learning",
        "Logical Consistency",
        "Deductive Reasoning",
        "Option Permutation Strategy"
      ]
    },
    "analyzed_at": "2026-01-08T17:35:35.369771Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04090",
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "authors": [
      "Jiaxin Huang",
      "Yuanbo Yang",
      "Bangbang Yang",
      "Lin Ma",
      "Yuewen Ma",
      "Yiyi Liao"
    ],
    "abstract": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04090.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04090",
    "published": "2026-01-07T16:57:30Z",
    "updated": "2026-01-07T16:57:30Z",
    "comment": "Project page: https://xdimlab.github.io/Gen3R/",
    "light_analysis": {
      "overview": "Gen3R方法通过桥梁基础重建模型和视频扩散模型的强先验，实现场景级3D生成，并生成对齐的RGB视频和3D几何。",
      "motivation": "该研究旨在解决场景级3D生成中的一致性和鲁棒性问题。当前方法在单独使用重建模型或生成模型时，往往无法充分利用两者的优势，导致生成结果在几何和外观上不一致。通过结合基础重建模型（如VGGT）的几何先验和视频扩散模型的外观先验，可以提升3D场景生成的质量，并为下游应用提供更可靠的输出。",
      "method": "Gen3R方法核心在于训练一个适配器，该适配器作用于VGGT重建模型的token上，生成几何潜变量。这些潜变量通过正则化与预训练视频扩散模型的外观潜变量对齐。通过联合生成这些解耦但对齐的潜变量，方法能够输出RGB视频和对应的3D几何，包括相机姿态、深度图和全局点云。关键创新是潜变量的对齐机制和适配器的设计。",
      "result": "实验结果显示，Gen3R在单图像和多图像条件下的3D场景生成任务中，取得了最先进的性能。具体来说，与基线方法相比，它在生成质量和一致性方面表现优异。摘要未明确说明具体数值，但强调了方法通过生成先验增强了重建的鲁棒性，展示了重建和生成模型紧密耦合的相互益处。",
      "conclusion": "Gen3R通过紧密结合重建和生成模型，提出了一种高效的3D场景生成方法，具有显著的学术和实际价值。它为计算机视觉和图形学领域提供了新的工具，可用于虚拟现实、游戏开发等应用。尽管摘要未提及局限性，但未来工作可能包括优化对齐过程或扩展至更复杂的场景。",
      "tags": [
        "3D Scene Generation",
        "Video Diffusion Models",
        "Reconstruction Models",
        "Latent Alignment",
        "Adapter Networks"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:00.819658Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04065",
    "title": "Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation",
    "authors": [
      "Raül Pérez-Gonzalo",
      "Riccardo Magro",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "abstract": "Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04065.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04065",
    "published": "2026-01-07T16:29:52Z",
    "updated": "2026-01-07T16:29:52Z",
    "comment": "Accepted to WACV 2026",
    "light_analysis": {
      "overview": "提出了一种基于无监督模块化自适应区域生长和RegionMix分类的标注高效风力涡轮机叶片分割方法，以减少对大量标注数据的依赖。",
      "motivation": "风力涡轮机的高效运行需要频繁检查，因为表面损伤会影响性能、降低输出并加速磨损，自动化检查的关键在于准确分割叶片。传统像素级深度学习方法需要大量标注数据，导致可扩展性差和成本高昂。本研究的动机是解决这一实际问题，通过减少标注需求，提升分割任务的效率和实际应用潜力。",
      "method": "该方法将像素级分割重构为二元区域分类问题。首先，采用无监督模块化自适应区域生长技术生成图像区域，结合自适应阈值指导和区域合并过程整合碎片。其次，引入RegionMix增强策略，通过组合不同区域合成新训练样本，提高分类模型的鲁棒性和泛化能力。",
      "result": "实验结果表明，该框架在风力涡轮机叶片分割任务中达到了最先进的准确率，并在不同风电场数据上展示了强的交叉站点泛化能力，优于传统像素级方法，尽管摘要未明确提供具体性能指标数字。",
      "conclusion": "本研究通过标注高效的分割方法，减少了传统深度学习的标注依赖，为风力涡轮机自动化检查提供了可扩展的解决方案。其学术价值在于引入无监督区域生成和增强技术，推动分割任务的技术创新，实际应用价值在于降低工业视觉任务的成本。未来工作可能涉及扩展至其他场景或进一步优化方法。",
      "tags": [
        "Unsupervised Learning",
        "Region Growing",
        "Data Augmentation",
        "Image Segmentation",
        "Binary Classification"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:50.210610Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04181",
    "title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition",
    "authors": [
      "Nia Touko",
      "Matthew O A Ellis",
      "Cristiano Capone",
      "Alessio Burrello",
      "Elisa Donati",
      "Luca Manneschi"
    ],
    "abstract": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, \"plug-and-play\" myoelectric control for long-term prosthetic use.",
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04181.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04181",
    "published": "2026-01-07T18:48:31Z",
    "updated": "2026-01-07T18:48:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种轻量级测试时间适应框架，结合时间卷积网络和自适应策略，以解决肌电图手势识别中的信号漂移问题，实现长期可靠的'即插即用'控制。",
      "motivation": "肌电图（EMG）手势识别在长期应用中面临电极移动、肌肉疲劳和姿势变化导致的信号漂移问题，导致模型在会话间准确率急剧下降。现有方法虽在会话内表现良好，但通常依赖大数据集或高计算资源，不适合资源受限的可穿戴设备。因此，研究旨在开发一种轻量级、高效的测试时间适应方案，以提升实际部署中的可靠性和适用性。",
      "method": "论文提出了一个基于时间卷积网络的轻量级测试时间适应框架。核心创新包括三种策略：使用因果自适应批归一化进行实时统计对齐；结合高斯混合模型对齐和经验回放以防止模型遗忘；以及采用元学习实现快速少量校准。这些策略在NinaPro DB6多会话数据集上进行评估，旨在测试时动态调整模型，适应信号变化，无需重新训练。",
      "result": "在NinaPro DB6数据集上的实验表明，该框架显著降低了会话间准确率差距，同时保持最小计算开销。经验回放策略在数据有限时展现出优越的稳定性；元学习在仅使用一两个样本的情况下达到竞争性性能，所需数据量远少于现有基准方法。结果表明，框架在平衡性能和效率方面具有优势。",
      "conclusion": "本研究的主要贡献是开发了一个轻量级测试时间适应框架，有效解决了EMG手势识别中的信号漂移问题。其学术价值在于整合了自适应技术和元学习，为资源受限环境下的模型适应提供了新思路。实际应用上，为长期假肢使用中的'即插即用'肌电控制铺平了道路，未来工作可进一步优化策略或扩展至其他应用场景。",
      "tags": [
        "Test-Time Adaptation",
        "Temporal Convolutional Network",
        "Gaussian Mixture Model",
        "Experience Replay",
        "Meta-Learning"
      ]
    },
    "analyzed_at": "2026-01-08T17:32:58.838842Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04176",
    "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation",
    "authors": [
      "Pietro de Oliveira Esteves"
    ],
    "abstract": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04176.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04176",
    "published": "2026-01-07T18:43:11Z",
    "updated": "2026-01-07T18:43:11Z",
    "comment": "9 pages, 4 figures, 2 tables. Code available at https://github.com/p-esteves/pinn-nlse-2026",
    "light_analysis": {
      "overview": "本研究提出一个物理信息神经网络框架，能在高噪声条件下准确恢复非线性薛定谔方程的物理参数。",
      "motivation": "研究动机源于从稀疏和噪声数据中恢复物理参数的挑战。传统数值方法如有限差分在噪声环境下因导数放大噪声而失效，而实验数据常受高不确定性影响，尤其在时空动力学的逆问题中。现有优化方法在数据稀缺和噪声时效率低下，因此开发鲁棒方法对实际应用至关重要。",
      "method": "研究方法基于物理信息神经网络与自动微分相结合，应用于非线性薛定谔方程。通过物理基正则化作为噪声过滤机制，网络从少量稀疏采样点（如500个）和20%高斯噪声数据中恢复非线性系数beta。框架利用深度学习模型，具体架构摘要未明确说明，但关键创新在于整合物理约束以增强鲁棒性。",
      "result": "实验结果显示，恢复非线性系数beta的相对误差小于0.2%（beta=1.0时）。在不同物理参数（beta范围0.5-2.0）和数据点（100-1000）下，准确率保持亚1%。统计验证表明鲁棒性高，标准偏差小于0.15%。计算效率良好，在NVIDIA Tesla T4 GPU上执行约80分钟，优于传统有限差分方法。",
      "conclusion": "结论强调物理基正则化能有效处理高测量不确定性，使PINNs成为传统优化方法的可行替代方案。该研究为数据稀缺噪声的时空动力学逆问题提供新工具，具有学术和实际价值。公开代码促进可重复性，未来工作可能涉及扩展到其他复杂方程或更高维场景。",
      "tags": [
        "Physics-Informed Neural Networks",
        "Nonlinear Schrödinger Equation",
        "Automatic Differentiation",
        "Inverse Problems",
        "Robustness to Noise"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:40.927785Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04171",
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "authors": [
      "Mohit Raghavendra",
      "Anisha Gunjal",
      "Bing Liu",
      "Yunzhong He"
    ],
    "abstract": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04171.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04171",
    "published": "2026-01-07T18:38:23Z",
    "updated": "2026-01-07T18:38:23Z",
    "comment": "31 pages, 11 Figures",
    "light_analysis": {
      "overview": "论文提出 Agentic Rubrics 方法，作为软件工程代理的上下文验证器，无需代码执行即可进行高效验证。",
      "motivation": "软件工程代理的验证对改进性能至关重要，它为强化学习提供奖励信号并通过 Test-Time Scaling 实现推理时优化。然而，现有方法依赖代码执行，因环境设置复杂而难以扩展；替代方案如补丁分类器和启发式方法虽可扩展，但缺乏代码库上下文接地性且解释性差。因此，需要一种更接地气、易于理解的验证方法来解决这些不足。",
      "method": "本论文提出 Agentic Rubrics 方法：使用专家代理与代码库交互，生成基于上下文的评分清单（rubric checklist），然后基于该清单对候选补丁进行评分，无需执行测试。关键创新在于代理收集代码库特定上下文，以创建明确、细粒度的验证准则。该方法在 SWE-Bench Verified 数据集上进行评估，采用并行 Test-Time Scaling 设置，确保验证过程的效率。",
      "result": "在 SWE-Bench Verified 的并行 TTS 评估中，Agentic Rubrics 在 Qwen3-Coder-30B-A3B 上获得 54.2% 的分数，在 Qwen3-32B 上获得 40.6%。相较于最强基线，性能至少提升 3.5 个百分点。分析显示，评分清单与真实测试结果一致，并能标记测试未捕获的问题；消融实验证明，代理上下文收集对生成代码库特定、无歧义标准至关重要。",
      "conclusion": "论文主要贡献是引入了 Agentic Rubrics 方法，为软件工程代理提供了高效、可扩展和细粒度的验证信号，避免了代码执行的开销，提高了验证的接地性和解释性。学术价值在于提出一种新颖的验证框架，实际应用有助于优化 SWE 代理性能；摘要未明确说明局限性，但未来工作可探索该方法在更广泛代码库中的泛化能力。",
      "tags": [
        "Agentic Rubrics",
        "Contextual Verification",
        "Software Engineering Agents",
        "Test-Time Scaling",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:03.040110Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04110",
    "title": "Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models",
    "authors": [
      "Magnus Bühler",
      "Lennart Purucker",
      "Frank Hutter"
    ],
    "abstract": "Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset. This approach augments limited real data with causally informed synthetic examples, preserving feature dependencies while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2300 fine-tuning runs, our CausalMixFT method consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators such as CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09). Moreover, it narrows the median validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable validation-based early stopping, a key step toward improving fine-tuning stability under data scarcity. These results demonstrate that incorporating causal structure into data augmentation provides an effective and principled route to fine-tuning tabular foundation models in low-data regimes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04110.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04110",
    "published": "2026-01-07T17:16:39Z",
    "updated": "2026-01-07T17:16:39Z",
    "comment": "Accepted for oral presentation at the EurIPS 2025 Workshop on AI for Tabular Data (Copenhagen)",
    "light_analysis": {
      "overview": "提出CausalMixFT方法，通过因果数据增强提升表格基础模型在数据稀缺环境下的微调鲁棒性和性能。",
      "motivation": "在数据稀缺时微调表格基础模型具有挑战性，因为验证数据同样有限，导致早期停止策略无法准确捕捉泛化性能，影响模型在实际应用中的可靠性。表格数据广泛用于金融、医疗等领域，现有统计生成方法可能破坏特征依赖关系，因此需要结合因果结构的鲁棒方法来提高微调效果，应对数据不足的问题。",
      "method": "CausalMixFT利用结构因果模型在目标数据集上拟合，生成结构一致的合成样本，用于扩充训练数据。该方法的核心创新在于结合因果推断原理，通过SCMs捕获特征间的依赖关系，生成保持数据结构的合成例子，从而增强训练多样性和微调鲁棒性，无需额外指定模型架构细节，确保数据增强的有效性。",
      "result": "在33个分类数据集和2300多个微调运行中，CausalMixFT将中位归一化ROC-AUC从0.10提升至0.12，优于统计生成方法如CTGAN（-0.01）、TabEBM（-0.04）和TableAugment（-0.09）。此外，该方法将验证-测试性能相关性差距从0.67缩小到0.30，提高了基于验证的早期停止的可靠性，有效改善了微调稳定性。",
      "conclusion": "研究表明，将因果结构纳入数据增强为低数据环境下的表格基础模型微调提供了有效且原则性的途径，显著提升了下游性能和微调稳定性，对推动表格AI应用具有重要价值。未来工作可能涉及扩展至更多数据场景或优化因果模型拟合方法，摘要未明确说明具体局限性。",
      "tags": [
        "Causal Data Augmentation",
        "Structural Causal Models",
        "Tabular Foundation Models",
        "Fine-Tuning",
        "Robustness"
      ]
    },
    "analyzed_at": "2026-01-08T17:33:58.748304Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04058",
    "title": "Minimum distance classification for nonlinear dynamical systems",
    "authors": [
      "Dominique Martinez"
    ],
    "abstract": "We address the problem of classifying trajectory data generated by some nonlinear dynamics, where each class corresponds to a distinct dynamical system. We propose Dynafit, a kernel-based method for learning a distance metric between training trajectories and the underlying dynamics. New observations are assigned to the class with the most similar dynamics according to the learned metric. The learning algorithm approximates the Koopman operator which globally linearizes the dynamics in a (potentially infinite) feature space associated with a kernel function. The distance metric is computed in feature space independently of its dimensionality by using the kernel trick common in machine learning. We also show that the kernel function can be tailored to incorporate partial knowledge of the dynamics when available. Dynafit is applicable to various classification tasks involving nonlinear dynamical systems and sensors. We illustrate its effectiveness on three examples: chaos detection with the logistic map, recognition of handwritten dynamics and of visual dynamic textures.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04058.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04058",
    "published": "2026-01-07T16:21:47Z",
    "updated": "2026-01-07T16:21:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了Dynafit方法，一种基于核函数的距离度量学习方法，用于分类非线性动力学系统生成的轨迹数据。",
      "motivation": "该研究旨在解决分类由不同非线性动力学系统生成的轨迹数据问题，每个类别对应一个独特的动力学系统。非线性动力学在现实世界应用（如传感器数据分析）中广泛存在，准确分类对系统识别和模式识别至关重要。现有方法可能难以有效处理动力学的复杂性和高维性，因此需要开发更高效的分类技术来提升鲁棒性和准确性。",
      "method": "Dynafit方法通过学习一个距离度量来比较训练轨迹和底层动力学，核心创新在于利用Koopman算子将非线性动力学在核函数关联的特征空间中全局线性化。该方法使用核技巧，在特征空间中计算距离而不依赖其维度，提高了计算效率。核函数可定制以整合部分动力学先验知识，增强了方法的灵活性和适用性。在实现中，通过近似Koopman算子来处理潜在无限维特征空间。",
      "result": "论文通过三个示例展示了Dynafit的有效性：logistic map的混沌检测、手写动态识别和视觉动态纹理识别。这些应用表明方法能够成功分类不同类型的非线性动力学系统，但摘要未明确说明具体性能指标（如准确率提升）或与基线方法的对比数据，因此效果描述基于应用实例的定性验证。",
      "conclusion": "Dynafit为非线性动力学系统分类提供了一种通用的核方法，基于Koopman算子线性化技术，具有重要的学术价值和实际应用潜力。研究扩展了核学习在动力学系统领域的应用，可适用于传感器数据等多种任务。潜在局限性包括未详述性能优化细节，未来工作可能包括改进核函数设计或扩展到更复杂的动态场景。",
      "tags": [
        "Kernel Method",
        "Koopman Operator",
        "Nonlinear Dynamical Systems",
        "Distance Metric Learning",
        "Classification"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:29.521504Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04057",
    "title": "Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia",
    "authors": [
      "M. Yin",
      "K. G. Ravindran",
      "C. Hadjipanayi",
      "A. Bannon",
      "A. Rapeaux",
      "C. Della Monica",
      "T. S. Lande",
      "Derk-Jan Dijk",
      "T. G. Constandinou"
    ],
    "abstract": "Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04057.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04057",
    "published": "2026-01-07T16:21:27Z",
    "updated": "2026-01-07T16:21:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种深度迁移学习框架，利用传统多导睡眠图数据训练超宽带雷达系统，提升老年人和痴呆患者睡眠监测的准确性。",
      "motivation": "超宽带雷达技术为家庭睡眠监测提供无侵入、低成本的解决方案，但雷达睡眠数据稀缺，限制了模型的鲁棒性和泛化能力，尤其在不同人群和环境中。现有方法因数据不足难以有效应用，导致模型性能下降，无法满足临床需求。本研究旨在通过迁移学习解决数据变异性和小样本问题，以改善老年人和神经退行性疾病患者的护理效果。",
      "method": "论文开发了一个端到端神经网络，基于夜间呼吸和运动信号分类睡眠阶段，包括唤醒、快速眼动、浅睡眠和深睡眠。核心创新是采用深度迁移学习框架，结合大规模多导睡眠图数据和雷达数据进行训练，并利用对抗学习方法进行域适应，以弥合PSG和雷达信号之间的知识差距，提高模型在目标域的泛化能力。",
      "result": "在47名老年人（平均年龄71.2岁，包括18名阿尔茨海默病患者）的雷达数据集上验证，所提网络在睡眠阶段分类中达到79.5%的准确率和0.65的Kappa值。实验结果确认深度迁移学习方法显著提升了自动睡眠分期在目标域的性能，有效解决了数据有限导致的泛化问题，优于传统方法。",
      "conclusion": "该方法成功应对了数据变异性和小样本挑战，显著提高了自动睡眠分期模型的可靠性。学术上，验证了迁移学习在跨域医疗数据中的应用价值；实际上，支持超宽带雷达作为非侵入性睡眠评估工具的可行性，对老年人和神经退行性疾病患者护理有重要贡献。未来工作可扩展到更多人群和优化模型性能。",
      "tags": [
        "Ultra-wideband Radar",
        "Deep Transfer Learning",
        "Adversarial Learning",
        "Sleep Stage Classification",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-08T17:34:53.913103Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04054",
    "title": "LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis",
    "authors": [
      "Yayati Jadhav",
      "Amir Barati Farimani"
    ],
    "abstract": "Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04054.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04054",
    "published": "2026-01-07T16:19:11Z",
    "updated": "2026-01-07T16:19:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种基于自回归扩散模型的框架，用于自动合成机械链，结合了Transformer和DDPM技术。",
      "motivation": "机械链设计是实现目标末端效应器轨迹的关键挑战，由于节点位置、拓扑配置和运动学约束的复杂耦合。高度非线性运动到配置关系意味着关节位置的微小变化会显著改变轨迹，而组合爆炸的设计空间使得传统优化和启发式方法在计算上不可行。这导致现有方法难以处理大规模或复杂设计，因此需要新的可扩展解决方案来提升设计效率和精度。",
      "method": "论文提出了一种自回归扩散框架，将机械机构表示为顺序构造的图，其中节点对应关节，边对应刚性链。核心创新在于结合因果Transformer和Denoising Diffusion Probabilistic Model (DDPM)，两者都条件于通过Transformer编码器编码的目标轨迹。因果Transformer自回归地预测离散拓扑结构，而DDPM则细化每个节点的空间坐标和与先前生成节点的边连接，这种顺序生成机制支持自适应试错合成，允许选择性再生存在运动锁定或碰撞的问题节点。",
      "result": "该方法能够成功合成包含多达20个节点的机械链系统，并可扩展到任意节点数量的机构。基于图的数据驱动方法超越了传统优化方法，实现了可扩展的逆向设计，使系统能泛化到不同规模的机制。摘要未明确说明具体性能指标如准确率或效率数据，但强调了方法在泛化性和处理复杂设计方面的优势。",
      "conclusion": "本工作推进了自回归图生成方法和计算运动学综合领域，为复杂机械系统的可扩展逆向设计建立了新范式。它展示了如何结合自回归建模和扩散模型来处理机械链的综合问题，具有重要学术价值和实际应用潜力，如自动化设计复杂机械系统。未来工作可能包括扩展模型以处理更复杂的约束或应用于其他机械设计领域。",
      "tags": [
        "Autoregressive Diffusion Model",
        "Transformer",
        "Denoising Diffusion Probabilistic Model (DDPM)",
        "Graph Generation",
        "Mechanical Linkage Synthesis"
      ]
    },
    "analyzed_at": "2026-01-08T17:35:00.164267Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04051",
    "title": "Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing",
    "authors": [
      "Viktor Martinek",
      "Roland Herzog"
    ],
    "abstract": "Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only \"non-shared\" (category-value-specific) parameters, and others also incorporate \"shared\" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04051.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04051",
    "published": "2026-01-07T16:12:14Z",
    "updated": "2026-01-07T16:12:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了通过引入中间级别参数共享来扩展符号回归的方法，以处理多个分类变量，减少参数数量并增强问题理解。",
      "motivation": "符号回归因其可解释性强，在科学发现中特别有效，但现有方法处理相似现象时引入分类变量，存在局限：一些只允许非共享参数，一些只允许共享参数。这导致参数效率不高，无法充分揭示问题结构。因此，本文旨在开发一种新方法，以处理多个分类变量并实现部分参数共享，从而改进参数减少和信息提取，解决现有方法的不足，提升符号回归在复杂数据分析中的应用价值。",
      "method": "本文扩展符号回归，考虑多个分类变量，引入中间级别的参数共享。具体地，当有两个分类变量时，参数可以跨一个类别共享但在另一个类别中变化，形成部分共享机制。这通过减少参数数量来优化模型，并利用符号回归框架结合参数共享技术，以揭示变量间的结构关系。方法在合成数据集上测试数据需求减少和迁移学习的潜力，技术细节涉及参数共享规则的实现，适应不同分类变量的组合，以增强模型的表达能力和效率。",
      "result": "在合成示例中，新方法测试了数据需求减少和迁移学习的极限。在真实世界的天体物理学数据集上，与先前仅考虑一个分类变量的研究相比，本文方法实现了相似的拟合质量，但使用了显著更少的个体参数，具体减少程度摘要未明确说明。此外，通过参数共享机制，提取了更多关于问题的信息，揭示了变量间的依赖关系，从而提升了模型的解释性和可扩展性，验证了新方法在参数效率和信息挖掘方面的优势。",
      "conclusion": "本文的主要贡献是提出了一种中间级别参数共享的符号回归方法，适用于多个分类变量，减少了参数数量并提高效率，同时通过部分共享揭示问题的深层结构。研究具有学术价值，扩展了符号回归的应用范围，增强了其在科学发现中的实用性，例如天体物理学分析。局限性可能包括处理更复杂变量时的挑战，未来工作可探索更多类别变量或结合其他机器学习技术，为实际应用提供更高效的分析工具。",
      "tags": [
        "Symbolic Regression",
        "Partial Parameter Sharing",
        "Categorical Variables",
        "Transfer Learning"
      ]
    },
    "analyzed_at": "2026-01-08T17:36:38.246903Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]