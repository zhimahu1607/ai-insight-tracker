[
  {
    "id": "2601.04170",
    "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
    "authors": [
      "Abhishek Rath"
    ],
    "abstract": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).   We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.   We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04170.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04170",
    "published": "2026-01-07T18:37:26Z",
    "updated": "2026-01-07T18:37:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入代理漂移概念，提出ASI量化框架及三种缓解策略，为多代理LLM系统长期稳定性研究奠定理论和方法基础。",
      "motivation": "多代理大型语言模型系统在复杂任务分解和协作问题解决中展现出强大能力，但其长期行为稳定性问题尚未得到充分研究。随着系统运行时间延长，代理行为可能逐渐退化，导致决策质量下降和代理间一致性减弱，这直接影响生产环境部署的可靠性和AI安全。现有方法主要关注短期性能，缺乏对长期交互中行为退化的系统性评估，因此需探索漂移现象及其影响，以提升系统的可持续性和可靠性。",
      "method": "本研究提出代理漂移的理论框架，将其分为语义漂移、协调漂移和行为漂移三种表现形式。开发了代理稳定性指数（ASI），一个复合度量框架，包括12个维度如响应一致性、工具使用模式、推理路径稳定性和代理间一致性率。此外，提出了三种缓解策略：情景记忆巩固、漂移感知路由协议和自适应行为锚定。通过基于模拟的分析和理论建模验证这些方法的有效性，但摘要未明确说明具体数据集或模型架构，仅强调了模拟方法论的应用。",
      "result": "通过模拟分析和理论建模，研究表明未控制的代理漂移会导致任务完成准确率的大幅降低和人为干预需求的显著增加。理论分析表明，提出的缓解策略如情景记忆巩固能显著减少漂移相关错误，同时维持系统吞吐量。与未缓解的基线系统相比，这些策略在提升行为稳定性方面显示出潜力，但具体性能指标如准确率提升百分比摘要未明确说明，仅基于理论推断其有效性。",
      "conclusion": "本研究的主要贡献在于为监控、测量和缓解生产代理AI系统中的代理漂移建立了基础方法论，推进了多代理LLM系统长期行为稳定性的学术研究，并对企业部署的可靠性和AI安全实践具有重要价值。未来工作可包括对缓解策略进行更广泛的实证验证，将其集成到实际系统中，以及探索其他潜在的漂移控制方法以进一步优化系统性能。",
      "tags": [
        "Multi-Agent Systems",
        "Large Language Models",
        "Agent Drift",
        "Stability Metrics",
        "Memory Consolidation"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:48.691757Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04164",
    "title": "Clinical Data Goes MEDS? Let's OWL make sense of it",
    "authors": [
      "Alberto Marfoglia",
      "Jong Ho Jhee",
      "Adrien Coulet"
    ],
    "abstract": "The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04164.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04164",
    "published": "2026-01-07T18:25:02Z",
    "updated": "2026-01-07T18:25:02Z",
    "comment": "12 pages, 5 tables, 4 figures",
    "light_analysis": {
      "overview": "本文提出MEDS-OWL本体和meds2rdf库，将医疗事件数据标准（MEDS）与语义网络集成，以增强医疗数据的互操作性和可重复性，提供可重用的语义层。",
      "motivation": "医疗数据在机器学习应用中常因缺乏标准化和语义明确的表示而受阻，导致数据集间互操作性和实验可重复性有限。MEDS标准引入了事件中心的数据模型来解决这些问题，但它仅是数据格式规范，未与语义网络生态系统集成，限制了数据的进一步利用和整合。因此，本研究旨在弥补这一缺口，通过将MEDS与语义网络技术结合，提升医疗数据的FAIR（可查找、可访问、可互操作、可重用）原则遵循度，以支持更有效的分析和共享。",
      "method": "本研究提出MEDS-OWL，一个轻量级OWL本体，定义正式的概念和关系，使MEDS数据集能以RDF图形式表示，从而集成语义网络。关键创新包括使用OWL公理和属性精确描述事件临床数据。此外，实现了meds2rdf，一个Python转换库，自动化将MEDS事件转换为RDF图，并确保与本体的一致性。技术特色涉及使用SHACL（Shapes Constraint Language）约束验证生成的RDF图，确保数据准确性和标准化。在数据集方面，使用了一个合成临床数据集（描述颅内动脉瘤破裂患者的护理路径）进行演示。",
      "result": "在合成临床数据集上应用MEDS-OWL和meds2rdf，成功将MEDS事件转换为RDF图，并通过SHACL约束验证了图的有效性。MEDS-OWL的首个版本包含13个类、10个对象属性、20个数据属性和24个OWL公理，提供了丰富的语义框架。结合meds2rdf，该方法实现了数据向FAIR对齐数据集的转换，支持来源感知发布，并显著提升了事件临床数据的互操作性。与基线方法（如原始MEDS格式）相比，通过语义网络集成，增强了数据的标准化和可重复性，为后续图形分析提供了可靠基础。",
      "conclusion": "本研究的主要贡献是提供一个可重用的语义层，将MEDS标准与语义网络集成，为事件临床数据建立稳健的基于图分析的基础。学术上，它推动了医疗数据语义表示的发展，促进更高效的机器学习工作流程；实际上，促进了数据转换和互操作性，支持FAIR数据实践和图形分析应用。局限性方面，摘要未明确说明，但可能包括需要更多真实数据集进行验证和扩展。未来工作方向可包括扩展本体以覆盖更多临床事件类型，或应用于多样化的医疗数据集以评估泛化能力。",
      "tags": [
        "OWL Ontology",
        "RDF Graphs",
        "SHACL",
        "Python Library",
        "Semantic Web Integration"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:39.368609Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04151",
    "title": "Klear: Unified Multi-Task Audio-Video Joint Generation",
    "authors": [
      "Jun Wang",
      "Chunyu Qiang",
      "Yuxin Guo",
      "Yiran Wang",
      "Xijuan Zeng",
      "Chen Zhang",
      "Pengfei Wan"
    ],
    "abstract": "Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM",
      "cs.SD"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04151.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04151",
    "published": "2026-01-07T18:03:45Z",
    "updated": "2026-01-07T18:03:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "Klear通过统一多任务架构、渐进训练策略和大规模数据整理，实现了高性能音频视频联合生成，显著提升同步性和泛化能力。",
      "motivation": "现有音频视频联合生成方法常面临音频-视觉不同步、唇语对齐不佳和单模态退化等问题，主要源于音频-视觉对应建模薄弱、模型泛化能力有限以及高质量密集标注数据稀缺。这些问题限制了生成内容的真实性和实用性，如多媒体内容创建，突显了开发更鲁棒、能紧密对齐的新方法的必要性，以应对现实场景中的挑战。",
      "method": "Klear的研究方法涵盖模型架构、训练策略和数据整理。架构上采用单塔设计，集成统一DiT块和Omni-Full Attention机制，促进音频-视频紧密对齐和高度可扩展性。训练方面实施渐进多任务机制，包括随机模态掩码以实现跨任务联合优化，以及多阶段课程学习，以增强表示鲁棒性、强化对齐知识并防止单模态崩溃。数据上构建首个大规模带密集标注的音频视频数据集，并引入自动管道筛选数百万个高质量、严格对齐的音频-视频-文本三元组。",
      "result": "Klear在多项任务中大幅优于先前方法，性能与Veo 3相当，能生成高保真、语义和时间严格对齐的内容，在联合和单模态设置下遵循指令，并对分布外场景展现出强大泛化能力。这些结果证明了其在音频视频合成领域的优越性，摘要未明确说明具体性能指标如准确率等数值。",
      "conclusion": "Klear的主要贡献在于通过统一架构、创新训练策略和数据整理，推动了音频视频联合生成的性能和可靠性，为下一代合成技术提供了可扩展路径。其学术价值在于解决现有方法不足，实际应用潜力广泛，如智能媒体创作。摘要未明确说明具体局限性，未来工作可能包括模型优化和数据集扩展。",
      "tags": [
        "Audio-Video Generation",
        "Multi-Task Learning",
        "Diffusion Transformer",
        "Attention Mechanism",
        "Dataset Curation"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:30.981299Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04131",
    "title": "ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models",
    "authors": [
      "Nikhil Anand",
      "Shwetha Somasundaram",
      "Anirudh Phukan",
      "Apoorv Saxena",
      "Koyel Mukherjee"
    ],
    "abstract": "Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model's internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04131.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04131",
    "published": "2026-01-07T17:45:20Z",
    "updated": "2026-01-07T17:45:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 ContextFocus，一种轻量级激活引导方法，无需模型微调，显著提升大型语言模型在处理知识冲突时的上下文忠实度。",
      "motivation": "大型语言模型在预训练中编码了海量参数知识，但随着世界知识不断进化，实际应用越来越依赖于模型忠实遵循外部检索上下文的能力。当外部证据与模型内部知识发生冲突时，模型往往优先使用记忆的事实，导致输出不忠实，影响部署的可靠性。现有方法如微调或提示策略可能效率低下、鲁棒性不足或需要高计算开销，因此迫切需要一种高效且无需大量修改的方法来改善知识冲突下的上下文忠实度，以支持更广泛的应用场景。",
      "method": "本研究提出 ContextFocus，一种基于激活引导的轻量级方法，通过调整模型内部激活来提高对上下文的忠实度。该方法无需进行模型微调，推理时仅引入最小开销，保持了高效性。具体技术涉及在知识冲突设置中引导模型注意力或表示，优先考虑外部证据，而不是依赖内部记忆。研究在 ConFiQA 基准上进行评估，采用激活引导机制，避免了传统方法如微调的复杂训练过程，从而简化部署。",
      "result": "在 ConFiQA 基准上的广泛实验表明，ContextFocus 显著提高了上下文忠实度，与强基线如 ContextDPO、COIECD 和基于提示的方法相比表现优异。结果显示，该方法不仅能有效改善忠实度，还与现有提示策略互补，且在更大规模的模型中保持有效性。此外，ContextFocus 在保持输出流畅性和推理效率的同时，突出了其鲁棒性和高效性，具体性能提升未在摘要中明确量化，但强调了对基线方法的显著改进。",
      "conclusion": "本文的主要贡献是提出 ContextFocus，一种无需微调的激活引导方法，有效提升大型语言模型在处理外部上下文冲突时的忠实度。该方法具有轻量级和高效的特点，适用于实际部署，展示了在改善模型可靠性和应用价值方面的潜力。未来工作可探索其在更多任务和模型中的扩展应用，或进一步优化激活引导机制以处理更复杂的知识冲突场景。",
      "tags": [
        "Large Language Models",
        "Activation Steering",
        "Contextual Faithfulness",
        "Knowledge Conflict",
        "ConFiQA Benchmark"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:35.851910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04127",
    "title": "Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images",
    "authors": [
      "Leandro Stival",
      "Ricardo da Silva Torres",
      "Helio Pedrini"
    ],
    "abstract": "Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04127.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04127",
    "published": "2026-01-07T17:41:11Z",
    "updated": "2026-01-07T17:41:11Z",
    "comment": "21 pages, 9 Figures",
    "light_analysis": {
      "overview": "本文提出PIMC方法，通过像素级别的二维表示和多模态对比学习，提升遥感图像时间序列的特征提取和表示质量。",
      "motivation": "卫星持续生成海量遥感数据，尤其是卫星图像时间序列（SITS），为地球观测提供丰富信息。然而，现有深度学习模型通常处理整个图像或完整时间序列，难以有效编码像素级别的视觉属性变化，限制了特征提取精度，影响下游任务如预测和分类的性能。因此，需要一种新方法能更精准地利用像素信息，以提升地球观测的准确性和效率。",
      "method": "本文提出像素级别多模态对比学习（PIMC）方法。首先，从像素级别的植被指数时间序列（如NDVI、EVI、SAVI）生成重现图作为二维表示，替代原始像素值以增加信息量。然后，设计PIMC框架，通过自监督对比学习结合二维像素时间序列表示和遥感影像（RSI）训练编码器。该方法利用多模态数据增强特征提取，特别针对卫星图像时间序列处理，并在PASTIS和EuroSAT数据集上验证。",
      "result": "实验在三个下游任务中验证：使用PASTIS数据集的像素级别预测和分类，以及EuroSAT数据集的土地覆盖分类。与最先进方法相比，PIMC方法表现出色，二维表示显著提升了SITS的特征提取效果，对比学习提高了像素时间序列和RSI的表示质量。尽管具体准确率数值摘要未明确说明，但方法在多种地球观测任务中显示出优越性能。",
      "conclusion": "本文的主要贡献是提出了PIMC方法，一种稳健的像素级别多模态对比学习框架，能有效处理卫星图像时间序列和遥感影像。该研究建立了自监督学习模型，在地球观测任务中超越现有方法，具有重要学术价值和实际应用潜力，如精准农业和环境监测。未来工作可探索更多模态数据和应用场景，尽管摘要未明确说明局限性。",
      "tags": [
        "Contrastive Learning",
        "Multimodal Learning",
        "Remote Sensing",
        "Satellite Image Time Series",
        "Self-Supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:33.092730Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04126",
    "title": "InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training",
    "authors": [
      "Ziyun Zhang",
      "Zezhou Wang",
      "Xiaoyi Zhang",
      "Zongyu Guo",
      "Jiahao Li",
      "Bin Li",
      "Yan Lu"
    ],
    "abstract": "GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04126.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04126",
    "published": "2026-01-07T17:40:08Z",
    "updated": "2026-01-07T17:40:08Z",
    "comment": "Work In Progress",
    "light_analysis": {
      "overview": "论文提出InfiniteWeb系统，通过可扩展生成功能性网页环境，解决了GUI代理训练中环境稀缺的挑战。",
      "motivation": "GUI代理作为实用AI助手，能够代表用户与图形界面交互，但其训练因缺乏合适的环境而受阻。现有大型语言模型（LLMs）在生成单个网页时表现良好，但构建具有多个互联页面的现实功能网站面临困难，这限制了代理的有效训练和应用，凸显了环境合成技术的重要性。",
      "method": "InfiniteWeb采用统一规范、任务中心的测试驱动开发方法，结合网站种子和参考设计图像来确保环境多样性和功能性。系统还生成可验证任务评估器，为强化学习提供密集奖励信号，从而提升GUI代理的训练效率和效果。",
      "result": "实验表明，InfiniteWeb在构建现实网站方面超越商业编码代理，并在OSWorld和Online-Mind2Web基准测试中，使用其生成环境训练的GUI代理实现了显著的性能改进，验证了系统对GUI代理训练的有效性。",
      "conclusion": "本研究贡献了InfiniteWeb系统，可扩展合成功能性网页环境，促进了GUI代理的训练和发展，具有学术和应用价值。摘要未明确说明具体局限性或未来工作方向。",
      "tags": [
        "GUI Agent Training",
        "Web Environment Synthesis",
        "Large Language Models",
        "Reinforcement Learning",
        "Test-Driven Development"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:17.183959Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04098",
    "title": "Layer-wise Positional Bias in Short-Context Language Modeling",
    "authors": [
      "Maryam Rahimi",
      "Mahdi Nouri",
      "Yadollah Yaghoobzadeh"
    ],
    "abstract": "Language models often show a preference for using information from specific positions in the input regardless of semantic relevance. While positional bias has been studied in various contexts, from attention sinks to task performance degradation in long-context settings, prior work has not established how these biases evolve across individual layers and input positions, or how they vary independent of task complexity. We introduce an attribution-based framework to analyze positional effects in short-context language modeling. Using layer conductance with a sliding-window approach, we quantify how each layer distributes importance across input positions, yielding layer-wise positional importance profiles. We find that these profiles are architecture-specific, stable across inputs, and invariant to lexical scrambling. Characterizing these profiles, we find prominent recency bias that increases with depth and subtle primacy bias that diminishes through model depth. Beyond positional structure, we also show that early layers preferentially weight content words over function words across all positions, while later layers lose this word-type differentiation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04098.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04098",
    "published": "2026-01-07T17:04:30Z",
    "updated": "2026-01-07T17:04:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出一种基于归因的框架，分析语言模型中各层的位置偏见如何演变，揭示了偏见的结构和变化趋势。",
      "motivation": "语言模型在处理输入时常常表现出对特定位置信息的非语义偏好，这被称为位置偏见。尽管位置偏见已在长上下文设置中被研究，但现有方法未能详细揭示这种偏见如何在不同网络层中演变，或如何独立于任务复杂性变化。这限制了对模型内部工作机制的理解，特别是在短上下文语言建模中。因此，本研究旨在填补这一空白，通过分析层间的位置效应，深入理解模型的偏好结构，以解决现有研究对偏见动态变化的忽视问题。",
      "method": "论文提出一个基于归因的框架来分析位置效应，具体使用层传导（layer conductance）结合滑动窗口方法。该方法量化了每个神经网络层如何将重要性分配到输入的不同位置，从而生成层间位置重要性剖面。关键创新点在于应用归因技术来分离和测量位置偏见，而不是依赖于任务性能，从而提供更精细的内部结构分析。研究中未明确指定具体的数据集或模型架构，但方法侧重于分析现有语言模型的内部动态，突出了层间分析的特色。",
      "result": "实验结果显示，位置重要性剖面是模型架构特定的，在不同输入间保持稳定，且对词汇扰动具有不变性。具体发现包括：随着模型深度增加，近期偏见变得更加显著；初期偏见则随深度减弱。此外，早期层在所有位置优先加权内容词而非功能词，而后期层逐渐失去这种词类区分。这些结果量化了偏见的变化趋势，提供了对模型内部动态的新见解，并与基线方法形成对比，强调偏见独立于任务复杂性的特点。",
      "conclusion": "本研究的贡献在于揭示了语言模型中位置偏见的层间演变规律，提出了一个有效的分析框架。学术上，这深化了对模型内部机制的理解，特别是在偏见如何随深度变化方面，有助于理论建模。实际应用中，这些发现可帮助改进模型设计，减少非语义偏见的影响，提升模型性能。局限性在于侧重于短上下文设置，未来工作可以扩展到更广泛的情境或探索其他类型的偏见，以进一步提升研究广度。",
      "tags": [
        "Language Models",
        "Positional Bias",
        "Attribution Methods",
        "Layer Conductance",
        "Short-Context Language Modeling"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:16.514149Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04073",
    "title": "Analyzing Reasoning Consistency in Large Multimodal Models under Cross-Modal Conflicts",
    "authors": [
      "Zhihao Zhu",
      "Jiafeng Liang",
      "Shixin Jiang",
      "Jinlan Fu",
      "Ming Liu",
      "Guanglu Sun",
      "See-Kiong Ng",
      "Bing Qin"
    ],
    "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive capabilities in video reasoning via Chain-of-Thought (CoT). However, the robustness of their reasoning chains remains questionable. In this paper, we identify a critical failure mode termed textual inertia, where once a textual hallucination occurs in the thinking process, models tend to blindly adhere to the erroneous text while neglecting conflicting visual evidence. To systematically investigate this, we propose the LogicGraph Perturbation Protocol that structurally injects perturbations into the reasoning chains of diverse LMMs spanning both native reasoning architectures and prompt-driven paradigms to evaluate their self-reflection capabilities. The results reveal that models successfully self-correct in less than 10% of cases and predominantly succumb to blind textual error propagation. To mitigate this, we introduce Active Visual-Context Refinement, a training-free inference paradigm which orchestrates an active visual re-grounding mechanism to enforce fine-grained verification coupled with an adaptive context refinement strategy to summarize and denoise the reasoning history. Experiments demonstrate that our approach significantly stifles hallucination propagation and enhances reasoning robustness.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04073.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04073",
    "published": "2026-01-07T16:39:34Z",
    "updated": "2026-01-07T16:39:34Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出Active Visual-Context Refinement，一种无需训练的推理范式，以增强大型多模态模型在跨模态冲突下的推理一致性。",
      "motivation": "研究动机在于大型多模态模型（LMMs）在视频推理中展现出能力，但其推理链鲁棒性不足，存在文本惯性问题：一旦推理过程中出现文本幻觉，模型会盲目坚持错误文本，忽视冲突的视觉证据。这一问题重要性在于影响模型实际应用的可靠性，现有方法如Chain-of-Thought（CoT）在自我修正方面表现薄弱，模型在少于10%案例中能成功自我修正，暴露了现有技术处理跨模态冲突时的缺陷。",
      "method": "论文首先提出LogicGraph Perturbation Protocol，通过结构性地注入扰动到不同LMMs的推理链中，评估模型的自我反思能力，覆盖原生推理架构和提示驱动范式。核心方法是Active Visual-Context Refinement，这是一种无需训练的推理范式，创新点包括主动视觉重基础机制，进行细粒度验证以检测冲突，结合自适应上下文细化策略，总结并去噪推理历史，从而减轻文本惯性问题。",
      "result": "实验结果显示，在LogicGraph Perturbation Protocol评估下，大型多模态模型在少于10%的案例中能成功自我修正错误，而普遍屈服于盲目文本错误传播。相比之下，Active Visual-Context Refinement方法显著抑制了幻觉传播，并增强了推理鲁棒性，具体性能指标在摘要中未明确说明，但表明新方法在提升推理一致性方面优于基线方法。",
      "conclusion": "研究结论指出，通过Active Visual-Context Refinement方法有效缓解了大型多模态模型中的文本惯性问题，增强了推理一致性。学术价值在于揭示了模型推理链的脆弱性，并提供了训练免费的改进方案；实际应用价值在于提升多模态模型在视频推理等任务中的可靠性。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Large Multimodal Models",
        "Chain-of-Thought",
        "Self-Reflection",
        "Hallucination Propagation",
        "Visual-Grounding"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:10.348485Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04068",
    "title": "Mind the Generative Details: Direct Localized Detail Preference Optimization for Video Diffusion Models",
    "authors": [
      "Zitong Huang",
      "Kaidong Zhang",
      "Yukang Ding",
      "Chao Gao",
      "Rui Ding",
      "Ying Chen",
      "Wangmeng Zuo"
    ],
    "abstract": "Aligning text-to-video diffusion models with human preferences is crucial for generating high-quality videos. Existing Direct Preference Otimization (DPO) methods rely on multi-sample ranking and task-specific critic models, which is inefficient and often yields ambiguous global supervision. To address these limitations, we propose LocalDPO, a novel post-training framework that constructs localized preference pairs from real videos and optimizes alignment at the spatio-temporal region level. We design an automated pipeline to efficiently collect preference pair data that generates preference pairs with a single inference per prompt, eliminating the need for external critic models or manual annotation. Specifically, we treat high-quality real videos as positive samples and generate corresponding negatives by locally corrupting them with random spatio-temporal masks and restoring only the masked regions using the frozen base model. During training, we introduce a region-aware DPO loss that restricts preference learning to corrupted areas for rapid convergence. Experiments on Wan2.1 and CogVideoX demonstrate that LocalDPO consistently improves video fidelity, temporal coherence and human preference scores over other post-training approaches, establishing a more efficient and fine-grained paradigm for video generator alignment.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04068.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04068",
    "published": "2026-01-07T16:32:17Z",
    "updated": "2026-01-07T16:32:17Z",
    "comment": "Under Review",
    "light_analysis": {
      "overview": "论文提出了LocalDPO，一种后训练框架，通过构建局部偏好对并引入区域感知损失，优化视频扩散模型的细节对齐，提高视频生成质量。",
      "motivation": "对齐文本到视频扩散模型与人类偏好对于生成高质量视频至关重要。现有直接偏好优化（DPO）方法依赖于多样本排名和任务特定批评模型，导致效率低下且提供全局监督，难以精确优化局部细节。这限制了模型在视频保真度和时间一致性方面的表现，因此需要更高效和细粒度的对齐方法来克服现有方法的不足，提升视频生成的整体质量。",
      "method": "论文提出LocalDPO框架，核心是构建局部偏好对并在时空区域级别优化对齐。方法包括自动化数据收集管道，每个提示仅需一次推理生成偏好对；通过随机时空掩码局部破坏高质量真实视频，并用冻结的基础模型恢复掩码区域，创建正负样本。训练时引入区域感知DPO损失，将偏好学习限制在破坏区域以实现快速收敛，无需外部批评模型或手动标注，提高了优化效率和精确性。",
      "result": "在Wan2.1和CogVideoX数据集上的实验表明，LocalDPO相比其他后训练方法，在视频保真度、时间一致性和人类偏好得分方面有持续提升。尽管摘要未提供具体性能指标如准确率数据，但通过与基线方法对比，确认了其在提高视频生成质量方面的优越性，展现了更高效和细粒度的对齐效果。",
      "conclusion": "论文的主要贡献是提出了LocalDPO，通过局部偏好对构建和区域级别优化，实现了视频扩散模型的细粒度对齐，建立了更高效和精确的对齐范式。这提升了视频生成质量和对齐效率，具有学术价值，推动了视频生成技术的发展。虽然摘要未明确讨论局限性或未来工作，但该方法为未来研究方向如个性化视频生成和应用优化提供了潜在基础。",
      "tags": [
        "Video Diffusion Models",
        "Direct Preference Optimization (DPO)",
        "Spatio-temporal Masking",
        "Local Optimization",
        "Region-aware Loss"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:25.415875Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04060",
    "title": "ComfySearch: Autonomous Exploration and Reasoning for ComfyUI Workflows",
    "authors": [
      "Jinwei Su",
      "Qizhen Lan",
      "Zeyu Wang",
      "Yinghui Xia",
      "Hairu Wen",
      "Yiqun Duan",
      "Xi Xiao",
      "Tianyu Shi",
      "Yang Jingsong",
      "Lewei He"
    ],
    "abstract": "AI-generated content has progressed from monolithic models to modular workflows, especially on platforms like ComfyUI, allowing users to customize complex creative pipelines. However, the large number of components in ComfyUI and the difficulty of maintaining long-horizon structural consistency under strict graph constraints frequently lead to low pass rates and workflows of limited quality. To tackle these limitations, we present ComfySearch, an agentic framework that can effectively explore the component space and generate functional ComfyUI pipelines via validation-guided workflow construction. Experiments demonstrate that ComfySearch substantially outperforms existing methods on complex and creative tasks, achieving higher executability (pass) rates, higher solution rates, and stronger generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04060.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04060",
    "published": "2026-01-07T16:24:01Z",
    "updated": "2026-01-07T16:24:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ComfySearch框架，通过代理机制和验证引导的构建，自动生成高质量的ComfyUI工作流，解决组件探索和结构一致性难题。",
      "motivation": "随着AI生成内容向模块化工作流发展，ComfyUI等平台允许用户自定义复杂创意流程，但组件数量庞大，且在严格的图约束下维护长距离结构一致性困难，导致工作流通过率低和质量有限。现有方法在处理这些挑战时表现不足，无法有效平衡探索效率和结构优化，限制了创意自动化的发展潜力，因此需要新方法提升工作流的生成可靠性和效率。",
      "method": "ComfySearch采用代理框架，核心方法是有效探索ComfyUI的组件空间，通过验证引导的工作流构建技术生成功能性管道。关键创新点在于结合自主探索和推理能力，利用验证反馈指导构建过程，以优化图结构一致性并提高通过率。摘要未明确说明具体的数据集或模型架构细节，但强调了框架的代理性和验证机制在组件选择与连接中的重要作用。",
      "result": "实验表明，在复杂和创意任务上，ComfySearch显著优于现有方法，实现了更高的可执行性（通过）率和更高解决率，并展现出更强的泛化能力。这些改进证明了该方法在提升工作流质量方面的有效性，尽管摘要未提供具体性能指标数据如百分比，但通过对比强调了在效率和效果上的实质性提升。",
      "conclusion": "本研究的主要贡献是提出ComfySearch框架，解决了ComfyUI工作流自动生成中的组件探索和结构优化问题，其学术价值在于推动了模块化AI系统自动化探索的研究，实际应用价值在于提高创意工作流的生成效率和可靠性。摘要未明确说明局限性或未来工作方向，但潜在扩展可能包括适应其他平台或进一步优化验证算法。",
      "tags": [
        "Autonomous Exploration",
        "Agentic Framework",
        "Workflow Construction",
        "ComfyUI",
        "Validation-Guided"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:03.331881Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04160",
    "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
    "authors": [
      "Yuechen Jiang",
      "Zhiwei Liu",
      "Yupeng Cao",
      "Yueru He",
      "Ziyang Xu",
      "Chen Xu",
      "Zhiyang Deng",
      "Prayag Tiwari",
      "Xi Chen",
      "Alejandro Lopez-Lira",
      "Jimin Huang",
      "Junichi Tsujii",
      "Sophia Ananiadou"
    ],
    "abstract": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
    "categories": [
      "cs.CL",
      "cs.CE",
      "q-fin.CP"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04160.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04160",
    "published": "2026-01-07T18:18:28Z",
    "updated": "2026-01-07T18:18:28Z",
    "comment": "39 pages; 24 figures",
    "light_analysis": {
      "overview": "本文提出RFC Bench基准，用于评估大语言模型在真实金融新闻中的错误信息检测能力，揭示了无参考推理的显著局限性。",
      "motivation": "本研究旨在解决金融错误信息检测问题，特别是在现实新闻环境中。金融错误信息可能误导投资者并引发经济风险，因此开发可靠检测方法至关重要。现有大型语言模型在无参考设置下表现不佳，常出现不稳定的预测和无效输出，这源于模型缺乏外部基础来维持一致的信念状态。通过创建结构化基准，本研究旨在评估和改进模型的无参考推理能力，以提升金融信息可靠性。",
      "method": "本文设计了RFC Bench基准，它在段落级别处理金融新闻，以捕捉分散线索中的上下文复杂性。核心方法包括两个互补任务：无参考错误信息检测，要求模型独立判断段落真伪；以及基于比较的诊断任务，提供配对原始和扰动输入作为上下文。基准使用真实新闻数据构建，关键创新在于任务定义，通过对比不同上下文条件来揭示模型表现差异。该方法不涉及具体模型架构细节，但侧重于基准的设计和评估流程。",
      "result": "实验揭示了一致模式：当提供比较上下文时，大型语言模型在金融错误信息检测上性能显著更强；而在无参考设置下，模型暴露了显著弱点，包括预测不稳定和无效输出比例升高。这些结果表明当前模型难以在没有外部基础时保持一致的信念状态。尽管摘要未明确说明具体数据或基线对比，但模式凸显了两种任务间的性能差距，强调了无参考推理的挑战，为模型鲁棒性评估提供了实证支撑。",
      "conclusion": "本研究的核心贡献是RFC Bench基准的引入，它系统评估了大语言模型的无参考推理能力，并揭示了其在金融错误信息检测中的局限性。通过强调这一差距，研究提供了结构化测试平台，推动学术研究聚焦于模型推理机制改进，并助力开发更可靠的现实应用检测工具。潜在局限性包括数据集范围可能有限，未来工作可扩展基准涵盖更多场景或探索高级上下文建模技术，以进一步提升模型性能。",
      "tags": [
        "Large Language Models",
        "Misinformation Detection",
        "Benchmark Evaluation",
        "Reference-Free Reasoning",
        "Comparative Context"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:40.598036Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04157",
    "title": "FLEx: Language Modeling with Few-shot Language Explanations",
    "authors": [
      "Adar Avsian",
      "Christopher Richardson",
      "Anirudh Sundar",
      "Larry Heck"
    ],
    "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04157.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04157",
    "published": "2026-01-07T18:12:05Z",
    "updated": "2026-01-07T18:12:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "FLEx提出了一种基于少量语言解释的方法，通过聚类错误和总结解释来提升语言模型性能，无需修改模型权重。",
      "motivation": "语言模型在多种任务中表现出色，但仍存在错误，且这些错误在相关查询中常重复出现。自然语言解释能帮助纠正错误，但大规模收集这些解释可能不可行，尤其是在需要专家注释的领域。现有方法依赖于大量解释数据，限制了其实际应用。因此，本研究旨在开发一种能够利用少量解释性示例来有效改进模型行为的方法，以解决数据收集难题。",
      "method": "FLEx方法首先使用基于嵌入的聚类技术，从模型错误中挑选出代表性样本。然后，验证相关的自然语言解释是否有效纠正了这些错误，确保解释的可靠性。接着，将这些解释总结成一个提示前缀，在推理阶段将其添加到输入前，引导语言模型避免类似错误。核心创新在于结合少量示例和不修改权重的设计，通过提示工程提升性能，适用于多个数据集如CounterBench、GSM8K和ReasonIF。",
      "result": "在CounterBench、GSM8K和ReasonIF三个数据集上的评估显示，FLEx一致优于思维链（CoT）提示方法，在所有数据集上都取得了更好的性能。具体而言，FLEx减少了高达83%的CoT剩余错误，这表明通过少量解释性示例能显著提升模型的准确性和可靠性。实验数据证实了FLEx在减少错误方面的有效性，优于基线方法。",
      "conclusion": "本论文的主要贡献是提出了FLEx方法，它利用少量自然语言解释改善语言模型行为，无需更新模型权重。这一研究具有学术价值，为高效错误纠正提供了新思路，并推动了提示工程的发展。在实际应用中，FLEx降低了对大规模解释数据的依赖，使其在专家知识领域更易于部署。摘要未明确说明局限性，未来工作可探索解释选择策略或扩展到更复杂任务。",
      "tags": [
        "Few-shot Learning",
        "Language Modeling",
        "Explanation-based Learning",
        "Embedding Clustering",
        "Prompt Engineering"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:01.408529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04135",
    "title": "LLMberjack: Guided Trimming of Debate Trees for Multi-Party Conversation Creation",
    "authors": [
      "Leonardo Bottona",
      "Nicolò Penzo",
      "Bruno Lepri",
      "Marco Guerini",
      "Sara Tonelli"
    ],
    "abstract": "We present LLMberjack, a platform for creating multi-party conversations starting from existing debates, originally structured as reply trees. The system offers an interactive interface that visualizes discussion trees and enables users to construct coherent linearized dialogue sequences while preserving participant identity and discourse relations. It integrates optional large language model (LLM) assistance to support automatic editing of the messages and speakers' descriptions. We demonstrate the platform's utility by showing how tree visualization facilitates the creation of coherent, meaningful conversation threads and how LLM support enhances output quality while reducing human effort. The tool is open-source and designed to promote transparent and reproducible workflows to create multi-party conversations, addressing a lack of resources of this type.",
    "categories": [
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04135.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04135",
    "published": "2026-01-07T17:49:17Z",
    "updated": "2026-01-07T17:49:17Z",
    "comment": "9 pages, 3 figures",
    "light_analysis": {
      "overview": "LLMberjack平台结合交互式树可视化和大型语言模型辅助，从辩论树中创建多方对话序列。",
      "motivation": "本研究旨在解决多方对话资源缺乏的问题。现有辩论通常以回复树形式结构化，难以转化为连贯的线性对话，传统方法需大量人工编辑，效率低下且耗时。通过开发自动化工具，可以高效生成高质量对话数据，满足对话系统研究和应用中对多样数据的需求，推动相关领域发展。",
      "method": "论文提出LLMberjack平台，其核心方法包括一个交互式界面，可视化讨论树，使用户能引导修剪树结构，构建连贯对话序列，同时保留参与者身份和话语关系。平台集成可选的大型语言模型（LLM）辅助功能，支持自动编辑消息和演讲者描述，结合人工引导与AI自动化，优化对话生成流程。",
      "result": "实验显示，LLMberjack平台的树可视化功能有效促进创建连贯、有意义的对话线程，LLM支持提升输出质量并减少人工努力。摘要未明确说明具体性能指标如准确率，但通过演示验证了方法的实用性和效果，表明工具能够高效生成多方对话，简化工作流程。",
      "conclusion": "结论总结LLMberjack作为一个开源平台，促进了创建多方对话的透明和可重复工作流程，解决资源缺乏问题。主要贡献在于整合可视化交互和AI辅助，为对话生成领域提供新工具，具有学术和实际应用价值，如用于对话系统研究和数据集构建，未来可扩展到更广泛对话场景。",
      "tags": [
        "Large Language Model",
        "Interactive Visualization",
        "Debate Tree",
        "Multi-Party Conversation",
        "Natural Language Generation"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:18.695329Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04093",
    "title": "SearchAttack: Red-Teaming LLMs against Real-World Threats via Framing Unsafe Web Information-Seeking Tasks",
    "authors": [
      "Yu Yan",
      "Sheng Sun",
      "Mingfeng Li",
      "Zheming Yang",
      "Chiwei Zhu",
      "Fei Ma",
      "Benfeng Xu",
      "Min Liu"
    ],
    "abstract": "Recently, people have suffered and become increasingly aware of the unreliability gap in LLMs for open and knowledge-intensive tasks, and thus turn to search-augmented LLMs to mitigate this issue. However, when the search engine is triggered for harmful tasks, the outcome is no longer under the LLM's control. Once the returned content directly contains targeted, ready-to-use harmful takeaways, the LLM's safeguards cannot withdraw that exposure. Motivated by this dilemma, we identify web search as a critical attack surface and propose \\textbf{\\textit{SearchAttack}} for red-teaming. SearchAttack outsources the harmful semantics to web search, retaining only the query's skeleton and fragmented clues, and further steers LLMs to reconstruct the retrieved content via structural rubrics to achieve malicious goals. Extensive experiments are conducted to red-team the search-augmented LLMs for responsible vulnerability assessment. Empirically, SearchAttack demonstrates strong effectiveness in attacking these systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04093.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04093",
    "published": "2026-01-07T16:59:34Z",
    "updated": "2026-01-07T16:59:34Z",
    "comment": "We find that the key to jailbreak the LLM is objectifying its safety responsibility, thus we delegate the open-web to inject harmful semantics and get the huge gain from unmoderated web resources",
    "light_analysis": {
      "overview": "论文提出SearchAttack，一种针对搜索增强大型语言模型的红队攻击方法，通过模拟不安全网页搜索任务评估现实世界威胁。",
      "motivation": "大型语言模型在开放知识密集型任务中存在不可靠性，搜索增强LLMs被引入以缓解此问题。然而，当搜索引擎被用于有害任务时，返回内容可能直接包含目标有害信息，导致LLM安全防护失效。这凸显了搜索增强LLMs的安全漏洞，现有红队测试方法可能不足以有效模拟此类真实威胁，因此需要新方法进行漏洞评估和系统强化。",
      "method": "SearchAttack的核心方法是将有害语义外包给网页搜索，仅保留查询骨架和碎片化线索，然后通过结构性评分标准引导LLMs重构检索内容，以实现恶意目标。创新点在于识别网页搜索作为攻击表面，并设计这种外包和重构策略以绕过LLM安全防护。摘要未明确说明使用的具体数据集或模型架构等关键细节。",
      "result": "论文进行了大量实验来红队测试搜索增强LLMs，以进行负责任漏洞评估。实验结果显示，SearchAttack在攻击这些系统时表现出强大的有效性。然而，摘要未提供具体性能指标如攻击成功率或与基线方法的对比数据，因此效果细节未明确说明。",
      "conclusion": "本研究的核心贡献是识别网页搜索为关键攻击表面，并提出SearchAttack红队方法，有效评估搜索增强LLMs的安全漏洞。学术价值在于推动LLM安全研究，特别是针对现实世界威胁；实际应用价值在于支持负责任漏洞评估，有助于改进系统防护。未来工作可包括扩展攻击方法或开发更强防护措施，潜在局限性可能依赖于特定场景。",
      "tags": [
        "Large Language Models",
        "Red-Teaming",
        "Search-Augmented Generation",
        "Web Security",
        "Vulnerability Assessment"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:39.573121Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04086",
    "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
    "authors": [
      "Jinbo Hao",
      "Kai Yang",
      "Qingzhen Su",
      "Yifan Li",
      "Chao Jiang"
    ],
    "abstract": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04086.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04086",
    "published": "2026-01-07T16:54:20Z",
    "updated": "2026-01-07T16:54:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出KDCM框架，通过引入可编程代码模块和增强的蒸馏推理结构来减少大型语言模型中的提示诱导幻觉。",
      "motivation": "本研究针对大型语言模型在推理过程中容易产生幻觉的问题，特别是由输入提示引起的错误，这影响了模型输出的可靠性和实际应用价值。现有方法往往缺乏对中间推理步骤的显式控制，导致预测不可靠，因此需要一种能够有效约束推理过程的技术。作者专注于利用结构化知识探索来弥补这一不足，旨在提升模型的鲁棒性和准确性。",
      "method": "方法的核心是扩展链式知识蒸馏，创新性地引入一个可编程模块，该模块作为可执行代码嵌入推理提示中，以指导知识图的探索。这允许模型在推理时动态利用外部结构化知识。基于此设计，开发了一个增强的蒸馏推理框架，能够显式调节中间推理步骤，从而提高预测的可靠性。关键细节包括使用GPT-4和LLaMA-3.3作为基础模型，在多个公共基准上进行评估。",
      "result": "实验结果表明，代码引导的推理显著改善了上下文建模，并有效减少了由提示引起的幻觉。具体性能指标上，HIT@1、HIT@3和HIT@5分别提升了15.64%、13.38%和13.28%，在多个评估设置中得分超过95%。这些改进显示了该方法在约束错误推理方面优于现有基线，同时提高了准确性和可解释性。",
      "conclusion": "本研究的主要贡献是提出了一个新颖框架，通过集成可编程模块和显式推理结构来减少大型语言模型中的幻觉，从而增强预测的可靠性和解释性。学术价值在于为知识蒸馏和推理增强提供了创新思路，具有实际应用潜力，如改进智能问答系统的准确性。未来工作可以探索方法在更复杂场景下的泛化能力或整合更多外部知识源。",
      "tags": [
        "Large Language Model",
        "Knowledge Distillation",
        "Knowledge Graph",
        "Code-guided Reasoning",
        "Programmable Module"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:15.988711Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04056",
    "title": "Bridging the Discrete-Continuous Gap: Unified Multimodal Generation via Coupled Manifold Discrete Absorbing Diffusion",
    "authors": [
      "Yuanfeng Xu",
      "Yuhao Chen",
      "Liang Lin",
      "Guangrun Wang"
    ],
    "abstract": "The bifurcation of generative modeling into autoregressive approaches for discrete data (text) and diffusion approaches for continuous data (images) hinders the development of truly unified multimodal systems. While Masked Language Models (MLMs) offer efficient bidirectional context, they traditionally lack the generative fidelity of autoregressive models and the semantic continuity of diffusion models. Furthermore, extending masked generation to multimodal settings introduces severe alignment challenges and training instability. In this work, we propose \\textbf{CoM-DAD} (\\textbf{Co}upled \\textbf{M}anifold \\textbf{D}iscrete \\textbf{A}bsorbing \\textbf{D}iffusion), a novel probabilistic framework that reformulates multimodal generation as a hierarchical dual-process. CoM-DAD decouples high-level semantic planning from low-level token synthesis. First, we model the semantic manifold via a continuous latent diffusion process; second, we treat token generation as a discrete absorbing diffusion process, regulated by a \\textbf{Variable-Rate Noise Schedule}, conditioned on these evolving semantic priors. Crucially, we introduce a \\textbf{Stochastic Mixed-Modal Transport} strategy that aligns disparate modalities without requiring heavy contrastive dual-encoders. Our method demonstrates superior stability over standard masked modeling, establishing a new paradigm for scalable, unified text-image generation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04056.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04056",
    "published": "2026-01-07T16:21:19Z",
    "updated": "2026-01-07T16:21:19Z",
    "comment": "10 pages, 5 figures",
    "light_analysis": {
      "overview": "本文提出CoM-DAD框架，通过耦合连续和离散扩散过程，实现统一的多模态文本-图像生成，解决了离散与连续数据生成的分裂问题。",
      "motivation": "生成建模分为处理离散数据（如文本）的自回归方法和处理连续数据（如图像）的扩散方法，这种分裂阻碍了统一多模态系统的开发。现有方法如掩码语言模型（MLMs）虽然提供高效双向上下文，但缺乏自回归模型的生成保真度和扩散模型的语义连续性。此外，扩展到多模态场景时，存在严重的模态对齐挑战和训练不稳定问题，影响了实际应用。因此，需要一种能够桥接离散-连续间隙的稳定生成框架。",
      "method": "CoM-DAD框架将多模态生成重新定义为分层双过程：首先，通过连续潜在扩散过程建模高层语义流形，实现语义规划；其次，将token生成视为离散吸收扩散过程，使用变量速率噪声调度进行调节，并基于演化的语义先验。关键创新包括引入随机混合模态传输策略，无需繁重的对比双编码器即可对齐不同模态，提升训练稳定性。该方法结合了连续和离散扩散的优势，通过解耦语义和token层级处理多模态数据。",
      "result": "论文表明CoM-DAD在标准掩码建模上表现出优越的稳定性，为可扩展的统一文本-图像生成建立了新范式。摘要未提供具体实验数据，但强调了该方法在克服训练不稳定性和模态对齐挑战方面的有效性，可能优于现有基线方法。这有助于推动多模态生成系统的实际部署，展示了在保持生成质量的同时提升稳定性的潜力。",
      "conclusion": "CoM-DAD框架的主要贡献是提出了一种统一的概率方法，桥接离散和连续数据的生成，通过分层解耦和耦合扩散过程实现稳健的多模态生成。其学术价值在于为多模态AI提供了新范式，促进跨模态生成研究；实际应用价值有助于开发更稳定和可扩展的统一生成系统。未来工作可能包括扩展到更多模态（如音频）、优化性能指标，以及在实际场景中验证泛化能力。",
      "tags": [
        "Multimodal Generation",
        "Discrete Absorbing Diffusion",
        "Stochastic Mixed-Modal Transport",
        "Variable-Rate Noise Schedule"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:22.751620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04055",
    "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
    "authors": [
      "Prith Sharma",
      "Austin Z. Henley"
    ],
    "abstract": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.04055.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04055",
    "published": "2026-01-07T16:20:08Z",
    "updated": "2026-01-07T16:20:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了 Modular Prompt Optimization (MPO) 框架，通过基于固定语义模式的提示优化和局部文本梯度，有效提升小型开源大型语言模型的推理性能。",
      "motivation": "大型语言模型（LLMs）的性能高度依赖提示质量，尤其是对于依赖显式结构的小型开源指令调优模型。现有自动提示优化方法（如文本梯度和自优化）将提示视为整体文本块，导致难以精确定位错误、保留关键指令或防止提示无控制增长，限制了优化的有效性和可解释性。因此，需要一种结构化方法来克服这些不足，实现更精确、高效的提示优化。",
      "method": "MPO 框架将提示定义为结构化对象，包含固定语义部分（如系统角色、上下文、任务描述、约束和输出格式），应用 critic 语言模型生成的 section-local textual gradients，独立优化每个部分同时保持整体提示模式不变。优化过程中通过去重整合部分更新，减少冗余和部分间干扰，提高可解释性和鲁棒性。研究在 ARC-Challenge 和 MMLU 基准测试上评估，使用 LLaMA-3 8B-Instruct 和 Mistral-7B-Instruct 作为求解模型。",
      "result": "在 ARC-Challenge 和 MMLU 两个推理基准测试中，使用 LLaMA-3 8B-Instruct 和 Mistral-7B-Instruct 模型，MPO 框架一致优于未调优的结构化提示和 TextGrad 基线方法。该框架在不修改模型参数或改变提示结构的情况下，实现了显著的准确性提升，证明了其有效性和实用性，为小型开源 LLMs 的推理性能提供了可靠优化路径。",
      "conclusion": "论文的主要贡献是 MPO 框架，通过模块化提示优化提升小型开源 LLMs 的推理性能，证明了在固定提示模式下应用局部优化的有效性。该研究具有学术价值，为提示优化领域提供了新方法，并在实际应用中可改进现有模型的性能。未来工作方面，摘要未明确说明，但可推断可能包括扩展到更复杂任务或更大模型，以及探索其他优化技术的结合。",
      "tags": [
        "Prompt Optimization",
        "Textual Gradients",
        "Structured Prompts",
        "Modular Design",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:21.365220Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04194",
    "title": "Choreographing a World of Dynamic Objects",
    "authors": [
      "Yanzhe Lyu",
      "Chen Geng",
      "Karthik Dharmarajan",
      "Yunzhi Zhang",
      "Hadi Alzayer",
      "Shangzhe Wu",
      "Jiajun Wu"
    ],
    "abstract": "Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04194.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04194",
    "published": "2026-01-07T18:59:40Z",
    "updated": "2026-01-07T18:59:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "CHORD提出一种通用生成管道，通过蒸馏从2D视频提取拉格朗日运动信息，以合成动态4D场景。",
      "motivation": "物理世界中的动态对象不断演变、变形和交互，生成4D场景动态需要高效方法。传统基于规则的方法依赖类别特定启发式，劳动密集型且不扩展；学习型方法需大规模数据集，但可能无法覆盖所有类别。因此，研究旨在解决通用、无需大量数据的动态生成问题，以提高效率和适用性。",
      "method": "CHORD采用蒸馏基管道，从2D视频的欧拉表示中提取隐藏的拉格朗日运动信息。该方法继承视频生成模型的通用性，实现类别无关的4D动态合成，无需特定对象类别数据。核心创新在于将复杂运动信息从视频表示中蒸馏出来，提升了生成多样性和可扩展性。",
      "result": "实验表明CHORD能生成多样化多体4D动态，与现有方法相比展示优势，并应用于机器人操作策略生成，验证了方法的有效性。摘要未提供具体性能指标（如准确率提升），但通过多个场景案例展示了通用性和应用潜力。",
      "conclusion": "CHORD提供了一种通用、高效的动态场景生成方法，克服传统方法的限制。学术价值在于提出新蒸馏技术，应用价值扩展到机器人等领域，为4D合成开辟新途径。摘要未明确说明局限性，未来工作可能涉及提升精度和扩展应用范围。",
      "tags": [
        "Generative Models",
        "Video Generation",
        "Distillation",
        "Lagrangian Motion",
        "4D Dynamics"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:01.949173Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04185",
    "title": "ImLoc: Revisiting Visual Localization with Image-based Representation",
    "authors": [
      "Xudong Jiang",
      "Fangjinhua Wang",
      "Silvano Galliani",
      "Christoph Vogel",
      "Marc Pollefeys"
    ],
    "abstract": "Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04185.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04185",
    "published": "2026-01-07T18:51:51Z",
    "updated": "2026-01-07T18:51:51Z",
    "comment": "Code will be available at https://github.com/cvg/Hierarchical-Localization",
    "light_analysis": {
      "overview": "本研究提出了一种结合估计深度图的2D图像表示方法，用于视觉定位，实现了高精度和高效率。",
      "motivation": "现有视觉定位方法存在显著局限性：2D图像基础方法易于构建和维护，但几何推理能力有限；3D结构基础方法精度高，但需要集中重建且更新困难。这在实际应用中导致精度与可维护性之间的权衡问题。本研究旨在通过结合两种方法的优势，开发一种既准确又易于更新的视觉定位方案，以应对挑战性环境中的定位需求。",
      "method": "该方法采用基于图像的表示，每个图像通过估计深度图增强以捕获几何结构。核心创新在于利用密集匹配器进行有效匹配，提高定位精度。技术特色包括使用紧凑压缩减少存储需求，以及GPU加速的LO-RANSAC实现优化计算效率。整个流程避免了复杂的3D重建，依赖于图像数据库和高效算法，实现灵活权衡。",
      "result": "在标准基准测试中，该方法达到新的最先进精度，优于现有方法。具体表现在挑战性条件下实现最高准确度，并在相同地图大小下，性能超过其他内存高效方法。摘要未明确说明具体性能指标如准确率数值，但强调了其基准测试中的优势，显示出高效存储和计算下的优异表现。",
      "conclusion": "本研究的主要贡献是提出了一种结合精度和效率的视觉定位方法，通过图像表示和深度增强实现高精度与低存储。这为解决视觉定位中的传统权衡问题提供了创新思路，具有学术价值并适用于机器人导航、增强现实等实际应用。潜在局限性包括摘要未明确说明，未来工作可包括算法优化或扩展至更多复杂场景。",
      "tags": [
        "Visual Localization",
        "Image-based Representation",
        "Depth Estimation",
        "Dense Matchers",
        "LO-RANSAC"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:29.828839Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04159",
    "title": "ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography",
    "authors": [
      "Vladimir Frants",
      "Sos Agaian",
      "Karen Panetta"
    ],
    "abstract": "Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04159.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04159",
    "published": "2026-01-07T18:15:09Z",
    "updated": "2026-01-07T18:15:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了ToTMNet，一个轻量级远程光电容积描记术架构，利用FFT加速的Toeplitz时间混合层替代传统时间注意力，实现高效长距离时间滤波和低计算成本。",
      "motivation": "远程光电容积描记术（rPPG）通过面部视频估计心率波形，在医疗监控和健康应用中至关重要。现有深度学习方法虽提升鲁棒性，但增加了计算成本和参数数量，特别是基于注意力的时间建模引入与时间长度平方相关的计算复杂度，限制了在资源受限环境下的部署。因此，迫切需要开发轻量级、高效的时间建模方法，以平衡性能和效率，促进实际应用。",
      "method": "ToTMNet采用FFT加速的Toeplitz时间混合层作为核心，以Toeplitz算子替代时间注意力。该算子通过线性参数提供全序列时间感受野，并借助循环嵌入和基于快速傅里叶变换（FFT）的卷积实现在近线性时间内的计算。模型整合了一个紧凑的门控时间混合器，结合局部深度时间卷积分支与门控全局Toeplitz混合，实现高效长距离时间滤波。整个架构仅有63k参数，并在UBFC-rPPG和SCAMPS数据集上进行验证，以轻量级设计应对计算挑战。",
      "result": "在实验中，ToTMNet在UBFC-rPPG数据集（真实视频）的内部评估中，达到1.055 bpm的平均绝对误差（MAE）和0.996的皮尔逊相关性，表现出高精度的心率估计。在合成到真实设置中（从SCAMPS合成视频到UBFC-rPPG），MAE为1.582 bpm，相关性为0.994。消融结果证实门控机制对有效利用全局Toeplitz混合至关重要，尤其在域偏移情况下。虽然摘要未明确对比基线方法，但数据表明模型在保持紧凑设计的同时实现了强性能。",
      "conclusion": "本文的主要贡献是证明了Toeplitz结构的时间混合作为注意力机制的实用高效替代方案，特别适用于轻量级rPPG应用。ToTMNet在降低参数和计算成本的同时保持高准确性，具有实际部署价值，推动了rPPG技术的发展。局限性在于仅使用两个数据集进行验证，未来工作可扩展到更多数据集以增强鲁棒性。总体而言，该方法为时间建模提供了新思路，在医疗和健康领域具有应用潜力。",
      "tags": [
        "Toeplitz Operator",
        "FFT-accelerated Convolution",
        "Temporal Mixing",
        "Remote Photoplethysmography",
        "Gated Mechanism"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:37.991528Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04153",
    "title": "Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning",
    "authors": [
      "Yifan Wang",
      "Yanyu Li",
      "Sergey Tulyakov",
      "Yun Fu",
      "Anil Kag"
    ],
    "abstract": "Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04153.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04153",
    "published": "2026-01-07T18:05:08Z",
    "updated": "2026-01-07T18:05:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "Diffusion-DRF提出了一种可微分奖励流方法，利用冻结的Vision-Language Model微调视频扩散模型，改善视频生成质量和训练稳定性。",
      "motivation": "当前Direct Preference Optimization (DPO)方法在文本到视频生成中依赖于人类标注或学习奖励模型的不可微分偏好信号，这导致训练过程需要大量标签、易产生偏见和奖励黑客攻击，使训练不稳定。为了解决这些问题，本研究旨在开发一种无需额外奖励模型或偏好数据集的微调方法，以提高可靠性和效率，克服现有方法的不足之处。",
      "method": "Diffusion-DRF使用冻结的现成Vision-Language Model (VLM)作为无需训练的批评者，通过扩散去噪链直接反向传播VLM反馈，将logit级响应转换为token感知梯度进行优化。关键创新包括自动化、方面结构化的提示管道获取可靠的多维VLM反馈，以及梯度检查点技术以实现通过最终去噪步骤的高效更新。该方法模型无关，可推广到其他基于扩散的生成任务。",
      "result": "论文摘要未明确提供具体实验数据，但指出Diffusion-DRF能够提高视频质量和语义对齐，同时有效缓解奖励黑客攻击和训练崩溃问题。与依赖不可微分偏好信号的方法相比，该方法在不需要额外奖励模型或偏好数据集的情况下，实现了更稳定的训练过程和改进的生成效果。具体性能指标如准确率提升等在摘要中未详细说明。",
      "conclusion": "结论表明，Diffusion-DRF成功解决了当前DPO方法在微调视频扩散模型时面临的挑战，通过引入可微分奖励流概念，避免了依赖不可微分偏好信号带来的问题。其学术价值在于提出了一种高效的、无需额外资源的训练策略，实际应用价值体现在模型无关性和对基于扩散的生成任务的广泛适用性。未来工作可能包括进一步优化反馈机制和扩展到更多领域。",
      "tags": [
        "Diffusion Models",
        "Direct Preference Optimization",
        "Vision-Language Model",
        "Differentiable Reward Flow",
        "Gradient Backpropagation"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:34.501658Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04121",
    "title": "MORPHFED: Federated Learning for Cross-institutional Blood Morphology Analysis",
    "authors": [
      "Gabriel Ansah",
      "Eden Ruffell",
      "Delmiro Fernandez-Reyes",
      "Petru Manescu"
    ],
    "abstract": "Automated blood morphology analysis can support hematological diagnostics in low- and middle-income countries (LMICs) but remains sensitive to dataset shifts from staining variability, imaging differences, and rare morphologies. Building centralized datasets to capture this diversity is often infeasible due to privacy regulations and data-sharing restrictions. We introduce a federated learning framework for white blood cell morphology analysis that enables collaborative training across institutions without exchanging training data. Using blood films from multiple clinical sites, our federated models learn robust, domain-invariant representations while preserving complete data privacy. Evaluations across convolutional and transformer-based architectures show that federated training achieves strong cross-site performance and improved generalization to unseen institutions compared to centralized training. These findings highlight federated learning as a practical and privacy-preserving approach for developing equitable, scalable, and generalizable medical imaging AI in resource-limited healthcare environments.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04121.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04121",
    "published": "2026-01-07T17:32:24Z",
    "updated": "2026-01-07T17:32:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MORPHFED联邦学习框架，用于跨机构血形态学分析，实现隐私保护下的协作训练，提高模型泛化能力。",
      "motivation": "自动化血形态学分析在低收入和中等收入国家支持血液学诊断，但对染色变异性、成像差异和罕见形态等数据偏移敏感。现有方法依赖集中式数据集，但由于隐私法规和数据共享限制，构建此类数据集不可行，导致模型在处理跨机构数据时泛化能力不足。这一问题限制了医疗AI在资源有限环境中的公平性和可扩展性，因此需要一种能在保护数据隐私的同时提高模型性能的新方法。",
      "method": "论文提出MORPHFED联邦学习框架，专注于白细胞形态分析。核心方法是让多个临床站点在不交换原始训练数据的情况下协作训练模型，以保护数据隐私。关键创新在于学习领域不变的表示，以应对不同站点的染色和成像差异。技术细节包括使用卷积神经网络和基于变换器的架构进行模型设计和评估，训练数据来自多个临床站点的血涂片，从而增强模型的稳健性和适应性。",
      "result": "实验评估表明，联邦训练在卷积和变换器架构上均实现了强大的跨站点性能，相比集中训练，显著提高了对未见过机构的泛化能力。具体数据如准确率提升等未在摘要中明确说明，但结果强调了联邦模型能够有效处理数据偏移，在不同临床站点间实现一致的高性能。这表明联邦学习在医学影像分析中具有优势，特别是在处理多样化和隐私敏感的数据时。",
      "conclusion": "该研究的主要贡献是证明了联邦学习作为一种实用且隐私保护的方法，适用于资源有限的医疗环境中开发公平、可扩展和可泛化的医学影像AI。其学术价值在于推动协作学习在不牺牲数据隐私前提下的应用，实际应用价值在于促进跨机构医学诊断工具的开发。未来工作可能涉及扩展到其他医疗领域或进一步优化模型效率，以应对更多挑战。",
      "tags": [
        "Federated Learning",
        "White Blood Cell Morphology Analysis",
        "Domain-invariant Representations",
        "Convolutional Neural Networks",
        "Transformer-based Architectures"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:10.740918Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04118",
    "title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning",
    "authors": [
      "Wenshuai Li",
      "Xiantai Xiang",
      "Zixiao Wen",
      "Guangyao Zhou",
      "Ben Niu",
      "Feng Wang",
      "Lijia Huang",
      "Qiantong Wang",
      "Yuxin Hu"
    ],
    "abstract": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04118.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04118",
    "published": "2026-01-07T17:26:41Z",
    "updated": "2026-01-07T17:26:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "GeoReason 框架通过逻辑一致性强化学习，增强了遥感视觉语言模型的推理可靠性和可解释性。",
      "motivation": "随着遥感视觉语言模型（RS-VLMs）从感知向高层次演绎推理演进，当前模型在复杂空间任务中常出现逻辑幻觉问题，即正确结果源于错误推理链或依赖位置捷径，而非空间逻辑。这种内部思维与外部答案的解耦严重损害了战略决策的可靠性。研究旨在解决这一缺陷，通过同步模型推理过程与最终决策，以提升认知可靠性，填补现有方法在逻辑一致性方面的不足。",
      "method": "论文提出 GeoReason 框架，首先构建逻辑驱动的数据集 GeoReason-Bench，包含 4,000 个基于几何原语和专家知识合成的推理轨迹。训练采用两阶段策略：监督知识初始化使模型掌握推理语法和领域专业知识；一致性感知强化学习通过集成新颖的逻辑一致性奖励来优化演绎可靠性，该奖励利用选项排列策略惩罚逻辑漂移，确保决策基于可验证的推理轨迹。",
      "result": "实验结果显示，GeoReason 框架显著提升了 RS-VLMs 的认知可靠性和可解释性，与其他先进方法相比实现了最先进的性能。摘要未明确说明具体性能指标如准确率提升，但强调框架在减少逻辑幻觉和增强决策可靠性方面效果显著，验证了逻辑一致性强化学习的有效性。",
      "conclusion": "主要贡献是开发了 GeoReason 框架，通过逻辑一致性强化学习同步模型的内部推理与外部输出，增强了遥感任务中的战略决策可靠性。这为高层次的视觉语言推理提供了新方法，具有重要的学术价值（如推动认知 AI 研究）和实际应用价值（如提升遥感分析精度）。未来工作可扩展数据集或应用于更广泛的视觉推理任务。",
      "tags": [
        "Remote Sensing Vision-Language Models",
        "Reinforcement Learning",
        "Logical Consistency",
        "Reasoning",
        "Dataset Construction"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:32.928665Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04090",
    "title": "Gen3R: 3D Scene Generation Meets Feed-Forward Reconstruction",
    "authors": [
      "Jiaxin Huang",
      "Yuanbo Yang",
      "Bangbang Yang",
      "Lin Ma",
      "Yuewen Ma",
      "Yiyi Liao"
    ],
    "abstract": "We present Gen3R, a method that bridges the strong priors of foundational reconstruction models and video diffusion models for scene-level 3D generation. We repurpose the VGGT reconstruction model to produce geometric latents by training an adapter on its tokens, which are regularized to align with the appearance latents of pre-trained video diffusion models. By jointly generating these disentangled yet aligned latents, Gen3R produces both RGB videos and corresponding 3D geometry, including camera poses, depth maps, and global point clouds. Experiments demonstrate that our approach achieves state-of-the-art results in single- and multi-image conditioned 3D scene generation. Additionally, our method can enhance the robustness of reconstruction by leveraging generative priors, demonstrating the mutual benefit of tightly coupling reconstruction and generative models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04090.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04090",
    "published": "2026-01-07T16:57:30Z",
    "updated": "2026-01-07T16:57:30Z",
    "comment": "Project page: https://xdimlab.github.io/Gen3R/",
    "light_analysis": {
      "overview": "Gen3R提出了一种桥接基础重建模型和视频扩散模型的创新方法，通过联合生成几何和外观潜在变量，实现了高效的场景级3D内容生成。",
      "motivation": "该研究旨在解决3D场景生成中如何有效结合重建模型和生成模型的先验知识问题。在计算机视觉和图形学领域，高效生成准确的3D场景对虚拟现实、自动驾驶等应用至关重要。现有方法通常独立处理重建和生成，可能导致生成质量不足或效率低下。因此，桥接这两种模型能够利用它们的互补优势，提升场景生成的完整性和鲁棒性，以应对复杂环境的需求。",
      "method": "Gen3R的核心方法是重新利用VGGT重建模型，通过训练一个适配器从其tokens中提取几何潜在变量，并将这些变量正则化以与预训练视频扩散模型的外观潜在变量对齐。关键创新点在于解耦几何和外观表示，并通过联合生成过程实现两者同步。这允许系统在单次推理中输出RGB视频和对应的3D几何信息，包括相机姿态、深度图和全局点云，无需额外复杂处理步骤。",
      "result": "实验结果表明，Gen3R在单图像和多图像条件下的3D场景生成任务中达到了最先进的性能。具体来说，它在生成精度和一致性方面优于基线方法，并显著提升了重建的鲁棒性。通过集成生成先验，方法在复杂场景下展现出更强的适应能力，验证了紧密耦合重建和生成模型的互惠效果，尽管摘要未明确说明具体数值指标。",
      "conclusion": "该研究的主要贡献是开发了Gen3R框架，成功桥接重建和生成模型，为3D场景生成提供了新范式。其学术价值在于促进多模态AI技术融合，推动场景理解领域的发展；实际应用潜力包括虚拟现实、游戏和机器人导航等。局限性可能在于对特定预训练模型的依赖，未来工作可探索更多模型集成和更大规模数据集评估。",
      "tags": [
        "3D Scene Generation",
        "Video Diffusion Models",
        "Reconstruction Models",
        "Geometric Latents",
        "Latent Space Alignment"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:21.532432Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04065",
    "title": "Unsupervised Modular Adaptive Region Growing and RegionMix Classification for Wind Turbine Segmentation",
    "authors": [
      "Raül Pérez-Gonzalo",
      "Riccardo Magro",
      "Andreas Espersen",
      "Antonio Agudo"
    ],
    "abstract": "Reliable operation of wind turbines requires frequent inspections, as even minor surface damages can degrade aerodynamic performance, reduce energy output, and accelerate blade wear. Central to automating these inspections is the accurate segmentation of turbine blades from visual data. This task is traditionally addressed through dense, pixel-wise deep learning models. However, such methods demand extensive annotated datasets, posing scalability challenges. In this work, we introduce an annotation-efficient segmentation approach that reframes the pixel-level task into a binary region classification problem. Image regions are generated using a fully unsupervised, interpretable Modular Adaptive Region Growing technique, guided by image-specific Adaptive Thresholding and enhanced by a Region Merging process that consolidates fragmented areas into coherent segments. To improve generalization and classification robustness, we introduce RegionMix, an augmentation strategy that synthesizes new training samples by combining distinct regions. Our framework demonstrates state-of-the-art segmentation accuracy and strong cross-site generalization by consistently segmenting turbine blades across distinct windfarms.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.04065.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04065",
    "published": "2026-01-07T16:29:52Z",
    "updated": "2026-01-07T16:29:52Z",
    "comment": "Accepted to WACV 2026",
    "light_analysis": {
      "overview": "本文提出一种注释高效的风涡轮叶片分割方法，通过无监督模块化自适应区域生长和RegionMix增强策略，将像素级任务转化为二值区域分类问题。",
      "motivation": "风涡轮可靠运行需要频繁检查，叶片表面损伤会降低气动性能、减少能量输出并加速磨损，自动化检查依赖准确分割叶片。传统方法采用密集像素级深度学习模型，需要大量标注数据集，导致可扩展性挑战。本研究旨在解决高标注需求问题，提升分割效率，以适应实际应用中对成本控制和泛化能力的迫切需求。",
      "method": "研究引入一种注释高效分割框架，首先使用无监督模块化自适应区域生长技术生成图像区域，该技术基于图像特定自适应阈值指导，并通过区域合并整合碎片化区域。然后将像素级分割重构为二值区域分类问题，以降低标注需求。关键创新包括RegionMix增强策略，通过结合不同区域合成新训练样本，提升泛化能力和分类鲁棒性。",
      "result": "该框架在风涡轮叶片分割任务中达到最先进准确性，并展现出强的跨站点泛化能力，在不同风电场一致分割叶片。摘要未明确说明具体性能指标或基线对比，但强调了分割准确性和泛化优势，表明方法在减少标注依赖的同时保持高效能。",
      "conclusion": "本研究的主要贡献是提出一种基于区域生长和增强的注释高效分割方法，通过技术革新减少了标注需求，提升了分割准确性和跨环境泛化能力，为风涡轮自动化检查提供实用解决方案。学术价值在于推进了无监督学习在分割任务中的应用，实际应用有助于降低检查成本。未来工作可扩展至更多场景验证或探索其他对象分割任务。",
      "tags": [
        "Unsupervised Learning",
        "Adaptive Region Growing",
        "RegionMix",
        "Binary Classification",
        "Image Segmentation"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:11.093501Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04181",
    "title": "Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition",
    "authors": [
      "Nia Touko",
      "Matthew O A Ellis",
      "Cristiano Capone",
      "Alessio Burrello",
      "Elisa Donati",
      "Luca Manneschi"
    ],
    "abstract": "Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, \"plug-and-play\" myoelectric control for long-term prosthetic use.",
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04181.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04181",
    "published": "2026-01-07T18:48:31Z",
    "updated": "2026-01-07T18:48:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种轻量级测试时适配框架，通过时域卷积网络和三种创新策略，解决了基于表面肌电图手势识别中的信号漂移问题，提升了长期解码稳定性。",
      "motivation": "基于表面肌电图的手势识别在长期使用中面临信号漂移问题，如电极移位、肌肉疲劳和姿势变化，导致解码性能跨会话时显著下降。现有方法虽在单次会话中准确率高，但通常需要大规模数据集或高计算资源，不适用于能效型可穿戴设备，限制了其在长期假肢控制中的实用性。因此，开发轻量化的适配方案成为关键挑战，以应对实时变化并确保可靠性。",
      "method": "本研究以时域卷积网络（TCN）为骨干，设计了三种测试时适配策略：因果自适应批归一化用于实时统计对齐；高斯混合模型（GMM）对齐结合经验回放以防止模型遗忘；元学习用于快速少量校准。这些策略部署就绪，能在有限数据下运行，评估使用NinaPro DB6多会话数据集，确保了方法在实际场景中的可行性和轻量化特性。",
      "result": "在NinaPro DB6数据集上的实验表明，该框架显著缩小了跨会话准确率差距，同时开销最小。经验回放更新在数据有限时提供优越稳定性，减少了性能波动；元学习在一和两样本制度下实现竞争性能，仅需当前基准所需数据的一小部分。这些结果验证了方法在提高长期解码可靠性和效率方面的有效性，摘要未明确说明具体基线对比数据。",
      "conclusion": "本研究通过轻量级测试时适配框架，显著提升了基于表面肌电图手势识别的长期稳定性和健壮性，为“即插即用”肌电控制奠定了技术基础，尤其适用于假肢等长期应用。贡献在于提出了计算高效的适配策略，解决了现有方法对大数据和高计算的依赖。未来工作可能包括进一步优化策略或扩展到其他生物信号领域，以增强实际部署的适应性。",
      "tags": [
        "Test-Time Adaptation",
        "Temporal Convolutional Network",
        "Gaussian Mixture Model",
        "Meta-Learning",
        "EMG-Based Gesture Recognition"
      ]
    },
    "analyzed_at": "2026-01-08T17:09:16.407557Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04176",
    "title": "Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation",
    "authors": [
      "Pietro de Oliveira Esteves"
    ],
    "abstract": "We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method's generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.",
    "categories": [
      "cs.LG",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04176.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04176",
    "published": "2026-01-07T18:43:11Z",
    "updated": "2026-01-07T18:43:11Z",
    "comment": "9 pages, 4 figures, 2 tables. Code available at https://github.com/p-esteves/pinn-nlse-2026",
    "light_analysis": {
      "overview": "提出一种结合物理信息神经网络的深度学习框架，能在高噪声条件下从非线性薛定谔方程稳健恢复物理参数。",
      "motivation": "本研究旨在解决非线性薛定谔方程逆问题中，数据稀疏且受高噪声污染时的参数恢复挑战。传统有限差分方法因数值导数对噪声敏感，导致噪声放大和失败，限制了在实验数据稀缺、噪声大的场景下的应用。现有优化方法难以处理此类不确定性，因此开发一种能整合物理约束的稳健方法对提升时空动力学研究精度至关重要。",
      "method": "论文提出一个基于物理信息神经网络（PINNs）的框架，通过自动微分技术避免数值导数的噪声问题。方法使用稀疏、随机采样的数据点，并引入物理方程作为正则化项来训练神经网络，以从被20%加性高斯噪声破坏的观测中恢复非线性系数beta等参数。关键创新在于将非线性薛定谔方程直接嵌入学习过程，实现物理约束下的稳健学习。",
      "result": "实验结果显示，在20%加性高斯噪声下，使用仅500个数据点，非线性系数beta的重构相对误差小于0.2%。方法在不同beta值（0.5至2.0）和数据点数量（100至1000）下均保持亚1%准确度，统计分析显示标准差小于0.15%（对于beta=1.0）。整个流程在NVIDIA Tesla T4 GPU上约80分钟完成，优于传统有限差分方法。",
      "conclusion": "研究证明基于物理的正则化能有效过滤高测量噪声，使PINNs成为处理稀疏噪声数据逆问题的可行替代方案，为时空动力学参数恢复提供了新方法。这具有学术价值，推动深度学习在物理建模中的应用，实际中可促进实验数据困难领域的研究。代码公开确保可重复性，未来工作可能扩展到更复杂系统或优化计算效率。",
      "tags": [
        "Physics-Informed Neural Networks",
        "Automatic Differentiation",
        "Nonlinear Schrödinger Equation",
        "Inverse Problems"
      ]
    },
    "analyzed_at": "2026-01-08T17:10:30.282733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04171",
    "title": "Agentic Rubrics as Contextual Verifiers for SWE Agents",
    "authors": [
      "Mohit Raghavendra",
      "Anisha Gunjal",
      "Bing Liu",
      "Yunzhong He"
    ],
    "abstract": "Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04171.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04171",
    "published": "2026-01-07T18:38:23Z",
    "updated": "2026-01-07T18:38:23Z",
    "comment": "31 pages, 11 Figures",
    "light_analysis": {
      "overview": "论文提出Agentic Rubrics方法，作为一种基于上下文的无需代码执行验证机制，以提升软件工程代理的验证效率和可扩展性。",
      "motivation": "验证在软件工程代理中至关重要，它为强化学习提供奖励信号，并通过测试时缩放实现推理时增益。然而，现有方法如代码执行因环境设置开销难以扩展；而替代方案如补丁分类器和启发式方法虽可扩展，但缺乏代码库上下文基础，导致验证结果不够精确且难以解释。因此，亟需一种更高效、可扩展且基于上下文的验证方法来解决这些问题。",
      "method": "本文提出Agentic Rubrics方法，核心是通过专家代理（基于大型语言模型如Qwen）与代码仓库交互，收集上下文信息以生成特定于代码库的准则检查表。该检查表包含明确的标准，用于直接评分候选补丁，无需执行测试。关键创新包括上下文感知的准则构建和免测试验证机制，实现了细粒度和可解释的评估。方法在SWE-Bench数据集上实施，结合并行TTS评估以提升效率。",
      "result": "在SWE-Bench Verified数据集上，采用并行TTS评估，Agentic Rubrics在Qwen3-Coder-30B-A3B模型上得分为54.2%，在Qwen3-32B模型上为40.6%，比最强基线方法至少提高3.5个百分点。分析显示，准则分数与真实测试结果一致，并能标识测试未覆盖的潜在问题。消融实验证实，代理上下文收集是生成代码库特定、明确标准的关键因素，验证了方法的有效性。",
      "conclusion": "Agentic Rubrics提供了一种高效、可扩展和细粒度的验证信号，显著改进软件工程代理的验证机制，弥补了现有方法的不足。其学术价值在于创新验证方法，实际应用价值在于提升代理性能和可解释性。未来工作可优化准则生成过程、扩展应用到其他领域或集成更复杂的交互策略，以进一步提高普适性和精度。",
      "tags": [
        "Agentic Rubrics",
        "Contextual Verification",
        "Software Engineering Agents",
        "Test-Time Scaling",
        "Large Language Models"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:15.980393Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04110",
    "title": "Causal Data Augmentation for Robust Fine-Tuning of Tabular Foundation Models",
    "authors": [
      "Magnus Bühler",
      "Lennart Purucker",
      "Frank Hutter"
    ],
    "abstract": "Fine-tuning tabular foundation models (TFMs) under data scarcity is challenging, as early stopping on even scarcer validation data often fails to capture true generalization performance. We propose CausalMixFT, a method that enhances fine-tuning robustness and downstream performance by generating structurally consistent synthetic samples using Structural Causal Models (SCMs) fitted on the target dataset. This approach augments limited real data with causally informed synthetic examples, preserving feature dependencies while expanding training diversity. Evaluated across 33 classification datasets from TabArena and over 2300 fine-tuning runs, our CausalMixFT method consistently improves median normalized ROC-AUC from 0.10 (standard fine-tuning) to 0.12, outperforming purely statistical generators such as CTGAN (-0.01), TabEBM (-0.04), and TableAugment (-0.09). Moreover, it narrows the median validation-test performance correlation gap from 0.67 to 0.30, enabling more reliable validation-based early stopping, a key step toward improving fine-tuning stability under data scarcity. These results demonstrate that incorporating causal structure into data augmentation provides an effective and principled route to fine-tuning tabular foundation models in low-data regimes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04110.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04110",
    "published": "2026-01-07T17:16:39Z",
    "updated": "2026-01-07T17:16:39Z",
    "comment": "Accepted for oral presentation at the EurIPS 2025 Workshop on AI for Tabular Data (Copenhagen)",
    "light_analysis": {
      "overview": "本文提出CausalMixFT方法，通过结合结构因果模型进行数据增强，提高了表格基础模型在数据稀缺环境下的微调稳健性。",
      "motivation": "在数据稀缺的背景下，微调表格基础模型（TFMs）面临挑战，因为验证数据有限时，早期停止策略往往无法准确捕捉模型的真实泛化性能，导致下游任务表现不佳。现有数据增强方法（如统计生成器CTGAN）主要依赖纯统计手段，未能有效保留特征间的因果依赖关系，生成的数据质量不足，限制了微调效果。因此，开发一种能维护数据结构特征的新方法至关重要，以解决低数据环境下模型的不稳定性和不可靠性问题。",
      "method": "CausalMixFT方法的核心是使用结构因果模型（SCMs）拟合目标数据集，生成结构一致的合成样本以增强训练数据。具体而言，通过构建因果模型来捕捉特征间的依赖关系，生成因果知情的合成示例，这些数据保留了原始特征结构，同时扩展了训练多样性，确保增强过程的原则性和有效性。该方法在33个分类数据集（来自TabArena）和超过2300次微调运行上进行评估，验证了其广泛适用性，关键创新在于将因果推理集成到数据增强中，提供了一种基于因果结构的数据扩展技术路线。",
      "result": "实验结果表明，CausalMixFT显著提升了微调性能：中位数归一化ROC-AUC从标准微调的0.10提升到0.12，优于其他统计生成器如CTGAN（-0.01）、TabEBM（-0.04）和TableAugment（-0.09）。此外，中位数验证-测试性能相关性差距从0.67缩小到0.30，这使得基于验证的早期停止策略更加可靠，有效提高了数据稀缺下的微调稳定性，证明了因果数据增强在改善表格基础模型下游任务中的有效性。",
      "conclusion": "本文的主要贡献在于提出了CausalMixFT方法，通过将因果结构融入数据增强，为表格基础模型在低数据环境下的微调提供了一种稳健且原则性的解决方案，不仅提升了模型性能，还增强了微调过程的可靠性。该研究具有重要的学术价值，为因果推理与机器学习结合提供了新思路，并有望在实际应用中应对数据稀缺挑战；未来工作可探索方法在其他数据类型的扩展或优化因果建模的精度。",
      "tags": [
        "Tabular Foundation Models",
        "Structural Causal Models",
        "Causal Data Augmentation",
        "Fine-Tuning",
        "ROC-AUC"
      ]
    },
    "analyzed_at": "2026-01-08T17:11:27.298171Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04058",
    "title": "Minimum distance classification for nonlinear dynamical systems",
    "authors": [
      "Dominique Martinez"
    ],
    "abstract": "We address the problem of classifying trajectory data generated by some nonlinear dynamics, where each class corresponds to a distinct dynamical system. We propose Dynafit, a kernel-based method for learning a distance metric between training trajectories and the underlying dynamics. New observations are assigned to the class with the most similar dynamics according to the learned metric. The learning algorithm approximates the Koopman operator which globally linearizes the dynamics in a (potentially infinite) feature space associated with a kernel function. The distance metric is computed in feature space independently of its dimensionality by using the kernel trick common in machine learning. We also show that the kernel function can be tailored to incorporate partial knowledge of the dynamics when available. Dynafit is applicable to various classification tasks involving nonlinear dynamical systems and sensors. We illustrate its effectiveness on three examples: chaos detection with the logistic map, recognition of handwritten dynamics and of visual dynamic textures.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04058.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04058",
    "published": "2026-01-07T16:21:47Z",
    "updated": "2026-01-07T16:21:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出基于核的Dynafit方法，通过学习距离度量来分类非线性动态系统生成的轨迹数据。",
      "motivation": "该研究旨在解决非线性动态系统生成的轨迹数据分类问题，其中每个类别对应一个独特的动态系统。这一问题在信号处理、传感器数据分析和模式识别等领域具有重要性，因为实际应用常涉及复杂动态。现有方法可能难以有效处理非线性和高维动态，而本方法通过近似Koopman操作符来线性化动态，以克服这些挑战。摘要未明确说明具体不足，但暗示传统分类技术在动态系统建模上存在局限性。",
      "method": "论文提出Dynafit方法，这是一种基于核的技术，用于学习训练轨迹与底层动态之间的距离度量。核心创新点包括近似Koopman操作符，以在核函数相关的特征空间中全局线性化动态，从而简化分类过程。此外，使用核技巧计算特征空间中的距离度量，确保高效性和维度无关性。核函数可根据部分动态知识定制，提升适应性。方法不依赖特定数据集，但在示例中涉及逻辑映射、手写动态和视觉动态纹理等应用场景。",
      "result": "论文在三个示例中展示了Dynafit的有效性：逻辑映射的混沌检测、手写动态识别和视觉动态纹理识别。摘要未提供具体性能指标如准确率或效率改进数据，但表明方法在这些任务中表现良好。与基线方法的对比未详细说明，仅强调其广泛适用性。实验结果表明，Dynafit能够成功处理非线性动态分类问题，但具体量化结果需要参考完整论文。",
      "conclusion": "论文的主要贡献是提出Dynafit方法，一种基于核的距离度量学习技术，用于非线性动态系统分类。其学术价值在于融合Koopman操作符理论和核方法，实现了动态的全局线性化处理；实际应用价值体现在传感器数据分析和模式识别等领域。摘要未明确说明局限性，但未来工作可能包括扩展到更复杂动态系统或集成更多先验知识。整体而言，该方法为动态系统分类提供了一种新颖且灵活的解决方案。",
      "tags": [
        "Kernel Method",
        "Koopman Operator",
        "Distance Metric Learning",
        "Nonlinear Dynamical Systems",
        "Classification"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:25.385500Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04057",
    "title": "Using Legacy Polysomnography Data to Train a Radar System to Quantify Sleep in Older Adults and People living with Dementia",
    "authors": [
      "M. Yin",
      "K. G. Ravindran",
      "C. Hadjipanayi",
      "A. Bannon",
      "A. Rapeaux",
      "C. Della Monica",
      "T. S. Lande",
      "Derk-Jan Dijk",
      "T. G. Constandinou"
    ],
    "abstract": "Objective: Ultra-wideband radar technology offers a promising solution for unobtrusive and cost-effective in-home sleep monitoring. However, the limited availability of radar sleep data poses challenges in building robust models that generalize across diverse cohorts and environments. This study proposes a novel deep transfer learning framework to enhance sleep stage classification using radar data. Methods: An end-to-end neural network was developed to classify sleep stages based on nocturnal respiratory and motion signals. The network was trained using a combination of large-scale polysomnography (PSG) datasets and radar data. A domain adaptation approach employing adversarial learning was utilized to bridge the knowledge gap between PSG and radar signals. Validation was performed on a radar dataset of 47 older adults (mean age: 71.2), including 18 participants with prodromal or mild Alzheimer disease. Results: The proposed network structure achieves an accuracy of 79.5% with a Kappa value of 0.65 when classifying wakefulness, rapid eye movement, light sleep and deep sleep. Experimental results confirm that our deep transfer learning approach significantly enhances automatic sleep staging performance in the target domain. Conclusion: This method effectively addresses challenges associated with data variability and limited sample size, substantially improving the reliability of automatic sleep staging models, especially in contexts where radar data is limited. Significance: The findings underscore the viability of UWB radar as a nonintrusive, forward-looking sleep assessment tool that could significantly benefit care for older people and people with neurodegenerative disorders.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04057.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04057",
    "published": "2026-01-07T16:21:27Z",
    "updated": "2026-01-07T16:21:27Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种基于对抗学习的深度迁移学习框架，利用传统多导睡眠图数据训练雷达系统，以解决雷达数据稀缺问题，提升老年人睡眠监测的准确性。",
      "motivation": "超宽带雷达技术提供了一种无创、低成本的家庭睡眠监测方案，尤其在老年人和痴呆患者护理中具有潜力。然而，雷达数据有限导致模型难以泛化到不同人群和环境，现有方法依赖大量标注雷达数据，这在实践中获取困难。因此，本研究旨在通过迁移学习来克服数据不足挑战，提高睡眠分期模型的鲁棒性和应用范围，以满足精准医疗需求。",
      "method": "本研究开发了一个端到端神经网络，基于夜间呼吸和运动信号进行睡眠阶段分类。核心创新在于结合大规模多导睡眠图数据和雷达数据，采用对抗学习进行域适应，以弥合PSG和雷达信号之间的知识差异。具体地，模型通过对抗训练策略从丰富的PSG数据中学习通用特征，并将其迁移到有限的雷达数据域，从而在不依赖大量雷达样本的情况下提升分类性能，网络架构优化了信号处理能力。",
      "result": "在包含47名老年人（平均年龄71.2岁，其中18名患有阿尔茨海默病）的雷达数据集上进行验证，所提出的网络在分类清醒、快速眼动、浅睡眠和深睡眠四个阶段时，达到了79.5%的准确率和0.65的Kappa值。实验结果表明，深度迁移学习方法显著优于基线模型，在数据有限的目标域中实现了性能提升，证实了该方法在增强自动睡眠分期可靠性方面的有效性。",
      "conclusion": "该方法有效解决了雷达数据变异性大和样本量小的问题，显著提高了自动睡眠分期模型的可靠性。学术上，展示了域适应技术在医疗监测领域的创新应用；实际上，为非侵入式睡眠评估工具的推广提供了技术支撑，有助于改善老年人和神经退行性疾病患者的护理质量。摘要未明确说明具体局限性，但未来工作可能包括扩展到更多人群或优化模型效率。",
      "tags": [
        "Deep Transfer Learning",
        "Adversarial Learning",
        "Ultra-Wideband Radar",
        "Sleep Stage Classification",
        "Domain Adaptation"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:18.477630Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04054",
    "title": "LinkD: AutoRegressive Diffusion Model for Mechanical Linkage Synthesis",
    "authors": [
      "Yayati Jadhav",
      "Amir Barati Farimani"
    ],
    "abstract": "Designing mechanical linkages to achieve target end-effector trajectories presents a fundamental challenge due to the intricate coupling between continuous node placements, discrete topological configurations, and nonlinear kinematic constraints. The highly nonlinear motion-to-configuration relationship means small perturbations in joint positions drastically alter trajectories, while the combinatorially expanding design space renders conventional optimization and heuristic methods computationally intractable. We introduce an autoregressive diffusion framework that exploits the dyadic nature of linkage assembly by representing mechanisms as sequentially constructed graphs, where nodes correspond to joints and edges to rigid links. Our approach combines a causal transformer with a Denoising Diffusion Probabilistic Model (DDPM), both conditioned on target trajectories encoded via a transformer encoder. The causal transformer autoregressively predicts discrete topology node-by-node, while the DDPM refines each node's spatial coordinates and edge connectivity to previously generated nodes. This sequential generation enables adaptive trial-and-error synthesis where problematic nodes exhibiting kinematic locking or collisions can be selectively regenerated, allowing autonomous correction of degenerate configurations during design. Our graph-based, data-driven methodology surpasses traditional optimization approaches, enabling scalable inverse design that generalizes to mechanisms with arbitrary node counts. We demonstrate successful synthesis of linkage systems containing up to 20 nodes with extensibility to N-node architectures. This work advances autoregressive graph generation methodologies and computational kinematic synthesis, establishing new paradigms for scalable inverse design of complex mechanical systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04054.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04054",
    "published": "2026-01-07T16:19:11Z",
    "updated": "2026-01-07T16:19:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种自回归扩散模型，用于机械连杆的自动化合成，通过结合Transformer和扩散模型实现可扩展的逆设计。",
      "motivation": "机械连杆设计面临核心挑战，即实现目标末端执行器轨迹时，连续节点放置、离散拓扑配置和非线性运动学约束之间存在复杂耦合。高度非线性的运动-配置关系导致小扰动大幅改变轨迹，且组合爆炸的设计空间使传统优化和启发式方法计算不可行。因此，需要一种高效方法来合成满足需求的机械系统，以应对设计复杂性和可扩展性问题。",
      "method": "方法基于自回归扩散框架，将机械连杆表示为顺序构建的图，节点对应关节，边对应刚性连杆。结合因果Transformer和去噪扩散概率模型（DDPM），两者都条件于通过Transformer编码器编码的目标轨迹。因果Transformer自回归预测离散拓扑节点，DDPM精炼每个节点的空间坐标和边连接。顺序生成支持自适应试错合成，可选择性再生运动锁定或碰撞的节点，从而在设计时自主纠正退化配置。",
      "result": "实验成功合成了包含多达20个节点的机械连杆系统，并可扩展到N节点架构。该方法超越了传统优化方法，实现了可扩展的逆设计，能够泛化到任意节点数的机械机制，展示了其在高复杂性设计任务中的高效性和通用性。摘要未明确说明具体性能指标如准确率提升。",
      "conclusion": "该研究推进了自回归图生成方法和计算运动学合成领域，为复杂机械系统的可扩展逆设计建立了新范式。具有重要的学术价值，促进了机械设计自动化的进展，并可能为未来扩展到更复杂系统设计或结合其他AI技术提供基础。摘要未明确说明潜在局限性。",
      "tags": [
        "Diffusion Models",
        "Transformers",
        "Graph Generation",
        "Mechanical Linkage Synthesis",
        "Inverse Design"
      ]
    },
    "analyzed_at": "2026-01-08T17:12:14.865623Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04051",
    "title": "Symbolic Regression for Shared Expressions: Introducing Partial Parameter Sharing",
    "authors": [
      "Viktor Martinek",
      "Roland Herzog"
    ],
    "abstract": "Symbolic Regression aims to find symbolic expressions that describe datasets. Due to better interpretability, it is a machine learning paradigm particularly powerful for scientific discovery. In recent years, several works have expanded the concept to allow the description of similar phenomena using a single expression with varying sets of parameters, thereby introducing categorical variables. Some previous works allow only \"non-shared\" (category-value-specific) parameters, and others also incorporate \"shared\" (category-value-agnostic) parameters. We expand upon those efforts by considering multiple categorical variables, and introducing intermediate levels of parameter sharing. With two categorical variables, an intermediate level of parameter sharing emerges, i.e., parameters which are shared across either category but change across the other. The new approach potentially decreases the number of parameters, while revealing additional information about the problem. Using a synthetic, fitting-only example, we test the limits of this setup in terms of data requirement reduction and transfer learning. As a real-world symbolic regression example, we demonstrate the benefits of the proposed approach on an astrophysics dataset used in a previous study, which considered only one categorical variable. We achieve a similar fit quality but require significantly fewer individual parameters, and extract additional information about the problem.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.04051.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04051",
    "published": "2026-01-07T16:12:14Z",
    "updated": "2026-01-07T16:12:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出引入中间级别参数共享的符号回归方法，扩展以处理多个分类变量，减少参数数量并揭示额外信息。",
      "motivation": "符号回归旨在通过符号表达式描述数据集，具有更好的可interpretability，尤其适用于科学发现。现有方法在处理相似现象时，要么只使用非共享参数（特定于类别值），要么结合共享参数（与类别值无关），但这些方法未考虑多个分类变量，无法实现参数的部分共享，限制了参数减少和信息提取的潜力，特别是在天体物理等科学领域，这可能导致模型复杂性增加和可解释性不足。",
      "method": "论文扩展符号回归框架，引入部分参数共享机制，特别关注多个分类变量。核心创新是考虑两个分类变量时，参数可以跨一个分类共享但在另一个分类中变化，形成中间级别的共享。方法包括使用合成数据集测试数据需求减少和迁移学习的能力，并在真实世界的天体物理数据集上验证，该数据集先前只考虑了一个分类变量。关键细节包括模型架构的扩展，以处理参数共享的层次结构。",
      "result": "在合成例子中，测试表明新方法能有效减少数据需求和评估迁移学习极限。在真实的天体物理数据集上，相比先前研究（仅考虑一个分类变量），新方法获得了相似的拟合质量，但显著减少了个体参数数量，并提取了额外的问题相关信息，尽管摘要未明确说明具体参数减少百分比或性能指标的精确数值。",
      "conclusion": "该研究的主要贡献是提出了部分参数共享的符号回归方法，有效减少模型参数并增强可解释性。学术价值在于扩展了符号回归技术在多个分类变量处理中的理论框架，实际应用价值体现在科学发现领域，如天体物理，有助于更高效的数据分析和模型简化。未来方向可能包括应用于更多分类变量或不同数据集，尽管摘要未明确说明局限性或详细未来工作。",
      "tags": [
        "Symbolic Regression",
        "Partial Parameter Sharing",
        "Categorical Variables",
        "Parameter Reduction",
        "Astrophysics Dataset"
      ]
    },
    "analyzed_at": "2026-01-08T17:13:00.153481Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]