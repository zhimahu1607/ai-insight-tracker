[
  {
    "id": "2601.09708",
    "title": "Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning",
    "authors": [
      "Chi-Pin Huang",
      "Yunze Man",
      "Zhiding Yu",
      "Min-Hung Chen",
      "Jan Kautz",
      "Yu-Chiang Frank Wang",
      "Fu-En Yang"
    ],
    "abstract": "Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09708.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09708",
    "published": "2026-01-14T18:59:59Z",
    "updated": "2026-01-14T18:59:59Z",
    "comment": "Project page: https://jasper0314-huang.github.io/fast-thinkact/",
    "light_analysis": {
      "overview": "提出Fast-ThinkAct框架，通过可言语化的潜在推理实现高效视觉-语言-动作推理，显著减少推理延迟同时保持性能。",
      "motivation": "Vision-Language-Action (VLA) 任务需在动态环境中进行复杂推理并执行自适应动作，这对机器人、自动驾驶等体现智能应用至关重要。当前基于显式思维链的方法虽能改善泛化能力，但生成冗长推理轨迹导致高推理延迟，限制了实时部署的可行性。因此，开发能平衡性能与效率的高效推理框架成为紧迫需求，以应对实际场景中的快速决策挑战。",
      "method": "Fast-ThinkAct框架采用可言语化的潜在规划机制，通过教师模型蒸馏技术学习压缩思维链。核心创新包括偏好引导目标，用于对齐操作轨迹，有效传递语言和视觉信息以指导动作执行。这种方法实现了紧凑推理到策略学习的直接连接，减少冗余计算，适用于动态环境中的体现控制，提升整体推理效率。",
      "result": "实验在多样体现操作和推理基准上进行，结果显示Fast-ThinkAct的推理延迟相比最先进的推理VLA方法减少了高达89.3%。同时，框架保持强性能，包括有效的长范围规划、少样本适应和失败恢复能力，验证了其在保持任务质量的同时显著提升推理速度的优势。",
      "conclusion": "论文贡献在于提出高效的Fast-ThinkAct框架，通过潜在推理优化了视觉-语言-动作任务的效率，为体现智能系统提供了新方法。研究具有学术价值，推动实时交互系统发展；摘要未明确说明局限性，但未来工作可探索框架在更复杂场景下的扩展和应用潜力。",
      "tags": [
        "Vision-Language-Action Reasoning",
        "Chain-of-Thought",
        "Latent Planning",
        "Model Distillation",
        "Embodied Manipulation"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:51.581934Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09706",
    "title": "Value-Aware Numerical Representations for Transformer Language Models",
    "authors": [
      "Andreea Dutulescu",
      "Stefan Ruseti",
      "Mihai Dascalu"
    ],
    "abstract": "Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09706.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09706",
    "published": "2026-01-14T18:59:14Z",
    "updated": "2026-01-14T18:59:14Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种值感知数值表示方法，通过显式编码数值值来增强Transformer语言模型的数值鲁棒性。",
      "motivation": "研究动机源于Transformer语言模型在数学推理基准测试中表现出色，但在基本数值理解和算术操作上存在脆弱性问题。这主要是由于数字被当作符号标记处理，其嵌入不显式编码数值值，导致系统错误。此问题的重要性在于，语言模型在处理涉及数字的现实任务（如财务计算或数据分析）时可能失败，现有方法未能有效捕捉数值语义，限制了模型的实用性和可靠性。",
      "method": "论文提出了一种值感知数值表示，通过向标准标记化输入添加一个专用前缀标记来实现，该标记的嵌入显式地依赖于基础数值值。这种机制将数值的幅度信息直接注入模型输入空间，同时保持与现有标记器和仅解码器Transformer架构的兼容性，避免了复杂的架构修改。摘要未明确说明具体使用的数据集或模型架构细节，但强调了方法的通用性和效率。",
      "result": "在算术任务上的评估结果显示，所提出的方法在多种数值格式、任务类型和操作数长度方面均优于基线。虽然摘要未提供具体的性能指标（如准确率提升百分比），但明确指出该方法展现出更好的鲁棒性。这表明通过显式编码数值值，模型在基本数值处理能力上得到显著改进。",
      "conclusion": "研究的主要贡献是提出了一种高效且有效的方法，通过值感知数值表示提高语言模型的基本数值鲁棒性。这不仅具有学术价值，推动了语言模型在数值理解领域的发展，还可能在实际应用（如教育或金融分析）中增强模型处理数字密集型任务的能力。未来工作可能涉及扩展到更复杂的数学推理任务或与其他模型架构结合。",
      "tags": [
        "Transformer Language Models",
        "Numerical Representations",
        "Value-Aware Embeddings",
        "Arithmetic Operations",
        "Token Prefix"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:41.607649Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09699",
    "title": "SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3",
    "authors": [
      "Ruiqi Shen",
      "Chang Liu",
      "Henghui Ding"
    ],
    "abstract": "Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09699.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09699",
    "published": "2026-01-14T18:52:14Z",
    "updated": "2026-01-14T18:52:14Z",
    "comment": "Code: https://github.com/FudanCVL/SAM3-DMS",
    "light_analysis": {
      "overview": "SAM3-DMS提出了一种无需训练的脱钩内存选择策略，以提升Segment Anything 3在多目标视频分割中的个体可靠性和跟踪稳定性。",
      "motivation": "该研究旨在解决Segment Anything 3 (SAM3) 在复杂多目标视频分割中存在的内存选择问题。现有SAM3使用组级集体内存选择，所有目标同步决策基于平均性能，这忽略了单个目标的可靠性，导致在目标密度高时性能下降。视频多目标分割在自动驾驶和监控等应用中有重要价值，因此改进其鲁棒性和准确性至关重要。",
      "method": "论文提出SAM3-DMS方法，这是一种无需额外训练的脱钩策略，通过对每个目标进行细粒度内存选择来优化性能。关键创新点在于将集体决策改为个体化决策，避免了基于平均性能的同步条件导致的公平性问题。摘要未明确说明具体数据集或模型架构细节，但推断可能涉及修改SAM3的内存管理模块以实现独立的目标记忆处理。",
      "result": "实验结果显示，SAM3-DMS方法在视频分割任务中显著提高了身份保持和跟踪稳定性。与原始SAM3相比，该方法在多目标场景下表现更优，优势随目标密度的增加而变得更加明显。摘要未提供具体的性能指标如准确率提升，但强调了方法在复杂环境下的稳健性和实用性，为多目标视频分割提供了有效解决方案。",
      "conclusion": "该研究的主要贡献是提出了SAM3-DMS，一个无需训练的内存选择策略，有效解决了SAM3在多目标视频分割中的不足。通过脱钩和细粒度选择，提升了跟踪的个体可靠性和整体稳定性，尤其在目标密度高时效果显著。这为野外多目标视频分割应用奠定了坚实基础，未来可进一步验证其在更广泛场景中的泛化能力和潜在局限性。",
      "tags": [
        "Video Object Segmentation",
        "Memory Selection",
        "Decoupling",
        "SAM3",
        "Multi-target Tracking"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:38.431955Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09698",
    "title": "COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation",
    "authors": [
      "Tony Danjun Wang",
      "Tolga Birdal",
      "Nassir Navab",
      "Lennart Bastian"
    ],
    "abstract": "3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09698.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09698",
    "published": "2026-01-14T18:50:17Z",
    "updated": "2026-01-14T18:50:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "COMPOSE 提出了一种基于超图分割优化的新框架，用于改进多视角 3D 人体姿态估计中的对应匹配问题。",
      "motivation": "多视角 3D 人体姿态估计在动作识别和运动分析等领域至关重要，现有优化方法通常采用两阶段流程：先检测各视图的 2D 关键点，再跨视图关联以三角化 3D 姿态。但现有方法仅依赖成对关联建模对应问题，将全局一致性（如循环一致性）作为软约束，导致虚假关联误差在多视图中传播，使得约束协调变得脆弱，影响姿态估计的鲁棒性。因此，本研究旨在解决成对关联方法在处理多视角对应时的不足，提升整体准确性和稳定性。",
      "method": "COMPOSE 将多视角姿态对应匹配问题重新定义为超图分割优化，而不是传统的成对关联方法。核心创新在于使用超图建模所有视图的全局一致性，从而更有效地处理多视图约束和避免误差传播。虽然超图分割涉及的整数线性程序在理论上有指数级复杂度，但论文引入了一种高效的几何剪枝策略，通过利用空间几何信息大幅减少搜索空间。摘要未明确说明具体数据集或模型架构，但该方法专注于优化阶段的改进，旨在实现更精确的对应匹配。",
      "result": "COMPOSE 在实验中展示了显著的性能提升，相比先前优化方法，平均精度最多提高了 23%；相比自监督端到端学习方法，最多提高了 11%。这表明 COMPOSE 能有效减少对应匹配中的错误，提高多视角 3D 人体姿态估计的准确性和鲁棒性。摘要未提供具体基线方法的详细实验设置，但结果突出了其在处理复杂多视图场景时的优势，为相关领域设定了新的性能基准。",
      "conclusion": "本研究的核心贡献是提出了 COMPOSE 框架，通过超图优化解决多视角 3D 姿态估计的对应匹配问题，克服了成对关联方法的局限性，实现了更高的全局一致性和精度。其学术价值在于为多视角几何问题引入了新视角，实际应用中可提升动作识别和人类-机器人交互等任务的性能。未来工作可能包括将方法扩展到动态场景或与深度学习技术集成，以进一步优化效率和适用性。",
      "tags": [
        "Multi-view 3D Pose Estimation",
        "Hypergraph Partitioning",
        "Integer Linear Programming",
        "Geometric Pruning",
        "Correspondence Matching"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:03.765053Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09697",
    "title": "Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering",
    "authors": [
      "Jieying Chen",
      "Jeffrey Hu",
      "Joan Lasenby",
      "Ayush Tewari"
    ],
    "abstract": "Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09697.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09697",
    "published": "2026-01-14T18:50:06Z",
    "updated": "2026-01-14T18:50:06Z",
    "comment": "Project page: https://ayushtewari.com/projects/srender/",
    "light_analysis": {
      "overview": "本研究提出SRENDER方法，通过结合稀疏扩散关键帧生成和3D渲染技术，实现高效的相机控制静态场景视频生成，显著提升生成速度。",
      "motivation": "现代基于扩散模型的视频生成技术虽能生成逼真视频，但计算效率低下，需要几分钟GPU时间生成几秒视频，这成为实时交互应用（如具身AI和VR/AR）部署的关键障碍。现有方法无法满足实时性需求，导致生成视频在交互场景中应用受限，因此研究旨在解决这一效率瓶颈，开发高效且可控的视频生成策略。",
      "method": "论文提出一种新策略，首先使用基于扩散的生成模型生成稀疏的关键帧集合，然后通过3D重建技术将这些关键帧转换为3D表示，并渲染中间视图以合成完整视频。这一方法摊销了生成成本，同时通过3D表示确保几何一致性。研究引入模型预测给定相机轨迹的最优关键帧数量，使系统自适应分配计算资源，最终方法SRENDER根据相机运动复杂度调整关键帧稀疏性。",
      "result": "实验结果显示，SRENDER方法在生成20秒视频时，比基于扩散的基线模型快40倍以上，极大提升生成效率。同时，该方法保持了高视觉保真度和时间稳定性，确保视频质量不受影响，与基线方法相比，在速度和视觉一致性方面均有显著改进，为实时应用提供可行路径。",
      "conclusion": "本研究的主要贡献是SRENDER方法，它通过稀疏扩散关键帧和3D渲染技术实现高效相机控制视频生成，解决了计算效率问题并提供了可控合成路径。这具有重要学术和实际应用价值，特别在具身AI和VR/AR领域，未来工作可扩展至动态场景或优化3D重建算法。",
      "tags": [
        "Diffusion Models",
        "3D Rendering",
        "Sparse Keyframes",
        "Camera Control",
        "Video Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:49.706932Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09696",
    "title": "Empathy Applicability Modeling for General Health Queries",
    "authors": [
      "Shan Randhawa",
      "Agha Ali Raza",
      "Kentaro Toyama",
      "Julie Hui",
      "Mustafa Naseem"
    ],
    "abstract": "LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09696.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09696",
    "published": "2026-01-14T18:47:02Z",
    "updated": "2026-01-14T18:47:02Z",
    "comment": "In Submission to ACL",
    "light_analysis": {
      "overview": "本研究提出了同理心适用性框架（EAF），用于预测一般健康查询中的同理心需求，并建立了相关基准。",
      "motivation": "大型语言模型（LLMs）正逐渐集成到临床工作流中，但常缺乏临床同理心，这是有效医患沟通的关键。现有自然语言处理框架主要集中在反应性地标注医生回应中的同理心，对于在一般健康查询中前瞻性建模同理心需求的支持有限，导致自动系统在回应时可能忽视情感需求。因此，亟需新方法来识别和预测同理心适用性，以改善异步医疗中的沟通质量。（约100字）",
      "method": "论文提出了同理心适用性框架（EAF），这是一种理论驱动的方法，基于临床、上下文和语言线索，分类患者查询的情感动反应和解释的适用性。关键创新点在于前瞻性建模同理心需求。研究发布了一个真实患者查询的基准数据集，由人类和GPT-4o双重注释，用于训练分类器来预测同理心适用性。该方法还包括验证框架的有效性，并通过基准数据支持模型训练。（约100字）",
      "result": "在人类共识子集中，观察到人类与GPT-4o之间的一致性较高，表明模型能有效对齐。训练的分类器在预测同理心适用性方面表现出强性能，显著优于启发式方法和零样本大型语言模型基线，具体性能指标未在摘要中详细说明。错误分析揭示了持久挑战，如隐性困扰、临床严重性模糊性和上下文困难，强调了进一步改进的需求。（约100字）",
      "conclusion": "本研究的主要贡献是提出了EAF框架，能在回应生成前识别同理心需求，建立了前瞻性同理心建模的基准，并支持异步医疗中的同理心沟通。错误分析指出了局限性，如隐性困扰和上下文困难，因此未来工作需包括多注释者建模、临床医生在环校准和文化多样性标注。该研究为提升临床人工智能的同理心能力提供了学术和实际价值。（约100字）",
      "tags": [
        "Large Language Model",
        "Empathy Modeling",
        "Natural Language Processing",
        "Benchmark Dataset",
        "Human-AI Alignment"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:48.185269Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09694",
    "title": "LLMs can Compress LLMs: Adaptive Pruning by Agents",
    "authors": [
      "Sai Varun Kodathala",
      "Rakesh Vunnam"
    ],
    "abstract": "As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09694.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09694",
    "published": "2026-01-14T18:45:36Z",
    "updated": "2026-01-14T18:45:36Z",
    "comment": "17 Pages",
    "light_analysis": {
      "overview": "本论文提出了一种基于大型语言模型代理的自适应剪枝方法，以智能压缩LLMs并显著减少知识退化。",
      "motivation": "随着大型语言模型规模扩大，后训练剪枝成为降低计算成本的关键技术，但现有方法如SparseGPT和Wanda依赖均匀或手工启发式确定剪层稀疏比，导致事实知识严重退化，尤其在结构化剪枝中近乎完全崩溃。因此，需要开发自适应方法来智能选择剪枝层，在压缩过程中保护关键知识路径，以提高模型效率并维持性能。",
      "method": "论文提出代理引导剪枝框架，利用基础模型作为自适应代理智能选择剪枝层。方法构建层间敏感度配置文件，结合Wanda启发的权重-激活度量和梯度重要性得分，归一化为z分数以实现模型无关比较。这些统计信息由具备自我反思能力的LLM代理处理，使其能迭代学习先前剪枝结果并优化策略，同时引入检查点回滚机制，当模型困惑度退化超过阈值时自动恢复，确保模型质量稳定。",
      "result": "在Qwen3模型（4B和8B参数）上，以约45%稀疏度评估该方法，结果显示显著优于结构化剪枝基线：MMLU准确率相对提升56%，FreebaseQA上事实知识保留能力增强19倍，困惑度退化降低69%。该框架无需重新训练，以模型无关方式操作，自我纠正机制有效，在21-40次迭代中仅需2-4次检查点回滚，证明了高效压缩和知识保护能力。",
      "conclusion": "本研究的主要贡献是提出了一种自适应剪枝框架，证明基础模型能有效指导其他基础模型的压缩，学术上展示了LLM代理在模型优化中的新应用，结合多种指标和迭代学习机制。实际上，该方法提高了剪枝效率，减少知识退化且无需重新训练，具有广泛适用性。未来工作可进一步探索代理学习的泛化能力和在不同任务上的扩展应用。",
      "tags": [
        "Large Language Models",
        "Adaptive Pruning",
        "Agent-guided Methods",
        "Gradient Importance",
        "Self-reflection"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:37.667486Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09693",
    "title": "Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design",
    "authors": [
      "Lisa Schneckenreiter",
      "Sohvi Luukkonen",
      "Lukas Friedrich",
      "Daniel Kuhn",
      "Günter Klambauer"
    ],
    "abstract": "Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09693.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09693",
    "published": "2026-01-14T18:45:08Z",
    "updated": "2026-01-14T18:45:08Z",
    "comment": "ELLIS ML4Molecules Workshop 2025, ELLIS Unconference, Copenhagen 2025",
    "light_analysis": {
      "overview": "ConGLUDe 通过对比几何学习，统一了结构-和配体-基于的药物设计，实现多任务支持。",
      "motivation": "传统结构-和配体-基于药物设计依赖不相关数据源和建模假设，导致数据分裂，限制了联合使用和规模化应用。现有方法往往需要预定义结合口袋或独立训练，造成性能瓶颈和效率低下。本研究旨在解决这一分裂问题，通过统一框架消除对口袋信息的依赖，提高药物设计的准确性和效率，推动计算药物发现的发展。摘要未明确说明具体应用案例，但强调了统一方法的必要性。",
      "method": "ConGLUDe 是一个对比几何模型，结合了几何蛋白编码器（生成全局蛋白表示和预测结合位点的隐式嵌入）和快速配体编码器。通过对比学习，将配体与蛋白表示和多个候选结合位点对齐，支持多任务训练，包括配体条件口袋预测。模型在蛋白-配体复合物和大规模生物活性数据上联合训练，无需预定义口袋，创新点在于统一结构和配体训练并消除对口袋的依赖。摘要未详细说明具体架构参数，但突出了对比学习作为核心技术。",
      "result": "在多样化基准测试中，ConGLUDe 在没有结合口袋信息输入的情况下，实现了零样本虚拟筛选的最先进性能，显著优于现有方法。在挑战性目标 fishing 任务中，表现突出，并在配体条件口袋选择任务上展示出竞争力。这些结果验证了统一训练的有效性，但摘要未提供具体准确率或效率数据，仅描述了相对性能改进和与基线方法的对比。",
      "conclusion": "ConGLUDe 展示了统一结构-和配体-基于训练的优势，为药物发现提供了通用基础模型的重要一步。其学术价值在于提出新颖的对比几何学习框架，实际应用价值在于提升虚拟筛选和目标 fishing 的效率。未来工作可能扩展到更多药物设计任务和数据集，以克服潜在局限性如数据泛化问题，摘要未明确说明这些细节，但暗示了模型的推广潜力。",
      "tags": [
        "Contrastive Learning",
        "Geometric Learning",
        "Drug Discovery",
        "Protein Encoding",
        "Virtual Screening"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:26.195137Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09692",
    "title": "Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection",
    "authors": [
      "Tianyi Niu",
      "Justin Chih-Yao Chen",
      "Genta Indra Winata",
      "Shi-Xiong Zhang",
      "Supriyo Chakraborty",
      "Sambit Sahu",
      "Yue Zhang",
      "Elias Stengel-Eskin",
      "Mohit Bansal"
    ],
    "abstract": "Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09692.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09692",
    "published": "2026-01-14T18:43:32Z",
    "updated": "2026-01-14T18:43:32Z",
    "comment": "Code: https://github.com/tianyiniu/RoutingGenData",
    "light_analysis": {
      "overview": "提出了RGD设置和CASCAL路由器，实现在无需标注数据下通过生成数据有效估计LLM技能并选择专家模型。",
      "motivation": "LLM路由器旨在动态选择最适合用户输入的模型，但现有方法通常依赖标注数据进行训练，这在用户请求分布异构且未知的实践中难以实现，尤其是当数据隐私或成本限制时。这个问题的重要性在于真实标注数据稀缺，限制了路由器的部署和实用性。现有方法不足在于无法适应无标注数据场景，导致模型选择效率低下，本研究旨在通过生成数据来解决这一挑战，提高路由器在现实世界中的灵活性和性能。",
      "method": "本研究引入了Routing with Generated Data (RGD)设置，训练路由器仅使用由生成器LLM根据高级任务描述生成的查询和答案。关键创新包括分析生成器的两个特性：需准确回答自己的问题，且其问题应在模型池中产生足够性能差异。基于此，提出了CASCAL路由器，它采用共识投票来估计模型正确性，并通过层次聚类识别模型特定技能领域。实验在四个多样化基准和12个模型上进行，使用生成数据评估路由器的性能。",
      "result": "实验结果表明，当生成器质量下降时，查询-答案路由器的性能退化速度超过仅查询路由器。通过过滤满足关键特性的生成器，生成数据质量得到提升。提出的CASCAL路由器展现出更强的鲁棒性，在弱生成器数据上训练时，其绝对准确率比最佳查询-答案路由器高出4.6%，有效证明了其在生成数据设置下的优越性能。这些结果基于四个基准对比，强调了CASCAL在处理数据不足场景时的实用性。",
      "conclusion": "本研究的主要贡献在于提出了RGD设置和CASCAL路由器，实现在无标注数据下优化LLM技能估计和模型选择。通过分析生成器特性并设计新型路由器，显著提升了路由器在生成数据上的鲁棒性和性能，具有学术价值，为无监督模型选择提供了新思路；在实际应用中，可降低数据依赖，促进LLM路由技术的更广泛部署。未来工作可能涉及探索更复杂生成策略或扩展到更大模型池，以进一步增强泛化能力。",
      "tags": [
        "LLM Routing",
        "Generated Data",
        "Consensus Voting",
        "Hierarchical Clustering",
        "Annotation-Free Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:30.046461Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09688",
    "title": "DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation",
    "authors": [
      "Yibo Wang",
      "Lei Wang",
      "Yue Deng",
      "Keming Wu",
      "Yao Xiao",
      "Huanjin Yao",
      "Liwei Kang",
      "Hai Ye",
      "Yongcheng Jing",
      "Lidong Bing"
    ],
    "abstract": "Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09688.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09688",
    "published": "2026-01-14T18:38:31Z",
    "updated": "2026-01-14T18:38:31Z",
    "comment": "Source code: https://github.com/Infinity-AILab/DeepResearchEval",
    "light_analysis": {
      "overview": "提出DeepResearchEval自动化框架，通过人物驱动的任务构建和代理评估，解决深度研究系统评估中的挑战，创新性地实现了动态评估和事实检查。",
      "motivation": "深度研究系统广泛应用于多步网络研究、分析和跨源合成，但其评估面临挑战。现有基准常需大量标注来构建任务，依赖静态评估维度，或在引用缺失时无法可靠验证事实，导致评估效率低下和结果不可靠，这限制了系统的实际应用和改进。该研究旨在克服这些问题，提供更自动化、灵活的评估方法，以提升深度研究系统的可靠性和实用性。",
      "method": "框架包含两个核心部分：任务构建方面，采用人物驱动管道生成基于多样化用户档案的真实、复杂研究任务，通过两阶段过滤器（任务资格和搜索必要性）筛选，确保任务需多源证据集成和外部检索。评估方面，设计代理管道，包含自适应点状质量评估，动态派生每个任务的具体评估维度、标准和权重，以及主动事实检查，自主提取和通过网络搜索验证报告声明，即使在引用缺失时也能操作。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "摘要未明确说明主要实验结果，如具体性能指标或与基线方法的对比数据。论文可能通过实验验证了框架在生成任务的复杂性和评估的可靠性方面的有效性，但缺乏详细数据支持。推断框架可能提升了任务构建的自动化程度和评估的准确性，但需参考全文或后续研究以获得量化结果。",
      "conclusion": "DeepResearchEval框架的主要贡献在于自动化深度研究任务的构建和评估，通过动态评估和事实检查提高了效率和准确性。其学术价值在于为深度研究系统评估提供了新方法，实际应用价值在于支持更可靠的系统开发和优化。潜在局限性可能包括任务生成的多样性和评估维度的扩展性，未来工作可探索更广泛的任务类型和增强事实检查的鲁棒性。",
      "tags": [
        "Deep Research Systems",
        "Automated Task Construction",
        "Agentic Evaluation",
        "Adaptive Evaluation",
        "Active Fact-Checking"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:11.527833Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09684",
    "title": "Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection",
    "authors": [
      "Ziyu Yang",
      "Guibin Chen",
      "Yuxin Yang",
      "Aoxiong Zeng",
      "Xiangquan Yang"
    ],
    "abstract": "Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09684.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09684",
    "published": "2026-01-14T18:36:22Z",
    "updated": "2026-01-14T18:36:22Z",
    "comment": "preprint",
    "light_analysis": {
      "overview": "Ortho-LoRA通过动态正交梯度投影，有效解决多任务LoRA中的任务冲突，提升模型性能。",
      "motivation": "本研究针对多任务学习（MTL）结合低秩适配（LoRA）在大型语言模型（LLMs）部署中存在的负转移问题。由于共享单一适配器，不同任务的梯度更新冲突导致性能下降，尤其在LoRA的低秩约束下，优化空间有限，无法满足多样任务需求。现有标准联合训练方法表现不佳，亟需新方法来减少任务干扰，以提高参数效率。",
      "method": "论文提出Ortho-LoRA方法，一种针对LoRA双部分结构的梯度投影技术。核心创新在于动态地将冲突任务梯度投影到彼此的正交补上，在LoRA的内在子空间内进行优化，以减少梯度干扰。该方法利用正交投影适应LoRA的低秩特性，具体在训练过程中调整梯度更新，无需额外复杂计算。",
      "result": "在GLUE基准测试上的实验显示，Ortho-LoRA有效减轻了任务干扰，性能优于标准联合训练。它恢复了多任务与单任务基线之间95%的性能差距，且计算开销可忽略不计。这表明该方法在提升参数效率方面显著有效，缓解了多任务学习中的性能下降问题。",
      "conclusion": "Ortho-LoRA成功解决了多任务LoRA中的梯度冲突，提升了模型在参数高效部署中的性能。学术上，它为多任务学习提供了新的优化策略；实际上，有助于大型语言模型的高效应用。未来工作可进一步探索其扩展性到更广泛的任务或模型架构，摘要未明确说明具体局限性。",
      "tags": [
        "Multi-Task Learning",
        "Low-Rank Adaptation",
        "Gradient Projection",
        "Orthogonal Projection",
        "Parameter Efficiency"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:15.227698Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09680",
    "title": "Automating Supply Chain Disruption Monitoring via an Agentic AI Approach",
    "authors": [
      "Sara AlMahri",
      "Liming Xu",
      "Alexandra Brintrup"
    ],
    "abstract": "Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \\rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \\$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09680.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09680",
    "published": "2026-01-14T18:28:31Z",
    "updated": "2026-01-14T18:28:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种最小监督的智能代理AI框架，用于自动化监控和响应供应链中断，实现从被动恢复到主动韧性的转变。",
      "motivation": "现代供应链频繁面临地缘政治事件、需求冲击、贸易限制和自然灾害等中断风险，但大多数公司仅能监控一级供应商，缺乏对上游多级网络的可见性，导致漏洞未被发现，中断影响向下游扩散。现有方法依赖人工分析，响应缓慢且成本高昂，无法实现主动风险管理和韧性提升，因此亟需自动化解决方案来克服这一盲点。",
      "method": "论文提出一个基于智能代理的AI框架，包含七个专门代理，结合大型语言模型和确定性工具。该框架从非结构化新闻中自动检测中断信号，将其映射到多级供应商网络，基于网络结构评估风险暴露，并推荐缓解措施如替代采购选项。关键创新在于智能代理的协同架构，实现端到端自动化分析，评估使用30个合成场景覆盖三家汽车制造商和五类中断。",
      "result": "实验结果显示系统在核心任务上达到高准确性，F1分数介于0.962至0.991之间。端到端分析平均耗时3.83分钟，每次中断成本为0.0836美元。相较于行业基准的多天人工评估，响应时间减少了三个数量级，显著提升了效率。2022年俄罗斯-乌克兰冲突的案例研究进一步验证了系统的实际应用能力。",
      "conclusion": "本研究的主要贡献是建立了一个向韧性、主动和自主供应链转型的基础框架，结合智能代理和AI技术自动化监控多级网络中断。学术价值在于创新应用AI于供应链管理，实践价值在于大幅降低响应时间和成本，增强企业韧性。未来工作可扩展至更多行业和复杂场景，并优化代理协同机制。",
      "tags": [
        "Large Language Models",
        "Agentic AI",
        "Supply Chain Monitoring",
        "Multi-tier Networks",
        "Disruption Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:25.568614Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09668",
    "title": "STEP3-VL-10B Technical Report",
    "authors": [
      "Ailin Huang",
      "Chengyuan Yao",
      "Chunrui Han",
      "Fanqi Wan",
      "Hangyu Guo",
      "Haoran Lv",
      "Hongyu Zhou",
      "Jia Wang",
      "Jian Zhou",
      "Jianjian Sun",
      "Jingcheng Hu",
      "Kangheng Lin",
      "Liang Zhao",
      "Mitt Huang",
      "Song Yuan",
      "Wenwen Qu",
      "Xiangfeng Wang",
      "Yanlin Lai",
      "Yingxiu Zhao",
      "Yinmin Zhang",
      "Yukang Shi",
      "Yuyang Chen",
      "Zejia Weng",
      "Ziyang Meng",
      "Ang Li",
      "Aobo Kong",
      "Bo Dong",
      "Changyi Wan",
      "David Wang",
      "Di Qi",
      "Dingming Li",
      "En Yu",
      "Guopeng Li",
      "Haiquan Yin",
      "Han Zhou",
      "Hanshan Zhang",
      "Haolong Yan",
      "Hebin Zhou",
      "Hongbo Peng",
      "Jiaran Zhang",
      "Jiashu Lv",
      "Jiayi Fu",
      "Jie Cheng",
      "Jie Zhou",
      "Jisheng Yin",
      "Jingjing Xie",
      "Jingwei Wu",
      "Jun Zhang",
      "Junfeng Liu",
      "Kaijun Tan",
      "Kaiwen Yan",
      "Liangyu Chen",
      "Lina Chen",
      "Mingliang Li",
      "Qian Zhao",
      "Quan Sun",
      "Shaoliang Pang",
      "Shengjie Fan",
      "Shijie Shang",
      "Siyuan Zhang",
      "Tianhao You",
      "Wei Ji",
      "Wuxun Xie",
      "Xiaobo Yang",
      "Xiaojie Hou",
      "Xiaoran Jiao",
      "Xiaoxiao Ren",
      "Xiangwen Kong",
      "Xin Huang",
      "Xin Wu",
      "Xing Chen",
      "Xinran Wang",
      "Xuelin Zhang",
      "Yana Wei",
      "Yang Li",
      "Yanming Xu",
      "Yeqing Shen",
      "Yuang Peng",
      "Yue Peng",
      "Yu Zhou",
      "Yusheng Li",
      "Yuxiang Yang",
      "Yuyang Zhang",
      "Zhe Xie",
      "Zhewei Huang",
      "Zhenyi Lu",
      "Zhimin Fan",
      "Zihui Cheng",
      "Daxin Jiang",
      "Qi Han",
      "Xiangyu Zhang",
      "Yibo Zhu",
      "Zheng Ge"
    ],
    "abstract": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09668.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09668",
    "published": "2026-01-14T17:58:24Z",
    "updated": "2026-01-14T17:58:24Z",
    "comment": "50 pages",
    "light_analysis": {
      "overview": "STEP3-VL-10B是一个轻量级开源多模态基础模型，通过创新的训练策略和并行协调推理方法，在紧凑参数下实现前沿性能。",
      "motivation": "当前多模态模型通常需要巨大参数规模才能达到高性能，这导致计算资源消耗高、部署效率低。本研究旨在解决紧凑模型与前沿智能之间的权衡问题，通过重新定义效率与性能的平衡点。摘要未明确说明现有方法的具体不足，但强调轻量级模型在减少资源需求方面的实用性，以满足实际应用中对高效智能系统的需求。",
      "method": "论文提出了STEP3-VL-10B模型，采用两个核心策略：首先，实施统一的预训练方法，在1.2T多模态令牌上结合语言对齐的感知编码器和Qwen3-8B解码器，以建立内在的视觉语言协同；其次，通过超过1000次迭代的强化学习进行规模化的后训练。关键创新点包括Parallel Coordinated Reasoning (PaCoRe)，在推理时分配资源进行可扩展的感知推理，探索和合成多样视觉假设，从而增强模型能力。",
      "result": "STEP3-VL-10B在多个基准测试中表现出色：在MMBench上准确率达到92.2%，MMMU上为80.11%，复杂推理任务如AIME2025达到94.43%，MathVision为75.95%。尽管仅有10B参数，其性能媲美或超越参数规模10-20倍更大的模型（如GLM-4.6V-106B、Qwen3-VL-235B）以及顶级专有模型如Gemini 2.5 Pro和Seed-1.5-VL，证实了其高效性与前沿性能。",
      "conclusion": "本研究成功开发了STEP3-VL-10B，一个紧凑而强大的多模态基础模型，通过创新策略显著提升了效率与性能的平衡。主要贡献在于提供开源、可复现的基准，促进了社区研究和实际应用，具有重要学术和实用价值。未来工作可能涉及进一步优化推理效率或扩展到更广泛的多模态任务领域。",
      "tags": [
        "Multimodal Foundation Model",
        "Reinforcement Learning",
        "Parallel Coordinated Reasoning",
        "Vision-Language Synergy",
        "Open-Source Model"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:37.418004Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09667",
    "title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning",
    "authors": [
      "Zhiyuan Hu",
      "Yunhai Hu",
      "Juncheng Liu",
      "Shuyue Stella Li",
      "Yucheng Wang",
      "Zhen Xu",
      "See-Kiong Ng",
      "Anh Tuan Luu",
      "Xinxing Xu",
      "Bryan Hooi",
      "Cynthia Breazeal",
      "Hae Won Park"
    ],
    "abstract": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09667.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09667",
    "published": "2026-01-14T17:57:43Z",
    "updated": "2026-01-14T17:57:43Z",
    "comment": "Work in Progress",
    "light_analysis": {
      "overview": "MATTRL框架通过推理时注入结构化文本经验，实现了无需调优的分布鲁棒多智能体推理，提高准确性。",
      "motivation": "多智能体系统在LLM驱动下已成为实用协作者，具有鲁棒性优势，但多智能体强化学习训练资源密集且不稳定，包括队友共适应导致的非平稳性和奖励稀疏高方差问题。因此，研究旨在解决传统训练方法的不足，提供更稳定高效的多智能体协作方案，以增强推理任务的可靠性和效率。",
      "method": "MATTRL框架在推理时注入结构化文本经验到多智能体审议中，形成多专家团队进行多轮讨论，检索和集成测试时经验，并通过共识机制进行最终决策。关键创新点包括信用分配机制，用于构建轮级经验池并重新注入对话中，以优化决策过程，摘要未明确说明具体数据集或模型架构，但推断可能基于LLM驱动的智能体。",
      "result": "在医学、数学和教育领域的挑战性基准测试中，MATTRL平均提高准确性3.67%相较于多智能体基线，8.67%相较于单智能体基线。消融研究考察了不同信用分配方案，验证了其对训练结果的影响，显示了方法的稳定性和有效性。",
      "conclusion": "MATTRL提供了一个稳定、有效和高效的路径，实现分布偏移鲁棒的多智能体推理，无需调优。其学术价值在于改进多智能体强化学习方法，应用价值在于可扩展到医疗、教育等领域的推理任务，未来工作可能包括优化信用分配方案或扩展应用场景。",
      "tags": [
        "Multi-Agent Reinforcement Learning",
        "Test-Time Training",
        "LLM-driven Collaborators",
        "Credit Assignment",
        "Structured Textual Experience"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:19.708816Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09665",
    "title": "SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings",
    "authors": [
      "Yuchen Wu",
      "Jiahe Li",
      "Xiaohan Yu",
      "Lina Yu",
      "Jin Zheng",
      "Xiao Bai"
    ],
    "abstract": "Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09665.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09665",
    "published": "2026-01-14T17:57:08Z",
    "updated": "2026-01-14T17:57:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "SCE-SLAM通过场景坐标嵌入实现尺度一致性，是一个解决单目SLAM中尺度漂移问题的端到端系统。",
      "motivation": "单目视觉SLAM技术广泛应用于从互联网视频进行3D重建和在资源受限平台上实现自主导航，但其长期存在尺度漂移问题，即估计尺度在长序列中逐渐偏离真实值。现有帧到帧方法依赖局部优化，虽能实时运行，但由于缺乏独立窗口间的全局约束，无法有效传播尺度信息，导致累积漂移，严重影响重建和导航准确性。因此，开发一种能维持全局尺度一致性的方法至关重要，以提升系统在复杂场景中的可靠性。",
      "method": "论文提出SCE-SLAM，一个端到端SLAM系统，核心创新是使用学习到的场景坐标嵌入，该嵌入编码补丁级表示以捕获3D几何关系在一个规范尺度参考下。系统包含两个关键模块：几何引导聚合模块，通过3D空间邻近性，利用几何调制注意力从历史观测传播尺度信息；以及场景坐标束调整模块，通过从嵌入解码的明确3D坐标约束，将当前估计锚定到参考尺度以实现全局一致性。这些模块共同工作，实时处理视觉数据。",
      "result": "实验在KITTI、Waymo和vKITTI数据集上进行，显示SCE-SLAM显著改进性能。在KITTI上，绝对轨迹误差比先前最佳方法减少8.36米，同时系统保持每秒36帧的实时处理速度，并实现大规模场景中的尺度一致性。其他数据集上的实验也证明了方法的有效性和泛化能力，但摘要未明确说明具体数字，总体显示了在减少漂移和保持效率方面的优势。",
      "conclusion": "SCE-SLAM的主要贡献在于提出了一种新颖的端到端SLAM框架，通过场景坐标嵌入解决尺度漂移问题，从而提升单目SLAM的准确性和稳定性。研究具有重要学术价值，推动了视觉SLAM中学习与几何约束结合的发展，并为实际应用如自主导航和3D重建提供了高效解决方案。未来工作可能涉及扩展到动态环境或多传感器融合，摘要未明确说明具体局限性或详细方向。",
      "tags": [
        "Monocular SLAM",
        "Scene Coordinate Embeddings",
        "Geometry-Modulated Attention",
        "Bundle Adjustment",
        "Scale Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:33.730494Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09663",
    "title": "Self-Supervised Animal Identification for Long Videos",
    "authors": [
      "Xuyang Fang",
      "Sion Hannuna",
      "Edwin Simpson",
      "Neill Campbell"
    ],
    "abstract": "Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($>$97\\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \\href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09663.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09663",
    "published": "2026-01-14T17:53:59Z",
    "updated": "2026-01-14T17:53:59Z",
    "comment": "11 pages, 1 figure",
    "light_analysis": {
      "overview": "该论文提出了一种高效的自监督动物识别方法，通过将长视频中的个体识别重构为全局聚类任务，实现了超过97%的准确率，并显著降低了计算资源需求。",
      "motivation": "研究旨在解决长视频中个体动物识别的实际问题，这在行为生态学、野生动物监测和牲畜管理中至关重要。传统方法依赖大量人工标注，耗时耗力；而现有自监督方法因内存限制和时间误差传播，计算资源消耗大，不适合长序列。这使得自动化识别成为瓶颈，限制了大规模应用。通过提出更高效的方法，本研究旨在降低资源需求，提高识别效率，以推动相关领域的可扩展性和实用性。",
      "method": "该方法将动物识别重构为全局聚类任务，而非顺序跟踪，假设视频中个体数量固定，仅需边界框检测和总数。核心技术包括采样帧对提取特征，使用冻结的预训练骨干网络，以及通过自举机制和匈牙利算法进行批内伪标签分配，无需身份标签即可学习区分性特征。关键创新在于改编视觉语言模型中的二元交叉熵损失，优化学习过程。整体架构设计注重低内存消耗，适用于长视频处理，提升了计算效率。",
      "result": "在真实世界数据集上评估，如3D-POP鸽子视频和8-calves喂食视频，该方法实现了超过97%的准确率。每个批次的GPU内存消耗少于1 GB，比标准对比方法少一个数量级。性能匹配或超越了在超过1000个标注帧上训练的有监督基线，有效去除了人工标注瓶颈。这表明该方法不仅在精度上表现出色，还在资源效率上具有显著优势，适用于资源有限的环境。",
      "conclusion": "本研究的核心贡献在于提出了一种高效的自监督动物识别方法，显著降低了计算资源要求，使消费者级硬件上实现高精度识别成为可能。学术上，它推动了自监督学习在视频分析中的应用；实践上，为资源有限的研究环境提供了可行的解决方案。摘要未明确说明局限性，但未来工作可能包括扩展到更复杂场景，如动态个体数量或更长的视频序列，以进一步验证方法的鲁棒性。",
      "tags": [
        "Self-Supervised Learning",
        "Global Clustering",
        "Hungarian Algorithm",
        "Binary Cross Entropy Loss",
        "Pre-trained Backbone"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:25.762114Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09661",
    "title": "LiteEmbed: Adapting CLIP to Rare Classes",
    "authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Vineet Gandhi"
    ],
    "abstract": "Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09661.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09661",
    "published": "2026-01-14T17:53:11Z",
    "updated": "2026-01-14T17:53:11Z",
    "comment": "14 pages, 12 figures",
    "light_analysis": {
      "overview": "LiteEmbed 是一个轻量级框架，通过优化 CLIP 的文本嵌入，无需重新训练编码器，使模型能够适应罕见类别，提升了少样本个性化能力。",
      "motivation": "大规模视觉语言模型如 CLIP 在零样本识别中表现优异，但在罕见类别（如新出现实体或文化特定类别）上表现不佳，因为这些类别在预训练中很少见。这个问题在实际应用中很重要，因为现实世界经常需要快速适应新类别或少数类别，而现有方法可能依赖重新训练，导致高计算成本或不灵活。因此，需要一种轻量级方法来增强 CLIP 对罕见类的识别能力，以应对动态变化的类别需求。",
      "method": "LiteEmbed 采用子空间引导的优化框架，在 CLIP 的词库内调整文本嵌入。方法基于 PCA 分解分离粗语义方向和细粒度变化，并通过粗对齐与细分离两个互补目标，确保全局语义一致性同时增强视觉相似类别的可区分性。优化后的嵌入是即插即用的，可无缝替换原始文本特征，应用于分类、检索、分割和检测等多种任务。摘要未明确说明具体数据集或模型架构细节。",
      "result": "广泛实验显示，LiteEmbed 相对于先前方法取得了显著性能增益，尽管摘要未提供具体数据指标，但论文指出它在罕见类、未见过类或代表性不足类上表现优异，建立了该框架为有效适应方法，提升了 CLIP 在这些挑战性类别上的识别能力。",
      "conclusion": "LiteEmbed 的核心贡献是提供了一种轻量级框架，使 CLIP 能够高效适应新类别而无需重新训练，具有重要学术价值，扩展了视觉语言模型的应用范围，并增强了实际应用中的灵活性。研究展示了在少样本场景下的有效性，未来工作可能涉及进一步优化方法或扩展到更多领域，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "CLIP",
        "Few-shot Learning",
        "Text Embedding Optimization",
        "PCA",
        "Subspace-guided Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:06.777039Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09658",
    "title": "Image2Garment: Simulation-ready Garment Generation from a Single Image",
    "authors": [
      "Selim Emir Can",
      "Jan Ackermann",
      "Kiyohiro Nakayama",
      "Ruofan Liu",
      "Tong Wu",
      "Yang Zheng",
      "Hugo Bertiche",
      "Menglei Chai",
      "Thabo Beeler",
      "Gordon Wetzstein"
    ],
    "abstract": "Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09658.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09658",
    "published": "2026-01-14T17:47:33Z",
    "updated": "2026-01-14T17:47:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种前馈框架，通过微调视觉语言模型和训练轻量级预测器，从单张图像生成模拟就绪的服装，并准确估计材料属性。",
      "motivation": "研究旨在解决从单张图像估计物理准确、模拟就绪服装的挑战性问题。由于缺乏图像到物理的数据集以及该问题本身的不适定性，传统方法受到限制：现有方法要么依赖多视角捕获和昂贵的可微分模拟，增加了计算成本，要么仅预测服装几何形状而忽略材料属性，导致无法实现逼真的物理模拟。在实际应用中，如虚拟试衣或计算机图形学，这限制了服装模拟的真实感和效率，因此需要一种更高效、全面的解决方案来直接生成包含材料属性的模拟就绪服装模型。",
      "method": "研究方法提出一个前馈框架，分为两个关键步骤：首先，微调一个视觉语言模型（如基于CLIP的模型），从单张真实图像中推断材料成分（如织物类型）和织物属性（如密度、弹性）；其次，训练一个轻量级预测器，将上述属性映射到相应的物理织物参数（如摩擦系数、刚度），使用新构建的小型材料物理测量数据集（包括FTAG和T2P数据集）。该框架无需迭代优化，直接实现端到端的预测，创新点在于结合了视觉语言模型的语义理解和物理参数的精确映射，避免了传统方法中的复杂计算或多视图依赖。",
      "result": "实验结果显示，该方法在材料成分估计和织物属性预测方面达到更高的准确性，优于现有技术。具体来说，估计器在预测材料组成和属性时表现优异，通过物理参数估计器进一步生成模拟时，实现了比最先进的图像到服装方法更高的模拟保真度。与基线方法相比，该方法无需迭代优化，提高了效率，同时确保了模拟的物理真实性，但摘要未提供具体性能指标如准确率数值，仅强调了定性改进和整体效果的提升。",
      "conclusion": "论文的主要贡献在于提出了一种新颖的前馈框架，能够从单张图像生成模拟就绪的服装，并准确估计材料属性，解决了现有方法的不足。学术价值体现在引入视觉语言模型和物理参数映射的融合，以及构建了两个新数据集（FTAG和T2P），为相关研究提供了数据支持。实际应用价值在于为计算机图形学、虚拟现实和增强现实等领域提供更高效、逼真的服装模拟工具。摘要未明确说明局限性或未来工作方向，可能包括扩展数据集、处理更复杂场景或优化模型泛化能力。",
      "tags": [
        "Vision-Language Model",
        "Material Property Estimation",
        "Physics Simulation",
        "Feed-forward Framework",
        "Image-to-Garment Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:40.729910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09654",
    "title": "Exploring Fine-Tuning for Tabular Foundation Models",
    "authors": [
      "Aditya Tanna",
      "Pratinav Seth",
      "Mohamed Bouadi",
      "Vinay Kumar Sankarapu"
    ],
    "abstract": "Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09654.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09654",
    "published": "2026-01-14T17:40:46Z",
    "updated": "2026-01-14T17:40:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文首次全面研究表格基础模型的微调效果，发现微调益处高度依赖模型和数据特性，提供了微调适用性的实用指南。",
      "motivation": "表格基础模型在结构化数据上展现出强大的零样本学习能力，但微调的有效性尚不明确，现有方法如监督微调可能损害性能或缺乏系统评估。本研究旨在解决这一实际问题，探索微调在表格基础模型中的价值，分析何时微调最为有益，以弥补现有研究对微调条件依赖性评估的不足，推动结构化数据处理的优化应用。",
      "method": "研究通过比较零样本学习、元学习、全监督微调和参数高效微调等方法，在TALENT、OpenML-CC18和TabZilla等基准数据集上进行实验。关键创新点在于首次对表格基础模型的微调进行跨数据集和方法的综合研究，分析数据集因素如不平衡性、大小和维度对性能、校准和公平性的影响，提供了实证分析框架。",
      "result": "研究结果显示，零样本表格基础模型已能达到强性能，而微调增益有限：元学习和参数高效微调在特定条件下提供中等改进，全监督微调常导致准确性或校准质量下降。通过对比基线方法，分析了数据集特性如何影响结果，为微调应用提供了具体指南，但摘要未明确说明具体性能指标如准确率数据。",
      "conclusion": "论文总结微调对表格基础模型效果不一，需根据模型和数据谨慎应用。主要贡献在于首次系统评估微调策略，提供了实践指南，揭示了微调的局限性，具有学术和实际价值。未来工作可优化微调方法以提高稳定性和泛化能力，推动更高效的结构化数据处理。",
      "tags": [
        "Tabular Foundation Models",
        "Fine-Tuning",
        "Meta-Learning",
        "Parameter-Efficient Fine-Tuning",
        "Zero-Shot Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:30.416279Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09652",
    "title": "AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking",
    "authors": [
      "Emanuel da Costa Silva",
      "Tatiana Taís Schein",
      "José David García Ramos",
      "Eduardo Lawson da Silva",
      "Stephanie Loi Brião",
      "Felipe Gomes de Oliveira",
      "Paulo Lilles Jorge Drews-Jr"
    ],
    "abstract": "Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09652.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09652",
    "published": "2026-01-14T17:38:41Z",
    "updated": "2026-01-14T17:38:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出 AquaFeat+，一种基于学习的水下视觉增强方法，通过端到端训练提升物体检测、分类和跟踪的性能。",
      "motivation": "水下视频分析面临低光照、颜色失真和浑浊度等环境挑战，这些因素严重损害视觉数据质量，并直接影响水下机器人应用中感知模块的性能。现有方法可能侧重于改善人类视觉感知，而非优化自动化任务的性能，导致在复杂水下环境中任务效果不佳。因此，需要开发专门针对自动化视觉任务的特征增强技术，以解决数据质量下降问题并提升机器人感知能力。",
      "method": "AquaFeat+ 是一种即插即用的管道，包括颜色校正模块用于纠正水下颜色失真，层次特征增强模块提取多尺度特征，以及自适应残差输出模块优化最终表示。该方法通过端到端训练，直接使用物体检测、分类和跟踪任务的损失函数进行指导，确保特征增强针对具体任务目标。在 FishTrack23 数据集上进行训练和评估，但具体模型架构摘要未明确说明。",
      "result": "在 FishTrack23 数据集上的评估显示，AquaFeat+ 在物体检测、分类和跟踪指标上取得了显著改进。摘要未提供具体性能指标数据，如准确率提升百分比，但通过与基线方法对比，验证了该方法能有效增强水下机器人感知任务的效果，并证实了其在实际应用中的实用性。",
      "conclusion": "本研究的主要贡献是提出了 AquaFeat+，一个专为自动化水下视觉任务设计的特征增强方法，通过端到端学习优化感知模块性能。验证了其在物体检测、分类和跟踪中的有效性，对水下机器人技术具有重要的学术和应用价值。摘要未明确说明局限性或未来工作方向，但可推断未来可能涉及扩展到更广泛的数据集或实时处理优化。",
      "tags": [
        "Underwater Vision",
        "Object Detection",
        "Classification",
        "Tracking",
        "Feature Enhancement"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:01.082895Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09648",
    "title": "Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation",
    "authors": [
      "Andrew Moore",
      "Paul Rayson",
      "Dawn Archer",
      "Tim Czerniak",
      "Dawn Knight",
      "Daisy Lal",
      "Gearóid Ó Donnchadha",
      "Mícheál Ó Meachair",
      "Scott Piao",
      "Elaine Uí Dhonnchadha",
      "Johanna Vuorinen",
      "Yan Yabo",
      "Xiaobin Yang"
    ],
    "abstract": "Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09648.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09648",
    "published": "2026-01-14T17:31:21Z",
    "updated": "2026-01-14T17:31:21Z",
    "comment": "12 pages, 2 figures",
    "light_analysis": {
      "overview": "论文开发了PyMUSAS框架，通过混合规则和神经网络使用银色标准数据进行多语言语义标注，并释放了开源资源。",
      "motivation": "研究动机源于UCREL语义分析系统（USAS）框架在词义消歧评估方面的不足，现有方法仅限于词汇覆盖或单语言评估，缺乏开放的大规模多语言评估。这导致基于规则的系统可能无法充分利用神经网络的优势，且手动标注训练数据稀缺，限制了语义标注技术的进步。因此，本研究旨在填补这一空白，通过综合评估和多语言数据集，提升语义分析的准确性和泛化能力。",
      "method": "研究方法包括创建一个新的银色标注英文数据集，以解决手动标注数据缺乏的问题。该数据集用于训练和评估多种单语和多语神经网络模型，如PyMUSAS框架所示，覆盖了五种语言（使用四个现有数据集和一个新的中文数据集）。核心创新点在于结合规则和神经网络方法，进行单语和跨语言评估，对比基于规则的USAS系统，以展示如何用神经网络增强传统系统。具体技术细节包括使用语义标注任务和模型架构优化。",
      "result": "主要实验结果包括进行了最大的语义标注评估，覆盖了五种语言，显示出神经网络模型可以增强基于规则的USAS系统。虽然摘要未明确说明具体性能指标如准确率提升，但评估表明混合方法在多语言设置中表现良好。与基线规则系统对比，神经网络模型在单语和跨语言评估中展示了改进潜力，并最终释放了模型、数据集和代码作为开源资源，促进了进一步研究和应用。",
      "conclusion": "结论表明，本研究的主要贡献是开发了PyMUSAS框架，为USAS框架提供了首次大规模多语言评估，并释放了开源资源。其学术价值在于推动了语义标注技术的发展，特别是通过混合方法解决了训练数据不足的挑战；实际应用价值体现在多语言自然语言处理任务中的潜在应用。未来工作方向可能包括扩展到更多语言或优化模型效率，但摘要未明确说明具体局限性。",
      "tags": [
        "Word Sense Disambiguation",
        "Semantic Tagging",
        "Rule-Based System",
        "Neural Network",
        "Multilingual NLP"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:40.430214Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09647",
    "title": "Identifying Models Behind Text-to-Image Leaderboards",
    "authors": [
      "Ali Naseh",
      "Yuefeng Peng",
      "Anshuman Suri",
      "Harsh Chaudhari",
      "Alina Oprea",
      "Amir Houmansadr"
    ],
    "abstract": "Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.",
    "categories": [
      "cs.CV",
      "cs.CR",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09647.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09647",
    "published": "2026-01-14T17:30:58Z",
    "updated": "2026-01-14T17:30:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究揭示了文本到图像排行榜中匿名模型可被轻易识别的安全缺陷，利用图像嵌入聚类实现高准确度去匿名化。",
      "motivation": "文本到图像模型在在线图像生成中广泛使用，基于投票的排行榜依赖匿名模型输出来确保公平比较。然而，现有匿名方法假设模型输出无法识别来源，存在安全漏洞。本研究旨在解决匿名模型被识别的风险，因为如果匿名性被打破，将破坏排行榜的公正性和安全性，威胁模型评估的可靠性，这对于促进模型发展和应用公平性至关重要。",
      "method": "研究方法采用基于质心的聚类分析，通过分析文本到图像模型生成图像在图像嵌入空间中的特征。关键创新在于无需控制提示或依赖训练数据，直接利用图像嵌入表示识别模型独特聚类。使用22个模型和280个提示生成150,000张图像，计算图像嵌入的质心以捕获模型特定签名，并引入提示级别的可区分性度量以量化模型可识别性。",
      "result": "实验结果显示，基于图像嵌入聚类的方法实现了高准确度的模型去匿名化，在22个模型上揭示出系统性的模型特定签名。通过提示级别分析，某些提示能导致近乎完美的模型可区分性，暴露了匿名机制的脆弱性。摘要未明确说明具体准确率数字，但方法展现出显著效果，突出了匿名性易被攻破的问题。",
      "conclusion": "本研究的主要贡献是暴露了文本到图像排行榜的基本安全缺陷，显示匿名模型可被轻易识别，强调现有匿名机制的不足。学术价值在于推动模型安全性和评估方法研究，实际应用价值在于提高模型比较的公平性意识。局限性包括未覆盖所有攻击场景，未来工作可探索更强匿名化防御技术，以增强模型评估的可靠性。",
      "tags": [
        "Text-to-Image Model",
        "Image Embedding",
        "Clustering",
        "Anonymization",
        "Distinguishability Metric"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:01.793504Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09636",
    "title": "PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records",
    "authors": [
      "Yibo Lyu",
      "Gongwei Chen",
      "Rui Shao",
      "Weili Guan",
      "Liqiang Nie"
    ],
    "abstract": "While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09636.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09636",
    "published": "2026-01-14T17:12:48Z",
    "updated": "2026-01-14T17:12:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出层次隐式意图对齐任务PersonalAlign，引入AndroidIntent基准和HIM-Agent方法，显著提升GUI代理的个性化和主动协助性能。",
      "motivation": "现有GUI代理在处理显式指令时表现优异，但现实应用中需对齐用户更复杂的隐式意图。隐式意图包括模糊指令中的省略偏好和基于用户状态的潜在习惯，这对于提供个性化主动协助至关重要。然而，当前方法常缺乏有效利用长期用户记录作为持续上下文来处理这些挑战，限制了代理的实际部署能力，突显了研究隐式意图对齐的重要性。",
      "method": "本研究提出层次意图记忆代理（HIM-Agent），它维护一个持续更新的个人记忆，层次化组织用户偏好和习惯以实现个性化。为支持研究，引入了AndroidIntent基准，该基准从20k条长期用户记录中标注了775个用户特定偏好和215个习惯，用于评估代理解析模糊指令和提供主动建议的能力。研究比较了包括GPT-5、Qwen3-VL和UI-TARS在内的多种GUI代理方法，以验证层次化记忆架构的有效性。",
      "result": "在AndroidIntent基准上的实验结果显示，HIM-Agent显著提升了代理的执行性能和主动性能。具体而言，执行性能提高了15.7%，主动性能提高了7.3%。与基线方法如GPT-5、Qwen3-VL和UI-TARS相比，HIM-Agent展现出更优的整体表现，验证了层次化记忆在隐式意图对齐和个性化中的有效性，为GUI代理的实际应用提供了数据支撑。",
      "conclusion": "本研究的贡献在于提出了PersonalAlign任务，开发了AndroidIntent基准和HIM-Agent方法，为GUI代理的隐式意图对齐提供了新思路。学术上，它推动了人机交互和AI代理的个性化研究；实际上，能通过主动协助提升用户体验。摘要未明确说明未来工作，但潜在方向包括扩展到更多应用场景或优化记忆更新机制，以进一步探索长期用户记录的有效利用。",
      "tags": [
        "GUI Agent",
        "Implicit Intent Alignment",
        "Hierarchical Memory",
        "Personalization",
        "Long-Term User Records"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:56.014582Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09635",
    "title": "LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach",
    "authors": [
      "Kuo Liang",
      "Yuhang Lu",
      "Jianming Mao",
      "Shuyi Sun",
      "Chunwei Yang",
      "Congcong Zeng",
      "Xiao Jin",
      "Hanzhang Qin",
      "Ruihao Zhu",
      "Chung-Piaw Teo"
    ],
    "abstract": "Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09635.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09635",
    "published": "2026-01-14T17:09:57Z",
    "updated": "2026-01-14T17:09:57Z",
    "comment": "Updated version of https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5329027",
    "light_analysis": {
      "overview": "论文提出了LEAN-LLM-OPT框架，一个轻量级少样本学习方法，利用大型语言模型代理自动制定大规模优化模型，通过工作流分解任务提高效率。",
      "motivation": "大规模优化模型是现代商业决策的关键支撑，但传统构建过程费时费力，高度依赖人工操作，导致效率低下。现有自动化方法往往灵活性不足，难以应对复杂建模需求，因此亟需开发高效、自动化的解决方案来减轻人力负担并加速决策流程，提升整体业务竞争力。",
      "method": "LEAN-LLM-OPT框架采用LLM代理协作方式：两个上游LLM代理基于问题描述和数据集动态构建工作流，将建模任务分解为结构化子任务；下游LLM代理遵循工作流生成最终优化模型。关键创新在于利用LLM的文本处理能力结合常见建模实践，将机械数据处理操作卸载到辅助工具，使下游代理专注于非标准化复杂部分，使用LLM如GPT-4.1和开源gpt-oss-20B实现轻量级少样本学习。",
      "result": "在广泛模拟中，LEAN-LLM-OPT展现出强劲性能，与最先进方法竞争激烈，摘要未提供具体数据但强调其有效性。该框架在新加坡航空收入管理应用案例中实现领先性能，覆盖多种场景，证明了实际应用价值；同时，引入了首个全面基准Large-Scale-OR和Air-NRM，为领域评估提供新标准。",
      "conclusion": "论文主要贡献是提出了LEAN-LLM-OPT框架，成功自动化大规模优化模型制定，并引入新基准推动领域发展。研究具有重要学术价值，拓展了LLM在优化任务中的应用，实际中能显著提升商业决策效率；未来工作可探索框架的泛化性和在更广泛场景中的适用性。",
      "tags": [
        "Large Language Model",
        "Optimization Model Formulation",
        "Few-Shot Learning",
        "Workflow Construction",
        "Benchmark Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:12.743380Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09633",
    "title": "TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion",
    "authors": [
      "Sahil Mishra",
      "Srinitish Srinivasan",
      "Srikanta Bedathur",
      "Tanmoy Chakraborty"
    ],
    "abstract": "Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric \"is-a\" relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09633.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09633",
    "published": "2026-01-14T17:08:37Z",
    "updated": "2026-01-14T17:08:37Z",
    "comment": "Accepted in The Web Conference (WWW) 2026",
    "light_analysis": {
      "overview": "本文提出TaxoBell框架，通过高斯盒嵌入建模语义不确定性，实现自监督分类法扩展，显著提升任务性能。",
      "motivation": "分类法作为结构化知识表示的基础，在电子商务、语义搜索和生物医学等领域至关重要，但手动扩展过程耗时费力，无法适应新概念的快速涌现。现有自动方法主要依赖点向量嵌入，这些方法虽能建模对称相似性，却难以处理分类法中核心的不对称\"is-a\"关系。盒嵌入虽通过支持包含和不相交关系提供改进潜力，但面临梯度不稳定、缺乏语义不确定性表示以及无法有效处理多义性或模糊性等关键限制，导致扩展效果不佳。因此，TaxoBell旨在解决这些问题，开发更鲁棒的分类法扩展方法。",
      "method": "TaxoBell采用高斯盒嵌入框架，将盒几何结构转换为多元高斯分布，其中均值编码概念的语义位置，协方差编码不确定性，从而更灵活地建模包含关系和语义模糊性。基于能量的优化方法用于稳定训练过程，避免传统盒嵌入的梯度不稳定问题，同时支持对模糊概念的鲁棒表示和可解释的层次推理。该框架在自监督设置下运行，无需额外标注数据，通过概率分布参数化增强嵌入能力，核心创新在于结合几何与统计模型以捕捉分类法中的不对称关系。摘要未明确说明具体数据集和模型架构细节。",
      "result": "在五个标准基准数据集上的实验显示，TaxoBell在分类法扩展任务中显著优于八种最先进的基线方法。具体性能指标上，平均倒数排名（MRR）提高了19%，召回率（Recall@k）提升了约25%，证明了其优越的性能。错误分析和消融研究进一步验证了TaxoBell在处理模糊概念和梯度稳定性方面的有效性和鲁棒性，凸显了高斯盒嵌入框架相比传统方法的优势。这些结果通过对比实验和定量评估得到支撑，展示了TaxoBell在真实场景中的高适用性。",
      "conclusion": "TaxoBell通过高斯盒嵌入框架解决了分类法扩展中的关键挑战，贡献了一种稳定、鲁棒且可解释的方法，其创新在于将几何嵌入与概率分布结合以建模语义不确定性。该研究在学术上推动了知识表示和自然语言处理领域的发展，在实际应用中可支持电子商务分类、语义搜索和生物医学发现等场景。尽管摘要未明确说明具体局限性和未来工作方向，但基于框架设计，潜在方向可能包括扩展到更复杂概念或跨领域应用。总体而言，TaxoBell为自动化分类法维护提供了高效工具。",
      "tags": [
        "Gaussian Box Embeddings",
        "Self-Supervised Learning",
        "Taxonomy Expansion",
        "Energy-Based Optimization",
        "Multivariate Gaussian Distribution"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:32.693825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09631",
    "title": "LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation",
    "authors": [
      "Stergios Chatzikyriakidis"
    ],
    "abstract": "Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant \"Reasoning Gap\": while native-like models (Claude 3.7) perform intuitively (40\\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\\% valid poems), while our hybrid verification loop restores performance to 73.1\\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09631.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09631",
    "published": "2026-01-14T17:05:17Z",
    "updated": "2026-01-14T17:05:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种结合大语言模型与确定性音韵算法的混合系统，以解决希腊诗歌押韵检测和生成的准确性问题。",
      "motivation": "尽管大语言模型在多种自然语言处理任务中表现卓越，但在基于音韵的现象如押韵检测和生成方面存在显著短板，这在低资源语言如现代希腊语中尤为突出。现有方法难以有效处理复杂的押韵模式，限制了诗歌自动生成和文学分析的应用。因此，研究如何整合LLMs与音韵算法以弥补这一不足，具有重要的学术和实际价值。",
      "method": "本研究开发了一个混合系统，通过实现希腊押韵类型的全面分类，包括Pure、Rich、Imperfect、Mosaic和Identical Pre-rhyme Vowel模式，并结合代理生成管道与音韵验证。系统评估了多种提示策略，如zero-shot、few-shot、Chain-of-Thought和RAG-augmented，并使用多个LLM模型进行测试，包括Claude 3.7、4.5、GPT-4o、Gemini 2.0以及开源模型Llama 3.1和Mistral Large。关键创新点在于将LLM的泛化能力与音韵算法的精确性相结合。",
      "result": "实验结果显示，纯LLM在押韵生成任务中表现极差，仅有少于4%的有效诗歌。混合系统通过音韵验证循环，显著提升了性能，达到73.1%的有效率。在检测任务中，推理密集型模型Claude 4.5在Chain-of-Thought提示下获得54%的准确率，优于其他模型和策略，揭示了明显的“推理差距”。这些结果表明混合方法在准确性方面优于纯LLM基线。",
      "conclusion": "本研究的主要贡献是提出了一种有效的混合方法，成功结合大语言模型与音韵算法，解决了希腊诗歌押韵的挑战。通过发布清理后的40,000+押韵律语料库和系统，研究为音韵自然语言处理领域提供了重要资源，支持未来探索。未来工作可以扩展到其他低资源语言或更广泛的音韵任务中，以进一步提升应用价值。",
      "tags": [
        "Large Language Models",
        "Phonological Filtering",
        "Chain-of-Thought",
        "Rhyme Detection",
        "Greek Poetry"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:04.332832Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09626",
    "title": "From Prompt to Protocol: Fast Charging Batteries with Large Language Models",
    "authors": [
      "Ge Lei",
      "Ferran Brosa Planella",
      "Sterling G. Baird",
      "Samuel J. Cooper"
    ],
    "abstract": "Efficiently optimizing battery charging protocols is challenging because each evaluation is slow, costly, and non-differentiable. Many existing approaches address this difficulty by heavily constraining the protocol search space, which limits the diversity of protocols that can be explored, preventing the discovery of higher-performing solutions. We introduce two gradient-free, LLM-driven closed-loop methods: Prompt-to-Optimizer (P2O), which uses an LLM to propose the code for small neural-network-based protocols, which are then trained by an inner loop, and Prompt-to-Protocol (P2P), which simply writes an explicit function for the current and its scalar parameters. Across our case studies, LLM-guided P2O outperforms neural networks designed by Bayesian optimization, evolutionary algorithms, and random search. In a realistic fast charging scenario, both P2O and P2P yield around a 4.2 percent improvement in state of health (capacity retention based health metric under fast charging cycling) over a state-of-the-art multi-step constant current (CC) baseline, with P2P achieving this under matched evaluation budgets (same number of protocol evaluations). These results demonstrate that LLMs can expand the space of protocol functional forms, incorporate language-based constraints, and enable efficient optimization in high cost experimental settings.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09626.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09626",
    "published": "2026-01-14T16:58:20Z",
    "updated": "2026-01-14T16:58:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了两种基于大语言模型的梯度自由方法来优化电池快速充电协议，显著提高电池状态健康性能。",
      "motivation": "电池充电协议优化面临评估缓慢、成本高昂和不可微分的挑战。现有方法如贝叶斯优化和进化算法通常通过约束搜索空间来应对这些困难，但这限制了协议多样性，阻碍了发现更高性能的解决方案。因此，需要一种新方法能够更有效地探索协议空间，提升优化效率，解决现有技术在复杂实验设置中的不足。",
      "method": "论文引入了两种基于大语言模型的梯度自由闭环方法：Prompt-to-Optimizer (P2O) 和 Prompt-to-Protocol (P2P)。P2O 使用 LLM 提出基于小神经网络的协议代码，然后通过内部循环进行训练；P2P 则直接使用 LLM 编写电流的显式函数及其标量参数。这些方法的关键创新在于利用 LLM 的自然语言能力扩展协议功能形式，并实现高效的优化过程，无需传统梯度计算。",
      "result": "在案例研究中，LLM 引导的 P2O 表现优于贝叶斯优化、进化算法和随机搜索。在现实的快速充电场景下，P2O 和 P2P 相比最先进的多步恒定电流基线，在状态健康指标上实现了约 4.2% 的改进。特别地，P2P 在匹配的评估预算下达到了这一性能提升，表明其在高成本实验设置中的效率优势，具体数据支持了方法的有效性。",
      "conclusion": "研究表明，大语言模型能够扩展充电协议的功能形式，整合基于语言的约束，并在高成本实验环境中实现高效优化。这项工作提供了电池充电协议优化的新途径，展示了 LLM 在复杂物理系统优化中的潜在应用价值。未来工作可能涉及扩展到其他领域或进一步改进方法效率，但摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Gradient-free Optimization",
        "Battery Fast Charging",
        "Protocol Optimization",
        "Neural Network"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:42.952775Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09624",
    "title": "Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric",
    "authors": [
      "Jiali Cheng",
      "Ziheng Chen",
      "Chirag Agarwal",
      "Hadi Amiri"
    ],
    "abstract": "Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09624.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09624",
    "published": "2026-01-14T16:55:58Z",
    "updated": "2026-01-14T16:55:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种基于模型电路的遗忘难度度量方法，通过电路级信号预评估样本遗忘的难易程度，从机制角度解析遗忘差异。",
      "motivation": "研究动机源于机器学习遗忘中样本遗忘效果的显著差异，这不仅涉及数据特性，还反映了模型内部信息编码和保护机制。由于遗忘对于构建可信赖和合规的语言模型至关重要，现有方法多为定性分析，缺乏对遗忘难度的系统化和可解释评估，导致遗忘过程不可预测和效果不一致。因此，深入探究模型机制以量化遗忘难度，有助于改进遗忘方法并提升模型的安全性和适应性。",
      "method": "研究方法采用机制视角，基于模型电路——结构化交互路径，提出 Circuit-guided Unlearning Difficulty (CUD) 指标。该指标在遗忘前使用电路级信号为每个样本分配连续难度分数，关键创新在于将遗忘难度与电路模式关联。具体技术包括分析交互路径的长短、深浅和位置，发现易遗忘样本对应短浅、早期交互路径，而难遗忘样本依赖长深、晚期计算路径。摘要未明确指定数据集或模型架构细节，但通过实验验证了方法有效性。",
      "result": "实验结果表明，CUD 指标能够可靠区分易遗忘和难遗忘样本，并且在多种遗忘方法中保持稳定性。识别出关键电路模式：易遗忘样本关联较短、较浅的交互路径，集中于模型早期到中期；难遗忘样本则与更长、更深的路径相关，靠近晚期计算阶段。与现有定性研究相比，CUD 提供了更系统化和可量化的分析框架，但摘要未明确提供具体数据如准确率提升，仅强调了其可靠性和可解释性优势。",
      "conclusion": "结论总结，该研究的主要贡献是提出了 CUD 度量，首次对遗忘难度进行了原则化、细粒度和可解释的分析。学术价值在于深化了对模型内部机制的理解，促进基于机制的遗忘方法开发；实际应用有助于优化遗忘过程，提高语言模型的合规性和可信度。局限性未明确说明，未来工作可能包括将方法扩展到更广泛的模型或任务中，以验证其泛化能力。",
      "tags": [
        "Machine Unlearning",
        "Model Circuits",
        "Unlearning Difficulty",
        "Circuit-guided Metric",
        "Interpretable Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:57.250014Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09613",
    "title": "CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems",
    "authors": [
      "Yonglin Tian",
      "Qiyao Zhang",
      "Wei Xu",
      "Yutong Wang",
      "Yihao Wu",
      "Xinyi Li",
      "Xingyuan Dai",
      "Hui Zhang",
      "Zhiyong Cui",
      "Baoqing Guo",
      "Zujun Yu",
      "Yisheng Lv"
    ],
    "abstract": "Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09613.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09613",
    "published": "2026-01-14T16:36:26Z",
    "updated": "2026-01-14T16:36:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文的核心贡献是提出了CogRail基准和一个联合微调框架，以增强视觉语言模型在铁路智能运输系统中的认知入侵感知能力。",
      "motivation": "现有铁路入侵感知系统主要关注固定视觉范围内的对象分类，并采用基于规则的启发式方法来确定入侵状态，这往往忽略了具有潜在入侵风险的目标。准确感知这些风险需要深入理解对象的空间上下文和时间动态，而传统视觉模型难以应对这一挑战。研究的重要性在于铁路运输系统的安全至关重要，现有方法的局限性可能导致安全隐患，因此急需开发更智能的认知方法来提升感知能力和系统可靠性。",
      "method": "论文首先引入了CogRail基准，该基准整合了开源数据集，并添加了认知驱动的问题-答案标注，以支持时空推理和预测任务。基于此基准，系统评估了当前先进的视觉语言模型（VLMs），使用多模态提示来识别其在认知入侵感知中的优缺点。进一步，论文通过微调VLMs以提高性能，并提出了一个联合微调框架，该框架整合了位置感知、运动预测和威胁分析三个核心任务，从而将通用基础模型有效地适配为专门用于认知入侵感知的专用模型，强调了多任务学习在结构化适应中的优势。",
      "result": "实验结果显示，当前的大型多模态模型在处理认知入侵感知任务所需的复杂时空推理方面表现不佳，这突显了现有基础模型在安全关键领域的局限性。相比之下，提出的联合微调框架通过针对性地适应领域特定的推理需求，显著增强了模型性能，提高了准确性和可解释性。这表明结构化多任务学习能有效改善模型在复杂场景中的表现，为安全关键应用提供了更有力的解决方案。",
      "conclusion": "论文的主要贡献包括创建了CogRail基准，用于评估视觉语言模型在认知入侵感知中的能力，并提出了一个联合微调框架来提升模型性能。该研究不仅促进了视觉语言模型在铁路安全等关键领域的应用，还强调了结构化多任务学习在增强模型适应性和可解释性方面的价值。局限性可能在于当前模型的时空推理能力仍有改进空间，未来工作方向可涉及更深入的模型优化或扩展到其他安全关键领域。",
      "tags": [
        "Visual-Language Models (VLMs)",
        "Spatio-temporal Reasoning",
        "Fine-tuning",
        "Multimodal Learning",
        "Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:52.027464Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09609",
    "title": "DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing",
    "authors": [
      "Qian Cao",
      "Yahui Liu",
      "Wei Bi",
      "Yi Zhao",
      "Ruihua Song",
      "Xiting Wang",
      "Ruiming Tang",
      "Guorui Zhou",
      "Han Li"
    ],
    "abstract": "Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09609.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09609",
    "published": "2026-01-14T16:30:20Z",
    "updated": "2026-01-14T16:30:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出DPWriter框架，通过多样化规划分支和组感知奖励，增强大型语言模型在创意写作中的输出多样性。",
      "motivation": "基于强化学习的大型语言模型增强通常导致输出多样性下降，这在开放任务如创意写作中削弱了模型的实用性。现有方法缺乏明确的机制来引导多样化探索，过于注重优化效率和性能，而忽视了多样性。因此，研究旨在开发一种框架，在保持生成质量的同时，显著提升输出多样性，以满足创意写作等任务的需求。",
      "method": "论文提出DPWriter框架，采用基于半结构化长链思维的强化学习方法，将创意写作生成过程分解为明确规划的中间步骤。核心创新是多样化规划分支方法，在规划阶段根据多样性变化战略性地引入分歧，并结合组感知多样性奖励机制，以鼓励不同的生成轨迹。该方法旨在通过规划层面的多样性控制，提升输出多样性而不影响质量。",
      "result": "在创意写作基准测试中，实验结果表明，该方法显著提高了输出多样性，同时保持了生成质量，一致优于现有基线方法。尽管摘要未提供具体性能指标，但明确指出多样性得到改善且质量未受损，证明了方法的有效性。",
      "conclusion": "该研究的主要贡献是提出了DPWriter框架，通过多样化规划分支和组感知奖励，有效解决了RL增强LLMs时多样性减少的问题。学术上，它推动了强化学习在大型语言模型多样性控制方面的发展；实际应用中，可提升创意写作等开放任务的模型表现。未来工作可能包括扩展到其他任务或优化奖励机制。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Chain-of-Thought",
        "Diverse Planning"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:50.665527Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09606",
    "title": "GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis",
    "authors": [
      "Manning Gao",
      "Leheng Zhang",
      "Shiqin Han",
      "Haifeng Hu",
      "Yuncheng Jiang",
      "Sijie Mai"
    ],
    "abstract": "Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09606.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09606",
    "published": "2026-01-14T16:26:44Z",
    "updated": "2026-01-14T16:26:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一个两阶段分组排序和校准框架（GRCF），用于多模态情感分析，通过自适应学习难排序样本和动态调整边界来提升性能和泛化能力。",
      "motivation": "多模态情感分析研究通常采用点向回归方法，这种方法虽然直接，但对标签噪声敏感，且忽视样本间的相对顺序，导致预测不稳定和相关对齐不良。成对序学习框架出现以弥补此缺陷，通过比较学习来捕捉相对顺序，但引入两个新权衡：首先，赋予所有比较均匀重要性，未能自适应地关注难排序样本；其次，采用静态排名边界，无法反映情感组间变化的语义距离。因此，需要开发一种能解决这些问题并提升分析稳定性和精度的框架。",
      "method": "论文提出一个两阶段分组排序和校准框架（GRCF），借鉴 Group Relative Policy Optimization（GRPO）思想。第一阶段引入优势加权动态边界排名损失，自适应地关注难排序样本并构建细粒度序结构；第二阶段采用 MAE（Mean Absolute Error）驱动的目标进行预测幅度校准，确保绝对分数对齐。此外，框架扩展到分类任务，如多模态幽默检测和讽刺检测，以验证其泛化能力，涵盖模型自适应和损失函数优化等关键细节。",
      "result": "GRCF 在核心回归基准测试中实现了最先进的性能，表明其能有效提升多模态情感分析的效果，具体表现为在标准数据集上的预测准确性和稳定性增强。当扩展应用到分类任务时，如多模态幽默检测和讽刺检测，框架展现出强大的泛化能力，摘要未明确说明具体性能指标如准确率提升，但强调其在回归和分类任务中优于基线方法，验证了方法的鲁棒性和适应性。",
      "conclusion": "本研究的主要贡献是提出 GRCF 框架，通过两阶段设计同时保持相对序结构、确保绝对分数校准，并自适应关注难排序样本。这具有学术价值，为解决多模态情感分析中的排序和校准问题提供了创新思路；实际应用价值体现在其在回归和分类任务中的高性能和泛化能力，可扩展到其他多模态应用。未来工作可进一步优化框架参数或探索其在更广泛任务中的应用，以克服潜在的局限性如计算复杂度。",
      "tags": [
        "Multimodal Sentiment Analysis",
        "Group Relative Policy Optimization",
        "Dynamic Margin Ranking",
        "MAE-driven Calibration",
        "Classification Generalization"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:12.501529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09605",
    "title": "Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets",
    "authors": [
      "Jeremiah Coholich",
      "Justin Wit",
      "Robert Azarcon",
      "Zsolt Kira"
    ],
    "abstract": "Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\\% on views that the non-augmented policy fails completely on.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09605.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09605",
    "published": "2026-01-14T16:25:13Z",
    "updated": "2026-01-14T16:25:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MANGO方法，通过sim2real图像翻译生成多样相机视角，增强机器人视觉策略的鲁棒性。",
      "motivation": "机器人视觉策略在操作中取得显著成功，但对分布变化如相机视角变化脆弱。真实世界机器人演示数据稀缺且通常视角单一，难以覆盖多样场景，导致策略泛化能力不足。模拟可大规模收集数据并提供全面视角覆盖，但存在视觉sim2real差距，使得模拟数据难以直接用于真实世界。因此，需开发方法桥接此差距，以低成本增强策略对视角变化的适应性。",
      "method": "MANGO是一种非配对图像翻译方法，核心创新包括分割条件InfoNCE损失，结合分割信息以保持视角一致性；高度正则化的鉴别器设计，防止过拟合并稳定训练；以及修改的PatchNCE损失，优化局部特征匹配。训练仅需少量真实固定相机数据，通过翻译模拟观察生成多样未见视角，用于数据增强。这些技术元素共同确保在sim2real转换中维持视觉特征不变性。",
      "result": "实验显示，MANGO在sim2real图像翻译任务中优于所有测试的其他方法。具体地，使用MANGO增强数据训练的模仿学习策略在非增强策略完全失败的视角上实现了高达60%的成功率。这表明MANGO能有效生成真实感图像，显著提升策略对新视角的适应能力，验证了其在数据增强和性能改进方面的优越性。",
      "conclusion": "本论文贡献在于提出MANGO方法，通过创新损失函数和正则化设计，有效解决sim2real图像翻译中的视角一致性挑战，增强了机器人策略对相机视角变化的鲁棒性。研究具有学术价值，推动了数据增强和机器人视觉泛化技术的发展，并具实际应用潜力，可用于机器人操作等场景。未来工作可探索更多分布变化类型或集成其他增强技术以扩展应用范围。",
      "tags": [
        "Sim2real Translation",
        "Image Translation",
        "InfoNCE Loss",
        "PatchNCE Loss",
        "Viewpoint Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:33.398134Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09601",
    "title": "Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric",
    "authors": [
      "Emmanuele Barberi",
      "Felice Sfravara",
      "Filippo Cucinotta"
    ],
    "abstract": "Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09601.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09601",
    "published": "2026-01-14T16:16:51Z",
    "updated": "2026-01-14T16:16:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于微分熵的新度量IDEM，用于3D点云细刚性配准，解决了传统方法依赖固定点云的局限性。",
      "motivation": "点云配准是计算机视觉中的核心任务，但传统方法如迭代最近点（ICP）需要选择一个点云作为固定参考，欧氏距离度量如均方根误差（RMSE）在点云存在密度差异、噪声、孔洞或有限重叠时效果不佳。这些问题在真实场景中普遍，导致配准结果不稳定，限制了算法的鲁棒性和实际应用，因此需要一种不依赖于固定点云选择、能处理复杂条件的度量方法。",
      "method": "作者引入了迭代微分熵最小化（IDEM）方法，将微分熵作为优化框架中的目标函数。该度量不依赖于固定点云的选择，在变换过程中能明确显示最小值对应最佳对齐。关键创新是使用微分熵来量化点云差异，避免了传统欧氏度量的非交换性问题，通过迭代优化实现精细的刚性配准，适用于各种挑战性场景。",
      "result": "通过多个案例研究，将IDEM与RMSE、Chamfer距离和Hausdorff距离进行比较。结果表明，即使在密度差异、噪声、孔洞和部分重叠条件下，IDEM也能有效工作，产生更优的对齐效果，而RMSE在这些场景下不一定产生最优对齐。新度量在复杂点云数据中展现了更高的鲁棒性和准确性。",
      "conclusion": "本研究提出了一种新的点云配准度量IDEM，解决了传统方法对固定点云的依赖问题，提高了在密度差异、噪声、孔洞和有限重叠条件下的配准鲁棒性。这为计算机视觉中的点云处理提供了更可靠的工具，具有重要的学术价值和实际应用潜力。摘要未明确说明具体局限性，但未来工作可能包括将该方法扩展到非刚性配准或集成到更广泛的优化框架中。",
      "tags": [
        "Point Cloud Registration",
        "Differential Entropy",
        "Iterative Optimization",
        "Rigid 3D Registration",
        "Metric Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:38.009999Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09588",
    "title": "Energy-Entropy Regularization: The True Power of Minimal Looped Transformers",
    "authors": [
      "Wai-Lun Lam"
    ],
    "abstract": "Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09588.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09588",
    "published": "2026-01-14T15:56:35Z",
    "updated": "2026-01-14T15:56:35Z",
    "comment": "19 pages, 2 figures",
    "light_analysis": {
      "overview": "论文提出了一种结合Tsallis熵和Hamiltonian动力学的训练框架，成功训练单头循环Transformer解决复杂任务，并揭示了其内部推理机制。",
      "motivation": "本研究旨在解决单头循环Transformers在训练中因高度非凸和不规则损失地形导致的优化困难问题。循环Transformers因其优越推理能力而备受关注，但现有训练方法常失败或性能不佳，使得优化过程容易陷入不良局部极小值和鞍点，阻碍了全局最小值的发现。这不仅限制了模型潜力，也使其内部机制难以理解，从而成为一个重要挑战。",
      "method": "论文提出了一种新颖的训练框架，利用Tsallis熵和Hamiltonian动力学来转换损失地形的几何形状。核心创新是将参数更新视为物理流，通过能量熵正则化平滑优化过程，改善训练稳定性。模型使用单头循环Transformer，模型维度为d=8，在归纳头任务上进行训练，输入序列长度为1000个令牌。技术特色包括结合信息论和物理动力学，以提高模型在复杂损失地形中的收敛性。",
      "result": "通过提出的框架，成功训练了模型维度d=8的单头循环Transformer，在输入序列长度1000的归纳头任务上取得了成功。摘要未明确说明具体性能指标如准确率提升，但成功训练表明框架能有效克服优化障碍，使模型能够发现损失地形的全局最小值。相比基线方法（传统训练方法常失败），这显示了显著改进，验证了框架在提升模型能力方面的有效性。",
      "conclusion": "主要贡献是开发了基于能量熵正则化的训练框架，成功训练单头循环Transformer并揭示了其内部推理机制。学术价值在于为理解循环架构提供了新视角，并引入物理动力学优化训练过程。实际应用价值包括改进循环Transformers的训练，增强其在复杂任务上的性能。未来工作可扩展至更高维模型或更广泛任务，但摘要未明确说明局限性。",
      "tags": [
        "Looped Transformer",
        "Tsallis Entropy",
        "Hamiltonian Dynamics",
        "Energy-Entropy Regularization",
        "Induction Head Task"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:47.725349Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09586",
    "title": "Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents",
    "authors": [
      "Said Yasin",
      "Torsten Zesch"
    ],
    "abstract": "Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09586.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09586",
    "published": "2026-01-14T15:55:26Z",
    "updated": "2026-01-14T15:55:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文评估了模块化和端到端系统在手写文档视觉错误反馈中的应用，识别了当前方法的不足。",
      "motivation": "手写技能在教育中至关重要，但为手写文档提供视觉错误反馈的研究相对不足。现有方法在将手写输入图像转化为准确的反馈时面临诸多挑战，这限制了教育和其他应用中的有效反馈支持。该研究旨在填补这一空白，通过探讨视觉反馈的重要性及其在技术实现中的难点。",
      "method": "论文采用经验性比较方法，评估了模块化系统和端到端系统在手写文档视觉错误反馈中的表现。摘要未明确说明具体技术路线细节，但提到了从图像到反馈的挑战概述，例如如何处理手写输入的复杂性和反馈的精确放置。研究关注于系统设计的方法比较，而非具体模型架构或数据集。",
      "result": "结果表明，模块化和端到端系统目前都没有达到可接受的整体质量。论文识别了主要挑战，如反馈的准确性和适应性，但摘要未提供具体的性能数据如准确率提升。这强调了现有方法的局限性，并暗示了需要改进的方向。",
      "conclusion": "论文总结了手写文档视觉错误反馈领域的现状，指出了现有方法的不足，并提出了未来研究的议程。这有助于推动该领域的学术研究，并为实际应用如教育工具的开发提供指导。潜在局限性包括缺乏详细实验数据，未来工作可聚焦于改进系统设计和集成先进技术。",
      "tags": [
        "Handwriting Recognition",
        "Visual Feedback",
        "Modular Systems",
        "End-to-End Systems",
        "Error Correction"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:41.296035Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09579",
    "title": "Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels",
    "authors": [
      "Fiona Murphy",
      "Alessio Benavoli"
    ],
    "abstract": "Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09579.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09579",
    "published": "2026-01-14T15:48:53Z",
    "updated": "2026-01-14T15:48:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过核主成分回归框架理论统一了两种先进的基于核的格兰杰因果方法，并引入了改进的高斯过程得分模型及同期因果识别算法，提升了非线性时间序列因果发现的性能。",
      "motivation": "研究动机在于解决时间序列数据中非线性因果关系的识别问题，这在金融、经济和生物等领域具有重要意义，因为现实世界中许多关系是非线性的。现有基于核的格兰杰因果方法虽能处理非线性，但在准确性和效率上仍有不足，特别是在同期因果发现方面，因此需要更强大的方法来克服这些局限性，推动因果推断的理论和实际应用。",
      "method": "研究方法首先在核主成分回归（KPCR）框架下对两种先进的核格兰杰因果方法进行理论统一，并基于此提出新方法。其次，引入高斯过程（GP）得分模型，采用平滑信息准则（SIC）对边际似然进行惩罚，记为$GP_{SIC}$，以优化因果识别。最后，提出一个完全基于格兰杰因果的同期因果识别算法，利用$GP_{SIC}$模型，并与现有先进算法比较，关键创新在于理论整合和模型改进。",
      "result": "主要实验结果表明，提出的方法在因果识别性能上有所改进，优于现有先进的非线性时间序列因果发现方法。具体地，基于KPCR的统一方法和$GP_{SIC}$模型展示了提升的准确性，同期因果识别算法在比较中也表现出优势，但摘要未明确说明具体数据指标，如准确率或效率提升的数值。",
      "conclusion": "本文的主要贡献是理论统一了核格兰杰因果方法，并引入$GP_{SIC}$模型和同期因果识别算法，推动了非线性因果发现领域的发展。其学术价值在于提供更有效的工具，实际应用可扩展到时间序列分析相关领域。局限性可能包括对数据假设的依赖，未来工作可集中于算法扩展和更广泛的实证验证。",
      "tags": [
        "Kernel Methods",
        "Granger Causality",
        "Gaussian Processes",
        "Nonlinear Causal Discovery",
        "Time Series Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:58.163474Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09575",
    "title": "OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding",
    "authors": [
      "Sheng-Yu Huang",
      "Jaesung Choe",
      "Yu-Chiang Frank Wang",
      "Cheng Sun"
    ],
    "abstract": "We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09575.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09575",
    "published": "2026-01-14T15:45:57Z",
    "updated": "2026-01-14T15:45:57Z",
    "comment": "project page: https://peterjohnsonhuang.github.io/openvoxel-pages/",
    "light_analysis": {
      "overview": "OpenVoxel是一种无需训练的算法，通过分组和标注稀疏体素来实现开放词汇3D场景理解。",
      "motivation": "开放词汇3D场景理解任务如分割和指代表达分割，现有方法通常需要训练并依赖CLIP/BERT等文本编码器，导致计算成本高和灵活性不足。该研究旨在解决这些局限性，通过开发训练免费的方法来提高效率和适应性，特别是在复杂任务中减少资源依赖。摘要指出，与先前研究相比，该方法避免了训练过程和传统嵌入的引入，以提供更高效且通用的解决方案。",
      "method": "方法基于从多视图图像获得的稀疏体素光栅化模型，利用视觉语言模型和多模态大语言模型对体素进行分组和标注。核心创新是训练免费，直接使用MLLMs进行文本到文本搜索，无需依赖CLIP/BERT编码器的嵌入。具体步骤包括处理SVR模型生成体素表示，应用VLMs和MLLMs为每组生成标注，构建场景地图以支持开放词汇分割和指代表达分割等任务。",
      "result": "通过广泛实验，OpenVoxel在开放词汇分割和指代表达分割任务中表现优异，特别是在复杂指代表达分割任务上超越了最近的研究。尽管摘要未明确说明具体性能指标如准确率提升数据，但结果展示了方法的有效性和领先优势，表明其在多种场景下能实现高质量的3D理解和分割效果。",
      "conclusion": "本研究的主要贡献是提出了训练免费的OpenVoxel算法，实现了高效的开放词汇3D场景理解。它具有学术创新价值，通过结合多模态模型推进了无需训练的方法发展，并具有实际应用前景，如支持三维场景分析和交互任务。未来方向可能包括扩展应用到更多场景或优化算法效率，代码开源将促进社区的进一步研究和应用。",
      "tags": [
        "Open-Vocabulary 3D Scene Understanding",
        "Sparse Voxel Rasterization",
        "Vision Language Models",
        "Multi-modal Large Language Models",
        "Training-Free Algorithm"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:04.923493Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09572",
    "title": "Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model",
    "authors": [
      "Tianli Tao",
      "Ziyang Wang",
      "Delong Yang",
      "Han Zhang",
      "Le Zhang"
    ],
    "abstract": "Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09572.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09572",
    "published": "2026-01-14T15:41:40Z",
    "updated": "2026-01-14T15:41:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出DF-DiffCom，一种基于Kolmogorov-Arnold Networks增强的扩散模型，通过变形场实现可信的纵向脑MRI图像补全，提高生成质量和应用灵活性。",
      "motivation": "纵向脑MRI对寿命研究至关重要，但高失访率常导致数据缺失，使分析复杂化。现有深度生成模型主要依赖图像强度，存在两个关键局限：一是生成图像的可信度不足，影响下游研究的可靠性；二是模型结构固定的指导导致使用灵活性受限，难以适应多样化应用场景。因此，需要开发更可靠、灵活的补全方法，以支持准确的医学分析和寿命研究，解决现有模型在信任度和适应性方面的不足。",
      "method": "本研究提出DF-DiffCom方法，采用Kolmogorov-Arnold Networks（KAN）增强的扩散模型框架。核心创新在于巧妙地利用变形场来指导纵向脑MRI图像补全过程，通过结合解剖结构变化信息，提升生成图像的可信度和生物合理性。模型训练在OASIS-3数据集上进行，利用扩散模型生成高质量图像，KAN的应用增强了模型的非线性表达能力和适应性，减少了对纯图像强度的依赖。这种变形场引导的机制使得补全更符合真实脑结构变化，为纵向医学图像分析提供了新的技术路线。",
      "result": "在OASIS-3数据集上训练和评估，DF-DiffCom显著优于现有最先进方法。具体性能指标显示，峰值信噪比（PSNR）提升了5.6%，结构相似性指数（SSIM）增加了0.12，表明图像质量和结构保真度有显著改善。此外，模型展现出模态不可知特性，能够无缝扩展到多种MRI模态，并成功应用于脑组织分割等属性图生成，验证了其在多样化应用场景中的有效性和鲁棒性，优于基线方法在可信度和灵活性上的表现。",
      "conclusion": "本文的主要贡献是开发了DF-DiffCom，一种基于KAN和变形场的扩散模型，用于可信的纵向脑MRI补全。学术上，该方法通过结合变形场提高了生成图像的可信度，推动了生成模型在医学图像领域的发展；实践上，模态不可知设计增强了应用灵活性，支持多模态数据分析和下游任务如组织分割。局限性可能包括对训练数据集的依赖或计算效率，未来工作可探索更多数据集验证、优化模型效率，并扩展至其他医学图像应用，以提升临床实用价值。",
      "tags": [
        "Diffusion Model",
        "Kolmogorov-Arnold Networks",
        "Deformation Fields",
        "Brain MRI",
        "Image Completion"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:26.420387Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09570",
    "title": "Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering",
    "authors": [
      "Dimitris Panagopoulos",
      "Adolfo Perrusquia",
      "Weisi Guo"
    ],
    "abstract": "Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09570.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09570",
    "published": "2026-01-14T15:39:52Z",
    "updated": "2026-01-14T15:39:52Z",
    "comment": "16 pages, 9 Figures, Version submitted to IEEE for publication",
    "light_analysis": {
      "overview": "论文提出了Dialogue Telemetry框架，通过Progress Estimator和Stalling Index实现轮次级别的自主对话测量，提高监控效率。",
      "motivation": "自主系统在执行基于模式的信息收集对话时，面临测量缺口，缺乏轮次级别的可观测性来监控获取效率和检测何时提问变得无效。现有方法通常难以实时识别对话停滞，导致资源浪费和效率低下。这一问题在诸如搜索救援等需要高效信息收集的领域尤为重要，因停滞可能导致操作成本增加和任务失败。摘要未明确说明其他现有方法的不足，但强调了turn-level observables的缺失。",
      "method": "论文引入了Dialogue Telemetry框架，在每个问答交换后产生两个模型无关的信号：Progress Estimator量化每个类别的剩余信息潜力（包括基于比特的变体），Stalling Index检测停滞模式，即重复类别探测且响应语义相似、边际增益低的特征，无需因果诊断。验证方法使用基于大型语言模型的模拟，在受控的搜索救援访谈中实施，并将DT信号集成到强化学习策略中。这突出了模型无关和可扩展的技术特色。",
      "result": "通过在受控搜索救援访谈的LLM模拟中验证，DT能有效区分高效对话轨迹和停滞对话。将DT信号集成到强化学习策略后，提升了策略性能，特别是在停滞导致操作成本增加的场景下。这展示了DT框架的下游实用性和有效性，但摘要未明确提供具体数值指标如准确率提升。与基线方法的对比方面，DT提供了可观测的信号以支持监控和改进。",
      "conclusion": "DT提供了一个可解释的轮次级别测量工具，改善了自主对话系统的监控能力，具有学术价值，并在实际应用中能减少操作成本。该研究的主要贡献在于解决了信息收集对话中的测量缺口，支持实时监控和策略优化。潜在的局限性包括摘要未明确说明的泛化能力，未来工作可能扩展DT到更多场景或优化算法。",
      "tags": [
        "Dialogue Telemetry",
        "Progress Estimator",
        "Stalling Index",
        "Large Language Model",
        "Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:16.906321Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09566",
    "title": "Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling",
    "authors": [
      "Shuyang Xiang",
      "Hao Guan"
    ],
    "abstract": "Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \\times 8$ pixels. Remarkably, these inputs achieve 39.2\\% accuracy, comparable to the index-based baseline of 39.1\\%. Such low-resource settings also exhibit a pronounced \\emph{hot-start} effect: by 0.4\\% of total training, accuracy reaches above 12\\%, while index-based models lag at below 6\\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09566.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09566",
    "published": "2026-01-14T15:34:37Z",
    "updated": "2026-01-14T15:34:37Z",
    "comment": "15 pages, 5 figures, submitted to ACL 2026",
    "light_analysis": {
      "overview": "该论文提出使用低分辨率视觉令牌作为中文语言建模的输入，替代传统索引令牌，证明了视觉结构的有效性。",
      "motivation": "在大型语言模型中，中文字符通常被表示为基于索引的离散令牌，这忽略了其视觉形式。然而，对于象形文字如中文，字符的视觉结构承载语义和语音信息，可能提升预测能力。现有方法主要依赖索引表示，缺乏对视觉信息的利用，可能导致在资源有限场景下效率低下。因此，研究视觉输入是否能作为字符级建模的替代方案，对于补充传统方法和提高模型效率具有重要意义。",
      "method": "论文提出了一种新颖方法，使用低分辨率灰度图像作为中文字符的视觉输入。具体地，解码器接收单个字符的图像，分辨率可低至8x8像素，替代传统的索引令牌。关键创新在于直接利用字符的视觉结构进行建模，而不依赖离散表示。尽管摘要未明确说明具体的模型架构或使用的数据集，但可以推断该方法基于现有语言模型框架，专注于处理低分辨率视觉输入以捕捉字符的基本视觉特征。",
      "result": "实验结果显示，使用低分辨率视觉输入的方法在准确率上达到39.2%，与基于索引的基线39.1%相当，证明了其有效性。在低资源训练设置下，该方法表现出明显的热启动效应：当训练量仅占总量的0.4%时，准确率就已超过12%，而索引基线的模型则低于6%。这表明视觉输入能更快速地从少量数据中学习，提供高效的学习信号，同时在整体性能上与基线方法持平。",
      "conclusion": "该研究的主要贡献是证明了最小视觉结构可以为中文语言建模提供稳健和高效的信号，为字符表示提供了新的视角，补充了传统的索引方法。学术上，这扩展了语言模型的输入方式，探索了视觉信息在语言处理中的潜力；在实际应用中，可能有助于开发更高效的中文处理系统。摘要未明确说明具体局限性或未来工作方向，但潜在发展可能包括优化视觉表示或扩展到其他类似语言。",
      "tags": [
        "Large Language Model",
        "Visual Tokens",
        "Chinese Language Modeling",
        "Low-Resolution Vision",
        "Grayscale Image Processing"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:37.298270Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09555",
    "title": "Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats",
    "authors": [
      "Manyi Zhang",
      "Ji-Fu Li",
      "Zhongao Sun",
      "Haoli Bai",
      "Hui-Ling Zhen",
      "Zhenhua Dong",
      "Xianzhi Yu"
    ],
    "abstract": "Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09555.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09555",
    "published": "2026-01-14T15:16:55Z",
    "updated": "2026-01-14T15:16:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文系统研究了大型语言模型在Microscaling浮点格式下的后训练量化，揭示了性能特性并提出了优化策略以减轻量化误差。",
      "motivation": "Microscaling Floating-Point（MXFP）是一种有前景的低精度格式，适用于大型语言模型，但现有后训练量化算法主要聚焦于整数量化，对MXFP格式下的适用性和行为研究不足。这一问题限制了LLM在资源受限环境中的高效部署。当前方法在MXFP下的探索缺失，导致量化性能未知，阻碍了低精度格式的广泛应用。因此，本工作旨在填补这一空白，通过系统性调查来指导PTQ在MXFP中的有效实施。",
      "method": "论文采用系统性调查方法，评估了超过7种后训练量化算法，涵盖15个评估基准和3个大型语言模型家族，重点分析MXFP8和MXFP4格式下的量化效果。关键创新包括探究MXFP格式与PTQ算法的兼容性，并引入简单的预缩放优化策略来减少MXFP4中的误差源。虽然摘要未明确说明具体数据集和模型名称，但研究涉及多模态LLMs，强调了量化敏感性主要来源于语言模型而非视觉编码器。",
      "result": "实验结果显示，MXFP8格式能实现接近无损的性能，而MXFP4导致显著的精度下降。PTQ在MXFP下的有效性高度依赖格式兼容性，某些算法范式更有效。性能趋势在不同模型家族和模态间高度一致，量化敏感性主要由语言模型主导。MXFP4中缩放因子是关键误差源，通过预缩放优化策略能显著减轻其影响，提升量化效果，为现有方法的适应提供了数据支撑。",
      "conclusion": "本研究的主要贡献是系统探索了MXFP格式下的后训练量化，填补了研究空白，提供了实用指南以适配现有PTQ方法。学术价值在于揭示了MXFP量化特性，实际应用有助于优化LLM部署效率。局限性在于MXFP4仍具挑战性，精度下降问题未完全解决。未来工作可围绕改进低精度量化算法和扩展更多模型家族展开。",
      "tags": [
        "Large Language Models",
        "Post-Training Quantization",
        "Microscaling Floating-Point",
        "Quantization Algorithms",
        "Low-Precision Formats"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:23.868239Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09536",
    "title": "Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning",
    "authors": [
      "Dongjie Cheng",
      "Yongqi Li",
      "Zhixin Ma",
      "Hongru Cai",
      "Yupeng Hu",
      "Wenjie Wang",
      "Liqiang Nie",
      "Wenjie Li"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09536.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09536",
    "published": "2026-01-14T14:57:33Z",
    "updated": "2026-01-14T14:57:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Omni-R1，通过生成中间图像实现统一多模态推理的生成式框架，并开发无需多模态标注的Omni-R1-Zero版本。",
      "motivation": "当前多模态大语言模型在多模态推理中取得进展，但现有方法多依赖单一任务特定模式，限制了在需要多样化推理技能（如图像区域放大或对象标记）的任务中的泛化能力。这导致模型适应性不足，无法高效处理广泛多模态任务。为解决这一问题，需要一种统一方法来整合不同推理能力，以提升模型的跨任务性能和效率。",
      "method": "研究提出统一生成式多模态推理范式，通过推理过程中动态生成中间图像来统一多种推理技能。具体实现为Omni-R1，采用两阶段监督微调与强化学习（SFT+RL）框架，结合感知对齐损失和感知奖励优化功能性图像生成。此外，Omni-R1-Zero版本通过从纯文本推理数据自举逐步可视化，消除了对多模态标注的需求，简化了训练过程并提高了可扩展性。",
      "result": "实证结果表明，Omni-R1在多种多模态任务上实现了统一的生成式推理，展现出良好的跨任务泛化能力。Omni-R1-Zero在平均性能上与Omni-R1相当甚至更优，表明在不依赖多模态标注的情况下也能取得高效结果。这些发现突显了生成式方法在减少数据需求的同时保持性能的优势，为多模态推理提供了可行的技术方向。",
      "conclusion": "本研究通过Omni-R1框架创新性地统一了多模态推理技能，利用生成式方法扩展了模型的推理能力。Omni-R1-Zero的成功强调了减少数据依赖的潜力，为领域提供了一种低成本高效解决方案，推动了多模态推理向更通用和实用方向发展。未来工作可进一步优化生成质量并扩展应用场景。",
      "tags": [
        "Multimodal Large Language Model",
        "Generative Reasoning",
        "Reinforcement Learning",
        "Supervised Fine-Tuning",
        "Bootstrapping"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:54.990695Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09531",
    "title": "Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server",
    "authors": [
      "Yue Yao",
      "Ruining Yang",
      "Tom Gedeon"
    ],
    "abstract": "We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09531.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09531",
    "published": "2026-01-14T14:55:14Z",
    "updated": "2026-01-14T14:55:14Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "该论文提出二分模式匹配算法（BMM），通过分层数据服务器优化训练集搜索，以减少无监督域适应中的领域差距，实现数据中心的优化。",
      "motivation": "研究背景是目标域可访问但实时数据标注不可行，需要从大规模数据服务器构建替代训练集。现有方法主要关注算法改进，而忽略了数据服务器结构的优化潜力，导致训练集可能不匹配目标域的模式（如语义簇），从而损害模型性能。本文旨在探索数据服务器优化，以直接弥补领域差距，提升模型训练效果。",
      "method": "论文引入分层数据服务器，模拟网络搜索引擎的层次结构，并提出二分模式匹配算法（BMM）来对齐源域和目标域的模式。该算法将源模式和目标模式视为二分图，通过匹配为每个目标模式在服务器数据树中找到最佳的源模式匹配，无论大小，确保一对一最优对齐，从而构建更有效的训练集。",
      "result": "实验在目标重识别（re-ID）和检测任务上进行，结果显示BMM构建的训练集与目标域的领域差距持续更小，相比现有训练集搜索算法，模型精度有显著提升。此外，BMM与伪标签等现有无监督域适应方法结合，进一步提高了性能，验证了其正交性和有效性。摘要未明确说明具体数据指标，但强调了性能改进。",
      "conclusion": "研究的主要贡献在于提出了一种数据中心的无监督域适应方法，通过优化训练集搜索减少领域差距，为域适应领域提供了新视角。BMM算法具有学术价值，可与模型中心方法互补，并在实际应用如视觉任务中提升模型性能。未来工作可探索更多任务集成和扩展应用场景，以验证其通用性。",
      "tags": [
        "Unsupervised Domain Adaptation",
        "Bipartite Matching",
        "Hierarchical Data Server",
        "Object Re-identification"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:46.114201Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09528",
    "title": "GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection",
    "authors": [
      "Alfio Spoto",
      "Rosario Leonardi",
      "Francesco Ragusa",
      "Giovanni Maria Farinella"
    ],
    "abstract": "Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09528.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09528",
    "published": "2026-01-14T14:52:20Z",
    "updated": "2026-01-14T14:52:20Z",
    "comment": "8 pages, accepted as a Short Paper at VISAPP 2026",
    "light_analysis": {
      "overview": "论文提出了GlovEgo-HOI数据集和GlovEgo-Net模型，通过结合合成数据和扩散过程的数据生成框架，增强工业第一人称人-物交互检测以解决数据稀缺问题。",
      "motivation": "研究动机源于工业第一人称人-物交互（EHOI）分析在安全监控中的重要性，但标注数据的稀缺阻碍了鲁棒模型的开发。工业环境中，现有方法因依赖大量真实标注数据而面临高昂成本，导致模型泛化能力不足和性能下降。摘要未明确说明具体现有方法的不足，但强调了数据不足的挑战，因此需要创新方法生成合成数据以弥补现实与合成的差距。",
      "method": "论文提出了一种数据生成框架，通过结合合成数据与基于扩散的过程，向真实图像添加逼真的个人防护装备（PPE），从而增强数据集。核心模型GlovEgo-Net集成了Glove-Head和Keypoint-Head模块，利用手部关键点信息来提升人-物交互检测的准确性。该方法的关键创新在于使用扩散模型生成高质量合成数据，并通过专门模块处理手部姿态，以应对工业场景中的复杂交互。",
      "result": "通过广泛的实验验证了所提数据生成框架和GlovEgo-Net模型的有效性，表明它们能显著改善工业EHOI检测性能。摘要未明确说明具体性能指标如准确率提升或效率改进，也未提供与基线方法的详细对比数据，但强调实验显示了方法的鲁棒性和实用性，支持其在工业安全应用中的潜力。",
      "conclusion": "论文的主要贡献是提出了GlovEgo-HOI数据集和GlovEgo-Net模型，有效解决了工业EHOI检测中的数据稀缺问题，推动了合成到现实差距的研究。这项研究具有重要的学术价值，促进了计算机视觉在工业安全领域的应用，并为后续研究提供了开源资源。潜在局限性可能包括生成数据的逼真度或模型对不同工业环境的泛化能力，未来工作可扩展到更多交互类型或优化技术细节。",
      "tags": [
        "Diffusion Models",
        "Synthetic-to-Real Transfer",
        "Human-Object Interaction Detection",
        "Keypoint Detection",
        "Egocentric Vision"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:09.805751Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09527",
    "title": "Private LLM Inference on Consumer Blackwell GPUs: A Practical Guide for Cost-Effective Local Deployment in SMEs",
    "authors": [
      "Jonathan Knoop",
      "Hendrik Holtmann"
    ],
    "abstract": "SMEs increasingly seek alternatives to cloud LLM APIs, which raise data privacy concerns. Dedicated cloud GPU instances offer improved privacy but with limited guarantees and ongoing costs, while professional on-premise hardware (A100, H100) remains prohibitively expensive. We present a systematic evaluation of NVIDIA's Blackwell consumer GPUs (RTX 5060 Ti, 5070 Ti, 5090) for production LLM inference, benchmarking four open-weight models (Qwen3-8B, Gemma3-12B, Gemma3-27B, GPT-OSS-20B) across 79 configurations spanning quantization formats (BF16, W4A16, NVFP4, MXFP4), context lengths (8k-64k), and three workloads: RAG, multi-LoRA agentic serving, and high-concurrency APIs. The RTX 5090 delivers 3.5-4.6x higher throughput than the 5060 Ti with 21x lower latency for RAG, but budget GPUs achieve the highest throughput-per-dollar for API workloads with sub-second latency. NVFP4 quantization provides 1.6x throughput over BF16 with 41% energy reduction and only 2-4% quality loss. Self-hosted inference costs $0.001-0.04 per million tokens (electricity only), which is 40-200x cheaper than budget-tier cloud APIs, with hardware breaking even in under four months at moderate volume (30M tokens/day). Our results show that consumer GPUs can reliably replace cloud inference for most SME workloads, except latency-critical long-context RAG, where high-end GPUs remain essential. We provide deployment guidance and release all benchmark data for reproducible SME-scale deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09527.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09527",
    "published": "2026-01-14T14:49:07Z",
    "updated": "2026-01-14T14:49:07Z",
    "comment": "15 pages, 18 tables, 7 figures. Includes link to GitHub repository and Docker image for reproducibility",
    "light_analysis": {
      "overview": "本文通过系统评估NVIDIA消费级Blackwell GPU在LLM推理中的性能，为中小企业提供了一种经济高效的本地部署方案。",
      "motivation": "中小企业（SMEs）越来越多地寻求云LLM API的替代方案，因为云服务引发了数据隐私担忧，而专用云GPU实例虽然隐私有所改善，但保证有限且成本持续，专业本地硬件（如A100、H100）价格过高，难以普及。因此，本研究旨在评估消费级Blackwell GPU是否能为SME提供既经济又隐私安全的本地LLM推理解决方案，以解决现有方法中成本与隐私之间的平衡问题。",
      "method": "论文采用系统性评估方法，测试了NVIDIA Blackwell消费级GPUs（RTX 5060 Ti、5070 Ti、5090）在生产环境中的LLM推理性能。评估覆盖了四个开源模型（Qwen3-8B、Gemma3-12B、Gemma3-27B、GPT-OSS-20B），共79种配置，包括不同量化格式（BF16、W4A16、NVFP4、MXFP4）、上下文长度（8k-64k）和三种工作负载：RAG、多LoRA智能体服务和API高并发。创新点在于全面量化了消费级GPU在多样化SME场景下的表现，为实际部署提供数据支撑。",
      "result": "实验结果表明，RTX 5090在RAG工作负载中比RTX 5060 Ti吞吐量高3.5-4.6倍，延迟降低21倍；NVFP4量化相比BF16格式使吞吐量提高1.6倍，能耗降低41%，质量损失仅为2-4%。自托管推理成本为每百万token $0.001-0.04（仅电费），比预算级云API便宜40-200倍，硬件成本在中等使用量（每天30M token）下可在四个月内回收。这些结果证明消费级GPU在大多数SME工作负载中可替代云推理。",
      "conclusion": "本研究的主要贡献是证明消费级Blackwell GPU能为中小企业提供经济高效的LLM推理本地部署方案，显著降低成本和提升隐私安全性，学术价值在于对消费级硬件在AI应用中的系统性评估，实际价值体现在提供部署指南和可复现数据。局限性在于延迟关键的长上下文RAG工作负载仍需高端GPU，未来工作可扩展更多模型和配置以进一步优化性能。",
      "tags": [
        "Consumer GPUs",
        "LLM Inference",
        "Quantization",
        "RAG",
        "Multi-LoRA"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:50.673631Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09524",
    "title": "Video Joint-Embedding Predictive Architectures for Facial Expression Recognition",
    "authors": [
      "Lennart Eing",
      "Cristina Luna-Jiménez",
      "Silvan Mertes",
      "Elisabeth André"
    ],
    "abstract": "This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09524.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09524",
    "published": "2026-01-14T14:48:11Z",
    "updated": "2026-01-14T14:48:11Z",
    "comment": "To appear in 2025 Proceedings of the 13th International Conference on Affective Computing and Intelligent Interaction (ACII), submitted to IEEE. \\c{opyright} 2025 IEEE",
    "light_analysis": {
      "overview": "本文提出将视频联合嵌入预测架构应用于面部表情识别，通过嵌入预测学习避免背景噪声，实现最先进性能。",
      "motivation": "该研究旨在改进面部表情识别，因为传统视频理解预训练方法依赖像素级重建，可能捕获不相关背景信息如颜色，影响模型性能。FER在人机交互和情感分析中至关重要，现有方法在泛化和效率上存在局限，V-JEPA通过嵌入预测聚焦语义特征，解决这些不足，提升识别准确性和鲁棒性。",
      "method": "论文采用视频联合嵌入预测架构，通过从非掩码区域嵌入预测掩码区域嵌入来预训练视频编码器，避免学习无关背景信息。使用预训练的V-JEPA编码器，在RAVDESS和CREMA-D数据集上训练浅层分类器进行微调，核心创新在于嵌入预测而非传统重建，优化特征提取过程。",
      "result": "实验结果显示，在RAVDESS数据集上达到最先进性能，在CREMA-D上优于所有其他基于视觉的方法，加权平均召回率提升1.48%。跨数据集评估证实了方法的强泛化能力，表明嵌入预测学习有效提升识别效果，与基线方法相比显著改进。",
      "conclusion": "结论表明V-JEPA在面部表情识别中有效，验证了基于嵌入的预训练方法的潜力。研究贡献在于应用新架构取得先进性能，学术价值为推进预训练策略，实际应用可增强情感分析系统。未来工作可探索更多数据集或扩展到其他视频任务，以进一步优化性能。",
      "tags": [
        "Video Joint-Embedding Predictive Architecture",
        "Facial Expression Recognition",
        "Masked Prediction",
        "Embedding Learning",
        "Pre-training"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:16.297961Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09522",
    "title": "Class Adaptive Conformal Training",
    "authors": [
      "Badr-Eddine Marani",
      "Julio Silva-Rodriguez",
      "Ismail Ben Ayed",
      "Maria Vakalopoulou",
      "Stergios Christodoulidis",
      "Jose Dolz"
    ],
    "abstract": "Deep neural networks have achieved remarkable success across a variety of tasks, yet they often suffer from unreliable probability estimates. As a result, they can be overconfident in their predictions. Conformal Prediction (CP) offers a principled framework for uncertainty quantification, yielding prediction sets with rigorous coverage guarantees. Existing conformal training methods optimize for overall set size, but shaping the prediction sets in a class-conditional manner is not straightforward and typically requires prior knowledge of the data distribution. In this work, we introduce Class Adaptive Conformal Training (CaCT), which formulates conformal training as an augmented Lagrangian optimization problem that adaptively learns to shape prediction sets class-conditionally without making any distributional assumptions. Experiments on multiple benchmark datasets, including standard and long-tailed image recognition as well as text classification, demonstrate that CaCT consistently outperforms prior conformal training methods, producing significantly smaller and more informative prediction sets while maintaining the desired coverage guarantees.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09522.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09522",
    "published": "2026-01-14T14:41:23Z",
    "updated": "2026-01-14T14:41:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "CaCT提出了一种新颖的自适应类条件共形训练方法，通过增广拉格朗日优化学习预测集，无需分布假设。",
      "motivation": "深度神经网络在多种任务中表现出色，但概率估计不可靠，常导致过度自信预测，影响模型可信度。共形预测（Conformal Prediction）提供不确定性量化的原则框架，能生成具有严格覆盖率保证的预测集。然而，现有共形训练方法主要优化整体集合大小，类条件地塑造预测集不直接，通常需要数据分布的先验知识，限制了其适应性和实用性。因此，本研究旨在开发一种自适应方法，无需假设分布就能改进类条件预测集，以增强不确定性量化和预测可靠性。",
      "method": "本研究提出了类自适应共形训练（CaCT），将共形训练表述为增广拉格朗日优化问题。关键创新在于自适应地学习类条件预测集，不依赖任何数据分布假设。该方法通过优化框架动态调整预测集形状，利用增广拉格朗日技术处理约束，平衡覆盖率和集合大小，适应不同类别数据特性。具体实现涉及在训练过程中集成优化算法，以类条件方式优化预测集，从而提升不确定性量化的精确性。",
      "result": "实验在多个基准数据集上进行，包括标准图像识别、长尾图像识别和文本分类任务。结果显示，CaCT在性能上一致优于先前的共形训练方法。具体而言，它生成的预测集显著更小且信息更丰富，同时保持了所需的覆盖率保证。与基线方法相比，CaCT在预测集质量和不确定性量化方面表现更优，证实了其有效性和适应性，但摘要未明确说明具体准确率或效率数据。",
      "conclusion": "CaCT的主要贡献是提出了一种无分布假设的类条件共形训练方法，通过增广拉格朗日优化自适应地学习预测集。这项研究在学术上推动了共形预测和不确定性量化领域的发展，提供了更灵活的训练框架。实际应用中，CaCT可增强深度神经网络的可信度，尤其在需要精确不确定性估计的场景。未来工作可探索该方法在更复杂数据分布或其他任务中的扩展，并进一步优化性能。",
      "tags": [
        "Conformal Prediction",
        "Uncertainty Quantification",
        "Augmented Lagrangian Optimization",
        "Class-Conditional Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:38.547380Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09515",
    "title": "SERM: Self-Evolving Relevance Model with Agent-Driven Learning from Massive Query Streams",
    "authors": [
      "Chenglong Wang",
      "Canjia Li",
      "Xingzhao Zhu",
      "Yifu Huo",
      "Huiyu Wang",
      "Weixiong Lin",
      "Yun Yang",
      "Qiaozhi He",
      "Tianhua Zhou",
      "Xiaojia Chang",
      "Jingbo Zhu",
      "Tong Xiao"
    ],
    "abstract": "Due to the dynamically evolving nature of real-world query streams, relevance models struggle to generalize to practical search scenarios. A sophisticated solution is self-evolution techniques. However, in large-scale industrial settings with massive query streams, this technique faces two challenges: (1) informative samples are often sparse and difficult to identify, and (2) pseudo-labels generated by the current model could be unreliable. To address these challenges, in this work, we propose a Self-Evolving Relevance Model approach (SERM), which comprises two complementary multi-agent modules: a multi-agent sample miner, designed to detect distributional shifts and identify informative training samples, and a multi-agent relevance annotator, which provides reliable labels through a two-level agreement framework. We evaluate SERM in a large-scale industrial setting, which serves billions of user requests daily. Experimental results demonstrate that SERM can achieve significant performance gains through iterative self-evolution, as validated by extensive offline multilingual evaluations and online testing.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09515.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09515",
    "published": "2026-01-14T14:31:16Z",
    "updated": "2026-01-14T14:31:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "SERM通过多代理样本挖掘和标注机制，实现相关性模型在动态查询流中的自我演化，以解决样本稀疏和伪标签不可靠的挑战。",
      "motivation": "研究动机源于真实世界查询流具有动态演化特性，导致相关性模型难以在实际搜索场景中有效泛化。现有自我演化技术在大规模工业应用中面临关键问题：信息性样本稀疏且难以自动识别，以及当前模型生成的伪标签可靠性低，这降低了搜索系统的性能和适应性，凸显了改进方法的必要性。",
      "method": "研究方法提出SERM模型，包括两个互补多代理模块：多代理样本挖掘器检测分布偏移并识别信息性训练样本，多代理相关性标注器通过两级同意框架提供可靠标签，实现代理驱动学习和迭代自我演化；该方法在大规模工业设置中评估，服务数十亿用户请求，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果表明，SERM通过迭代自我演化实现显著性能提升，在大规模工业环境中得到验证；广泛的离线多语言评估和在线测试证实其有效性，与基线方法对比显示改进，但摘要未明确说明具体性能指标如准确率提升数值。",
      "conclusion": "论文主要贡献是提出SERM方法，通过多代理机制解决自我演化中的样本稀疏和伪标签不可靠问题，学术价值在于增强模型对动态环境的适应性，实际应用价值在于提升大规模工业搜索系统的性能；未来工作方向可能包括扩展到其他领域或进一步优化代理框架，但摘要未明确说明局限性。",
      "tags": [
        "Self-Evolving Model",
        "Multi-Agent Learning",
        "Relevance Learning",
        "Distribution Shift Detection",
        "Pseudo-Labeling"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:25.559478Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09504",
    "title": "MVSS: A Unified Framework for Multi-View Structured Survey Generation",
    "authors": [
      "Yinqi Liu",
      "Yueqi Zhu",
      "Yongkang Zhang",
      "Xinfeng Li",
      "Feiran Liu",
      "Yufei Sun",
      "Xin Wang",
      "Renzhao Liang",
      "Yidong Wang",
      "Cunxiang Wang"
    ],
    "abstract": "Scientific surveys require not only summarizing large bodies of literature, but also organizing them into clear and coherent conceptual structures. Existing automatic survey generation methods typically focus on linear text generation and struggle to explicitly model hierarchical relations among research topics and structured methodological comparisons, resulting in gaps in structural organization compared to expert-written surveys. We propose MVSS, a multi-view structured survey generation framework that jointly generates and aligns citation-grounded hierarchical trees, structured comparison tables, and survey text. MVSS follows a structure-first paradigm: it first constructs a conceptual tree of the research domain, then generates comparison tables constrained by the tree, and finally uses both as structural constraints for text generation. This enables complementary multi-view representations across structure, comparison, and narrative. We introduce an evaluation framework assessing structural quality, comparative completeness, and citation fidelity. Experiments on 76 computer science topics show MVSS outperforms existing methods in organization and evidence grounding, achieving performance comparable to expert surveys.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09504.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09504",
    "published": "2026-01-14T14:11:39Z",
    "updated": "2026-01-14T14:11:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "MVSS是一个统一的多视图结构化综述生成框架，通过联合生成层次树、比较表和文本，实现了结构优先的科学综述自动生成。",
      "motivation": "科学综述需要总结大量文献并组织成清晰的概念结构，但现有自动生成方法主要专注于线性文本生成，难以显式建模研究主题的层次关系或结构化比较，导致在结构组织上与专家撰写的综述存在显著差距。这限制了自动综述的实际应用价值，因此迫切需要一种新方法来弥补这一不足，提升综述的整体质量和可读性。",
      "method": "MVSS框架采用结构优先的范式，首先构建研究领域的层次树以捕获主题间的概念关系，然后基于该树生成结构化比较表，最后将树和表作为约束条件用于生成综述文本。这种方法联合生成并对齐了引用基础的层次树、比较表和文本，实现了结构、比较和叙述之间的互补多视图表示，增强了综述的组织性和连贯性。",
      "result": "在76个计算机科学主题上的实验显示，MVSS在结构组织和证据基础方面优于现有方法，其性能接近专家撰写的综述。评估框架涵盖了结构质量、比较完整性和引用保真度等指标，MVSS在这些方面均表现出色，尽管摘要未提供具体数值，但对比基线方法显示出明显的改进优势。",
      "conclusion": "论文主要贡献是提出了MVSS框架，通过多视图结构化生成提升科学综述的质量，学术价值在于引入结构优先的生成范式和综合评估方法，实际应用价值在于自动化综述写作以辅助科研。未来工作可扩展至更多学科领域或优化模型细节，摘要未明确说明具体局限性。",
      "tags": [
        "Multi-View Framework",
        "Hierarchical Tree Generation",
        "Structured Comparison",
        "Natural Language Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:44.195294Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09503",
    "title": "What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding",
    "authors": [
      "Siyuan Liu",
      "Hongbang Yuan",
      "Xinze Li",
      "Ziyue Zhu",
      "Yixin Cao",
      "Yu-Gang Jiang"
    ],
    "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09503.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09503",
    "published": "2026-01-14T14:09:11Z",
    "updated": "2026-01-14T14:09:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 Task-to-Quiz (T2Q) 范式，用于评估大型语言模型（LLM）代理的环境理解能力，通过解耦任务执行与世界状态理解。",
      "motivation": "大型语言模型（LLM）代理在复杂决策和工具使用任务中表现出色，但其在不同环境中的泛化能力研究不足。现有评估方法主要依赖轨迹度量来评估任务成功，却未能检测代理是否拥有基于环境、可转移的世界模型。这导致无法准确衡量代理的真正理解水平，影响其实际应用和进一步发展。因此，迫切需要新的评估范式来填补这一研究空白。",
      "method": "研究提出 Task-to-Quiz (T2Q) 范式，这是一个确定性和自动化的评估方法，旨在将任务执行与环境理解分离。通过在 T2QBench 中实例化该范式，创建了一个包含 30 个环境和 1,967 个基于难度的 grounded QA 对的评估套件。关键创新在于使用问答（QA）形式直接评估代理对世界状态的理解，避免依赖任务完成度，从而实现更客观的度量。",
      "result": "实验结果表明，任务成功往往不能有效代理环境理解，显示现有评估指标存在缺陷。具体地，当前记忆机制未能帮助代理获得基于环境的模型。此外，主动探索和细粒度状态表示被识别为主要瓶颈。这些发现基于 T2QBench 的广泛实验，为改进代理设计提供了实证依据。",
      "conclusion": "论文的主要贡献是引入 T2Q 范式，推动了 LLM 代理环境理解的评估研究，有助于开发更通用的自主代理。研究识别了关键瓶颈，如主动探索和状态表示，为未来工作指明了方向。然而，摘要未明确说明具体应用场景或进一步验证，这可能是潜在的局限性。",
      "tags": [
        "Large Language Model (LLM) Agents",
        "Task-to-Quiz (T2Q)",
        "Environment Understanding",
        "Memory Mechanisms",
        "Evaluation Paradigm"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:53.338176Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09499",
    "title": "V-DPM: 4D Video Reconstruction with Dynamic Point Maps",
    "authors": [
      "Edgar Sucar",
      "Eldar Insafutdinov",
      "Zihang Lai",
      "Andrea Vedaldi"
    ],
    "abstract": "Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09499.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09499",
    "published": "2026-01-14T14:03:42Z",
    "updated": "2026-01-14T14:03:42Z",
    "comment": "Project page: https://www.robots.ox.ac.uk/~vgg/research/vdpm/",
    "light_analysis": {
      "overview": "提出V-DPM方法，将动态点图扩展到视频输入，实现动态场景的4D重建和全3D运动恢复。",
      "motivation": "该研究旨在解决动态场景的3D和4D重建问题，现有方法如DUSt3R的静态点图和动态点图仅限于图像对，处理多视图时需后处理优化，效率低下且难以适应视频数据。研究动机是扩展动态点图到视频应用，以更高效地重建动态内容，弥补现有方法在实时性和精度上的不足。",
      "method": "论文提出V-DPM方法，通过为视频输入制定动态点图表示，最大化表示能力并促进神经预测，重用VGGT等预训练模型。具体实现基于VGGT，利用少量合成数据进行微调，使其能预测动态场景的4D重建，关键创新点包括优化点图结构以适应动态内容和提升模型泛化性。",
      "result": "实验结果显示，V-DPM在动态场景的3D和4D重建中达到最先进性能，优于基线方法如P3，不仅能恢复动态深度，还能精确重建每个点的完整3D运动，显著提升了重建质量和场景理解的完整性。",
      "conclusion": "研究的主要贡献是V-DPM方法成功扩展了动态点图到视频领域，实现了高效且精确的动态重建，学术上推动了4D重建技术的发展，实际应用上可支持视频分析、虚拟现实等场景，未来工作可能包括优化复杂动态场景的处理和提升计算效率。",
      "tags": [
        "Dynamic Point Maps",
        "Video Reconstruction",
        "3D Motion Estimation",
        "Neural Prediction",
        "VGGT"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:56.437186Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09497",
    "title": "Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity",
    "authors": [
      "Ritabrata Chakraborty",
      "Hrishit Mitra",
      "Shivakumara Palaiahnakote",
      "Umapada Pal"
    ],
    "abstract": "Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \\href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09497.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09497",
    "published": "2026-01-14T14:03:11Z",
    "updated": "2026-01-14T14:03:11Z",
    "comment": "15 pages, 4 figures, 6 tables",
    "light_analysis": {
      "overview": "该论文通过分析设置特异性，提供了跨数据集目标检测的原则性描述和实用评估指南，核心贡献在于深入解析域偏移对检测器泛化的影响。",
      "motivation": "目标检测器在训练数据集上表现良好，但在不同基准测试中性能急剧下降，这限制了其在实际应用中的泛化能力。现有方法可能未充分考虑数据集设置类型（如多样化场景与窄环境）的影响，导致跨数据集转移时性能不稳定。该研究旨在解决在域特异性设置下的跨数据集目标检测问题，以揭示影响泛化的关键因素，并为改进检测器评估提供理论基础。",
      "method": "研究将基准数据集分组为设置无关数据集（包含多样化日常场景）和设置特定数据集（与窄环境相关），使用标准检测器家族评估所有训练-测试对。为分离域偏移和标签不匹配，引入开放标签协议，利用 CLIP 相似性将预测类别映射到最近的目标标签，以分析转移过程中的性能变化。关键创新点在于基于设置特异性进行结构化评估，并结合 CLIP 进行语义对齐。",
      "result": "实验结果显示，跨数据集目标检测在设置类型内转移相对稳定，但跨类型转移性能大幅下降且常不对称，从特定源到无关目标的转移崩溃最严重，开放标签对齐后仍存在。开放标签评估带来一致但有限的增益，提升幅度受限于域偏移，许多纠正案例对应于图像证据支持的语义近失。与基线方法对比，这表明域偏移是影响泛化的主要挑战。",
      "conclusion": "该论文的主要贡献是对跨数据集目标检测在设置特异性下的原则性描述，为评估检测器在分布偏移下提供了实用指导。研究深化了对域偏移和标签不匹配影响的理解，有助于提升检测器泛化能力，实际应用包括改进跨域检测评估框架。局限性可能涉及数据集范围的扩展，未来工作可探索更复杂的域适应技术或结合多模态信息。",
      "tags": [
        "Cross-Dataset Object Detection",
        "Domain Specificity",
        "CLIP",
        "Open-label Alignment",
        "Generalization"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:16.339523Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09495",
    "title": "Parallelizable memory recurrent units",
    "authors": [
      "Florent De Geeter",
      "Gaspard Lambrechts",
      "Damien Ernst",
      "Guillaume Drion"
    ],
    "abstract": "With the emergence of massively parallel processing units, parallelization has become a desirable property for new sequence models. The ability to parallelize the processing of sequences with respect to the sequence length during training is one of the main factors behind the uprising of the Transformer architecture. However, Transformers lack efficiency at sequence generation, as they need to reprocess all past timesteps at every generation step. Recently, state-space models (SSMs) emerged as a more efficient alternative. These new kinds of recurrent neural networks (RNNs) keep the efficient update of the RNNs while gaining parallelization by getting rid of nonlinear dynamics (or recurrence). SSMs can reach state-of-the art performance through the efficient training of potentially very large networks, but still suffer from limited representation capabilities. In particular, SSMs cannot exhibit persistent memory, or the capacity of retaining information for an infinite duration, because of their monostability. In this paper, we introduce a new family of RNNs, the memory recurrent units (MRUs), that combine the persistent memory capabilities of nonlinear RNNs with the parallelizable computations of SSMs. These units leverage multistability as a source of persistent memory, while getting rid of transient dynamics for efficient computations. We then derive a specific implementation as proof-of-concept: the bistable memory recurrent unit (BMRU). This new RNN is compatible with the parallel scan algorithm. We show that BMRU achieves good results in tasks with long-term dependencies, and can be combined with state-space models to create hybrid networks that are parallelizable and have transient dynamics as well as persistent memory.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09495.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09495",
    "published": "2026-01-14T14:01:11Z",
    "updated": "2026-01-14T14:01:11Z",
    "comment": "19 pages, 12 figures. This work has been the subject of a patent application (Number: EP26151077). This work has been submitted to the IEEE for possible publication",
    "light_analysis": {
      "overview": "论文提出了记忆递归单元（MRUs），结合了非线性RNN的持久记忆和状态空间模型的并行计算能力，特别引入了双稳态记忆递归单元（BMRU）作为具体实现。",
      "motivation": "随着大规模并行处理单元的普及，序列模型的并行化成为关键需求。Transformers在训练时可并行化序列长度处理，但在序列生成时效率低下，需要重新处理所有过去时间步。状态空间模型（SSMs）作为高效替代出现，但它们由于单稳态性缺乏持久记忆能力，限制了在长期依赖任务中的表示能力。因此，本研究旨在解决现有方法在结合高效计算和强大记忆能力方面的不足。",
      "method": "论文引入了一个新的RNN家族，称为记忆递归单元（MRUs），它利用多稳态性作为持久记忆的来源，同时消除瞬态动态以实现高效计算。核心实现是双稳态记忆递归单元（BMRU），该模型兼容并行扫描算法，支持训练时的并行化处理。摘要未明确说明具体数据集或详细架构，但强调MRUs的设计原则和BMRU的技术特色，作为概念验证。",
      "result": "论文显示，BMRU在具有长期依赖的任务中取得了良好结果。它可以与状态空间模型结合形成可并行化的混合网络，这些网络同时具备瞬态动态和持久记忆能力。摘要未提供具体性能指标如准确率提升数据，但强调了相对于SSMs的改进，特别是在长期记忆任务中的有效性，并提到与基线方法的对比展示了好结果。",
      "conclusion": "论文的主要贡献是提出了记忆递归单元（MRUs）和具体实现BMRU，成功结合了非线性RNN的持久记忆和SSMs的并行计算优势。这增强了序列模型的表示能力和训练效率，对需要处理长期依赖的AI应用具有学术价值和实际潜力。未来工作可能包括进一步优化模型设计和扩展到更广泛的任务，以克服潜在局限性。",
      "tags": [
        "Recurrent Neural Networks",
        "State-Space Models",
        "Parallel Scan Algorithm",
        "Persistent Memory",
        "Multistability"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:23.347549Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09491",
    "title": "Deep Operator Networks for Surrogate Modeling of Cyclic Adsorption Processes with Varying Initial Conditions",
    "authors": [
      "Beatrice Ceccanti",
      "Mattia Galanti",
      "Ivo Roghair",
      "Martin van Sint Annaland"
    ],
    "abstract": "Deep Operator Networks are emerging as fundamental tools among various neural network types to learn mappings between function spaces, and have recently gained attention due to their ability to approximate nonlinear operators. In particular, DeepONets offer a natural formulation for PDE solving, since the solution of a partial differential equation can be interpreted as an operator mapping an initial condition to its corresponding solution field. In this work, we applied DeepONets in the context of process modeling for adsorption technologies, to assess their feasibility as surrogates for cyclic adsorption process simulation and optimization. The goal is to accelerate convergence of cyclic processes such as Temperature-Vacuum Swing Adsorption (TVSA), which require repeated solution of transient PDEs, which are computationally expensive. Since each step of a cyclic adsorption process starts from the final state of the preceding step, effective surrogate modeling requires generalization across a wide range of initial conditions. The governing equations exhibit steep traveling fronts, providing a demanding benchmark for operator learning. To evaluate functional generalization under these conditions, we construct a mixed training dataset composed of heterogeneous initial conditions and train DeepONets to approximate the corresponding solution operators. The trained models are then tested on initial conditions outside the parameter ranges used during training, as well as on completely unseen functional forms. The results demonstrate accurate predictions both within and beyond the training distribution, highlighting DeepONets as potential efficient surrogates for accelerating cyclic adsorption simulations and optimization workflows.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09491.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09491",
    "published": "2026-01-14T13:56:25Z",
    "updated": "2026-01-14T13:56:25Z",
    "comment": "36 pages, 11 figures",
    "light_analysis": {
      "overview": "本研究应用深度算子网络（DeepONets）作为循环吸附过程的代理模型，以加速模拟和优化。",
      "motivation": "循环吸附过程如温度-真空摇摆吸附（TVSA）需要重复求解计算昂贵的瞬态偏微分方程（PDE），导致模拟和优化效率低下。现有方法在处理频繁变化的初始条件时泛化能力不足，无法有效支持连续循环过程。因此，开发高效代理模型以加速收敛至关重要，DeepONets 作为一种能近似非线性算子的工具，为解决此问题提供了新途径。",
      "method": "研究采用深度算子网络（DeepONets）来学习从初始条件到解场的算子映射。通过构建包含异构初始条件的混合训练数据集，训练 DeepONets 以近似循环吸附过程的偏微分方程解算子。关键创新在于处理具有陡峭传播前沿的复杂 PDE 系统，并确保模型在广泛初始条件下的泛化能力。摘要未明确说明具体数据集和模型架构细节。",
      "result": "实验结果显示，训练后的 DeepONets 模型在训练分布内外的初始条件下均能做出准确预测，包括超出参数范围和未见过的函数形式。这表明模型具有良好的函数泛化能力，可作为高效的代理模型加速循环吸附模拟和优化工作流。尽管摘要未提供具体的性能指标数据，但结果验证了模型作为潜在加速工具的可行性。",
      "conclusion": "本研究的主要贡献是验证了深度算子网络（DeepONets）在循环吸附过程代理建模中的有效性，展示了其能学习复杂 PDE 的解算子并在广泛初始条件下准确预测。这为加速工业中的循环吸附模拟和优化提供了重要工具，具有实际应用价值。未来工作可扩展到其他循环过程或进一步优化模型泛化能力，以应对更复杂的场景。",
      "tags": [
        "Deep Operator Networks",
        "PDE Solving",
        "Surrogate Modeling",
        "Cyclic Processes",
        "Adsorption Technology"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:50.216381Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09487",
    "title": "SlidesGen-Bench: Evaluating Slides Generation via Computational and Quantitative Metrics",
    "authors": [
      "Yunqiao Yang",
      "Wenbo Li",
      "Houxing Ren",
      "Zimu Lu",
      "Ke Wang",
      "Zhiyuan Huang",
      "Zhuofan Zong",
      "Mingjie Zhan",
      "Hongsheng Li"
    ],
    "abstract": "The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments. In this paper, we introduce SlidesGen-Bench, a benchmark designed to evaluate slide generation through a lens of three core principles: universality, quantification, and reliability. First, to establish a unified evaluation framework, we ground our analysis in the visual domain, treating terminal outputs as renderings to remain agnostic to the underlying generation method. Second, we propose a computational approach that quantitatively assesses slides across three distinct dimensions - Content, Aesthetics, and Editability - offering reproducible metrics where prior works relied on subjective or reference-dependent proxies. Finally, to ensure high correlation with human preference, we construct the Slides-Align1.5k dataset, a human preference aligned dataset covering slides from nine mainstream generation systems across seven scenarios. Our experiments demonstrate that SlidesGen-Bench achieves a higher degree of alignment with human judgment than existing evaluation pipelines. Our code and data are available at https://github.com/YunqiaoYang/SlidesGen-Bench.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09487.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09487",
    "published": "2026-01-14T13:50:30Z",
    "updated": "2026-01-14T13:50:30Z",
    "comment": "37 pages, 34 figures",
    "light_analysis": {
      "overview": "该论文提出了SlidesGen-Bench基准，通过统一视觉框架、计算性评估维度和人类对齐数据集，创新地评估自动幻灯片生成系统的性能，解决了现有评估方法的主观性和不可比性问题。",
      "motivation": "随着大型语言模型的发展，自动幻灯片生成系统呈现多样化范式，但现有评估方法面临挑战，常难以在不同架构间提供可比性评分，或依赖未经校准的主观判断，这导致系统性能难以客观比较和改进。因此，建立一个统一、量化、可靠的评估标准对于推动生成技术进步至关重要，本文旨在解决这一实际问题，弥补现有协议的不足。",
      "method": "论文提出SlidesGen-Bench基准，基于universality、quantification、reliability三个原则。首先，采用视觉域统一框架，将幻灯片输出视为渲染，保持与具体生成方法无关。其次，设计计算性评估方法，从内容、美学和可编辑性三个维度进行定量评估，提供可复制的量化指标，替代了传统主观或参考依赖的代理。此外，构建Slides-Align1.5k数据集，包含九个主流生成系统在七个场景下的幻灯片，用于对齐人类偏好，确保评估的可靠性。",
      "result": "实验表明，SlidesGen-Bench在评估自动幻灯片生成系统时，比现有评估管道更与人类判断对齐，具体性能提升未在摘要中明确量化。该基准通过量化维度和人类对齐数据集，提供了更高的客观性和可比性，但摘要未给出具体的准确率或效率改进数据，仅强调了在人类偏好对齐方面的优势。",
      "conclusion": "论文的主要贡献是SlidesGen-Bench基准，它为自动幻灯片生成系统提供了一个统一、量化、可靠的评估框架，解决了现有方法的主观性和不可比性问题。学术价值在于推动了评估标准的规范化，实际应用价值在于帮助开发者和研究者优化系统性能。未来工作可能涉及扩展数据集或应用于更多场景，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Slide Generation",
        "Evaluation Benchmark",
        "Computational Metrics",
        "Human Preference Alignment",
        "Visual Rendering"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:22.946517Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09474",
    "title": "Terminally constrained flow-based generative models from an optimal control perspective",
    "authors": [
      "Weiguo Gao",
      "Ming Li",
      "Qianxiao Li"
    ],
    "abstract": "We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09474.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09474",
    "published": "2026-01-14T13:32:15Z",
    "updated": "2026-01-14T13:32:15Z",
    "comment": "59 pages, 9 figures",
    "light_analysis": {
      "overview": "本研究提出TOCFlow方法，通过最优控制理论提升预训练流模型在终端约束下的采样性能，实现约束满足和生成质量的双重优化。",
      "motivation": "论文关注从终端约束分布采样的实际问题，在科学计算任务中，如Darcy流、约束轨迹规划和湍流快照生成，需要处理等式、不等式和全局统计约束。现有方法如欧几里得指导和投影基线可能在几何一致性上不足，导致约束满足不佳或计算效率低下。因此，研究旨在通过最优控制视角提供一种更有效的采样方法，解决这些不足并扩展流模型的应用范围。",
      "method": "论文提出Terminal Optimal Control with Flow-based models (TOCFlow)算法，基于最优控制理论处理终端约束。理论部分通过Hamilton-Jacobi-Bellman方程刻画值函数，导出最优反馈控制。算法在终端共动框架中求解，生成Riemannian梯度上的闭形式标量阻尼因子，捕捉二阶曲率效应而无需矩阵求逆。这匹配了Gauss-Newton更新的几何一致性，计算成本与标准梯度指导相当，结合了预训练流模型的特点。",
      "result": "在三个高维科学任务上评估TOCFlow：Darcy流、约束轨迹规划和具有Kolmogorov谱缩放的湍流快照生成。实验结果显示，TOCFlow在约束满足上优于欧几里得指导和投影基线，具体性能提升摘要未明确说明数据，但表明在所有设置中都有改进。同时，TOCFlow保持了参考模型的生成质量，验证了方法的有效性和鲁棒性。",
      "conclusion": "论文的主要贡献是开发TOCFlow方法，将最优控制与流模型结合，实现高效且几何一致的终端约束采样。学术上，丰富了流模型和最优控制交叉领域的研究，提供了理论分析和算法实现。实际上，为高维科学计算任务提供了实用的采样工具，具有潜在的应用价值。未来工作可扩展至更多约束类型或实际场景，进一步提升算法适应性。",
      "tags": [
        "Flow-based Generative Models",
        "Optimal Control",
        "Geometric Guidance",
        "Riemannian Gradient",
        "Wasserstein Projection"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:44.654629Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09473",
    "title": "SimMerge: Learning to Select Merge Operators from Similarity Signals",
    "authors": [
      "Oliver Bolton",
      "Aakanksha",
      "Arash Ahmadian",
      "Sara Hooker",
      "Marzieh Fadaee",
      "Beyza Ermis"
    ],
    "abstract": "Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \\simmerge{}, \\emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \\simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \\simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09473.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09473",
    "published": "2026-01-14T13:30:00Z",
    "updated": "2026-01-14T13:30:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "SimMerge提出一种基于相似性信号的预测性合并选择方法，通过计算模型间的功能性和结构性特征，高效选择最佳合并操作符、模型子集和合并顺序，消除昂贵的合并与评估循环。",
      "motivation": "模型合并作为整合多个大语言模型（LLM）的技术，在多任务训练中具有重要价值。然而，在实际应用中，选择正确的合并操作符、模型和合并顺序面临挑战，通常需要运行昂贵的合并与评估搜索，效率低下。现有方法在模型目录大且评估预算紧张时难以扩展，因此需要一种更高效且任务无关的合并选择策略来解决这些不足。",
      "method": "SimMerge使用少量无标签探针计算模型间的功能性和结构性特征，作为相似性信号来预测给定2-way合并的性能。基于这些预测，自动选择最佳合并操作符、模型子集和合并顺序。关键创新点包括任务无关的预测机制、可扩展设计以及引入bandit变体以支持动态添加新任务、模型和操作符，适用于不同参数规模的模型如7B和111B的LLMs。",
      "result": "实验显示，在7B-parameter LLMs的2-way合并中，SimMerge超越了标准合并操作符的性能。该方法能推广到多路合并和111B-parameter LLM合并，且无需重新训练，表明其有效性和可扩展性。Bandit变体支持在运行时动态添加元素。与基线合并与评估方法相比，SimMerge避免了昂贵循环，显著提高了效率，证明了在有限评估预算下实现高效模型组合的潜力。",
      "conclusion": "SimMerge的主要贡献是提出了一种基于相似性信号的合并选择方法，减少了对昂贵合并与评估的依赖，推动了高效且可扩展的模型组合。其学术价值在于验证了预测性方法在模型合并中的可行性，实际应用价值在于支持大规模LLM开发和集成，特别是在资源受限场景下。未来工作可能包括扩展到更多模型类型和优化预测机制，摘要未明确说明局限性。",
      "tags": [
        "Model Merging",
        "Large Language Models",
        "Similarity Signals",
        "Merge Operators",
        "Bandit Algorithms"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:55.249120Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09469",
    "title": "FairGU: Fairness-aware Graph Unlearning in Social Network",
    "authors": [
      "Renqiang Luo",
      "Yongshuai Yang",
      "Huafei Huang",
      "Qing Qing",
      "Mingliang Hou",
      "Ziqi Xu",
      "Yi Yu",
      "Jingjing Zhou",
      "Feng Xia"
    ],
    "abstract": "Graph unlearning has emerged as a critical mechanism for supporting sustainable and privacy-preserving social networks, enabling models to remove the influence of deleted nodes and thereby better safeguard user information. However, we observe that existing graph unlearning techniques insufficiently protect sensitive attributes, often leading to degraded algorithmic fairness compared with traditional graph learning methods. To address this gap, we introduce FairGU, a fairness-aware graph unlearning framework designed to preserve both utility and fairness during the unlearning process. FairGU integrates a dedicated fairness-aware module with effective data protection strategies, ensuring that sensitive attributes are neither inadvertently amplified nor structurally exposed when nodes are removed. Through extensive experiments on multiple real-world datasets, we demonstrate that FairGU consistently outperforms state-of-the-art graph unlearning methods and fairness-enhanced graph learning baselines in terms of both accuracy and fairness metrics. Our findings highlight a previously overlooked risk in current unlearning practices and establish FairGU as a robust and equitable solution for the next generation of socially sustainable networked systems. The codes are available at https://github.com/LuoRenqiang/FairGU.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09469.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09469",
    "published": "2026-01-14T13:21:39Z",
    "updated": "2026-01-14T13:21:39Z",
    "comment": "9 pages, 2 figs, WWW 2026 accepted",
    "light_analysis": {
      "overview": "FairGU是一个公平感知的图遗忘框架，旨在在社交网络中移除节点时同时保持模型效用和算法公平性。",
      "motivation": "图遗忘技术对于构建可持续和隐私保护的社交网络至关重要，因为它能移除已删除节点的影响以保护用户信息。然而，现有图遗忘方法在保护敏感属性方面存在不足，常导致算法公平性下降，相比传统图学习方法更差。这个问题的重要性在于，公平性在社交网络应用中至关重要，但现有技术未充分考虑，造成潜在风险。因此，研究FairGU旨在填补这一空白，确保在遗忘过程中不损害公平性。",
      "method": "FairGU框架通过集成一个专门的公平感知模块和有效的数据保护策略来实现公平性保持。关键创新在于，在节点移除过程中，确保敏感属性不会被无意放大或结构性地暴露。该框架旨在在遗忘过程中同时维护模型效用和算法公平性，摘要未明确说明具体的模型架构或使用的数据集，但强调了模块化集成和保护策略的设计，以应对公平性挑战。",
      "result": "通过在多个真实世界数据集上的广泛实验，FairGU在准确性和公平性指标上均一致优于最先进的图遗忘方法以及公平性增强的图学习基线。实验结果表明，FairGU能够有效提升公平性而不损害模型性能，显示出其在平衡效用和公平性方面的优越性。具体性能数据摘要未提供，但对比显示了其在两方面均优于现有方法。",
      "conclusion": "本研究揭示了当前图遗忘实践中被忽视的公平性风险，并提出了FairGU作为一个稳健且公平的解决方案。其主要贡献在于将公平性集成到图遗忘框架中，为下一代社交可持续网络系统提供了重要参考。研究具有学术价值，推动了图学习和公平性研究的交叉，实际应用价值在于改善社交网络的隐私和公平性保护。未来工作方向摘要未明确说明，可能涉及进一步优化和扩展。",
      "tags": [
        "Graph Unlearning",
        "Fairness-aware Learning",
        "Social Network Analysis",
        "Algorithmic Fairness"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:56.108460Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09467",
    "title": "Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting",
    "authors": [
      "Tianye Li",
      "Qi Liu",
      "Hao Li",
      "Lei Chen",
      "Wencong Cheng",
      "Fei Zheng",
      "Xiangao Xia",
      "Ya Wang",
      "Gang Huang",
      "Weiwei Wang",
      "Xuan Tong",
      "Ziqing Zu",
      "Yi Fang",
      "Shenming Fu",
      "Jiang Jiang",
      "Haochen Li",
      "Mingxing Li",
      "Jiangjiang Xia"
    ],
    "abstract": "Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09467.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09467",
    "published": "2026-01-14T13:20:17Z",
    "updated": "2026-01-14T13:20:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Searth Transformer架构，结合地球物理先验和Relay Autoregressive微调策略，显著提升全球中期天气预测的准确性和效率。",
      "motivation": "全球中期天气预测对地球系统科学至关重要，但现有Transformer模型基于视觉中心架构，忽视地球球形几何和经向周期性，导致物理不一致。传统自回归训练方法计算昂贵且因误差积累限制预测范围，亟需引入物理先验和高效训练策略来解决这些问题。现有方法的不足在于忽略物理约束和效率低下，影响了预测精度和实用性。",
      "method": "论文提出Shifted Earth Transformer架构，将经向周期性和纬向边界整合到基于窗口的自注意力中，实现物理一致的全球信息交换。同时，引入Relay Autoregressive（RAR）微调策略，在有限内存和计算预算下学习长程大气演化。基于这些方法，开发了YanTian模型，结合Transformer架构和地球物理先验，具体数据集摘要未明确说明，但架构专注于天气预测应用。",
      "result": "YanTian模型在准确度上超越欧洲中期天气预报中心的高分辨率预报，与最先进AI模型在1度分辨率上表现竞争。计算成本比标准自回归微调低约200倍，Z500预测技能领先时间达到10.3天，优于HRES的9天，具体数据支撑了性能和效率的显著提升。对比基线方法，在精度和成本方面均有明显改进。",
      "conclusion": "本研究贡献了Searth Transformer架构和RAR微调策略，为全球中期天气预测提供了新的算法基础。其学术价值在于将物理先验融入深度学习模型，为复杂地球物理环流系统的预测建模开辟新途径。实际应用价值包括提升预测准确性和降低计算成本，未来工作方向可能涉及更广泛的地球物理问题，但摘要未明确说明局限性。",
      "tags": [
        "Transformer Architecture",
        "Self-Attention",
        "Autoregressive Fine-tuning",
        "Physics-Informed Models",
        "Global Weather Forecasting"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:19.093096Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09465",
    "title": "EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines",
    "authors": [
      "Shuo Zhang",
      "Chaofa Yuan",
      "Ryan Guo",
      "Xiaomin Yu",
      "Rui Xu",
      "Zhangquan Chen",
      "Zinuo Li",
      "Zhi Yang",
      "Shuhao Guan",
      "Zhenheng Tang",
      "Sen Hu",
      "Liwen Zhang",
      "Ronghao Chen",
      "Huacan Wang"
    ],
    "abstract": "While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09465.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09465",
    "published": "2026-01-14T13:19:13Z",
    "updated": "2026-01-14T13:19:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "EvoFSM提出基于有限状态机的结构化自演化框架，实现LLM智能体的可控自演化以提升适应性和稳定性。",
      "motivation": "LLM智能体在深度研究中面临固定工作流程难以适应开放查询的挑战。现有自演化方法通过重写代码或提示来改进能力，但不受约束的优化常导致不稳定、幻觉和指令漂移，限制实际应用。因此，开发一种既能自适应复杂任务又能保持行为控制的自演化框架至关重要，以解决现实世界中多变查询的需求。",
      "method": "EvoFSM采用结构化自演化方法，通过演化显式有限状态机（FSM）替代自由形式重写，优化空间解耦为宏观Flow（状态转移逻辑）和微观Skill（状态特定行为）。在critic机制指导下，使用一组受限操作精炼FSM，并集成自演化记忆，提取成功轨迹作为可重用先验和失败模式作为未来约束，确保在明确边界下进行目标改进。",
      "result": "在五个多跳问答基准上的评估显示EvoFSM的有效性，如在DeepSearch基准上达到58.0%准确率。额外实验在交互决策任务中验证了其泛化能力，表明框架在不同场景下保持性能，但与基线方法的对比情况摘要未明确说明。",
      "conclusion": "EvoFSM的核心贡献是提供了一种平衡适应性和控制的自演化框架，为LLM智能体研究带来新思路，减少不稳定性和幻觉问题。该框架在学术上促进了结构化自演化方法的发展，实际应用价值在于支持复杂查询的深度研究，未来工作可能涉及优化记忆机制或扩展任务类型。",
      "tags": [
        "Finite State Machine",
        "Self-Evolution",
        "LLM-based Agents",
        "Critic Mechanism",
        "Multi-hop QA"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:29.903272Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09455",
    "title": "On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI",
    "authors": [
      "André Artelt",
      "Martin Olsen",
      "Kevin Tierney"
    ],
    "abstract": "Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09455.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09455",
    "published": "2026-01-14T13:02:24Z",
    "updated": "2026-01-14T13:02:24Z",
    "comment": "Accepted in Transactions on Machine Learning Research (TMLR), 2025 -- https://openreview.net/pdf?id=aELzBw0q1O",
    "light_analysis": {
      "overview": "这篇论文通过贡献不可近似性结果，证明了在可解释人工智能中计算反事实和半事实解释的计算困难性。",
      "motivation": "随着机器学习模型在关键应用如医疗或金融中的部署增加，提供清晰的解释变得至关重要，以增强用户信任和模型透明度。反事实和半事实解释作为新兴的解释机制，帮助用户理解模型决策，但现有研究表明生成这些解释在计算上具有挑战性，可能导致效率低下或不可行，限制了实际应用。因此，本研究旨在分析这些解释的计算复杂性，揭示现有方法的不足，并为 XAI 社区和决策者提供理论依据，以指导未来开发和规范制定。",
      "method": "该研究采用理论计算复杂性分析方法。首先，作者综述了文献中关于生成反事实和半事实解释的计算复杂性结果，识别出常见困难场景。然后，扩展这些分析，提出自己的不可近似性定理，证明在某些标准假设下，这些解释不仅难以精确生成，也难以有效近似。方法基于计算理论和复杂度理论框架，未使用具体数据集或模型架构，而是聚焦于一般性理论证明，以揭示解释生成的固有计算限制。",
      "result": "论文的主要结果显示，生成反事实和半事实解释在许多情况下是计算困难的，这通过文献综述得到确认。作者进一步贡献了不可近似性结果，表明在一定假设下，这些解释也缺乏有效的近似算法。这些结果以理论形式呈现，强调了 XAI 中解释生成的固有复杂性，例如在某些模型类别中可能无法找到高效解。摘要未提供具体实验数据如准确率或效率指标，因为研究侧重于理论分析而非实证对比，但结论明确指出解释生成的难度超出预期。",
      "conclusion": "该研究的主要贡献在于通过理论分析揭示了 XAI 中反事实和半事实解释的计算复杂性，包括不可近似性，为可解释人工智能领域提供了重要理论依据。学术上，这深化了对解释生成限制的理解，有助于推动更高效算法的发展。实际应用中，结果提醒决策者和开发者在部署解释机制时需考虑计算可行性，可能影响 AI 监管政策的制定。摘要未明确说明局限性，但可推断未来工作可能涉及开发近似方法或探索替代解释策略以克服这些困难。",
      "tags": [
        "Counterfactual Explanations",
        "Semifactual Explanations",
        "Computational Complexity",
        "Inapproximability",
        "XAI"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:36.311646Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09452",
    "title": "MAD: Motion Appearance Decoupling for efficient Driving World Models",
    "authors": [
      "Ahmad Rahimi",
      "Valentin Gerard",
      "Eloi Zablocki",
      "Matthieu Cord",
      "Alexandre Alahi"
    ],
    "abstract": "Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively \"dressing\" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09452.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09452",
    "published": "2026-01-14T12:52:23Z",
    "updated": "2026-01-14T12:52:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出MAD框架，通过解耦运动与外观，高效地将通用视频扩散模型转化为可控自动驾驶世界模型。",
      "motivation": "视频扩散模型虽能生成逼真、时间连贯的视频，但作为自动驾驶世界模型不可靠，因缺乏结构化运动和物理一致性交互，而自动驾驶需精确模拟环境以确保安全。现有方法将通用视频模型适应到驾驶领域虽可行，但依赖大量领域特定数据和昂贵微调，效率低下。本研究旨在解决此问题，提出高效适应框架，减少数据需求和计算成本，提升世界模型的实用性。",
      "method": "本研究提出一个两阶段解耦框架：首先，适应通用视频扩散模型预测结构化运动，以骨架化代理和场景元素视频形式学习物理和社会合理性，聚焦于动态推理；然后，重用相同骨干模型，基于运动序列合成逼真RGB视频，为运动添加纹理和光照，实现外观渲染。该方法采用推理-渲染范式，关键创新在于分离运动学习和外观合成，利用现有模型如SVD，并通过最小监督实现高效适应，减少了计算开销。",
      "result": "实验结果显示，该框架在适应SVD模型时，仅用少于先前SOTA模型6%的计算资源即匹配其性能，表现出高效性。扩展到LTX模型时，MAD-LTX模型优于所有开源竞争者，并在计算效率上显著改进。同时，模型支持全面的文本、自我和对象控制，验证了其在可控生成和性能提升方面的有效性，与基线方法相比在资源消耗和功能灵活性上均有优势。",
      "conclusion": "本研究的主要贡献是提出了一种高效适应框架，通过运动与外观解耦，将通用视频扩散模型转化为自动驾驶世界模型，降低了计算成本和数据依赖。学术上，为视频模型在专业领域的应用提供了新方法；实际应用上，可增强自动驾驶模拟的效率和可控性，具有广泛价值。未来工作可探索模型在其他领域的扩展或优化细节，但摘要未明确说明具体局限性。",
      "tags": [
        "Video Diffusion Models",
        "Motion Appearance Decoupling",
        "Autonomous Driving",
        "Controllable Generation",
        "Two-stage Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:45.905323Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09451",
    "title": "Late Breaking Results: Quamba-SE: Soft-edge Quantizer for Activations in State Space Models",
    "authors": [
      "Yizhi Chen",
      "Ahmed Hemani"
    ],
    "abstract": "We propose Quamba-SE, a soft-edge quantizer for State Space Model (SSM) activation quantization. Unlike existing methods, using standard INT8 operation, Quamba-SE employs three adaptive scales: high-precision for small values, standard scale for normal values, and low-precision for outliers. This preserves outlier information instead of hard clipping, while maintaining precision for other values. We evaluate on Mamba- 130M across 6 zero-shot benchmarks. Results show that Quamba- SE consistently outperforms Quamba, achieving up to +2.68% on individual benchmarks and up to +0.83% improvement in the average accuracy of 6 datasets.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09451.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09451",
    "published": "2026-01-14T12:52:08Z",
    "updated": "2026-01-14T12:52:08Z",
    "comment": "Accepted to DATE Late Breaking Results 2026, Verona, Italy",
    "light_analysis": {
      "overview": "本文提出 Quamba-SE，一种软边缘量化器，通过自适应尺度改善状态空间模型激活量化的性能，避免了异常值的硬裁剪，提升了量化模型的准确性。",
      "motivation": "状态空间模型（SSM）在序列建模中应用广泛，但量化部署时常需处理激活值异常问题。现有方法如标准 INT8 量化往往对异常值进行硬裁剪，导致关键信息丢失和模型性能下降。量化对于提高计算效率和降低内存占用至关重要，但如何在量化过程中有效保留异常值信息是一个未解决的挑战。Quamba-SE 的研究动机是开发一种更精细的量化策略，以在量化 SSM 激活时平衡精度和信息保留，从而提升量化后模型的鲁棒性和实际应用价值。",
      "method": "Quamba-SE 的核心方法是一种软边缘量化器，它使用三个自适应尺度进行激活量化：高精度尺度处理小值以减少量化误差，标准尺度处理正常范围内的值以保持效率，低精度尺度处理异常值以避免硬裁剪。这种设计通过动态调整尺度来保留异常值信息，而非采用传统硬裁剪方法。技术实现上，它应用于状态空间模型的激活层，评估中使用 Mamba-130M 模型架构和 6 个零样本基准测试，以验证其在不同任务中的有效性。创新点在于自适应量化机制，优化了量化精度与信息保留之间的平衡。",
      "result": "在 Mamba-130M 模型上，Quamba-SE 在 6 个零样本基准测试中一致优于基线方法 Quamba。具体性能指标显示，在个别基准测试中，准确率提升最高达到 +2.68%；在 6 个数据集的平均准确率上，提升最高为 +0.83%。这些结果证明了 Quamba-SE 通过自适应量化有效提高了量化模型的性能，同时与基线方法相比显示出显著优势。数据支撑了该方法在保留异常值信息的同时，维持了整体量化精度的有效性，为实际部署提供了有力证据。",
      "conclusion": "Quamba-SE 的主要贡献是提出了一种创新的软边缘量化器，通过自适应尺度解决了状态空间模型激活量化中异常值处理的难题，有效提升了量化性能。研究的学术价值在于为量化技术提供了新思路，特别是在异常值保留方面；实际应用价值在于能促进 SSM 的高效部署，提高模型准确性和效率。摘要未明确说明局限性或未来工作方向，但可以推断未来可能包括优化自适应策略、扩展到更多模型架构或探索更复杂的量化场景，以进一步推动量化技术的发展。",
      "tags": [
        "State Space Model",
        "Activation Quantization",
        "Soft-edge Quantizer",
        "Adaptive Scaling",
        "Zero-shot Benchmarking"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:12.166622Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09449",
    "title": "PrivLEX: Detecting legal concepts in images through Vision-Language Models",
    "authors": [
      "Darya Baranouskaya",
      "Andrea Cavallaro"
    ],
    "abstract": "We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09449.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09449",
    "published": "2026-01-14T12:51:48Z",
    "updated": "2026-01-14T12:51:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "PrivLEX提出了一种基于Vision-Language Models的可解释图像隐私分类器，首次与法律定义的个人数据概念对齐。",
      "motivation": "图像隐私检测在数字化时代至关重要，现有方法多为黑箱模型，缺乏可解释性，难以满足法律合规需求。法律定义的个人数据概念要求透明且可靠的检测机制，而传统隐私分类器往往无法提供决策依据，导致分类结果不可信。因此，开发一个基于法律概念的可解释分类器具有迫切意义，以提升AI系统的实用性和法律对齐性。",
      "method": "PrivLEX利用Vision-Language Models进行零-shot概念检测，通过无标签的Concept Bottleneck Model提供可解释分类。该方法无需在训练阶段使用显式概念标签，而是依赖VLM的预训练知识识别图像中的法律概念。关键创新点包括将VLM与法律概念对齐，实现无需大量标注数据的高效检测，增强了分类过程的透明度和解释性。",
      "result": "论文展示了PrivLEX能够识别图像中的个人数据概念，验证了其概念检测能力。进一步，通过分析人类标注者在图像隐私数据集上的敏感性感知，评估了概念的有效性。然而，摘要未明确说明具体性能指标如准确率或与基线方法的对比结果，因此具体效果数据未知。",
      "conclusion": "PrivLEX的研究贡献在于提出首个与法律概念对齐的可解释隐私分类器，利用VLM和Concept Bottleneck Model实现无需显式标签的检测。该工作提升了图像隐私分类的可解释性和法律合规性，为隐私保护AI提供了新思路。未来工作可能包括优化检测精度、扩展概念范围，并在实际应用中进一步验证其性能。",
      "tags": [
        "Vision-Language Models",
        "zero-shot learning",
        "Concept Bottleneck Model",
        "interpretable AI",
        "privacy classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:11.902363Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09446",
    "title": "Improving Symbolic Translation of Language Models for Logical Reasoning",
    "authors": [
      "Ramya Keerthy Thatikonda",
      "Jiuzhou Han",
      "Wray Buntine",
      "Ehsan Shareghi"
    ],
    "abstract": "The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09446.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09446",
    "published": "2026-01-14T12:47:14Z",
    "updated": "2026-01-14T12:47:14Z",
    "comment": "The Third workshop of NeusymBridge @AAAI 2026 (Bridging Neurons and Symbols for NLP and Knowledge Graph Reasoning)",
    "light_analysis": {
      "overview": "本文提出了一种增量推理框架和大模型合成数据微调方法，显著提升了小型语言模型在逻辑推理任务中的符号翻译准确性。",
      "motivation": "小型语言模型在将自然语言翻译成一阶逻辑时，常因格式和翻译错误产生不准确的符号输出，这影响了逻辑推理系统的可靠性和可验证性。现有方法通常依赖自我迭代纠正错误，但效果严重受限于模型能力，导致翻译性能不佳。因此，研究旨在解决符号翻译错误问题，通过新方法提高小型模型的翻译精度，推动可靠和可访问的符号推理系统发展。",
      "method": "论文首先对常见错误进行分类，并利用大语言模型合成数据来微调小型语言模型。核心创新是引入增量推理，将推理过程分解为谓词生成和一阶逻辑翻译两个阶段，以增强生成质量控制。该框架还集成了针对谓词-arity错误的验证模块，进一步优化性能。方法在四个逻辑推理数据集上评估了三种模型家族，通过分解和验证提升翻译能力。",
      "result": "实验在四个逻辑推理数据集上评估了三种模型家族。结果显示，通过综合微调、增量推理和验证模块，有效降低了错误率，增加了谓词覆盖率，并改善了推理性能，使小型语言模型的翻译能力更接近可靠系统目标。但与基线方法（如自我迭代）的具体对比数据在摘要中未明确说明，仅描述了总体性能提升。",
      "conclusion": "论文的主要贡献在于提出了增量推理框架和大模型辅助微调方法，显著提高了小型语言模型在符号翻译任务中的表现，有助于开发更可靠和可访问的符号推理系统。这具有学术价值，为逻辑推理领域提供了新思路，未来工作可能包括扩展到其他任务或优化验证模块，以解决潜在局限性。",
      "tags": [
        "Logical Reasoning",
        "Incremental Inference",
        "Fine-tuning",
        "First-Order Logic",
        "Verification Module"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:17.656310Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09445",
    "title": "Where Knowledge Collides: A Mechanistic Study of Intra-Memory Knowledge Conflict in Language Models",
    "authors": [
      "Minh Vu Pham",
      "Hsuvas Borkakoty",
      "Yufang Hou"
    ],
    "abstract": "In language models (LMs), intra-memory knowledge conflict largely arises when inconsistent information about the same event is encoded within the model's parametric knowledge. While prior work has primarily focused on resolving conflicts between a model's internal knowledge and external resources through approaches such as fine-tuning or knowledge editing, the problem of localizing conflicts that originate during pre-training within the model's internal representations remain unexplored. In this work, we design a framework based on mechanistic interpretability methods to identify where and how conflicting knowledge from the pre-training data is encoded within LMs. Our findings contribute to a growing body of evidence that specific internal components of a language model are responsible for encoding conflicting knowledge from pre-training, and we demonstrate how mechanistic interpretability methods can be leveraged to causally intervene in and control conflicting knowledge at inference time.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09445.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09445",
    "published": "2026-01-14T12:45:52Z",
    "updated": "2026-01-14T12:45:52Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于机理可解释性方法的框架，用于识别和干预语言模型中预训练数据的内部知识冲突。",
      "motivation": "语言模型中的内部知识冲突问题指的是预训练数据编码了关于同一事件的不一致信息，这可能导致模型推理错误。现有研究主要关注通过微调或知识编辑解决模型内部知识与外部资源的冲突，但预训练期间产生的内部冲突的定位问题尚未被充分探索。理解这些冲突对于提升模型的可靠性、可解释性和在复杂任务中的表现至关重要，尤其是在需要准确知识推理的应用场景中。",
      "method": "研究方法基于机理可解释性技术，设计了一个框架来识别冲突知识在语言模型内部的具体编码位置和方式。关键创新点在于利用这些方法内部定位冲突，而不依赖外部干预，聚焦于预训练数据引发的内部表示问题。摘要未明确说明使用的具体数据集或模型架构，但推断可能涉及标准预训练语言模型和机理分析工具，以实现对冲突机制的深度剖析。",
      "result": "主要实验结果表明，语言模型的特定内部组件负责编码预训练数据中的冲突知识。研究通过机理可解释性方法展示了如何在推理时进行因果干预和控制这些冲突，验证了框架的有效性。尽管摘要未提供具体的性能指标如准确率提升或与基线方法的量化对比，但强调了该方法在理解和操纵内部知识冲突方面的潜力和可行性。",
      "conclusion": "本研究的主要贡献在于利用机理可解释性方法定位和干预语言模型内部的知识冲突，为模型内部机制的理解提供了新视角。其学术价值体现在推动可解释AI的发展，实际应用价值则在于提高模型在推理过程中的可控性和可靠性。未来工作可进一步探索冲突的泛化性、扩展到更多模型类型或针对更复杂冲突场景的干预策略。",
      "tags": [
        "Language Models",
        "Mechanistic Interpretability",
        "Intra-Memory Knowledge Conflict",
        "Pre-training",
        "Causal Intervention"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:20.078936Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09439",
    "title": "DeepLight: A Sobolev-trained Image-to-Image Surrogate Model for Light Transport in Tissue",
    "authors": [
      "Philipp Haim",
      "Vasilis Ntziachristos",
      "Torsten Enßlin",
      "Dominik Jüstel"
    ],
    "abstract": "In optoacoustic imaging, recovering the absorption coefficients of tissue by inverting the light transport remains a challenging problem. Improvements in solving this problem can greatly benefit the clinical value of optoacoustic imaging. Existing variational inversion methods require an accurate and differentiable model of this light transport. As neural surrogate models allow fast and differentiable simulations of complex physical processes, they are considered promising candidates to be used in solving such inverse problems. However, there are in general no guarantees that the derivatives of these surrogate models accurately match those of the underlying physical operator. As accurate derivatives are central to solving inverse problems, errors in the model derivative can considerably hinder high fidelity reconstructions. To overcome this limitation, we present a surrogate model for light transport in tissue that uses Sobolev training to improve the accuracy of the model derivatives. Additionally, the form of Sobolev training we used is suitable for high-dimensional models in general. Our results demonstrate that Sobolev training for a light transport surrogate model not only improves derivative accuracy but also reduces generalization error for in-distribution and out-of-distribution samples. These improvements promise to considerably enhance the utility of the surrogate model in downstream tasks, especially in solving inverse problems.",
    "categories": [
      "cs.LG",
      "physics.med-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09439.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09439",
    "published": "2026-01-14T12:40:02Z",
    "updated": "2026-01-14T12:40:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于Sobolev训练的光传输代理模型，用于提高导数准确性和泛化性能，以解决光声成像中的反问题。",
      "motivation": "本研究针对光声成像中通过反演光传输恢复组织吸收系数的挑战性问题。现有变分反演方法需要准确且可微的光传输模型，而神经网络代理模型虽能快速模拟复杂物理过程，但其导数可能无法精确匹配基础物理算子，导致重建质量下降。改进此问题对提升光声成像的临床价值至关重要，因此需要开发导数更准确的代理模型来增强反问题求解的可靠性。",
      "method": "论文提出了一个图像到图像的代理模型DeepLight，采用Sobolev训练策略来优化导数准确性。Sobolev训练通过同时最小化输出函数和其导数的误差，确保模型在值和梯度上都接近真实光传输算子。该方法适用于高维模型，无需依赖特定架构，旨在为光传输模拟提供快速且精确的替代方案，从而支持下游反问题求解任务。",
      "result": "实验结果显示，Sobolev训练显著提高了代理模型的导数准确性，并减少了分布内和分布外样本的泛化误差。与未使用Sobolev训练的基线模型相比，改进后的模型在反问题求解中表现更优，有助于提升光声成像中吸收系数重建的保真度。这些结果表明该方法能增强模型的鲁棒性和下游应用性能。",
      "conclusion": "本研究贡献了一种基于Sobolev训练的光传输代理模型，有效提升了导数准确性和泛化能力，为光声成像等领域的反问题求解提供了更可靠的工具。学术上推动了代理模型在物理模拟中的发展，实际应用中可能增强临床诊断的精度。摘要未明确说明局限性，但未来工作可探索该方法在不同场景下的泛化性。",
      "tags": [
        "Surrogate Model",
        "Sobolev Training",
        "Light Transport Simulation",
        "Inverse Problems",
        "Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:27.802328Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09433",
    "title": "Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?",
    "authors": [
      "David Reid",
      "Ognjen Arandjelovic"
    ],
    "abstract": "Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09433.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09433",
    "published": "2026-01-14T12:30:49Z",
    "updated": "2026-01-14T12:30:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文首次应用Vision Transformer（ViT）于古代硬币语义元素识别，并发现ViT在准确性上优于卷积神经网络（CNN）。",
      "motivation": "古代硬币的自动分析对历史研究和收藏市场有重要价值，能高效提取历史见解并辅助交易决策。现有研究多采用卷积神经网络（CNN）识别语义元素，但性能仍有提升空间。本文旨在探索更先进的Vision Transformer（ViT）模型，以解决传统方法的局限性，提升分析准确性和效率，推动文化遗产数字化进程。",
      "method": "本文采用Vision Transformer（ViT）深度学习架构，结合多模态数据（包括图像和非结构化文本），实现古代硬币语义元素的完全自动识别。关键创新在于首次将ViT应用于此任务，并通过对比ViT和CNN模型的训练与实现，展示其技术优势。摘要未明确说明具体数据集和模型架构细节，但强调了端到端学习过程。",
      "result": "实验评估显示，ViT模型在识别准确性上优于新训练的CNN模型。尽管摘要未提供具体性能指标如准确率百分比，但明确指出了ViT的优越表现。这一结果验证了ViT在计算机视觉任务中的有效性，为后续研究提供了实证基础，表明先进模型如ViT能在特定应用场景中超越传统方法。",
      "conclusion": "本研究的主要贡献是首次将Vision Transformer应用于古代硬币分析，并证明其在语义元素识别中优于CNN。学术上，这拓展了深度学习在文化遗产领域的应用，为自动分析提供了新思路。实际应用中，能更高效地辅助历史研究和收藏市场。未来方向可包括优化模型架构、扩展数据集或探索其他先进模型。",
      "tags": [
        "Vision Transformer",
        "Convolutional Neural Networks",
        "Multimodal Learning",
        "Ancient Coin Analysis",
        "Semantic Element Identification"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:52.310012Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09430",
    "title": "Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs",
    "authors": [
      "Rui Zhu",
      "Xin Shen",
      "Shuchen Wu",
      "Chenxi Miao",
      "Xin Yu",
      "Yang Li",
      "Weikang Li",
      "Deguo Xia",
      "Jizhou Huang"
    ],
    "abstract": "Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09430.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09430",
    "published": "2026-01-14T12:24:47Z",
    "updated": "2026-01-14T12:24:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了Video-MSR基准，首个评估动态视频中多模态大语言模型多跳空间推理能力的测试集，并通过指令微调提升模型性能。",
      "motivation": "空间推理是多模态大语言模型（MLLMs）的关键能力，但现有基准主要集中于单步感知到判断任务，未能充分评估需要复杂视觉-空间逻辑链的动态场景。这种不足限制了模型在真实世界应用中的实用性，例如在视频理解或机器人导航中，多跳推理是必不可少的。因此，开发专门的基准以系统测试和改进MLLMs的多跳空间推理能力，填补了这一研究空白。",
      "method": "论文引入Video-MSR基准，通过四个具体任务（受限定位、链式参考检索、路径规划、反事实物理推理）评估多跳空间推理能力。基准包含3,052个高质量视频实例和4,993个问答对，采用可扩展的视觉基础管道构建，结合先进模型生成和人工验证确保数据可靠性。为增强模型性能，制作了MSR-9K指令微调数据集，并对Qwen-VL模型进行微调，以探究多跳空间指令数据的影响。",
      "result": "对20个前沿MLLM的全面评估显示，模型在多跳空间推理任务中表现不佳，出现显著性能下降，常见空间迷失和幻觉问题。微调Qwen-VL后，在Video-MSR上实现了7.82%的绝对提升，表明多跳空间指令数据能有效增强模型推理能力，同时验证了基准的有效性和模型的改进潜力。",
      "conclusion": "该研究的主要贡献在于建立了Video-MSR基准和MSR-9K数据集，为评估和提升MLLMs的多跳空间推理提供了重要工具，强调了指令微调在复杂推理任务中的价值。它为未来研究奠定了基础，推动模型在动态视频场景中的应用。潜在局限性可能涉及数据集规模或泛化性，未来工作可扩展基准范围或探索更高效微调方法。",
      "tags": [
        "Multimodal Large Language Models",
        "Spatial Reasoning",
        "Multi-hop Reasoning",
        "Benchmark Evaluation",
        "Instruction Tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:04.591821Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09428",
    "title": "Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps",
    "authors": [
      "Siyi Li",
      "Joseph G. Lambourne",
      "Longfei Zhang",
      "Pradeep Kumar Jayaraman",
      "Karl. D. D. Willis"
    ],
    "abstract": "We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.",
    "categories": [
      "cs.LG",
      "cs.GR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09428.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09428",
    "published": "2026-01-14T12:17:34Z",
    "updated": "2026-01-14T12:17:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于transformer模型和几何构造序列的CAD轮廓生成方法，并通过强化学习优化生成质量。",
      "motivation": "研究动机是解决计算机辅助设计（CAD）中轮廓生成的参数化编辑和质量控制问题。CAD设计需要高精度和可调整的轮廓，但现有方法可能在生成质量或自由度控制上不足，难以实现灵活的几何编辑。摘要指出，通过引入几何构造序列，可以减少形状的自由度，类似于参数化CAD模型中的约束，从而允许设计师进行精细的参数调整，提升设计的实用性和效率。",
      "method": "研究方法使用transformer模型生成CAD轮廓的构造序列，序列包括曲线偏移、旋转和相交等简单几何操作。关键创新点是在设计师输入几何和最终轮廓之间添加构造步骤，类似于语言模型中的思维链，这减少了建模的自由度并提高了生成精度。技术特色还包括应用强化学习来优化构造序列，进一步提升性能。摘要未明确说明具体数据集或模型架构细节。",
      "result": "主要实验结果表明，添加几何构造步骤能显著提高CAD轮廓的生成质量，类似于思维链在语言模型中的改进效果。强化学习的应用进一步优化了多个性能指标，包括一些未明确优化的方面，整体提升了生成轮廓的精度和参数化编辑能力。摘要未提供具体数据指标，但与基线方法相比，该方法在灵活性和精度上有所改进。",
      "conclusion": "论文主要贡献是结合transformer模型和几何构造序列的CAD轮廓生成方法，强化学习进一步优化了生成过程。学术价值在于推动了机器学习与几何设计的融合，为智能CAD系统提供了新思路。实际应用中，该方法支持高精度的参数化编辑，提高了设计效率。未来工作可能涉及扩展构造步骤或应用于更广泛的设计场景，摘要未明确说明局限性。",
      "tags": [
        "Transformer",
        "Reinforcement Learning",
        "CAD",
        "Geometric Construction",
        "Chain of Thought"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:49.149860Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09421",
    "title": "Bias Dynamics in BabyLMs: Towards a Compute-Efficient Sandbox for Democratising Pre-Training Debiasing",
    "authors": [
      "Filip Trhlik",
      "Andrew Caines",
      "Paula Buttery"
    ],
    "abstract": "Pre-trained language models (LMs) have, over the last few years, grown substantially in both societal adoption and training costs. This rapid growth in size has constrained progress in understanding and mitigating their biases. Since re-training LMs is prohibitively expensive, most debiasing work has focused on post-hoc or masking-based strategies, which often fail to address the underlying causes of bias. In this work, we seek to democratise pre-model debiasing research by using low-cost proxy models. Specifically, we investigate BabyLMs, compact BERT-like models trained on small and mutable corpora that can approximate bias acquisition and learning dynamics of larger models. We show that BabyLMs display closely aligned patterns of intrinsic bias formation and performance development compared to standard BERT models, despite their drastically reduced size. Furthermore, correlations between BabyLMs and BERT hold across multiple intra-model and post-model debiasing methods. Leveraging these similarities, we conduct pre-model debiasing experiments with BabyLMs, replicating prior findings and presenting new insights regarding the influence of gender imbalance and toxicity on bias formation. Our results demonstrate that BabyLMs can serve as an effective sandbox for large-scale LMs, reducing pre-training costs from over 500 GPU-hours to under 30 GPU-hours. This provides a way to democratise pre-model debiasing research and enables faster, more accessible exploration of methods for building fairer LMs.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09421.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09421",
    "published": "2026-01-14T12:12:40Z",
    "updated": "2026-01-14T12:12:40Z",
    "comment": "21 pages, 18 figures",
    "light_analysis": {
      "overview": "本研究提出使用低成本BabyLMs作为计算效率沙盒，模拟大规模语言模型的偏见动态，以民主化预训练去偏见研究。",
      "motivation": "预训练语言模型（LMs）随着规模增大和训练成本上升，偏见问题日益严重，但重新训练模型过于昂贵，限制了去偏见研究的深入。现有方法如后处理或掩码策略虽常用，但未能解决偏见的根本原因，导致缓解效果有限。因此，亟需一种低成本途径来探索预训练阶段的去偏见技术，以促进公平性研究的民主化，降低研究门槛并应对偏见形成的复杂性。",
      "method": "本研究采用BabyLMs作为核心方法，这些是紧凑的BERT-like模型，训练在小型和可变的语料上。关键创新点在于利用BabyLMs近似标准BERT模型的偏见获取和学习动态，通过调整语料和训练参数来模拟大模型行为。该方法支持多种去偏见策略的测试，包括模型内和模型后方法。摘要未明确说明具体数据集名称，但提及语料可变性，推断模型架构基于BERT的自注意力和预训练任务，以实现低成本沙盒环境。",
      "result": "实验结果显示，BabyLMs在内在偏见形成和性能发展上与标准BERT模型有紧密对齐模式，尽管模型大小显著减小。相关性在多个去偏见方法中保持一致，验证了其作为代理模型的有效性。研究还复制了先前的去偏见发现，并提出关于性别不平衡和毒性影响偏见形成的新见解。计算成本从超过500 GPU小时降至30 GPU小时以下，大幅提高了研究效率，证明了BabyLMs作为沙盒在民主化研究中的实用性。",
      "conclusion": "论文的主要贡献是使用BabyLMs作为计算效率高的沙盒，民主化预训练去偏见研究，降低了研究成本，使更多研究者能够探索偏见缓解方法。学术上，这为模拟大模型动态提供了新工具；实际上，促进了更公平语言模型的构建。摘要未明确说明局限性，但可推断未来工作可能包括扩展BabyLMs到其他模型或偏见类型，以及进一步优化沙盒方法的可扩展性。",
      "tags": [
        "Pre-trained Language Models",
        "Debiasing",
        "Bias Dynamics",
        "BabyLMs",
        "Compute Efficiency"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:04.688785Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09416",
    "title": "Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification",
    "authors": [
      "Yaxi Chen",
      "Zi Ye",
      "Shaheer U. Saeed",
      "Oliver Yu",
      "Simin Ni",
      "Jie Huang",
      "Yipeng Hu"
    ],
    "abstract": "Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09416.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09416",
    "published": "2026-01-14T12:09:34Z",
    "updated": "2026-01-14T12:09:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出结合放射组学特征和层次损失的深度学习方法，显著提升骨肉瘤组织学分类的性能和可解释性。",
      "motivation": "骨肉瘤是一种侵袭性原发性骨恶性肿瘤，化疗后肿瘤区域存活与否的准确评估对预后和治疗计划至关重要。现有手动评估方法劳动密集型、主观性强且存在观察者间差异，尽管数字病理学进展已实现自动化坏死量化，但先前深度学习模型在患者层面的测试中泛化能力显著下降，表明需要改进方法来提高自动化分类的可靠性和效率，以支持临床决策并减少人为误差。",
      "method": "本研究提出两个核心方法创新：一是集成放射组学特征作为额外的多模态输入，尽管这些特征源于图像，但与深度学习模型结合，可提升分类性能并增强可解释性；二是采用层次损失优化两个二元分类任务（肿瘤vs非肿瘤和存活vs非存活），与传统的“扁平”三分类任务（非肿瘤、非存活肿瘤、存活肿瘤）相比，通过可训练的任务权重，实现更优的类别性能。实验基于TCIA OS Tumor Assessment数据集进行，利用层次化学习策略改进模型架构。",
      "result": "通过实验验证，集成放射组学特征和层次损失的方法在分类性能上显著提升，与基线方法相比，改善了模型在患者层面的泛化能力，并在TCIA OS Tumor Assessment公开数据集上达到了新的最先进水平。摘要未明确说明具体性能指标如准确率数值，但强调了每个新方法及其组合均带来性能改进，克服了先前研究中模型性能下降的问题，为自动化病理分析提供了更强有力的工具。",
      "conclusion": "本研究的核心贡献在于提出了一种创新的深度学习方法，结合放射组学特征和层次损失，有效提升了骨肉瘤组织学分类的准确性和可解释性。这项工作具有重要学术价值，为医学图像分析领域提供了多模态集成和层次化学习的范例，并具有实际应用价值，可支持临床自动化病理评估以提高效率和客观性。未来工作可能包括扩展到其他肿瘤类型或进一步优化模型以应对更复杂的分类任务，但摘要未明确说明具体局限性。",
      "tags": [
        "Radiomics",
        "Deep Learning",
        "Hierarchical Loss",
        "Histology Classification",
        "Multimodal Input"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:07.011335Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09410",
    "title": "Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process",
    "authors": [
      "Sangjun Han",
      "Youngmi Hur"
    ],
    "abstract": "With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09410.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09410",
    "published": "2026-01-14T11:57:15Z",
    "updated": "2026-01-14T11:57:15Z",
    "comment": "Accepted for publication in IET Image Processing. This is the authors' final accepted manuscript",
    "light_analysis": {
      "overview": "论文提出基于拉普拉斯金字塔的细节损失和重复上采样下采样过程，以增强超分辨率图像的高频细节，提升图像质量。",
      "motivation": "图像超分辨率是实际应用中的关键技术，如计算机视觉和医学成像，但现有方法在处理精细细节时可能不足，导致高频信息丢失。该研究旨在解决超分辨率任务中细节增强不足的问题，强调高频像素的重要性，因为细节保留直接影响图像质量和后续分析。现有模型往往无法有效突出高频成分，从而限制了性能提升，这使得开发新方法来优化细节处理尤为重要。",
      "method": "论文提出两种核心方法：首先，基于拉普拉斯金字塔的细节损失，通过单独生成超分辨率和细节图像来指导模型，使其专注于高频组件；其次，引入重复上采样和下采样过程，从多个低分辨率特征中提取多样信息，放大细节损失的效果。这些方法集成到CNN-based模型中，并在小规模上应用于现有attention-based模型，关键创新点在于结合细节损失和重复采样，以优化高频细节的生成和控制。",
      "result": "实验结果显示，设计的CNN-based模型取得了SOTA性能，超越了所有当前可用的CNN-based模型和部分attention-based模型。在小规模实验中，将细节损失应用于现有attention-based模型后，这些模型相对于原始版本均展现出性能提升。尽管摘要未明确说明具体数据指标，如准确率或效率改进，但结果证实了方法在不同模型结构中的有效性和泛化能力。",
      "conclusion": "论文的主要贡献是提出细节损失和重复采样过程，显著增强了超分辨率图像的高频细节。研究具有学术价值，通过创新损失函数和过程优化，推动了超分辨率技术的发展，并具备实际应用潜力，如提升图像处理系统的性能。局限性方面，摘要未明确说明未来工作方向，但可推断进一步探索可能包括在大规模数据集上的验证或扩展到其他视觉任务。",
      "tags": [
        "Super-Resolution",
        "Laplacian Pyramid",
        "Detail Loss",
        "CNN",
        "Attention-based Models"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:58.591700Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09402",
    "title": "Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation",
    "authors": [
      "Xinze Li",
      "Zhenghao Liu",
      "Haidong Xin",
      "Yukun Yan",
      "Shuo Wang",
      "Zheni Zeng",
      "Sen Mei",
      "Ge Yu",
      "Maosong Sun"
    ],
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09402.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09402",
    "published": "2026-01-14T11:44:31Z",
    "updated": "2026-01-14T11:44:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出PAGER框架，通过结构化认知大纲和迭代检索构建连贯知识页面，显著提升RAG模型的性能和知识表示质量。",
      "motivation": "当前检索增强生成模型在迭代知识积累过程中，由于缺乏组织性结构，难以构建全面且连贯的知识表示，这限制了模型整合外部知识的能力，影响回答生成的准确性和完整性。现有方法虽然尝试渐进式积累知识，但未能有效结构化，导致知识冲突和效率低下，因此亟需改进知识组织方式以优化RAG性能。",
      "method": "PAGER采用页面驱动的知识表示框架，首先利用大型语言模型为给定问题生成结构化认知大纲，该大纲包含多个代表不同知识方面的槽位。然后，通过迭代检索和精炼相关外部文档，逐步填充每个槽位，最终形成一个连贯的页面作为上下文输入，以指导答案生成。其创新点在于引入结构化页面表示，系统化组织知识，提升表示的一致性和质量。",
      "result": "在多个知识密集型基准测试和不同骨干模型上，PAGER consistently outperforms all RAG baselines。分析表明，它能构建更高质量、信息更密集的知识表示，有效缓解知识冲突问题，并使得大型语言模型更有效地利用外部知识，显示了框架的鲁棒性和性能提升，具体实验数据摘要未明确说明。",
      "conclusion": "PAGER框架通过结构化页面表示，显著改进了检索增强生成模型的知识整合能力，提供了系统化方法来组织和利用外部知识，提升了模型的准确性和一致性。学术上，它推动了知识表示研究；实际应用中，可增强AI系统在复杂任务中的表现。未来工作可能包括扩展到更多领域和优化检索过程，摘要未明确说明具体局限性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Large Language Models",
        "Structured Knowledge Representation",
        "Iterative Retrieval",
        "Cognitive Outline"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:19.812935Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09400",
    "title": "Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay",
    "authors": [
      "Olgierd Unold",
      "Stanisław Franczyk"
    ],
    "abstract": "This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \\texttt{Maze 6} and the stochastic \\texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09400.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09400",
    "published": "2026-01-14T11:43:36Z",
    "updated": "2026-01-14T11:43:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出ACS2HER模型，集成Anticipatory Classifier System与Hindsight Experience Replay，以解决稀疏奖励环境中的学习效率问题。",
      "motivation": "ACS2作为一种基于潜在学习的方法，在构建认知地图方面表现优异，但在奖励稀疏的环境中，其性能常停滞不前，限制了实际应用。为了解决这一问题，本研究结合Hindsight Experience Replay机制，旨在通过重标失败轨迹中的状态为虚拟目标来增加学习信号，弥补稀疏奖励带来的学习瓶颈，并探索预期与事后学习的互补潜力。",
      "method": "本研究提出了ACS2HER，一个特定的架构变体，通过修改ACS2以在智能体未能达成主要目标时触发事后学习过程，将访问过的状态重新标记为虚拟目标。这一方法融合了ACS2的预期能力和HER的目标重标技术，关键创新在于动态调整学习信号，并在两个基准环境（确定性的Maze 6和随机的FrozenLake）中进行测试，以评估其效果。",
      "result": "实验在Maze 6和FrozenLake环境中进行，结果表明ACS2HER相比标准ACS2能显著加速知识获取和环境掌握，证明了集成HER的有效性。然而，这种性能提升伴随着计算开销的增加和分类器数量的显著扩张，揭示了在效率与资源消耗之间的权衡，具体数据摘要未明确说明。",
      "conclusion": "本文首次在学习分类器系统中结合了预期机制和回顾性目标重标记，提出了ACS2HER模型，有效改善了稀疏奖励环境下的学习性能。这为该领域提供了新的研究方向，展示了两种技术的互补潜力，并具有实际应用价值。局限性包括计算效率较低和资源消耗增加，未来工作可优化这些方面以减少开销。",
      "tags": [
        "Anticipatory Classifier System",
        "Hindsight Experience Replay",
        "Learning Classifier Systems",
        "Goal Relabeling",
        "Sparse Rewards"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:32.770039Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09398",
    "title": "Ability Transfer and Recovery via Modularized Parameters Localization",
    "authors": [
      "Songyao Jin",
      "Kun Zhou",
      "Wenqi Li",
      "Peng Wang",
      "Biwei Huang"
    ],
    "abstract": "Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09398.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09398",
    "published": "2026-01-14T11:42:39Z",
    "updated": "2026-01-14T11:42:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出ACT方法，通过激活引导的通道级能力转移，实现大型语言模型能力的恢复与整合。",
      "motivation": "当前，大型语言模型在针对特定领域或技能进行持续预训练或微调时，虽然提升了目标性能，但常导致其他能力的退化，即灾难性遗忘，这限制了模型的通用性和多功能性。现有方法难以在增强新能力的同时保持现有技能，造成资源浪费和模型适应性下降。因此，本研究旨在探索能力在参数中的分布机制，并开发一种新方法来有效转移和恢复能力，以减少遗忘现象。",
      "method": "论文首先通过分析模块激活，发现能力相关激活高度集中在少量通道（<5%），这些通道具有解耦性和稳定性。基于此，提出ACT方法：利用激活差异定位能力相关通道，仅选择性地转移这些通道的参数，然后进行轻量级微调以确保兼容性。该方法减少了参数干扰，提高了能力转移的效率；实验在多语言数学和科学推理任务上进行，验证了技术路线的可行性。",
      "result": "实验结果表明，ACT方法在多语言数学和科学推理任务中有效恢复了模型遗忘的能力，同时保持了现有技能的完整性。此外，ACT能够合并多个专门化模型，整合多种能力到单个模型中，并最小化干扰，显示出较高的兼容性。与基线方法相比，ACT减少了灾难性遗忘的影响，提升了模型的综合性能，但摘要未明确说明具体数值指标。",
      "conclusion": "本研究的主要贡献是提出ACT方法，实现了能力的转移与恢复，为解决大型语言模型中的灾难性遗忘问题提供了新途径。研究揭示了能力在参数中的集中分布特性，加深了对模型内部机制的理解，具有重要学术价值。实际应用中，该方法可促进多功能LLMs的开发，提升模型在复杂任务中的适应能力；未来工作可能包括扩展到更多能力类型或优化参数定位效率。",
      "tags": [
        "Large Language Model",
        "Activation-Guided Transfer",
        "Channel-wise Localization",
        "Model Merging",
        "Lightweight Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:57.385958Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09382",
    "title": "Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments",
    "authors": [
      "Qinglong Shi",
      "Donghai Wang",
      "Hantao Zhou",
      "Jiguo Li",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He"
    ],
    "abstract": "Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09382.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09382",
    "published": "2026-01-14T11:15:40Z",
    "updated": "2026-01-14T11:15:40Z",
    "comment": "8 pages, 2 figures",
    "light_analysis": {
      "overview": "论文提出一种主动任务导向代理新范式，通过意图条件监控和事件触发跟进实现长期意图维护和动态环境适应。",
      "motivation": "当前大型语言模型代理主要基于反应式范式，只能在短期会话中响应用户查询，无法有效维持用户的长期意图或适应不断变化的外部环境。这一局限性在实际应用中尤为突出，因为用户需求可能随时间演变，环境信息频繁更新。现有方法缺乏主动性和长期互动能力，导致代理在动态环境中的交互效率低下和用户体验不佳，因此亟需开发能够弥合静态需求与动态环境之间差距的新互动范式。",
      "method": "论文提出一个主动任务导向代理框架，核心包含两个关键能力：意图条件监控，即代理基于对话历史自主制定触发条件；事件触发跟进，即代理在检测到有用环境更新时主动与用户互动。为训练代理，研究者引入了一个高质量的数据合成管道，用于生成动态环境中的复杂多轮对话数据。此外，论文提出了新的评估基准ChronosBench，以解决动态环境中任务导向互动缺乏标准评估的问题，方法采用监督学习使用合成数据微调模型。",
      "result": "论文评估了当前领先的闭源和开源模型，揭示了它们在长期任务导向互动中存在缺陷，例如无法有效处理动态环境。通过使用合成数据进行监督学习微调的模型，在包含用户意图变化的复杂任务中，实现了85.19%的任务完成率，优于其他测试模型。这一结果表明新提出的数据驱动策略在提升代理性能方面有效，验证了主动互动范式的优越性。",
      "conclusion": "本研究的主要贡献在于提出了一个主动任务导向代理范式，有效解决了现有LLM代理在长期意图维护和动态环境适应中的不足。通过开发数据合成管道和新基准ChronosBench，为领域提供了关键资源和评估工具。学术上，该工作推动了任务导向互动和代理系统的发展；实际应用中，可增强智能助手的长期互动能力。未来工作可能包括扩展数据合成的多样性以及探索更广泛的环境适应性。",
      "tags": [
        "Large Language Model Agents",
        "Task-oriented Dialogue Systems",
        "Proactive Interaction",
        "Synthetic Data Generation",
        "Dynamic Environment Adaptation"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:38.457089Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09373",
    "title": "The Imperfective Paradox in Large Language Models",
    "authors": [
      "Bolei Ma",
      "Yusuke Miyao"
    ],
    "abstract": "Do Large Language Models (LLMs) genuinely grasp the compositional semantics of events, or do they rely on surface-level probabilistic heuristics? We investigate the Imperfective Paradox, a logical phenomenon where the past progressive aspect entails event realization for activities (e.g., running $\\to$ ran) but not for accomplishments (e.g., building $\\nrightarrow$ built). We introduce ImperfectiveNLI, a diagnostic dataset designed to probe this distinction across diverse semantic classes. Evaluating state-of-the-art open-weight models, we uncover a pervasive Teleological Bias: models systematically hallucinate completion for goal-oriented events, often overriding explicit textual negation. Representational analyses show that while internal embeddings often distinguish process from result, inference decisions are dominated by strong priors about goal attainment. We further find that prompting-based interventions reduce hallucinated completions but also increase incorrect rejections of valid entailments. Our findings suggest that current LLMs lack structural aspectual awareness, operating as predictive narrative engines rather than faithful logical reasoners.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09373.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09373",
    "published": "2026-01-14T10:57:16Z",
    "updated": "2026-01-14T10:57:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "研究揭示了大型语言模型在处理Imperfective Paradox时的Teleological Bias，表明其缺乏结构性时体意识，依赖于预测性叙事而非忠实逻辑推理。",
      "motivation": "研究动机是探究大型语言模型是否真正理解事件的组合语义，还是仅依赖表面级概率启发式。背景中，Imperfective Paradox作为一个逻辑现象，能区分不同语义类别的事件（如活动与成就），用以测试LLMs的逻辑一致性。这一问题重要，因为现有方法可能不足，模型可能过度依赖表面模式而非深层语义结构，影响其在推理任务中的可靠性。",
      "method": "论文提出了一种诊断方法，包括引入ImperfectiveNLI数据集，专门设计来探究不同语义类别下Imperfective Paradox的区别。研究评估了先进的开放权重模型，通过表示分析检查内部嵌入如何区分过程与结果，并使用基于提示的干预实验测试模型响应。关键创新在于构建了针对时体语义的基准测试和揭示了模型内部的决策机制。",
      "result": "实验结果显示，模型普遍存在Teleological Bias，系统性产生目标导向事件的完成幻觉，甚至覆盖文本中的显式否定。表示分析表明，虽然嵌入能区分过程与结果，但推理决策被目标达成的强先验主导。基于提示的干预虽减少了幻觉，但也增加了对有效蕴含的错误拒绝，表明模型在逻辑一致性上存在不足（摘要未明确说明具体性能指标，如准确率）。",
      "conclusion": "结论指出，当前大型语言模型缺乏结构性时体意识，更多地作为预测性叙事引擎而非忠实逻辑推理器。这项研究的主要贡献在于揭示了LLMs在语义理解上的系统偏差，强调了改进模型结构意识的必要性。实际应用中，这表明在依赖LLMs进行逻辑推理任务时需要谨慎，未来研究可探索如何增强模型的逻辑推理能力。",
      "tags": [
        "Large Language Model",
        "Imperfective Paradox",
        "Natural Language Inference",
        "Teleological Bias",
        "Representational Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:04.183162Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09367",
    "title": "Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish",
    "authors": [
      "Aidana Aidynkyzy",
      "Oğuz Dikenelli",
      "Oylum Alatlı",
      "Şebnem Bora"
    ],
    "abstract": "The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09367.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09367",
    "published": "2026-01-14T10:49:46Z",
    "updated": "2026-01-14T10:49:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了首个英语-土耳其语临床关系提取双语评估，引入平行数据集和基于对比学习的关系感知检索方法RAR，显著提升大型语言模型性能。",
      "motivation": "研究动机源于非英语语言临床文本关系提取中标注数据稀缺的问题，特别是土耳其语，这导致基于英语开发的大型语言模型方法在非英语环境中评估受限。临床自然语言处理需要多语言支持以促进全球医疗公平，但现有方法主要针对英语，缺乏对资源匮乏语言的系统评估。因此，通过双语评估弥补资源差距，推动临床信息提取技术的普及和改进。",
      "method": "研究方法包括构建首个英语-土耳其语平行临床关系提取数据集，源自2010 i2b2/VA关系分类语料库，并进行精心整理。评估了多种提示策略，如上下文学习和链式思维，并与微调基线模型PURE比较。创新性地提出了关系感知检索方法RAR，基于对比学习优化示例选择，捕捉句子级和关系级语义。使用大型语言模型如Gemini 1.5 Flash和DeepSeek-V3进行实验，以验证方法的有效性。",
      "result": "实验结果表明，基于提示的大型语言模型方法在所有评估中均优于传统微调模型如PURE。英语表现普遍优于土耳其语，例如，在ICL方法中，RAR达到最高性能，Gemini 1.5 Flash在英语和土耳其语的micro-F1分数分别为0.906和0.888。当RAR与DeepSeek-V3的结构化推理提示结合时，英语F1进一步提升至0.918。这些数据证实了高质量检索和提示策略在提升模型性能方面的重要作用。",
      "conclusion": "本论文的主要贡献在于首次完成英语-土耳其语临床关系提取双语评估，并开发了创新的关系感知检索方法RAR，展示了先进检索和提示技术在弥补资源差距中的潜力。学术上，推动了多语言临床自然语言处理研究，提高了模型泛化能力；实际上，有助于减少非英语临床数据处理的障碍，促进医疗信息公平。未来工作可扩展至更多语言或进一步优化检索技术，以增强应用范围。",
      "tags": [
        "Relation Extraction",
        "Large Language Models",
        "In-Context Learning",
        "Contrastive Learning",
        "Bilingual Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:07.314498Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09365",
    "title": "Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs",
    "authors": [
      "Biswesh Mohapatra",
      "Théo Charlot",
      "Giovanni Duca",
      "Mayank Palan",
      "Laurent Romary",
      "Justine Cassell"
    ],
    "abstract": "Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09365.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09365",
    "published": "2026-01-14T10:45:22Z",
    "updated": "2026-01-14T10:45:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出并评估在情境对话中明确表示和存储共同点的方法，以改进LLMs的接地能力和对话连贯性。",
      "motivation": "本研究动机源于情境对话中共同点表示的挑战。在对话中，共同点对于建立和维持共享参考至关重要，以确保交互的连贯性。对话系统需要正确接地内容以供后续使用，但现有LLMs虽能执行接地行为如请求澄清，却缺乏明确的共同点表示和存储机制，导致无法确认这些行为是否真正反映接地理解。因此，研究如何明确表示和存储共同点以解决这一不足具有重要意义。",
      "method": "论文通过评估模型在情境对话中通过关系引用建立和利用共同点的能力，测试了多种共同点表示方法。具体而言，在共享上下文中引用实体和关系，并提出改进共同点建立和后续使用的方法。摘要未明确说明具体的数据集和模型架构细节，但推断可能涉及使用LLMs进行实验，专注于技术路线如关系引用和表示机制的优化。",
      "result": "论文评估了模型在情境对话中建立和利用共同点的能力，测试了多种表示方法并提出了改进方法。然而，摘要未明确说明具体的实验结果、性能指标（如准确率提升或效率改进）以及与基线方法的对比情况，因此需要参考完整论文以获取详细数据。",
      "conclusion": "本研究的主要贡献在于提出并评估了在情境对话中明确表示和存储共同点的方法，有助于改进对话系统的接地能力。学术上，填补了LLMs在共同点表示方面的研究空白；实际应用上，可提升对话系统的连贯性和用户体验。未来工作可能涉及更复杂的共同点机制、实际部署验证以及扩展到更多对话场景，摘要未明确说明具体局限性。",
      "tags": [
        "Common Ground Representation",
        "Situational Dialogues",
        "Large Language Models",
        "Dialog Systems",
        "Reference Resolution"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:05.907779Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09361",
    "title": "GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR",
    "authors": [
      "Jiaying Zhang",
      "Lei Shi",
      "Jiguo Li",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09361.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09361",
    "published": "2026-01-14T10:41:34Z",
    "updated": "2026-01-14T10:41:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出GeoRA，一种针对RLVR的几何感知低秩适配方法，解决现有参数高效方法的几何不匹配和效率瓶颈。",
      "motivation": "强化学习与可验证奖励（RLVR）对于推进大规模推理模型至关重要。然而，现有参数高效方法如PiSSA和MiLoRA专为监督微调设计，无法适应RLVR特有的优化动态和几何结构，直接应用会导致谱崩溃和优化不稳定，严重影响性能。同时，依赖稀疏更新的方法由于非结构化计算在现代硬件上遭遇效率瓶颈。因此，需要一种新方法来兼顾稳定性和高效性。",
      "method": "GeoRA方法利用强化学习更新子空间的各向异性和可压缩性。通过在几何约束子空间中使用奇异值分解提取主方向初始化适配器，同时冻结残差组件，保留预训练模型的几何结构。该方法通过密集算子实现高效的GPU计算，避免硬件瓶颈，确保优化稳定性和参数效率。",
      "result": "在Qwen和Llama模型上的实验表明，GeoRA有效减轻了由几何错位引起的优化瓶颈。在关键数学基准测试中，GeoRA一致优于已有的低秩基线方法，取得了最先进的性能。此外，在域外任务中，GeoRA表现出优越的泛化能力和对灾难性遗忘的韧性，验证了其实际应用价值。",
      "conclusion": "本研究提出的GeoRA方法成功解决了RLVR中的几何不匹配问题，显著提升模型性能。学术上，为参数高效微调提供了新思路，结合几何约束和低秩适配；实际上，推动了大规模推理模型的发展。未来工作可进一步优化扩展到其他领域或探索更多参数配置，但摘要未明确说明具体局限性。",
      "tags": [
        "Reinforcement Learning with Verifiable Rewards",
        "Low-Rank Adaptation",
        "Singular Value Decomposition",
        "Geometry-Aware Optimization",
        "Parameter-Efficient Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:29.293033Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09353",
    "title": "Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving",
    "authors": [
      "Ioannis Peridis",
      "Dimitrios Troullinos",
      "Georgios Chalkiadakis",
      "Pantelis Giankoulidis",
      "Ioannis Papamichail",
      "Markos Papageorgiou"
    ],
    "abstract": "Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09353.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09353",
    "published": "2026-01-14T10:35:21Z",
    "updated": "2026-01-14T10:35:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了结合蒙特卡洛树搜索和神经网络指导的方法，用于无车道自动驾驶环境的规划，创新点在于神经网络指导MCTS选择阶段，以提升性能。",
      "motivation": "无车道交通环境允许车辆更好地利用道路横向能力，不受车道限制，从而提高交通流量，但为自动驾驶带来了更复杂和挑战性的规划需求。现有基于车道保持的方法在动态的无车道场景中可能效率低下或存在安全风险，特别是在处理多车辆交互时计算资源有限。因此，需要开发智能规划方法来解决这些不足，优化决策过程以适应无车道环境的高自由度。",
      "method": "本论文采用蒙特卡洛树搜索（MCTS）作为规划方法，用于单智能体自动驾驶在无车道交通中。MCTS配备预训练的神经网络（NN）来指导搜索的选择阶段，通过神经网络的预测能力，在计算约束下进行更智能的树搜索，以提高决策效率。相关的马尔可夫决策过程（MDP）参考了现有强化学习框架，但摘要未明确说明神经网络的具体架构、训练数据集或详细技术实现细节。",
      "result": "实验评估中，论文考虑了安全指标（碰撞率）和效率指标（测量速度）。结果显示：（a）各向同性状态信息导致推动行为，即车辆策略因更快尾随车辆的出现而调整；（b）神经网络引导的MCTS变体性能加速，表明NN指导能提升搜索效率；（c）计算资源与解决方案质量之间存在权衡，揭示了在有限资源下优化决策的可能性。但摘要未提供具体的性能指标数值，如准确率提升或改进百分比。",
      "conclusion": "本论文的主要贡献是提出了一种结合蒙特卡洛树搜索和神经网络指导的规划方法，适用于无车道自动驾驶环境，能够在计算约束下提高搜索效率和决策质量。学术价值在于探索了在动态环境中智能规划的创新方法，为自动驾驶领域提供新思路；实际应用可能提升车辆的安全性和交通流畅度。未来工作方向可能包括扩展到多智能体场景、优化神经网络训练过程，或进一步研究不同交通模式下的适应性。",
      "tags": [
        "Monte-Carlo Tree Search",
        "Neural Network",
        "Autonomous Driving",
        "Reinforcement Learning",
        "Markov Decision Process"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:46.419436Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09352",
    "title": "Spectral Complex Autoencoder Pruning: A Fidelity-Guided Criterion for Extreme Structured Channel Compression",
    "authors": [
      "Wei Liu",
      "Xing Deng",
      "Haijian Shao",
      "Yingtao Jiang"
    ],
    "abstract": "We propose Spectral Complex Autoencoder Pruning (SCAP), a reconstruction-based criterion that measures functional redundancy at the level of individual output channels. For each convolutional layer, we construct a complex interaction field by pairing the full multi-channel input activation as the real part with a single output-channel activation (spatially aligned and broadcast across input channels) as the imaginary part. We transform this complex field to the frequency domain and train a low-capacity autoencoder to reconstruct normalized spectra. Channels whose spectra are reconstructed with high fidelity are interpreted as lying close to a low-dimensional manifold captured by the autoencoder and are therefore more compressible; conversely, channels with low fidelity are retained as they encode information that cannot be compactly represented by the learned manifold. This yields an importance score (optionally fused with the filter L1 norm) that supports simple threshold-based pruning and produces a structurally consistent pruned network. On VGG16 trained on CIFAR-10, at a fixed threshold of 0.6, we obtain 90.11% FLOP reduction and 96.30% parameter reduction with an absolute Top-1 accuracy drop of 1.67% from a 93.44% baseline after fine-tuning, demonstrating that spectral reconstruction fidelity of complex interaction fields is an effective proxy for channel-level redundancy under aggressive compression.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09352.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09352",
    "published": "2026-01-14T10:34:18Z",
    "updated": "2026-01-14T10:34:18Z",
    "comment": "17 pages, 9 figures",
    "light_analysis": {
      "overview": "本文提出光谱复数自编码器剪枝（SCAP）方法，通过频谱重构保真度评估卷积层输出通道的冗余性，实现高效的极端结构化压缩。",
      "motivation": "本研究旨在解决卷积神经网络在极端结构化通道压缩中功能冗余评估的挑战。现有剪枝标准如权重范数方法，往往不能精确衡量通道间的信息冗余，特别是在高压缩率下，容易导致模型性能显著下降。通过引入基于重构的准则，本文探索更准确的冗余度量，以支持有效的模型压缩，减少计算成本和内存占用，适用于资源受限环境。",
      "method": "方法核心是SCAP，为每个卷积层构建复数交互场：以多通道输入激活为实部，单个输出通道激活（空间对齐并广播）为虚部。将复数场转换到频域，训练一个低容量自编码器重构归一化频谱。重构保真度高的通道被认为位于自编码器捕捉的低维流形附近，更可压缩；低保真度通道则保留。重要性分数可结合滤波器L1范数，支持阈值剪枝，生成结构一致的剪枝网络。",
      "result": "实验在VGG16网络上使用CIFAR-10数据集进行。固定阈值0.6下，剪枝后获得90.11%的FLOP减少和96.30%的参数减少，Top-1准确率从基准的93.44%下降1.67%。经过微调后，相较于基线，SCAP在极端压缩下实现了高计算效率提升，同时保持了相对较小的精度损失，证明了频谱重构保真度作为通道级冗余代理的有效性。",
      "conclusion": "本文主要贡献是提出SCAP准则，利用复数场和频谱重构保真度来识别卷积层中的冗余通道，支持结构化剪枝。学术价值在于创新性地将频谱分析和自编码器结合到剪枝任务中，为模型压缩提供新方法。实际应用中，高压缩率有助于部署到边缘设备。未来工作可能包括扩展到其他网络架构或探索更复杂的重构机制。",
      "tags": [
        "Spectral Analysis",
        "Autoencoder",
        "Channel Pruning",
        "Convolutional Neural Networks",
        "Model Compression"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:17.689062Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09350",
    "title": "See More, Store Less: Memory-Efficient Resolution for Video Moment Retrieval",
    "authors": [
      "Mingyu Jeon",
      "Sungjin Han",
      "Jinkwon Hwang",
      "Minchol Kwon",
      "Jonghee Kim",
      "Junyeong Kim"
    ],
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have improved image recognition and reasoning, but video-related tasks remain challenging due to memory constraints from dense frame processing. Existing Video Moment Retrieval (VMR) methodologies rely on sparse frame sampling, risking potential information loss, especially in lengthy videos. We propose SMORE (See MORE, store less), a framework that enhances memory efficiency while maintaining high information resolution. SMORE (1) uses query-guided captions to encode semantics aligned with user intent, (2) applies query-aware importance modulation to highlight relevant segments, and (3) adaptively compresses frames to preserve key content while reducing redundancy. This enables efficient video understanding without exceeding memory budgets. Experimental validation reveals that SMORE achieves state-of-the-art performance on QVHighlights, Charades-STA, and ActivityNet-Captions benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09350.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09350",
    "published": "2026-01-14T10:28:11Z",
    "updated": "2026-01-14T10:28:11Z",
    "comment": null,
    "light_analysis": {
      "overview": "SMORE框架通过查询引导编码、重要性调制和自适应压缩，实现了内存高效的视频时刻检索，避免了现有方法因稀疏采样导致的信息丢失。",
      "motivation": "视频时刻检索（VMR）在AI应用中至关重要，但多模态大语言模型（MLLMs）处理密集视频帧时面临内存限制。现有方法依赖稀疏帧采样，尤其在长视频中，可能导致关键信息丢失，影响检索精度。这一问题突出显示了提高内存效率的重要性，以在不牺牲信息完整性的前提下实现高效视频理解。本研究旨在解决这一瓶颈，提出一种新框架来优化内存使用，确保视频内容的高分辨率处理。",
      "method": "本研究提出SMORE框架，核心方法包括三个技术组件：首先，使用查询引导的标题编码，将用户意图与视频语义对齐，以紧凑表示形式捕获关键信息；其次，应用查询感知重要性调制，动态加权视频片段，突出与查询相关的部分，减少无关内容的处理负荷；最后，采用自适应帧压缩技术，根据内容冗余性选择性地保留关键帧，从而降低内存占用。整体架构融合了多模态处理，以查询为中心优化资源分配，确保高效且准确的视频理解。",
      "result": "实验在QVHighlights、Charades-STA和ActivityNet-Captions三个基准数据集上进行验证，SMORE框架实现了最先进的性能，超越了现有基线方法。这表明该方法在保持高信息分辨率的同时，显著提升了内存效率，为视频时刻检索任务提供了可靠的解决方案。具体性能指标摘要未明确说明，但实验结果支持了框架的有效性和优越性。",
      "conclusion": "本研究的主要贡献是提出了SMORE框架，通过创新技术实现了内存高效的视频时刻检索，解决了密集帧处理的内存挑战。学术上，该方法推动了视频理解领域的发展，为多模态大语言模型在视频任务中的应用提供了新思路；实际应用上，适用于需要高效处理长视频的场景，如视频监控和内容检索。潜在局限性或未来工作方向摘要未明确说明，但可能包括扩展框架到更广泛视频任务或优化压缩策略。",
      "tags": [
        "Video Moment Retrieval",
        "Multimodal Large Language Models",
        "Query-Guided Captions",
        "Query-Aware Modulation",
        "Adaptive Compression"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:37.480856Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09342",
    "title": "Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework",
    "authors": [
      "Ewelina Gajewska",
      "Katarzyna Budzynska",
      "Jarosław A Chudziak"
    ],
    "abstract": "This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09342.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09342",
    "published": "2026-01-14T10:20:32Z",
    "updated": "2026-01-14T10:20:32Z",
    "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings",
    "light_analysis": {
      "overview": "本论文提出了一种基于社区驱动的多智能体框架，用于提高隐式仇恨言论检测的准确性和公平性。",
      "motivation": "隐式仇恨言论检测是自然语言处理中的重要挑战，因其表达隐晦，传统方法如零样本、少样本和思维链提示可能缺乏对社会文化上下文的集成，导致检测不准确和公平性问题。现有方法在复杂数据集上表现有限，本研究旨在通过集成上下文信息，开发更公平有效的检测框架，以解决现有技术忽视身份和文化因素的不足。",
      "method": "论文提出一个上下文化检测框架，实现为多智能体系统，包括一个中央调解员智能体和动态构建的社区智能体，这些智能体代表特定人口群体。该方法利用公开知识源集成社会文化上下文，实现身份感知调解，并使用ToxiGen数据集进行评估。关键创新在于通过社区驱动的咨询框架动态调整策略，提高检测的公平性和准确性。",
      "result": "在挑战性的ToxiGen数据集上，该框架超越了现有的零样本、少样本和思维链提示方法及其他方法。通过引入平衡准确性作为公平性指标，显著提高了分类准确性和所有目标群体的公平性，实验结果表明在准确率和公平性方面均有显著改进，优于基线方法。",
      "conclusion": "本研究的主要贡献是开发了一个社区驱动的多智能体框架，显著提升了隐式仇恨言论检测的准确性和公平性。学术价值在于结合了社会文化上下文和多智能体技术，增强了公平性评估的严谨性。未来工作可能包括扩展框架到更多数据集或优化性能，以应用于实际仇恨言论检测系统。",
      "tags": [
        "Multi-Agent System",
        "Hate Speech Detection",
        "Contextualised Detection",
        "Balanced Accuracy",
        "Socio-cultural Context"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:06.584883Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09322",
    "title": "Beyond the final layer: Attentive multilayer fusion for vision transformers",
    "authors": [
      "Laure Ciernik",
      "Marco Morik",
      "Lukas Thede",
      "Luca Eyring",
      "Shinichi Nakajima",
      "Zeynep Akata",
      "Lukas Muttenthaler"
    ],
    "abstract": "With the rise of large-scale foundation models, efficiently adapting them to downstream tasks remains a central challenge. Linear probing, which freezes the backbone and trains a lightweight head, is computationally efficient but often restricted to last-layer representations. We show that task-relevant information is distributed across the network hierarchy rather than solely encoded in any of the last layers. To leverage this distribution of information, we apply an attentive probing mechanism that dynamically fuses representations from all layers of a Vision Transformer. This mechanism learns to identify the most relevant layers for a target task and combines low-level structural cues with high-level semantic abstractions. Across 20 diverse datasets and multiple pretrained foundation models, our method achieves consistent, substantial gains over standard linear probes. Attention heatmaps further reveal that tasks different from the pre-training domain benefit most from intermediate representations. Overall, our findings underscore the value of intermediate layer information and demonstrate a principled, task aware approach for unlocking their potential in probing-based adaptation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09322.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09322",
    "published": "2026-01-14T09:50:09Z",
    "updated": "2026-01-14T09:50:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于注意力机制的多层融合方法，动态整合视觉变换器各层表示，以提升下游任务适应性能。",
      "motivation": "本研究动机在于大规模基础模型高效适应下游任务是一个关键挑战。线性探测方法虽计算高效，但仅利用最后一层表示，而任务相关信息实际上分布于整个网络层次中，而不是只编码在末层。现有线性探测忽略了中间层信息，限制了模型性能，尤其在与预训练领域不同的任务中，这一不足更为突出。因此，需要一种能充分利用网络多层次信息的适应策略来改善这一问题。",
      "method": "研究方法提出一个注意力探测机制，动态融合视觉变换器中所有层的表示。该机制通过学习过程，自动识别与目标任务最相关的层，并结合低层结构线索与高层语义抽象，以实现更有效的特征提取。实验基于20个多样化数据集和多个预训练基础模型进行，关键创新在于利用注意力权重实现任务感知的多层融合，从而优化下游任务适应。",
      "result": "主要实验结果显示，在20个不同数据集和多个预训练模型上，该方法相比标准线性探测取得了持续且显著的性能提升，具体增益在摘要中未提供精确数值。通过注意力热图分析，进一步发现与预训练领域不同的任务最受益于中间层表示，验证了该方法在整合多层次信息方面的有效性。这些结果证明了多层融合策略优于传统的最后一层探测基线。",
      "conclusion": "论文结论强调了中间层信息在模型适应中的价值，并提出一种原则性的任务感知方法，通过注意力机制动态融合表示来释放其潜力。学术上，这为视觉变换器的适应策略提供了新视角，促进更高效的下游任务处理。应用上，有助于提升领域外任务的性能。未来工作可进一步探索注意力机制的优化或扩展到其他模型架构，摘要未明确说明具体局限性。",
      "tags": [
        "Vision Transformers",
        "Attentive Probing",
        "Multilayer Fusion",
        "Intermediate Representations",
        "Linear Probing"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:51.000964Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09316",
    "title": "Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction",
    "authors": [
      "Xinming Fang",
      "Chaoyan Huang",
      "Juncheng Li",
      "Jun Wang",
      "Jun Shi",
      "Guixu Zhang"
    ],
    "abstract": "Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09316.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09316",
    "published": "2026-01-14T09:40:34Z",
    "updated": "2026-01-14T09:40:34Z",
    "comment": "44 pages, 12 figures, 7 tables",
    "light_analysis": {
      "overview": "本论文提出一个频率误差引导的联合优化框架，用于多对比 MRI 重建，通过条件扩散模型和深度展开技术提高重建效率和可解释性。",
      "motivation": "磁共振成像（MRI）在临床诊断中至关重要，但受限于长采集时间和运动伪影。多对比 MRI 重建通过利用全采样参考扫描的互补信息，有望加速成像过程，但现有方法存在三个主要局限性：参考融合策略过于浅层（如简单拼接），未充分利用参考对比度的互补信息，以及使用固定的欠采样模式，这限制了重建质量和效率。因此，需要一种新方法来克服这些不足，优化欠采样和重建过程。",
      "method": "本研究提出一个频率误差引导的重建框架。首先，使用条件扩散模型学习频率误差先验（FEP），以捕捉参考扫描的互补信息。然后，构建一个统一框架，联合优化欠采样模式和重建网络。重建模型采用模型驱动的深度展开框架，联合利用频率域和图像域信息。此外，引入空间对齐模块和参考特征分解策略，以增强重建质量，并桥接基于模型的优化与数据驱动学习，从而提高物理可解释性。关键创新点包括 FEP 的集成和联合优化策略。",
      "result": "论文在多种成像模态、加速率（4-30倍）和不同采样方案上进行了综合验证。实验结果表明，提出的方法在定量指标和视觉质量上均优于当前最先进的方法，展现出更高的重建准确性和更好的细节保留。虽然没有提供具体的准确率提升数字，但摘要强调了一致的优越性，表明该方法在不同设置下都能有效改进性能。",
      "conclusion": "本论文的主要贡献是提出了一种频率误差引导的联合优化框架，有效解决了多对比 MRI 重建中的关键问题。通过集成条件扩散模型、深度展开和空间对齐模块，该方法提高了重建质量和物理可解释性。在学术上，为 MRI 重建提供了新的优化思路；在实际应用中，有望加速 MRI 采集，减少运动伪影，改善临床诊断。摘要未明确说明具体局限性，未来工作可探索更广泛的模态或优化计算效率。",
      "tags": [
        "Multi-Contrast MRI Reconstruction",
        "Conditional Diffusion Model",
        "Deep Unfolding",
        "Frequency Error Prior",
        "Under-sampling Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:15.432327Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09313",
    "title": "Understanding or Memorizing? A Case Study of German Definite Articles in Language Models",
    "authors": [
      "Jonathan Drechsel",
      "Erisa Bytyqi",
      "Steffen Herbold"
    ],
    "abstract": "Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09313.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09313",
    "published": "2026-01-14T09:31:41Z",
    "updated": "2026-01-14T09:31:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过梯度可解释性方法揭示语言模型在处理德语定冠词时更多依赖记忆关联而非抽象语法规则。",
      "motivation": "语言模型在语法一致任务中表现优异，但其内部机制是否基于规则泛化尚不明确。本研究针对德语定冠词的单数形式，探究模型是理解语法规则还是仅记忆特定模式。这一问题对AI可解释性和语言理解至关重要，现有方法往往缺乏对模型内部编码方式的深入分析，难以区分泛化与记忆，因此需要新方法揭示底层机制。",
      "method": "研究采用GRADIEND，一种基于梯度的可解释性方法，学习德语定冠词在性和格特定转换中的参数更新方向。该方法通过分析模型参数的变化，识别影响冠词形式的神经元，关键创新在于将梯度技术应用于具体语言现象，以揭示语法处理的内在机制。摘要未明确说明使用的具体模型或数据集，但推断可能涉及常见语言模型架构如Transformer。",
      "result": "实验发现，为特定性-格冠词转换学习的参数更新经常影响其他无关性-格设置，且在不同设置中最受影响的神经元存在显著重叠。这表明模型的编码方式并非严格基于抽象规则，而是部分依赖记忆关联。摘要未提供具体性能指标如准确率，但通过神经元分析提供了定性证据，与基线方法对比，结果支持模型更倾向于记忆而非规则泛化的观点。",
      "conclusion": "研究的主要贡献是通过梯度方法揭示语言模型在处理德语定冠词时依赖记忆而非规则，对理解模型的语言学习机制有重要意义。学术价值在于推动AI可解释性研究，特别是在语法任务中的应用。未来工作可扩展至其他语言现象或改进方法量化记忆程度，潜在局限性包括分析范围局限和未涉及具体模型性能提升。",
      "tags": [
        "Language Models",
        "Gradient-based Interpretability",
        "Grammatical Agreement",
        "German Language Processing",
        "Neuron Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:12.904202Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09304",
    "title": "Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data",
    "authors": [
      "Sota Sugawara",
      "Yuji Kawamata",
      "Akihiro Toyoda",
      "Tomoru Nakayama",
      "Yukihiko Okada"
    ],
    "abstract": "Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09304.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09304",
    "published": "2026-01-14T09:14:44Z",
    "updated": "2026-01-14T09:14:44Z",
    "comment": "9 pages, 3 figures",
    "light_analysis": {
      "overview": "提出DC-CFL单轮聚类联邦学习框架，通过数据协作分析处理非IID数据，在一轮通信中完成客户端聚类和模型学习。",
      "motivation": "联邦学习允许分布式学习而不共享数据，但当客户端数据分布差异大（非IID）时，模型性能下降。聚类联邦学习通过分组相似客户端来改善性能，但现有方法依赖多轮通信进行聚类和更新，在通信受限场景（如移动网络）中不实用。因此，需要开发高效单轮框架以降低通信成本，解决非IID数据下的协作学习问题。",
      "method": "DC-CFL框架结合数据协作分析，在一轮通信中完成客户端聚类和聚类学习。首先，通过计算客户端标签分布的总变差距离量化相似性；然后，使用层次聚类算法将相似客户端分组；最后，通过数据协作分析在每个聚类内训练模型，共享中间表示而非原始数据。关键技术包括相似性度量和聚类策略。",
      "result": "在多个开放数据集上的实验显示，DC-CFL在代表性非IID条件下，实现了与多轮基线方法可比较的准确性，同时仅需一轮通信。这显著减少了通信开销，提高了效率。具体性能指标摘要未明确给出数字，但实验结果验证了方法在保持性能的同时优化通信轮次。",
      "conclusion": "DC-CFL的主要贡献是提出高效单轮聚类联邦学习框架，适用于非IID数据环境，减少通信需求。这具有重要实际应用价值，尤其在通信资源受限的边缘计算中。学术上扩展了联邦学习和聚类方法。未来工作可能包括优化相似性算法或探索更多场景，摘要未详细讨论局限性。",
      "tags": [
        "Federated Learning",
        "Clustered Federated Learning",
        "Data Collaboration Analysis",
        "Total Variation Distance",
        "Hierarchical Clustering"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:45.288316Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09298",
    "title": "Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain",
    "authors": [
      "Lianying Chao",
      "Haoran Cai",
      "Xubin Li",
      "Kai Zhang",
      "Sijie Wu",
      "Rui Xu"
    ],
    "abstract": "In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09298.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09298",
    "published": "2026-01-14T09:01:46Z",
    "updated": "2026-01-14T09:01:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出多阶段渐进训练策略，训练出仅7B参数的ICT领域图像标注模型DICModel，在性能上超越大型模型，实现了领域知识与多模态能力的结合。",
      "motivation": "在ICT行业，高效提取领域知识对于训练特定LLM和构建检索增强系统至关重要，但传统方法仅限于文本解析，无法处理图像中的知识；而多模态LLM虽能理解图像，却缺乏领域特定知识，导致图像中隐藏的领域信息提取不完整。因此，本研究旨在解决这一缺口，通过结合领域知识和多模态能力，开发一种高效图像标注方法，以促进ICT系统的知识整合与应用。",
      "method": "本文采用多阶段渐进训练策略来训练Domain-specific Image Captioning Model (DICModel)。第一阶段利用Mermaid工具和LLMs合成约7K图像-文本对进行监督微调；第二阶段由ICT专家手动标注约2K图像-文本对进一步微调；第三阶段通过专家和LLMs联合合成约1.5K视觉问答数据进行指令微调，逐步整合合成数据和专家知识，增强模型在ICT领域的图像理解与标注能力。",
      "result": "实验结果显示，DICModel仅用7B参数，在性能上优于参数更大的32B SOTA模型。具体地，与7B和32B参数的基线模型相比，BLEU分数分别提升约56.8%和20.8%；在ICT专家构建的客观题测试中，准确率比Qwen2.5-VL 32B高出1%，验证了其在领域特定图像标注任务上的高效性和准确性，展示了小参数模型超越大型模型的优势。",
      "conclusion": "本工作成功开发了ICT领域的特定图像标注模型DICModel，通过多阶段训练策略显著提升性能，贡献包括提出高效训练方法和构建标准评估系统，有助于促进多模态模型在专业领域的应用。实际应用价值在于支持ICT行业的领域知识提取，学术上为结合领域知识和多模态能力提供新思路；未来可扩展至其他领域或优化数据合成策略，进一步探索模型泛化能力。",
      "tags": [
        "Multi-Modal LLM",
        "Image Captioning",
        "Supervised Fine-Tuning",
        "Domain-specific",
        "Visual Question Answering"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:01.440712Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09293",
    "title": "Policy-Based Reinforcement Learning with Action Masking for Dynamic Job Shop Scheduling under Uncertainty: Handling Random Arrivals and Machine Failures",
    "authors": [
      "Sofiene Lassoued",
      "Stefan Lier",
      "Andreas Schwung"
    ],
    "abstract": "We present a novel framework for solving Dynamic Job Shop Scheduling Problems under uncertainty, addressing the challenges introduced by stochastic job arrivals and unexpected machine breakdowns. Our approach follows a model-based paradigm, using Coloured Timed Petri Nets to represent the scheduling environment, and Maskable Proximal Policy Optimization to enable dynamic decision-making while restricting the agent to feasible actions at each decision point. To simulate realistic industrial conditions, dynamic job arrivals are modeled using a Gamma distribution, which captures complex temporal patterns such as bursts, clustering, and fluctuating workloads. Machine failures are modeled using a Weibull distribution to represent age-dependent degradation and wear-out dynamics. These stochastic models enable the framework to reflect real-world manufacturing scenarios better. In addition, we study two action-masking strategies: a non-gradient approach that overrides the probabilities of invalid actions, and a gradient-based approach that assigns negative gradients to invalid actions within the policy network. We conduct extensive experiments on dynamic JSSP benchmarks, demonstrating that our method consistently outperforms traditional heuristic and rule-based approaches in terms of makespan minimization. The results highlight the strength of combining interpretable Petri-net-based models with adaptive reinforcement learning policies, yielding a resilient, scalable, and explainable framework for real-time scheduling in dynamic and uncertain manufacturing environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09293.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09293",
    "published": "2026-01-14T08:53:46Z",
    "updated": "2026-01-14T08:53:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种结合Petri Nets和强化学习的框架，用于不确定性下的动态作业车间调度，通过动作掩码处理随机作业到达和机器故障。",
      "motivation": "动态作业车间调度在制造环境中面临随机作业到达和机器故障的不确定性挑战，这些问题导致传统启发式和规则方法难以自适应处理动态变化，调度效率低下且缺乏弹性。现有方法通常无法有效模拟真实工业场景的复杂时间模式和退化动态，限制了实时决策能力。因此，需要开发一种能处理不确定性并动态调整的策略，以提高生产系统的鲁棒性和效率。",
      "method": "本方法采用模型化范式，使用Coloured Timed Petri Nets表示调度环境，并应用Maskable Proximal Policy Optimization进行强化学习决策。关键创新包括使用Gamma分布模拟动态作业到达的复杂时间模式（如突发和波动），以及Weibull分布表示机器故障的年龄依赖退化。此外，研究了两种动作掩码策略：非梯度法覆盖无效动作概率，梯度法在策略网络中分配负梯度，以确保动作可行性。框架结合了可解释模型与自适应学习，以增强调度决策的精确性。",
      "result": "在动态作业车间调度基准上进行了广泛实验，结果表明该方法在最小化制造时间方面 consistently outperforms 传统启发式和规则方法。通过结合Petri-net模型与自适应强化学习策略，框架显著提升了调度性能，突显了优于基线方法的优势，但具体数据如准确率或效率改进未在摘要中提供，仅说明效果被证实。",
      "conclusion": "本研究的贡献在于提出了一种弹性和可扩展的框架，用于动态和不确定制造环境中的实时调度，结合了Petri Nets的模型化优势与强化学习的自适应能力。该框架提高了调度效率和系统韧性，具有实际工业应用价值，如优化生产流程。未来工作方向如扩展到其他不确定性因素或更多调度场景，摘要未明确说明，但可合理推断进一步探索。",
      "tags": [
        "Reinforcement Learning",
        "Proximal Policy Optimization",
        "Petri Nets",
        "Action Masking",
        "Dynamic Job Shop Scheduling"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:56.422719Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09285",
    "title": "Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction",
    "authors": [
      "Mianzhi Pan",
      "JianFei Li",
      "Peishuo Liu",
      "Botian Wang",
      "Yawen Ouyang",
      "Yiming Rong",
      "Hao Zhou",
      "Jianbing Zhang"
    ],
    "abstract": "Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09285.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09285",
    "published": "2026-01-14T08:45:07Z",
    "updated": "2026-01-14T08:45:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出MOF-LLM框架，通过整合空间感知训练和强化学习，提升大语言模型在金属有机框架结构预测中的空间推理能力。",
      "motivation": "MOFs作为多孔晶体材料，在碳捕获和药物传递等应用领域具有重要价值，但准确预测其三维结构是一项关键挑战。大型语言模型在晶体生成任务中展现出潜力，然而由于MOFs的高原子复杂性，其应用受到限制。现有方法，如去噪或基于LLM的技术，在处理MOFs复杂结构时效果不佳，因此需要发展专门方法来增强空间推理能力，以促进MOFs的科学研究和实际应用。",
      "method": "本研究提出MOF-LLM，首个专门用于块级MOF结构预测的大型语言模型框架。方法结合空间感知持续预训练来增强空间理解，结构监督微调以优化生成准确性，以及匹配驱动强化学习提高性能。关键创新点包括引入空间先验和使用Soft Adaptive Policy Optimization算法优化结构稳定性。该方法基于Qwen-3 8B模型进行适配，专注于模块化组装任务，整合了CPT、SFT和RL三个训练阶段。",
      "result": "综合实验结果表明，MOF-LLM在MOF结构预测任务中，超越了基于去噪和基于大型语言模型的现有方法。同时，该方法展示了优越的采样效率，显著提高了预测的准确性和效率。性能指标的具体数值在摘要中未明确说明，但与基线方法相比，有明显的改进和超越，证实了框架的有效性。",
      "conclusion": "本研究的主要贡献是开发了MOF-LLM框架，有效增强了大语言模型在MOF结构预测中的空间推理能力。学术上，该工作拓展了大型语言模型在复杂材料科学中的应用，为类似问题提供新方法；实际上，加速了MOFs的设计和优化，促进了其在环保和医疗领域的应用。未来工作可探索扩展到其他晶体材料，或进一步优化训练算法以提高性能。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Spatial Reasoning",
        "Metal-Organic Frameworks",
        "Soft Adaptive Policy Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:04.677476Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09282",
    "title": "Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing",
    "authors": [
      "Leszek Sliwko",
      "Jolanta Mizeria-Pietraszko"
    ],
    "abstract": "Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.",
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.LG",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09282.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09282",
    "published": "2026-01-14T08:36:21Z",
    "updated": "2026-01-14T08:36:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一个基于大型语言模型的语义软亲和性调度方法，通过自然语言简化集群工作负载分配。",
      "motivation": "集群工作负载分配通常需要复杂配置（如Kubernetes的标签和选择器），导致可用性差距，使用户难以高效管理。现有调度方法依赖于繁琐的技术语法，不够直观且易出错，限制了非专家的使用。本研究旨在通过自然语言处理技术，使调度过程更易于访问，降低配置复杂度，提升用户体验和系统管理效率，解决传统调度中的可用性问题。",
      "method": "研究方法开发了一个原型系统，将大型语言模型集成到Kubernetes调度器中作为扩展器。系统利用LLM解析自然语言注释来推断软亲和性偏好，实现意图驱动的调度。关键创新包括集群状态缓存和意图分析器（基于AWS Bedrock调用LLM），使用户能以自然语言表达需求，无需掌握复杂配置语法，从而简化调度决策过程。",
      "result": "实验结果表明，在评估数据集上，高性能LLM（如Amazon Nova Pro/Premier和Mistral Pixtral Large）的解析准确率超过95%（Subset Accuracy），显著优于基线引擎。调度质量测试覆盖六种场景，原型在复杂和量化情境中表现优越，能有效处理冲突软偏好，并与标准Kubernetes配置相比实现相同或更好的放置效果，验证了语义软亲和性调度的实用性。",
      "conclusion": "本研究证实了利用大型语言模型实现语义软亲和性调度的可行性，为简化工作负载编排提供创新途径。学术价值在于扩展NLP在系统调度领域的应用，实际价值是提升集群管理的用户友好性。局限性包括LLM同步延迟，建议未来采用异步处理以提高生产就绪度，并为进一步优化指明方向。",
      "tags": [
        "Large Language Model",
        "Natural Language Processing",
        "Kubernetes",
        "Semantic Scheduling",
        "Soft Affinity"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:13.381825Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09281",
    "title": "STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models",
    "authors": [
      "Jingjing Zhou",
      "Gaoxiang Cong",
      "Li Su",
      "Liang Li"
    ],
    "abstract": "Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09281.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09281",
    "published": "2026-01-14T08:35:23Z",
    "updated": "2026-01-14T08:35:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了STaR框架，这是一种无参数的推理时轨迹感知遗忘方法，旨在保护大型推理模型中的隐私。",
      "motivation": "Large Reasoning Models（LRMs）在自动多步推理方面取得了进展，但生成复杂的Chain-of-Thought（CoT）轨迹可能导致敏感信息泄露，带来严重的隐私风险。现有的Large Language Models（LLMs）遗忘方法通常只关注修改最终答案，无法处理中间推理步骤中的敏感内容，导致隐私持续泄漏和安全性能下降。因此，迫切需要一种更全面的方法来解决LRMs中的隐私保护问题，尤其是在推理过程中确保敏感信息的有效移除。",
      "method": "研究方法提出了Sensitive Trajectory Regulation（STaR）框架，这是一个无参数的推理时遗忘方法。首先，通过语义感知检测识别敏感内容；其次，注入全局安全约束，通过安全提示前缀实现；接着，执行轨迹感知抑制，动态阻塞整个推理链中的敏感内容；最后，应用令牌级自适应过滤，防止生成精确或改写后的敏感令牌。该框架无需修改模型参数，仅需在推理时进行调整，从而实现了对整个推理过程的隐私保护。",
      "result": "实验在R-TOFU基准上进行，结果表明STaR能够实现全面且稳定的遗忘，显著减少隐私泄漏，同时保持较高的模型效用，效用损失最小化。摘要未明确说明具体性能指标，但通过多解码一致性评估和多粒度成员推理攻击评估，验证了STaR在多个方面优于现有方法，提供了更强的隐私保护。",
      "conclusion": "论文的主要贡献是提出了STaR框架，有效解决了大型推理模型中的隐私保护挑战。通过轨迹感知的遗忘方法，不仅保护了最终答案，还确保了整个推理链的隐私安全。该研究具有重要的学术价值，为大型模型的隐私保护领域提供了新的思路和评估标准；在实际应用中，可以提高模型在隐私敏感场景下的安全性和可靠性。未来工作可能包括扩展到更多模型类型和优化评估方法。",
      "tags": [
        "Large Reasoning Models",
        "Unlearning",
        "Privacy Protection",
        "Chain-of-Thought",
        "Inference-Time"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:40.540360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09280",
    "title": "ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering",
    "authors": [
      "Chaerin Lee",
      "Sohee Park",
      "Hyunsik Na",
      "Daseon Choi"
    ],
    "abstract": "Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09280.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09280",
    "published": "2026-01-14T08:33:14Z",
    "updated": "2026-01-14T08:33:14Z",
    "comment": "18 pages, 2 figures. Preprint",
    "light_analysis": {
      "overview": "ReGraM 提出一种区域优先知识图谱推理框架，用于医学问答，通过构建查询对齐子图并局部推理，显著提升事实准确性和稳定性。",
      "motivation": "现有医学问答方法常结合大语言模型与生物医学知识图谱，但依赖遍历整个图谱或大规模检索，导致噪声干扰和多跳推理不稳定。在医疗等专业领域，知识图谱中并非所有关系都同等有用，现有方法假设平等使用关系不切实际。因此，需一种能精准识别查询相关证据子集的方法，以提高推理的准确性和效率。ReGraM 旨在解决这一核心问题，避免不必要的信息访问。",
      "method": "ReGraM 框架的核心是区域优先推理，首先构建查询对齐的知识子图，将推理约束于该局部区域。然后，在多证据意识模式下进行逐步跳步推理，确保推理过程专注于最相关的部分。这种方法创新性地放弃了全局遍历的假设，通过子图构造和分步推理优化了知识利用。具体技术包括对齐子图生成和基于证据的推理步骤，提高了效率与精确度。",
      "result": "在七个医学问答基准测试中，ReGraM 显著优于基线 KGARevion，实现了多项性能提升：多选题准确率绝对增益 8.04%，简答题增益 4.50%，幻觉率降低 42.9%。消融和定性分析表明，区域构造与跳步推理的对齐是改进的主要驱动力。这些结果验证了区域优先推理在减少噪声和提高一致性方面的有效性。",
      "conclusion": "ReGraM 证明了区域优先知识图谱推理是提高医学问答事实准确性的有效范式，为结合大语言模型与知识图谱的复杂推理任务提供了新方法。研究具有重要学术价值，推动了领域特定推理技术的发展；应用上，能提升医疗AI系统的可靠性。未来工作可探索该方法在其他专业领域的扩展，并优化推理效率或处理更复杂的查询类型。摘要未明确说明局限性，但潜在方向包括适应更多场景。",
      "tags": [
        "Medical Question Answering",
        "Knowledge Graph Reasoning",
        "Large Language Models",
        "Biomedical Knowledge Graphs",
        "Region-First Reasoning"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:35.969392Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09278",
    "title": "M$^3$Searcher: Modular Multimodal Information Seeking Agency with Retrieval-Oriented Reasoning",
    "authors": [
      "Xiaohan Yu",
      "Chao Feng",
      "Lang Mei",
      "Chong Chen"
    ],
    "abstract": "Recent advances in DeepResearch-style agents have demonstrated strong capabilities in autonomous information acquisition and synthesize from real-world web environments. However, existing approaches remain fundamentally limited to text modality. Extending autonomous information-seeking agents to multimodal settings introduces critical challenges: the specialization-generalization trade-off that emerges when training models for multimodal tool-use at scale, and the severe scarcity of training data capturing complex, multi-step multimodal search trajectories. To address these challenges, we propose M$^3$Searcher, a modular multimodal information-seeking agent that explicitly decouples information acquisition from answer derivation. M$^3$Searcher is optimized with a retrieval-oriented multi-objective reward that jointly encourages factual accuracy, reasoning soundness, and retrieval fidelity. In addition, we develop MMSearchVQA, a multimodal multi-hop dataset to support retrieval centric RL training. Experimental results demonstrate that M$^3$Searcher outperforms existing approaches, exhibits strong transfer adaptability and effective reasoning in complex multimodal tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09278.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09278",
    "published": "2026-01-14T08:27:40Z",
    "updated": "2026-01-14T08:27:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "M$^3$Searcher是一个模块化的多模态信息寻求代理，通过检索导向的推理和多目标奖励优化，解决了多模态信息获取的挑战。",
      "motivation": "现有DeepResearch-style代理主要局限于文本模态，难以处理现实世界中的多模态信息，限制了自主信息获取系统的应用。多模态设置引入了专业化-泛化权衡和数据稀缺的挑战，导致现有方法在复杂多模态搜索任务中表现不足。因此，开发能够有效整合多模态输入并优化检索过程的新方法至关重要，以应对日益增长的多模态环境需求。",
      "method": "M$^3$Searcher采用模块化架构，明确将信息获取与答案推导过程解耦，以独立优化每个模块。核心创新在于引入检索导向的多目标奖励函数，该函数同时鼓励事实准确性、推理健全性和检索保真度，并使用强化学习进行训练。此外，研究者开发了MMSearchVQA数据集，这是一个多模态多跳数据集，专门用于支持以检索为中心的强化学习，以提升代理在多模态任务中的性能。",
      "result": "实验结果显示，M$^3$Searcher在性能上优于现有方法，展现出强大的迁移适应性和在复杂多模态任务中的有效推理能力。尽管摘要未明确说明具体指标如准确率或效率改进，但与基线方法相比，该代理表现出更好的泛化性和鲁棒性，证实了检索导向优化和模块化设计的有效性。",
      "conclusion": "M$^3$Searcher的主要贡献是提出了一种模块化多模态信息寻求代理，通过检索导向的奖励机制解决了多模态信息获取的挑战。其学术价值在于整合了强化学习与多模态处理，为自主代理研究提供了新方向；实际应用中，可提升多模态搜索任务的效率和准确性。未来工作可能包括扩展数据集或探索更多应用场景，尽管摘要未明确说明局限性。",
      "tags": [
        "Modular Agents",
        "Multimodal Search",
        "Retrieval-Oriented Reasoning",
        "Reinforcement Learning",
        "Multimodal Dataset"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:55.896651Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09274",
    "title": "$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation",
    "authors": [
      "Jian Zhang",
      "Yu He",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Kai He",
      "Fangzhi Xu",
      "Qika Lin",
      "Jun Liu"
    ],
    "abstract": "Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \\textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09274.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09274",
    "published": "2026-01-14T08:17:41Z",
    "updated": "2026-01-14T08:17:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了 $A^3$-Bench 基准，用于通过锚点和吸引子激活来评估记忆驱动的科学推理机制，填补了现有基准测试在评估记忆机制方面的空白。",
      "motivation": "研究动机在于，尽管科学推理依赖于逻辑推断和记忆激活，但现有基准测试主要评估最终答案或推理步骤的连贯性，忽视了记忆驱动的机制，如通过锚点和吸引子激活来驱动推理。记忆能高效重用知识并增强推理的稳定性和一致性，但当前评估方法无法捕捉这一点，这限制了AI系统模拟人类科学推理的能力。因此，需要新的基准来专门评估记忆驱动的推理过程，以促进对这一关键机制的深入理解。",
      "method": "研究方法包括三个核心部分：首先，使用SAPM过程（包括主题、锚点和吸引子、问题、记忆发展）对2,198个跨领域科学推理问题进行详细注释。其次，提出了一个双尺度记忆评估框架，利用锚点和吸引子作为评估单元，并引入了AAUI（锚点-吸引子利用指数）指标来衡量记忆激活率。最后，通过基于多种基础模型和推理范式的实验来验证基准的有效性，确保框架能客观反映记忆驱动推理的性能。",
      "result": "实验结果表明，$A^3$-Bench 基准被成功验证，并分析了记忆激活对科学推理性能的影响。摘要未明确说明具体的性能指标数据，如准确率提升或效率改进，但通过基准的应用，提供了对记忆驱动推理机制的洞见，例如揭示了锚点和吸引子激活如何增强推理的连贯性和稳定性。基准帮助验证了记忆激活在推理中的重要性，为后续研究提供了实用工具和分析基础。",
      "conclusion": "论文的主要贡献是提出了 $A^3$-Bench 基准和评估框架，用于评估记忆驱动的科学推理，填补了现有研究的空白。其学术价值在于为研究记忆机制在推理中的作用提供了新工具，实际应用价值在于可能促进AI系统科学推理能力的提升。未来工作可以包括扩展基准到更多领域，或者结合更先进的机器学习模型深入探索记忆激活的机制和局限性。",
      "tags": [
        "Benchmarking",
        "Scientific Reasoning",
        "Memory Activation",
        "Anchor-Attractor Activation",
        "Evaluation Framework"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:11.721833Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09270",
    "title": "MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus",
    "authors": [
      "Yexing Du",
      "Kaiyuan Liu",
      "Bihe Zhang",
      "Youcheng Pan",
      "Bo Yang",
      "Liangyu Huo",
      "Xiyuan Zhang",
      "Jian Xie",
      "Daojing He",
      "Yang Xiang",
      "Ming Liu",
      "Bin Qin"
    ],
    "abstract": "With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09270.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09270",
    "published": "2026-01-14T08:05:16Z",
    "updated": "2026-01-14T08:05:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出MCGA语料库，用于评估多模态大语言模型在中文古典文学音频任务上的多维能力。",
      "motivation": "随着多模态大语言模型的快速发展，其在中文古典文学研究中的潜力备受关注。然而，现有研究主要集中于文本和视觉模态，音频语料库在该领域仍严重不足。这种缺失限制了模型在音频处理方面的发展，阻碍了跨模态研究的全面性。因此，本研究旨在填补这一空白，通过构建专门针对古典中文文学的音频语料库，解决音频模态未被充分探索的问题，并为提升MLLMs的音频理解和生成能力提供基础资源。",
      "method": "论文提出MCGA语料库，涵盖多种古典中文文学流派，并定义了六个任务：自动语音识别、语音到文本翻译、语音情感描述、口语问答、语音理解和语音推理。方法的核心是构建多任务音频数据集，整合不同文学类型以评估MLLMs的音频处理能力。关键技术特色在于任务的多维性和领域特定性，强调在古典文学背景下的音频挑战。数据集设计包括多样化的音频样本，但模型架构细节摘要未明确说明，仅提到评估了十个MLLMs。",
      "result": "在MCGA测试集上评估了十个多模态大语言模型，实验结果显示当前模型处理该语料库时仍面临重大挑战，表明在古典中文文学音频任务上的性能有限。此外，论文引入了新评估指标：一个用于语音情感描述的度量，另一个用于衡量MLLMs在语音和文本能力之间的一致性。这些指标有助于更全面评估模型，但摘要未提供具体数值如准确率或效率改进，仅强调模型在复杂音频任务上的不足之处。与基线方法的对比摘要未明确说明。",
      "conclusion": "本研究的核心贡献是发布MCGA语料库和代码，为学术界提供了首个专门针对中文古典文学的音频多任务评估资源。这促进了多模态大语言模型在音频能力上的发展，并拓宽了中文古典文学研究的跨模态应用。学术价值在于填补音频语料库领域的空白，实际应用价值在于为未来模型优化提供基准。潜在局限性包括语料库规模和多样性，未来工作可扩展更多文学类型和优化模型性能。",
      "tags": [
        "Multimodal Large Language Models",
        "Automatic Speech Recognition",
        "Speech-to-Text Translation",
        "Spoken Question Answering",
        "Speech Emotion Captioning"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:55.889431Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09269",
    "title": "RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering",
    "authors": [
      "Wencheng Ye",
      "Liang Peng",
      "Xiaoyang Yuan",
      "Yi Bin",
      "Pengpeng Zeng",
      "Hengyu Jin",
      "Heng Tao Shen"
    ],
    "abstract": "Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09269.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09269",
    "published": "2026-01-14T08:04:33Z",
    "updated": "2026-01-14T08:04:33Z",
    "comment": null,
    "light_analysis": {
      "overview": "RISER是一种基于路由器的自适应激活引导框架，通过动态组合推理向量提升大语言模型推理效率和准确率。",
      "motivation": "当前大语言模型在特定领域推理中常依赖需要参数更新的训练密集型方法，效率低下且适应性差。激活引导作为参数高效的替代，但现有方法多为静态手动干预，无法应对复杂推理的动态变化。因此，研究旨在开发一种自适应机制，以灵活引导模型激活，弥补现有技术对动态推理任务处理的不足，提升模型的可控性和性能。",
      "method": "论文提出RISER框架，核心是构建可重用推理向量库，并设计轻量级路由器根据输入动态组合这些向量，形成自适应干预策略。路由器通过强化学习在任务级奖励下优化，以涌现和组合方式激活潜在的认知原语。该方法无需修改模型参数，实现即插即用的激活空间引导，关键创新在于动态、自适应的向量组合机制，提升了推理的灵活性。",
      "result": "实验在七个多样化基准上进行，RISER相对于基础模型实现了3.4%至6.5%的平均零样本准确率提升。与思维链推理相比，RISER具有2-3倍的标记效率提升，同时保持稳健的准确率增益。进一步分析显示，框架能自主组合多个推理向量，形成可解释和精确的控制策略，验证了其高效性和适应性。",
      "conclusion": "研究贡献是提出RISER框架，实现了自适应激活引导，显著提升了大语言模型推理的效率和可控性。该工作具有重要学术价值，推动了参数高效干预方法的发展，并为实际应用如复杂推理任务提供了新途径。未来可探索优化路由器策略或扩展到更多领域，以进一步增强模型的泛化能力。",
      "tags": [
        "Large Language Model",
        "Activation Steering",
        "Reinforcement Learning",
        "Router-based Intervention",
        "Interpretable Control"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:56.170321Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09265",
    "title": "GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials",
    "authors": [
      "Bei Huang",
      "Yixin Chen",
      "Ruijie Lu",
      "Gang Zeng",
      "Hongbin Zha",
      "Yuru Pei",
      "Siyuan Huang"
    ],
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09265.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09265",
    "published": "2026-01-14T07:59:52Z",
    "updated": "2026-01-14T07:59:52Z",
    "comment": "16 pages",
    "light_analysis": {
      "overview": "GaussianFluent 提出了一个统一框架，结合生成模型和优化物理模拟，用于高保真动态场景的模拟和渲染，特别解决了脆性断裂问题。",
      "motivation": "该研究旨在解决 3D 高斯泼溅（3DGS）在动态场景模拟中脆性断裂未被充分处理的问题。现有方法主要针对柔软、可变形材料，缺乏对脆性断裂的有效模拟，这源于两个关键障碍：GS 表示中缺少具有连贯纹理的体积内部，以及缺乏针对高斯的断裂感知模拟方法。这个问题对于实现真实动态渲染至关重要，尤其在混合材料物体中，限制了 VR 和机器人等应用的发展。",
      "method": "论文提出的 GaussianFluent 框架包括两个核心创新：首先，通过生成模型引导的内部高斯密度化，合成逼真的体积内部结构，以克服纹理不连贯的难题。其次，集成了优化的连续损伤材料点方法（CD-MPM），实现高速的脆性断裂模拟。该方法能够处理复杂场景，如混合材料物体和多阶段断裂传播，统一了模拟和渲染过程。",
      "result": "实验结果表明，GaussianFluent 能够实现逼真、实时的渲染，具有结构一致的内部结构，处理了包括混合材料物体和多阶段断裂在内的复杂场景。与先前方法相比，该框架取得了显著改进，实现了先前方法不可行的结果，但摘要未提供具体的性能指标数据。",
      "conclusion": "论文的主要贡献是开发了 GaussianFluent 框架，解决了脆性断裂在 3D 高斯泼溅中的模拟难题，推动了动态场景模拟的进展。该研究具有学术价值，如增强物理模拟的真实性，并具有实际应用潜力，如在 VR 和机器人领域。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "3D Gaussian Splatting",
        "Generative Models",
        "CD-MPM",
        "Mixed Materials",
        "Dynamic Simulation"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:00.648828Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09264",
    "title": "Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants",
    "authors": [
      "Ziyi Shi",
      "Xusen Guo",
      "Hongliang Lu",
      "Mingxing Peng",
      "Haotian Wang",
      "Zheng Zhu",
      "Zhenning Li",
      "Yuxuan Liang",
      "Xinhu Zheng",
      "Hai Yang"
    ],
    "abstract": "Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09264.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09264",
    "published": "2026-01-14T07:59:44Z",
    "updated": "2026-01-14T07:59:44Z",
    "comment": "20pages, 6 figures, a 60-page supporting material pdf file",
    "light_analysis": {
      "overview": "提出一个基于大型语言模型多智能体的政策制定框架，用于实现跨区域协调的大流行控制。",
      "motivation": "有效的大流行控制需跨行政区域协调政策制定，因区域间在流行病传播中相互依存，但人类驱动响应常碎片化、反应性，政策在孤立环境中制定，仅于疫情升级后调整，削弱了主动干预和全球缓解。本研究旨在解决政策协调性和前瞻性问题，通过AI技术优化控制效果，以弥补现有孤立政策制定方法在及时响应和跨区域协同方面的不足。",
      "method": "论文提出一个LLM多智能体政策制定框架，每个行政区域分配一个LLM智能体作为AI助手，能推理区域特定流行病动态并通过通信考虑跨区域依赖。框架整合真实世界数据（如COVID-19病例和流动性记录）、大流行演化模拟器及结构化智能体间通信，通过闭环模拟过程共同探索反事实干预场景，并合成协调政策决策，关键创新在于结合LLM推理与多智能体协调机制。",
      "result": "使用美国2020年4月至12月州级COVID-19数据、真实流动性记录和观察政策干预进行验证。与真实世界结果相比，框架在个体州级减少累计感染和死亡高达63.7%和40.1%；跨州汇总时减少39.0%和27.0%。这些结果显著优于基线（真实响应），展示了框架在提升大流行控制效果和协调政策制定方面的实质性优势。",
      "conclusion": "论文的主要贡献是开发了LLM多智能体框架，实现协调大流行控制政策制定，能显著降低感染和死亡人数。学术上，推动了多智能体系统与LLM结合的应用研究；实际上，为公共卫生决策提供了AI辅助工具。未来工作可扩展至其他流行病或优化通信机制，以进一步提升框架的泛化能力和性能。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Systems",
        "Pandemic Control",
        "Policy Making",
        "Simulation"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:55.115606Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09263",
    "title": "BrainSegNet: A Novel Framework for Whole-Brain MRI Parcellation Enhanced by Large Models",
    "authors": [
      "Yucheng Li",
      "Xiaofan Wang",
      "Junyi Wang",
      "Yijie Li",
      "Xi Zhu",
      "Mubai Du",
      "Dian Sheng",
      "Wei Zhang",
      "Fan Zhang"
    ],
    "abstract": "Whole-brain parcellation from MRI is a critical yet challenging task due to the complexity of subdividing the brain into numerous small, irregular shaped regions. Traditionally, template-registration methods were used, but recent advances have shifted to deep learning for faster workflows. While large models like the Segment Anything Model (SAM) offer transferable feature representations, they are not tailored for the high precision required in brain parcellation. To address this, we propose BrainSegNet, a novel framework that adapts SAM for accurate whole-brain parcellation into 95 regions. We enhance SAM by integrating U-Net skip connections and specialized modules into its encoder and decoder, enabling fine-grained anatomical precision. Key components include a hybrid encoder combining U-Net skip connections with SAM's transformer blocks, a multi-scale attention decoder with pyramid pooling for varying-sized structures, and a boundary refinement module to sharpen edges. Experimental results on the Human Connectome Project (HCP) dataset demonstrate that BrainSegNet outperforms several state-of-the-art methods, achieving higher accuracy and robustness in complex, multi-label parcellation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09263.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09263",
    "published": "2026-01-14T07:58:36Z",
    "updated": "2026-01-14T07:58:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "BrainSegNet是一个新颖框架，通过集成Segment Anything Model和U-Net来增强全脑MRI分割的精度，实现高效的多标签区域划分。",
      "motivation": "全脑MRI分割是神经影像领域的关键任务，旨在将大脑细分成多个不规则小区域，用于脑科学研究和临床诊断。传统模板注册方法效率低且精度不足，而现有深度学习模型如SAM虽具泛化能力，但未针对脑分割的高精度需求优化。因此，开发专门方法以提高分割准确性和鲁棒性至关重要，以应对复杂大脑结构的挑战。",
      "method": "BrainSegNet基于SAM框架进行改造，通过集成U-Net的跳跃连接和专用模块来增强模型。核心创新包括：混合编码器结合U-Net跳跃连接和SAM的变换器块；多尺度注意力解码器采用金字塔池化处理不同大小结构；边界细化模块提升边缘清晰度。该方法使用Human Connectome Project（HCP）数据集进行训练，实现精细的解剖学分割。",
      "result": "在Human Connectome Project（HCP）数据集上的实验结果显示，BrainSegNet在准确性和鲁棒性方面优于多个先进的分割方法，适用于复杂的多标签脑分割任务。摘要未明确说明具体性能指标如准确率数值，但指出该方法在对比中表现更佳，增强了分割的可靠性。",
      "conclusion": "BrainSegNet通过优化SAM模型，显著提升了全脑MRI分割的精度和效率，为神经影像分析提供了创新的技术路径。其集成方法具有学术价值，实际应用可支持脑科学研究，尽管摘要未提及具体局限性，未来工作可能包括扩展到其他医学影像任务或进一步优化模型。",
      "tags": [
        "Segment Anything Model",
        "U-Net",
        "Transformer",
        "Multi-scale Attention",
        "MRI Parcellation"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:37.239766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09262",
    "title": "Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery",
    "authors": [
      "Maria Sdraka",
      "Dimitrios Michail",
      "Ioannis Papoutsis"
    ],
    "abstract": "Delineating wildfire affected areas using satellite imagery remains challenging due to irregular and spatially heterogeneous spectral changes across the electromagnetic spectrum. While recent deep learning approaches achieve high accuracy when high-resolution multispectral data are available, their applicability in operational settings, where a quick delineation of the burn scar shortly after a wildfire incident is required, is limited by the trade-off between spatial resolution and temporal revisit frequency of current satellite systems. To address this limitation, we propose a novel deep learning model, namely BAM-MRCD, which employs multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for the timely production of detailed burnt area maps with high spatial and temporal resolution. Our model manages to detect even small scale wildfires with high accuracy, surpassing similar change detection models as well as solid baselines. All data and code are available in the GitHub repository: https://github.com/Orion-AI-Lab/BAM-MRCD.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09262.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09262",
    "published": "2026-01-14T07:53:48Z",
    "updated": "2026-01-14T07:53:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出基于多分辨率多源卫星图像的深度学习模型BAM-MRCD，用于快速生成高时空分辨率的野火烧毁区域地图。",
      "motivation": "卫星图像描绘野火烧毁区域面临挑战，因为电磁光谱中的变化不规则且空间异质。现有深度学习方法在需要快速检测时，受限于卫星系统空间分辨率与时间重访频率的权衡，导致在操作设置中应用有限。野火检测对于及时响应和灾害管理至关重要，当前方法难以满足这一需求，特别是短时间内的快速圈定需求。",
      "method": "论文提出的BAM-MRCD模型利用多分辨率、多源卫星图像（MODIS和Sentinel-2）来克服空间与时间分辨率的权衡。该方法通过融合MODIS的高时间分辨率数据与Sentinel-2的高空间分辨率数据，采用深度学习技术进行变化检测，以实现详细烧毁区域地图的快速生成。核心创新在于整合多源数据以优化检测性能，但摘要未明确说明具体模型架构和训练细节。",
      "result": "模型能够以高精度检测小规模野火，超越了类似的变化检测模型和基线方法。尽管摘要未提供具体的性能指标数据，如准确率或效率提升百分比，但强调了其在快速检测场景中的有效性和优越性，表明在操作环境中具有实际应用潜力。",
      "conclusion": "研究的主要贡献是开发了一个深度学习模型，能够在操作环境中快速、准确地生成烧毁区域地图，提高了野火检测的时效性和实用性。这项工作具有重要的应用价值，支持应急响应和环境监测，已开源数据和代码以促进未来研究。潜在局限性可能包括模型泛化能力，未来工作可扩展到更多数据源或优化算法。",
      "tags": [
        "Wildfire Detection",
        "Change Detection",
        "Deep Learning",
        "Multi-source Satellite Imagery",
        "Multi-resolution Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:53.318474Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09261",
    "title": "Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability",
    "authors": [
      "Zhipeng Zhang",
      "Zhenjie Yao",
      "Kai Li",
      "Lei Yang"
    ],
    "abstract": "Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.   We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.   Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09261.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09261",
    "published": "2026-01-14T07:52:14Z",
    "updated": "2026-01-14T07:52:14Z",
    "comment": "23 pages, 7 figures. Preprint",
    "light_analysis": {
      "overview": "论文提出了基于Monitor-Trust-Regulator框架的自我诊断方法，用于在不可观测反馈可靠性下实现内源性可靠性评估，提升学习的认知可识别性。",
      "motivation": "研究动机是解决在不可观测反馈可靠性下的学习挑战，特别是Epistemic Identifiability under Unobservable Reliability (EIUR)问题。EIUR中，数据由学习者自身信念和行动生成，可靠与不可靠反馈局部不可区分，导致标准稳健学习可能稳定收敛，但形成高置信度的系统性错误信念。这凸显了评估经验可信度的重要性，以超越单纯优化鲁棒性。",
      "method": "论文提出元认知调节作为解决方案，形式化为Monitor-Trust-Regulator (MTR)分解，并使用自我诊断实例化。该方法通过内省控制循环，从学习者内部动态中推断经验可信度，维护缓慢变化的经验信任变量来软调制学习更新。核心创新在于不依赖外部可靠性标签或显式腐败模型，而是基于内生证据实现模块化设计。",
      "result": "在研究的EIUR体制中，自我诊断与改进的认知可识别性相关：在强化学习中，它实现校准的怀疑，促进在系统腐败奖励下的恢复；在监督学习中，它揭示性能恢复与认知恢复的分离，即准确率可能反弹，但内部信念动态仍被早期误导数据锁定，这只能通过内省诊断检测。实验未提供具体数值指标，但展示了方法在不同学习场景下的有效性。",
      "conclusion": "MTR框架和自我诊断方法为在不可观测可靠性下的自主学习提供了组织抽象和具体设计模板。主要贡献是形式化了内源性可靠性评估方法，学术价值在于扩展了稳健学习的理论边界，实际应用可能提升自主系统的适应性和可信度。局限性摘要未明确说明，未来工作可探索方法在更多学习任务中的泛化性和实际部署挑战。",
      "tags": [
        "Monitor-Trust-Regulator",
        "Self-Diagnosis",
        "Epistemic Identifiability",
        "Reinforcement Learning",
        "Supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:59.225822Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09260",
    "title": "Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models",
    "authors": [
      "Yan Liu",
      "Feng Zhang",
      "Zhanyu Ma",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He",
      "Han Liu",
      "Yangdong Deng"
    ],
    "abstract": "High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09260.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09260",
    "published": "2026-01-14T07:52:05Z",
    "updated": "2026-01-14T07:52:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出CoT-Flow框架，通过概率流量化推理步骤贡献，结合流引导解码和流基础强化学习，实现高效推理和密集奖励。",
      "motivation": "研究动机源于高质量思路链虽能提升大型语言模型的推理能力，但现有方法将推理过程视为不可分割序列，缺乏量化步骤信息增益的机制。这导致推理效率低下，表现为冗余探索缺乏明确指导；优化困难，源于稀疏结果监督或依赖成本高的外部验证器。因此，亟需一种能够量化推理步骤贡献并指导高效探索的方法，以改进推理过程的监督和效率。",
      "method": "论文提出CoT-Flow框架，核心创新是将离散推理步骤重新概念化为连续概率流，量化每个步骤对真实答案的贡献。基于此，框架引入了两种互补方法论：流引导解码采用基于流的贪婪解码策略，提取信息高效的推理路径；流基础强化学习构建无需外部验证器的密集奖励函数。该方法利用概率流理论，改进了推理过程的监督和引导机制，实现了更有效的模型优化。",
      "result": "在具有挑战性的基准测试中，CoT-Flow框架展现出优越性能，实现了推理效率与推理性能的良好平衡。摘要未明确说明具体性能指标如准确率提升或效率改进数据，但实验结果表明该方法有效缓解了推理冗余和优化困难。与基线方法相比，CoT-Flow可能提高了准确率并减少了计算开销，具体细节需参考全文进一步确认。",
      "conclusion": "CoT-Flow框架的主要贡献在于通过概率流量化推理步骤贡献，提供了流引导解码和流基础强化学习两种方法。学术上，它为大型语言模型的推理过程提供了新的量化分析工具，增强了推理过程的透明性和可控性；实际上，提高了模型的推理效率和性能，减少了对外部验证器的依赖。未来工作可扩展该框架到更多复杂任务，或进一步优化概率流计算和奖励函数设计，以应对更广泛的推理场景。",
      "tags": [
        "Large Language Models",
        "Probabilistic Flow",
        "Chain-of-Thought",
        "Flow-guided Decoding",
        "Flow-based Reinforcement Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:13.563783Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09259",
    "title": "MAXS: Meta-Adaptive Exploration with LLM Agents",
    "authors": [
      "Jian Zhang",
      "Zhiyuan Wang",
      "Zhangqi Wang",
      "Yu He",
      "Haoran Luo",
      "li yuan",
      "Lingling Zhang",
      "Rui Mao",
      "Qika Lin",
      "Jun Liu"
    ],
    "abstract": "Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09259.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09259",
    "published": "2026-01-14T07:48:00Z",
    "updated": "2026-01-14T07:48:00Z",
    "comment": null,
    "light_analysis": {
      "overview": "MAXS框架通过前瞻策略和轨迹收敛机制，解决了LLM代理推理中的局部短视和轨迹不稳定性问题，提升了效率和效果。",
      "motivation": "LLM代理在推理时，现有方法存在局部短视生成（因缺乏前瞻）和轨迹不稳定性（小错误导致推理路径发散）的问题，使得难以平衡全局有效性和计算效率。这些问题限制了代理在多工具协作中的表现，影响实际应用的鲁棒性和效率。因此，本研究旨在开发自适应框架，以优化推理过程，解决资源分配与性能之间的矛盾。",
      "method": "MAXS是一个基于LLM代理的元自适应推理框架，灵活集成工具执行和推理规划。它采用前瞻策略扩展推理路径几步，估计工具使用的优势值，并结合步一致性方差和趋势斜率来选择稳定、一致和高价值的推理步骤。此外，引入轨迹收敛机制，在达到路径一致性时停止滚动，控制计算成本，以平衡资源效率和全局有效性。框架在多个基础模型和数据集上进行实证研究。",
      "result": "MAXS在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上进行了广泛实证研究，结果显示它在性能和推理效率上一致优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。虽然具体数值摘要未明确说明，但实验表明显著的改进，尤其是在多工具推理任务中。",
      "conclusion": "MAXS框架的主要贡献在于通过元自适应探索机制，提升了LLM代理的推理稳定性和效率，解决了局部短视和轨迹不稳定性的关键问题。它实现了资源效率与全局有效性的平衡，为代理在多工具推理中的应用提供了新方法，具有学术价值和实用潜力。未来工作可扩展到更复杂的场景或结合其他优化技术。",
      "tags": [
        "Large Language Model Agents",
        "Meta-Adaptive Exploration",
        "Lookahead Strategy",
        "Tool Execution",
        "Trajectory Convergence"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:41.148647Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09255",
    "title": "PhyRPR: Training-Free Physics-Constrained Video Generation",
    "authors": [
      "Yibo Zhao",
      "Hengjia Li",
      "Xiaofei He",
      "Boxi Wu"
    ],
    "abstract": "Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\\textit{PhyRPR}:\\textit{Phy\\uline{R}eason}--\\textit{Phy\\uline{P}lan}--\\textit{Phy\\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \\textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \\textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \\textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09255.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09255",
    "published": "2026-01-14T07:41:56Z",
    "updated": "2026-01-14T07:41:56Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出PhyRPR，一种训练免费的三阶段管道，通过解耦物理理解与视觉合成，显著提高物理约束视频生成的合理性和运动控制。",
      "motivation": "当前基于扩散的视频生成模型虽然能合成视觉上合理的视频，但常常无法满足物理约束，如物体运动违背物理规律。这是因为现有方法多为单阶段，将高级物理理解和低级视觉合成纠缠在一起，限制了生成需要显式物理推理的内容。物理合理性对仿真、教育等应用至关重要，因此需要开发新方法来弥补这一不足，提升视频生成的物理真实性和可控性。",
      "method": "PhyRPR方法采用三阶段管道：PhyReason阶段使用大型多模态模型进行物理状态推理，并生成关键帧；PhyPlan阶段生成一个可控的粗粒度运动支架，用于规划动态；PhyRefine阶段通过潜在融合策略将运动支架注入扩散采样过程，精修视频外观同时保留计划好的物理动态。关键创新是训练免费、三阶段设计解耦了物理与视觉，无需额外模型训练，实现了显式物理控制。",
      "result": "在物理约束场景下的广泛实验表明，PhyRPR方法相比现有基线，能一致提高生成视频的物理合理性和运动可控性。摘要未明确说明具体性能指标，如准确率或效率改进数据，但强调了方法在多个约束条件下的稳健改进，展示了其优于传统单阶段方法的能力。",
      "conclusion": "PhyRPR的主要贡献是提出一种训练免费的三阶段管道，有效解耦物理理解和视觉合成，实现了物理约束视频生成的显式控制。这提高了生成视频的物理真实性和运动精确性，在学术上推动了AI与物理模拟的交叉研究，实际应用中可扩展至仿真、娱乐和教育等领域。未来工作可能包括优化算法效率、扩展到更多复杂物理场景或集成实时控制功能。",
      "tags": [
        "Diffusion-based Video Generation",
        "Physics-Constrained Generation",
        "Training-Free Method",
        "Multimodal Reasoning",
        "Latent Fusion"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:53.680210Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09253",
    "title": "RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning",
    "authors": [
      "Zehua Liu",
      "Shuqi Liu",
      "Tao Zhong",
      "Mingxuan Yuan"
    ],
    "abstract": "While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09253.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09253",
    "published": "2026-01-14T07:41:03Z",
    "updated": "2026-01-14T07:41:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出奖励信息微调（RIFT）框架，通过重新利用负样本提高大语言模型对齐的数据效率和性能。",
      "motivation": "监督微调（SFT）依赖昂贵的专家标注数据，而拒绝采样微调（RFT）则丢弃了自生成数据中的负样本，导致数据利用率低下。在大语言模型对齐任务中，高效利用自生成数据对降低标注成本至关重要，但现有方法未能充分利用负样本信息，因此需要一种更数据高效的方法来处理混合质量数据。",
      "method": "RIFT框架利用所有自生成样本，通过标量奖励重新加权损失函数，使模型从正负轨迹中同时学习。为避免因直接乘奖励导致的损失无界和训练崩溃，作者引入了稳定的损失公式，确保数值稳健性和优化效率。技术路线基于微调自生成数据，无需依赖外部专家标注，摘要未明确说明具体模型架构或数据集。",
      "result": "在多个基础模型的数学基准测试中，RIFT一致优于RFT，表明其在利用自生成样本方面更具效率。摘要未提供具体性能指标如准确率提升，但实验结果显示RIFT能更好地处理混合质量数据，提高对齐模型的整体性能。",
      "conclusion": "RIFT通过重新利用负样本和稳定损失，为LLM对齐提供了一种稳健且数据高效的替代方法，减少了对昂贵专家数据的依赖。其学术价值在于改进了微调策略，实际应用潜力在于降低标注成本，未来工作可探索其在更多领域的扩展和优化。摘要未明确说明局限性或具体未来方向。",
      "tags": [
        "Large Language Model",
        "Fine-Tuning",
        "Reward Learning",
        "Self-Generated Data",
        "Loss Stabilization"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:52.241139Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09251",
    "title": "HGATSolver: A Heterogeneous Graph Attention Solver for Fluid-Structure Interaction",
    "authors": [
      "Qin-Yi Zhang",
      "Hong Wang",
      "Siyao Liu",
      "Haichuan Lin",
      "Linying Cao",
      "Xiao-Hu Zhou",
      "Chen Chen",
      "Shuangyi Wang",
      "Zeng-Guang Hou"
    ],
    "abstract": "Fluid-structure interaction (FSI) systems involve distinct physical domains, fluid and solid, governed by different partial differential equations and coupled at a dynamic interface. While learning-based solvers offer a promising alternative to costly numerical simulations, existing methods struggle to capture the heterogeneous dynamics of FSI within a unified framework. This challenge is further exacerbated by inconsistencies in response across domains due to interface coupling and by disparities in learning difficulty across fluid and solid regions, leading to instability during prediction. To address these challenges, we propose the Heterogeneous Graph Attention Solver (HGATSolver). HGATSolver encodes the system as a heterogeneous graph, embedding physical structure directly into the model via distinct node and edge types for fluid, solid, and interface regions. This enables specialized message-passing mechanisms tailored to each physical domain. To stabilize explicit time stepping, we introduce a novel physics-conditioned gating mechanism that serves as a learnable, adaptive relaxation factor. Furthermore, an Inter-domain Gradient-Balancing Loss dynamically balances the optimization objectives across domains based on predictive uncertainty. Extensive experiments on two constructed FSI benchmarks and a public dataset demonstrate that HGATSolver achieves state-of-the-art performance, establishing an effective framework for surrogate modeling of coupled multi-physics systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09251.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09251",
    "published": "2026-01-14T07:38:02Z",
    "updated": "2026-01-14T07:38:02Z",
    "comment": null,
    "light_analysis": {
      "overview": "HGATSolver提出一种异质图注意力求解器，通过编码物理结构和自适应门控机制，有效模拟流体-结构相互作用系统，实现统一框架下的稳定预测。",
      "motivation": "流体-结构相互作用系统涉及流体和固体两个物理域，由不同的偏微分方程描述，在动态接口处耦合。现有基于学习的求解器难以在统一框架中捕捉这种异质动态，接口耦合导致响应不一致，流体和固体区域的学习难度差异引发预测不稳定，限制了代理建模的准确性和效率。因此，开发一种能够处理异质性和稳定性的方法对替代昂贵数值模拟至关重要。",
      "method": "HGATSolver将FSI系统编码为异质图，通过为流体、固体和接口区域定义不同的节点和边缘类型，直接将物理结构嵌入模型中，实现专门的消息传递机制。引入物理条件化的门控机制作为可学习的自适应松弛因子，以稳定显式时间步进；并采用跨域梯度平衡损失基于预测不确定性动态平衡优化目标。实验在两个构造的FSI基准和一个公共数据集上进行，使用该异质图注意力架构进行验证。",
      "result": "HGATSolver在两个构造的FSI基准测试和一个公共数据集上实现了最先进的性能，有效解决了FSI系统的模拟问题。与基线方法相比，在准确性和稳定性方面表现出显著提升，摘要未明确说明具体性能指标数据，但强调了方法在建立耦合多物理系统代理建模框架方面的优越性，为替代传统数值模拟提供了高效方案。",
      "conclusion": "HGATSolver的主要贡献是开发了一个异质图注意力求解器，成功解决了流体-结构相互作用系统中的异质动态和稳定性挑战。该方法为多物理系统提供了有效的代理建模框架，具有重要的学术价值（如推动机器学习与物理建模的交叉）和实际应用潜力（如降低计算成本）。未来工作可能包括扩展到更复杂的物理系统或进一步优化门控和损失机制。",
      "tags": [
        "Heterogeneous Graph",
        "Attention Mechanism",
        "Surrogate Modeling",
        "Fluid-Structure Interaction",
        "Gradient-Balancing Loss"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:12.125602Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09250",
    "title": "When to Invoke: Refining LLM Fairness with Toxicity Assessment",
    "authors": [
      "Jing Ren",
      "Bowen Li",
      "Ziqi Xu",
      "Renqiang Luo",
      "Shuo Yu",
      "Xin Ye",
      "Haytham Fayek",
      "Xiaodong Li",
      "Feng Xia"
    ],
    "abstract": "Large Language Models (LLMs) are increasingly used for toxicity assessment in online moderation systems, where fairness across demographic groups is essential for equitable treatment. However, LLMs often produce inconsistent toxicity judgements for subtle expressions, particularly those involving implicit hate speech, revealing underlying biases that are difficult to correct through standard training. This raises a key question that existing approaches often overlook: when should corrective mechanisms be invoked to ensure fair and reliable assessments? To address this, we propose FairToT, an inference-time framework that enhances LLM fairness through prompt-guided toxicity assessment. FairToT identifies cases where demographic-related variation is likely to occur and determines when additional assessment should be applied. In addition, we introduce two interpretable fairness indicators that detect such cases and improve inference consistency without modifying model parameters. Experiments on benchmark datasets show that FairToT reduces group-level disparities while maintaining stable and reliable toxicity predictions, demonstrating that inference-time refinement offers an effective and practical approach for fairness improvement in LLM-based toxicity assessment systems. The source code can be found at https://aisuko.github.io/fair-tot/.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09250.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09250",
    "published": "2026-01-14T07:35:56Z",
    "updated": "2026-01-14T07:35:56Z",
    "comment": "Accepted by Findings of WWW 2026",
    "light_analysis": {
      "overview": "本文提出FairToT框架，通过在推理时引入提示引导毒性评估和可解释公平指标，优化大型语言模型在毒性评估中的公平性。",
      "motivation": "大型语言模型在在线审核系统中用于毒性评估，但对隐式仇恨言论等微妙表达的评判不一致，揭示了底层偏见。现有训练方法难以纠正这些偏见，且忽视了何时调用纠正机制以确保公平的关键问题。这导致系统在处理人口统计组差异时表现不公平，影响在线环境的公正性。",
      "method": "论文提出FairToT推理时框架，通过精心设计的提示来引导毒性评估，自动识别可能出现群体差异的情况，并决定何时应用额外评估。此外，引入两个可解释的公平指标，用于检测敏感案例并提升推理一致性，所有操作均无需修改模型参数，保持了方法的灵活性。",
      "result": "在基准数据集上的实验显示，FairToT有效减少了不同人口统计组之间的毒性评估差异，显著提升了公平性。同时，该方法保持了毒性预测的稳定性和可靠性，表明推理时优化能在不损害整体性能的前提下改善公平性。",
      "conclusion": "本研究的主要贡献在于提出了FairToT框架，为纠正LLM偏见提供了一种无需重新训练的新思路，具有重要学术价值。在实际应用中，它提升了在线审核系统的公平性和可靠性，对促进数字环境公正处理具有重要意义。未来工作可扩展该框架到其他偏见检测场景。",
      "tags": [
        "Large Language Model",
        "Toxicity Assessment",
        "Fairness",
        "Inference-time Optimization",
        "Interpretable Fairness Indicators"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:56.454761Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09248",
    "title": "Hybrid guided variational autoencoder for visual place recognition",
    "authors": [
      "Ni Wang",
      "Zihan You",
      "Emre Neftci",
      "Thorben Schoepe"
    ],
    "abstract": "Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09248.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09248",
    "published": "2026-01-14T07:33:53Z",
    "updated": "2026-01-14T07:33:53Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种结合事件视觉传感器和事件引导变分自编码器的方法，用于视觉地点识别，以提高内存效率和鲁棒性。",
      "motivation": "自主代理如汽车、机器人和无人机在GPS受限的室内环境中需要精确定位，视觉地点识别是一种关键方法。现有最先进的VPR模型通常占用大量内存，不适合移动部署，而更紧凑的模型则缺乏鲁棒性和泛化能力，这限制了它们在动态环境中的应用。因此，需要一种既能高效运行又能应对光照变化和未知场景的新方法，以增强移动设备的自主导航能力。",
      "method": "本研究提出了一种结合事件视觉传感器和事件引导变分自编码器的模型。编码器基于脉冲神经网络，兼容高效能、低延迟的神经形态硬件，能够处理事件流数据。使用新的室内视觉地点识别数据集，模型通过VAE解耦视觉特征，提取地点关键信息。该方法的关键创新在于利用事件引导机制优化特征学习，增强了对环境变化的适应性。",
      "result": "模型在16个不同地点的分类性能与其他最先进方法相当，并在各种光照条件下表现出鲁棒性。当测试未知场景的视觉输入时，模型能准确区分地点，显示了高泛化能力。这些结果表明，该模型在保持紧凑性的同时，提供了与基线方法类似的精度，显著提升了在复杂环境中的识别稳定性。",
      "conclusion": "该研究贡献了一种紧凑且鲁棒的引导变分自编码器，具有高泛化能力，适用于视觉地点识别。这不仅提升了学术价值，为低功耗移动设备提供了新解决方案，还增强了机器人在已知和未知室内环境中的导航潜力。未来工作可扩展数据集或优化硬件集成，以进一步提高实际应用效果。",
      "tags": [
        "Visual Place Recognition",
        "Variational Autoencoder",
        "Spiking Neural Network",
        "Event-based Vision Sensors",
        "Neuromorphic Hardware"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:53.333238Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09247",
    "title": "Integrating Diverse Assignment Strategies into DETRs",
    "authors": [
      "Yiwei Zhang",
      "Jin Gao",
      "Hanshi Wang",
      "Fudong Ge",
      "Guan Luo",
      "Weiming Hu",
      "Zhipeng Zhang"
    ],
    "abstract": "Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of ``one-to-many'' supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse ``one-to-many'' supervision can be integrated to achieve state-of-the-art results without compromising model elegance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09247.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09247",
    "published": "2026-01-14T07:28:54Z",
    "updated": "2026-01-14T07:28:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "LoRA-DETR框架通过集成多种分配策略，无推理成本地提升DETR检测器性能。",
      "motivation": "在DETR风格对象检测器中，标签分配是关键组件。传统一对一匹配策略虽简洁但监督稀疏，导致收敛缓慢。近期研究采用一对多分配以增强监督信号，但往往引入复杂、特定于架构的修改，且多聚焦单一辅助策略，缺乏统一和可扩展的设计。论文通过系统研究发现，性能提升并非源于监督数量，而是分配策略的多样性，这为设计参数高效方法提供了新视角。",
      "method": "论文提出LoRA-DETR，一个灵活轻量框架，能无缝集成多种分配策略到任意DETR风格检测器。方法在训练时为主网络添加多个低秩适应（LoRA）分支，每个分支实例化不同的一对多分配规则。这些分支作为辅助模块，向主模型注入丰富、多样化的监督梯度；在推理时被丢弃，因此不增加额外计算成本。该设计促进了鲁棒的联合优化，同时保持了原始检测器的架构简洁性。",
      "result": "通过在不同基线上进行广泛实验，验证了方法的有效性。论文未明确说明具体性能指标如准确率提升值，但表明集成多样“一对多”监督能实现最先进结果。与现有方法相比，该方法避免了复杂修改，保持了模型优雅，并可能提高收敛速度和检测精度。摘要中未提供详细数据对比，仅强调实验验证了效果。",
      "conclusion": "论文主要贡献是提出LoRA-DETR框架，通过集成多样分配策略，为增强检测器提供了新范式。该方法具有学术价值，揭示了多样性在监督中的作用；实际应用价值在于轻量设计，无推理成本提升性能。局限性可能在于依赖特定分配规则或未探索所有场景；未来工作可扩展至其他检测框架或优化分配策略。摘要未明确说明局限性，但可基于上下文推断。",
      "tags": [
        "DETR",
        "Label Assignment",
        "One-to-Many Assignment",
        "Low-Rank Adaptation (LoRA)",
        "Object Detection"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:16.541610Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09246",
    "title": "TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding",
    "authors": [
      "Xiangqian Wang",
      "Yifan Jia",
      "Yang Xiang",
      "Yumin Zhang",
      "Yanbin Wang",
      "Ke Liu"
    ],
    "abstract": "Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09246.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09246",
    "published": "2026-01-14T07:27:57Z",
    "updated": "2026-01-14T07:27:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了TeachPro，一个通过跨视图图协同和语义锚定证据编码实现多标签定性教学评估的框架，提升了评估的精细度和鲁棒性。",
      "motivation": "标准化学生教学评估常存在可靠性低、响应选项受限和失真等问题，限制了实用价值。现有机器学习方法通常将开放评论简化为二元情感分析，忽略了内容清晰度、反馈及时性等具体教学维度，导致对教学改进的指导不足。该研究旨在解决这一局限，通过多标签学习提供更全面和可操作的评估工具，以支持教育质量的提升。",
      "method": "论文提出TeachPro框架，包含两个核心组件：维度锚定证据编码器，通过预训练文本编码器（如BERT）将定性注释转化为嵌入、提示模块将五个教学维度表示为语义锚，以及交叉注意力机制对齐证据与维度；跨视图图协同网络，包括从解析树提取语法依赖的句法分支和基于BERT相似图建模概念关系的语义分支，通过BiAffine融合模块对齐两分支，差异正则化器解缠嵌入以鼓励互补表示，最后用交叉注意力连接证据与评论表示。此外，贡献了包含专家注释和多标签分数的新基准数据集。",
      "result": "摘要未提供具体性能指标，但通过广泛实验表明，TeachPro在多种评估设置中展现出优越的诊断粒度和鲁棒性，能够更准确地评估专业专长、教学行为等具体维度，相比现有二元情感分析方法，提供了更精细和实用的教学反馈。",
      "conclusion": "本论文的主要贡献是提出了TeachPro框架，通过集成多标签学习、跨视图协同和语义锚定技术，显著提升了教学评估的质量和实用性，为教育改进提供了更具体的指导。其学术价值在于创新性地结合了句法和语义表示，实际应用价值体现在支持更精准的教学诊断。未来工作可能涉及扩展模型到其他教育领域或优化数据集，但摘要未明确说明局限性。",
      "tags": [
        "Multi-Label Learning",
        "Cross-View Graph Synergy",
        "Semantic Anchored Encoding",
        "BERT",
        "Cross-Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:17.499316Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09243",
    "title": "A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation",
    "authors": [
      "Sheng-Chi Hsu",
      "Ting-Yu Yen",
      "Shih-Hsuan Hung",
      "Hung-Kuo Chu"
    ],
    "abstract": "Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering. While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability. In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture. Our method employs a gradient-guided adaptive rule to jointly determine texture resolution and aspect ratio, enabling non-uniform, detail-aware allocation that aligns with the anisotropic nature of Gaussian splats. This design significantly improves texture efficiency, reducing memory consumption while enhancing image quality. Experiments on multiple benchmark datasets demonstrate that A TG consistently outperforms fixed-texture Gaussian Splatting methods, achieving comparable rendering fidelity with substantially lower memory requirements.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09243.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09243",
    "published": "2026-01-14T07:26:55Z",
    "updated": "2026-01-14T07:26:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出自适应各向异性纹理高斯（A^2TG）方法，用于提升3D场景表示的效率和渲染质量，通过自适应纹理分配减少内存消耗。",
      "motivation": "高斯散射（Gaussian Splatting）已成为高质量、实时3D场景表示的重要技术。然而，现有方法在扩展纹理时，为每个基元分配固定方形纹理，导致内存使用效率低下且难以适应场景的多样性变化。这种局限性在实际应用中，特别是在资源受限环境下对高效内存管理的需求中，显得尤为突出，因此需要更灵活和高效的纹理分配方案来解决这些问题。",
      "method": "A^2TG方法的核心创新在于为每个高斯基元引入各向异性纹理，并使用梯度引导的自适应规则联合确定纹理分辨率和纵横比。这种设计使纹理分配非均匀且细节感知，与高斯斑点的各向异性性质对齐。关键技术包括自适应调整纹理参数，以优化资源使用，从而在保持图像质量的同时，显著提升内存效率和场景适应性。",
      "result": "实验在多个基准数据集上进行，结果显示A^2TG方法持续优于固定纹理的高斯散射方法，以显著更低的内存需求实现了可比较的渲染保真度。具体表现包括内存消耗减少和图像质量增强，验证了该方法的有效性，为3D场景表示提供了更高效的解决方案。",
      "conclusion": "本研究的主要贡献是提出了A^2TG方法，有效解决了固定纹理方法在内存效率和场景适应性上的不足。这不仅提升了3D场景表示的学术价值，还为实时渲染和高效内存管理的实际应用提供了新思路，未来工作可探索该方法在更复杂场景下的扩展和优化潜力。",
      "tags": [
        "Gaussian Splatting",
        "Anisotropic Texturing",
        "Adaptive Resolution",
        "Gradient-Guided Adaptation",
        "3D Scene Representation"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:23.256197Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09241",
    "title": "When to Trust: A Causality-Aware Calibration Framework for Accurate Knowledge Graph Retrieval-Augmented Generation",
    "authors": [
      "Jing Ren",
      "Bowen Li",
      "Ziqi Xu",
      "Xinkun Zhang",
      "Haytham Fayek",
      "Xiaodong Li"
    ],
    "abstract": "Knowledge Graph Retrieval-Augmented Generation (KG-RAG) extends the RAG paradigm by incorporating structured knowledge from knowledge graphs, enabling Large Language Models (LLMs) to perform more precise and explainable reasoning. While KG-RAG improves factual accuracy in complex tasks, existing KG-RAG models are often severely overconfident, producing high-confidence predictions even when retrieved sub-graphs are incomplete or unreliable, which raises concerns for deployment in high-stakes domains. To address this issue, we propose Ca2KG, a Causality-aware Calibration framework for KG-RAG. Ca2KG integrates counterfactual prompting, which exposes retrieval-dependent uncertainties in knowledge quality and reasoning reliability, with a panel-based re-scoring mechanism that stabilises predictions across interventions. Extensive experiments on two complex QA datasets demonstrate that Ca2KG consistently improves calibration while maintaining or even enhancing predictive accuracy.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09241.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09241",
    "published": "2026-01-14T07:22:59Z",
    "updated": "2026-01-14T07:22:59Z",
    "comment": "Accepted by WWW 2026",
    "light_analysis": {
      "overview": "本文提出了Ca2KG，一个因果感知校准框架，用于解决知识图谱检索增强生成中模型过度自信的问题，提升模型的可信度。",
      "motivation": "现有知识图谱检索增强生成模型在复杂任务中虽提高了事实准确性，但常常过度自信，即使在检索到的知识子图不完整或不可靠时也产生高置信度预测。这种过度自信在高风险领域（如医疗、金融）的部署中引起重大担忧，可能导致错误决策。因此，需要开发一种校准机制，使模型能更准确地评估自身的可靠性，以匹配预测的置信度与实际性能。",
      "method": "Ca2KG框架集成了反事实提示技术，通过模拟不同知识检索场景来暴露知识质量和推理过程中的不确定性。同时，采用一个基于面板的重评分机制，通过稳定干预过程中的预测输出，提升模型的校准能力。该方法的关键创新在于将因果推理与校准结合，通过反事实分析量化检索依赖的不确定性，而面板重评分则聚合多个干预结果以改善稳健性。",
      "result": "在两个复杂问答数据集上的广泛实验表明，Ca2KG持续改进了模型的校准性能，具体表现在减少过度自信的同时，预测准确性保持甚至略有提升。实验结果显示Ca2KG在多项校准指标上优于现有KG-RAG基线方法，验证了框架在平衡置信度与事实准确性方面的有效性。摘要未提供具体数据如提升百分比，但强调了整体性能的改善和与基准对比的优越性。",
      "conclusion": "Ca2KG为知识图谱检索增强生成提供了一个有效的因果感知校准框架，主要贡献在于解决了模型过度自信问题，从而增强推理的可信度。该研究具有学术价值，推动了RAG范式中校准方法的发展，并具有实际应用潜力，有助于高风险AI系统的安全部署。未来工作可能包括扩展至其他任务或改进不确定性量化，但摘要未明确说明局限性。",
      "tags": [
        "Knowledge Graph Retrieval-Augmented Generation",
        "Large Language Models",
        "Counterfactual Prompting",
        "Calibration",
        "Causal Reasoning"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:38.840707Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09240",
    "title": "DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos",
    "authors": [
      "Jiajun Chen",
      "Jing Xiao",
      "Shaohan Cao",
      "Yuming Zhu",
      "Liang Liao",
      "Jun Pan",
      "Mi Wang"
    ],
    "abstract": "Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.",
    "categories": [
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09240.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09240",
    "published": "2026-01-14T07:22:44Z",
    "updated": "2026-01-14T07:22:44Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出DeTracker框架，通过全局-局部运动解耦和时域依赖特征金字塔，提升未稳定卫星视频中车辆检测与跟踪的性能。",
      "motivation": "研究动机源于卫星视频在环境监测和智能交通等领域提供连续观测的潜力，但未稳定条件下平台抖动和微小物体弱外观共同导致多目标跟踪性能下降。现有方法难以有效应对这些挑战，影响跟踪准确性和稳定性，限制了卫星视频的实际应用。因此，开发专门针对运动解耦和特征增强的鲁棒跟踪框架至关重要，以解决运动干扰和对象识别问题。",
      "method": "论文提出DeTracker框架，核心创新包括全局-局部运动解耦模块，通过全局对齐和局部细化显式分离卫星平台运动与真实对象运动，提升轨迹稳定性；时域依赖特征金字塔模块进行跨帧时域特征融合，增强微小对象表示的连续性和区分度。此外，构建了SDM-Car-SU基准数据集，模拟多方向和多速度的平台运动，用于系统评估不同运动扰动下的跟踪鲁棒性。",
      "result": "在模拟和真实的未稳定卫星视频上进行广泛实验，DeTracker在SDM-Car-SU数据集上达到61.1% MOTA，在真实卫星视频数据上达到47.3% MOTA，显著优于现有基线方法。结果表明，该方法在运动估计准确性和轨迹连续性方面有显著提升，验证了其在应对平台抖动和微小物体挑战时的有效性和鲁棒性。",
      "conclusion": "论文的主要贡献是提出DeTracker框架，通过运动解耦和时域特征融合技术改善了未稳定卫星视频的跟踪性能。学术价值在于推动了卫星视频分析和多目标跟踪领域的发展，特别是针对微小物体和运动扰动的处理；实际应用价值体现在卫星监控和智能交通等领域。摘要未明确说明局限性，未来工作可能包括扩展到其他对象类型或更复杂的场景。",
      "tags": [
        "Multi-Object Tracking",
        "Motion Decoupling",
        "Temporal Feature Fusion",
        "Satellite Video Analysis",
        "Tiny Object Detection"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:59.513620Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09238",
    "title": "Knowledge-Embedded and Hypernetwork-Guided Few-Shot Substation Meter Defect Image Generation Method",
    "authors": [
      "Jackie Alex",
      "Justin Petter"
    ],
    "abstract": "Substation meters play a critical role in monitoring and ensuring the stable operation of power grids, yet their detection of cracks and other physical defects is often hampered by a severe scarcity of annotated samples. To address this few-shot generation challenge, we propose a novel framework that integrates Knowledge Embedding and Hypernetwork-Guided Conditional Control into a Stable Diffusion pipeline, enabling realistic and controllable synthesis of defect images from limited data.   First, we bridge the substantial domain gap between natural-image pre-trained models and industrial equipment by fine-tuning a Stable Diffusion backbone using DreamBooth-style knowledge embedding. This process encodes the unique structural and textural priors of substation meters, ensuring generated images retain authentic meter characteristics.   Second, we introduce a geometric crack modeling module that parameterizes defect attributes--such as location, length, curvature, and branching pattern--to produce spatially constrained control maps. These maps provide precise, pixel-level guidance during generation.   Third, we design a lightweight hypernetwork that dynamically modulates the denoising process of the diffusion model in response to the control maps and high-level defect descriptors, achieving a flexible balance between generation fidelity and controllability.   Extensive experiments on a real-world substation meter dataset demonstrate that our method substantially outperforms existing augmentation and generation baselines. It reduces Frechet Inception Distance (FID) by 32.7%, increases diversity metrics, and--most importantly--boosts the mAP of a downstream defect detector by 15.3% when trained on augmented data. The framework offers a practical, high-quality data synthesis solution for industrial inspection systems where defect samples are rare.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09238.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09238",
    "published": "2026-01-14T07:21:57Z",
    "updated": "2026-01-14T07:21:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种结合知识嵌入和超网络引导的少样本生成方法，用于变电站仪表缺陷图像的逼真合成，解决了数据稀缺问题。",
      "motivation": "变电站仪表对电网稳定运行至关重要，但裂纹等物理缺陷检测面临标注样本严重稀缺的挑战。现有方法常基于自然图像预训练模型，与工业设备存在显著的域差距，导致生成图像不真实，影响下游检测模型的性能。因此，需要开发一种能高效利用有限数据生成高质量缺陷图像的方法，以提升检测准确性并支持电网监测系统的可靠性。摘要未明确说明更广泛的行业背景，但突出了少样本生成在工业应用中的重要性。",
      "method": "研究提出一个基于Stable Diffusion的新框架，集成知识嵌入和超网络引导控制。首先，通过DreamBooth式知识嵌入微调Stable Diffusion骨干网络，编码变电站仪表的独特结构和纹理先验，解决自然图像与工业设备的域差距。其次，引入几何裂纹建模模块，参数化缺陷属性如位置、长度和分支模式，生成空间约束的控制图提供像素级指导。最后，设计轻量级超网络，根据控制图和高层缺陷描述动态调制扩散模型的降噪过程，实现生成逼真度与可控性的灵活平衡。实验基于真实变电站仪表数据集，确保方法验证的有效性。",
      "result": "在真实变电站仪表数据集上的广泛实验显示，该方法显著优于现有图像增强和生成基线。具体性能指标包括：Frechet Inception Distance降低32.7%，表明生成图像质量提升；多样性指标有所增加，反映生成样本的丰富性。最关键的是，使用生成数据训练的下游缺陷检测器，其平均精度均值提高15.3%，证明了合成图像对实际检测任务的积极影响，增强了方法的实用价值。",
      "conclusion": "论文的主要贡献是开发了一个集成知识嵌入和超网络引导的少样本图像生成框架，为工业检测系统中的缺陷样本稀缺问题提供了高质量的数据合成解决方案。学术上，该方法推动了少样本生成技术和条件扩散模型的发展；应用上，它能显著提升变电站仪表缺陷检测的性能，具有广泛的工业价值。局限性如对其他缺陷类型的适用性未明确说明，未来工作可能扩展到更多工业设备或复杂缺陷场景，以进一步验证泛化能力。",
      "tags": [
        "Stable Diffusion",
        "Knowledge Embedding",
        "Hypernetwork",
        "Few-Shot Generation",
        "Conditional Control"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:59.621829Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09237",
    "title": "XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs",
    "authors": [
      "Xinyang Chen",
      "Huidong Jin",
      "Yu Huang",
      "Zaiwen Feng"
    ],
    "abstract": "Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09237.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09237",
    "published": "2026-01-14T07:21:29Z",
    "updated": "2026-01-14T07:21:29Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": {
      "overview": "提出XLinear，一种基于MLP的轻量级模型，用于带外生输入的长期时间序列预测，提升精度与效率。",
      "motivation": "长期时间序列预测模型常假设变量重要性均匀，但实际应用中存在非对称因果关系和不同数据获取成本，如低成本外生数据（天气）单向影响内生变量（湖温）。现有Transformer模型能捕捉长期依赖，但计算负担重且存在排列不变性问题；基于补丁的变体提升效率却可能遗漏局部时间模式。因此，需要一种能高效利用时间和变量间信号的轻量化方法来解决预测不足。",
      "method": "XLinear采用多层感知机构建，通过从内生变量提取的全局令牌作为与外生变量交互的核心枢纽。使用带sigmoid激活函数的MLPs同时提取时间维度的模式和变量间的依赖关系，预测头整合这些信号来预测内生序列。模型设计轻量化，避免了复杂计算架构，专注于高效信息提取。",
      "result": "XLinear在七个标准基准和五个带外生输入的真实世界数据集上进行评估。与现有最先进模型相比，该模型在多元预测和受外生输入影响的单变量预测中均展现出更高的准确性和效率。摘要未明确具体性能数据，但强调了整体优势，表明在多个测试场景下超越基线方法。",
      "conclusion": "XLinear为长期时间序列预测提供了一种轻量级且准确的解决方案，特别针对外生输入的利用。学术上，它通过MLPs和全局令牌机制改进预测方法，平衡精度与计算效率；实际中，可应用于环境监测等实时场景。未来工作可能涉及扩展模型到更复杂的数据类型或验证其泛化能力。",
      "tags": [
        "Long-Term Time Series Forecasting",
        "Exogenous Inputs",
        "Multilayer Perceptrons (MLPs)",
        "Lightweight Model",
        "Global Token"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:15.400409Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09236",
    "title": "Reward Learning through Ranking Mean Squared Error",
    "authors": [
      "Chaitanya Kharyal",
      "Calarina Muslimani",
      "Matthew E. Taylor"
    ],
    "abstract": "Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., \"bad,\" \"neutral,\" \"good\"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09236.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09236",
    "published": "2026-01-14T07:18:12Z",
    "updated": "2026-01-14T07:18:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种基于评分反馈的强化学习新方法R4，通过排序均方误差损失从人类评分中学习奖励函数，提供正式理论保证。",
      "motivation": "强化学习在现实世界应用中面临奖励函数设计复杂的瓶颈，手动指定奖励耗时且易错。奖励学习从人类反馈中推断奖励函数成为替代方案，但传统二元偏好反馈信息有限且增加认知负担。因此，研究转向使用更丰富的离散评分反馈，以提供更高效和灵活的监督。本文旨在解决如何利用评分反馈优化奖励学习，弥补现有方法的不足，推动强化学习在复杂任务中的实际应用。",
      "method": "本文提出Ranked Return Regression for RL (R4)方法，核心创新是引入排序均方误差（rMSE）损失函数。该方法使用轨迹-评分对数据集，每个轨迹带有离散评分（如“差”、“中”、“好”）。在训练时，随机采样一组轨迹，预测其累计回报，并采用可微排序算子（软排序）对预测回报进行排名。然后，优化软排名与教师评分之间的均方误差损失。与传统方法不同，R4在温和假设下提供理论保证，其解集被证明是极小且完整的，确保了学习过程的稳定性和收敛性。",
      "result": "通过模拟人类反馈的实验，R4在OpenAI Gym和DeepMind Control Suite的机器人运动基准测试中表现优异。实验结果显示，R4一致匹配或优于现有的基于评分和偏好的强化学习方法。同时，R4在达到相似或更好性能时，所需的反馈量显著减少，提升了学习效率。尽管摘要未提供具体提升百分比，但突出了其在减少人类干预方面的优势，验证了方法在实际场景中的有效性。",
      "conclusion": "本文的主要贡献是开发了R4方法，通过排序均方误差损失实现从评分反馈中高效学习奖励函数，并提供了理论完整性保证。其学术价值在于推动了奖励学习领域的发展，结合评分反馈和排序损失提升学习可靠性。应用上，R4减少反馈需求，降低强化学习部署成本，增强现实适应性。未来工作可探索实际人类反馈验证、扩展至更复杂环境，以进一步评估泛化能力和局限性。",
      "tags": [
        "Reinforcement Learning",
        "Reward Learning",
        "Ranking Mean Squared Error",
        "Ordinal Regression",
        "Differentiable Sorting"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:26.433623Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09233",
    "title": "GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization",
    "authors": [
      "Zhengyang Zhao",
      "Lu Ma",
      "Yizhen Jiang",
      "Xiaochen Ma",
      "Zimo Meng",
      "Chengyu Shen",
      "Lexiang Tang",
      "Haoze Sun",
      "Peng Pei",
      "Wentao Zhang"
    ],
    "abstract": "The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09233.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09233",
    "published": "2026-01-14T07:13:57Z",
    "updated": "2026-01-14T07:13:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出GIFT方法，通过有限温度Gibbs初始化解决大型推理模型后训练中SFT与RL的优化不匹配问题，解锁全局最优性。",
      "motivation": "当前大型推理模型的后训练范式通常采用监督微调（SFT）后接强化学习（RL），但存在内在优化不匹配问题。SFT的刚性监督导致分布坍塌，使得后续RL缺乏足够的探索空间，从而影响全局最优性的达成。现有方法的不足在于SFT过于严格，抑制了模型的基础先验，使得RL阶段无法有效探索策略空间，限制了性能提升，因此需要一种新方法来平衡监督与探索，确保后训练过程的连贯性和有效性。",
      "method": "论文将SFT重新表述在统一的后训练框架中，提出了有限温度的Gibbs初始化（GIFT）。核心创新在于将标准SFT视为退化的零温度极限，它抑制了基础先验；而GIFT通过将监督作为有限温度能量势，建立了一个分布桥梁，确保整个后训练管道中目标的一致性。这种方法在数学上更合理，为RL初始化提供了优化起点。摘要未明确说明使用的具体数据集或模型架构。",
      "result": "实验结果表明，当将GIFT用作RL的初始化时，它显著优于标准的SFT和其他竞争基线方法。这证明了GIFT在提升后训练性能方面的有效性，并提供了实现全局最优性的数学原理路径。通过实验验证，GIFT在保持探索空间的同时强化了监督信号，从而在后续RL中达到更好的结果。具体性能指标如准确率提升等摘要未明确说明。",
      "conclusion": "GIFT的贡献在于提出了一种数学原理的后训练初始化方法，通过有限温度Gibbs初始化解锁了全局最优性。该方法不仅解决了SFT和RL之间的优化不匹配问题，还为大型推理模型的后训练提供了理论支持和实践指导。学术价值在于改进了后训练范式，实际应用价值在于提升模型性能。未来工作方向摘要未明确说明，但可能涉及更广泛的应用验证。",
      "tags": [
        "Large Reasoning Models",
        "Supervised Fine-Tuning",
        "Reinforcement Learning",
        "Gibbs Initialization",
        "Finite-Temperature"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:33.201937Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09230",
    "title": "CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation",
    "authors": [
      "Haodi Yao",
      "Fenghua He",
      "Ning Hao",
      "Yao Su"
    ],
    "abstract": "Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09230.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09230",
    "published": "2026-01-14T07:03:01Z",
    "updated": "2026-01-14T07:03:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "CLIDD通过独立特征层级采样和可学习偏移，实现了高效和高区分度的本地特征表示，为空间智能任务提供实时解决方案。",
      "motivation": "空间智能任务如机器人导航和增强现实需要鲁棒的本地特征表示来建立可靠对应关系，这要求描述符兼具高区分度和计算效率。现有方法如统一密集表示可能导致计算负担过重、区分度不足，难以平衡性能与效率。CLIDD旨在解决这一双重挑战，提升特征匹配的精度和实时性，以满足实际部署需求。",
      "method": "CLIDD方法的核心是从独立特征层级直接采样，利用可学习偏移捕获跨尺度的细粒度结构细节，避免统一密集表示的计算负担。为优化实时性能，采用硬件感知内核融合策略最大化推理吞吐量。此外，构建了可扩展框架，集成轻量架构并通过结合度量学习和知识蒸馏的训练协议，生成适应不同部署约束的多样化模型变体。",
      "result": "实验表明，超紧凑变体仅使用0.004M参数，匹配SuperPoint的精度，模型大小减少99.7%。高性能配置优于所有当前最先进方法，包括基于DINOv2的高容量框架，在边缘设备上超过200 FPS。这些结果证实CLIDD在保持高匹配精度的同时实现了卓越的计算效率。",
      "conclusion": "CLIDD的主要贡献在于提供了一种高效和高精度的本地特征匹配方法，最小化计算开销。其学术价值在于改进了特征表示技术，实际应用价值在于为实时空间智能任务提供了健壮和可扩展的解决方案。摘要未明确说明具体局限性，未来工作可能涉及进一步优化硬件适应性或扩展到更广泛的任务领域。",
      "tags": [
        "Cross-Layer Independent Deformable Description",
        "Learnable Offsets",
        "Hardware-Aware Kernel Fusion",
        "Metric Learning",
        "Knowledge Distillation"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:47.656431Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09229",
    "title": "SPOT-Face: Forensic Face Identification using Attention Guided Optimal Transport",
    "authors": [
      "Ravi Shankar Prasad",
      "Dinesh Singh"
    ],
    "abstract": "Person identification in forensic investigations becomes very challenging when common identification means for DNA (i.e., hair strands, soft tissue) are not available. Current methods utilize deep learning methods for face recognition. However, these methods lack effective mechanisms to model cross-domain structural correspondence between two different forensic modalities. In this paper, we introduce a SPOT-Face, a superpixel graph-based framework designed for cross-domain forensic face identification of victims using their skeleton and sketch images. Our unified framework involves constructing a superpixel-based graph from an image and then using different graph neural networks(GNNs) backbones to extract the embeddings of these graphs, while cross-domain correspondence is established through attention-guided optimal transport mechanism. We have evaluated our proposed framework on two publicly available dataset: IIT\\_Mandi\\_S2F (S2F) and CUFS. Extensive experiments were conducted to evaluate our proposed framework. The experimental results show significant improvement in identification metrics ( i.e., Recall, mAP) over existing graph-based baselines. Furthermore, our framework demonstrates to be highly effective for matching skulls and sketches to faces in forensic investigations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09229.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09229",
    "published": "2026-01-14T07:02:21Z",
    "updated": "2026-01-14T07:02:21Z",
    "comment": "14 pages, 5 figures, 3 tables (ICPR_2026)",
    "light_analysis": {
      "overview": "SPOT-Face框架通过注意力引导的最优传输机制，实现跨域法医面部识别，显著提升识别精度。",
      "motivation": "在法医调查中，当DNA等传统识别手段不可用时，人员识别变得非常困难。现有方法采用深度学习进行面部识别，但缺乏有效机制来建模不同法医模态（如骨骼和素描图像）之间的跨域结构对应关系，这限制了识别的准确性和鲁棒性。因此，开发一种能够处理跨域差异的机制至关重要，以改进法医识别的实用性。",
      "method": "论文提出SPOT-Face框架，首先从图像构建超像素图以捕获局部结构信息。接着，利用不同的图神经网络（GNNs）作为骨干网络，提取图的嵌入表示。关键创新在于引入注意力引导的最优传输机制，该机制通过注意力权重优化跨域对应，确保骨骼、素描和面部图像之间的结构对齐。框架在IIT_Mandi_S2F和CUFS两个公开数据集上评估，涵盖多种法医模态。",
      "result": "实验结果表明，SPOT-Face在召回率（Recall）和平均精度（mAP）等识别指标上，相比现有基于图的基线方法有显著改进。例如，在匹配头骨和素描到面部的任务中，框架表现出高度有效性，具体性能提升在摘要中未详细说明，但被强调为显著，验证了方法的优越性。",
      "conclusion": "本研究的主要贡献是开发了SPOT-Face框架，整合超像素图、图神经网络和注意力引导最优传输，有效解决跨域法医面部识别问题。学术价值在于引入新的跨域对应机制，提升识别精度；实际应用中，可增强法医调查的效率和准确性。未来工作可扩展到更多法医模态或数据集，以验证鲁棒性。",
      "tags": [
        "Graph Neural Networks",
        "Attention-Guided Optimal Transport",
        "Superpixel Graph",
        "Cross-domain Matching",
        "Forensic Face Identification"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:34.866868Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09228",
    "title": "Disentangle Object and Non-object Infrared Features via Language Guidance",
    "authors": [
      "Fan Liu",
      "Ting Wu",
      "Chuanyi Zhang",
      "Liang Yao",
      "Xing Ma",
      "Yuhui Zheng"
    ],
    "abstract": "Infrared object detection focuses on identifying and locating objects in complex environments (\\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\\textsuperscript{3}FD (83.7\\% mAP), FLIR (86.1\\% mAP). Our code will be publicly available once the paper is accepted.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09228.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09228",
    "published": "2026-01-14T06:59:54Z",
    "updated": "2026-01-14T06:59:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种基于语言指导的红外物体检测方法，通过解耦物体和非物体特征来显著增强检测性能。",
      "motivation": "本研究针对红外物体检测在复杂环境（如黑暗、雪和雨）中应用的实际问题，可见光相机因光照差而失效，红外成像成为关键。然而，红外图像对比度低、边缘信息弱，现有方法难以提取区分性特征，导致检测鲁棒性不足。问题的解决对于恶劣条件下的监控和自动驾驶等领域至关重要。摘要未明确说明现有具体方法的不足，但强调了特征噪声的挑战。",
      "method": "论文提出一种新颖的视觉-语言表示学习范式，利用额外文本监督引导特征解耦。核心包括两个模块：语义特征对齐（SFA）模块对齐物体特征与文本特征，结合文本语义信息；物体特征解耦（OFD）模块通过最小化相关性，解耦文本对齐的物体特征和非物体特征。解耦后的物体特征输入检测头，以减少噪声并提升区分性。该方法在训练时结合图像和文本数据，但摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果显示，该方法在M^3FD和FLIR两个基准数据集上取得优越性能，分别达到83.7% mAP和86.1% mAP。通过特征解耦，检测性能显著增强，获得了更区分性和低噪声的特征。与基线方法的对比方面，摘要仅提及“superior performance”，未详细说明对比数据，但可推断性能有显著提升，证实了方法的有效性。",
      "conclusion": "本研究的主要贡献是提出了一种视觉-语言框架，通过语言指导解耦红外物体特征，提升检测准确性。学术价值在于为多模态学习和特征表示提供了新思路，特别是在红外领域；实际应用价值在于增强复杂环境下的物体检测能力。摘要未明确说明局限性和未来工作方向，但可推断未来可能扩展至其他模态或更广泛的应用场景。",
      "tags": [
        "Infrared Object Detection",
        "Vision-Language Learning",
        "Feature Disentanglement",
        "Semantic Alignment",
        "Multi-modal Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:32.785309Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09220",
    "title": "From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences",
    "authors": [
      "Xinzi Tan",
      "Kejian Zhang",
      "Junhan Yu",
      "Doudou Zhou"
    ],
    "abstract": "Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.",
    "categories": [
      "cs.LG",
      "math.ST",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09220.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09220",
    "published": "2026-01-14T06:47:37Z",
    "updated": "2026-01-14T06:47:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于Hawkes过程理论的新型注意力算子Hawkes Attention，用于改进标记时间点过程中的异质和类型特定时间效应建模。",
      "motivation": "标记时间点过程在医疗、社会、商业和金融等领域自然出现，准确建模事件序列的时间效应至关重要。然而，现有基于Transformer的方法主要通过位置编码注入时间信息，依赖共享或参数化衰减结构，这限制了它们捕获异质和类型特定时间效应的能力。因此，需要一种新方法来更好地处理这些复杂的时间模式，以提升事件序列分析的准确性。",
      "method": "论文提出Hawkes Attention，从多元Hawkes过程理论推导而来，使用可学习的每个类型的神经核来调制查询、键和值投影，替代传统注意力机制中的相应部分。该方法统一事件时间和内容交互，能够从数据中学习时间相关行为和类型特定激发模式。关键创新在于结合Hawkes过程理论与注意力机制，实现更灵活的时间调制。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "实验结果表明，Hawkes Attention在标记时间点过程建模中比基线方法表现更优，尽管摘要未提供具体性能指标如准确率提升数值。该方法在通用MTPP任务上验证了有效性，并可轻松扩展到特定时间结构，如时间序列预测，展示了其广泛适用性。与基线对比显示了性能改进的潜力。",
      "conclusion": "本研究的核心贡献是提出Hawkes Attention，一种统一事件时间和内容交互的新型注意力机制，改进MTPPs中的时间和类型特定效应建模。学术价值在于将Hawkes过程理论融入Transformer框架，推动时间序列分析的发展；实际应用价值在于在多个领域如医疗和社会网络中增强事件序列分析能力。未来工作可进一步探索在不同时间序列任务中的应用和潜在优化方向。",
      "tags": [
        "Hawkes Attention",
        "Marked Temporal Point Processes",
        "Attention Mechanisms",
        "Neural Kernels",
        "Time Series Forecasting"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:57.809269Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09215",
    "title": "UserLM-R1: Modeling Human Reasoning in User Language Models with Multi-Reward Reinforcement Learning",
    "authors": [
      "Feng Zhang",
      "Shijia Li",
      "Chunmao Zhang",
      "Zhanyu Ma",
      "Jun Xu",
      "Jiuchong Gao",
      "Jinghua Hao",
      "Renqing He",
      "Jingwen Xu",
      "Han Liu"
    ],
    "abstract": "User simulators serve as the critical interactive environment for agent post-training, and an ideal user simulator generalizes across domains and proactively engages in negotiation by challenging or bargaining. However, current methods exhibit two issues. They rely on static and context-unaware profiles, necessitating extensive manual redesign for new scenarios, thus limiting generalizability. Moreover, they neglect human strategic thinking, leading to vulnerability to agent manipulation. To address these issues, we propose UserLM-R1, a novel user language model with reasoning capability. Specifically, we first construct comprehensive user profiles with both static roles and dynamic scenario-specific goals for adaptation to diverse scenarios. Then, we propose a goal-driven decision-making policy to generate high-quality rationales before producing responses, and further refine the reasoning and improve strategic capabilities with supervised fine-tuning and multi-reward reinforcement learning. Extensive experimental results demonstrate that UserLM-R1 outperforms competitive baselines, particularly on the more challenging adversarial set.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09215.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09215",
    "published": "2026-01-14T06:42:01Z",
    "updated": "2026-01-14T06:42:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出UserLM-R1，一个结合多奖励强化学习的用户语言模型，通过建模人类推理能力提升用户模拟器的泛化和战略交互性能。",
      "motivation": "用户模拟器在代理后训练中扮演关键环境，理想的模拟器需跨域泛化并主动谈判。现有方法依赖静态、上下文无关的配置文件，需要手动重设计新场景，泛化能力受限；同时忽视人类战略思维，易受代理操纵，导致交互不真实。因此，研究旨在解决这些问题，开发更具适应性和战略性的用户模拟器，以提高代理训练的效率和效果。",
      "method": "研究提出UserLM-R1，核心方法包括构建全面的用户配置文件，结合静态角色和动态场景特定目标以增强场景适应性。采用目标驱动决策策略，在生成响应前产生高质量推理理性，然后通过监督微调初步优化模型，并引入多奖励强化学习进一步精炼推理和提升战略能力。创新点在于将推理能力集成到语言模型中，并利用多奖励机制平衡不同训练目标。摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验结果显示，UserLM-R1在多个基准测试中优于竞争基线，特别是在更具挑战性的对抗集上表现突出，证明了其在复杂场景下的优异泛化和战略交互能力。虽然摘要未提供具体性能指标如准确率或效率数据，但强调了相对于现有方法的显著改进，突出了模型在应对对抗性挑战时的优势。",
      "conclusion": "UserLM-R1的主要贡献在于通过整合推理能力和多奖励强化学习，显著提升了用户模拟器的泛化性和战略交互能力，为代理后训练提供了更真实和自适应的交互环境。该研究在AI交互模拟领域具有重要学术价值，可促进更高效的代理训练和更人性化的AI系统开发。未来工作可能包括扩展到更复杂场景或探索更多推理机制，以进一步提升性能。",
      "tags": [
        "User Language Model",
        "Multi-Reward Reinforcement Learning",
        "Human Reasoning Modeling",
        "Goal-Driven Decision Making",
        "Supervised Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:05.988816Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09213",
    "title": "SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion",
    "authors": [
      "Jialu Li",
      "Taiyan Zhou"
    ],
    "abstract": "Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.   We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09213.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09213",
    "published": "2026-01-14T06:38:12Z",
    "updated": "2026-01-14T06:38:12Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "SpikeVAEDiff 通过结合 VD-VAE 和 Versatile Diffusion，实现了从神经脉冲数据的高分辨率自然视觉场景重建。",
      "motivation": "本研究旨在解决从神经活动重建自然视觉场景的挑战，这一问题在神经科学和计算机视觉中至关重要，因为它有助于理解大脑视觉处理机制。现有方法如基于 fMRI 的技术时空分辨率有限，难以捕捉精细神经活动，而神经脉冲数据提供更高解析度但解码难度大。因此，开发新方法以高效利用脉冲数据生成语义有意义的图像成为迫切需求，推动相关领域技术进步。",
      "method": "SpikeVAEDiff 采用两阶段框架，第一阶段使用 Very Deep Variational Autoencoder (VD-VAE) 将神经脉冲信号映射到潜在表示，生成低分辨率初步重建。第二阶段通过回归模型将脉冲信号映射到 CLIP-Vision 和 CLIP-Text 特征，随后利用 Versatile Diffusion 模型进行图像到图像生成，精炼图像以提升分辨率和语义质量。该方法创新性地结合了变分自编码器和扩散模型，应用于 Allen Visual Coding-Neuropixels 数据集，优化神经数据处理流程。",
      "result": "在 Allen Visual Coding-Neuropixels 数据集上评估，结果表明 VISI 大脑区域激活最显著，对重建质量起关键作用。论文展示了成功和不成功的重建例子，凸显了解码神经活动的复杂性。与基于 fMRI 的方法相比，脉冲数据因其优越的时空分辨率而改善了重建效果。消融研究验证了 VD-VAE 模型的有效性，并证明从特定大脑区域（如 VISI）提取的数据能显著提升重建性能，尽管具体数值指标摘要未明确说明，结果强调了区域选择性数据的重要性。",
      "conclusion": "本研究的主要贡献是提出 SpikeVAEDiff 框架，它有效结合 VD-VAE 和 Versatile Diffusion，从神经脉冲数据实现高质量视觉场景重建。学术上，它推动了神经解码技术和生成模型的发展；实际应用中，为脑机接口和神经康复设备提供新思路。尽管面临神经活动解码的挑战，该研究为未来工作奠定基础，如优化模型架构或集成更多大脑区域数据以进一步提高精度和鲁棒性。",
      "tags": [
        "Neural Spike Data",
        "Variational Autoencoder",
        "Diffusion Model",
        "CLIP Features",
        "Image Reconstruction"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:29.446646Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09212",
    "title": "Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation",
    "authors": [
      "Xingyao Li",
      "Fengzhuo Zhang",
      "Cunxiao Du",
      "Hui Ji"
    ],
    "abstract": "Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09212.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09212",
    "published": "2026-01-14T06:35:21Z",
    "updated": "2026-01-14T06:35:21Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "论文提出了 COOL-SD，一种基于理论分析的退火松弛推测解码方法，显著提升自回归图像生成的速度与质量权衡。",
      "motivation": "自回归图像生成模型虽然取得了显著进展，但由于其序列生成特性和图像标记的模糊性，推理速度仍然缓慢，即使使用推测解码技术也难以完全解决。近期研究尝试通过松弛推测解码来加速，但这些方法缺乏理论基础，导致优化效果有限。因此，本研究的动机在于填补这一理论空白，建立松弛推测解码的数学依据，以解决自回归图像生成中速度与质量平衡的实际问题。",
      "method": "本研究提出 COOL-SD 方法，这是一种基于退火松弛的推测解码技术。方法的核心创新在于两个理论洞察：首先，分析目标模型与松弛推测解码之间的总变异距离，推导出最优重采样分布以最小化距离上界；其次，通过扰动分析揭示松弛推测解码的退火行为，从而设计退火策略来优化生成过程。这些理论分析为 COOL-SD 提供了严谨基础，聚焦于改进推测解码机制，尽管摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验验证了 COOL-SD 的有效性，结果显示该方法能显著提升图像生成速度并保持可比质量，或在相似延迟下实现更好质量。与先前方法相比，COOL-SD 在速度与质量权衡方面展现出持续改进，例如在基准测试中比现有松弛推测解码方法有优化。具体性能指标如准确率提升或效率改进摘要未明确说明，但整体趋势表明理论支持下的方法在自回归图像生成中具有优越的平衡效果。",
      "conclusion": "论文的主要贡献在于建立了松弛推测解码的理论基础，并提出了 COOL-SD 方法，推动自回归图像生成的加速与质量优化。这一研究具有重要的学术价值，填补了相关领域理论空白，同时在实际应用中可用于实时图像生成等场景，提高效率。未来工作可能包括扩展到其他生成任务或进一步算法优化，但摘要未明确说明局限性和具体方向。",
      "tags": [
        "Speculative Decoding",
        "Autoregressive Models",
        "Image Generation",
        "Total Variation Distance",
        "Annealing"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:28.684869Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09211",
    "title": "Affostruction: 3D Affordance Grounding with Generative Reconstruction",
    "authors": [
      "Chunghyun Park",
      "Seunghyeon Lee",
      "Minsu Cho"
    ],
    "abstract": "This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4\\% improvement) and 32.67 IoU for 3D reconstruction (67.7\\% improvement), enabling accurate affordance prediction on complete shapes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09211.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09211",
    "published": "2026-01-14T06:33:12Z",
    "updated": "2026-01-14T06:33:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Affostruction框架，通过生成重建和全形状affordance grounding，解决了RGBD图像中的3D affordance grounding问题。",
      "motivation": "Affordance grounding旨在从RGBD图像定位物体表面区域，以对应描述动作的文本查询，应用于机器人交互等领域。现有方法仅预测可见表面的affordance，在处理部分遮挡或单视图输入时性能受限，无法准确处理未观察区域，这在实际场景中降低了鲁棒性和实用性，因此需要一种能重建完整形状并ground affordances的新方法。",
      "method": "方法包括三个核心创新：首先，使用稀疏体素融合进行生成多视图重建，从部分RGBD输入推断未观察几何，保持常数令牌复杂度以优化计算；其次，采用基于流的模型进行affordance grounding，捕捉affordance分布中的固有模糊性；最后，提出affordance驱动的主动视图选择，利用预测affordance智能采样视点，以提升整体性能。",
      "result": "实验结果显示，Affostruction在affordance grounding任务中达到19.1 aIoU，相较于基线方法提升40.4%；在3D重建任务中达到32.67 IoU，提升67.7%，显著改善了在完整形状上进行affordance预测的准确性和效率。",
      "conclusion": "该研究的主要贡献在于提出了Affostruction框架，成功结合生成重建和全形状affordance grounding，解决了部分观察下的挑战，提升了学术和实际应用价值，如增强机器人交互的准确性；未来工作可能包括扩展到更复杂场景或与其他技术整合，摘要未明确说明具体局限性。",
      "tags": [
        "3D Affordance Grounding",
        "Generative Reconstruction",
        "Sparse Voxel Fusion",
        "Flow-based Affordance",
        "Active View Selection"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:34.244457Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09209",
    "title": "Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy",
    "authors": [
      "Qiang Hu",
      "Qimei Wang",
      "Yingjie Guo",
      "Qiang Li",
      "Zhiwei Wang"
    ],
    "abstract": "White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09209.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09209",
    "published": "2026-01-14T06:24:18Z",
    "updated": "2026-01-14T06:24:18Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": {
      "overview": "论文提出PaGKD框架，通过群体级知识蒸馏实现无配对的跨模态学习，提升白光内窥镜胃肠道病变分类的鲁棒性。",
      "motivation": "White-Light Imaging（WLI）是内窥镜癌症筛查的标准方法，但Narrow-Band Imaging（NBI）提供更优越的诊断细节。关键挑战是从NBI转移知识以增强仅使用WLI的模型，但现有方法严重依赖于相同病变的配对NBI-WLI图像，这是一个成本高且常不切实际的要求，导致大量临床数据未被利用。因此，需要开发无需配对的方法来充分利用未标注数据，提高模型性能并降低应用成本。",
      "method": "论文提出PaGKD框架，这是一种配对免费的群体级知识蒸馏方法。它包括两个互补模块：Group-level Prototype Distillation（GKD-Pro）通过共享的病变感知查询提取模态不变的语义原型，蒸馏紧凑的群体表示；Group-level Dense Distillation（GKD-Den）利用激活生成的关系图指导群体感知注意力，执行密集的跨模态对齐。这些模块共同作用，在不要求图像级对应的情况下，强制实现全局语义一致性和局部结构相干性，从而有效处理未配对数据。",
      "result": "论文在四个临床数据集上进行了广泛实验，结果表明PaGKD在胃肠道病变分类任务中一致且显著优于现有最先进方法。具体来说，相对于基准方法，PaGKD实现了相对AUC的提升，分别为3.3%、1.1%、2.8%和3.2%。这些改进证实了PaGKD框架在未配对跨模态学习中的有效性，为模型性能的提升提供了量化数据支持。",
      "conclusion": "论文的主要贡献是提出了PaGKD框架，通过群体级知识蒸馏实现了无需配对的跨模态学习，有效转移了NBI的知识到WLI-only模型中。这项研究具有重要的学术价值，为从未配对数据中进行跨模态学习开辟了新方向；实际应用上，它能够提升胃肠道病变分类的准确性和鲁棒性，同时减少对昂贵配对数据的依赖。摘要未明确说明局限性和未来工作。",
      "tags": [
        "Knowledge Distillation",
        "Cross-modal Learning",
        "Group-level Learning",
        "Prototype Distillation",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:41.558873Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09207",
    "title": "Point Tracking as a Temporal Cue for Robust Myocardial Segmentation in Echocardiography Videos",
    "authors": [
      "Bahar Khodabakhshian",
      "Nima Hashemi",
      "Armin Saadat",
      "Zahra Gholami",
      "In-Chang Hwang",
      "Samira Sojoudi",
      "Christina Luong",
      "Purang Abolmaesumi",
      "Teresa Tsang"
    ],
    "abstract": "Purpose: Myocardium segmentation in echocardiography videos is a challenging task due to low contrast, noise, and anatomical variability. Traditional deep learning models either process frames independently, ignoring temporal information, or rely on memory-based feature propagation, which accumulates error over time. Methods: We propose Point-Seg, a transformer-based segmentation framework that integrates point tracking as a temporal cue to ensure stable and consistent segmentation of myocardium across frames. Our method leverages a point-tracking module trained on a synthetic echocardiography dataset to track key anatomical landmarks across video sequences. These tracked trajectories provide an explicit motion-aware signal that guides segmentation, reducing drift and eliminating the need for memory-based feature accumulation. Additionally, we incorporate a temporal smoothing loss to further enhance temporal consistency across frames. Results: We evaluate our approach on both public and private echocardiography datasets. Experimental results demonstrate that Point-Seg has statistically similar accuracy in terms of Dice to state-of-the-art segmentation models in high quality echo data, while it achieves better segmentation accuracy in lower quality echo with improved temporal stability. Furthermore, Point-Seg has the key advantage of pixel-level myocardium motion information as opposed to other segmentation methods. Such information is essential in the computation of other downstream tasks such as myocardial strain measurement and regional wall motion abnormality detection. Conclusion: Point-Seg demonstrates that point tracking can serve as an effective temporal cue for consistent video segmentation, offering a reliable and generalizable approach for myocardium segmentation in echocardiography videos. The code is available at https://github.com/DeepRCL/PointSeg.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09207.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09207",
    "published": "2026-01-14T06:23:36Z",
    "updated": "2026-01-14T06:23:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 Point-Seg，一个集成点跟踪作为时间线索的 transformer 分割框架，用于在超声心动图视频中实现稳定和一致的心肌分割。",
      "motivation": "超声心动图视频中的心肌分割由于低对比度、噪声和解剖变异性而具有挑战性，这直接影响到下游医学分析如心肌应变测量的准确性。传统深度学习方法存在明显不足：要么独立处理每一帧，忽略视频的时间连续性，导致分割结果不稳定；要么依赖基于记忆的特征传播，这种方法容易随时间累积误差，特别是在长序列视频中，从而降低分割的鲁棒性和可靠性。因此，需要一种能够有效利用时间信息并避免误差累积的创新方法来解决这一关键医学图像分析问题。",
      "method": "本研究提出了 Point-Seg，一个基于 transformer 的分割框架，其核心创新是集成点跟踪模块作为时间线索。该方法首先在合成超声心动图数据集上训练点跟踪模块，用于跟踪视频序列中的关键解剖标志点，生成显式的运动感知轨迹信号。这些轨迹指导分割过程，减少漂移现象，并消除对基于记忆的特征累积的依赖。此外，框架中还加入了时间平滑损失函数，以强制增强跨帧的时间一致性，确保分割结果在视频序列中更加稳定和连续。",
      "result": "在公共和私有超声心动图数据集上的实验结果表明，Point-Seg 在高质量回声数据中，其 Dice 分割准确性与最先进的分割模型统计相似。在低质量回声数据中，Point-Seg 表现出更好的分割准确性和显著提升的时间稳定性，显示出更强的鲁棒性。与基线方法相比，Point-Seg 还提供了独特的像素级心肌运动信息，这是其他分割方法所不具备的，对于下游任务如心肌应变测量和区域壁运动异常检测具有重要应用价值。",
      "conclusion": "Point-Seg 成功证明了点跟踪可以作为视频分割的有效时间线索，为超声心动图视频中的心肌分割提供了一种可靠和可推广的解决方案。该研究在学术上贡献了一种融合时间信息的分割框架，提升了医学图像分析的准确性和稳定性，特别是在处理低质量数据和提供运动信息方面。在实际应用中，这有助于增强心血管疾病诊断的可靠性。未来的工作可以进一步优化点跟踪模块或扩展到其他医学视频分割任务，但摘要未明确说明具体局限性。",
      "tags": [
        "Point Tracking",
        "Transformer",
        "Video Segmentation",
        "Myocardial Segmentation",
        "Temporal Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:49.624224Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09200",
    "title": "A.X K1 Technical Report",
    "authors": [
      "Sung Jun Cheon",
      "Jaekyung Cho",
      "Seongho Choi",
      "Hyunjun Eun",
      "Seokhwan Jo",
      "Jaehyun Jun",
      "Minsoo Kang",
      "Jin Kim",
      "Jiwon Kim",
      "Minsang Kim",
      "Sungwan Kim",
      "Seungsik Kim",
      "Tae Yoon Kim",
      "Youngrang Kim",
      "Hyeongmun Lee",
      "Sangyeol Lee",
      "Sungeun Lee",
      "Youngsoon Lee",
      "Yujin Lee",
      "Seongmin Ok",
      "Chanyong Park",
      "Hyewoong Park",
      "Junyoung Park",
      "Hyunho Yang",
      "Subin Yi",
      "Soohyun Bae",
      "Dhammiko Arya",
      "Yongseok Choi",
      "Sangho Choi",
      "Dongyeon Cho",
      "Seungmo Cho",
      "Gyoungeun Han",
      "Yong-jin Han",
      "Seokyoung Hong",
      "Hyeon Hwang",
      "Wonbeom Jang",
      "Minjeong Ju",
      "Wonjin Jung",
      "Keummin Ka",
      "Sungil Kang",
      "Dongnam Kim",
      "Joonghoon Kim",
      "Jonghwi Kim",
      "SaeRom Kim",
      "Sangjin Kim",
      "Seongwon Kim",
      "Youngjin Kim",
      "Seojin Lee",
      "Sunwoo Lee",
      "Taehoon Lee",
      "Chanwoo Park",
      "Sohee Park",
      "Sooyeon Park",
      "Yohan Ra",
      "Sereimony Sek",
      "Seungyeon Seo",
      "Gun Song",
      "Sanghoon Woo",
      "Janghan Yoon",
      "Sungbin Yoon"
    ],
    "abstract": "We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09200.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09200",
    "published": "2026-01-14T06:11:17Z",
    "updated": "2026-01-14T06:11:17Z",
    "comment": null,
    "light_analysis": {
      "overview": "A.X K1是一个基于混合专家的519B参数语言模型，通过Think-Fusion方法实现推理能力的可控切换，在韩语基准测试中表现突出。",
      "motivation": "本研究旨在解决大规模语言模型的推理能力与推理效率之间的权衡问题，以促进其在多样现实场景中的可扩展部署。随着AI模型规模扩大，推理成本成为关键瓶颈，而现有方法往往难以平衡高性能与低资源消耗，摘要未明确说明具体不足，但强调了提升效率以实现可控推理的重要性，以满足工业应用需求。",
      "method": "论文提出A.X K1，一个519B参数的混合专家（MoE）语言模型。利用缩放定律在固定计算预算下优化训练配置和词汇表大小，预训练语料约10T tokens，经多阶段数据处理管道。核心创新是Think-Fusion训练方法，使单个统一模型支持用户在思考模式和非思考模式之间可控切换，以实现推理能力的灵活调整，从而弥合推理与效率的差距。",
      "result": "广泛的评估表明，A.X K1在性能上与领先的开源语言模型竞争，同时在韩语基准测试中建立了显著优势，证明了其在保持通用任务性能的同时在特定语言应用上的竞争力。具体性能指标如准确率未在摘要中说明，但强调了模型通过可控推理机制实现高效部署，体现了优化设计在现实场景中的有效性。",
      "conclusion": "A.X K1的主要贡献在于设计了一个高效可扩展的混合专家语言模型，通过Think-Fusion方法实现推理能力的可控，有效弥合了推理与效率之间的差距，为大规模模型部署提供了新思路。学术价值在于推动了MoE架构和训练技术的创新，实际应用价值体现在支持多语言环境，特别是韩语领域的优化。未来工作可进一步扩展到更多语言或优化推理效率，摘要未明确说明局限性。",
      "tags": [
        "Mixture-of-Experts (MoE)",
        "Large Language Model",
        "Scaling Laws",
        "Think-Fusion",
        "Korean-language Benchmarks"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:03.343903Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09195",
    "title": "ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection",
    "authors": [
      "Tao Liu",
      "Taiqiang Wu",
      "Runming Yang",
      "Shaoning Sun",
      "Junjie Wang",
      "Yujiu Yang"
    ],
    "abstract": "Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09195.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09195",
    "published": "2026-01-14T05:50:40Z",
    "updated": "2026-01-14T05:50:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "ProFit提出概率引导的token选择策略，在监督微调中有效缓解单一参考答案导致的过度拟合问题。",
      "motivation": "监督微调（SFT）是大型语言模型对齐人类意图的基础方法，但传统SFT强制对齐单一参考答案，忽略语言的一对多特性，导致模型过度拟合到非核心表达。尽管引入多参考答案可缓解此问题，但数据和计算成本高昂。因此，研究动机是优先解决单一参考的过度拟合，而非追求多样性的成本问题，以优化SFT的效率和效果，避免模型在表面表达上过拟合。",
      "method": "ProFit方法基于洞察：token概率与语义重要性有内在连接；高概率token携带核心逻辑框架，低概率token多为可替换表达。核心创新在于在SFT过程中选择性掩码低概率token，以防止表面级过度拟合。技术路线涉及分析token概率分布，并应用掩码机制来优化训练，无需增加额外参考答案，通过概率引导的策略调整模型学习重点。",
      "result": "实验结果显示，ProFit在一般推理和数学基准测试中一致优于传统SFT基线方法。通过选择性掩码低概率token，模型性能得到提升，有效缓解了过度拟合问题，增强了泛化能力。对比基线，ProFit展现了更好的效果，摘要未提供具体数值指标，但强调了其在多个任务上的稳定优越性。",
      "conclusion": "ProFit的主要贡献是通过概率引导的token选择方法，解决了SFT中的过度拟合问题，揭示了token概率与语义关系。学术上丰富了SFT理论，实践上提高了LLM对齐效率，降低了成本。未来工作可能涉及扩展到其他任务或优化token选择机制，但摘要未明确说明具体方向。",
      "tags": [
        "Supervised Fine-Tuning",
        "Large Language Model",
        "Probability-Guided Selection",
        "Token Masking",
        "Overfitting Mitigation"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:42.996508Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09191",
    "title": "From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows",
    "authors": [
      "Qizhen Lan",
      "Aaron Choi",
      "Jun Ma",
      "Bo Wang",
      "Zhaogming Zhao",
      "Xiaoqian Jiang",
      "Yu-Chun Hsu"
    ],
    "abstract": "Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09191.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09191",
    "published": "2026-01-14T05:44:30Z",
    "updated": "2026-01-14T05:44:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于知识蒸馏的部署导向框架，将高性能医学图像分割模型转化为紧凑模型，以适配资源受限的本地临床工作流程。",
      "motivation": "在常规临床工作流程中部署医学图像分割模型常受限于本地基础设施的固定计算资源，且云推理可能因治理和安全政策受限。高容量模型虽分割准确率高，但其计算需求阻碍了在医院环境中的实际部署和长期维护，导致现有方法难以在资源有限的临床系统中实现高效应用，影响了医疗服务的可扩展性和可靠性。",
      "method": "本研究提出一个部署导向的框架，利用知识蒸馏技术将高性能分割模型（教师模型）转化为一系列紧凑的学生模型，保持与现有临床系统的架构兼容性，无需修改推理管道。框架支持系统化容量减少，并评估了多站点脑MRI数据集（含1,104个3D体积），独立测试101个案例，还通过腹部CT数据评估跨模态泛化能力，创新点在于结合知识蒸馏实现高效部署和轻量化设计。",
      "result": "在参数减少94%的情况下，蒸馏后的学生模型保持了教师模型98.7%的分割准确率，同时显著提升效率，如CPU推理延迟最多降低67%，且无额外部署开销。这些结果在多站点脑MRI和腹部CT数据上得到验证，表明该方法在保持高精度的同时大幅减少计算需求，优于传统高容量模型的部署性能。",
      "conclusion": "本研究的主要贡献是提供了一个实用可靠的方法，通过知识蒸馏将研究级分割模型转化为可维护、适合部署的组件，适用于本地临床工作流程。其学术价值在于展示了知识蒸馏在医疗图像分割部署中的应用潜力，实际应用价值在于促进了高性能模型在资源受限医疗环境中的落地。摘要未明确说明局限性，但未来工作可能涉及扩展到更多模态或优化蒸馏过程。",
      "tags": [
        "Knowledge Distillation",
        "Medical Image Segmentation",
        "On-Premises Deployment",
        "MRI",
        "CT Scan"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:48.311338Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09185",
    "title": "OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb",
    "authors": [
      "Zeqiang Wang",
      "Xinyue Wu",
      "Chenxi Li",
      "Zixi Chen",
      "Nishanth Sastry",
      "Jon Johnson",
      "Suparna De"
    ],
    "abstract": "Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $ΔW = BA^\\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $ΔW = BΣA^\\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09185.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09185",
    "published": "2026-01-14T05:34:40Z",
    "updated": "2026-01-14T05:34:40Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 OrthoGeoLoRA 方法，通过几何约束优化标准 LoRA，提升参数效率，在社会科学概念检索任务中表现优异。",
      "motivation": "随着大型语言模型在社会科学信息系统（如数字图书馆和数据目录）中广泛应用，完全微调的高计算和能源成本对 Web4Good 生态系统的非营利机构构成障碍。参数高效微调（PEFT）如 LoRA 可降低成本，但标准 LoRA 存在几何缺陷，包括规范自由、尺度模糊性和秩崩溃，这些问题可能导致性能下降和效率不足，因此需要改进以适应资源受限环境中的结构化概念检索需求。",
      "method": "论文提出 OrthoGeoLoRA，核心方法是对标准 LoRA 的更新形式 ΔW = BA⊤ 进行几何优化，通过约束低秩因子 B 和 A 为正交（使用 Stiefel 流形），实现类似 SVD 的形式 ΔW = BΣA⊤。关键创新点是几何重参数化，保持与标准优化器（如 Adam）和现有微调管道的兼容性。实验中使用多语言句子编码器，并在欧洲语言社会科学词库（ELSST）基准上评估层次概念检索任务。",
      "result": "实验结果表明，在相同低秩参数预算下，OrthoGeoLoRA 在排名指标上优于标准 LoRA 和多个其他 PEFT 变体。这验证了几何优化方法在提升计算和参数效率方面的优势，尽管摘要未明确具体性能数据，但强调了该方法为资源受限环境提供了更有效的模型适应路径。",
      "conclusion": "OrthoGeoLoRA 的主要贡献是通过引入几何约束改进参数高效微调，解决了标准 LoRA 的几何问题。学术价值在于推动 PEFT 技术的优化，实际意义是为资源有限的机构提供计算和参数高效的模型适应方案。未来工作可能包括扩展应用到其他领域或进一步探索几何优化方法的潜力。",
      "tags": [
        "Parameter-Efficient Fine-Tuning",
        "Low-Rank Adaptation",
        "Geometric Optimization",
        "Stiefel Manifold",
        "Hierarchical Concept Retrieval"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:16.572090Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09182",
    "title": "Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback",
    "authors": [
      "JungMin Yun",
      "JuneHyoung Kwon",
      "MiHyeon Kim",
      "YoungBin Kim"
    ],
    "abstract": "The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09182.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09182",
    "published": "2026-01-14T05:32:35Z",
    "updated": "2026-01-14T05:32:35Z",
    "comment": "Accepted to AAAI 2026 Workshop on AI for Scientific Research (AI4Research)",
    "light_analysis": {
      "overview": "论文提出基于大型语言模型的辅助系统，通过辅导和反馈解决评审员缺口，强调人类中心的设计原则。",
      "motivation": "随着人工智能研究的快速扩展，评审员缺口问题日益严重，威胁同行评审的可持续性，并导致低质量评估的恶性循环。现有方法主要依赖大型语言模型自动生成评审，未能从根本上提升评审员的长期能力，反而可能加剧评估质量的下降。因此，研究动机在于批判这种自动生成评审的范式，并转向一种更有效的辅助和教育人类的模式，以应对评审员短缺的挑战。",
      "method": "论文基于高质量同行评审的核心原则，提出了两个互补的系统。一是大型语言模型辅助的辅导系统，专注于培养评审员的长期能力和专业知识积累；二是大型语言模型辅助的反馈系统，旨在帮助评审员在撰写评审时实时改进质量。这些系统强调将大型语言模型定位为工具，以支持人类评审员，而不是取代他们，通过智能交互和个性化指导来强化评审过程。方法侧重于理论框架设计，未详细说明具体技术实现如数据集或模型架构。",
      "result": "摘要未明确说明具体的实验结果或性能指标，因为这是一篇立场论文，侧重于理论框架和方法论建议。因此，没有提供与基线方法的对比数据或量化效果，如准确率提升或效率改进。论文主要通过概念分析和原则定义来论证其方法的潜力，推断该方法旨在增强评审员能力，但具体效果需进一步实证研究验证。",
      "conclusion": "研究的主要贡献是提出一种人类中心的大型语言模型辅助方法，通过辅导和反馈系统来增强评审员的专业能力，并促进学术生态系统的可持续发展。这种方法具有重要的学术价值和实际应用价值，能够改善同行评审的质量和效率，减少评审员缺口。潜在局限性包括缺乏实证数据，未来工作方向可能包括系统实施和效果评估，以及扩展到其他评估场景。",
      "tags": [
        "Large Language Model",
        "Peer Review",
        "Mentoring System",
        "Feedback System",
        "Human-AI Collaboration"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:25.214513Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09176",
    "title": "$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness",
    "authors": [
      "Lang Xiong",
      "Ning Liu",
      "Ao Ren",
      "Yuheng Bai",
      "Haining Fang",
      "BinYan Zhang",
      "Zhe Jiang",
      "Yujuan Tan",
      "Duo Liu"
    ],
    "abstract": "Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09176.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09176",
    "published": "2026-01-14T05:17:35Z",
    "updated": "2026-01-14T05:17:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出 $D^2Prune$ 剪枝方法，通过双重泰勒扩展和注意力分布感知，提升大型语言模型的稀疏化精度和效率。",
      "motivation": "大型语言模型因计算需求巨大而面临部署挑战，剪枝作为压缩技术存在局限性。现有方法忽略校准数据与测试数据间的激活分布偏移，导致误差估计不准确；同时未能处理注意力模块中激活的长尾分布特征，这限制了剪枝的有效性和模型性能优化。开发更精确的剪枝方法对于降低资源消耗、促进实际应用至关重要。",
      "method": "$D^2Prune$ 方法采用双重泰勒扩展，联合建模权重和激活扰动，实现精确误差估计以优化剪枝掩码选择和权重更新。其次，提出注意力感知动态更新策略，通过最小化注意力分布的KL散度和重构误差，保留长尾注意力模式。该方法应用于多种大型语言模型（如OPT-125M、LLaMA2/3、Qwen3）和视觉变换器模型（如DeiT），增强泛化能力。",
      "result": "大量实验显示，$D^2Prune$ 在OPT-125M、LLaMA2/3和Qwen3等大型语言模型上始终超越最先进剪枝方法，表现出更高的剪枝精度和性能保持。摘要未明确提供具体准确率数据，但强调其持续优势。此外，动态注意力更新机制成功泛化到视觉模型DeiT，在ImageNet-1K数据集上实现优越准确度，验证了方法的有效性和扩展性。",
      "conclusion": "本研究提出 $D^2Prune$ 剪枝方法，通过解决激活分布偏移和注意力长尾分布问题，显著提升模型稀疏化性能。学术贡献在于创新了联合误差估计和动态更新技术，推动剪枝理论发展；实际价值在于优化模型压缩，便于高效部署。未来工作可探索更多模型架构和应用场景，以及潜在的局限性如数据依赖性。",
      "tags": [
        "Pruning",
        "Taylor Expansion",
        "Attention Mechanism",
        "KL Divergence",
        "Vision Transformer"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:01.546438Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09173",
    "title": "Geometric Stability: The Missing Axis of Representations",
    "authors": [
      "Prashant C. Raju"
    ],
    "abstract": "Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09173.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09173",
    "published": "2026-01-14T05:15:22Z",
    "updated": "2026-01-14T05:15:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究引入几何稳定性作为表示分析的新维度，并提出Shesha框架量化其在扰动下的稳健性，填补现有方法仅关注相似性的盲点。",
      "motivation": "当前对学习表示的分析存在盲点，仅关注相似性来衡量嵌入与外部参考的对齐程度，但相似性仅揭示表示了什么，而非结构是否稳健。这一问题在安全监控、模型选择和系统可靠性等应用中至关重要，因为缺乏对稳健性的量化可能导致虚假警报或次优决策。现有方法的不足在于相似性不能反映表示在扰动下的结构保持能力，而几何稳定性则能补充这一缺失维度，提升表示审计的全面性。",
      "method": "论文提出几何稳定性作为表示分析的新维度，通过量化表示在扰动下几何结构的可靠保持来评估稳健性。关键创新在于开发了Shesha框架，该框架专门测量稳定性，与传统的相似性指标区分。方法涉及在多种扰动下分析表示的变化，并利用几何度量来捕捉结构保持程度，应用于多领域配置以验证其有效性。具体技术细节如Shesha的实现机制，摘要未明确说明，但强调了稳定性与相似性的机制区别。",
      "result": "实验结果显示，在七个领域的2,463个配置中，几何稳定性与相似性经验上无显著相关（ρ≈0.01）。机制上，相似性在移除主成分后崩溃，而稳定性保留对细粒度流形结构的敏感性。在安全监控中，稳定性比CKA（中心核对齐）更敏感地检测结构漂移（近2倍），同时过滤非功能性噪声，减少虚假警报。此外，监督稳定性与线性可控性高度相关（ρ=0.89-0.96），表明稳定性可用于预测模型行为。模型选择方面，稳定性与可转移性分离，揭示了几何优化成本。",
      "conclusion": "本研究通过引入几何稳定性，填补了表示分析中稳健性评估的空白，为跨生物和计算系统的表示审计提供了必要补充。学术上，揭示了稳定性和相似性的机制区别，拓展了表示学习理论；实际上，在安全监控、可控性和模型选择中提供可操作的见解，提升系统可靠性。潜在局限性包括测量框架的泛化性，未来工作可探索在更多领域的应用和优化稳定性度量方法，以进一步增强实际效用。",
      "tags": [
        "Geometric Stability",
        "Representation Analysis",
        "Stability Metrics",
        "Shesha Framework",
        "CKA"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:31.439932Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09172",
    "title": "BalDRO: A Distributionally Robust Optimization based Framework for Large Language Model Unlearning",
    "authors": [
      "Pengyang Shao",
      "Naixin Zhai",
      "Lei Chen",
      "Yonghui Yang",
      "Fengbin Zhu",
      "Xun Yang",
      "Meng Wang"
    ],
    "abstract": "As Large Language Models (LLMs) increasingly shape online content, removing targeted information from well-trained LLMs (also known as LLM unlearning) has become critical for web governance. A key challenge lies in sample-wise imbalance within the forget set: different samples exhibit widely varying unlearning difficulty, leading to asynchronous forgetting where some knowledge remains insufficiently erased while others become over-forgotten. To address this, we propose BalDRO, a novel and efficient framework for balanced LLM unlearning. BalDRO formulates unlearning as a min-sup process: an inner step identifies a worst-case data distribution that emphasizes hard-to-unlearn samples, while an outer step updates model parameters under this distribution. We instantiate BalDRO via two efficient variants: BalDRO-G, a discrete GroupDRO-based approximation focusing on high-loss subsets, and BalDRO-DV, a continuous Donsker-Varadhan dual method enabling smooth adaptive weighting within standard training pipelines. Experiments on TOFU and MUSE show that BalDRO significantly improves both forgetting quality and model utility over existing methods, and we release code for reproducibility.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09172.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09172",
    "published": "2026-01-14T05:15:10Z",
    "updated": "2026-01-14T05:15:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "BalDRO提出了一个基于分布鲁棒优化的框架，用于平衡大型语言模型的知识遗忘，解决遗忘集中的样本不平衡问题。",
      "motivation": "随着大型语言模型（LLMs）在在线内容中日益重要，从训练好的LLMs中删除特定信息（即知识遗忘）对网络治理至关重要。然而，现有方法面临遗忘集中样本不平衡的挑战：不同样本的遗忘难度差异大，导致异步遗忘，即部分知识未完全删除而其他被过度遗忘。这削弱了遗忘质量和模型效用，凸显了开发更有效解决方案的需求。摘要未明确说明现有具体方法的不足，但指出了样本不平衡是核心问题。",
      "method": "BalDRO框架将知识遗忘建模为最小-最大化过程：内部步骤通过分布鲁棒优化识别最差数据分布，强调难遗忘样本；外部步骤在该分布下更新模型参数以最小化损失。具体实现有两种高效变体：BalDRO-G基于离散GroupDRO方法，专注于高损失子集；BalDRO-DV采用连续的Donsker-Varadhan双方法，实现在标准训练管道中的平滑自适应加权。这些方法旨在在TOFU和MUSE等数据集上应用，但摘要未明确说明模型架构细节。",
      "result": "实验在TOFU和MUSE数据集上进行，结果表明BalDRO显著提升了遗忘质量和模型效用，优于现有知识遗忘方法。通过平衡遗忘过程，有效减少了异步遗忘现象，确保目标知识被充分删除同时保持模型其他性能。摘要未提供具体数值指标（如准确率提升），但强调了其框架的有效性和优于基线方法。作者发布了代码以确保结果的可复现性。",
      "conclusion": "该研究的主要贡献是提出BalDRO，一个基于分布鲁棒优化的框架，有效解决了LLM知识遗忘中的样本不平衡问题。学术上，它为LLM unlearning领域提供了新的优化方法；实践中，有助于改进在线内容治理和模型安全。摘要未明确讨论局限性或未来工作方向，但潜在方向可能包括扩展到更多复杂场景或处理其他数据集。",
      "tags": [
        "Large Language Model Unlearning",
        "Distributionally Robust Optimization",
        "GroupDRO",
        "Donsker-Varadhan Dual Method",
        "Min-Sup Process"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:14.885983Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09170",
    "title": "N-EIoU-YOLOv9: A Signal-Aware Bounding Box Regression Loss for Lightweight Mobile Detection of Rice Leaf Diseases",
    "authors": [
      "Dung Ta Nguyen Duc",
      "Thanh Bui Dang",
      "Hoang Le Minh",
      "Tung Nguyen Viet",
      "Huong Nguyen Thanh",
      "Dong Trinh Cong"
    ],
    "abstract": "In this work, we propose N EIoU YOLOv9, a lightweight detection framework based on a signal aware bounding box regression loss derived from non monotonic gradient focusing and geometric decoupling principles, referred to as N EIoU (Non monotonic Efficient Intersection over Union). The proposed loss reshapes localization gradients by combining non monotonic focusing with decoupled width and height optimization, thereby enhancing weak regression signals for hard samples with low overlap while reducing gradient interference. This design is particularly effective for small and low contrast targets commonly observed in agricultural disease imagery. The proposed N EIoU loss is integrated into a lightweight YOLOv9t architecture and evaluated on a self collected field dataset comprising 5908 rice leaf images across four disease categories and healthy leaves. Experimental results demonstrate consistent performance gains over the standard CIoU loss, achieving a mean Average Precision of 90.3 percent, corresponding to a 4.3 percent improvement over the baseline, with improved localization accuracy under stricter evaluation criteria. For practical validation, the optimized model is deployed on an Android device using TensorFlow Lite with Float16 quantization, achieving an average inference time of 156 milliseconds per frame while maintaining accuracy. These results confirm that the proposed approach effectively balances accuracy, optimization stability, and computational efficiency for edge based agricultural monitoring systems.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09170.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09170",
    "published": "2026-01-14T05:13:36Z",
    "updated": "2026-01-14T05:13:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种基于非单调梯度聚焦和几何解耦的信号感知边界框回归损失N-EIoU，集成到YOLOv9t框架中，用于轻量级移动水稻叶病害检测。",
      "motivation": "在农业病害图像检测中，小目标和低对比度目标常见，这使得标准边界框回归损失如CIoU在处理困难样本时效率低下，导致梯度干扰和弱回归信号问题。这些问题限制了检测精度，尤其是在移动设备上需要轻量化和实时性能的背景下。因此，开发一种信号感知的损失函数来增强弱信号并减少干扰，对于提高农业监测系统的准确性和效率至关重要。",
      "method": "论文提出了N-EIoU损失函数，基于非单调梯度聚焦和几何解耦原则，通过结合非单调聚焦与宽度和高度解耦优化来重塑定位梯度，以增强低重叠困难样本的回归信号并减少梯度干扰。该方法被集成到轻量级YOLOv9t检测框架中，并在一个自收集的水稻叶病害数据集上进行了评估，该数据集包含5908张图像，涵盖四种病害类别和健康叶子，用于模型训练和测试。",
      "result": "实验结果显示，使用N-EIoU损失的模型在平均精度上达到90.3%，相比基线CIoU损失提升了4.3%，并且在更严格的评估标准下提高了定位准确性。模型在Android设备上使用TensorFlow Lite进行Float16量化部署时，实现了平均每帧156毫秒的推理时间，同时保持了准确性，证明了其在实际移动应用中的高效性能。",
      "conclusion": "论文的主要贡献是提出了N-EIoU损失函数，该损失通过优化定位梯度，有效提升了农业病害图像的检测性能，平衡了精度、优化稳定性和计算效率。这为基于边缘的农业监测系统提供了高效解决方案，具有实际应用价值，未来工作可探索该方法在其他领域的扩展和进一步优化。",
      "tags": [
        "Bounding Box Regression Loss",
        "YOLOv9",
        "TensorFlow Lite",
        "Quantization",
        "Mobile Detection"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:05.307498Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09169",
    "title": "Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies",
    "authors": [
      "Jamie Magrill",
      "Leah Gornstein",
      "Sandra Seekins",
      "Barry Magrill"
    ],
    "abstract": "Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.",
    "categories": [
      "cs.CV",
      "cs.CY"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09169.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09169",
    "published": "2026-01-14T05:13:04Z",
    "updated": "2026-01-14T05:13:04Z",
    "comment": "24 pages, 7 figures",
    "light_analysis": {
      "overview": "论文评估了五个生成式AI图像平台在建筑风格、元素和类型学方面的准确性，揭示了其在历史规则领域中的局限性。",
      "motivation": "生成式AI文本到图像系统越来越多地用于生成建筑图像，但它们在历史规则领域中生成准确图像的能力尚未得到充分评估。建筑领域有严格的风格和元素规则，而现有评估不足，这可能导致误用或不准确的图像生成，影响教育和实际应用。因此，研究旨在系统评估这些系统的准确性，以填补这一知识空白并指导未来使用。",
      "method": "研究评估了五个广泛使用的GenAI图像平台：Adobe Firefly、DALL-E 3、Google Imagen 3、Microsoft Image Generator和Midjourney。使用30个建筑提示，涵盖风格、类型和元素。每个提示-生成器对产生四张图像，总计600张。两位建筑历史学家基于预定义标准独立评分，并通过共识解决分歧。性能总结为每个四图像集的准确性（零到四个准确图像），并比较了Common和Rare提示以分析性能差异。",
      "result": "实验结果显示，GenAI平台整体准确性有限，最高准确率为52%，最低为32%，平均为42%。Common提示的准确性是Rare提示的2.7倍（p < 0.05）。所有正确（4/4）结果在各平台相似，但所有错误（0/4）结果差异显著，Imagen 3失败最少，Microsoft Image Generator失败最多。定性分析发现常见错误模式，如过度装饰、混淆中世纪风格和后续复兴。",
      "conclusion": "研究表明GenAI系统在生成准确建筑图像方面存在局限性，支持需要显式标记合成内容、制定训练数据集来源标准，并在教育中谨慎使用。论文提供了实证数据，揭示了模型在特定领域的不足，对学术研究和实际应用具有指导意义。未来工作可包括改进模型训练或开发更专业的评估方法。",
      "tags": [
        "Generative AI",
        "Text-to-Image Models",
        "Architectural Evaluation",
        "Image Accuracy",
        "Evaluation Study"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:57.459741Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09166",
    "title": "DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix",
    "authors": [
      "Sidhant R. Nair",
      "Tanmay Sen",
      "Mrinmay Sen"
    ],
    "abstract": "Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09166.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09166",
    "published": "2026-01-14T05:11:28Z",
    "updated": "2026-01-14T05:11:28Z",
    "comment": "17 pages, 1 figure. Submitted to ICML 2026",
    "light_analysis": {
      "overview": "本论文提出了DP-FedSOFIM框架，通过服务器端二阶优化和Fisher信息矩阵预处理，在差分私有联邦学习中实现了高效收敛和低内存需求。",
      "motivation": "差分私有联邦学习在严格隐私约束下，由于引入大量噪声保护隐私，导致收敛速度缓慢，限制了实际应用。现有二阶优化方法如DP-FedNew虽然能加速收敛，但需要每个客户端存储O(d^2)的局部特征协方差矩阵，对于高维模型内存消耗巨大，不切实际。因此，开发一种内存效率高的优化框架至关重要，以解决收敛慢和资源消耗大的问题，推动隐私保护学习的发展。",
      "method": "论文提出DP-FedSOFIM框架，在服务器端利用Fisher信息矩阵作为自然梯度预处理器进行二阶优化，每个客户端只需O(d)内存。通过Sherman-Morrison公式高效计算矩阵逆，实现每轮O(d)计算复杂度，同时保持二阶方法的收敛优势。该方法还通过后处理定理确保(epsilon, delta)-差分隐私。摘要未明确说明具体模型架构或更多技术细节。",
      "result": "在CIFAR-10数据集上的实验表明，DP-FedSOFIM在多个隐私预算设置下，相比一阶基线方法实现了更优的测试准确率，这证实了该框架在保持差分隐私的同时，有效加速了收敛过程。摘要未提供具体准确率数字，但强调了性能的显著提升，显示出在实际场景中的潜力。",
      "conclusion": "本研究的主要贡献是提出了DP-FedSOFIM框架，它通过服务器端二阶优化和Fisher信息矩阵预处理，显著降低了内存需求并加速了收敛，为差分私有联邦学习提供了高效的解决方案。这在学术上具有创新价值，实际中适用于高维模型部署。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Differential Privacy",
        "Federated Learning",
        "Stochastic Optimization",
        "Fisher Information Matrix",
        "Sherman-Morrison Formula"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:22.213327Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09165",
    "title": "Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation",
    "authors": [
      "Aaron R. Flouro",
      "Shawn P. Chadwick"
    ],
    "abstract": "Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.   Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09165.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09165",
    "published": "2026-01-14T05:10:36Z",
    "updated": "2026-01-14T05:10:36Z",
    "comment": "7 pages, 1 table",
    "light_analysis": {
      "overview": "论文提出了一个公理化的算子理论框架，用于多教师集成知识蒸馏的概率域知识聚合。",
      "motivation": "多教师知识蒸馏旨在通过集成多个教师模型的知识提升学生模型性能，但现有方法缺乏统一的数学框架来指导知识聚合，导致在异构教师模型下性能不稳定。本文基于Sparse-KD的概率域蒸馏框架，旨在解决这一问题，为多教师蒸馏提供坚实的理论基础，以增强鲁棒性和可扩展性，应对实际应用中前沿模型集成的挑战。",
      "method": "本文提出一个公理化的数学框架，定义了五个核心公理（凸性、正性、连续性、权重单调性和温度一致性）来约束有效的知识聚合算子。基于算子理论，证明了满足这些公理的算子族的存在性和非唯一性，表明多种聚合机制符合相同原理。方法建立在概率域蒸馏上，不指定具体公式，而是提供理论指导，允许灵活实现。摘要未明确说明使用的数据集或模型架构。",
      "result": "论文建立了算子无关的理论保证，显示多教师聚合能减少随机方差和系统监督偏差，特别是在异构教师模型中。提供了Jensen型边界、对数损失保证和安全衰减性质，增强了蒸馏稳定性。对于线性权重的聚合算子，在标准独立假设下，证明了经典集成方差减少结果，并可扩展到相关误差体制，为实践提供理论支撑。摘要未提供具体实验性能指标。",
      "conclusion": "本研究的主要贡献是提出了一个公理化的算子理论框架，为多教师集成知识蒸馏提供了坚实的数学基础。框架通过公理化定义和算子证明，为知识聚合提供理论指导，同时允许多种实现策略，增强了理论深度和实际应用价值。未来工作可能涉及具体算子的实证验证和扩展，以进一步提升框架的实用性。",
      "tags": [
        "Knowledge Distillation",
        "Multi-Teacher Ensemble",
        "Probability-Domain",
        "Axiomatic Framework",
        "Operator Theory"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:25.096932Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09162",
    "title": "Efficient Clustering in Stochastic Bandits",
    "authors": [
      "G Dhinesh Chandran",
      "Kota Srinivas Reddy",
      "Srikrishna Bhashyam"
    ],
    "abstract": "We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09162.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09162",
    "published": "2026-01-14T05:05:58Z",
    "updated": "2026-01-14T05:05:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出Efficient Bandit Clustering (EBC)算法及其变体EBC-H，通过简化优化步骤在保持渐近最优的同时显著提升计算效率。",
      "motivation": "本研究针对Bandit Clustering (BC)问题在固定置信度设置下的计算效率瓶颈。现有方法每步需解决优化问题，导致高计算成本，且通常假设高斯分布，限制了应用范围。问题的重要性在于实际场景中数据分布可能更复杂，需要高效算法来处理更广泛的向量参数分布类，从而增强算法的泛化能力和实用性。摘要未明确说明具体应用领域，但推断其在数据序列聚类中具有潜在价值。",
      "method": "研究方法的核心是提出Efficient Bandit Clustering (EBC)算法，通过每步向最优值迈进一步，而非完整优化问题求解，大幅降低计算复杂度。关键创新包括设计计算高效的采样规则，适用于满足温和正则条件的向量参数分布。变体EBC-H进一步简化，采样基于停止规则中的计算量。该方法未指定具体数据集或模型架构，但通过模拟支持其有效性，突出了技术特色在于平衡渐近最优性和计算效率。",
      "result": "实验结果显示，EBC和EBC-H在合成数据集上验证了渐近最优性，并在真实世界数据集中表现出优于现有方法的性能。具体地，比较每个样本的运行时间突显了计算效率的提升，但摘要未提供具体数据。模拟表明算法在错误概率控制下有效聚类数据序列，与基线方法相比，在减少计算开销的同时保持准确性。这些结果支持了算法在实际应用中的优势。",
      "conclusion": "论文的主要贡献是提出了EBC和EBC-H算法，为Bandit Clustering问题提供了计算高效的解决方案，适用于更广泛的分布类。研究增强了该领域的实用性和泛化能力，具有学术价值，如推动序列采样算法的理论发展，以及实际应用价值，如在数据聚类任务中降低计算资源需求。局限性或未来工作方向摘要未明确说明，但可推断包括扩展到其他分布类型或更复杂场景。",
      "tags": [
        "Bandit Clustering",
        "Stochastic Bandits",
        "Asymptotic Optimality",
        "Computational Efficiency",
        "Sequential Sampling"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:28.629806Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09156",
    "title": "KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education",
    "authors": [
      "Woojin Kim",
      "Changkwon Lee",
      "Hyeoncheol Kim"
    ],
    "abstract": "Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09156.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09156",
    "published": "2026-01-14T04:51:54Z",
    "updated": "2026-01-14T04:51:54Z",
    "comment": "Accepted to AAAI-26 Special Track AI for Social Impact (oral presentation)",
    "light_analysis": {
      "overview": "论文提出了KTCF方法，通过反事实解释为知识追踪生成可操作的教育指令，以提升AI在教育中的负责任应用。",
      "motivation": "研究旨在解决教育中AI模型的可解释性和实用性问题。知识追踪在建模学生知识状态方面表现优异，但现有方法可能缺乏有效解释机制，无法为教育利益相关者（如教师、学生等非专家）提供可操作的反馈。反事实解释作为一种可解释AI技术，因其因果性、局部化和易于理解的特点，被视为连接AI与教育的桥梁。通过将反事实解释应用于知识追踪，可以生成具体、可执行的教育建议，从而促进AI在教育中的负责任使用，弥补现有方法在实用性和透明度方面的不足。",
      "method": "论文提出KTCF方法，一种专为知识追踪设计的反事实解释生成方法。该方法的关键创新在于考虑了知识概念之间的相互关系，以生成更准确和相关的反事实解释。此外，方法包括一个后处理方案，将反事实解释转化为序列化的教育指令，使其更易于实施和应用于实际教学场景。实验基于大规模教育数据集进行，但具体模型架构和算法细节未在摘要中明确说明。总体而言，KTCF通过整合知识概念关系和实用性转化，为知识追踪提供教育接地气的解释方案。",
      "result": "在大规模教育数据集上的实验结果表明，KTCF方法在性能上显著优于现有方法，提升范围从5.7%到34%不等，涵盖多个未具体说明的评估指标。定性评估显示，通过后处理方案生成的教育指令能够有效减少学生的学习负担，展示了该方法的实用价值。这些结果验证了KTCF在提供可操作反事实解释方面的有效性和稳健性，表明它在教育应用中优于传统基线方法，并有助于优化学习体验。",
      "conclusion": "本研究的主要贡献是提出KTCF方法和后处理方案，成功将反事实解释应用于知识追踪，为教育提供可操作的反馈。学术价值在于连接可解释AI与教育领域，推动了负责任AI在教育中的发展，强调了教育接地气概念化的重要性。实际应用中，该方法有助于生成个性化的教学指令，减轻学生负担。未来工作可以进一步基于教育概念开发利益相关者为中心的方法，扩展反事实解释在教育AI中的应用，并探索其在更多场景中的潜力。",
      "tags": [
        "Knowledge Tracing",
        "Counterfactual Explanations",
        "XAI",
        "Educational Dataset"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:52.714434Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09153",
    "title": "From Snow to Rain: Evaluating Robustness, Calibration, and Complexity of Model-Based Robust Training",
    "authors": [
      "Josué Martínez-Martínez",
      "Olivia Brown",
      "Giselle Zeno",
      "Pooya Khorrami",
      "Rajmonda Caceres"
    ],
    "abstract": "Robustness to natural corruptions remains a critical challenge for reliable deep learning, particularly in safety-sensitive domains. We study a family of model-based training approaches that leverage a learned nuisance variation model to generate realistic corruptions, as well as new hybrid strategies that combine random coverage with adversarial refinement in nuisance space. Using the Challenging Unreal and Real Environments for Traffic Sign Recognition dataset (CURE-TSR), with Snow and Rain corruptions, we evaluate accuracy, calibration, and training complexity across corruption severities. Our results show that model-based methods consistently outperform baselines Vanilla, Adversarial Training, and AugMix baselines, with model-based adversarial training providing the strongest robustness under across all corruptions but at the expense of higher computation and model-based data augmentation achieving comparable robustness with $T$ less computational complexity without incurring a statistically significant drop in performance. These findings highlight the importance of learned nuisance models for capturing natural variability, and suggest a promising path toward more resilient and calibrated models under challenging conditions.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09153.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09153",
    "published": "2026-01-14T04:49:44Z",
    "updated": "2026-01-14T04:49:44Z",
    "comment": "11 pages",
    "light_analysis": {
      "overview": "本研究提出并评估了基于模型的训练方法，通过学习的nuisance变化模型生成真实扰动，以增强深度学习在自然扰动下的鲁棒性、校准性和效率。",
      "motivation": "在安全敏感领域如自动驾驶中，深度学习模型对自然扰动（如雪、雨）的鲁棒性至关重要，但现有方法如传统训练、对抗训练和AugMix数据增强可能无法充分捕捉真实世界的变异性，导致在复杂条件下的性能不足。因此，本研究旨在探索更有效的方法，通过学习的nuisance模型来提高模型的稳健性和校准能力，以应对可靠性挑战。",
      "method": "论文研究了一系列基于模型的训练方法，核心是使用学习的nuisance变化模型生成真实的自然扰动，如雪和雨。关键创新包括开发混合策略，结合随机覆盖与nuisance空间的对抗性精炼，以生成多样化的训练样本。实验使用CURE-TSR数据集，专注于交通标志识别，系统评估了准确性、校准误差和训练复杂度在不同扰动严重程度下的表现。",
      "result": "实验结果显示，基于模型的方法在所有扰动下均优于Vanilla训练、对抗训练和AugMix基线。具体而言，模型基础的对抗训练提供最强的鲁棒性，但计算复杂度较高；而模型基础的数据增强方法实现可比较的鲁棒性，同时显著降低了计算复杂度，且性能下降无统计显著性，突出了效率与效果的平衡。",
      "conclusion": "本研究强调了学习的nuisance模型在捕获自然变异性、提升模型鲁棒性和校准性方面的重要性，为开发在挑战性条件下更具韧性的深度学习模型提供了有前景的路径。未来工作可优化混合策略或扩展到其他扰动类型，以增强实际应用价值。",
      "tags": [
        "Model-Based Training",
        "Nuisance Variation Model",
        "Adversarial Training",
        "Data Augmentation",
        "Robustness Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:07.839988Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09152",
    "title": "PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?",
    "authors": [
      "Yiwen Tu",
      "Xuan Liu",
      "Lianhui Qin",
      "Haojian Jin"
    ],
    "abstract": "This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's \"privacy mind\", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \\PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09152.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09152",
    "published": "2026-01-14T04:47:06Z",
    "updated": "2026-01-14T04:47:06Z",
    "comment": null,
    "light_analysis": {
      "overview": "PRA是一种基于隐私和认知理论的AI-agent，旨在模拟个体用户如何基于个人历史形成隐私顾虑。",
      "motivation": "现有隐私分析多集中于群体层面的情感分析，无法捕捉个体用户的隐私推理过程，这限制了隐私保护措施的个性化。论文旨在解决这一问题，因为隐私是个性化议题，需要结合个人评论历史和上下文线索来准确理解用户行为。研究背景源于隐私理论和认知科学的融合需求，以弥补现有方法在模拟用户特定隐私顾虑方面的不足，从而提升隐私预测的准确性和应用价值。",
      "method": "PRA集成隐私与认知理论，设计为一个AI-agent，通过个人评论历史和上下文线索模拟用户特定的隐私推理。核心创新包括重建用户的'隐私思维'，利用上下文过滤器动态激活隐私记忆以模拟有界理性，并生成反映用户可能响应的合成评论。补充方法涉及一个LLM-as-a-Judge评估器，基于隐私顾虑分类法校准，以量化生成推理的忠实度，确保模拟过程符合人类认知特性。",
      "result": "实验在真实世界的Hacker News讨论数据上进行，结果显示PRA在隐私顾虑预测方面优于基线AI-agent。它能够捕捉用户推理模式，并在AI、电子商务和医疗保健等多个领域中实现可转移的推理模式，表明其具有良好的泛化能力。具体性能指标如准确率提升摘要未明确说明，但实验验证了模型在跨领域应用中的有效性。",
      "conclusion": "PRA的研究贡献在于提出了一种模拟个体用户隐私顾虑的AI-agent设计，结合隐私和认知理论，成功在预测和推理模式转移方面表现优异。这为个性化隐私保护、用户行为分析和AI系统设计提供了新的学术见解和实际应用价值。未来工作可进一步优化模型，扩展到更多隐私相关领域，或整合更多认知因素以提高模拟准确性。",
      "tags": [
        "Large Language Model",
        "AI-agent",
        "Privacy Reasoning",
        "Cognitive Modeling",
        "Contextual Filtering"
      ]
    },
    "analyzed_at": "2026-01-15T03:38:55.791207Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09151",
    "title": "Interpretable Probability Estimation with LLMs via Shapley Reconstruction",
    "authors": [
      "Yang Nan",
      "Qihao Wen",
      "Jiahao Wang",
      "Pengfei He",
      "Ravi Tandon",
      "Yong Ge",
      "Han Xu"
    ],
    "abstract": "Large Language Models (LLMs) demonstrate potential to estimate the probability of uncertain events, by leveraging their extensive knowledge and reasoning capabilities. This ability can be applied to support intelligent decision-making across diverse fields, such as financial forecasting and preventive healthcare. However, directly prompting LLMs for probability estimation faces significant challenges: their outputs are often noisy, and the underlying predicting process is opaque. In this paper, we propose PRISM: Probability Reconstruction via Shapley Measures, a framework that brings transparency and precision to LLM-based probability estimation. PRISM decomposes an LLM's prediction by quantifying the marginal contribution of each input factor using Shapley values. These factor-level contributions are then aggregated to reconstruct a calibrated final estimate. In our experiments, we demonstrate PRISM improves predictive accuracy over direct prompting and other baselines, across multiple domains including finance, healthcare, and agriculture. Beyond performance, PRISM provides a transparent prediction pipeline: our case studies visualize how individual factors shape the final estimate, helping build trust in LLM-based decision support systems.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09151.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09151",
    "published": "2026-01-14T04:45:36Z",
    "updated": "2026-01-14T04:45:36Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出PRISM框架，利用Shapley值使大型语言模型的概率估计可解释且准确。",
      "motivation": "大型语言模型（LLMs）具备估计不确定事件概率的潜力，可应用于金融预测和预防性医疗等领域，以支持智能决策。然而，直接提示LLMs进行概率估计面临重大挑战：输出往往嘈杂且预测过程不透明，这限制了其在实际应用中的可靠性和可信度。因此，需要一种既能提高准确性又能提供解释性的方法来克服现有方法的不足。",
      "method": "论文提出PRISM框架（Probability Reconstruction via Shapley Measures），该框架的核心方法是利用Shapley值分解LLM的预测过程，量化每个输入因素对最终概率估计的边际贡献。通过聚合这些因素级贡献来重建经过校准的最终估计，从而提供透明的预测管道。关键创新点在于将Shapley值引入LLM解释性中，以增强预测的精度和可理解性，尽管摘要未明确说明使用的具体LLM模型或数据集。",
      "result": "实验结果表明，PRISM框架在多个领域（包括金融、医疗和农业）中提高了预测准确性，优于直接提示和其他基线方法。虽然摘要未提供具体的性能指标（如准确率提升百分比），但强调了PRISM在性能上的显著改进，并通过案例研究可视化各个因素如何影响最终估计，帮助构建用户对LLM决策支持系统的信任。",
      "conclusion": "论文的主要贡献是PRISM框架，该框架为LLM的概率估计带来了透明度和精度，推动了可解释人工智能在概率估计领域的学术发展，并具有实际应用价值，有助于增强决策支持系统的可信度。尽管摘要未明确提及局限性或未来工作，但该方法的有效性已得到验证，为未来在更广泛领域中的应用和优化奠定了基础。",
      "tags": [
        "Large Language Model",
        "Shapley Value",
        "Probability Estimation",
        "Interpretable AI",
        "Machine Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:53.900354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09147",
    "title": "SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection",
    "authors": [
      "Chenhao Fu",
      "Han Fang",
      "Xiuzheng Zheng",
      "Wenbo Wei",
      "Yonghua Li",
      "Hao Sun",
      "Xuelong Li"
    ],
    "abstract": "Zero-Shot Anomaly Detection (ZSAD) leverages Vision-Language Models (VLMs) to enable supervision-free industrial inspection. However, existing ZSAD paradigms are constrained by single visual backbones, which struggle to balance global semantic generalization with fine-grained structural discriminability. To bridge this gap, we propose Synergistic Semantic-Visual Prompting (SSVP), that efficiently fuses diverse visual encodings to elevate model's fine-grained perception. Specifically, SSVP introduces the Hierarchical Semantic-Visual Synergy (HSVS) mechanism, which deeply integrates DINOv3's multi-scale structural priors into the CLIP semantic space. Subsequently, the Vision-Conditioned Prompt Generator (VCPG) employs cross-modal attention to guide dynamic prompt generation, enabling linguistic queries to precisely anchor to specific anomaly patterns. Furthermore, to address the discrepancy between global scoring and local evidence, the Visual-Text Anomaly Mapper (VTAM) establishes a dual-gated calibration paradigm. Extensive evaluations on seven industrial benchmarks validate the robustness of our method; SSVP achieves state-of-the-art performance with 93.0\\% Image-AUROC and 92.2\\% Pixel-AUROC on MVTec-AD, significantly outperforming existing zero-shot approaches.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09147.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09147",
    "published": "2026-01-14T04:42:19Z",
    "updated": "2026-01-14T04:42:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出SSVP方法，通过协同语义-视觉提示机制，在工业零样本异常检测中平衡全局语义和细粒度结构判别，实现最先进性能。",
      "motivation": "该研究旨在解决工业检查中的零样本异常检测问题，现有方法受限于单视觉骨干，难以平衡全局语义泛化与细粒度结构判别，导致检测精度和泛化能力不足。零样本检测减少了对监督数据的依赖，但现有ZSAD方法性能有限，无法有效处理复杂工业场景中的细微异常，亟需新方法来提升检测效果，以支持高效的自动化工业检查。",
      "method": "SSVP方法引入Hierarchical Semantic-Visual Synergy (HSVS)机制，将DINOv3的多尺度结构先验深度集成到CLIP的语义空间中，以增强细粒度感知。Vision-Conditioned Prompt Generator (VCPG)采用跨模态注意力生成动态提示，使语言查询能精确锚定到特定异常模式。此外，Visual-Text Anomaly Mapper (VTAM)建立双门校准范式，解决全局评分与局部证据之间的不一致性，整体提升检测精度。",
      "result": "在七个工业基准数据集上进行广泛评估，SSVP在MVTec-AD上取得了93.0%的图像级AUROC和92.2%的像素级AUROC，性能显著优于现有零样本方法，达到最先进水平。这些结果验证了SSVP在融合语义和视觉信息方面的有效性，展示了其在异常检测任务中的强大泛化能力和高精度表现。",
      "conclusion": "SSVP通过协同语义-视觉提示，成功解决了零样本异常检测中的关键问题，提升了检测性能。其学术价值在于创新地结合了多层次视觉编码和动态提示生成技术，为视觉-语言模型的应用提供了新思路；实际应用价值在于为工业检查提供了一种高效的、无需监督的检测方案，降低了对标注数据的依赖。未来工作可进一步优化计算效率或扩展到其他领域。",
      "tags": [
        "Zero-Shot Anomaly Detection",
        "Vision-Language Models",
        "Cross-Modal Attention",
        "DINOv3",
        "CLIP"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:12.329210Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09143",
    "title": "Discrete Solution Operator Learning for Geometry-Dependent PDEs",
    "authors": [
      "Jinshuai Bai",
      "Haolin Li",
      "Zahra Sharif Khodaei",
      "M. H. Aliabadi",
      "YuanTong Gu",
      "Xi-Qiao Feng"
    ],
    "abstract": "Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.",
    "categories": [
      "cs.LG",
      "math.NA",
      "physics.comp-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09143.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09143",
    "published": "2026-01-14T04:34:49Z",
    "updated": "2026-01-14T04:34:49Z",
    "comment": "15 pages main text, 40 pages SI",
    "light_analysis": {
      "overview": "本论文提出了离散解算子学习（DiSOL），一种学习离散解过程的新范式，用于处理几何依赖偏微分方程中的结构变化。",
      "motivation": "神经算子学习通过近似连续函数空间之间的映射来加速偏微分方程求解，但在工程应用中，几何变化常导致离散结构变化，如拓扑变化、边界条件突变和计算域变化。这些变化破坏了现有方法的平滑性假设，使得传统连续算子方法难以有效处理不连续性，从而影响求解稳定性和准确性。因此，需要一种能够适应几何依赖离散结构的新方法，以应对实际工程中常见的复杂几何场景。",
      "method": "DiSOL方法将求解器分解为三个可学习的阶段：局部贡献编码、多尺度组装和嵌入网格上的隐式解重构。这些阶段模仿了经典离散化过程，确保在几何依赖的离散结构下保持过程级一致性。关键创新点在于从连续函数空间算子转向离散过程学习，通过这种分解适应边界条件突变和拓扑变化，而不依赖平滑函数空间映射的假设，从而提升了对结构变化的鲁棒性。",
      "result": "在几何依赖的偏微分方程问题中，包括泊松方程、对流扩散、线性弹性和时空热传导问题，DiSOL在分布内和强分布外几何条件下均产生稳定和准确的预测。实验涵盖不连续边界和拓扑变化等挑战性场景，结果表明该方法在这些条件下仍保持高精度和稳定性，突显了其优于传统连续算子方法的适应性，但具体性能数据如准确率提升未在摘要中明确说明。",
      "conclusion": "DiSOL的提出强调了在几何主导体系中程序化算子表示的重要性，将离散解算子学习定位为科学机器学习中的一个独特、互补方向。该方法能克服传统连续算子方法在处理不连续几何变化时的局限性，具有理论价值和实际应用潜力，如在工程模拟和计算流体动力学中。未来工作可扩展到更复杂的几何类型和偏微分方程问题，并探索与其他机器学习方法的集成。",
      "tags": [
        "Discrete Solution Operator Learning",
        "Partial Differential Equations",
        "Neural Operator Learning",
        "Geometry Modeling",
        "Scientific Machine Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:29.460066Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09142",
    "title": "EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge",
    "authors": [
      "Shijian Ma",
      "Yan Lin",
      "Yi Yang"
    ],
    "abstract": "Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09142.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09142",
    "published": "2026-01-14T04:26:43Z",
    "updated": "2026-01-14T04:26:43Z",
    "comment": "Shijian Ma and Yan Lin contributed equally. Corresponding author: Yan Lin",
    "light_analysis": {
      "overview": "本文提出EvasionBench基准数据集和一个基于多模型共识与LLM作为judge的逃避回答检测框架，以提升财务问答中的逃避回答识别性能。",
      "motivation": "财务问答，特别是收益电话会议中的逃避回答检测，对维护市场透明度和投资者信任至关重要。然而，现有研究因缺乏大规模、高质量的数据集而进展缓慢，这阻碍了算法的开发和评估。本研究的动机在于解决这一瓶颈，通过构建标准化基准来推动逃避回答检测领域的发展，强调数据稀缺是主要障碍，并指出当前方法在泛化和效率方面的不足。",
      "method": "研究方法引入了一个多模型注释框架，核心是利用前沿大型语言模型（LLM）之间的分歧来挖掘训练中最有价值的硬样本。我们构建了EvasionBench数据集，包含30,000个训练样本和1,000个人工标注测试样本（Cohen's Kappa 0.835），覆盖三种逃避级别。具体操作中，当两个强注释器产生冲突时，使用一个LLM作为judge来解析标签，这种分歧挖掘方法被设计为提升模型泛化能力，并作为隐式正则化机制。",
      "result": "实验结果表明，提出的多模型注释框架在准确率上比单模型蒸馏方法高出2.4个百分点。训练得到的Eva-4B模型（40亿参数）在测试集上达到81.3%的准确率，比基础模型提升25个百分点，性能接近前沿LLM但推理成本大幅降低。训练损失数据显示，judge-resolved样本导致更高的训练损失（0.421对比0.393），这证明了分歧挖掘起到隐式正则化的作用，有助于改善模型泛化。",
      "conclusion": "本研究的主要贡献是提供了EvasionBench大规模基准数据集和有效的多模型注释框架，为财务逃避回答检测领域提供了标准化工具。该方法展示了分歧挖掘在提升模型性能和泛化能力中的价值，并实现了在有限资源下接近前沿模型性能的目标。未来工作可探索该方法在其他领域的应用，或进一步优化模型架构以降低计算成本。",
      "tags": [
        "Evasion Detection",
        "Large Language Model",
        "Multi-Model Consensus",
        "Benchmark Dataset",
        "LLM-as-Judge"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:42.957645Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09141",
    "title": "Identity-Robust Language Model Generation via Content Integrity Preservation",
    "authors": [
      "Miao Zhang",
      "Kelly Chen",
      "Md Mehrab Tanjim",
      "Rumi Chunara"
    ],
    "abstract": "Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09141.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09141",
    "published": "2026-01-14T04:25:29Z",
    "updated": "2026-01-14T04:25:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一个轻量级、无需训练的框架，通过选择性中和非关键身份信息，实现身份稳健的大语言模型生成，保持内容完整性。",
      "motivation": "大语言模型的输出常随用户社会人口属性变化，即使对于客观问题，也会导致事实准确性、效用和安全性差异。现有研究多关注刻板印象或代表性偏见，但本文聚焦于身份依赖性对核心响应质量的下降问题。这种下降源于生成行为的偏见，而非知识编码的不足，因此需要在不损害内容完整性的情况下减少偏见，以提升LLM在不同用户身份下的公平性和实用性。",
      "method": "论文提出一个轻量级、无需训练的框架，用于身份稳健的生成。核心方法是选择性地中和非关键身份信息，同时保留语义重要属性，从而保持输出内容的完整性。具体技术细节如模型架构未在摘要中明确说明，但框架在四个基准测试和18个社会人口身份上进行验证，依赖提示调整和信息过滤策略来实现偏见缓解。",
      "result": "在四个基准测试和18个社会人口身份上的实验显示，该框架相比原始提示（vanilla prompting）实现了平均77%的身份依赖性偏见减少，相比基于提示的防御减少了45%。这一改进表明，通过选择性中和非关键身份信息，能有效减少身份信息对生成质量的负面影响，同时在不牺牲内容完整性的前提下，提升LLM的身份稳健性。具体性能指标基于偏见评分，实验验证了框架的优越性。",
      "conclusion": "本文的主要贡献是提出一个轻量级、无需训练的框架，用于实现身份稳健的LLM生成，显著减少身份依赖性偏见。学术上，它填补了研究用户身份线索对核心生成质量影响的空白；实际上，有助于改善LLM在多样化场景中的公平性和可靠性。未来工作可扩展到更多身份类型或探索其他偏见缓解技术，以进一步提升泛化能力。",
      "tags": [
        "Large Language Model",
        "Bias Mitigation",
        "Content Integrity",
        "Training-Free Framework",
        "Identity-Robust Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:43.840423Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09136",
    "title": "SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL",
    "authors": [
      "Lijun Liu",
      "Linwei Chen",
      "Zhishou Zhang",
      "Meng Tian",
      "Hengfu Cui",
      "Ruiyang Li",
      "Zhaocheng Liu",
      "Qiang Ju",
      "Qianxi Li",
      "Hong-Yu Zhou"
    ],
    "abstract": "General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to \"diffuse attention\" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to \"unfold\" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09136.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09136",
    "published": "2026-01-14T04:21:07Z",
    "updated": "2026-01-14T04:21:07Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出SkinFlow框架，通过动态视觉编码和分阶段强化学习优化皮肤病诊断的信息传输效率，挑战参数规模化的传统假设。",
      "motivation": "通用大型视觉语言模型在皮肤病诊断中因'扩散注意力'问题表现不佳，难以从背景噪声中分离细微病理病变，导致诊断精度不足。这一问题的重要性在于医学应用需要高可靠性，而现有方法过度依赖参数扩展，忽略了信息传输效率，造成计算冗余和注意力分散。因此，研究旨在通过优化几何容量和信息流来替代单纯参数缩放，以提升诊断性能。",
      "method": "研究方法采用SkinFlow框架，核心包括Virtual-Width Dynamic Vision Encoder (DVE) 动态展开复杂病理流形而无物理参数扩展，结合两阶段强化学习策略。第一阶段对齐明确医学描述，第二阶段在受限语义空间内重建隐含诊断纹理，以优化视觉信息传输效率。模型基于7B参数架构，在Fitzpatrick17k数据集上评估，创新点在于动态编码与分阶段强化学习的协同作用。",
      "result": "实验结果显示，SkinFlow的7B模型在Fitzpatrick17k基准上达到新的最先进水平，Top-1准确率提升12.06%，Top-6准确率提升28.57%。相比大规模通用模型如Qwen3VL-235B和GPT-5.2，该模型在较小参数规模下显著改善诊断准确性，证明了优化几何容量和信息流的有效性。",
      "conclusion": "论文主要贡献是证明优化几何容量和信息流比原始参数缩放更优，提升了诊断推理。研究具有学术价值，挑战了医学AI中参数规模化的路径，并提供基于临床评估协议的方法论，强调诊断安全性和层次相关性。未来工作可能包括扩展应用到其他医学领域或进一步优化动态编码技术。",
      "tags": [
        "Large Vision-Language Models",
        "Dynamic Vision Encoder",
        "Reinforcement Learning",
        "Clinical Evaluation Protocol",
        "Dermatological Diagnosis"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:35.857733Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09121",
    "title": "Beyond Seen Bounds: Class-Centric Polarization for Single-Domain Generalized Deep Metric Learning",
    "authors": [
      "Xin Yuan",
      "Meiqi Wan",
      "Wei Liu",
      "Xin Xu",
      "Zheng Wang"
    ],
    "abstract": "Single-domain generalized deep metric learning (SDG-DML) faces the dual challenge of both category and domain shifts during testing, limiting real-world applications. Therefore, aiming to learn better generalization ability on both unseen categories and domains is a realistic goal for the SDG-DML task. To deliver the aspiration, existing SDG-DML methods employ the domain expansion-equalization strategy to expand the source data and generate out-of-distribution samples. However, these methods rely on proxy-based expansion, which tends to generate samples clustered near class proxies, failing to simulate the broad and distant domain shifts encountered in practice. To alleviate the problem, we propose CenterPolar, a novel SDG-DML framework that dynamically expands and constrains domain distributions to learn a generalizable DML model for wider target domain distributions. Specifically, \\textbf{CenterPolar} contains two collaborative class-centric polarization phases: (1) Class-Centric Centrifugal Expansion ($C^3E$) and (2) Class-Centric Centripetal Constraint ($C^4$). In the first phase, $C^3E$ drives the source domain distribution by shifting the source data away from class centroids using centrifugal expansion to generalize to more unseen domains. In the second phase, to consolidate domain-invariant class information for the generalization ability to unseen categories, $C^4$ pulls all seen and unseen samples toward their class centroids while enforcing inter-class separation via centripetal constraint. Extensive experimental results on widely used CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, and Office-Home datasets demonstrate the superiority and effectiveness of our CenterPolar over existing state-of-the-art methods. The code will be released after acceptance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09121.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09121",
    "published": "2026-01-14T03:44:46Z",
    "updated": "2026-01-14T03:44:46Z",
    "comment": "Submitted to ACM TOMM",
    "light_analysis": {
      "overview": "提出CenterPolar框架，通过类中心极化策略解决单域泛化深度度量学习中的类别和域偏移挑战。",
      "motivation": "研究动机在于解决单域泛化深度度量学习（SDG-DML）在测试时面临的类别和域偏移双重挑战，这限制了模型在实际应用中的部署。该问题的重要性源于现实世界场景中常遇到未知类别和新域，需要模型具备强泛化能力。现有方法采用域扩展-均衡策略，但依赖代理扩展，生成样本集中在类代理附近，无法模拟实践中广泛的域偏移，导致泛化能力不足，因此需改进方法以处理更复杂分布。",
      "method": "论文提出CenterPolar框架，包含两个协同的类中心极化阶段：第一阶段是类中心离心扩展（C^3E），通过将源数据远离类中心以动态扩展域分布，泛化到更多未见域；第二阶段是类中心向心约束（C^4），将所有样本拉向类中心并强制类间分离，以巩固域不变类信息，泛化到未见类别。关键创新在于动态调整域分布以学习适用于更广目标域的可泛化模型，实验中使用CUB-200-2011 Ext.、Cars196 Ext.、DomainNet、PACS和Office-Home等多个数据集验证方法。",
      "result": "在CUB-200-2011 Ext.、Cars196 Ext.、DomainNet、PACS和Office-Home等广泛使用的数据集上进行实验，结果表明CenterPolar在单域泛化深度度量学习任务上优于现有的最先进方法，展示了其优越性和有效性。摘要未明确说明具体的性能指标如准确率提升，但强调了方法在泛化能力上的显著改进，为后续研究提供了实证支持。",
      "conclusion": "论文的主要贡献是提出了CenterPolar框架，通过类中心极化策略显著提升了SDG-DML在未见类别和域上的泛化能力。学术价值在于改进了深度度量学习的泛化方法，推动了领域自适应和迁移学习的发展；实际应用价值在于增强了模型在多变现实环境中的鲁棒性。未来工作可能包括进一步优化极化策略或扩展到其他机器学习任务，以应对更复杂的泛化场景。",
      "tags": [
        "Single-Domain Generalized Deep Metric Learning",
        "Class-Centric Polarization",
        "Centrifugal Expansion",
        "Centripetal Constraint",
        "Deep Metric Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:54.471918Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09120",
    "title": "Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment",
    "authors": [
      "Chen-Wei Liang",
      "Bin Guo",
      "Zhen-Yuan Wei",
      "Mu-Jiang-Shan Wang"
    ],
    "abstract": "Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\\% cross-jurisdictional performance retention versus 76.2\\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09120.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09120",
    "published": "2026-01-14T03:44:27Z",
    "updated": "2026-01-14T03:44:27Z",
    "comment": "18 pages, 7 figures. Preprint",
    "light_analysis": {
      "overview": "提出自适应多阶段专利权利要求生成框架，通过统一质量评估解决跨司法管辖区泛化、语义关系建模和质量评估不足的问题。",
      "motivation": "当前专利权利要求生成系统面临三个根本局限：跨司法管辖区的泛化能力差，导致在不同法律体系下应用受限；权利要求与现有技术之间的语义关系建模不足，影响生成准确性；质量评估不可靠，难以确保生成内容可靠性。这些问题阻碍了自动化专利起诉工作流的发展，因为现有方法缺乏综合处理能力，迫切需要创新解决方案以提升生成质量和适应性，支持更高效的专利管理流程。",
      "method": "论文提出一个三阶段框架：首先进行关系感知相似性分析，使用多头部注意力机制配备八个专业头显式建模权利要求与现有技术的关系；其次，通过领域自适应权利要求生成，集成课程学习和动态LoRA适配器选择，适应五个专利领域的特性；最后，实现统一质量评估，利用交叉注意力机制在不同评估方面之间综合评判。该方法基于USPTO HUPD数据集、EPO专利集合和Patent-CE基准进行开发和验证，旨在全面优化生成过程。",
      "result": "在广泛实验中，该方法展示显著性能改进：与GPT-4o相比，ROUGE-L得分提升了7.6分；与Llama-3.1-8B相比，BERTScore提高了8.3%；与人类专家的相关性达到0.847，远高于单独评估模型的0.623。此外，跨司法管辖区的性能保留率为89.4%，而基线方法仅为76.2%，这表明方法在生成质量、泛化能力和可靠性方面优于现有基线，为实际应用提供了更强支撑。",
      "conclusion": "本研究贡献了一个自适应多阶段专利权利要求生成框架，成功解决了跨司法管辖区泛化、语义关系建模和质量评估等关键问题。学术价值在于创新整合了多头部注意力、课程学习和动态适配器技术，推动了AI在专利领域的应用；实际价值体现在为自动化专利起诉工作流提供了全面解决方案。未来工作可能涉及扩展至更多领域或优化评估机制，但摘要未明确说明具体局限性。",
      "tags": [
        "Multi-head Attention",
        "Domain Adaptation",
        "Unified Quality Assessment",
        "Curriculum Learning",
        "LoRA"
      ]
    },
    "analyzed_at": "2026-01-15T03:38:16.727160Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09119",
    "title": "Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms",
    "authors": [
      "Yongming Sun"
    ],
    "abstract": "Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.",
    "categories": [
      "cs.CL",
      "econ.GN"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09119.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09119",
    "published": "2026-01-14T03:43:45Z",
    "updated": "2026-01-14T03:43:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出一种零样本技能提取框架，利用大语言模型合成训练数据，结合对比双编码器和分层约束生成，增强ESCO分类法匹配。",
      "motivation": "研究旨在解决将非结构化招聘广告映射到标准化技能分类法（如ESCO）的问题，这对于细粒度劳动力市场分析至关重要。现有监督方法面临极大多标签分类的挑战，依赖大规模标注数据，但标注成本高且数据稀缺，尤其是在非英语环境下，招聘广告语言与正式技能定义差异显著，限制了方法的可扩展性和实用性。因此，开发无需人工标注的零样本方法变得尤为重要。",
      "method": "方法采用大语言模型从ESCO定义中自动生成合成训练实例，并引入基于ESCO Level-2类别的分层约束多技能生成，以提升多标签场景下的语义连贯性。在此基础上，训练一个对比双编码器，将招聘广告句子和ESCO技能描述对齐到共享嵌入空间；编码器以BERT为骨干，集成BiLSTM和注意力池化层，有效处理长且信息密集的语句。上游使用基于RoBERTa的二元过滤器筛选非技能句子，提高端到端精度。",
      "result": "实验结果显示，分层条件生成相对于无约束生成，显著改善了训练数据的流畅性和区分性。模型在真实中国招聘广告数据集上实现零样本迁移，取得了强检索性能，F1@5达到0.72，优于TF-IDF和标准BERT基线方法，证明了框架的有效性和在现实场景中的适用性。",
      "conclusion": "该框架为劳动力经济学和劳动力分析提供了一条可扩展且数据高效的自动化技能编码路径，解决了标注数据稀缺的挑战，特别是在非英语环境中。通过零样本学习，展示了实用应用价值，并为未来研究提供了基础，可能拓展到其他领域或改进生成策略以优化性能。",
      "tags": [
        "Large Language Model",
        "Contrastive Learning",
        "BERT",
        "Attention Mechanisms",
        "Multi-Label Classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:38:07.989850Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09118",
    "title": "LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data",
    "authors": [
      "Jackie Alex",
      "Guoqiang Huan"
    ],
    "abstract": "This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\\% in $S_α$, +0.86\\% in IOU, and +1.77\\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09118.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09118",
    "published": "2026-01-14T03:35:09Z",
    "updated": "2026-01-14T03:35:09Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出轻量级金字塔交叉注意力网络LPCANet，利用RGB-D数据进行高效准确的铁路表面缺陷检测。",
      "motivation": "当前基于视觉的铁路缺陷检测方法存在高计算复杂度、参数过多和准确率不足的问题，这限制了工业应用中的实时检测需求。现有方法通常依赖单一RGB数据，忽略深度信息，导致缺陷识别不准确，难以平衡精度与效率。铁路安全至关重要，但传统人工检测效率低，深度学习模型往往资源消耗大，因此需要一种轻量级多模态解决方案来克服这些局限性。",
      "method": "LPCANet采用MobileNetv2作为骨干网络提取RGB特征，轻量级金字塔模块处理深度数据以降低计算开销。交叉注意力机制动态融合RGB和深度信息，增强多模态交互，空间特征提取器进一步提升结构特征分析能力。该架构在三个无监督RGB-D铁路数据集上评估，专注于优化参数数量和FLOPs，确保高效推理，关键创新在于结合轻量化设计与多模态融合技术。",
      "result": "在NEU-RSDDS-AUG、RSDD-TYPE1和RSDD-TYPE2数据集上，LPCANet以仅9.90百万参数、2.50 G FLOPs和162.60 fps推理速度，实现最先进性能。与18个基线方法相比，Sα提升1.48%、IOU提升0.86%、MAE提升1.77%。消融实验证实交叉注意力机制和空间特征提取器的关键作用，非铁路数据集实验验证了其泛化能力，展示了在工业缺陷检测中的高效与准确优势。",
      "conclusion": "LPCANet通过轻量级设计和多模态融合，显著提升铁路表面缺陷检测的准确性与效率，有效连接传统与深度学习方法，具有重要工业应用价值。该研究为资源受限环境下的实时检测提供新思路，未来工作将集中于进一步模型压缩以实现实际部署，并探索更广泛的工业场景应用，以应对更复杂需求。",
      "tags": [
        "Rail Surface Defect Detection",
        "RGB-D Data",
        "Cross-Attention",
        "Lightweight Network",
        "Multimodal Fusion"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:33.543903Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09116",
    "title": "LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models",
    "authors": [
      "Haoyan Gong",
      "Hongbin Liu"
    ],
    "abstract": "Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing \"restoration-then-recognition\" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09116.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09116",
    "published": "2026-01-14T03:32:55Z",
    "updated": "2026-01-14T03:32:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出基于Qwen3-VL的端到端结构感知多模态推理框架，通过字符感知查询实现退化车牌文本识别。",
      "motivation": "现实世界车牌识别面临运动模糊、低分辨率和复杂照明等严重退化挑战，传统'恢复后识别'两阶段方法存在像素级优化目标与字符识别语义目标不对齐的根本缺陷，导致artifact干扰和错误累积。尽管Vision-Language Models (VLMs)展现出强大能力，但缺乏对车牌字符序列（如固定长度、特定顺序）的显式结构建模，因此需新方法解决这一不足，提升识别准确性和鲁棒性，以应对复杂环境下的实际应用需求。",
      "method": "作者提出基于Qwen3-VL的端到端结构感知多模态推理框架。核心创新是Character-Aware Multimodal Reasoning Module (CMRM)，引入一组可学习的Character Slot Queries，通过跨注意力机制从视觉特征中主动检索与字符位置对应的细粒度证据。然后通过残差调制将这些字符感知表征注入回视觉令牌，使语言模型能够基于显式结构先验进行自回归生成。结合LoRA参数高效微调策略，实现领域适应同时保留大型模型的泛化能力，实验在合成和现实世界严重退化数据集上进行。",
      "result": "在合成和现实世界严重退化数据集上的广泛实验表明，该方法显著优于现有的恢复-识别组合方法和通用VLMs，验证了将结构化推理整合到大型模型中进行低质量文本识别任务的优越性。但摘要未明确说明具体的性能指标如准确率提升百分比，仅基于实验对比推断其效果突出。",
      "conclusion": "本研究提出端到端结构感知多模态推理框架，通过字符感知查询和残差调制解决了退化车牌识别中像素与语义目标不对齐问题，提升了识别性能。其学术价值在于展示了结构化推理在大型多模态模型中的应用潜力，实际应用价值可改进现实世界车牌识别系统，未来工作可探索该方法在其他低质量文本识别任务中的扩展和优化。",
      "tags": [
        "Large Multimodal Models",
        "Cross-Attention Mechanism",
        "Character Slot Queries",
        "LoRA Fine-Tuning",
        "End-to-End Framework"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:43.068789Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09113",
    "title": "The AI Hippocampus: How Far are We From Human Memory?",
    "authors": [
      "Zixia Jia",
      "Jiaqi Li",
      "Yipeng Kang",
      "Yuxuan Wang",
      "Tong Wu",
      "Quansen Wang",
      "Xiaobo Wang",
      "Shuyi Zhang",
      "Junzhe Shen",
      "Qing Li",
      "Siyuan Qi",
      "Yitao Liang",
      "Di He",
      "Zilong Zheng",
      "Song-Chun Zhu"
    ],
    "abstract": "Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09113.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09113",
    "published": "2026-01-14T03:24:08Z",
    "updated": "2026-01-14T03:24:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "该综述提出了一个结构化的分类法，将大型语言模型和多模态大型语言模型中的记忆机制系统性地分为隐式、显式和代理三种范式。",
      "motivation": "记忆在现代大型语言模型和多模态模型中扮演着基础角色，能够增强模型的推理能力、适应性和上下文保真度。随着模型从静态预测器向交互式系统转型，纳入记忆机制已成为其架构和功能演化的核心主题。当前研究分散，缺乏系统性分类，因此本综述旨在解决这一问题，为理解和改进记忆机制提供全面视角。",
      "method": "该综述采用文献合成的方法，构建了一个结构化分类法，将记忆机制分为三类：隐式记忆指预训练模型参数中的知识，包括记忆和关联检索；显式记忆涉及外部组件存储和检索动态知识；代理记忆则关注自主代理中的持久结构，支持长期规划和协作。此外，综述还扩展至多模态设置，分析跨模态一致性。",
      "result": "作为一篇综述，本文没有提供具体实验结果，而是总结了记忆机制的关键架构进展、基准任务和开放挑战。例如，它讨论了内存容量、对齐性、事实一致性和跨系统互操作性等问题。通过与现有文献的对比，综述揭示了当前研究的不足和未来方向。摘要未明确说明具体性能改进。",
      "conclusion": "该综述的主要贡献在于提出了一个全面的记忆分类法，系统化了大型语言模型和多模态模型中的记忆研究。其学术价值在于为理解和分析记忆机制提供了统一框架，实际应用价值在于指导模型设计以支持持续学习和交互。未来研究方向包括解决内存容量、事实一致性等开放挑战。",
      "tags": [
        "Large Language Model",
        "Multi-Modal Large Language Model",
        "Implicit Memory",
        "Explicit Memory",
        "Agentic Memory"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:40.208684Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09111",
    "title": "Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning",
    "authors": [
      "Yang Li",
      "Aming Wu",
      "Zihao Zhang",
      "Yahong Han"
    ],
    "abstract": "Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09111.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09111",
    "published": "2026-01-14T03:22:16Z",
    "updated": "2026-01-14T03:22:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出slow4fast-VLN框架，通过动态交互的快速慢速推理机制，增强视觉语言导航在开放环境下的泛化能力。",
      "motivation": "传统视觉语言导航（VLN）方法基于闭集假设，即训练和测试数据具有相同的输入图像和指令风格，但真实世界开放多样，包含各种未见环境和不一致指令，导致现有方法在动态场景中导航困难。GSA-VLN任务旨在解决这一问题，要求代理学习泛化导航能力，以应对开放环境带来的挑战，从而提升导航系统在实际应用中的适应性和鲁棒性。",
      "method": "论文提出slow4fast-VLN框架，包括快速推理模块和慢速推理模块。快速模块是一个端到端策略网络，实时处理输入并输出动作，同时积累执行记录到历史仓库中构建记忆。慢速模块分析这些记忆，通过深度反思提取提升决策泛化能力的经验，并结构化存储。关键创新在于两模块动态交互，慢速模块的经验不断优化快速模块，与传统独立机制不同，从而增强系统对开放场景的适应能力。摘要未明确说明使用的数据集和具体模型架构细节。",
      "result": "摘要未明确说明具体实验结果，但论文提出的slow4fast-VLN框架旨在通过动态交互的快速慢速推理，提高视觉语言导航在开放环境下的泛化性能。该框架预计能帮助代理更有效地应对未见环境和指令，提升导航任务的适应性和效率，但需进一步实验验证与基线方法的对比效果，如准确率提升或效率改进。",
      "conclusion": "该研究主要贡献在于提出一个动态交互的快速慢速推理框架，用于解决开放环境下的视觉语言导航问题。通过模拟人类认知系统，该框架增强了导航代理的泛化能力，具有重要学术价值，推动了VLN领域向真实世界应用的发展。潜在局限性可能包括计算复杂性或对特定场景的依赖，未来工作可扩展实验验证、优化交互机制或探索其他开放任务的应用。",
      "tags": [
        "Vision-Language Navigation",
        "Fast-Slow Reasoning",
        "General Scene Adaptation",
        "Interactive Framework",
        "End-to-End Strategy Network"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:30.688632Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09110",
    "title": "SAM-Aug: Leveraging SAM Priors for Few-Shot Parcel Segmentation in Satellite Time Series",
    "authors": [
      "Kai Hu",
      "Yaozu Feng",
      "Vladimir Lysenko",
      "Ya Guo Member",
      "Huayi Wu"
    ],
    "abstract": "Few-shot semantic segmentation of time-series remote sensing images remains a critical challenge, particularly in regions where labeled data is scarce or costly to obtain. While state-of-the-art models perform well under full supervision, their performance degrades significantly under limited labeling, limiting their real-world applicability. In this work, we propose SAM-Aug, a new annotation-efficient framework that leverages the geometry-aware segmentation capability of the Segment Anything Model (SAM) to improve few-shot land cover mapping. Our approach constructs cloud-free composite images from temporal sequences and applies SAM in a fully unsupervised manner to generate geometry-aware mask priors. These priors are then integrated into training through a proposed loss function called RegionSmoothLoss, which enforces prediction consistency within each SAM-derived region across temporal frames, effectively regularizing the model to respect semantically coherent structures. Extensive experiments on the PASTIS-R benchmark under a 5 percent labeled setting demonstrate the effectiveness and robustness of SAM-Aug. Averaged over three random seeds (42, 2025, 4090), our method achieves a mean test mIoU of 36.21 percent, outperforming the state-of-the-art baseline by +2.33 percentage points, a relative improvement of 6.89 percent. Notably, on the most favorable split (seed=42), SAM-Aug reaches a test mIoU of 40.28 percent, representing an 11.2 percent relative gain with no additional labeled data. The consistent improvement across all seeds confirms the generalization power of leveraging foundation model priors under annotation scarcity. Our results highlight that vision models like SAM can serve as useful regularizers in few-shot remote sensing learning, offering a scalable and plug-and-play solution for land cover monitoring without requiring manual annotations or model fine-tuning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09110.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09110",
    "published": "2026-01-14T03:18:04Z",
    "updated": "2026-01-14T03:18:04Z",
    "comment": "13 pages, 6 figures",
    "light_analysis": {
      "overview": "提出SAM-Aug框架，通过利用Segment Anything Model的先验改进少样本遥感时间序列分割性能。",
      "motivation": "研究动机源于少样本遥感时间序列语义分割在标注数据稀缺地区面临的挑战，如土地覆盖监测中标注成本高或不可得。现有方法在全监督下表现良好，但在标注有限时性能显著下降，限制了其在实际应用中的可扩展性，尤其是在资源有限区域。因此，需要开发标注高效的框架来解决这一瓶颈，提升模型在少量标签下的鲁棒性和实用性。",
      "method": "SAM-Aug方法首先从卫星时间序列构建无云合成图像，以去除噪声干扰。然后，无监督应用Segment Anything Model生成几何感知的掩码先验，这些先验捕捉图像中的语义结构。通过提出的RegionSmoothLoss损失函数，强制模型在每个SAM衍生区域内跨时间帧的预测一致性，从而正则化训练过程。关键创新在于利用基础模型先验作为正则化器，无需额外标注或对SAM进行微调，提供了一种可扩展的技术路线。",
      "result": "在PASTIS-R基准的5%标注设置下进行实验，SAM-Aug在三个随机种子（42、2025、4090）上的平均测试mIoU达到36.21%，优于最先进基线2.33个百分点，相对改进6.89%。在最优分割（seed=42）上，测试mIoU达到40.28%，相对增益11.2%。所有种子上的结果均显示一致的性能提升，验证了方法在不同初始条件下的泛化能力和鲁棒性，表明利用SAM先验能有效缓解标注稀缺问题。",
      "conclusion": "论文主要贡献是展示了SAM-Aug如何利用Segment Anything Model的先验作为正则化器，显著提升少样本遥感时间序列分割的性能。这不仅强调了基础模型在标注稀缺场景中的学术价值，还为土地覆盖监测提供了一种可扩展、即插即用的解决方案，无需手动标注或额外微调。潜在局限性包括方法对SAM模型质量的依赖，未来工作可扩展到其他遥感任务或探索更多基础模型的应用。",
      "tags": [
        "Few-Shot Learning",
        "Semantic Segmentation",
        "Time Series Analysis",
        "Segment Anything Model (SAM)",
        "Remote Sensing"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:36.788987Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09108",
    "title": "Small but Mighty: Dynamic Wavelet Expert-Guided Fine-Tuning of Large-Scale Models for Optical Remote Sensing Object Segmentation",
    "authors": [
      "Yanguang Sun",
      "Chao Wang",
      "Jian Yang",
      "Lei Luo"
    ],
    "abstract": "Accurately localizing and segmenting relevant objects from optical remote sensing images (ORSIs) is critical for advancing remote sensing applications. Existing methods are typically built upon moderate-scale pre-trained models and employ diverse optimization strategies to achieve promising performance under full-parameter fine-tuning. In fact, deeper and larger-scale foundation models can provide stronger support for performance improvement. However, due to their massive number of parameters, directly adopting full-parameter fine-tuning leads to pronounced training difficulties, such as excessive GPU memory consumption and high computational costs, which result in extremely limited exploration of large-scale models in existing works. In this paper, we propose a novel dynamic wavelet expert-guided fine-tuning paradigm with fewer trainable parameters, dubbed WEFT, which efficiently adapts large-scale foundation models to ORSIs segmentation tasks by leveraging the guidance of wavelet experts. Specifically, we introduce a task-specific wavelet expert extractor to model wavelet experts from different perspectives and dynamically regulate their outputs, thereby generating trainable features enriched with task-specific information for subsequent fine-tuning. Furthermore, we construct an expert-guided conditional adapter that first enhances the fine-grained perception of frozen features for specific tasks by injecting trainable features, and then iteratively updates the information of both types of feature, allowing for efficient fine-tuning. Extensive experiments show that our WEFT not only outperforms 21 state-of-the-art (SOTA) methods on three ORSIs datasets, but also achieves optimal results in camouflage, natural, and medical scenarios. The source code is available at: https://github.com/CSYSI/WEFT.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09108.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09108",
    "published": "2026-01-14T03:11:50Z",
    "updated": "2026-01-14T03:11:50Z",
    "comment": "Accepted at AAAI 2026",
    "light_analysis": {
      "overview": "提出了WEFT方法，通过动态小波专家指导微调，以更少参数高效适应大规模基础模型到光学遥感图像分割任务。",
      "motivation": "光学遥感图像（ORSIs）中对象的精确分割对遥感应用至关重要。现有方法基于中等规模预训练模型的全参数微调，虽性能良好，但难以利用大规模基础模型的潜力，因其参数多导致训练困难，如GPU内存消耗大和计算成本高，限制了实际探索。因此，需要一种参数高效的方法来克服这些挑战。",
      "method": "论文提出WEFT范式，包括任务特定小波专家提取器和专家指导条件适配器。小波专家提取器从多角度建模小波特征，动态调节生成富含任务信息的可训练特征；条件适配器通过注入这些特征增强冻结特征的细粒度感知，并迭代更新特征，实现参数高效微调。该方法利用小波变换和动态调节技术，适应大规模模型。",
      "result": "在三个光学遥感图像数据集上的实验表明，WEFT超越21个最先进方法，展示了其性能优势。此外，在伪装、自然和医疗场景中也取得最优结果，验证了方法的通用性和有效性，具体指标如准确率提升摘要未明确说明，但结果优于基线。",
      "conclusion": "WEFT成功解决大规模模型在遥感分割中的微调难题，通过参数高效方法显著提升性能。其学术价值在于推动遥感图像分析技术，实际应用可扩展到多场景分割。局限性或未来工作摘要未明确说明，但可能涉及进一步优化或扩展到更多任务。",
      "tags": [
        "Dynamic Wavelet Expert-Guided Fine-Tuning",
        "Large-Scale Models",
        "Optical Remote Sensing Segmentation",
        "Conditional Adapter",
        "Wavelet Transform"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:32.382704Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09107",
    "title": "Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams",
    "authors": [
      "Lachlan Holden",
      "Feras Dayoub",
      "Alberto Candela",
      "David Harvey",
      "Tat-Jun Chin"
    ],
    "abstract": "Accurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use ground-aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09107.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09107",
    "published": "2026-01-14T03:11:05Z",
    "updated": "2026-01-14T03:11:05Z",
    "comment": "7 pages, 10 figures. Presented at the International Conference on Space Robotics (iSpaRo) 2025 in Sendai, Japan. Dataset available: https://doi.org/10.5281/zenodo.17364038",
    "light_analysis": {
      "overview": "提出一种利用视觉基础模型和高容量合成数据实现领域泛化的交叉视图定位方法，用于行星漫游车在空地图中的准确定位。",
      "motivation": "行星机器人任务如地面-空中团队需要高精度的自主定位以支持未来太空任务扩展，但真实行星环境中训练数据稀缺，地面真实位置标签不足，导致传统机器学习方法难以直接应用。现有方法面临领域差距挑战，从合成数据泛化到真实图像效果有限，开发能克服这些限制的技术至关重要。",
      "method": "论文采用双编码器深度神经网络处理地面视图RGB图像和空地图，实现交叉视图定位。创新点包括整合预训练视觉基础模型进行语义分割，并使用高容量合成数据弥合领域差异。贡献了新数据集，包含真实世界漫游车轨迹和合成图像对，应用粒子滤波器对图像序列进行状态估计以提高定位稳定性。",
      "result": "摘要未明确说明具体实验数据或性能指标。基于方法描述，该方法通过合成数据和视觉基础模型改善了定位准确性，在简单和复杂轨迹上实现了稳定位置估计，但缺乏与基线方法的对比结果和详细量化指标。",
      "conclusion": "论文主要贡献是提出领域泛化的交叉视图定位框架，结合视觉基础模型和合成数据解决真实数据稀缺问题，并贡献相关数据集。学术上推动了机器人定位和跨视图学习发展，实际中增强了行星漫游车自主导航能力，支持未来太空探索任务。未来可优化模型泛化或扩展更多行星环境。",
      "tags": [
        "Vision Foundation Models",
        "Cross-View Localisation",
        "Dual-Encoder Neural Networks",
        "Semantic Segmentation",
        "Particle Filter"
      ]
    },
    "analyzed_at": "2026-01-15T03:38:46.713438Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09105",
    "title": "AviationLMM: A Large Multimodal Foundation Model for Civil Aviation",
    "authors": [
      "Wenbin Li",
      "Jingling Wu",
      "Xiaoyong Lin. Jing Chen",
      "Cong Chen"
    ],
    "abstract": "Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09105.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09105",
    "published": "2026-01-14T03:10:33Z",
    "updated": "2026-01-14T03:10:33Z",
    "comment": "Accepted by 2025 7th International Conference on Interdisciplinary Computer Science and Engineering (ICICSE 2025) conference, Chongqing, China; 9 pages,1 figure,5 tables",
    "light_analysis": {
      "overview": "论文提出了AviationLMM，一个用于民航领域的大型多模态基础模型，旨在统一异构数据流并支持理解、推理、生成和代理应用。",
      "motivation": "民航是全球交通和商业的基石，确保其安全、效率和客户满意度至关重要。然而，现有AI解决方案在民航中孤立且狭窄，专注于单模态或孤立任务，无法整合语音通信、雷达轨迹、传感器流和文本报告等异构数据。这导致态势感知能力受限、系统适应性不足以及实时决策支持困难，亟需一种集成化多模态方法以克服这些局限性，提升航空系统的整体性能。",
      "method": "论文描述了AviationLMM的模型架构，它能摄入多模态输入，包括空地语音、监视数据、机载遥测、视频和结构化文本。核心方法涉及跨模态对齐和融合技术，以整合这些异构数据流，并生成灵活输出，如态势摘要、风险警报、预测诊断和多模态事件重建。方法的关键创新点在于统一处理民航数据的多样来源，支持从理解到应用的端到端处理，同时论文识别了未来研究机会，如数据获取、预训练和推理算法，但未详述具体实现细节。",
      "result": "摘要未明确说明具体实验结果或性能指标。论文主要聚焦于愿景提出和设计挑战的阐述，未报告实证数据、准确率提升或效率改进的对比分析。因此，无法提供与基线方法的具体比较，但强调该模型旨在解决现有方法的整合不足问题，潜在效果需通过后续研究验证。",
      "conclusion": "论文的主要贡献是提出了AviationLMM的愿景和设计框架，以统一民航异构数据流并促进多模态应用，从而提升航空AI的集成度和实用性。该研究具有重要的学术价值，推动了大型多模态基础模型在民航领域的发展，并强调了信任度、隐私保护和鲁棒性等关键挑战。实际应用价值在于增强航空安全、效率和实时决策支持，未来工作方向包括数据对齐、合成场景生成和模型可靠性改进。",
      "tags": [
        "Large Multimodal Model",
        "Cross-Modal Alignment",
        "Foundation Model",
        "Multimodal Data Fusion",
        "Aviation Applications"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:13.803476Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09103",
    "title": "Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion",
    "authors": [
      "Haijian Shao",
      "Wei Liu",
      "Xing Deng",
      "Daze Lu"
    ],
    "abstract": "Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\\% and 98\\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09103.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09103",
    "published": "2026-01-14T03:09:13Z",
    "updated": "2026-01-14T03:09:13Z",
    "comment": "18 pages, 9 figures, 3 tables, 1 algorithm",
    "light_analysis": {
      "overview": "论文提出了一种集成小波变换和类间融合的数据增强方法，显著提升了不平衡心电图分类的准确性和鲁棒性。",
      "motivation": "心电图（ECG）数据在自动分类中常面临类别不平衡问题，导致罕见心脏疾病如某些心律失常在数据集中代表性不足，影响深度学习模型的性能。现有方法如过采样和算法生成虽能缓解类别倾斜，但效果存疑且缺乏共识。此外，ECG采集过程中引入的噪声进一步加剧了数据处理难度。因此，开发一种能同时处理类别不平衡和噪声的方法是提升ECG自动诊断系统可靠性的关键。",
      "method": "本研究提出了一种基于小波变换的特征融合方法，重点在于类间融合，以生成训练和测试特征库。首先，应用小波变换提取ECG信号特征；然后，通过类间融合技术增强少数类样本的特征表示，形成特征数据库。接着，将原始训练和测试数据与这些特征库融合，从而构建更平衡的数据集。该方法在CPSC 2018数据集上实施，但摘要未明确说明具体使用的模型架构。",
      "result": "在CPSC 2018数据集上，该方法取得了显著的分类准确率：正常节律达99%，心房颤动98%，一度房室传导阻滞97%，左束支传导阻滞98%，右束支传导阻滞96%，房性早搏92%，室性早搏93%，ST段压低和抬高类别平均准确率在92%至98%之间。总体平均识别准确率在92%到98%范围内，超越了该数据集上任何已知算法的性能。",
      "conclusion": "论文的主要贡献是提出了一种集成小波变换和类间融合的数据增强方法，有效解决了ECG分类中的类别不平衡和噪声问题。该研究提升了ECG自动分类的准确性和鲁棒性，对心血管疾病诊断具有重要应用价值。摘要未明确说明局限性，但可能涉及计算复杂度或泛化能力；未来工作可扩展到其他医疗数据集或优化融合策略。",
      "tags": [
        "Wavelet Transform",
        "Feature Fusion",
        "Data Augmentation",
        "Imbalanced Classification",
        "Electrocardiogram Classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:28.254709Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09100",
    "title": "DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model",
    "authors": [
      "Lixiang Zhang",
      "Chenggong Zhao",
      "Qing Gao",
      "Xiaoke Zhao",
      "Gengyi Bai",
      "Jinhu Lv"
    ],
    "abstract": "Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09100.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09100",
    "published": "2026-01-14T03:02:22Z",
    "updated": "2026-01-14T03:02:22Z",
    "comment": "14 pages, 6 figures",
    "light_analysis": {
      "overview": "本研究提出DScheLLM，一种通过微调双系统大语言模型实现动态调度的创新方法。",
      "motivation": "生产调度高度易受动态干扰影响，如处理时间变化、机器可用性波动和意外任务插入。传统方法通常依赖事件特定模型和显式分析公式，这限制了其适应性和对未知干扰的泛化能力。该问题在现实生产环境中至关重要，因为动态变化频繁发生，现有方法难以灵活应对，亟需更智能、自适应的调度优化方案来提升效率和鲁棒性。",
      "method": "DScheLLM采用统一的大语言模型框架，结合双系统（快速-慢速）推理架构处理不同规模的动态事件。关键创新在于使用操作研究求解器生成的精确调度数据来训练快速和慢速推理模式的数据集，并基于华为OpenPangu Embedded-7B模型，通过LoRA技术进行微调。该方法整合了混合推理范式，以高效适应动态调度需求，强调了从数据驱动角度构建可泛化模型的技术特色。",
      "result": "在标准作业车间调度基准上的实验评估表明，快速思维模式能高效生成高质量调度，慢速思维模式则可产生与求解器兼容、格式良好的决策输入。尽管摘要未提供具体性能指标（如准确率或效率改进数据），但结果证明了该方法在动态环境中的有效性，与传统方法相比，展示了更好的适应性和泛化潜力。",
      "conclusion": "该研究是早期将大语言模型应用于动态作业车间调度的探索之一，主要贡献在于提出了一种基于微调大语言模型的双系统架构，突显了其在智能和自适应调度优化中的巨大潜力。学术价值在于拓宽了大语言模型在工业优化领域的应用，实际应用价值体现在提升生产调制的灵活性和效率。未来工作可能包括扩展到更广泛的干扰类型和调度场景。",
      "tags": [
        "Large Language Model",
        "Fine-tuning",
        "Dual-System Reasoning",
        "Dynamic Scheduling",
        "Job Shop Scheduling"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:13.175789Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09097",
    "title": "Programming over Thinking: Efficient and Robust Multi-Constraint Planning",
    "authors": [
      "Derrick Goh Xin Deik",
      "Quanyu Long",
      "Zhengyuan Liu",
      "Nancy F. Chen",
      "Wenya Wang"
    ],
    "abstract": "Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.09097.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09097",
    "published": "2026-01-14T02:58:07Z",
    "updated": "2026-01-14T02:58:07Z",
    "comment": "8 pages of main text, 2 pages of references and and limitations, 37 pages of appendices",
    "light_analysis": {
      "overview": "SCOPE框架通过分离推理与代码执行，实现了多约束规划的高效性和鲁棒性，显著提升性能并降低成本。",
      "motivation": "多约束规划需处理多个冲突约束，但现有大语言模型方法存在根本局限性。纯推理范式如思维链（CoT）在约束复杂时易产生不一致、错误累积和高成本；而结合编码的策略缺乏灵活性，常生成问题特定代码或依赖固定求解器，无法捕获跨问题的通用逻辑。因此，需要开发一种灵活、高效且可重用的解决方案来解决这些不足。",
      "method": "SCOPE框架将查询特定推理与通用代码执行分离，核心创新在于生成一致、确定和可重用的求解函数，仅需最小输入参数更改，避免从头编写代码或依赖固定求解器。这种方法提高了跨查询的通用性，通过分离逻辑实现稳健规划，使用大语言模型如GPT-4o来整合推理与代码生成。",
      "result": "在TravelPlanner基准测试中，使用GPT-4o时，SCOPE达到93.1%的成功率，比最佳基线（CoT）提升61.6%。同时，推理成本降低1.4倍，时间减少约4.67倍，展示了框架在性能、效率和成本上的显著改进，实现了state-of-the-art性能。",
      "conclusion": "SCOPE框架通过分离推理与执行，提供了多约束规划的先进解决方案，具有一致、确定和可重用特性，显著降低推理成本和时间，提升实际应用价值。贡献包括高效性和鲁棒性的提升，未来工作可扩展到更多领域或优化框架以应对复杂约束场景。",
      "tags": [
        "Multi-Constraint Planning",
        "Large Language Models (LLM)",
        "Code Planning",
        "Reasoning-Execution Separation",
        "Scalability"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:13.494026Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09096",
    "title": "Comparative Assessment of Concrete Compressive Strength Prediction at Industry Scale Using Embedding-based Neural Networks, Transformers, and Traditional Machine Learning Approaches",
    "authors": [
      "Md Asiful Islam",
      "Md Ahmed Al Muzaddid",
      "Afia Jahin Prema",
      "Sreenath Reddy Vuske"
    ],
    "abstract": "Concrete is the most widely used construction material worldwide; however, reliable prediction of compressive strength remains challenging due to material heterogeneity, variable mix proportions, and sensitivity to field and environmental conditions. Recent advances in artificial intelligence enable data-driven modeling frameworks capable of supporting automated decision-making in construction quality control. This study leverages an industry-scale dataset consisting of approximately 70,000 compressive strength test records to evaluate and compare multiple predictive approaches, including linear regression, decision trees, random forests, transformer-based neural networks, and embedding-based neural networks. The models incorporate key mixture design and placement variables such as water cement ratio, cementitious material content, slump, air content, temperature, and placement conditions. Results indicate that the embedding-based neural network consistently outperforms traditional machine learning and transformer-based models, achieving a mean 28-day prediction error of approximately 2.5%. This level of accuracy is comparable to routine laboratory testing variability, demonstrating the potential of embedding-based learning frameworks to enable automated, data-driven quality control and decision support in large-scale construction operations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09096.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09096",
    "published": "2026-01-14T02:58:05Z",
    "updated": "2026-01-14T02:58:05Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究比较了多种机器学习方法，发现基于嵌入的神经网络在混凝土抗压强度预测中表现最优，误差约2.5%，为施工质量控制提供有效工具。",
      "motivation": "混凝土是全球最广泛使用的建筑材料，但抗压强度预测因材料异质性、混合比例多变以及环境条件敏感性而面临挑战。现有传统方法可能无法充分处理大规模数据或复杂非线性关系，导致预测精度不足。人工智能的发展为数据驱动建模提供了新机遇，本研究旨在利用大规模行业数据解决这一实际问题，提升施工质量控制的自动化决策能力。",
      "method": "研究采用了一个行业规模数据集，包含约70,000个混凝土抗压强度测试记录，评估了多种预测方法，包括传统机器学习如线性回归、决策树和随机森林，以及神经网络如基于变压器的神经网络和基于嵌入的神经网络。所有模型整合了关键混合设计和放置变量，如水灰比、胶凝材料含量、坍落度、空气含量、温度和放置条件，以全面捕捉影响强度的因素。",
      "result": "实验结果表明，基于嵌入的神经网络在所有方法中表现最出色，平均28天预测误差约为2.5%。这一精度水平与常规实验室测试的变异性相当，显示出高可靠性。相比之下，传统机器学习和基于变压器的模型在预测准确度上较低，突出了嵌入方法在混凝土强度预测中的优越性能。",
      "conclusion": "本研究的主要贡献是证实了基于嵌入的神经网络在混凝土抗压强度预测中的有效性和优越性，为AI在建筑材料工程中的应用提供了实证支持。学术上，它推动了数据驱动建模的发展；实际上，它有望实现自动化、数据驱动的质量控制和决策支持，提升大规模施工项目的效率。未来工作可探索模型泛化到更多变量或更大数据集的应用潜力。",
      "tags": [
        "Embedding-based Neural Networks",
        "Transformers",
        "Random Forests",
        "Compressive Strength Prediction",
        "Concrete Engineering"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:40.491968Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09093",
    "title": "Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling",
    "authors": [
      "Zhixiang Liang",
      "Beichen Huang",
      "Zheng Wang",
      "Minjia Zhang"
    ],
    "abstract": "Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09093.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09093",
    "published": "2026-01-14T02:54:55Z",
    "updated": "2026-01-14T02:54:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出STEP框架，利用隐藏状态进行步级推理评估和动态修剪，有效降低大语言模型推理延迟并提升准确性。",
      "motivation": "大语言模型通过测试时扩展生成多个推理踪迹来增强推理能力，但冗长的踪迹和多次采样导致高计算成本和端到端延迟。现有加速方法依赖于基于相似性或置信度的修剪，但这些信号不能可靠地指示踪迹质量，限制了效率和准确性。因此，需要一种更可靠的评估机制来优化推理过程，解决实时应用中的延迟问题。",
      "method": "论文提出STEP框架，使用隐藏状态评估推理步骤，实现步级踪迹评估和动态修剪。方法包括训练一个轻量级步评分器，基于隐藏状态估计踪迹质量；并设计GPU内存感知的修剪策略，在KV缓存导致内存饱和时触发修剪，以减少端到端延迟。创新点在于利用早期隐藏状态信号和资源管理优化，提升推理效率。",
      "result": "实验在多个挑战性推理基准上进行，结果显示STEP框架相比self-consistency基线方法，平均将端到端推理延迟降低了45%-70%，同时提高了推理准确性。这表明基于隐藏状态的修剪策略能有效减少计算开销，实现速度和精度的双重提升，验证了方法的优越性。",
      "conclusion": "本研究通过STEP框架，成功利用隐藏状态进行步级评估和动态修剪，显著降低了大语言模型的推理延迟并提升准确性。这为高效推理提供了新方法，具有重要的学术和应用价值，未来工作可探索该框架在不同模型和任务中的泛化能力，摘要未明确说明具体局限性。",
      "tags": [
        "Large Language Model",
        "Hidden State Evaluation",
        "Dynamic Pruning",
        "GPU Memory Management",
        "KV Cache Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:36.428344Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09089",
    "title": "SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding",
    "authors": [
      "Shuyang Hou",
      "Yi Hu",
      "Muhan Zhang"
    ],
    "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their reasoning capabilities. However, they continue to struggle with basic character-level tasks, such as counting letters in words, a problem rooted in their tokenization process. While existing benchmarks have highlighted this weakness through basic character operations, such failures are often dismissed due to lacking practical relevance. Yet, many real-world applications, such as navigating text-based maps or interpreting structured tables, rely heavily on precise sub-token understanding. In this regard, we introduce SubTokenTest, a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks. Our benchmark includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. We provide a comprehensive evaluation of nine advanced LLMs. Additionally, we investigate the impact of test-time scaling on sub-token reasoning and explore how character-level information is encoded within the hidden states.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09089.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09089",
    "published": "2026-01-14T02:45:08Z",
    "updated": "2026-01-14T02:45:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出 SubTokenTest 基准测试，评估大型语言模型在子标记理解上的实际能力，以解决字符级任务缺陷。",
      "motivation": "大型语言模型在推理能力上虽有显著提升，但在基础字符级任务如统计单词中的字母数量上仍表现不佳，问题源于其标记化过程。现有基准测试通过基本字符操作揭示了这一弱点，但常因缺乏实际相关性而被忽视。然而，许多真实世界应用，如导航文本地图或解释结构化表格，严重依赖精确的子标记理解。因此，开发一个实用、基于任务的评估标准至关重要，以推动模型改进并满足应用需求。",
      "method": "论文引入 SubTokenTest，一个全面的基准测试，包括十个实用任务，覆盖四个领域，旨在通过实用性驱动的任务评估子标记理解。关键创新在于将性能与复杂推理解耦，以隔离标记化相关的失败。此外，研究评估了九个先进的 LLMs，并探究了测试时缩放对子标记推理的影响，以及字符级信息在隐藏状态中的编码方式，从而深入分析模型内部机制。",
      "result": "摘要未明确说明具体实验数据。论文全面评估了九个先进的 LLMs，揭示了这些模型在子标记理解任务上的普遍不足。研究表明，通过 SubTokenTest 基准测试，可以有效识别模型在字符级处理中的缺陷，并探索了测试时缩放如何影响子标记推理能力。同时，对隐藏状态的编码分析提供了关于信息表示的见解，但未提供具体的性能指标对比。",
      "conclusion": "本研究的核心贡献是开发了 SubTokenTest 基准测试，为评估大型语言模型的子标记理解能力提供了新标准。其学术价值在于揭示了标记化过程对模型性能的关键影响，而实际应用价值在于促进如文本处理和界面交互等场景的改进。未来工作可进一步优化任务设计，探索更高效的标记化策略，并扩展基准测试以涵盖更多实际应用领域。",
      "tags": [
        "Large Language Models",
        "Tokenization",
        "Benchmarking",
        "Sub-token Understanding",
        "Neural Encoding"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:29.792613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09088",
    "title": "Distribution-Aligned Sequence Distillation for Superior Long-CoT Reasoning",
    "authors": [
      "Shaotian Yan",
      "Kaiyuan Liu",
      "Chen Shen",
      "Bing Wang",
      "Sinan Fan",
      "Jun Zhang",
      "Yue Wu",
      "Zheng Wang",
      "Jieping Ye"
    ],
    "abstract": "In this report, we introduce DASD-4B-Thinking, a lightweight yet highly capable, fully open-source reasoning model. It achieves SOTA performance among open-source models of comparable scale across challenging benchmarks in mathematics, scientific reasoning, and code generation -- even outperforming several larger models. We begin by critically reexamining a widely adopted distillation paradigm in the community: SFT on teacher-generated responses, also known as sequence-level distillation. Although a series of recent works following this scheme have demonstrated remarkable efficiency and strong empirical performance, they are primarily grounded in the SFT perspective. Consequently, these approaches focus predominantly on designing heuristic rules for SFT data filtering, while largely overlooking the core principle of distillation itself -- enabling the student model to learn the teacher's full output distribution so as to inherit its generalization capability. Specifically, we identify three critical limitations in current practice: i) Inadequate representation of the teacher's sequence-level distribution; ii) Misalignment between the teacher's output distribution and the student's learning capacity; and iii) Exposure bias arising from teacher-forced training versus autoregressive inference. In summary, these shortcomings reflect a systemic absence of explicit teacher-student interaction throughout the distillation process, leaving the essence of distillation underexploited. To address these issues, we propose several methodological innovations that collectively form an enhanced sequence-level distillation training pipeline. Remarkably, DASD-4B-Thinking obtains competitive results using only 448K training samples -- an order of magnitude fewer than those employed by most existing open-source efforts. To support community research, we publicly release our models and the training dataset.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09088.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09088",
    "published": "2026-01-14T02:43:17Z",
    "updated": "2026-01-14T02:43:17Z",
    "comment": "Project Page: https://github.com/D2I-ai/dasd-thinking",
    "light_analysis": {
      "overview": "本文提出DASD-4B-Thinking模型，通过分布对齐的序列蒸馏方法显著提升长链思维推理能力，实现轻量级开源模型在数学、科学推理和代码生成等基准上的SOTA性能。",
      "motivation": "当前社区广泛采用的蒸馏范式（基于教师生成响应的SFT）虽有效，但主要关注SFT数据过滤的启发式规则，忽视了蒸馏的核心原则：让学生模型学习教师的完整输出分布以继承泛化能力。论文指出三个关键不足：教师序列级分布表示不足、输出分布与学生能力不对齐、以及教师强制训练与自回归推理之间的暴露偏差，导致蒸馏过程缺乏显式师生交互，限制了性能提升，凸显了改进的必要性。",
      "method": "论文提出多个方法创新，构建了一个增强的序列级蒸馏训练管道，旨在解决现有局限性。关键创新包括优化教师序列级分布表示、对齐教师输出分布与学生学习能力，以及减少暴露偏差。该方法专注于分布对齐，通过增强师生交互，让学生模型更好地学习教师的行为，而不仅仅是响应过滤。摘要未明确说明具体模型架构或数据集细节，但强调了管道设计。",
      "result": "DASD-4B-Thinking在数学、科学推理和代码生成等挑战性基准上，在规模相近的开源模型中取得SOTA性能，甚至超越了一些更大的模型。它仅使用448K训练样本，比现有开源方法少一个数量级，显示了高效性。性能对比表明，该方法在减少数据量的同时，仍能维持或超过基线方法的准确性，突显其蒸馏效果的优势。",
      "conclusion": "论文的主要贡献在于改进了序列蒸馏方法，通过分布对齐提升了推理模型的泛化能力和性能。这具有学术价值，深化了对蒸馏原理的理解；实际应用中，提供了高效的轻量级开源模型，支持社区研究。未来工作可能包括进一步优化对齐策略或扩展到更广泛的任务，摘要未明确说明局限性，但隐含了蒸馏方法的进一步探索空间。",
      "tags": [
        "Distillation",
        "Sequence Distillation",
        "Long Chain of Thought Reasoning",
        "Generalization",
        "Open Source Model"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:04.972012Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09085",
    "title": "MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting",
    "authors": [
      "Kangda Wei",
      "Ruihong Huang"
    ],
    "abstract": "Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09085.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09085",
    "published": "2026-01-14T02:35:19Z",
    "updated": "2026-01-14T02:35:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "MMR-GRPO提出了一种基于完成多样性的奖励重加权方法，显著加速GRPO风格训练并保持模型性能。",
      "motivation": "Group Relative Policy Optimization（GRPO）作为训练数学推理模型的标准方法，依赖于每个提示生成多个完成，导致计算成本高昂。尽管近期研究减少了达到峰值性能所需的训练步骤，但每步成本增加使得总体壁钟时间往往不变甚至上升。这突显了现有方法在效率上的不足，亟需解决训练资源浪费问题，以支持大规模模型部署和更高效的AI开发。",
      "method": "MMR-GRPO方法集成了Maximal Marginal Relevance（MMR），通过分析完成语义的多样性来重新加权奖励。核心创新在于识别并优先考虑多样化的解决方案，减少冗余完成带来的有限学习信号，从而提供更具信息量的梯度更新以加速收敛。方法应用于多个GRPO变体，并在不同模型尺寸（如1.5B、7B、8B）和数学推理基准上进行验证，利用强化学习框架优化策略。",
      "result": "实验在三个模型规模（1.5B、7B、8B）、三个GRPO变体和五个数学推理基准上展开。结果显示，MMR-GRPO达到与基线可比较的峰值性能，同时平均减少47.9%的训练步骤和70.2%的壁钟时间。这些改进在不同模型、方法和基准中一致，证明了方法在加速训练和降低资源消耗方面的有效性。",
      "conclusion": "本研究的主要贡献是提出了MMR-GRPO，通过多样性感知奖励重加权有效加速GRPO训练。学术上，它为强化学习效率提升提供了新思路；实际上，有助于降低AI模型开发的计算成本和资源需求。摘要未明确说明局限性，未来工作可能涉及方法在其他任务领域的扩展或进一步优化奖励机制。",
      "tags": [
        "Group Relative Policy Optimization (GRPO)",
        "Maximal Marginal Relevance (MMR)",
        "Reward Reweighting",
        "Mathematical Reasoning",
        "Model Training Acceleration"
      ]
    },
    "analyzed_at": "2026-01-15T03:16:22.874529Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09084",
    "title": "How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation",
    "authors": [
      "Wilson Y. Lee"
    ],
    "abstract": "Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.09084.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09084",
    "published": "2026-01-14T02:34:58Z",
    "updated": "2026-01-14T02:34:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文揭示了人类偏好评估中判断数量的可行性限制，并证明在偏好信号扩散时比例分配是最优策略。",
      "motivation": "人类偏好评估广泛用于比较生成模型，但缺乏确定可靠检测小改进所需判断数量的标准。现有方法常低估必要判断数，导致评估能力不足，可能得出不明确或错误结论，影响模型改进和公平比较。这一问题的重要性在于，不充分的评估会误导AI系统开发，阻碍性能优化和技术进步。",
      "method": "论文通过理论分析和实证验证相结合。理论部分推导出当偏好信号在提示间扩散时，比例分配是最小化最大风险的最优分配策略，没有其他策略能显著提高检测能力。实证方面，分析大规模人类偏好数据集，覆盖聊天、图像生成和代码生成等多模态，验证理论并比较不同评估协议，如策划基准对减少提示级变异性的效果。",
      "result": "实证分析表明，大多数人类偏好比较处于扩散状态，偏好裕度小，需要大量判断才能可靠检测改进，远超常规收集量。相反，精心策划的基准通过减少提示级方差达1.5倍，显著提高检测能力。这表明当前评估实践中判断不足是常见问题，而优化协议设计可有效提升评估效率和可靠性。",
      "conclusion": "论文结论强调，人类偏好评估中的不明确结果常源于评估能力不足而非模型等价。主要贡献在于揭示判断数量的可行性限制，并倡导评估设计时明确考虑效应大小、预算和协议因素。这为学术研究提供理论指导，对实际应用帮助设计更可靠的模型评估方法，未来可扩展至更多模态或改进分配策略以应对挑战。",
      "tags": [
        "Human Preference Evaluation",
        "Generative Models",
        "Proportional Allocation",
        "Minimax-Optimal",
        "Variance Reduction"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:35.948221Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09083",
    "title": "SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache",
    "authors": [
      "Chi-Chih Chang",
      "Siqi Zhu",
      "Zhichen Zeng",
      "Haibin Lin",
      "Jiaxuan You",
      "Mohamed S. Abdelfattah",
      "Ziheng Jiang",
      "Xuehai Qian"
    ],
    "abstract": "We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09083.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09083",
    "published": "2026-01-14T02:34:48Z",
    "updated": "2026-01-14T02:34:48Z",
    "comment": null,
    "light_analysis": {
      "overview": "SRT提出了一种基于树状缓存和推测解码的方法，以加速语言模型中基于策略的强化学习训练，而不牺牲分布正确性。",
      "motivation": "在语言模型的强化学习训练中，传统方法通常面临计算密集和耗时长的挑战，可能导致训练效率低下或影响模型性能的准确性。当前方法在加速训练时往往牺牲分布正确性，从而限制了实际应用。本研究旨在解决这一问题，通过开发高效且无损失的技术来优化训练过程，提升强化学习在语言模型中的实用性。",
      "method": "SRT方法的核心是利用树状缓存存储先前生成提示的延续结果，将其作为推测解码的草案模型。关键创新包括构建每个提示的专属缓存树，以复用相似生成路径；通过在线更新机制从持续训练中维护缓存新鲜度，并在GPU空闲时段执行预运行生成以提高草案质量。该方法模型无关，可集成到多种标准强化学习算法如PPO、GRPO和DAPO中，并适用于多轮交互设置。",
      "result": "实验结果证实，SRT能显著减少生成延迟和训练步骤时间，降低每个令牌的推理成本。具体表现为在挂钟时间上实现高达2.08倍的加速比，这表明SRT在保持分布正确性的同时，优化了计算资源使用。与基线方法相比，SRT在标准强化学习管道中持续提升效率，展示了其在加速训练方面的有效性。",
      "conclusion": "本研究的主要贡献是提出了SRT方法，它通过树状缓存和推测解码的结合，实现了强化学习训练的高效加速而无损正确性，为学术研究提供了新颖的优化途径。在实际应用中，SRT有助于降低语言模型训练成本，促进更快速部署。潜在未来方向可能包括扩展至更多算法场景或进一步优化缓存管理策略。",
      "tags": [
        "Reinforcement Learning",
        "Speculative Decoding",
        "Tree-Structured Cache",
        "Language Models"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:23.010099Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09078",
    "title": "Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking",
    "authors": [
      "Junze Shi",
      "Yang Yu",
      "Jian Shi",
      "Haibo Luo"
    ],
    "abstract": "Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.09078.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09078",
    "published": "2026-01-14T02:22:05Z",
    "updated": "2026-01-14T02:22:05Z",
    "comment": "8 pages, 6 figures",
    "light_analysis": {
      "overview": "STDTrack通过集成可靠时空依赖和密集视频采样，实现了高性能轻量级实时视觉跟踪，弥补了现有方法的性能差距。",
      "motivation": "该研究旨在解决轻量级视觉跟踪中时空信息利用不足的问题。现有基于transformer的跟踪器在训练时采用稀疏采样，仅使用每序列的一个模板和搜索图像，无法全面捕捉视频中的时空依赖，这限制了跟踪性能，导致轻量级与高性能非实时跟踪器之间的显著差距。因此，研究需要在保持实时效率的同时，提升时空信息利用以缩小这一差距。",
      "method": "STDTrack框架引入密集视频采样以最大化时空信息利用，采用时间传播的时空令牌指导每帧特征提取。核心创新包括多帧信息融合模块（MFIFM），通过历史上下文增强当前依赖，并基于时空令牌维护器（STM）存储特征，结合质量更新机制确保信息可靠性。此外，设计了多尺度预测头以动态适应跟踪目标的尺度变化，提升模型泛化能力。",
      "result": "实验表明，STDTrack在六个基准测试中达到了state-of-the-art性能。在GOT-10k数据集上，它与某些高性能非实时跟踪器（如MixFormer）媲美，同时在GPU上运行速度高达192 FPS，CPU上为41 FPS，验证了其高效的实时跟踪能力，显著超越了现有轻量级方法。",
      "conclusion": "STDTrack通过可靠时空依赖集成有效弥合了轻量级与高性能跟踪器的差距，提升了跟踪精度和效率。这项研究为视频分析领域提供了学术创新和实际应用价值，如视频监控和自动驾驶。未来工作可能包括进一步优化时空依赖建模或在更复杂场景中验证模型性能。",
      "tags": [
        "Visual Tracking",
        "Transformer",
        "Spatiotemporal Dependencies",
        "Multi-frame Fusion",
        "Real-time Efficiency"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:20.167559Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.09076",
    "title": "Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated Learning",
    "authors": [
      "Zhoubin Kou",
      "Zihan Chen",
      "Jing Yang",
      "Cong Shen"
    ],
    "abstract": "Split Federated Learning (SFL) enables collaborative training between resource-constrained edge devices and a compute-rich server. Communication overhead is a central issue in SFL and can be mitigated with auxiliary networks. Yet, the fundamental client-side computation challenge remains, as back-propagation requires substantial memory and computation costs, severely limiting the scale of models that edge devices can support. To enable more resource-efficient client computation and reduce the client-server communication, we propose HERON-SFL, a novel hybrid optimization framework that integrates zeroth-order (ZO) optimization for local client training while retaining first-order (FO) optimization on the server. With the assistance of auxiliary networks, ZO updates enable clients to approximate local gradients using perturbed forward-only evaluations per step, eliminating memory-intensive activation caching and avoiding explicit gradient computation in the traditional training process. Leveraging the low effective rank assumption, we theoretically prove that HERON-SFL's convergence rate is independent of model dimensionality, addressing a key scalability concern common to ZO algorithms. Empirically, on ResNet training and language model (LM) fine-tuning tasks, HERON-SFL matches benchmark accuracy while reducing client peak memory by up to 64% and client-side compute cost by up to 33% per step, substantially expanding the range of models that can be trained or adapted on resource-limited devices.",
    "categories": [
      "cs.LG",
      "cs.DC",
      "cs.IT",
      "cs.NI",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.09076.pdf",
    "abs_url": "https://arxiv.org/abs/2601.09076",
    "published": "2026-01-14T02:17:49Z",
    "updated": "2026-01-14T02:17:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出 HERON-SFL，一个混合零阶和一阶优化的 Split Federated Learning 框架，显著降低客户端计算资源需求并保持准确率。",
      "motivation": "Split Federated Learning (SFL) 旨在促进资源受限边缘设备与计算丰富服务器的协作训练，但通信开销和客户端计算挑战是关键问题。尽管辅助网络能缓解通信开销，反向传播仍需大量内存和计算，严重限制了边缘设备支持的模型规模，导致大规模模型难以部署。现有方法主要优化通信，而客户端计算效率不足，因此本研究旨在开发资源高效方法，扩展 SFL 在边缘计算中的实用性。",
      "method": "论文提出 HERON-SFL，一个混合零阶（ZO）和一阶（FO）优化的框架：客户端使用 ZO 优化，通过扰动前向评估近似局部梯度，消除内存密集的激活缓存，避免显式梯度计算；服务器保留 FO 优化。关键创新是结合辅助网络，使 ZO 更新更高效。基于低有效秩假设，理论证明了收敛率独立于模型维度，解决 ZO 算法的可扩展性问题。方法应用于 ResNet 训练和语言模型微调任务。",
      "result": "HERON-SFL 在 ResNet 训练和语言模型微调任务中匹配基准准确率，同时显著减少客户端资源消耗。具体而言，客户端峰值内存降低了高达 64%，每步计算成本减少了高达 33%。与基线方法对比，HERON-SFL 在保持准确率不变的情况下，大大扩展了资源受限设备可训练模型的范围，验证了其高效性和可扩展性。",
      "conclusion": "本研究的主要贡献是提出 HERON-SFL，有效降低 Split Federated Learning 中客户端的计算需求。学术上，通过理论证明收敛率独立于模型维度，解决了零阶优化的可扩展性问题。实际应用中，该框架使资源受限边缘设备能够训练或适应更大模型，提升联邦学习的实用性。未来工作可探索更多优化变体和应用场景（摘要未明确说明具体局限性）。",
      "tags": [
        "Split Federated Learning",
        "Zeroth-Order Optimization",
        "First-Order Optimization",
        "Auxiliary Networks",
        "Low Effective Rank"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:44.616172Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.08026",
    "title": "FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures",
    "authors": [
      "Jifeng Song",
      "Arun Das",
      "Pan Wang",
      "Hui Ji",
      "Kun Zhao",
      "Yufei Huang"
    ],
    "abstract": "Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.08026.pdf",
    "abs_url": "https://arxiv.org/abs/2601.08026",
    "published": "2026-01-12T21:57:52Z",
    "updated": "2026-01-14T15:49:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "FigEx2是一个视觉条件框架，用于定位科学复合图中的面板并生成面板级标题，通过噪声感知门控融合和分阶段优化策略提升性能。",
      "motivation": "科学复合图将多个面板组合成单个图像，但在实际应用中，标题往往缺失或仅提供图级摘要，这导致面板级理解困难，阻碍了科学图像的多模态分析。现有方法可能无法有效处理开放式标题中的措辞多样性和面板定位的精确性问题，需要更稳健的框架来支持高质量的面板级监督和理解。该研究旨在解决这些问题，以促进科学领域中的自动图像分析。",
      "method": "FigEx2提出一个视觉条件框架，通过噪声感知门控融合模块自适应过滤令牌级特征，稳定检测查询空间，减少开放式标题中措辞噪声的影响。采用分阶段优化策略，结合监督学习与强化学习，使用CLIP进行对齐和BERTScore提供语义奖励，强制多模态一致性。为支持监督，策划了BioSci-Fig-Cap基准数据集，涵盖面板级接地，并包含物理和化学的跨学科测试套件，增强模型的泛化能力。",
      "result": "实验结果显示，FigEx2在面板检测方面达到0.726 mAP@0.5:0.95，显著优于基线方法。在标题生成方面，相比Qwen3-VL-8B，METEOR分数提升0.51，BERTScore分数提升0.24。此外，模型展现出优异的零样本迁移性，无需微调即可适应分布外的科学领域，验证了其鲁棒性和泛化能力。这些结果证明了方法在性能和可扩展性上的优势。",
      "conclusion": "该研究的主要贡献是提出FigEx2框架，结合创新模块和优化策略，有效解决了科学复合图的面板检测和标题生成问题。学术价值在于推动了多模态科学图像理解的进展，实际应用可辅助科研人员进行自动化图像分析。摘要未明确说明未来工作方向，但可能包括扩展到更广泛的科学领域或集成更多模态信息。",
      "tags": [
        "Visual-Conditioned Framework",
        "Panel Detection and Captioning",
        "Noise-Aware Gated Fusion",
        "Reinforcement Learning",
        "Zero-shot Transfer"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:23.433213Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.07994",
    "title": "DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs",
    "authors": [
      "Nayoung Choi",
      "Jonathan Zhang",
      "Jinho D. Choi"
    ],
    "abstract": "Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.07994.pdf",
    "abs_url": "https://arxiv.org/abs/2601.07994",
    "published": "2026-01-12T20:47:50Z",
    "updated": "2026-01-14T15:26:22Z",
    "comment": "Accepted (B) to TACL 2026",
    "light_analysis": {
      "overview": "DyCP是一种用于长对话LLMs的动态上下文剪枝方法，通过动态分割和检索内存来提升答案质量并降低响应延迟。",
      "motivation": "大型语言模型在长对话中常面临响应延迟增加和答案质量下降的问题，现有方法如依赖额外LLM调用构建内存或进行离线内存构建，常忽略当前用户话语，导致效率低下或对话连续性中断。这使得开发高效、自适应且考虑对话语境的上下文管理方法变得至关重要，以应对长对话场景中日益增长的上下文长度带来的挑战。",
      "method": "DyCP是一种轻量级的上下文管理方法，在查询时动态分割对话历史并检索相关内存，避免额外LLM调用。其关键创新在于保留对话的时序结构，无需预定义主题边界，支持高效自适应的上下文检索。该方法应用于多种LLMs和基准测试环境，如LoCoMo、MT-Bench+和SCM4LLMs，但具体模型架构和数据集细节摘要未明确说明。",
      "result": "在LoCoMo、MT-Bench+和SCM4LLMs等长对话基准测试及多个LLMs上，DyCP一致地改善了答案质量并减少了响应延迟，具体数据摘要未明确说明，但表现出优于现有方法的效果，显示出其在长上下文管理中的有效性和泛化能力。",
      "conclusion": "DyCP通过动态上下文剪枝显著提升了LLMs在长对话中的性能，强调了有效上下文管理的重要性，不仅增强了对话系统的实用价值，还揭示了现代LLMs长上下文处理能力与实际窗口扩展之间的差距，为未来优化上下文管理提供了研究方向。",
      "tags": [
        "Dynamic Context Pruning",
        "Large Language Models",
        "Long-Form Dialogue",
        "Memory Retrieval",
        "Context Management"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:59.639144Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.07903",
    "title": "Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning",
    "authors": [
      "Jianqi Zhang",
      "Jingyao Wang",
      "Wenwen Qiang",
      "Fanjiang Xu",
      "Changwen Zheng"
    ],
    "abstract": "The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.07903.pdf",
    "abs_url": "https://arxiv.org/abs/2601.07903",
    "published": "2026-01-12T14:55:05Z",
    "updated": "2026-01-14T15:32:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种向量注入的上下文学习方法（LVICL），以增强大型语言模型在时间序列预测中的性能，同时通过冻结模型参数降低计算开销。",
      "motivation": "研究动机源于万维网需要对用户行为和使用模式的变化进行可靠预测，时间序列预测是关键手段。近年来，基于大型语言模型的时间序列预测方法取得进展，但由于预训练语料与时间序列数据存在显著差异，直接应用LLM难以保证预测质量；微调LLM可缓解此问题，但计算开销大。因此，LLM4TSF面临预测性能和计算开销的双重挑战，论文旨在探索在冻结LLM参数的同时提升预测性能的方法，以解决现有方法在效率和准确性上的不足。",
      "method": "研究方法提出LVICL，通过向量注入的上下文学习来改进LLM的时间序列预测能力。具体技术包括使用LLM结合可学习的上下文向量适配器，从多个时间序列示例中自适应提取上下文向量，该向量包含压缩的、与任务相关的信息。然后，在前向传播过程中，将此向量注入到LLM的每一层，以激活模型的上下文学习能力并提升预测性能。关键创新点在于与传统ICL相比，该方法不增加输入提示长度，并自适应提取向量以抑制有害成分，从而优化模型表现。摘要未明确说明使用的具体数据集和模型架构细节。",
      "result": "主要实验结果通过大量实验验证了LVICL方法的有效性。摘要表明该方法在时间序列预测任务上表现出色，相比于传统上下文学习方法，能够在保持计算效率的同时提高预测性能。具体性能指标如准确率提升百分比或效率改进数据在摘要中未明确提供，但实验证实了方法的优越性，展示了其在应对预测准确性和计算开销双重挑战中的潜力。",
      "conclusion": "结论指出，论文的主要贡献是提出了LVICL方法，有效解决了大型语言模型在时间序列预测中面临的预测性能和计算开销问题。该方法通过向量注入技术增强了LLM的上下文学习能力，无需微调模型参数，从而降低了计算成本并提高了预测可靠性。学术上，这为改进LLM在非文本数据中的应用提供了新思路；实际上，可应用于万维网用户行为预测等场景。未来工作可能包括方法的进一步优化和扩展到其他序列预测任务，摘要未明确说明局限性。",
      "tags": [
        "Large Language Models",
        "Time-Series Forecasting",
        "In-Context Learning",
        "Vector Injection",
        "Adaptive Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:01.227416Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06730",
    "title": "Why are there many equally good models? An Anatomy of the Rashomon Effect",
    "authors": [
      "Harsh Parikh"
    ],
    "abstract": "The Rashomon effect -- the existence of multiple, distinct models that achieve nearly equivalent predictive performance -- has emerged as a fundamental phenomenon in modern machine learning and statistics. In this paper, we explore the causes underlying the Rashomon effect, organizing them into three categories: statistical sources arising from finite samples and noise in the data-generating process; structural sources arising from non-convexity of optimization objectives and unobserved variables that create fundamental non-identifiability; and procedural sources arising from limitations of optimization algorithms and deliberate restrictions to suboptimal model classes. We synthesize insights from machine learning, statistics, and optimization literature to provide a unified framework for understanding why the multiplicity of good models arises. A key distinction emerges: statistical multiplicity diminishes with more data, structural multiplicity persists asymptotically and cannot be resolved without different data or additional assumptions, and procedural multiplicity reflects choices made by practitioners. Beyond characterizing causes, we discuss both the challenges and opportunities presented by the Rashomon effect, including implications for inference, interpretability, fairness, and decision-making under uncertainty.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ME",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.06730.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06730",
    "published": "2026-01-11T00:48:15Z",
    "updated": "2026-01-14T04:15:47Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一个统一框架，将Rashomon效应的原因分为统计、结构和程序三类来源，阐明了它们的性质和影响。",
      "motivation": "Rashomon效应指机器学习和统计中存在多个预测性能相似的模型，这一现象对模型选择、推理、公平性和决策等领域提出了挑战。现有研究未能系统分析其根源，导致实践中难以有效处理多模型带来的不确定性。本研究旨在深入探讨该效应的原因，以弥补理论空白，并为模型解释性和应用决策提供更坚实的理论基础，从而提高机器学习系统的可靠性和透明度。",
      "method": "论文通过综合机器学习、统计学和优化理论的文献，提出了一个框架来分析Rashomon效应的原因。核心方法是将原因分为三类：统计来源（如有限样本和数据噪声）、结构来源（如优化目标的非凸性和未观测变量导致的不可识别性）、程序来源（如优化算法限制和实践者选择的子最优模型类）。该框架的关键创新点在于系统性组织这些原因，并区分它们的特性，例如统计多重性随数据增加而减少，结构多重性在渐进情况下持续存在。",
      "result": "论文通过理论分析阐明了Rashomon效应的三类原因及其特征：统计多重性随数据量增加而减弱；结构多重性在渐进情况下持续存在，需不同数据或假设才能解决；程序多重性则反映实践者的选择。这一区分有助于理解模型多样性在推理、解释性和公平性等应用中的影响，但摘要未明确提供具体性能指标或实验对比数据，主要贡献在于框架的建立和理论洞察。",
      "conclusion": "论文总结了Rashomon效应的多原因分类框架，核心贡献在于系统性解释模型多样性现象的根源。研究意义在于为学术探讨提供了统一视角，并应用于模型选择、解释性、公平性和不确定性决策等实际问题。局限性在于框架是理论性的，需要未来实证研究验证或扩展。未来工作可涉及基于此框架的算法改进或在具体数据集上的应用探索，以进一步增强其实际价值。",
      "tags": [
        "Rashomon Effect",
        "Model Multiplicity",
        "Non-convex Optimization",
        "Statistical Analysis",
        "Interpretability"
      ]
    },
    "analyzed_at": "2026-01-15T03:17:51.057744Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06431",
    "title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "authors": [
      "Qingyu Ren",
      "Qianyu He",
      "Jingwen Chang",
      "Jie Zeng",
      "Jiaqing Liang",
      "Yanghua Xiao",
      "Han Xia",
      "Zeye Sun",
      "Fei Yu"
    ],
    "abstract": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.06431.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06431",
    "published": "2026-01-10T05:11:38Z",
    "updated": "2026-01-14T02:51:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "LSRIF 提出一个逻辑结构化强化学习框架，通过显式建模指令逻辑结构来增强大语言模型的指令遵循能力。",
      "motivation": "指令遵循是大语言模型的关键功能，但真实指令常包含逻辑结构如顺序依赖和条件分支，导致现有方法基于并行约束和平均奖励优化时忽略逻辑依赖，产生噪声训练信号和性能瓶颈。这凸显了开发显式处理逻辑结构的框架的重要性，以应对复杂现实指令场景，提升模型的可靠性和适应性。",
      "method": "研究方法首先构建 LSRInstruct 数据集，包含并行、顺序和条件等约束结构类型。然后设计结构感知奖励方法 LSRIF，针对并行结构采用平均聚合，顺序结构使用失败惩罚传播，条件分支实施选择性奖励，通过强化学习显式建模指令逻辑。尽管具体模型架构在摘要中未明确说明，但框架聚焦于优化注意力层更新和逻辑运算符的关注。",
      "result": "实验结果显示，LSRIF 在指令遵循任务（包括域内和域外）和通用推理方面带来显著改进。分析表明，学习显式逻辑结构导致注意力层参数更新，并增强了对约束和逻辑运算符的词级注意力，相比忽略逻辑依赖的基线方法表现出性能优势，但摘要未提供具体准确率或效率数据。",
      "conclusion": "LSRIF 的主要贡献是提出逻辑结构化强化学习框架，提升大语言模型在复杂指令下的遵循能力，学术上引入结构感知奖励机制，实际中增强模型在逻辑推理场景的鲁棒性。未来工作可能扩展到更多逻辑类型或不同模型架构，以进一步优化和验证方法的普适性。",
      "tags": [
        "Logic-Structured Reinforcement Learning",
        "Large Language Model",
        "Instruction Following",
        "Constraint Structures",
        "Attention Mechanism"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:23.493682Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05966",
    "title": "VideoAR: Autoregressive Video Generation via Next-Frame & Scale Prediction",
    "authors": [
      "Longbin Ji",
      "Xiaoxiong Liu",
      "Junyuan Shang",
      "Shuohuan Wang",
      "Yu Sun",
      "Hua Wu",
      "Haifeng Wang"
    ],
    "abstract": "Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05966.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05966",
    "published": "2026-01-09T17:34:59Z",
    "updated": "2026-01-14T14:12:54Z",
    "comment": "Project page: https://ernie-research.github.io/VideoAR/",
    "light_analysis": {
      "overview": "VideoAR 提出了首个结合多尺度下一帧预测与自回归建模的大规模视觉自回归框架，用于高效、可扩展的视频生成。",
      "motivation": "视频生成领域当前主要由扩散模型和流匹配模型主导，这些模型能生成高质量视频，但计算成本高且难以扩展，限制了实际应用部署。这导致了对更高效、可扩展方法的迫切需求，以降低成本并提高效率。VideoAR 旨在填补这一空白，通过自回归框架减少计算负担，同时维持或提升生成质量，解决现有方法的不足。",
      "method": "VideoAR 框架通过集成自回归建模与多尺度下一帧预测，分解空间和时间依赖，使用 3D 多尺度标记器编码时空动态。关键技术包括多尺度时间 RoPE 增强一致性、跨帧错误校正减少误差传播、随机帧掩码稳定训练，并采用多阶段预训练管道逐步对齐学习和扩展分辨率与持续时间。",
      "result": "在 UCF-101 数据集上，VideoAR 将 FVD 从 99.5 显著降低至 88.6，同时推理步骤减少超过 10 倍，效率大幅提升。VBench 评分达 81.74，与规模大一个数量级的基于扩散模型竞争，表明其在自回归模型中达到新最先进水平，并缩小了与扩散范式的性能差距。",
      "conclusion": "VideoAR 的主要贡献是提供了一个高效、可扩展的视频生成框架，通过多尺度预测和错误校正技术改善长期一致性，具有重要学术价值，推动了高效生成模型研究，为视频编辑和合成等应用奠定基础。未来工作可优化模型并扩展到更复杂场景。",
      "tags": [
        "Autoregressive Modeling",
        "Next-Frame Prediction",
        "Multi-scale Temporal RoPE",
        "Cross-Frame Error Correction",
        "Video Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:43.018092Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05699",
    "title": "Afri-MCQA: Multimodal Cultural Question Answering for African Languages",
    "authors": [
      "Atnafu Lambebo Tonja",
      "Srija Anand",
      "Emilio Villa-Cueva",
      "Israel Abebe Azime",
      "Jesujoba Oluwadara Alabi",
      "Muhidin A. Mohamed",
      "Debela Desalegn Yadeta",
      "Negasi Haile Abadi",
      "Abigail Oppong",
      "Nnaemeka Casmir Obiefuna",
      "Idris Abdulmumin",
      "Naome A Etori",
      "Eric Peter Wairagala",
      "Kanda Patrick Tshinu",
      "Imanigirimbabazi Emmanuel",
      "Gabofetswe Malema",
      "Alham Fikri Aji",
      "David Ifeoluwa Adelani",
      "Thamar Solorio"
    ],
    "abstract": "Africa is home to over one-third of the world's languages, yet remains underrepresented in AI research. We introduce Afri-MCQA, the first Multilingual Cultural Question-Answering benchmark covering 7.5k Q&A pairs across 15 African languages from 12 countries. The benchmark offers parallel English-African language Q&A pairs across text and speech modalities and was entirely created by native speakers. Benchmarking large language models (LLMs) on Afri-MCQA shows that open-weight models perform poorly across evaluated cultures, with near-zero accuracy on open-ended VQA when queried in native language or speech. To evaluate linguistic competence, we include control experiments meant to assess this specific aspect separate from cultural knowledge, and we observe significant performance gaps between native languages and English for both text and speech. These findings underscore the need for speech-first approaches, culturally grounded pretraining, and cross-lingual cultural transfer. To support more inclusive multimodal AI development in African languages, we release our Afri-MCQA under academic license or CC BY-NC 4.0 on HuggingFace (https://huggingface.co/datasets/Atnafu/Afri-MCQA)",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.05699.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05699",
    "published": "2026-01-09T10:40:09Z",
    "updated": "2026-01-14T09:03:28Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文引入了首个覆盖15种非洲语言的多模态文化问答基准Afri-MCQA，用于评估大型语言模型在非洲语言和文化上的性能，并揭示了现有模型的严重不足。",
      "motivation": "非洲拥有全球超过三分之一的语言，但人工智能研究中代表性严重不足，导致现有模型在处理非洲语言和文化内容时表现滞后。这一问题的重要性在于，它限制了AI技术在非洲的应用和包容性发展，加剧了数字鸿沟。现有方法如大型语言模型主要针对英语等主流语言，缺乏对非洲语言的多模态（文本和语音）和文化特定任务的评估，因此需要专门的基准来推动相关研究，解决模型在语音查询和文化知识方面的薄弱环节。",
      "method": "论文提出Afri-MCQA基准，包含7.5千个问答对，覆盖15种非洲语言，并整合文本和语音两种模态，所有数据均由母语者创建以确保文化准确性。方法的核心创新在于提供平行英语-非洲语言问答对，并设计控制实验以区分语言能力和文化知识的评估，避免混淆因素。技术路线涉及数据收集、预处理和基准测试过程，使用大型语言模型进行评估，重点关注开放权重模型在文本和语音查询下的表现，为多语言AI研究提供基础。",
      "result": "在Afri-MCQA基准上测试大型语言模型的结果显示，开放权重模型在评估的文化中表现极差，特别是在以母语或语音进行开放式视觉问答查询时，准确率接近零。控制实验进一步表明，在文本和语音模态下，母语和英语之间存在显著的性能差距，突显了模型在非洲语言能力上的不足。与基线方法（如现有LLMs）相比，这些结果揭示了当前AI模型在处理非洲语言和文化内容时的严重缺陷，支持了研究中的关键发现。",
      "conclusion": "论文的主要贡献是推出了首个针对非洲语言的多模态文化问答基准Afri-MCQA，并实证揭示了大型语言模型在该领域的性能瓶颈。研究强调了采用语音优先方法、文化基础预训练和跨语言文化转移的重要性，学术价值在于填补了非洲语言AI研究的空白，推动更公平的AI发展。实际应用价值在于支持多模态AI系统在非洲的部署和优化。未来工作可基于此基准开发改进模型，并扩展更多语言和文化维度。",
      "tags": [
        "Multimodal Question Answering",
        "African Languages",
        "Benchmark Evaluation",
        "Large Language Models",
        "Cross-lingual Cultural Transfer"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:43.274163Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05679",
    "title": "Do Sparse Autoencoders Identify Reasoning Features in Language Models?",
    "authors": [
      "George Ma",
      "Zhongyuan Liang",
      "Irene Y. Chen",
      "Somayeh Sojoudi"
    ],
    "abstract": "We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). We first show through a simple theoretical analysis that $\\ell_1$-regularized SAEs are intrinsically biased toward low-dimensional patterns, providing a mechanistic explanation for why shallow linguistic cues may be preferentially captured over distributed reasoning behaviors. Motivated by this bias, we introduce a falsification-oriented evaluation framework that combines causal token injection and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that features identified by contrastive methods are highly sensitive to token-level interventions, with 45% to 90% activating when a small number of associated tokens are injected into non-reasoning text. For the remaining features, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields no improvements in benchmark performance. Overall, our results suggest that SAE features identified by current contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves. Code is available at https://github.com/GeorgeMLP/reasoning-probing.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.05679.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05679",
    "published": "2026-01-09T09:54:36Z",
    "updated": "2026-01-14T15:46:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文通过理论分析和实验评估，发现稀疏自编码器识别的特征主要捕捉大语言模型中推理的语言相关而非推理计算本身。",
      "motivation": "研究动机是探究稀疏自编码器是否真能识别大语言模型中的推理特征。现有方法可能不足，因为理论分析表明 ℓ₁-正则化稀疏自编码器内在偏向低维模式，优先捕捉浅层语言线索而非分布式推理行为，这影响模型解释性。问题的重要性在于，理解模型内部特征对于提高AI的可解释性和可靠性至关重要，而当前方法可能误导特征提取结果，导致对推理行为的误判。",
      "method": "研究方法包括理论分析和实验验证。首先，进行简单理论分析，揭示 ℓ₁-正则化稀疏自编码器偏向低维模式。接着，引入一个面向 falsification 的评估框架，结合因果令牌注入和LLM引导的 falsification 技术，以测试特征激活是否反映推理过程而非表面语言相关。实验在多个大语言模型家族、不同层和推理数据集上进行，涵盖20个配置，使用对比方法来识别特征，并应用干预措施进行分析。",
      "result": "实验结果显示，对比方法识别的特征对令牌级干预高度敏感：45%到90%的特征会在非推理文本中注入少量相关令牌时激活。对于剩余特征，LLM引导的 falsification 一致产生非推理输入激活特征和推理输入不激活特征，没有任何特征满足真正推理行为的标准。此外，引导这些特征未能改善基准性能指标，表明当前方法提取的特征未有效反映推理计算，与基线相比缺乏实质性改进。",
      "conclusion": "结论表明稀疏自编码器识别的特征主要捕捉推理的语言相关而非推理计算本身，这通过 falsification 框架得到验证。研究贡献在于提供一种新评估方法来测试特征真实性，对模型解释性有学术价值。意义是提示当前特征提取方法需改进，以更准确地识别推理行为；未来工作可探索更鲁棒的特征识别技术或改进稀疏自编码器设计来克服偏差。摘要未明确说明具体局限性，但可推断现有方法在区分语言线索和推理计算上存在挑战。",
      "tags": [
        "Sparse Autoencoders",
        "Large Language Models",
        "Reasoning Features",
        "Causal Token Injection",
        "Falsification Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:58.547839Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.05302",
    "title": "Effects of personality steering on cooperative behavior in Large Language Model agents",
    "authors": [
      "Mizuki Sakai",
      "Mizuki Yokoyama",
      "Wakaba Tateishi",
      "Genki Ichinose"
    ],
    "abstract": "Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality scores of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.05302.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05302",
    "published": "2026-01-08T14:23:45Z",
    "updated": "2026-01-14T12:54:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究通过实验揭示，在大型语言模型代理中，人格引导主要通过宜人性促进合作，并表现为行为偏差而非确定性控制机制。",
      "motivation": "随着大型语言模型（LLMs）在战略和社交互动中作为自主代理的应用日益增多，人格特质对行为的影响成为关注焦点。然而，在控制条件下，人格引导如何具体影响合作行为尚不清楚。这一问题的研究对于优化LLM代理在复杂社交环境中的行为至关重要，现有方法缺乏系统性评估人格引导的效应，因此本研究旨在填补这一空白，探索人格引导对合作行为的影响机制。",
      "method": "本研究采用重复囚徒困境游戏作为实验环境，基于大五人格框架使用Big Five Inventory测量GPT-3.5-turbo、GPT-4o和GPT-5模型的人格分数。通过比较基线和人格引导条件下的行为，并独立操纵每个人格维度到极端值，分析其对合作行为的影响。关键创新在于系统量化人格引导效果，并通过控制实验设计实现多模型对比评估。",
      "result": "实验结果表明，宜人性是促进合作的主导因素，其他性格特质影响有限。明确人格信息增加了合作行为，但也提高了被利用的脆弱性，特别是在早期模型如GPT-3.5-turbo中。相比之下，后期模型如GPT-5表现出更选择性的合作模式。这些发现显示人格引导更倾向于行为偏差而非确定性控制机制。",
      "conclusion": "本研究的主要贡献在于明确了人格引导对LLM代理合作行为的影响机制，突出宜人性的关键作用。这深化了对AI代理社交行为调控的理解，具有重要学术价值。在实际应用中，有助于设计更安全和合作性强的AI系统。局限性在于实验环境局限于囚徒困境，未来工作可扩展到更多社交场景和模型类型。",
      "tags": [
        "Large Language Model",
        "Cooperative Behavior",
        "Personality Steering",
        "Big Five Framework",
        "Repeated Prisoner's Dilemma"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:10.392948Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04809",
    "title": "SCALER:Synthetic Scalable Adaptive Learning Environment for Reasoning",
    "authors": [
      "Caijun Xu",
      "Changyi Xiao",
      "Zhongyuan Peng",
      "Xinrun Wang",
      "Yixin Cao"
    ],
    "abstract": "Reinforcement learning (RL) offers a principled way to enhance the reasoning capabilities of large language models, yet its effectiveness hinges on training signals that remain informative as models evolve. In practice, RL progress often slows when task difficulty becomes poorly aligned with model capability, or when training is dominated by a narrow set of recurring problem patterns. To jointly address these issues, we propose SCALER (Synthetic sCalable Adaptive Learning Environment for Reasoning), a framework that sustains effective learning signals through adaptive environment design. SCALER introduces a scalable synthesis pipeline that converts real-world programming problems into verifiable reasoning environments with controllable difficulty and unbounded instance generation, enabling RL training beyond finite datasets while preserving strong correctness guarantees. Building on this, SCALER further employs an adaptive multi-environment RL strategy that dynamically adjusts instance difficulty and curates the active set of environments to track the model's capability frontier and maintain distributional diversity. This co-adaptation prevents reward sparsity, mitigates overfitting to narrow task patterns, and supports sustained improvement throughout training. Extensive experiments show that SCALER consistently outperforms dataset-based RL baselines across diverse reasoning benchmarks and exhibits more stable, long-horizon training dynamics.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04809.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04809",
    "published": "2026-01-08T10:42:04Z",
    "updated": "2026-01-14T13:43:32Z",
    "comment": "19 pages,5 figures",
    "light_analysis": {
      "overview": "提出SCALER框架，一个合成可扩展的自适应学习环境，通过自适应环境设计维持强化学习训练信号的有效性，从而持续改善推理能力。",
      "motivation": "强化学习在增强大语言模型推理能力时，常因任务难度与模型能力不匹配或训练被狭窄问题模式主导而导致进展放缓。现有基于数据集的强化学习方法容易遇到奖励稀疏和过拟合问题，限制了推理任务的持续改进。因此，开发一种能动态适应模型能力并保持训练环境多样性的方法至关重要，以提升训练效率和稳定性。",
      "method": "SCALER框架包括一个可扩展的合成管道，将真实世界的编程问题转化为可验证的推理环境，支持可控难度和无限实例生成，确保训练不限于有限数据集。在此基础上，采用自适应多环境强化学习策略，动态调整实例难度和策划活动环境集，以跟踪模型的能力前沿并维持分布多样性，防止奖励稀疏性和过拟合，促进持续学习。",
      "result": "在多种推理基准测试中，SCALER始终优于基于数据集的强化学习基线方法，展现出更稳定和长视野的训练动态。这表明SCALER能有效缓解性能波动并支持持续提升，尽管摘要未提供具体数值，但实验证实了其在效果和训练稳定性方面的优势。",
      "conclusion": "SCALER框架的主要贡献在于通过自适应环境设计解决了强化学习训练中的关键瓶颈，支持推理能力的持续改进。该研究具有重要学术价值，为增强大语言模型推理能力提供了新方法，并具有实际应用潜力。未来工作可扩展到更广泛任务或结合其他学习技术。",
      "tags": [
        "Reinforcement Learning",
        "Large Language Models",
        "Adaptive Environments",
        "Synthetic Data Generation",
        "Reasoning"
      ]
    },
    "analyzed_at": "2026-01-15T03:18:55.467665Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.04748",
    "title": "When Single-Agent with Skills Replace Multi-Agent Systems and When They Fail",
    "authors": [
      "Xiaoxiao Li"
    ],
    "abstract": "Multi-agent AI systems have proven effective for complex reasoning. These systems are compounded by specialized agents, which collaborate through explicit communication, but incur substantial computational overhead. A natural question arises: can we achieve similar modularity benefits with a single agent that selects from a library of skills? We explore this question by viewing skills as internalized agent behaviors. From this perspective, a multi-agent system can be compiled into an equivalent single-agent system, trading inter-agent communication for skill selection. Our preliminary experiments suggest this approach can substantially reduce token usage and latency while maintaining competitive accuracy on reasoning benchmarks. However, this efficiency raises a deeper question that has received little attention: how does skill selection scale as libraries grow?   Drawing on principles from cognitive science, we propose that LLM skill selection exhibits bounded capacity analogous to human decision-making. We investigate the scaling behavior of skill selection and observe a striking pattern. Rather than degrading gradually, selection accuracy remains stable up to a critical library size, then drops sharply, indicating a phase transition reminiscent of capacity limits in human cognition. Furthermore, we find evidence that semantic confusability among similar skills, rather than library size alone, plays a central role in this degradation. This perspective suggests that hierarchical organization, which has long helped humans manage complex choices, may similarly benefit AI systems. Our initial results with hierarchical routing support this hypothesis. This work opens new questions about the fundamental limits of semantic-based skill selection in LLMs and offers a cognitive-grounded framework and practical guidelines for designing scalable skill-based agents.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.04748.pdf",
    "abs_url": "https://arxiv.org/abs/2601.04748",
    "published": "2026-01-08T09:14:26Z",
    "updated": "2026-01-14T07:18:10Z",
    "comment": "25 pages, technical report",
    "light_analysis": {
      "overview": "本研究提出将多智能体系统编译为单智能体技能库系统，并揭示其基于语义混淆的容量限制。",
      "motivation": "多智能体AI系统在复杂推理中有效，但由专门代理组成并通过明确通信协作，导致大量计算开销。本研究旨在探索是否能用单智能体选择技能库中的技能来实现类似模块化优势，以减少开销。现有方法中，技能选择的可扩展性问题未得到充分关注，特别是在技能库增大时性能可能下降，因此研究此问题具有重要意义。",
      "method": "论文提出将技能视为内部化的代理行为，从而将多智能体系统编译为等价的单智能体技能库系统，用技能选择替代代理间通信。基于认知科学原理，分析大型语言模型（LLM）技能选择的容量限制，类比人类决策的有界能力。研究技能库大小和语义混淆对选择准确性的影响，并初步实验分层路由方法来验证假设。",
      "result": "初步实验显示，单智能体技能库系统能显著减少令牌使用和延迟，同时在推理基准上保持竞争性准确性。进一步研究表明，技能选择准确性在技能库达到临界大小前保持稳定，然后急剧下降，呈现类似人类认知的相变模式。语义混淆，而非单纯库大小，是性能退化的关键因素。分层路由的初步结果支持了改善可扩展性的假设。",
      "conclusion": "本研究的主要贡献是提出编译多智能体系统为单智能体技能库的方法，并基于认知科学框架揭示技能选择的容量限制。学术价值在于为AI系统设计提供了新的视角，强调语义混淆在技能选择中的核心作用。实际应用价值包括提供设计可扩展技能代理的实用指南。未来工作可进一步探索分层组织等方法以克服容量限制。",
      "tags": [
        "Multi-agent Systems",
        "Skill-based Agents",
        "Large Language Model",
        "Skill Selection",
        "Hierarchical Routing"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:16.027624Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.03727",
    "title": "Stuttering-Aware Automatic Speech Recognition for Indonesian Language",
    "authors": [
      "Fadhil Muhammad",
      "Alwin Djuliansah",
      "Adrian Aryaputra Hamzah",
      "Kurniawati Azizah"
    ],
    "abstract": "Automatic speech recognition systems have achieved remarkable performance on fluent speech but continue to degrade significantly when processing stuttered speech, a limitation that is particularly acute for low-resource languages like Indonesian where specialized datasets are virtually non-existent. To overcome this scarcity, we propose a data augmentation framework that generates synthetic stuttered audio by injecting repetitions and prolongations into fluent text through a combination of rule-based transformations and large language models followed by text-to-speech synthesis. We apply this synthetic data to fine-tune a pre-trained Indonesian Whisper model using transfer learning, enabling the architecture to adapt to dysfluent acoustic patterns without requiring large-scale real-world recordings. Our experiments demonstrate that this targeted synthetic exposure consistently reduces recognition errors on stuttered speech while maintaining performance on fluent segments, validating the utility of synthetic data pipelines for developing more inclusive speech technologies in under-represented languages.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.03727.pdf",
    "abs_url": "https://arxiv.org/abs/2601.03727",
    "published": "2026-01-07T09:21:12Z",
    "updated": "2026-01-14T07:30:02Z",
    "comment": "Preprint",
    "light_analysis": {
      "overview": "论文提出了一种结合规则变换和大型语言模型的数据增强框架，生成合成口吃音频以微调预训练的印尼语 Whisper 模型，改善低资源语言中的口吃语音识别。",
      "motivation": "自动语音识别（ASR）系统在流畅语音上表现优异，但在处理口吃语音时性能显著下降，这尤其对于印尼语等低资源语言构成挑战，因为专门的数据集几乎不存在。该研究旨在解决这一实际问题，强调现有方法因缺乏真实口吃数据而导致识别错误增多，影响了语音技术的包容性和可访问性，特别是在资源有限的语言环境中。通过生成合成数据，研究试图弥补数据稀缺，推动更具适应性的 ASR 发展。",
      "method": "研究提出一个数据增强框架，通过结合基于规则的变换和大型语言模型，将重复和延长注入流畅文本，然后使用文本到语音合成生成合成口吃音频。这些合成数据用于通过迁移学习微调预训练的印尼语 Whisper 模型，使模型能够适应不流利的声学模式，而无需依赖大规模真实录音。关键创新在于利用合成数据应对数据稀缺，并融合多种技术手段来模拟口吃特征。",
      "result": "实验表明，通过合成数据暴露，模型在口吃语音上的识别错误得到持续减少，同时保持了流畅片段的性能，验证了方法的有效性。摘要未明确说明具体性能指标（如准确率提升），但与未使用合成数据的基线相比，目标暴露改善了模型适应性，证明了合成数据管道在提升识别精度方面的潜力，尤其在低资源语言背景下。",
      "conclusion": "论文验证了合成数据管道在开发包容性语音技术方面的学术和实际价值，特别是在印尼语等低资源语言中。主要贡献是提供了一种克服数据稀缺的方法，通过合成数据增强 ASR 模型的鲁棒性，促进更具可访问性的语音技术。未来工作可能包括扩展到更多语言或更复杂的口吃类型，以进一步提升泛化能力。",
      "tags": [
        "Automatic Speech Recognition",
        "Data Augmentation",
        "Large Language Model",
        "Transfer Learning",
        "Text-to-Speech Synthesis"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:04.508060Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02316",
    "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
    "authors": [
      "DatologyAI",
      ":",
      "Siddharth Joshi",
      "Haoli Yin",
      "Rishabh Adiga",
      "Ricardo Monti",
      "Aldo Carranza",
      "Alex Fang",
      "Alvin Deng",
      "Amro Abbas",
      "Brett Larsen",
      "Cody Blakeney",
      "Darren Teh",
      "David Schwab",
      "Fan Pan",
      "Haakon Mongstad",
      "Jack Urbanek",
      "Jason Lee",
      "Jason Telanoff",
      "Josh Wills",
      "Kaleigh Mentzer",
      "Luke Merrick",
      "Parth Doshi",
      "Paul Burstein",
      "Pratyush Maini",
      "Scott Loftin",
      "Spandan Das",
      "Tony Jiang",
      "Vineeth Dorna",
      "Zhengping Wang",
      "Bogdan Gaza",
      "Ari Morcos",
      "Matthew Leavitt"
    ],
    "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.02316.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02316",
    "published": "2026-01-05T18:07:51Z",
    "updated": "2026-01-14T06:10:23Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了DatBench，通过转换和过滤现有基准来提升视觉-语言模型评估的忠实性、辨别性和效率。",
      "motivation": "当前视觉-语言模型评估方法尚不成熟，存在多项选择格式导致猜测、无法反映下游用例、易饱和；盲目可解问题占比较高，可达70%；样本误标或模糊高达42%，这些问题误判模型能力。此外，评估计算负担大，消耗近20%的开发资源，阻碍了客观比较和可持续研究。这些不足凸显了改进评估实践的重要性，以确保评估更准确、高效。",
      "method": "论文的核心方法包括转换多项选择问题为生成任务，以减少猜测并更好地评估模型能力；同时过滤盲目可解和误标样本，提高数据质量。通过优化33个现有数据集，构建了DatBench-Full清理套件和DatBench高效子集，涵盖九种VLM能力，旨在最大化忠实性和辨别性，同时提升计算效率。",
      "result": "实验结果表明，将多项选择转换为生成任务后，模型能力下降高达35%，更真实反映其局限性；过滤样本后，辨别性提高，计算成本降低。DatBench子集实现了平均13倍加速（最高50倍），同时接近原始数据集的辨别能力，验证了方法在提高评估准确性和效率方面的有效性。",
      "conclusion": "论文贡献了更严格和可持续的VLM评估框架，通过改进基准提升了忠实性和效率，支持模型规模的扩展。该方法为评估实践提供了新路径，具有学术价值和实际应用潜力，未来工作可扩展至更多模态或应用场景。",
      "tags": [
        "Vision-Language Models",
        "Evaluation Benchmark",
        "Generative Tasks",
        "Discriminative Evaluation",
        "Efficiency Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:18.922008Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02307",
    "title": "Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck",
    "authors": [
      "Dina El Zein",
      "James Henderson"
    ],
    "abstract": "We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy (DP) approach, integrating a nonparametric variational information bottleneck (NVIB) layer into the transformer architecture to inject noise into its multivector embeddings and thereby hide information, and measuring privacy protection with Rényi Divergence (RD) and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to the utility of the downstream task. We test NVDP on the General Language Understanding Evaluation (GLUE) benchmark and show that varying the noise level gives us a useful trade-off between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.02307.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02307",
    "published": "2026-01-05T17:49:39Z",
    "updated": "2026-01-14T15:01:54Z",
    "comment": "11 pages, 2 figures",
    "light_analysis": {
      "overview": "提出一种结合非参数变分信息瓶颈的差分隐私方法，用于保护transformer嵌入中的文本隐私。",
      "motivation": "深度模型的隐藏表示可能编码输入数据的敏感信息，导致隐私泄露风险。在transformer嵌入中，由于包含多个向量（每个标记一个向量），隐私风险加剧，攻击者可能高精度恢复原始数据。这个问题在数据共享场景中尤为重要，现有方法在平衡隐私保护和数据效用方面存在不足，因此需要开发更有效的方法来确保强隐私保护的同时维持数据实用性。",
      "method": "论文提出NVDP方法，通过将非参数变分信息瓶颈（NVIB）层集成到transformer架构中，向多向量嵌入注入噪声以隐藏信息。使用Rényi Divergence（RD）和相应的贝叶斯差分隐私（BDP）保证来测量隐私保护水平。NVIB层的训练根据下游任务效用校准噪声水平，实现隐私与准确性之间的自适应平衡。关键创新点在于结合NVIB和DP，提供一种灵活且可靠的隐私保护机制。",
      "result": "在GLUE基准测试中，NVDP方法通过调整噪声水平展示了隐私保护与模型准确性之间的有效权衡。当噪声水平较低时，模型在保持高准确性的同时提供强隐私保证，实现隐私和效用的良好平衡。摘要未明确说明具体准确率提升数值，但表明该方法优于基线方法在隐私保护方面的性能。与现有方法相比，NVDP在保护隐私的同时维持了较好的下游任务性能。",
      "conclusion": "论文的主要贡献是提出了NVDP方法，整合非参数变分信息瓶颈和差分隐私，为transformer嵌入提供了有效的隐私保护。该研究在学术上推进了隐私保护技术，具有实际应用价值，适用于需要共享文本数据但保护用户隐私的自然语言处理任务。摘要未明确说明局限性或未来工作方向，但暗示了在隐私-效用权衡方面的进一步优化潜力。",
      "tags": [
        "Differential Privacy",
        "Transformer Embeddings",
        "Nonparametric Variational Information Bottleneck",
        "Rényi Divergence",
        "Bayesian Differential Privacy"
      ]
    },
    "analyzed_at": "2026-01-15T03:19:34.797195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.02094",
    "title": "Horizon Activation Mapping for Neural Networks in Time Series Forecasting",
    "authors": [
      "Hans Krupakar",
      "V A Kandappan"
    ],
    "abstract": "Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, architectural choices, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.",
    "categories": [
      "cs.LG",
      "math.FA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.02094.pdf",
    "abs_url": "https://arxiv.org/abs/2601.02094",
    "published": "2026-01-05T13:21:30Z",
    "updated": "2026-01-14T13:10:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出 Horizon Activation Mapping (HAM)，一种用于时间序列预测神经网络的视觉可解释性技术，灵感来自 grad-CAM，旨在实现跨模型家族的解释。",
      "motivation": "现有时间序列预测模型的解释和选择依赖于误差度量和特定架构的可解释性方法，但这些方法无法通用地应用于不同模型家族，如基于 MLP 或自注意力的模型，限制了跨模型比较和优化。这导致在模型选择和验证中缺乏统一的解释框架，因此需要一种与模型类型无关的视觉技术来增强可解释性和决策支持。",
      "method": "HAM 是一种基于梯度的视觉解释技术，受 grad-CAM 启发，通过计算每个时间步的子序列梯度范数平均来生成激活图，用于分析时间序列预测模型。关键创新包括引入因果和反因果模式来处理时间序列的方向性，以及研究批量大小、早期停止、训练-验证-测试分割等优化因素对激活图的影响。在实验中，使用了 ETTm2 数据集和多种模型架构，如基于 MLP 的 CycleNet、自注意力的 FEDformer、SSM 的 SpaceTime 和扩散模型的 Multi-Resolution DDPM。",
      "result": "通过应用 HAM 于不同模型，观察到在 ETTm2 数据集上的训练趋势，例如 NHITS 的神经逼近定理和 SpaceTime 的指数自回归活动在 HAM 图中得到体现，这表明 HAM 能捕捉模型特定行为。批量大小差异可能暗示存在指数近似关系，但摘要未明确说明具体性能指标如准确率提升。HAM 图提供了训练、验证和测试集上的视觉洞察，支持跨模型家族的比较和分析。",
      "conclusion": "HAM 提供了一种通用的视觉解释框架，可用于细粒度模型选择、验证集决策和跨神经网络模型家族的比较，增强了时间序列预测模型的可解释性和实用性。这项研究具有学术价值，为模型优化和评估提供了新工具，并可能扩展到更多数据集和应用场景。未来工作可包括验证更多模型类型和探索 HAM 在实时预测中的应用。",
      "tags": [
        "Horizon Activation Mapping",
        "Time Series Forecasting",
        "Neural Networks",
        "Gradient-based Methods",
        "Visual Interpretability"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:09.941924Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.01295",
    "title": "Sobolev Approximation of Deep ReLU Networks in Log-Barron Space",
    "authors": [
      "Changhoon Song",
      "Seungchan Ko",
      "Youngjoon Hong"
    ],
    "abstract": "Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \\le 1/2$. In this paper, we introduce a log-weighted Barron space $\\mathscr{B}^{\\log}$, which requires a strictly weaker assumption than $\\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\\mathscr{B}^{\\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\\mathscr{B}^{s,\\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.",
    "categories": [
      "cs.LG",
      "math.NA"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.01295.pdf",
    "abs_url": "https://arxiv.org/abs/2601.01295",
    "published": "2026-01-03T22:03:19Z",
    "updated": "2026-01-14T15:01:03Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入对数加权的Barron空间，并证明深层ReLU网络在该空间中能有效近似函数，降低了正则性要求，为深层架构性能提供更精确理论解释。",
      "motivation": "本研究的实际问题是解释深层神经网络在高维数据上的高效表示能力。现有通用近似定理表明网络能逼近任何连续函数，但参数数量可能随维度指数增长，无法完全解释实践成功。Barron空间理论通过两阶段网络提供O(n^{-1/2})近似误差，但经典Barron空间需比Sobolev空间更强的正则性，且现有深度敏感结果常假设约束如sL ≤ 1/2，限制了理论解释力。因此，本研究旨在开发更弱假设来更准确地阐明深度架构的优越性，有助于理解深度学习模型在处理高维问题时的稳定性和效率。",
      "method": "论文提出一个新型对数加权的Barron空间B^{log}，其正则性要求比经典Barron空间更弱。方法包括：首先研究该空间的嵌入性质，并通过Rademacher复杂度进行统计分析以评估其统计特性；然后证明B^{log}中的函数可由深层ReLU网络以显式深度依赖的方式近似，展示网络参数如何随深度变化而优化。接着，定义B^{s,log}函数族，建立H^1范数下的近似界限，并识别保持这些速率的极大深度尺度。关键创新点包括对数加权技术、深度敏感的理论分析以及与传统Sobolev空间的对比，未明确指定具体数据集或模型架构，但强调理论推导和函数空间分析。",
      "result": "本研究的主要结果是理论证明，表明在B^{log}空间中，深层ReLU网络能实现高效的函数近似，并具有显式深度依赖性。通过建立H^1范数下的近似界限，识别出最大深度尺度，确保近似率在这些尺度下得以维持。与经典Barron空间相比，新空间允许更弱的正则性假设，从而更精确地解释了深度架构在实践中的性能提升，例如降低了对函数光滑度的要求，但摘要未明确提供具体实验数据如准确率或效率数值，仅通过理论分析展示了深度如何有效减少正则性需求并提升表示能力。",
      "conclusion": "论文的主要贡献是引入对数加权的Barron空间，并建立了深层ReLU网络在该空间中的近似理论，阐明了深度如何降低对函数正则性的要求。学术价值在于提供更精确的理论框架来解释深度架构在高维问题中的高效性能，深化了对神经网络表示能力的理解；实际应用价值在于为高维数据处理和深度学习模型设计提供了理论指导，促进其在复杂任务中的稳定使用。局限性可能包括理论分析需实证验证或扩展到更广泛函数空间，未来工作方向可涉及与其他函数空间的比较或实际应用的测试，以进一步验证理论结果。",
      "tags": [
        "Barron Space",
        "ReLU Networks",
        "Sobolev Approximation",
        "Rademacher Complexity",
        "Depth-dependent Bounds"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:07.667996Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00097",
    "title": "The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs",
    "authors": [
      "Akash Kumar Panda",
      "Olaoluwa Adigun",
      "Bart Kosko"
    ],
    "abstract": "We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.00097.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00097",
    "published": "2025-12-31T20:06:48Z",
    "updated": "2026-01-14T06:26:14Z",
    "comment": "15 figures",
    "light_analysis": {
      "overview": "论文提出了一种基于大型语言模型（LLM）代理的因果反馈模糊认知图（FCMs）提取方法，通过三步指令实现动力系统的准自主性。",
      "motivation": "研究旨在自动从文本中提取因果反馈模糊认知图（FCMs），这对于建模复杂系统的因果动力学至关重要。现有方法可能依赖人工提取，效率低且主观，而自动化技术难以处理模糊和非线性因果关系，因此开发基于LLM的智能代理方法，可提高提取效率和准确性，具有重要理论和应用价值。",
      "method": "论文设计了一个LLM代理，通过三个精细调优的系统指令步骤提取FCMs：首先从文本提取关键名词和名词短语，然后识别FCM概念节点，最后推断节点间的部分或模糊因果边。关键创新是双向过程：LLM的半自主行为驱动文本获取，而FCM动力系统的均衡（如极限环和固定点吸引子）指导代理提取，形成自适应因果结构。测试基于一篇关于AI前景的文章，使用Gemini和ChatGPT LLM代理进行生成和混合。",
      "result": "实验显示，三步过程生成的FCM动力系统收敛到与人类生成FCM相同的均衡极限环，尽管人类FCM的节点和边数量不同，表明提取的一致性。混合Gemini和ChatGPT代理生成的FCMs后，混合FCM吸收主导组件的均衡，并创建新均衡，更好地近似底层因果动力系统。与基线（人类生成FCM）对比验证了方法的有效性。",
      "conclusion": "论文的主要贡献是开发了一个基于LLM的FCM提取框架，结合动力系统理论和智能代理技术，实现准自主文本分析。学术价值在于为因果建模提供了新颖自动化方法，扩展LLM在复杂系统分析中的应用。实际应用潜力包括政策分析和社会科学研究等领域。未来工作可能涉及更多数据集验证和性能优化。",
      "tags": [
        "Large Language Model (LLM)",
        "Fuzzy Cognitive Maps (FCM)",
        "Causal Feedback",
        "Equilibrium Limit Cycles",
        "Agentic Systems"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:28.640259Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.24952",
    "title": "Beyond the Last Frame: Process-aware Evaluation for Generative Video Reasoning",
    "authors": [
      "Yifan Li",
      "Yukai Gu",
      "Yingqian Min",
      "Zikang Liu",
      "Yifan Du",
      "Kun Zhou",
      "Min Yang",
      "Wayne Xin Zhao",
      "Minghui Qiu"
    ],
    "abstract": "Recent breakthroughs in video generation have demonstrated an emerging capability termed Chain-of-Frames (CoF) reasoning, where models resolve complex tasks through the generation of continuous frames. While these models show promise for Generative Video Reasoning (GVR), existing evaluation frameworks often rely on single-frame assessments, which can lead to outcome-hacking, where a model reaches a correct conclusion through an erroneous process. To address this, we propose a process-aware evaluation paradigm. We introduce VIPER, a comprehensive benchmark spanning 16 tasks across temporal, structural, symbolic, spatial, physics, and planning reasoning. Furthermore, we propose Process-outcome Consistency (POC@r), a new metric that utilizes VLM-as-Judge with a hierarchical rubric to evaluate both the validity of the intermediate steps and the final result. Our experiments reveal that state-of-the-art video models achieve POC@1.0 only about 20% and exhibit a significant outcome-hacking. We further explore the impact of test-time scaling and sampling robustness, highlighting a substantial gap between current video generation and true generalized visual reasoning. Our benchmark are released at https://github.com/RUCAIBox/VIPER.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.24952.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24952",
    "published": "2025-12-31T16:31:59Z",
    "updated": "2026-01-14T06:32:56Z",
    "comment": "Work in progress",
    "light_analysis": {
      "overview": "本文提出过程感知评估范式和VIPER基准，以解决生成性视频推理中现有评估框架的不足，通过POC@r度量标准评估模型过程有效性。",
      "motivation": "近期视频生成的突破展示了生成性视频推理的潜力，特别是帧链推理能力。然而，现有评估框架主要依赖单帧评估，这可能导致结果篡改，即模型通过错误过程达到正确结论，从而无法准确反映模型的实际推理能力。这一问题的重要性在于它阻碍了生成性视频推理的可靠发展和实际应用，因为不完善的评估方法可能误导模型优化方向，限制技术进步。因此，亟需新的评估方法来评估中间步骤的有效性，确保模型推理过程的正确性。",
      "method": "论文提出过程感知评估范式，以评估生成性视频推理。具体方法包括引入VIPER基准，涵盖16个任务，覆盖时间、结构、符号、空间、物理和规划推理等多个领域。关键创新点是提出了过程-结果一致性度量标准POC@r，利用VLM-as-Judge方法配合层次化评分标准，评估中间步骤和最终结果的一致性，从而检测结果篡改。这避免了传统单帧评估的局限性，提供了更全面的评估框架，但摘要未明确说明具体模型架构或数据集细节。",
      "result": "实验结果显示，将VIPER基准和POC@r度量应用于最先进的视频模型后，模型仅在大约20%的情况下达到POC@1.0，表明过程有效性较低。这表明模型普遍存在显著的结果篡改现象，即经常通过错误过程获得正确结论，突显了现有方法的缺陷。此外，研究探讨了测试时间缩放和采样稳健性的影响，进一步揭示当前视频生成与真正泛化视觉推理之间存在巨大差距，为未来改进提供了方向，但摘要未提供具体性能指标的详细对比数据。",
      "conclusion": "论文的主要贡献是提出了过程感知评估范式和VIPER基准，改进了生成性视频推理的评估方法，通过POC@r度量标准确保评估过程与结果的一致性。学术上，这提供了更准确的评估标准，有助于推动生成性视频推理领域的研究进展；实际应用中，可提高模型可靠性，促进在复杂任务中的部署。局限性在于当前视频模型仍与泛化视觉推理存在差距，暗示未来工作可探索模型优化和评估方法的扩展。基准已公开，以促进社区使用和进一步研究。",
      "tags": [
        "Generative Video Reasoning",
        "Chain-of-Frames",
        "Process-aware Evaluation",
        "VLM-as-Judge",
        "Process-outcome Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:14.599826Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.23903",
    "title": "Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale",
    "authors": [
      "Charith Wickrema",
      "Eliza Mace",
      "Hunter Brown",
      "Heidys Cabrera",
      "Nick Krall",
      "Matthew O'Neill",
      "Shivangi Sarkar",
      "Lowell Weissman",
      "Eric Hughes",
      "Guido Zarrella"
    ],
    "abstract": "We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern multimodal machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and MITRE's Federal AI Sandbox, we train progressively larger vision transformer (ViT) backbones, report successes and failure modes observed at peta-scale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data-limited regime rather than a model parameter-limited one. These practical insights are intended to inform data collection strategies, compute budgets, and optimization schedules that advance the future development of frontier scale RS foundation models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.23903.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23903",
    "published": "2025-12-29T23:53:11Z",
    "updated": "2026-01-14T02:28:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究探索了遥感基础模型在peta-scale下的缩放行为，揭示性能主要受数据限制而非模型参数，为未来发展提供实用见解。",
      "motivation": "现代多模态机器学习应用，如图像生成、搜索和推理的生成式AI系统，依赖于领域专业化的编码器。在自然图像领域，丰富的互联网数据和成熟缩放定律支持模型优化；但在高价值遥感领域，这些关系理解不足，数据稀缺且规模巨大，现有方法难以有效训练基础模型。本研究旨在填补这一空白，探索在peta-scale规模下遥感数据的缩放技术，以克服数据限制，提升AI应用性能。",
      "method": "研究方法采用逐步增大的视觉变换器骨干网络，利用超过一千万亿像素的商业卫星电光数据和MITRE联邦AI Sandbox平台进行训练。关键创新包括评估不同规模模型与数据集的缩放行为，分析训练过程中的成功和失败模式，以探索模型容量与数据大小之间的权衡。这为遥感基础模型提供了技术路线，并帮助弥合领域差距。",
      "result": "实验结果表明，即使在peta-scale规模下，遥感基础模型的性能仍处于数据受限状态，而非模型参数受限。虽然没有提供具体性能数字，但与现有方法对比，这强调了数据量对模型效果的关键影响。研究发现为缩放行为提供了关键洞察，指导优化数据收集策略以改进性能，如计算预算和训练计划。",
      "conclusion": "本研究的结论是遥感基础模型在peta-scale下主要受数据限制，这为数据收集、计算预算和优化计划提供了实用指导。贡献在于揭示了缩放行为的实用见解，推动遥感AI技术的未来发展。局限性可能在于数据覆盖范围，未来工作可扩展更多模态和增强数据多样性，以进一步弥合领域差距。",
      "tags": [
        "Remote Sensing",
        "Foundation Models",
        "Vision Transformer (ViT)",
        "Scaling Laws",
        "Electro-Optical Data"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:18.849769Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.22991",
    "title": "Fusion or Confusion? Multimodal Complexity Is Not All You Need",
    "authors": [
      "Tillmann Rheude",
      "Roland Eils",
      "Benjamin Wild"
    ],
    "abstract": "Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions. We evaluate them across nine diverse datasets with up to 23 modalities, and test their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analyses show that complex methods perform on par with SimBaMM and often fail to consistently outperform well-tuned unimodal baselines, especially in small-data settings. To support our findings, we include a case study highlighting common methodological shortcomings in the literature followed by a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.22991.pdf",
    "abs_url": "https://arxiv.org/abs/2512.22991",
    "published": "2025-12-28T16:20:36Z",
    "updated": "2026-01-14T13:38:18Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文挑战多模态学习中架构复杂性的假设，提出简单基线SimBaMM并证明其与复杂方法相当，主张研究焦点转向方法严谨性。",
      "motivation": "研究动机源于多模态学习领域常假设复杂架构能提升性能，但这种假设缺乏实证支持，且现有方法可能存在方法学缺陷，如评估标准不一和过度设计。本研究旨在通过大规模实证研究验证复杂性是否必要，解决领域内模型比较的不一致性问题，促进更可靠的评估实践。多模态学习在跨领域应用中日益重要，如计算机视觉与自然语言处理的结合，方法学严谨性对实际部署和学术进展至关重要。摘要未明确说明具体应用案例，但强调了标准化评估的背景需求。",
      "method": "研究方法包括提出简单基线SimBaMM，这是一种后融合Transformer架构，利用Transformer处理多模态数据进行融合。通过重新实现19种高影响力方法，在标准化实验条件下进行评估，涉及9个数据集，最多23种模态，并测试模型在任务泛化和模态缺失情况下的表现。关键创新在于严格的超参数调优和统一评估框架，确保可比性。此外，研究还包括一个案例研究，揭示常见方法学缺陷，并提出可靠性检查表以支持未来研究的方法学改进。",
      "result": "主要实验结果显示，在标准化条件和严格调优下，复杂架构未能可靠地优于SimBaMM。统计分析表明，复杂方法与SimBaMM表现相当，且常无法稳定超越调优的单模态基线，特别是在小数据场景中。这些结果挑战了复杂架构必然提升性能的假设，强调了方法学严谨的重要性。研究还通过案例研究突出常见方法学问题，并提供了可靠性检查表，以促进更稳健和可比较的评估。摘要未提供具体数值数据，但基于实验结果支持方法学改进的必要性。",
      "conclusion": "结论是本研究质疑多模态学习中架构复杂性的必要性，显示简单基线SimBaMM能与复杂方法匹敌，推动领域焦点从追求架构创新转向方法学严谨性。其主要贡献在于提升评估的可比性和可靠性，学术价值在于挑战现有研究范式，实际应用价值是为模型设计和评估提供更实用的框架。潜在局限性未明确说明，但未来工作可扩展SimBaMM或在不同应用场景验证，以加强方法学实践的推广。",
      "tags": [
        "Multimodal Learning",
        "Transformer",
        "Late Fusion",
        "Empirical Study",
        "Methodological Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:25.011359Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.20975",
    "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
    "authors": [
      "Yujin Roh",
      "Inho Jake Park",
      "Chigon Hwang"
    ],
    "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.20975.pdf",
    "abs_url": "https://arxiv.org/abs/2512.20975",
    "published": "2025-12-24T06:04:58Z",
    "updated": "2026-01-14T14:06:43Z",
    "comment": "33 pages, 27figures",
    "light_analysis": {
      "overview": "SPOT是一种无需训练、基于地图引导的大语言模型代理，能够在多CCTV环境的盲点中持续跟踪车辆，提高轨迹连续性。",
      "motivation": "该研究旨在解决多CCTV车辆跟踪系统中盲点导致的轨迹不连续问题。由于CCTV间隔和有限视场，现有方法在盲点区域容易出现对象ID切换和轨迹丢失，降低实时路径预测的可靠性，影响监控系统的实用性和准确性。因此，开发一种无需先验训练、能有效处理盲点的跟踪方法成为关键研究动机。",
      "method": "SPOT方法首先将道路结构和CCTV放置信息编码为基于2D空间坐标的文档，并通过分块技术组织以支持实时查询和推理。然后，利用CCTV图像中对象的相对位置和视场信息将车辆位置转换到实际世界坐标系。最后，结合地图空间信息及车辆的移动方向、速度和驾驶模式，在交叉口级别进行光束搜索，推导出车辆在盲点后最可能进入的下一个CCTV候选位置，从而实现无监督的连续跟踪。",
      "result": "实验基于CARLA模拟器在虚拟城市环境中进行，结果显示SPOT能够在盲点区域准确预测车辆下一个出现的CCTV位置，相比现有技术，更有效地维持连续车辆轨迹，减少了ID切换和轨迹损失。这证实了该方法在多CCTV环境中的跟踪性能，提高了实时路径预测的可靠性，摘要未提供具体数据指标，但强调了效果的改进。",
      "conclusion": "论文提出SPOT方法，通过地图引导和LLM代理实现无监督的多CCTV车辆跟踪，有效解决了盲点导致的轨迹中断问题。该研究在学术上展示了LLM与空间信息结合的应用潜力，实际中可提升监控系统的跟踪能力。未来工作可扩展到更复杂的交通场景或扩展到其他动态对象跟踪任务，进一步优化实时性和准确性。",
      "tags": [
        "Large Language Model",
        "Multi-CCTV Tracking",
        "Spatial Prediction",
        "Beam Search",
        "Unsupervised Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:35.338027Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.20249",
    "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
    "authors": [
      "Xuanyu Hu"
    ],
    "abstract": "Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "eess.IV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.20249.pdf",
    "abs_url": "https://arxiv.org/abs/2512.20249",
    "published": "2025-12-23T11:04:34Z",
    "updated": "2026-01-14T07:14:01Z",
    "comment": "15 pages, 2 figures, 4 tables. Submitted to ICPR 2026",
    "light_analysis": {
      "overview": "本文提出BrainROI模型，通过跨主体软功能区域融合和可解释提示优化，解决了多模态脑解码的泛化和可解释性问题。",
      "motivation": "多模态脑解码旨在从fMRI等脑活动信号重建与视觉刺激一致的语义信息，并生成自然语言描述。然而，现有方法面临跨主体泛化挑战，由于不同受试者的脑功能拓扑结构异质性，导致解码性能下降；同时，可解释性不足，手工或黑箱提示方法在稳定性和透明度上有限制，影响解码的可靠性和可审计性。因此，研究旨在开发统一且可解释的解码方法。",
      "method": "方法包括三个核心部分：首先，设计新的fMRI编码器，使用多图谱软功能分割（soft-ROI）作为共享空间，扩展离散ROI串联策略为体素级门控融合机制（Voxel-gate），并通过全局标签对齐确保跨主体一致性。其次，引入可解释的提示优化过程，在少量样本闭环中，利用本地部署的Qwen模型迭代生成和选择人类可读提示，提升提示设计的稳定性和可审计性。最后，在推理时施加参数化解码约束，进一步提高生成描述的质量。实验在NSD数据集上进行。",
      "result": "在跨主体设置下，BrainROI模型在NSD数据集上的脑-描述评估中取得领先结果。与近期先进方法和代表性基线相比，指标如BLEU-4和CIDEr显示出显著改进，具体表现为分数提升，这证明了模型在跨主体泛化方面的优越性，增强了脑解码的鲁棒性和准确性。摘要未明确说明具体数值，但强调了性能的明显改善。",
      "conclusion": "本研究提出统一的BrainROI模型，通过跨主体软-ROI融合和可解释提示优化，有效解决了多模态脑解码的泛化和可解释性挑战。学术上，为脑信号解码提供了创新方法，提升了模型的可转移性和透明度；应用上，有助于脑机接口和神经科学研究的进展。未来工作可进一步探索更多数据集以验证泛化能力，摘要未明确说明局限性。",
      "tags": [
        "Multimodal Brain Decoding",
        "Soft-ROI Fusion",
        "Voxel-gated Fusion",
        "Prompt Optimization",
        "Large Language Model"
      ]
    },
    "analyzed_at": "2026-01-15T03:20:53.511161Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.16401",
    "title": "Navigating the Reality Gap: Privacy-Preserving On-Device Continual Adaptation of ASR for Clinical Telephony",
    "authors": [
      "Darshil Chauhan",
      "Adityasinh Solanki",
      "Vansh Patel",
      "Kanav Kapoor",
      "Ritvik Jain",
      "Aditya Bansal",
      "Pratik Narang",
      "Dhruv Kumar"
    ],
    "abstract": "Automatic Speech Recognition (ASR) holds immense potential to assist in clinical documentation and patient report generation, particularly in resource-constrained regions. However, deployment is currently hindered by a technical deadlock: a severe \"Reality Gap\" between laboratory performance and noisy, real-world clinical audio, coupled with strict privacy and resource constraints. Such adaptation is essential for clinical telephony systems, where patient speech is highly variable and transcription errors can directly impact downstream clinical workflows. We quantify this gap, showing that a robust multilingual model (IndicWav2Vec) degrades up to a 40.94% WER on rural clinical telephony speech from India, rendering it unusable. We demonstrate consistent improvements on these helpline interactions without transmitting raw patient data off-device via an on-device continual adaptation framework using Low-Rank Adaptation (LoRA). We conduct an investigative study of stabilization strategies, characterizing the trade-offs between data-driven and parameter-driven approaches. Our results demonstrate that multi-domain Experience Replay (ER) yields the primary performance gains, achieving a 17.1% relative improvement in target WER and reducing catastrophic forgetting by 55% compared to naive adaptation. Furthermore, we investigate a stabilized importance estimation strategy (Absolute Fisher) to ensure robust convergence against the high-variance gradients common in clinical telephony speech. Finally, we verify via a domain-specific spot check that acoustic adaptation is a fundamental prerequisite for usability in healthcare settings which cannot be bypassed by language models alone.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2512.16401.pdf",
    "abs_url": "https://arxiv.org/abs/2512.16401",
    "published": "2025-12-18T10:56:27Z",
    "updated": "2026-01-14T15:22:47Z",
    "comment": "17 pages, 13 figures. Under review",
    "light_analysis": {
      "overview": "该论文提出隐私保护的设备上持续适应框架，利用低秩适应和经验回放提升临床电话语音识别性能。",
      "motivation": "自动语音识别（ASR）在临床文档和患者报告生成中潜力巨大，特别是在资源受限地区。然而，部署受限于“现实差距”，即实验室性能与嘈杂真实世界临床音频之间的差异，加上严格的隐私和资源约束。临床电话系统中，患者语音高度可变，转录错误直接影响下游工作流，适应至关重要。研究量化此差距，显示标准模型性能下降显著，需在不泄露数据前提下进行适应。",
      "method": "论文提出一个设备上的持续适应框架，使用低秩适应（LoRA）进行参数高效调整，避免传输原始患者数据。关键创新点包括多域经验回放（ER）以减轻灾难性遗忘，以及绝对费舍尔重要性估计策略（Absolute Fisher）处理临床电话语音中的高方差梯度。研究比较了数据驱动和参数驱动方法，重点分析了稳定化策略的权衡，以实现在资源受限设备上的鲁棒收敛。",
      "result": "实验结果显示，多域经验回放（ER）带来主要性能提升，目标词错误率（WER）相对改善17.1%，并与朴素适应相比，灾难性遗忘减少55%。稳定化重要性估计策略确保了在高方差梯度下的鲁棒收敛。研究还验证了声学适应在医疗保健设置中的必要性，不能仅依赖语言模型绕过。这些改进有助于缩小现实差距，提升ASR在实际场景中的可用性。",
      "conclusion": "论文的主要贡献是开发了隐私保护的设备上持续适应框架，有效应对临床电话ASR的现实差距。研究强调声学适应在医疗保健中的基础作用，并通过稳定化策略优化性能。其学术价值在于推动持续学习和自适应系统在现实世界应用，实际价值是支持资源受限地区的临床自动化。未来工作可能包括扩展到其他医疗领域或进一步优化适应算法。",
      "tags": [
        "Automatic Speech Recognition (ASR)",
        "Low-Rank Adaptation (LoRA)",
        "Experience Replay (ER)",
        "Continual Learning",
        "Clinical Telephony"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:48.859372Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.06037",
    "title": "TeleMem: Building Long-Term and Multimodal Memory for Agentic AI",
    "authors": [
      "Chunliang Chen",
      "Ming Guan",
      "Xiao Lin",
      "Jiaxu Li",
      "Qiyi Wang",
      "Xiangyu Chen",
      "Jixiang Luo",
      "Changzhi Sun",
      "Dell Zhang",
      "Xuelong Li"
    ],
    "abstract": "Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2601.06037.pdf",
    "abs_url": "https://arxiv.org/abs/2601.06037",
    "published": "2025-12-12T11:24:52Z",
    "updated": "2026-01-14T09:21:10Z",
    "comment": null,
    "light_analysis": {
      "overview": "TeleMem是一个统一的长期和多模态记忆系统，通过叙事动态提取和结构化写入管道，提升AI代理在长期交互中的记忆相干性和效率。",
      "motivation": "大型语言模型（LLMs）在自然语言处理任务中表现优异，但由于注意力机制有限，难以处理扩展对话历史，导致长期交互受限。检索增强生成（RAG）虽然部分缓解问题，但缺乏可靠机制来更新或精化存储记忆，引发模式驱动的幻觉、写入操作低效，以及对多模态推理支持不足。这些问题限制了AI代理的长期任务处理能力，因此亟需开发高效且多模态的记忆系统来提升交互质量和任务完成度。",
      "method": "TeleMem的核心方法包括：首先，通过叙事动态提取技术，从对话中动态提取并维护一致的用户配置文件，确保记忆仅基于实际交互信息。其次，引入结构化的写入管道，对记忆条目进行批量处理、检索、聚类和整合，从而显著提高存储效率，减少标记使用，并加速记忆操作。此外，系统集成多模态记忆模块，结合ReAct风格推理，实现闭环的观察、思考和行动过程，以准确理解长期上下文中的复杂视频内容，如游戏场景。",
      "result": "实验在ZH-4O长期角色扮演游戏基准上进行，结果显示TeleMem相比最先进的Mem0基线，准确率提高了19%，表明任务理解更精确；标记使用减少了43%，显示资源效率提升；操作速度提升了2.1倍，证明系统响应更快。这些性能指标验证了TeleMem在长期记忆管理和多模态推理方面的有效性，超越了现有方法在精度和效率上的表现。",
      "conclusion": "TeleMem的主要贡献是提出一个统一的长期和多模态记忆系统，通过创新方法解决了LLMs在长期交互中的记忆局限性。它提高了记忆的相干性和存储效率，增强了复杂内容的理解能力，具有学术价值，推动了AI代理记忆系统的发展，并为实际应用如游戏和客服提供了技术支持。未来工作可能包括扩展到更多模态、优化算法处理更大规模数据，或探索与其他AI技术的集成。",
      "tags": [
        "Large Language Models",
        "Retrieval-Augmented Generation",
        "ReAct-Style Reasoning",
        "Multimodal Memory",
        "Memory Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:22.207551Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.11423",
    "title": "JoyAvatar-Flash: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
    "authors": [
      "Chaochao Li",
      "Ruikui Wang",
      "Liangbo Zhou",
      "Jinheng Feng",
      "Huaishao Luo",
      "Huan Zhang",
      "Youzheng Wu",
      "Xiaodong He"
    ],
    "abstract": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar-Flash, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.11423.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11423",
    "published": "2025-12-12T10:06:01Z",
    "updated": "2026-01-14T06:50:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "JoyAvatar-Flash提出了一种基于自回归扩散的音频驱动头像生成模型，通过创新技术实现了实时推理和无限长度视频生成，解决了现有方法的计算开销和长度限制问题。",
      "motivation": "现有的基于扩散变换器（DiT）的音频驱动头像生成方法在计算效率上受限，无法生成长视频，阻碍了实际应用。自回归方法虽能处理长度问题，但面临误差累积和质量下降的挑战。因此，开发一种既能实时推理又支持无限长度生成的模型，对于虚拟现实、实时交互等广泛场景至关重要，以弥补现有方法的不足。",
      "method": "论文提出JoyAvatar-Flash，一个音频驱动的自回归扩散模型，核心创新包括：(1) Progressive Step Bootstrapping (PSB)，为视频初始帧分配更多去噪步骤，以稳定生成并减少误差累积；(2) Motion Condition Injection (MCI)，通过注入噪声污染的前一帧作为运动条件，增强时间一致性；(3) Unbounded RoPE via Cache-Resetting (URCR)，利用动态位置编码技术实现无限长度生成。模型采用因果架构，拥有1.3B参数，专注于音频到视频的转换。",
      "result": "JoyAvatar-Flash在单个GPU上实现了16 FPS的实时推理速度，表明其高效性能。在视觉质量、时间一致性和唇语同步方面取得了竞争性结果，与基线方法相比表现出色。摘要未提供具体数值指标，但强调了模型在这些关键维度上的竞争力，验证了所提方法的有效性。",
      "conclusion": "该研究的主要贡献是提出了JoyAvatar-Flash模型及其核心技术PSB、MCI和URCR，成功解决了音频驱动头像生成中的实时性和无限长度问题。学术上，它推动了自回归扩散模型在视频生成领域的发展；实际应用中，为虚拟化身、游戏和实时通信等场景提供了更高效的解决方案。未来工作可能包括进一步优化性能和应用扩展，摘要未明确说明具体局限性。",
      "tags": [
        "Audio-Driven Avatar Generation",
        "Autoregressive Diffusion",
        "Temporal Coherence",
        "Real-time Inference",
        "Infinite Video Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:19.009809Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.11354",
    "title": "A Multi-Mode Structured Light 3D Imaging System with Multi-Source Information Fusion for Underwater Pipeline Detection",
    "authors": [
      "Qinghan Hu",
      "Haijiang Zhu",
      "Na Sun",
      "Lei Chen",
      "Zhengqiang Fan",
      "Zhiqing Li"
    ],
    "abstract": "Underwater pipelines are highly susceptible to corrosion, which not only shorten their service life but also pose significant safety risks. Compared with manual inspection, the intelligent real-time imaging system for underwater pipeline detection has become a more reliable and practical solution. Among various underwater imaging techniques, structured light 3D imaging can restore the sufficient spatial detail for precise defect characterization. Therefore, this paper develops a multi-mode underwater structured light 3D imaging system for pipeline detection (UW-SLD system) based on multi-source information fusion. First, a rapid distortion correction (FDC) method is employed for efficient underwater image rectification. To overcome the challenges of extrinsic calibration among underwater sensors, a factor graph-based parameter optimization method is proposed to estimate the transformation matrix between the structured light and acoustic sensors. Furthermore, a multi-mode 3D imaging strategy is introduced to adapt to the geometric variability of underwater pipelines. Given the presence of numerous disturbances in underwater environments, a multi-source information fusion strategy and an adaptive extended Kalman filter (AEKF) are designed to ensure stable pose estimation and high-accuracy measurements. In particular, an edge detection-based ICP (ED-ICP) algorithm is proposed. This algorithm integrates pipeline edge detection network with enhanced point cloud registration to achieve robust and high-fidelity reconstruction of defect structures even under variable motion conditions. Extensive experiments are conducted under different operation modes, velocities, and depths. The results demonstrate that the developed system achieves superior accuracy, adaptability and robustness, providing a solid foundation for autonomous underwater pipeline detection.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.11354.pdf",
    "abs_url": "https://arxiv.org/abs/2512.11354",
    "published": "2025-12-12T08:04:24Z",
    "updated": "2026-01-14T06:47:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文开发了一个基于多源信息融合的多模式水下结构光3D成像系统，用于精确检测水下管道的缺陷。",
      "motivation": "水下管道易受腐蚀，这不仅缩短其使用寿命，还带来重大安全风险。与人工检查相比，智能实时成像系统已成为更可靠和实用的解决方案。现有方法中，结构化光3D成像能恢复足够空间细节用于精确缺陷表征，但水下环境复杂，存在传感器校准困难、几何可变性和扰动干扰等问题，因此需要更鲁棒和自适应的技术来提高检测精度。",
      "method": "论文提出UW-SLD系统，基于多源信息融合。首先，采用快速失真校正（FDC）方法进行高效水下图像校正。为解决水下传感器外部校准挑战，引入基于因子图的参数优化方法，估计结构化光和声学传感器之间的变换矩阵。还设计多模式3D成像策略以适应管道几何变化，使用多源信息融合策略和自适应扩展卡尔曼滤波器（AEKF）确保稳定姿态估计和高精度测量。关键创新是提出基于边缘检测的ICP算法（ED-ICP），集成管道边缘检测网络与增强点云配准，实现可变运动条件下的稳健高保真缺陷重建。",
      "result": "摘要未明确说明具体性能指标数据。实验在不同操作模式、速度和深度下进行广泛测试，结果表明，所开发的系统展现出卓越的准确性、适应性和鲁棒性。这为自主水下管道检测提供了坚实基础，与基线方法相比，系统在复杂水下环境中表现稳定，有效应对扰动和可变性挑战。",
      "conclusion": "本研究贡献在于开发了多模式水下结构光3D成像系统，集成多种创新技术，如ED-ICP算法和AEKF，显著提升了检测的精确性和可靠性。学术价值体现在为水下传感和机器人导航领域提供了新的技术框架，实际应用价值在于支持自主化管道检测，减少人工干预并增强安全性。摘要未明确说明局限性，未来工作可能扩展到更复杂环境或与其他传感器融合。",
      "tags": [
        "Structured Light 3D Imaging",
        "Multi-Source Information Fusion",
        "Underwater Imaging",
        "Factor Graph Optimization",
        "Adaptive Extended Kalman Filter"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:03.315195Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2601.00005",
    "title": "Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems",
    "authors": [
      "Lesley Wheat",
      "Martin v. Mohrenschildt",
      "Saeid Habibi"
    ],
    "abstract": "Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.00005.pdf",
    "abs_url": "https://arxiv.org/abs/2601.00005",
    "published": "2025-12-07T03:49:54Z",
    "updated": "2026-01-14T02:17:36Z",
    "comment": "21 pages, 14 figures, 11 tables",
    "light_analysis": {
      "overview": "本文通过模拟极端不平衡工业数据集评估14种异常检测算法，发现最佳方法高度依赖于训练中故障示例的数量。",
      "motivation": "机器学习在工业系统如质量控制和预测性维护中具有潜力，但面临极端类别不平衡的挑战，主要由于训练期间故障数据稀缺。这种不平衡限制现有异常检测算法的有效性，影响工业应用的可靠性和效率。因此，需要系统评估不同算法在不平衡数据下的表现，以找到适合工业环境的解决方案。",
      "method": "研究使用一个与问题无关的合成数据集，基于超球面分布模拟异常，特征维度为2D和10D。对14个异常检测器进行基准测试，涵盖无监督（如kNN、LOF）、半监督（XGBOD）和监督（SVM、CatBoost）方法。训练数据集参数包括异常率（0.05%至20%）和规模（1,000至10,000），测试集固定为40,000个样本，以评估性能和泛化误差。",
      "result": "实验结果显示，训练数据中故障示例数量是关键因素：少于20个时，无监督方法（kNN/LOF）表现最佳；约30-50个时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升，额外健康示例益处不大。半监督方法在仅两个特征时无优势，但在十个特征时改进明显，同时小数据集导致泛化性能下降。",
      "conclusion": "本文通过全面评估揭示了异常检测算法在极端不平衡数据下的性能依赖关系，为工业应用提供算法选择指导。研究强调数据稀缺对泛化的影响，建议特征丰富时采用半监督方法。学术上加深了对不平衡学习的理解，实践上有助于优化工业部署，未来工作可扩展至更多维度或真实数据验证。",
      "tags": [
        "Anomaly Detection",
        "Class Imbalance",
        "Synthetic Dataset",
        "Unsupervised Learning",
        "Semi-supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:43.372406Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.05209",
    "title": "DEAR: Dataset for Evaluating the Aesthetics of Rendering",
    "authors": [
      "Vsevolod Plohotnuk",
      "Artyom Panshin",
      "Nikola Banić",
      "Simone Bianco",
      "Michael Freeman",
      "Egor Ershov"
    ],
    "abstract": "Traditional Image Quality Assessment~(IQA) focuses on quantifying technical degradations such as noise, blur, or compression artifacts, using both full-reference and no-reference objective metrics. However, evaluation of rendering aesthetics, a growing domain relevant to photographic editing, content creation, and AI-generated imagery, remains underexplored due to the lack of datasets that reflect the inherently subjective nature of style preference. In this work, a novel benchmark dataset designed to model human aesthetic judgments of image rendering styles is introduced: the Dataset for Evaluating the Aesthetics of Rendering (DEAR). Built upon the MIT-Adobe FiveK dataset, DEAR incorporates pairwise human preference scores collected via large-scale crowdsourcing, with each image pair evaluated by 25 distinct human evaluators with a total of 13,648 of them participating overall. These annotations capture nuanced, context-sensitive aesthetic preferences, enabling the development and evaluation of models that go beyond traditional distortion-based IQA, focusing on a new task: Evaluation of Aesthetics of Rendering (EAR). The data collection pipeline is described, human voting patterns are analyzed, and multiple use cases are outlined, including style preference prediction, aesthetic benchmarking, and personalized aesthetic modeling. To the best of the authors' knowledge, DEAR is the first dataset to systematically address image aesthetics of rendering assessment grounded in subjective human preferences. A subset of 100 images with markup for them is published on HuggingFace (huggingface.co/datasets/vsevolodpl/DEAR).",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.05209.pdf",
    "abs_url": "https://arxiv.org/abs/2512.05209",
    "published": "2025-12-04T19:25:48Z",
    "updated": "2026-01-14T15:23:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了DEAR数据集，这是首个基于人类主观偏好系统评估渲染美学的基准数据集。",
      "motivation": "研究旨在解决渲染美学评估的不足，这在摄影编辑、内容创作和AI生成图像中日益重要。传统图像质量评估（IQA）主要关注技术退化如噪声和模糊，使用客观指标，但忽略了主观风格偏好，导致美学评估研究受限。现有方法缺乏能反映人类主观偏好的数据集，因此开发一个新数据集至关重要，以促进美学评估领域的发展。",
      "method": "论文的核心方法是构建DEAR数据集，基于MIT-Adobe FiveK数据集，通过大规模众包收集成对人类偏好分数。每个图像对由25个独立评估者评分，总计13,648个评估者参与。数据收集流程被详细描述，以捕捉细致和上下文相关的美学偏好，支持新任务EAR（评估渲染美学）。关键创新点在于首次系统化地基于主观人类偏好构建数据集，为超越传统IQA的模型开发提供基础。",
      "result": "摘要未明确说明具体实验结果，但描述了数据集的特点：包含了大量人类偏好分数，捕捉了细致的美学偏好，并概述了用例如风格偏好预测和美学基准测试。数据集已发布一个子集，包含100张图像在HuggingFace平台，为未来研究提供了数据支撑。与基线方法的对比未详细说明，但数据集的规模和人类投票模式分析表明其适用于美学评估任务。",
      "conclusion": "论文的主要贡献是引入了DEAR数据集，填补了传统图像质量评估和主观美学评估之间的空白，为渲染美学评估提供了首个基于人类偏好的基准。学术价值在于支持新研究任务如EAR，促进美学评估模型的发展。实际应用价值涉及摄影编辑、AI内容创建等领域。数据集已公开，为未来研究提供了基础，潜在局限性可在开发基于该数据集的模型时进一步探讨。",
      "tags": [
        "Dataset",
        "Human Preference Scoring",
        "Crowdsourcing",
        "Aesthetic Evaluation",
        "Image Rendering"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:39.599401Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2512.04562",
    "title": "LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models",
    "authors": [
      "Siddharth Betala",
      "Samuel P. Gleason",
      "Ali Ramlaoui",
      "Andy Xu",
      "Georgia Channing",
      "Daniel Levy",
      "Clémentine Fourrier",
      "Nikita Kazeev",
      "Chaitanya K. Joshi",
      "Sékou-Oumar Kaba",
      "Félix Therrien",
      "Alex Hernandez-Garcia",
      "Rocío Mercado",
      "N. M. Anoop Krishnan",
      "Alexandre Duval"
    ],
    "abstract": "Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.04562.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04562",
    "published": "2025-12-04T08:25:16Z",
    "updated": "2026-01-14T15:55:32Z",
    "comment": "46 pages, 17 figures, 16 tables",
    "light_analysis": {
      "overview": "论文提出了LeMat-GenBench，一个统一的评估框架，用于标准化晶体生成模型的评估和比较。",
      "motivation": "生成式机器学习模型在无机晶体的逆设计中展现出巨大潜力，能加速材料发现并探索化学空间。然而，当前缺乏标准化的评估框架，使得不同模型难以公平比较，阻碍了模型的进一步发展和下游应用。现有方法往往依赖非标准指标，导致评估结果不可靠，因此建立一个统一基准对推动该领域进步至关重要。该研究旨在解决这一实际问题，强调评估标准化对模型开发和材料科学的重要性。",
      "method": "论文引入了LeMat-GenBench作为统一基准，包括一套评估指标，旨在更好地指导模型开发和下游应用。核心创新在于设计综合指标以评估模型多个维度，如稳定性、新颖性和多样性。方法还包括发布开源评估套件和Hugging Face上的公共排行榜，实现可重复的模型比较。研究对12个近期生成模型进行了基准测试，利用晶体材料数据展示框架的实用性，强调其可扩展和可重现的特性，以促进未来研究。",
      "result": "通过基准测试12个生成模型，LeMat-GenBench揭示出模型性能的权衡关系：稳定性增加通常导致新颖性和多样性降低。没有一个模型在所有评估维度上都表现出色，这表明了不同模型在特定指标上的优势。结果基于评估指标显示模型间的差异，但没有提供具体数值数据，而是强调了标准评估的重要性。与现有方法相比，该框架提供了公平比较的基础，突出了模型发展的多样性和挑战。",
      "conclusion": "LeMat-GenBench建立了一个可重复和可扩展的评估框架，为晶体生成模型的公平比较奠定基础。主要贡献在于标准化评估方法，其学术价值在于促进模型研究进展，实际价值在于指导开发更可靠的、面向发现的生成模型，以加速材料发现。虽然摘要未明确说明局限性，但未来工作可能包括扩展框架以涵盖更多模型类型或评估维度，进一步提升其在材料科学中的应用。",
      "tags": [
        "Generative Models",
        "Crystalline Materials",
        "Benchmarking",
        "Evaluation Metrics",
        "Inverse Design"
      ]
    },
    "analyzed_at": "2026-01-15T03:21:27.008171Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.22812",
    "title": "LC4-DViT: Land-cover Creation for Land-cover Classification with Deformable Vision Transformer",
    "authors": [
      "Kai Wang",
      "Siyi Chen",
      "Weicong Pang",
      "Chenchen Zhang",
      "Renjun Gao",
      "Ziru Chen",
      "Cheng Li",
      "Dasa Gu",
      "Rui Huang",
      "Alexis Kai Hon Lau"
    ],
    "abstract": "Land-cover underpins ecosystem services, hydrologic regulation, disaster-risk reduction, and evidence-based land planning; timely, accurate land-cover maps are therefore critical for environmental stewardship. Remote sensing-based land-cover classification offers a scalable route to such maps but is hindered by scarce and imbalanced annotations and by geometric distortions in high-resolution scenes. We propose LC4-DViT (Land-cover Creation for Land-cover Classification with Deformable Vision Transformer), a framework that combines generative data creation with a deformation-aware Vision Transformer. A text-guided diffusion pipeline uses GPT-4o-generated scene descriptions and super-resolved exemplars to synthesize class-balanced, high-fidelity training images, while DViT couples a DCNv4 deformable convolutional backbone with a Vision Transformer encoder to jointly capture fine-scale geometry and global context. On eight classes from the Aerial Image Dataset (AID)-Beach, Bridge, Desert, Forest, Mountain, Pond, Port, and River-DViT achieves 0.9572 overall accuracy, 0.9576 macro F1-score, and 0.9510 Cohen' s Kappa, improving over a vanilla ViT baseline (0.9274 OA, 0.9300 macro F1, 0.9169 Kappa) and outperforming ResNet50, MobileNetV2, and FlashInternImage. Cross-dataset experiments on a three-class SIRI-WHU subset (Harbor, Pond, River) yield 0.9333 overall accuracy, 0.9316 macro F1, and 0.8989 Kappa, indicating good transferability. An LLM-based judge using GPT-4o to score Grad-CAM heatmaps further shows that DViT' s attention aligns best with hydrologically meaningful structures. These results suggest that description-driven generative augmentation combined with deformation-aware transformers is a promising approach for high-resolution land-cover mapping.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.22812.pdf",
    "abs_url": "https://arxiv.org/abs/2511.22812",
    "published": "2025-11-27T23:56:35Z",
    "updated": "2026-01-14T07:27:26Z",
    "comment": "This work has been submitted to the IEEE for possible publication.The project is available at https://github.com/weicongpang/LVC2-DViT.git",
    "light_analysis": {
      "overview": "LC4-DViT框架通过结合文本引导的生成数据增强和可变形视觉变换器，显著提升了高分辨率土地覆盖分类的精度和鲁棒性。",
      "motivation": "土地覆盖图对生态系统服务、水文调节和灾害风险减少至关重要，但遥感分类面临挑战：标注数据稀缺且类别不平衡，高分辨率图像中的几何失真限制模型性能。现有方法在处理这些问题时效果有限，因此需要创新方法来生成平衡数据并捕捉变形结构，以支持环境管理和规划应用。",
      "method": "LC4-DViT框架结合生成数据创建和可变形视觉变换器：一个文本引导扩散管道使用GPT-4o生成的场景描述和高分辨率示例合成类别平衡的训练图像；DViT将DCNv4可变形卷积骨干与视觉变换器编码器耦合，共同捕捉细尺度几何特征和全局上下文，以应对数据不足和几何变形问题。",
      "result": "在AID数据集的8个类别上，DViT达到0.9572整体准确率、0.9576宏观F1分数和0.9510 Kappa系数，优于标准ViT（0.9274 OA）及ResNet50、MobileNetV2和FlashInternImage。跨数据集实验在SIRI-WHU子集获得0.9333准确率，显示良好可迁移性；GPT-4o评估Grad-CAM热图表明DViT注意力与水文结构对齐，验证模型有效性。",
      "conclusion": "本研究提出了一种结合生成数据增强和可变形视觉变换器的方法，通过缓解标注不足和几何失真，显著提高了土地覆盖分类性能，具有在环境遥感领域的实际应用价值。未来工作可扩展至更多类别和数据集，并探索进一步优化生成过程或模型架构。",
      "tags": [
        "Land-cover Classification",
        "Generative Data Augmentation",
        "Deformable Vision Transformer",
        "Diffusion Models",
        "Attention Mechanisms"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:52.702833Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.22313",
    "title": "Sentiment Analysis Of Shopee Product Reviews Using Distilbert",
    "authors": [
      "Zahri Aksa Dautd",
      "Aviv Yuniar Rahman"
    ],
    "abstract": "The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.22313.pdf",
    "abs_url": "https://arxiv.org/abs/2511.22313",
    "published": "2025-11-27T10:43:51Z",
    "updated": "2026-01-14T11:52:53Z",
    "comment": "The authors have decided to withdraw this manuscript because substantial improvements are needed in the methodology, data analysis, and presentation of the research results to ensure the article's scientific quality meets journal publication standards. Therefore, the authors plan to conduct a thorough revision before resubmitting to an appropriate journal",
    "light_analysis": {
      "overview": "使用DistilBERT模型对Shopee产品评论进行情感分析，实现了准确性与计算效率的优化平衡。",
      "motivation": "数字商务的快速发展导致在线平台积累了大量消费者评论，例如Shopee作为东南亚主要电商平台，每天接收数百万条产品评论，这些评论包含关于客户满意度和偏好的宝贵信息。手动分析这些评论效率低下、成本高昂，无法实时处理大规模数据，因此需要自动化的情感分析方法来提升效率和精度。现有方法如传统机器学习模型（如SVM）可能准确性有限，而深度学习模型（如BERT）虽然准确但计算资源需求高，难以大规模部署，因此寻求轻量级解决方案以平衡性能与开销。",
      "method": "本研究采用DistilBERT，一个基于知识蒸馏的轻量级Transformer深度学习模型，对Shopee产品评论进行情感分类。使用约一百万条英语评论的数据集，经过预处理后，使用distilbert-base-uncased模型进行训练和微调。DistilBERT通过压缩BERT模型减少了参数数量和计算复杂度，同时保持了Transformer架构的自注意力机制和上下文理解能力。关键创新在于利用轻量化模型设计，实现高效的大规模文本分类，适用于资源受限的电商平台部署环境，提高了可扩展性和实用性。",
      "result": "实验评估使用准确率、精确率、召回率和F1分数等指标，与BERT和SVM等基准模型对比。DistilBERT在Shopee评论数据集上达到94.8%的准确率，略低于BERT的95.3%，但显著高于SVM的90.2%，同时计算时间比BERT减少超过55%，显示出在效率上的显著改进。其他评估指标如F1分数也表现出色，验证了DistilBERT在保持高准确性的同时，有效降低了计算开销，适合大规模情感分析任务，为电商平台提供了性能与资源消耗的折中方案。",
      "conclusion": "本研究表明DistilBERT在电商平台情感分析中能提供准确性与效率的最佳平衡，使得大规模自动化评论分析成为可行。主要贡献在于验证了轻量级Transformer模型在实际应用中的有效性，为电商行业提供了高效的情感分析解决方案。学术上，这项工作推动了轻量化深度学习模型在自然语言处理领域的发展，具有实用价值。未来工作可扩展到多语言评论或结合其他优化技术以提升性能，局限性包括仅基于特定数据集和英文评论，通用性有待进一步验证和研究。",
      "tags": [
        "Sentiment Analysis",
        "DistilBERT",
        "Transformer Models",
        "Natural Language Processing",
        "Shopee Reviews"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:36.115860Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.21519",
    "title": "Self-Paced Learning for Images of Antinuclear Antibodies",
    "authors": [
      "Yiyang Jiang",
      "Guangwu Qian",
      "Jiaxin Wu",
      "Qi Huang",
      "Qing Li",
      "Yongkang Wu",
      "Xiao-Yong Wei"
    ],
    "abstract": "Antinuclear antibody (ANA) testing is a crucial method for diagnosing autoimmune disorders, including lupus, Sjögren's syndrome, and scleroderma. Despite its importance, manual ANA detection is slow, labor-intensive, and demands years of training. ANA detection is complicated by over 100 coexisting antibody types, resulting in vast fluorescent pattern combinations. Although machine learning and deep learning have enabled automation, ANA detection in real-world clinical settings presents unique challenges as it involves multi-instance, multi-label (MIML) learning. In this paper, a novel framework for ANA detection is proposed that handles the complexities of MIML tasks using unaltered microscope images without manual preprocessing. Inspired by human labeling logic, it identifies consistent ANA sub-regions and assigns aggregated labels accordingly. These steps are implemented using three task-specific components: an instance sampler, a probabilistic pseudo-label dispatcher, and self-paced weight learning rate coefficients. The instance sampler suppresses low-confidence instances by modeling pattern confidence, while the dispatcher adaptively assigns labels based on instance distinguishability. Self-paced learning adjusts training according to empirical label observations. Our framework overcomes limitations of traditional MIML methods and supports end-to-end optimization. Extensive experiments on one ANA dataset and three public medical MIML benchmarks demonstrate the superiority of our framework. On the ANA dataset, our model achieves up to +7.0% F1-Macro and +12.6% mAP gains over the best prior method, setting new state-of-the-art results. It also ranks top-2 across all key metrics on public datasets, reducing Hamming loss and one-error by up to 18.2% and 26.9%, respectively. The source code can be accessed at https://github.com/fletcherjiang/ANA-SelfPacedLearning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.21519.pdf",
    "abs_url": "https://arxiv.org/abs/2511.21519",
    "published": "2025-11-26T15:50:03Z",
    "updated": "2026-01-14T07:02:35Z",
    "comment": "IEEE Transactions on Medical Imaging",
    "light_analysis": {
      "overview": "本文提出一种基于自步学习的框架，用于抗核抗体图像的多实例多标签检测，无需手动预处理，实现高效自动化。",
      "motivation": "抗核抗体（ANA）检测对诊断自身免疫疾病如狼疮等至关重要，但手动检测慢、费时且需专业训练，涉及100多种抗体类型和复杂荧光图案组合，形成多实例、多标签问题。现有机器学习方法虽能自动化，但在真实临床环境中处理MIML任务面临挑战，传统方法难以处理未修改图像和自适应标签分配。本研究旨在开发新框架，克服这些限制，提升自动化检测的准确性和效率，以满足临床需求。",
      "method": "论文提出一个新框架，直接使用未修改的显微镜图像处理多实例多标签（MIML）任务。核心创新包括三个组件：实例采样器建模图案置信度以抑制低置信实例；概率伪标签分发器自适应分配标签，基于实例可区分性；以及自步学习权重学习率系数，根据经验观测调整训练。这些组件模拟人类标注逻辑，识别一致性子区域并聚合标签，实现端到端优化，避免手动预处理，提升了学习效率和泛化能力。",
      "result": "在ANA数据集上，该框架比最佳现有方法提高了7.0%的F1-Macro分数和12.6%的平均精度（mAP）。在三个公共医学MIML基准测试中，它在所有关键指标上排名前二，Hamming损失最多减少18.2%，one-error最多减少26.9%。这些实验结果证明了框架在处理多实例多标签任务上的优越性，建立了新的最先进性能，显著超越传统基线方法。",
      "conclusion": "本研究贡献了一种新框架，有效处理抗核抗体图像的多实例多标签检测，通过自步学习和自适应组件实现性能提升。学术上，它为MIML学习提供了创新方法，推动自步学习在医学图像中的应用。实际上，该框架有望加速临床诊断，减少人工依赖，提高检测可靠性。尽管摘要未明确说明局限性，但未来工作可能涉及扩展到其他医学图像任务或优化泛化能力。",
      "tags": [
        "Self-Paced Learning",
        "Multi-Instance Multi-Label Learning",
        "Instance Sampler",
        "Probabilistic Pseudo-Label Dispatcher",
        "End-to-End Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:28.676421Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.18856",
    "title": "Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos",
    "authors": [
      "Sana Alamgeer",
      "Mylene Farias",
      "Marcelo Carvalho"
    ],
    "abstract": "The main goal of the project is to design a new model that predicts regions of interest in 360$^{\\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.18856.pdf",
    "abs_url": "https://arxiv.org/abs/2511.18856",
    "published": "2025-11-24T07:52:06Z",
    "updated": "2026-01-14T18:25:35Z",
    "comment": "I need to withdraw this as it contains some confidential information related to FAPESP funding agency",
    "light_analysis": {
      "overview": "论文提出一种混合显著模型，用于预测360度视频中的兴趣区域。",
      "motivation": "研究动机在于解决360度视频中兴趣区域（ROI）检测的问题，以优化视频流应用。ROI可用于预测用户视口、智能剪辑视频，从而减少带宽使用、改善观看体验，并降低头戴设备观看时的头部移动。现有方法可能在ROI检测的准确性或效率方面存在不足，尤其是在处理复杂全景视频时，需要更有效的检测技术来提升流媒体性能和用户体验。",
      "method": "研究方法包括三个核心步骤：首先预处理视频提取帧，然后开发一个混合显著模型来预测ROI，其中模型可能结合了深度学习和传统视觉技术以增强检测能力，但摘要未详细说明具体架构；最后通过后处理步骤生成每帧的ROI输出。该方法利用360RAT数据集进行训练和测试，以验证模型的有效性。",
      "result": "实验结果方面，摘要提到将提出的方法与360RAT数据集的主观注释进行了比较，但未明确说明具体性能指标，如准确率提升或效率改进百分比。这表明研究通过对比模型输出与人工标注来评估检测效果，但缺乏量化数据来展示相对于基线方法的优势。",
      "conclusion": "结论总结，该研究开发了一个混合显著模型用于360度视频的ROI检测，有助于视频流优化和用户体验提升，具有实际应用价值。学术上，它为ROI检测领域提供了新方法，但局限性可能包括模型通用性或性能未详细评估，未来工作可扩展到更多数据集或改进模型架构。",
      "tags": [
        "Region of Interest Detection",
        "Saliency Model",
        "360-degree Video",
        "Hybrid Model",
        "Video Processing"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:20.100831Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.13529",
    "title": "Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets",
    "authors": [
      "Máté Gedeon",
      "Piroska Zsófia Barta",
      "Péter Mihajlik",
      "Tekla Etelka Gráczi",
      "Anna Kohári",
      "Katalin Mády"
    ],
    "abstract": "The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18% on spontaneous and 4.8% on repeated speech. Diarization experiments yield diarization error rates between 12.46% and 17.40%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.13529.pdf",
    "abs_url": "https://arxiv.org/abs/2511.13529",
    "published": "2025-11-17T16:02:08Z",
    "updated": "2026-01-14T07:26:24Z",
    "comment": "Submitted to LREC 2026",
    "light_analysis": {
      "overview": "论文引入 BEA-Large 和 BEA-Dialogue 数据集，为匈牙利语对话语音识别提供新资源和基线基准。",
      "motivation": "自动语音识别（ASR）在高资源语言中依赖广泛数据集进展迅速，但匈牙利语等低资源语言由于缺乏自发性和对话性语料库而研究受限。现有方法的主要不足在于数据集有限，无法有效支持对话 ASR 和说话者日记化研究，这阻碍了匈牙利语音技术的进步。因此，本研究旨在解决匈牙利语 ASR 数据不足的实际问题，推动低资源语言语音研究的发展。",
      "method": "论文从匈牙利语 BEA 语料库未处理的部分构建了两个新数据集：BEA-Large 包含 255 小时自发语音和详细元数据，扩展了现有 BEA-Base；BEA-Dialogue 包含 85 小时自发对话，划分为说话者独立子集，支持对话 ASR 和说话者日记化研究。研究方法包括使用公开可用的 ASR 模型建立可重复基线，关键创新在于通过微调 Fast Conformer 模型来评估性能，并提供了数据集的详细划分和元数据。",
      "result": "实验结果显示，微调后的 Fast Conformer 模型在自发语音上达到 14.18% 的词错误率，在重复语音上达到 4.8% 的词错误率。说话者日记化实验的日记化错误率介于 12.46% 到 17.40% 之间，为未来改进提供了参考点。与基线方法的对比摘要未明确说明，但结果强调了对话 ASR 的挑战，如不流畅、重叠和非正式语音模式，表明在这些领域仍有改进空间。",
      "conclusion": "论文的主要贡献是发布了 BEA-Large 和 BEA-Dialogue 数据集，并设置了可重复的基线结果，具有重要的学术和实际价值。这推动了匈牙利语音技术的发展，并为其他语言开发自发性和对话性基准提供了方法论框架。局限性包括对话 ASR 仍然困难，未来工作可专注于改进模型性能和处理语音中的复杂性，如重叠和非正式模式。",
      "tags": [
        "Automatic Speech Recognition",
        "Conversational ASR",
        "Speaker Diarization",
        "Fast Conformer Model",
        "Spontaneous Speech Dataset"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:37.081701Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.11334",
    "title": "LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models",
    "authors": [
      "Jian Gao",
      "Richeng Xuan",
      "Zhaolu Kang",
      "Dingshi Liao",
      "Wenxin Huang",
      "Zongmou Huang",
      "Yangdi Xu",
      "Bowen Qin",
      "Zheqi He",
      "Xi Yang",
      "Changjin Li",
      "Yonghua Lin"
    ],
    "abstract": "The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce \\textbf{LaoBench}, the first large-scale, high-quality, and multidimensional benchmark for assessing LLM language understanding and reasoning in Lao. LaoBench contains \\textbf{17,000+} expert-curated samples across three dimensions: culturally grounded knowledge application, curriculum-aligned K12 education, and bilingual translation among Lao, Chinese, and English. It includes open-source and held-out subsets, where the held-out portion enables secure black-box evaluation via a controlled service to improve fairness and data security. We construct LaoBench with a hybrid pipeline that combines expert authoring with agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational validity. We evaluate diverse state-of-the-art open-source and closed-source LLMs, and find that even strong multilingual models lag behind human experts, particularly in culturally grounded reasoning and translation fidelity. We hope LaoBench will catalyze research on Lao and other underrepresented Southeast Asian languages for more inclusive multilingual evaluation.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.11334.pdf",
    "abs_url": "https://arxiv.org/abs/2511.11334",
    "published": "2025-11-14T14:13:07Z",
    "updated": "2026-01-14T16:47:57Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了LaoBench，首个大规模、多维度的老挝语基准，用于评估大型语言模型在语言理解和推理方面的能力，填补低资源语言评估空白。",
      "motivation": "研究动机在于大型语言模型（LLMs）快速发展，但评估主要集中在高资源语言，低资源语言如老挝语等东南亚语言缺乏高质量基准，导致AI系统在这些语言上表现不足，影响公平性和包容性。现有评估方法往往忽略文化相关性和多维度覆盖，限制了多语言AI的进步，因此需要开发专门基准以促进更全面的评估。",
      "method": "研究方法包括构建LaoBench，该基准包含超过17,000个专家策划样本，覆盖文化基础知识应用、K12教育课程对齐和双语翻译（老挝语、中文、英语）。采用混合管道结合专家编写与智能体辅助验证，确保样本的语言准确性、文化相关性和教育有效性。基准分为开源和保留子集，保留部分通过控制服务支持安全黑盒评估，以提高公平性和数据安全。",
      "result": "主要实验结果评估了多种最先进的开源和闭源LLMs，发现即使强大的多语言模型在老挝语任务上仍落后于人类专家，特别是在文化基础推理和翻译忠实度方面。摘要未明确说明具体性能指标如准确率，但强调模型不足，突显了基准的有效性，为未来改进提供了方向。",
      "conclusion": "结论指出LaoBench填补了老挝语评估的空白，为低资源语言提供了高质量基准，促进多语言AI的公平发展。其学术价值在于推动对东南亚语言的研究，实践意义在于实现更包容的评估。潜在局限性可能包括样本覆盖有限，未来工作可扩展到其他语言或增加评估维度。",
      "tags": [
        "Large Language Model",
        "Benchmark",
        "Lao Language",
        "Multilingual Evaluation",
        "Cultural Groundedness"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:33.479594Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.11134",
    "title": "GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models",
    "authors": [
      "Jingxuan Wei",
      "Caijun Jia",
      "Xi Bai",
      "Xinglong Xu",
      "Siyuan Li",
      "Linzhuang Sun",
      "Bihui Yu",
      "Conghui He",
      "Lijun Wu",
      "Cheng Tan"
    ],
    "abstract": "The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.11134.pdf",
    "abs_url": "https://arxiv.org/abs/2511.11134",
    "published": "2025-11-14T10:07:53Z",
    "updated": "2026-01-14T08:29:19Z",
    "comment": "35 pages, 22 figures",
    "light_analysis": {
      "overview": "本文提出了GGBench基准，专门用于评估统一多模态模型的几何生成推理能力，以弥补现有评估方法的不足。",
      "motivation": "随着统一多模态模型（UMMs）的兴起，人工智能正从被动感知转向主动的跨模态生成。然而，现有基准主要评估判别性理解或无约束图像生成，未能测量生成推理的综合认知过程，这限制了模型全面能力的评估。几何构造需要语言理解和精确视觉生成的融合，因此提供了一个理想的测试平台，以解决评估中的关键缺口。该研究旨在填补这一空白，为更严格的智能系统标准奠定基础。",
      "method": "论文引入了GGBench基准，它通过几何构造任务来评估模型的理解、推理和主动构建能力。GGBench提供了一个系统框架，要求模型融合语言和视觉模态，以实现生成推理的精确评估。尽管摘要未详细说明具体的数据集或模型架构细节，但基准设计旨在通过多模态输入和输出任务来诊断模型的综合能力。关键创新点在于利用几何构建的自然复杂性，作为测试统一多模态模型认知过程的工具。",
      "result": "摘要未明确说明具体的实验结果，如准确率提升或效率改进。然而，GGBench作为基准，旨在为统一多模态模型的生成推理能力设置更严格的评估标准。未来实验预计将通过对比现有基线方法，评估模型在理解语言指令、生成精确几何图像等方面的性能。论文主要聚焦于基准的设计和潜在评估价值，为后续研究提供了标准化的测试平台。",
      "conclusion": "本研究的主要贡献是提出了GGBench基准，它系统性地评估了统一多模态模型的几何生成推理能力，弥补了现有评估方法在测量综合认知过程方面的不足。这为下一代智能系统提供了更严格的评估标准，具有重要的学术价值，促进了多模态AI向更主动生成方向的发展。潜在应用包括教育、设计自动化等领域，未来工作可能涉及基准的扩展或应用到更多复杂场景中。",
      "tags": [
        "Unified Multimodal Models",
        "Geometric Reasoning",
        "Generative Reasoning",
        "Benchmarking",
        "Multimodal Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:37.335942Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.10501",
    "title": "Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling for Strategic Multiagent Settings",
    "authors": [
      "Georgios Chalkiadakis",
      "Charilaos Akasiadis",
      "Gerasimos Koresis",
      "Stergios Plataniotis",
      "Leonidas Bakopoulos"
    ],
    "abstract": "This paper provides a comprehensive review of mainly GNN, DRL, and PTM methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) ML methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of GNN. Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of RL, and in particular that of multiagent deep reinforcement learning. Single-agent deep RL has been widely used for decision making in demanding game settings. Its application in multiagent settings though is hindered due to, e.g., varying relationships between agents, and non-stationarity of the environment. We describe existing relevant game theoretic solution concepts, and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes probabilistic topic modeling (PTM) in domains other than that of document analysis and classification. Finally, we identify certain open challenges -- specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.10501.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10501",
    "published": "2025-11-13T17:06:56Z",
    "updated": "2026-01-14T13:06:54Z",
    "comment": "27 pages",
    "light_analysis": {
      "overview": "本文综述了图神经网络、深度强化学习和概率主题建模在战略多智能体设置中的应用，并倡导整合游戏理论概念以处理现实世界的不确定性和异构性。",
      "motivation": "研究动机在于解决战略对手建模中未知模型结构的问题，现实世界应用常涉及不确定性和异构性，现有方法如单智能体深度强化学习在多智能体设置中受阻，例如环境非平稳性和智能体间关系变化。传统游戏理论假设如共同先验假设和自利假设往往不切实际，因此需要整合机器学习方法来规避这些局限，提高可扩展性和适应性。",
      "method": "研究方法包括对图神经网络、深度强化学习和概率主题建模的全面回顾，重点探讨它们在多智能体设置中的潜在整合。关键创新点在于倡导使用图神经网络建模智能体间关系和交互，结合游戏理论概念如公平性和稳定性。回顾了相关游戏理论解决方案，并讨论了这些方法处理不确定性、异构性的能力，但具体数据集或模型架构在摘要中未明确说明。",
      "result": "摘要未明确说明具体实验结果或性能指标，但论文分析了图神经网络、深度强化学习和概率主题建模在战略多智能体设置中的能力，如处理不确定性和可扩展性。与基线方法的对比未提及，主要侧重于方法回顾和潜在优势的讨论，而非实证数据支撑。",
      "conclusion": "论文主要贡献在于综述了机器学习与游戏理论的整合方法，学术价值在于为多智能体系统建模提供新视角，实际应用价值在于改进战略决策和对手建模。结论指出了开放挑战，如适应非平稳环境、平衡稳定性和适应性、应对不确定性并确保可扩展性，为未来研究方向提供了指导。",
      "tags": [
        "Graph Neural Networks",
        "Deep Reinforcement Learning",
        "Probabilistic Topic Modeling",
        "Multiagent Systems",
        "Game Theory"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:29.887306Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.10045",
    "title": "Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism",
    "authors": [
      "Jinhong Jeong",
      "Sunghyun Lee",
      "Jaeyoung Lee",
      "Seonah Han",
      "Youngjae Yu"
    ],
    "abstract": "Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs' performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models' layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs' phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models' focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs' interpretability.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.10045.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10045",
    "published": "2025-11-13T07:46:09Z",
    "updated": "2026-01-14T15:00:34Z",
    "comment": "33 pages, 27 tables, 10 figures",
    "light_analysis": {
      "overview": "本研究通过定量分析多模态大语言模型对声音象征的处理，揭示了模型如何将语音形式与意义关联，提供首个大规模可解释性研究。",
      "motivation": "声音象征指语音形式与意义之间的非任意关联，研究旨在探讨多模态大语言模型如何解释听觉信息，以理解其语言处理能力。现有方法在模型对声音象征的定量分析方面不足，尤其是在多模态环境中，缺乏大规模实证研究。本研究通过声音象征作为探针，评估模型在多模态输入下的表现，填补了人工智能与认知语言学交叉领域的空白。",
      "method": "研究构建了LEX-ICON数据集，包含8,052个来自英语、法语、日语和韩语的单词及2,930个伪单词，覆盖文本和音频模态，并标注语义特征。核心方法涉及使用多模态输入（如文本的正字和IPA形式以及音频），测量模型在多达25个语义维度上的音素级注意力分数。通过层级信息处理分析，评估MLLMs的注意模式，特别是对图标音素的关注，以探索语音象征处理机制。",
      "result": "主要发现显示，多模态大语言模型在多个语义维度上表现出与现有语言学研究成果一致的语音直觉，验证了声音象征的关联。具体地，模型的音义注意模式突显了对图标音素的关注，表明其能够有效处理语音与意义的关系。摘要未提供具体性能指标如准确率，但研究基于大规模定量分析，为模型可解释性提供了实证支持，与基线方法的对比摘要未明确说明。",
      "conclusion": "本研究首次对多模态大语言模型在语音象征方面进行大规模定量分析，揭示了模型如何关联声音与意义。主要贡献在于桥接人工智能和认知语言学领域，提供模型可解释性的新视角，有助于理解多模态模型的语言处理机制。潜在局限性或未来工作方向摘要未明确说明，可能涉及扩展数据集或深化模型内部机制的探索。",
      "tags": [
        "Multimodal Large Language Models (MLLMs)",
        "Sound Symbolism",
        "Phonetic Iconicity",
        "Attention Mechanisms",
        "Phonosemantic Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:22:49.720318Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.16682",
    "title": "Bench360: Benchmarking Local LLM Inference from 360 Degrees",
    "authors": [
      "Linus Stuhlmann",
      "Mauricio Fadel Argerich",
      "Jonathan Fürst"
    ],
    "abstract": "Running LLMs locally has become increasingly common, but users face a complex design space across models, quantization levels, inference engines, and serving scenarios. Existing inference benchmarks are fragmented and focus on isolated goals, offering little guidance for practical deployments. We present Bench360, a framework for evaluating local LLM inference across tasks, usage patterns, and system metrics in one place. Bench360 supports custom tasks, integrates multiple inference engines and quantization formats, and reports both task quality and system behavior (latency, throughput, energy, startup time). We demonstrate it on four NLP tasks across three GPUs and four engines, showing how design choices shape efficiency and output quality. Results confirm that tradeoffs are substantial and configuration choices depend on specific workloads and constraints. There is no universal best option, underscoring the need for comprehensive, deployment-oriented benchmarks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.16682.pdf",
    "abs_url": "https://arxiv.org/abs/2511.16682",
    "published": "2025-11-12T09:57:21Z",
    "updated": "2026-01-14T08:53:59Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出Bench360框架，用于全面评估本地大型语言模型推理的性能，整合任务质量和系统指标。",
      "motivation": "随着本地运行大型语言模型越来越普遍，用户在设计选择上面临复杂空间，涉及模型、量化级别、推理引擎和服务场景。现有推理基准测试分散，仅关注孤立目标，对实际部署的指导有限，导致用户难以优化配置和平衡效率与质量。因此，需要一种综合性的基准测试框架来解决这一问题，以支持更有效的本地LLM部署决策。",
      "method": "研究提出了Bench360框架，支持自定义任务，集成多个推理引擎和量化格式，并同时报告任务质量和系统行为（如延迟、吞吐量、能源消耗和启动时间）。关键创新点在于将多维度评估统一到一个平台中，通过演示四个NLP任务、三个GPU和四个引擎，展示了设计选择如何影响效率与输出质量的权衡，为部署提供详细分析工具。",
      "result": "实验结果表明，设计选择对本地LLM推理的效率和输出质量有显著影响，权衡非常明显。配置选择高度依赖于具体工作负载和约束条件，没有普遍的最佳选项。例如，不同引擎和GPU组合在任务性能上表现各异，但摘要未明确说明具体性能数据。这强调了在实际部署中需要基于场景进行定制化评估和优化。",
      "conclusion": "论文的主要贡献是开发了Bench360框架，填补了现有基准测试在全面性和部署导向性方面的不足。其学术价值在于提供了一种多维度评估方法，实际应用价值是帮助用户更好地理解和优化本地LLM推理配置。未来工作可扩展到更多任务、硬件或引擎，并探索自动化配置推荐，以进一步提升框架的实用性和覆盖范围。",
      "tags": [
        "Large Language Model",
        "Benchmarking",
        "Quantization",
        "Inference Engine",
        "System Metrics"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:03.925919Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.06209",
    "title": "Efficient Test-Time Scaling of Multi-Step Reasoning by Probing Internal States of Large Language Models",
    "authors": [
      "Jingwei Ni",
      "Ekaterina Fadeeva",
      "Tianyi Wu",
      "Mubashara Akhtar",
      "Jiaheng Zhang",
      "Elliott Ash",
      "Markus Leippold",
      "Timothy Baldwin",
      "See-Kiong Ng",
      "Artem Shelmanov",
      "Mrinmaya Sachan"
    ],
    "abstract": "LLMs can solve complex tasks by generating long, multi-step reasoning chains. Test-time scaling (TTS) can further improve LLM performance by sampling multiple variants of intermediate reasoning steps, verifying their correctness, and strategically choosing the best steps for continuation. However, existing verification approaches, such as Process Reward Models (PRMs), are computationally expensive, limited to specific domains, and require large-scale human or model-generated annotations. We propose a lightweight alternative for step-level reasoning verification based on probing the internal states of LLMs. We train a transformer-based probe that uses the internal states of the frozen LLM to estimate the credibility of its reasoning steps during generation. Annotation can be generated either by another larger LLM (e.g., DeepSeek-R1) or in a self-supervised manner by the original model itself. The probes are both effective and lightweight, containing fewer than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, our probes match or even exceed the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their confidence in reasoning processes and can serve as reliable signals for reasoning step verification, offering a promising direction towards scalable and generalizable TTS and introspective LLMs.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.06209.pdf",
    "abs_url": "https://arxiv.org/abs/2511.06209",
    "published": "2025-11-09T03:38:29Z",
    "updated": "2026-01-14T10:08:56Z",
    "comment": "Preprint under review",
    "light_analysis": {
      "overview": "提出一种轻量级方法，通过探测大型语言模型的内部状态来验证推理步骤，实现高效的测试时多步推理扩展。",
      "motivation": "LLMs通过生成长推理链解决复杂任务，测试时缩放通过采样和验证步骤来提升性能，但现有验证方法如过程奖励模型计算昂贵、领域特定且需大规模标注，限制了可扩展性和通用性，这使得高效验证成为重要研究问题。",
      "method": "论文提出一个基于transformer的探测器，利用冻结LLM的内部状态评估推理步骤可信度，标注可由更大LLM或自监督生成，探测器参数少于10M，轻量级设计减少了计算开销，关键创新在于直接使用内部状态作为验证信号。",
      "result": "在数学、规划和常识问答等领域，探测器匹配或超过过程奖励模型的性能，后者规模大810倍，展示了高效的验证能力，具体数据表明内部状态可作为可靠信号，显著降低了计算成本。",
      "conclusion": "研究主要贡献是揭示了LLM内部状态编码推理置信度，为步骤验证提供可靠信号，推动了可扩展测试时缩放和内省式LLM的发展，潜在局限和未来工作方向摘要未明确说明。",
      "tags": [
        "Large Language Models",
        "Probing",
        "Test-Time Scaling",
        "Transformer-based Probe",
        "Self-Supervised Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:32.874756Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.04133",
    "title": "Testing the Testers: Human-Driven Quality Assessment of Voice AI Testing Platforms",
    "authors": [
      "Miguel E. Andres",
      "Vadim Fedorov",
      "Rida Sadek",
      "Enric Spagnolo-Arrizabalaga",
      "Nadescha Trudel"
    ],
    "abstract": "Voice AI agents are rapidly transitioning to production deployments, yet systematic methods for ensuring testing reliability remain underdeveloped. Organizations cannot objectively assess whether their testing approaches (internal tools or external platforms) actually work, creating a critical measurement gap as voice AI scales to billions of daily interactions.   We present the first systematic framework for evaluating voice AI testing quality through human-centered benchmarking. Our methodology addresses the fundamental dual challenge of testing platforms: generating realistic test conversations (simulation quality) and accurately evaluating agent responses (evaluation quality). The framework combines established psychometric techniques (pairwise comparisons yielding Elo ratings, bootstrap confidence intervals, and permutation tests) with rigorous statistical validation to provide reproducible metrics applicable to any testing approach.   To validate the framework and demonstrate its utility, we conducted comprehensive empirical evaluation of three leading commercial platforms focused on Voice AI Testing using 21,600 human judgments across 45 simulations and ground truth validation on 60 conversations. Results reveal statistically significant performance differences with the proposed framework, with the top-performing platform, Evalion, achieving 0.92 evaluation quality measured as f1-score versus 0.73 for others, and 0.61 simulation quality using a league based scoring system (including ties) vs 0.43 for other platforms.   This framework enables researchers and organizations to empirically validate the testing capabilities of any platform, providing essential measurement foundations for confident voice AI deployment at scale. Supporting materials are made available to facilitate reproducibility and adoption.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2511.04133.pdf",
    "abs_url": "https://arxiv.org/abs/2511.04133",
    "published": "2025-11-06T07:22:58Z",
    "updated": "2026-01-14T08:55:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了首个系统性框架，通过人类中心基准测试评估语音AI测试平台的质量，解决测试可靠性评估的测量缺口。",
      "motivation": "研究动机源于语音AI代理正快速部署到生产中，但现有系统性方法对确保测试可靠性不足，导致组织无法客观评估其内部工具或外部平台的测试方法是否真正有效。随着语音AI每日交互量达数十亿，这造成了关键的测量缺口，亟需开发可靠的质量评估框架以应对测试平台在生成真实对话和准确评估响应方面的双挑战，确保大规模部署的信任基础。",
      "method": "研究方法构建了一个系统性框架，专注于评估语音AI测试平台的质量，针对模拟质量（生成真实测试对话）和评估质量（准确评估代理响应）的双挑战。关键技术结合了心理测量学方法，如成对比较生成Elo评分、引导置信区间和置换检验，并通过统计验证提供可复现的指标，适用于任何测试方法。验证中使用了21,600个人类判断，覆盖45个模拟，并对60个对话进行基准真实验证，确保方法的严谨性和通用性。",
      "result": "实证评估三个领先商业平台的结果揭示了统计显著的性能差异：最佳平台Evalion的评估质量以f1分数衡量为0.92，远高于其他平台的0.73；模拟质量使用联盟评分系统为0.61，对比其他平台的0.43。这些数据基于21,600个人类判断和45个模拟的验证，表明框架能有效量化平台间的性能差距，提供客观比较依据。",
      "conclusion": "该框架的主要贡献在于提供了首个系统性方法，使研究人员和组织能够经验性地验证任何语音AI测试平台的能力，解决了测试可靠性的测量缺口，并为大规模部署语音AI奠定必要基础。学术上，它结合心理测量学和统计验证，推动测试评估领域发展；实际应用上，支持材料可促进复现和采纳，未来工作可扩展至更多平台和应用场景，提升测试标准化。",
      "tags": [
        "Voice AI Testing",
        "Human-Centered Benchmarking",
        "Psychometric Techniques",
        "Elo Ratings",
        "Statistical Validation"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:34.341121Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.01131",
    "title": "Weakly Supervised Concept Learning with Class-Level Priors for Interpretable Medical Diagnosis",
    "authors": [
      "Md Nahiduzzaman",
      "Steven Korevaar",
      "Alireza Bab-Hadiashar",
      "Ruwan Tennakoon"
    ],
    "abstract": "Human-interpretable predictions are essential for deploying AI in medical imaging, yet most interpretable-by-design (IBD) frameworks require concept annotations for training data, which are costly and impractical to obtain in clinical contexts. Recent attempts to bypass annotation, such as zero-shot vision-language models or concept-generation frameworks, struggle to capture domain-specific medical features, leading to poor reliability. In this paper, we propose a novel Prior-guided Concept Predictor (PCP), a weakly supervised framework that enables concept answer prediction without explicit supervision or reliance on language models. PCP leverages class-level concept priors as weak supervision and incorporates a refinement mechanism with KL divergence and entropy regularization to align predictions with clinical reasoning. Experiments on PH2 (dermoscopy) and WBCatt (hematology) show that PCP improves concept-level F1-score by over 33% compared to zero-shot baselines, while delivering competitive classification performance on four medical datasets (PH2, WBCatt, HAM10000, and CXR4) relative to fully supervised concept bottleneck models (CBMs) and V-IP.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.01131.pdf",
    "abs_url": "https://arxiv.org/abs/2511.01131",
    "published": "2025-11-03T00:43:04Z",
    "updated": "2026-01-14T05:13:20Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了Prior-guided Concept Predictor (PCP)，一种利用类级先验的弱监督概念预测框架，通过KL散度和熵正则化改进医学影像中的可解释诊断。",
      "motivation": "医学影像中可解释AI对于临床应用至关重要，但现有可解释性设计框架需要大量概念标注数据，这在临床实践中成本高昂且不切实际。零样本视觉语言模型或概念生成方法难以准确捕获医学领域特定特征，导致可靠性不足。因此，开发一种无需显式标注的弱监督方法，以减少对昂贵数据标注的依赖，提高诊断的可靠性，是该研究的主要动机。",
      "method": "论文提出Prior-guided Concept Predictor (PCP)，这是一个弱监督框架，通过类级概念先验作为监督信号，无需显式标注或依赖语言模型。核心创新包括一个细化机制，结合KL散度和熵正则化来优化预测，确保其与临床推理对齐。技术特色在于利用先验知识作为弱监督，减少标注需求，同时通过正则化方法提升概念学习的准确性，实现无需额外标注的医学影像分析。",
      "result": "实验在PH2（皮肤镜检查）和WBCatt（血液学）数据集上进行，PCP将概念级F1分数提高了33%以上，显著优于零样本基线方法。此外，在四个医学数据集（PH2、WBCatt、HAM10000、CXR4）上，PCP的分类性能与全监督概念瓶颈模型（CBMs）和V-IP方法相当或接近，展示了其在弱监督下的有效性和竞争力。这些结果证明了PCP在概念学习和分类任务上的优越性。",
      "conclusion": "PCP框架在无需显式标注的情况下实现了医学影像中的可解释概念预测，通过弱监督和类级先验克服了现有方法的局限性。该研究为临床AI应用提供了无需昂贵标注的解决方案，推动了可解释AI的发展，具有重要的学术价值和实际应用潜力。摘要未明确说明研究的局限性或未来工作方向。",
      "tags": [
        "Weakly Supervised Learning",
        "Concept Learning",
        "KL Divergence",
        "Entropy Regularization",
        "Interpretable Medical Diagnosis"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:45.446250Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.27155",
    "title": "Hierarchical Fusion of Local and Global Visual Features with Mixture-of-Experts for Remote Sensing Image Scene Classification",
    "authors": [
      "Yuanhao Tang",
      "Xuechao Zou",
      "Zhengpei Hu",
      "Junliang Xing",
      "Chengkun Zhang",
      "Jianqiang Huang"
    ],
    "abstract": "Remote sensing image scene classification remains a challenging task, primarily due to the complex spatial structures and multi-scale characteristics of ground objects. Although CNN-based methods excel at extracting local inductive biases, and Mamba-based approaches demonstrate impressive capabilities in efficiently capturing global sequential context, relying on a single paradigm restricts the model's ability to simultaneously characterize fine-grained textures and complex spatial structures. To tackle this, we propose a parallel heterogeneous encoder, a hierarchical fusion module designed to achieve effective local-global co-representation. It consists of two parallel pathways: a local visual encoder for extracting multi-scale local visual features, and a global visual encoder for capturing efficient global visual features. The core innovation lies in its hierarchical fusion module, which progressively aggregates multi-scale features from both pathways, enabling dynamic cross-level feature interaction and contextual reconstruction to produce highly discriminative representations. These fused features are then adaptively routed through a mixture-of-experts classifier head, which dynamically dispatches them to the most suitable experts for fine-grained scene recognition. Experiments on AID, NWPU-RESISC45, and UC Merced show that our model achieves 93.72%, 95.54%, and 96.92% accuracy, surpassing SOTA methods with an optimal balance of performance and efficiency. Code is available at https://anonymous.4open.science/r/classification-41DF.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.27155.pdf",
    "abs_url": "https://arxiv.org/abs/2510.27155",
    "published": "2025-10-31T03:55:16Z",
    "updated": "2026-01-14T08:36:13Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出了一种并行异构编码器和层次融合模块，结合混合专家分类器，有效融合局部与全局视觉特征以提升遥感图像场景分类的准确性。",
      "motivation": "遥感图像场景分类因物体空间结构复杂和多尺度特性而具挑战性。现有方法中，CNN虽擅长提取局部特征，Mamba虽能有效捕获全局上下文，但依赖单一范式限制了模型同时处理细粒度纹理和复杂结构的能力。因此，需一种融合局部与全局特征的方法来提高分类性能，解决现有技术难以平衡局部细节与全局结构的不足。",
      "method": "论文提出一种并行异构编码器，包含局部视觉编码器提取多尺度局部特征，以及全局视觉编码器捕获高效全局特征。核心创新是层次融合模块，通过逐步聚合多尺度特征，实现动态跨级交互和上下文重构，生成判别性表示。融合特征随后由混合专家分类器头自适应路由，动态分派给最合适的专家进行细粒度场景识别。",
      "result": "实验在AID、NWPU-RESISC45和UC Merced数据集上进行，模型分别取得了93.72%、95.54%和96.92%的准确率。这些结果超越了现有的最佳方法（SOTA），同时在性能和效率之间达到了最优平衡，验证了所提方法的有效性，具体表现为准确率提升和模型优化。",
      "conclusion": "本论文的主要贡献是提出了一种有效融合局部与全局视觉特征的方法，显著提升了遥感图像场景分类的准确性。学术上，该研究提供了一种结合CNN和Mamba优势的新框架，具有创新性；实际应用中，能改进遥感图像分析的性能。摘要未明确说明局限性，但未来工作可包括扩展到其他视觉任务或优化模型效率。",
      "tags": [
        "Local Feature Extraction",
        "Global Context Modeling",
        "Hierarchical Fusion",
        "Mixture-of-Experts",
        "Remote Sensing Scene Classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:40.490929Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2511.00056",
    "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
    "authors": [
      "Yuxi Liu",
      "Renjia Deng",
      "Yutong He",
      "Xue Wang",
      "Tao Yao",
      "Kun Yuan"
    ],
    "abstract": "The substantial memory demands of pre-training and fine-tuning large language models (LLMs) require memory-efficient optimization algorithms. One promising approach is layer-wise optimization, which treats each transformer block as a single layer and optimizes it sequentially, while freezing the other layers to save optimizer states and activations. Although effective, these methods ignore the varying importance of the modules within each layer, leading to suboptimal performance. Moreover, layer-wise sampling provides only limited memory savings, as at least one full layer must remain active during optimization. To overcome these limitations, we propose Module-wise Importance SAmpling (MISA), a novel method that divides each layer into smaller modules and assigns importance scores to each module. MISA uses a weighted random sampling mechanism to activate modules, provably reducing gradient variance compared to layer-wise sampling. Additionally, we establish an \\(\\mathcal{O}(1/\\sqrt{K})\\) convergence rate under non-convex and stochastic conditions, where $K$ is the total number of block updates, and provide a detailed memory analysis showcasing MISA's superiority over existing baseline methods. Experiments on diverse learning tasks validate the effectiveness of MISA. Source code is available at https://github.com/pkumelon/MISA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.00056.pdf",
    "abs_url": "https://arxiv.org/abs/2511.00056",
    "published": "2025-10-28T17:06:27Z",
    "updated": "2026-01-14T17:04:50Z",
    "comment": "This paper is accepted to Neural Information Processing Systems (NeurIPS) 2025",
    "light_analysis": {
      "overview": "MISA提出一种基于模块重要性采样的内存高效优化方法，用于改进大型语言模型的训练过程，通过精细划分模块减少内存消耗并提升性能。",
      "motivation": "大型语言模型（LLMs）在预训练和微调阶段对内存需求极高，现有方法如层优化（layer-wise optimization）虽然通过冻结其他层来节省优化器状态和激活内存，但忽略了每层内部模块的重要性差异，导致性能次优，且内存节省有限，因为至少需要保持一整层活跃。因此，研究旨在克服这些限制，开发更精细的内存高效优化算法。",
      "method": "MISA方法将每个transformer层划分为更小的模块，并为每个模块分配重要性分数，采用加权随机采样机制来激活模块。这种方法理论上减少了梯度方差，相比层优化更有效。研究在非凸和随机条件下建立了O(1/√K)的收敛率，其中K是块更新总数，并进行了详细的内存分析，展示了MISA相对于基线方法的优势。模型架构基于标准LLM transformer块，摘要未明确说明具体数据集细节。",
      "result": "实验在多种学习任务上验证了MISA的有效性，但摘要未明确说明具体性能指标如准确率或内存节省百分比。研究结果表明MISA在内存效率方面优于现有基线方法，通过模块重要性采样实现了更好的优化效果，但用户需参考论文全文获取详细数据对比。",
      "conclusion": "MISA的主要贡献在于提供了一种内存高效的优化方法，通过模块重要性采样平衡了内存节省和性能提升。该研究具有学术价值，为LLMs训练提供了新思路，并公开了源代码促进实际应用。局限性可能包括对特定任务或模型规模的泛化能力，未来工作可扩展至更多场景或结合其他优化技术。",
      "tags": [
        "Large Language Models",
        "Memory-Efficient Optimization",
        "Importance Sampling",
        "Stochastic Optimization",
        "Convergence Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:23:36.274953Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.23111",
    "title": "Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data",
    "authors": [
      "Felix Koehler",
      "Nils Thuerey"
    ],
    "abstract": "Neural operators or emulators for PDEs trained on data from numerical solvers are conventionally assumed to be limited by their training data's fidelity. We challenge this assumption by identifying \"emulator superiority,\" where neural networks trained purely on low-fidelity solver data can achieve higher accuracy than those solvers when evaluated against a higher-fidelity reference. Our theoretical analysis reveals how the interplay between emulator inductive biases, training objectives, and numerical error characteristics enables superior performance during multi-step rollouts. We empirically validate this finding across different PDEs using standard neural architectures, demonstrating that emulators can implicitly learn dynamics that are more regularized or exhibit more favorable error accumulation properties than their training data, potentially surpassing training data limitations and mitigating numerical artifacts. This work prompts a re-evaluation of emulator benchmarking, suggesting neural emulators might achieve greater physical fidelity than their training source within specific operational regimes. Project Page: https://tum-pbs.github.io/emulator-superiority",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.23111.pdf",
    "abs_url": "https://arxiv.org/abs/2510.23111",
    "published": "2025-10-27T08:31:55Z",
    "updated": "2026-01-14T08:02:51Z",
    "comment": "Accepted at NeurIPS 2025: https://neurips.cc/virtual/2025/poster/116770 ; V2: Updated references, fix typos",
    "light_analysis": {
      "overview": "本论文提出‘模拟器优越性’概念，揭示神经模拟器基于低保真数据训练时可超越训练数据，实现比数值求解器更高的精度。",
      "motivation": "传统观点认为，基于数值求解器数据训练的神经模拟器受限于训练数据的保真度，性能不能超越数据源。这一假设限制了神经模拟器在偏微分方程数值解中的应用潜力，因为忽略了神经网络可能学习到比训练数据更优动力学的可能性。本研究旨在挑战这一假设，探索神经模拟器如何通过自身特性克服训练数据限制，从而提高模拟精度。",
      "method": "论文提出‘模拟器优越性’概念，通过理论分析探讨模拟器归纳偏置、训练目标和数值误差特性之间的相互作用，以解释多步展开中实现优越性能的机制。经验验证部分使用标准神经架构在不同偏微分方程上进行实验，如神经算子等，以证明模拟器能隐含学习更规范化或误差积累更有利的动力学。摘要未明确说明具体数据集或详细模型架构，但强调了理论与实证相结合的方法。",
      "result": "实验结果表明，神经模拟器仅基于低保真求解器数据训练后，在评估高保真参考标准时，可以达到比训练数据源更高的准确性。尽管摘要未提供具体性能指标如准确率数值，但与基线低保真求解器相比，模拟器展示了优越性能，超越了传统假设中训练数据的限制。这种优越性体现在多步模拟中误差积累更小，从而减轻了数值伪影。",
      "conclusion": "本研究的主要贡献是提出并验证了‘模拟器优越性’，挑战了神经模拟器性能受限于训练数据的传统观点。学术上，这丰富了神经网络在偏微分方程数值解中的理论理解；实际上，表明神经模拟器在特定操作体系下可能提供比传统数值方法更高的物理保真度，为改进PDEs模拟提供了新思路。未来工作可能涉及更广泛的理论扩展和应用验证。",
      "tags": [
        "Neural Emulators",
        "Partial Differential Equations (PDEs)",
        "Numerical Solvers",
        "Inductive Bias",
        "Machine Learning for PDEs"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:02.778910Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.22129",
    "title": "egoEMOTION: Egocentric Vision and Physiological Signals for Emotion and Personality Recognition in Real-World Tasks",
    "authors": [
      "Matthias Jammot",
      "Björn Braun",
      "Paul Streli",
      "Rafael Wampfler",
      "Christian Holz"
    ],
    "abstract": "Understanding affect is central to anticipating human behavior, yet current egocentric vision benchmarks largely ignore the person's emotional states that shape their decisions and actions. Existing tasks in egocentric perception focus on physical activities, hand-object interactions, and attention modeling - assuming neutral affect and uniform personality. This limits the ability of vision systems to capture key internal drivers of behavior. In this paper, we present egoEMOTION, the first dataset that couples egocentric visual and physiological signals with dense self-reports of emotion and personality across controlled and real-world scenarios. Our dataset includes over 50 hours of recordings from 43 participants, captured using Meta's Project Aria glasses. Each session provides synchronized eye-tracking video, headmounted photoplethysmography, inertial motion data, and physiological baselines for reference. Participants completed emotion-elicitation tasks and naturalistic activities while self-reporting their affective state using the Circumplex Model and Mikels' Wheel as well as their personality via the Big Five model. We define three benchmark tasks: (1) continuous affect classification (valence, arousal, dominance); (2) discrete emotion classification; and (3) trait-level personality inference. We show that a classical learning-based method, as a simple baseline in real-world affect prediction, produces better estimates from signals captured on egocentric vision systems than processing physiological signals. Our dataset establishes emotion and personality as core dimensions in egocentric perception and opens new directions in affect-driven modeling of behavior, intent, and interaction.",
    "categories": [
      "cs.CV",
      "cs.HC"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.22129.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22129",
    "published": "2025-10-25T03:04:51Z",
    "updated": "2026-01-14T15:52:28Z",
    "comment": "Accepted for publication at NeurIPS 2025",
    "light_analysis": {
      "overview": "本文提出 egoEMOTION 数据集，首次结合第一人称视觉与生理信号，用于真实场景中的情感和人格识别。",
      "motivation": "当前的第一人称视觉基准主要关注物理活动、手-物体交互和注意力建模，假设情感中性和人格统一，从而忽略了影响决策和行为的关键情感状态。理解情感对于预测人类行为至关重要，但现有方法因忽视情感维度而无法捕捉行为的内在驱动力。因此，该研究旨在填补这一空白，通过引入情感和人格识别，提升第一人称感知系统对行为内部因素的建模能力，促进更智能的人机交互应用。",
      "method": "论文创建了 egoEMOTION 数据集，包含超过 50 小时的记录，来自 43 名参与者，使用 Meta 的 Project Aria 眼镜捕获同步多模态数据，包括眼动视频、头戴式光电容积描记法、惯性运动数据和生理基线。参与者在控制场景和真实世界活动中完成情感引发任务和自然活动，同时使用 Circumplex Model 和 Mikels' Wheel 自我报告情感状态，以及 Big Five 模型报告人格特质。基于此，论文定义了三个基准任务：连续情感分类（涉及效价、唤醒和主导维度）、离散情感分类和特质级人格推断。",
      "result": "实验使用经典学习基方法作为简单基线，在现实世界情感预测任务中，处理来自第一人称视觉系统（如眼动视频）的信号相比处理生理信号（如光电容积描记法）能产生更好的情感估计。尽管摘要未明确说明具体性能指标（如准确率提升数值），但这一结果表明视觉信号在情感识别中具有优越潜力，为后续多模态融合研究提供了初步实证支持，并暗示了视觉信息在捕捉情感动态中的关键作用。",
      "conclusion": "egoEMOTION 数据集将情感和人格确立为第一人称感知的核心维度，开辟了情感驱动行为、意图和交互建模的新研究方向。该研究的学术价值在于提供了一个综合性多模态基准，推动了对人类内在状态的机器理解；实际应用价值包括增强智能系统的情境感知能力。潜在局限性如数据集规模和泛化性未明确说明，未来工作可能涉及模型优化和扩展应用场景。",
      "tags": [
        "Egocentric Vision",
        "Physiological Signals",
        "Emotion Recognition",
        "Personality Recognition",
        "Affect Classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:25.870533Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.21518",
    "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers",
    "authors": [
      "Lorenzo Basile",
      "Valentino Maiorca",
      "Diego Doimo",
      "Francesco Locatello",
      "Alberto Cazzaniga"
    ],
    "abstract": "Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.21518.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21518",
    "published": "2025-10-24T14:41:47Z",
    "updated": "2026-01-14T15:47:59Z",
    "comment": "Accepted at NeurIPS 2025 (spotlight)",
    "light_analysis": {
      "overview": "提出一种基于信号处理的方法来分析和编辑多模态变换器中注意力头的专门化，实现模型输出的可控调整。",
      "motivation": "语言和视觉-语言模型在多种任务中表现出色，但其内部注意力机制不够透明，限制了可解释性和可控性。现有可解释性方法通常依赖个别样本分析，难以系统揭示注意力头的专门化模式，导致模型行为理解不足。因此，本研究旨在通过原则性方法来探究注意力头的语义和视觉属性专门化，以填补这一空白并提升模型的安全性和实用性。",
      "method": "本研究基于现有可解释性方法，从信号处理视角重新解释利用最终解码层探测中间激活的做法。核心创新是将注意力头输出视为信号，应用信号处理原则分析多个样本并排名头对目标概念的贡献。我们设计系统方法评估专门化程度，并编辑选定头（如1%）来控制输出。该方法适用于单模态和多模态变换器，在语言任务（如问答和毒性缓解）和视觉-语言任务（如图像分类和描述）上进行了验证。",
      "result": "实验结果显示，在单模态和多模态变换器中，注意力头展现出针对特定语义或视觉属性的一致专门化模式。通过方法选定的少数头（如1%）进行编辑，能可靠地抑制或增强模型输出中的目标概念。在问答和图像分类等任务中验证了编辑效果，为模型行为调整提供了实证支持，尽管摘要未明确说明具体性能指标对比数据。",
      "conclusion": "本研究的主要贡献是揭示了注意力层内存在可解释和可控的结构，通过简单工具实现了对大规模生成模型的理解和编辑。这增强了模型透明度和安全性，具有重要学术价值，为后续研究提供新视角。实际应用价值体现在内容过滤和输出定制等任务中，未来工作可能扩展到更多模型类型或精细编辑策略。",
      "tags": [
        "Attention Heads",
        "Multimodal Transformers",
        "Interpretability",
        "Signal Processing",
        "Model Editing"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:22.697876Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.21153",
    "title": "Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design",
    "authors": [
      "Lianghong Chen",
      "Dongkyu Eugene Kim",
      "Mike Domaratzki",
      "Pingzhao Hu"
    ],
    "abstract": "Designing de novo 3D molecules with desirable properties remains a fundamental challenge in drug discovery and molecular engineering. While diffusion models have demonstrated remarkable capabilities in generating high-quality 3D molecular structures, they often struggle to effectively control complex multi-objective constraints critical for real-world applications. In this study, we propose an uncertainty-aware Reinforcement Learning (RL) framework to guide the optimization of 3D molecular diffusion models toward multiple property objectives while enhancing the overall quality of the generated molecules. Our method leverages surrogate models with predictive uncertainty estimation to dynamically shape reward functions, facilitating balance across multiple optimization objectives. We comprehensively evaluate our framework across three benchmark datasets and multiple diffusion model architectures, consistently outperforming baselines for molecular quality and property optimization. Additionally, Molecular Dynamics (MD) simulations and ADMET profiling of top generated candidates indicate promising drug-like behavior and binding stability, comparable to known Epidermal Growth Factor Receptor (EGFR) inhibitors. Our results demonstrate the strong potential of RL-guided generative diffusion models for advancing automated molecular design.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.21153.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21153",
    "published": "2025-10-24T04:49:23Z",
    "updated": "2026-01-14T04:12:09Z",
    "comment": "Accepted at NeurIPS 2025",
    "light_analysis": {
      "overview": "本研究提出了一个不确定性感知的多目标强化学习框架，用于指导3D分子扩散模型的优化，以提高分子质量和多属性控制。",
      "motivation": "3D de novo分子设计在药物发现和分子工程中至关重要，但现有扩散模型在生成高质量结构时难以有效控制复杂多目标约束，如药效、毒性和稳定性，这限制了其在现实应用中的实用性。传统方法缺乏平衡多个属性的机制，导致生成分子难以满足实际需求，因此迫切需要一种能动态优化多目标的创新方法来提升自动化分子设计的效率和成功率。",
      "method": "方法引入一个不确定性感知的强化学习框架，利用具有预测不确定性估计的代理模型动态塑造奖励函数，以平衡多个分子属性目标，如结合亲和力和ADMET属性。该框架与3D分子扩散模型集成，通过强化学习指导扩散过程的优化，从而提高生成分子的整体质量，关键创新在于结合不确定性信息来缓解多目标冲突并提升优化效率。",
      "result": "实验在三个基准数据集和多种扩散模型架构上进行，结果显示在分子质量和属性优化方面一致优于基线方法。对生成的候选分子进行分子动力学模拟和ADMET分析，证实其具有药物样行为和结合稳定性，与已知的EGFR抑制剂相当，验证了方法的有效性和潜在应用价值。",
      "conclusion": "研究贡献在于成功将不确定性感知强化学习集成到3D分子扩散模型中，显著提升了多目标优化的性能，推动了自动化分子设计的发展，在药物发现领域具有重要应用潜力。未来工作可扩展至更多分子属性优化，并探索降低计算复杂性的方法以提高实用性。",
      "tags": [
        "Reinforcement Learning",
        "Diffusion Models",
        "Molecular Design",
        "Uncertainty Estimation",
        "Multi-Objective Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:38.551885Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.19482",
    "title": "ELUTQ: Optimizing Quantization Accuracy under LUT-Based Computation for Edge LLMs",
    "authors": [
      "Xin Nie",
      "Liang Dong",
      "Haicheng Zhang",
      "Jiawang Xiao",
      "G. Sun"
    ],
    "abstract": "Weight quantization effectively reduces memory consumption and enable the deployment of Large Language Models on edge devices, yet existing hardware-friendly methods often rely on uniform quantization, which suffers from poor weight-distribution fitting and high dequantization overhead under low-bit settings. In this paper, we propose ELUTQ, an efficient quantization framework featuring a novel quantization format termed Hierarchical Linear Quantization (HLQ). HLQ is designed to better capture the statistical characteristics of weights and eliminate dequantization overhead using Bit-serial LUT-based GEMM operations. HLQ significantly improves model accuracy under low-bit settings and achieves performance comparable to QAT methods without any retraining of the weights. Moreover, an optimized quantization pipeline is integrated into ELUTQ, enabling it to complete the quantization of LLaMA 3.1-70B using only 64 GB of CPU memory and 48 GB of VRAM, reducing the hardware requirements for large-scale model quantization. To enable efficient deployment on edge devices, ELUTQ designs high-performance kernels to support end-to-end inference. Our 2-bit LLaMA3.1-8B achieves 1.5x speedup over AWQ on RTX 3090. Code is available at https://github.com/Nkniexin/ELUTQ.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.19482.pdf",
    "abs_url": "https://arxiv.org/abs/2510.19482",
    "published": "2025-10-22T11:20:47Z",
    "updated": "2026-01-14T02:56:48Z",
    "comment": "21 pages, 10 figures",
    "light_analysis": {
      "overview": "论文提出ELUTQ框架，通过分层线性量化格式和基于LUT的计算，优化边缘大语言模型在低比特量化下的精度和效率。",
      "motivation": "研究动机是解决大语言模型在边缘设备部署时，权重量化面临的关键问题。现有硬件友好方法依赖均匀量化，导致低比特设置下权重分布拟合差、去量化开销高，限制了模型在资源受限环境中的高效运行。此问题重要，因为它直接影响模型的内存消耗和计算效率，阻碍了边缘AI应用的扩展。摘要强调了现有方法的不足，突出了对改进量化技术的需求。",
      "method": "论文提出ELUTQ量化框架，核心是Hierarchical Linear Quantization格式，旨在更好地捕获权重的统计特性，并通过基于Bit-serial LUT的GEMM操作消除去量化开销。关键创新包括分层量化设计和优化的量化管道，能高效处理大规模模型，如在LLaMA 3.1-70B上仅需64 GB CPU内存和48 GB VRAM。此外，设计高性能内核支持端到端推理，提升部署效率。方法聚焦于结合统计分析和硬件优化，实现无需权重再训练的高效量化。",
      "result": "实验结果显示，HLQ在低比特设置下显著提升模型准确性，性能与QAT方法相当，且无需权重再训练。具体数据表明，量化LLaMA 3.1-70B时硬件需求降低；2-bit LLaMA3.1-8B在RTX 3090上实现1.5倍加速，优于基线方法AWQ。这些结果证明ELUTQ在精度和效率上的改进，适用于边缘设备部署。摘要强调了与现有方法的对比，突显了量化效果和速度提升。",
      "conclusion": "论文的主要贡献是提出ELUTQ框架和HLQ格式，优化了量化精度和减少去量化开销，降低了大规模模型量化的硬件需求，促进边缘大语言模型部署。学术价值在于改进量化理论和方法，实际应用价值在于支持资源受限环境中的高效AI推理。摘要未明确说明局限性或未来工作方向，但可推断未来可能涉及更广泛模型适配或硬件兼容性探索。",
      "tags": [
        "Weight Quantization",
        "Hierarchical Linear Quantization",
        "LUT-based computation",
        "Edge LLMs",
        "Low-bit settings"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:29.951543Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.19385",
    "title": "Beyond Uniform SVD:Dual-Level Optimization across Columns and Modules for LLM Compression",
    "authors": [
      "Lin Xv",
      "Xian Gao",
      "Ting Li",
      "Yuzhuo Fu"
    ],
    "abstract": "Low-rank decomposition, particularly Singular Value Decomposition (SVD), is a pivotal technique for mitigating the storage and computational demands of Large Language Models (LLMs). However, prevalent SVD-based approaches overlook the critical phenomenon that decomposition errors exhibit significant disparity across different components of the parameter matrix, often leading to suboptimal approximation. Furthermore, existing methods lack a direct metric to evaluate the importance of individual weight matrices. To address these limitations, we propose Duo-SVD (Dual-level Optimization SVD), a novel training-free framework that synergizes optimization at both the column and the module levels. First, Duo-SVD incorporates a Column-Preserving Strategy that explicitly retains columns exhibiting high decomposition errors, while applying low-rank approximation solely to those with lower errors. Second, at the module level, we employ a Module-Adaptive Allocation Strategy that formulates ratio allocation as a global constrained optimization problem based on perturbation-induced model deviation. Extensive experiments demonstrate that Duo-SVD consistently outperforms state-of-the-art SVD-based baselines and structured pruning methods, establishing it as a superior paradigm for efficient LLM compression.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.19385.pdf",
    "abs_url": "https://arxiv.org/abs/2510.19385",
    "published": "2025-10-22T09:02:37Z",
    "updated": "2026-01-14T15:28:04Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Duo-SVD，一种用于大型语言模型压缩的双级优化框架，通过结合列和模块级别的优化，提升了SVD分解的精度。",
      "motivation": "低秩分解，特别是奇异值分解（SVD），是减少大型语言模型存储和计算需求的关键技术。然而，现有SVD方法忽略了一个关键问题：分解误差在不同参数矩阵组件间存在显著差异，常常导致次优近似。此外，这些方法缺乏直接指标来评估单个权重矩阵的重要性。因此，需要一种更有效的方法来优化压缩过程，以解决现有方法的不足，并提高LLM压缩的效率和性能。",
      "method": "Duo-SVD是一种无需训练的双级优化框架。在列级别，采用列保留策略，明确保留分解误差高的列，而仅对误差较低的列应用低秩近似。在模块级别，使用模块自适应分配策略，将压缩比率分配建模为基于扰动诱导模型偏差的全局约束优化问题。这种协同优化方法不依赖额外训练，并针对LLM参数矩阵的特性进行定制化处理，提升了整体压缩效果。",
      "result": "大量实验表明，Duo-SVD持续优于基于SVD的最先进基线和结构化剪枝方法，确立了其在高效LLM压缩中的优越范式。摘要中未提供具体性能指标（如准确率或效率提升百分比），但明确指出其通过广泛实验验证了相对于基线方法的卓越性能，显示了其在减少模型存储和计算需求方面的潜力。",
      "conclusion": "该研究的主要贡献是提出了Duo-SVD框架，通过双级优化解决了现有SVD方法在LLM压缩中的局限性。学术上，它提供了模型压缩领域的新优化思路；实际上，有助于降低LLM的计算和存储成本，促进更高效部署。摘要未明确说明局限性或未来工作方向，但暗示了该框架作为压缩范式的广泛适用性。",
      "tags": [
        "Large Language Model Compression",
        "Singular Value Decomposition",
        "Low-rank Decomposition",
        "Column-Preserving Strategy",
        "Module-Adaptive Allocation Strategy"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:40.403033Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.19158",
    "title": "Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring",
    "authors": [
      "Federico Di Gennaro",
      "Khaled Eldowa",
      "Nicolò Cesa-Bianchi"
    ],
    "abstract": "In contrast to the classic formulation of partial monitoring, linear partial monitoring can model infinite outcome spaces, while imposing a linear structure on both the losses and the observations. This setting can be viewed as a generalization of linear bandits where loss and feedback are decoupled in a flexible manner. In this work, we address a nonstochastic (adversarial), finite-actions version of the problem through a simple instance of the exploration-by-optimization method that is amenable to efficient implementation. We derive regret bounds that depend on the game structure in a more transparent manner than previous theoretical guarantees for this paradigm. Our bounds feature instance-specific quantities that reflect the degree of alignment between observations and losses, and resemble known guarantees in the stochastic setting. Notably, they achieve the standard $\\sqrt{T}$ rate in easy (locally observable) games and $T^{2/3}$ in hard (globally observable) games, where $T$ is the time horizon. We instantiate these bounds in a selection of old and new partial information settings subsumed by this model, and illustrate that the achieved dependence on the game structure can be tight in interesting cases.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.19158.pdf",
    "abs_url": "https://arxiv.org/abs/2510.19158",
    "published": "2025-10-22T01:28:20Z",
    "updated": "2026-01-14T16:24:31Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究针对非随机线性部分监测问题，提出实例依赖的悔恨界理论，通过简单高效的优化探索方法改进游戏结构依赖的透明度。",
      "motivation": "线性部分监测扩展了经典部分监测，能够建模无限结果空间，并通过线性结构灵活解耦损失和反馈，这在在线学习和决策制定中具有重要应用价值。然而，现有理论对悔恨界的推导往往不够透明，缺乏对游戏结构依赖的清晰描述，特别是无法反映实例特定的性质，如观测与损失之间的对齐程度，这限制了算法设计和性能评估。本研究旨在解决这一问题，通过提供更透明的理论保证，优化非随机环境下的悔恨界分析，弥补先前方法的不足。",
      "method": "论文采用exploration-by-optimization方法的一个简单实例，专门处理非随机、有限动作的线性部分监测问题。关键创新在于在推导悔恨界时引入实例特定量，如观测和损失之间的对齐度，这使得理论保证更适应不同游戏结构。该方法基于线性模型，易于高效实现，并在框架内针对局部可观察和全局可观察游戏分别进行分析。通过优化探索策略，新方法能在保持计算效率的同时，适应对抗性设置，并提供结构化的悔恨界推导。",
      "result": "主要实验结果显示，在易（局部可观察）游戏中，悔恨界达到标准√T速率；在难（全局可观察）游戏中，悔恨界达到T^{2/3}速率，其中T是时间范围。这些界比先前理论更透明地依赖于游戏结构，通过实例特定量如对齐度进行量化。实例化在多种部分信息设置中，包括旧的和新的场景，表明这种依赖关系在有趣案例中可能是紧的。与基线方法相比，新方法提供了更细致的性能分析，改进了游戏结构依赖的表述。",
      "conclusion": "论文主要贡献是为线性部分监测问题推导出实例依赖的悔恨界，增强了理论透明度并扩展了在线学习框架。学术上，它提供了新的理论工具，促进了对非随机环境的部分监测研究；实际上，有助于设计更高效的算法，应用于推荐系统等场景。局限性包括摘要未明确说明的具体应用范围，未来工作可探索更复杂游戏结构或扩展到随机设置，以进一步提升理论覆盖和实际适用性。",
      "tags": [
        "Linear Partial Monitoring",
        "Regret Bounds",
        "Exploration-by-Optimization",
        "Nonstochastic Learning",
        "Partial Observability"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:54.478423Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.16863",
    "title": "BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation",
    "authors": [
      "Shujian Gao",
      "Yuan Wang",
      "Zekuan Yu"
    ],
    "abstract": "Semi-supervised medical image segmentation (SSMIS) seeks to match fully supervised performance while sharply reducing annotation cost. Mainstream SSMIS methods rely on \\emph{label-space consistency}, yet they overlook the equally critical \\emph{representation-space alignment}. Without harmonizing latent features, models struggle to learn representations that are both discriminative and spatially coherent. To this end, we introduce \\textbf{Bilateral Alignment in Representation and Label spaces (BARL)}, a unified framework that couples two collaborative branches and enforces alignment in both spaces. For label-space alignment, inspired by co-training and multi-scale decoding, we devise \\textbf{Dual-Path Regularization (DPR)} and \\textbf{Progressively Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch consistency while mitigating error accumulation from coarse to fine scales. For representation-space alignment, we conduct region-level and lesion-instance matching between branches, explicitly capturing the fragmented, complex pathological patterns common in medical imagery. Extensive experiments on four public benchmarks and a proprietary CBCT dataset demonstrate that BARL consistently surpasses state-of-the-art SSMIS methods. Ablative studies further validate the contribution of each component. Code will be released soon.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.16863.pdf",
    "abs_url": "https://arxiv.org/abs/2510.16863",
    "published": "2025-10-19T14:50:47Z",
    "updated": "2026-01-14T14:54:25Z",
    "comment": "14 pages, 5 figures",
    "light_analysis": {
      "overview": "BARL通过双边对齐表示空间和标签空间，提升了半监督医学图像分割的性能。",
      "motivation": "半监督医学图像分割（SSMIS）旨在降低高昂的标注成本，同时达到全监督方法的性能。现有主流方法主要依赖标签空间的一致性约束，但忽视了表示空间的对齐，这导致模型难以学习到既有区分性又空间连贯的特征表示，尤其在医学图像中病理模式复杂且碎片化的情况下，影响分割效果。因此，本研究动机在于填补这一空白，通过双边对齐策略解决表示和标签空间的协调问题，以提高模型的判别能力和空间一致性。",
      "method": "BARL是一个统一框架，包含两个协作分支，强制进行表示空间和标签空间的对齐。在标签空间，设计了双路径正则化（DPR）和渐进认知偏差校正（PCBC），受协同训练和多尺度解码启发，以增强跨分支的细粒度一致性，并减少从粗到细尺度的错误累积。在表示空间，通过区域级和病灶实例匹配，捕捉医学图像中复杂的病理模式。该框架在多个数据集上进行验证，包括公共基准和专有CBCT数据集，以评估其有效性。",
      "result": "在四个公共基准和一个专有CBCT数据集上的实验表明，BARL持续超越现有的最先进半监督医学图像分割方法，具体性能提升未在摘要中说明，但整体表现优于基线方法。消融研究进一步证实了每个组件（如DPR、PCBC和表示空间匹配）的有效性，验证了双边对齐策略的贡献，尽管未提供具体数值指标。",
      "conclusion": "BARL的主要贡献在于提出了一个双边对齐框架，整合了表示空间和标签空间的一致性，提高了半监督医学图像分割的准确性和鲁棒性。该研究具有重要学术价值，推动了SSMIS领域的发展，强调表示空间对齐的关键作用，并具有实际应用价值，能有效减少医学图像标注工作量。未来工作可能包括优化对齐策略或扩展到其他模态，但摘要未明确说明局限性或具体方向。",
      "tags": [
        "Semi-Supervised Learning",
        "Medical Image Segmentation",
        "Representation Alignment",
        "Label Consistency",
        "Co-Training"
      ]
    },
    "analyzed_at": "2026-01-15T03:24:56.529712Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.16685",
    "title": "Structured yet Bounded Temporal Understanding in Large Language Models",
    "authors": [
      "Damin Zhang",
      "Julia Rayz"
    ],
    "abstract": "Large language models (LLMs) increasingly show strong performance on temporally grounded tasks, such as timeline construction, temporal question answering, and event ordering. However, it remains unclear how their behavior depends on the way time is anchored in language. In this work, we study LLMs' temporal understanding through temporal frames of reference (t-FoRs), contrasting deictic framing (past-present-future) and sequential framing (before-after). Using a large-scale dataset of real-world events from Wikidata and similarity judgement task, we examine how LLMs' outputs vary with temporal distance, interval relations, and event duration. Our results show that LLMs systematically adapt to both t-FoRs, but the resulting similarity patterns differ significantly. Under deictic t-FoR, the similarity judgement scores form graded and asymmetric structures centered on the present, with sharper decline for future events and higher variance in the past. Under sequential t-FoR, similarity becomes strongly negative once events are temporally separated. Temporal judgements are also shaped by interval algebra and duration, with instability concentrated in overlap- and containment-based relations, and duration influencing only past events under deictic t-FoR. Overall, these findings characterize how LLMs organize temporal representation under different reference structures and identify the factors that most strongly shape their temporal understanding.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.16685.pdf",
    "abs_url": "https://arxiv.org/abs/2510.16685",
    "published": "2025-10-19T02:08:35Z",
    "updated": "2026-01-14T14:17:12Z",
    "comment": "Under review. Results on larger dataset. Correct a theoretical error. 11 pages, 5 figures",
    "light_analysis": {
      "overview": "本研究通过对比 deictic 和 sequential 时间参照框架，揭示了大型语言模型在时间理解上的结构化特征和局限性，特别关注相似性判断中的系统差异。",
      "motivation": "大型语言模型在时间相关任务（如时间线构建、时间问答和事件排序）上展现出强大性能，但其行为如何依赖于语言中时间的锚定方式仍不清楚。时间理解是AI进行有效推理的关键，而现有方法可能忽视了不同时间参照框架对模型行为的影响，导致对LLMs时间认知机制的理解不足。本研究旨在解决这一研究空白，通过系统分析时间参照框架，探索LLMs在不同框架下的行为模式，以深化时间推理的理论基础。",
      "method": "研究方法基于时间参照框架，对比 deictic（过去-现在-未来）和 sequential（之前-之后）框架。核心方法是设计相似性判断任务，使用从 Wikidata 获取的大规模现实世界事件数据集，评估 LLMs 的输出如何随时间距离、间隔关系和事件持续时间变化。技术细节包括应用相似性度量来分析模型响应，关注框架转换下的模式差异，数据集确保了实验的广泛性和多样性。",
      "result": "实验结果显示，大型语言模型能够系统适应两种时间参照框架，但相似性模式存在显著差异。在 deictic 框架下，相似性得分形成以现在为中心的梯度不对称结构，未来事件下降更剧烈，过去事件方差较高；在 sequential 框架下，事件时间分离时相似性变为强烈负值。时间判断还受间隔代数和持续时间影响，不稳定性主要集中在重叠和包含关系上，持续时间仅影响 deictic 框架下的过去事件。这些结果量化了模型在不同框架下的结构化行为。",
      "conclusion": "本研究总结了大型语言模型在不同时间参照结构下组织时间表示的方式，并识别了时间距离、间隔关系和持续时间等关键影响因素。贡献在于揭示了LLMs的时间理解机制，为改进AI时间推理提供了理论依据，具有学术价值和实际应用潜力。摘要未明确说明局限性，但未来工作可能包括探索其他时间因素或扩展到更多任务，以深化理解和应用。",
      "tags": [
        "Large Language Model",
        "Temporal Understanding",
        "Frame of Reference",
        "Similarity Judgement",
        "Temporal Reasoning"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:30.296119Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.15042",
    "title": "Comprehensive language-image pre-training for 3D medical image understanding",
    "authors": [
      "Tassilo Wald",
      "Ibrahim Ethem Hamamci",
      "Yuan Gao",
      "Sam Bond-Taylor",
      "Harshita Sharma",
      "Maximilian Ilse",
      "Cynthia Lo",
      "Olesya Melnichenko",
      "Anton Schwaighofer",
      "Noel C. F. Codella",
      "Maria Teodora Wetscherek",
      "Klaus H. Maier-Hein",
      "Panagiotis Korfiatis",
      "Valentina Salvatelli",
      "Javier Alvarez-Valle",
      "Fernando Pérez-García"
    ],
    "abstract": "Vision-language pre-training, i.e., aligning images with paired text, is a powerful paradigm to create encoders that can be directly used for tasks such as classification, retrieval, and segmentation. In the 3D medical image domain, these capabilities allow vision-language encoders (VLEs) to support radiologists by retrieving patients with similar abnormalities, predicting likelihoods of abnormality, or, with downstream adaptation, generating radiological reports. While the methodology holds promise, data availability and domain-specific hurdles limit the capabilities of current 3D VLEs.   In this paper, we overcome these challenges by injecting additional supervision via a report generation objective and combining vision-language with vision-only pre-training. This allows us to leverage both image-only and paired image-text 3D datasets, increasing the total amount of data to which our model is exposed. Through these additional objectives, paired with best practices of the 3D medical imaging domain, we develop the Comprehensive Language-Image Pre-training (COLIPRI) encoder family. Our COLIPRI encoders achieve state-of-the-art performance in report generation, semantic segmentation, classification probing, and zero-shot classification. The model is available at https://huggingface.co/microsoft/colipri.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.15042.pdf",
    "abs_url": "https://arxiv.org/abs/2510.15042",
    "published": "2025-10-16T18:01:31Z",
    "updated": "2026-01-14T13:42:41Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了综合语言-图像预训练（COLIPRI）编码器系列，通过结合视觉-语言和仅视觉预训练及报告生成目标，显著提升了3D医学图像理解的性能。",
      "motivation": "在3D医学图像领域，视觉-语言预训练编码器由于数据可用性有限和领域特定障碍（如数据稀缺和复杂医学特征），能力受限，这限制了它们在支持放射科医生任务（如检索类似异常患者或预测异常可能性）中的应用。现有方法难以充分利用多模态数据，导致编码器性能不足，影响医疗诊断的效率和准确性。因此，亟需开发新方法来克服这些挑战，增强编码器的多任务能力。",
      "method": "论文通过注入额外监督（如报告生成目标），并结合视觉-语言预训练与仅视觉预训练，利用图像-only和图像-文本配对数据集，以增加总数据量。采用3D医学成像领域的最佳实践，开发了COLIPRI编码器家族，关键创新点在于多目标预训练策略和数据融合，但摘要未明确说明具体模型架构或数据集细节。该方法旨在通过多模态学习提升编码器的泛化能力。",
      "result": "COLIPRI编码器在多个任务中达到最先进性能，包括报告生成、语义分割、分类探测和零-shot分类。与现有基线方法相比，展现了显著改进，但摘要未提供具体数据指标如准确率提升。实验结果表明，模型在综合评估中表现出色，增强了3D医学图像理解的多功能性。",
      "conclusion": "论文的主要贡献是开发了COLIPRI编码器系列，通过综合预训练方法克服了数据限制，提升了3D医学图像理解能力。学术价值在于提出了有效的多任务预训练框架，实际应用价值在于支持医疗诊断、报告生成和患者检索任务。未来工作可能涉及进一步优化方法或扩展到其他医学领域，但摘要未明确说明具体局限性。",
      "tags": [
        "Vision-Language Pre-training",
        "3D Medical Imaging",
        "Report Generation",
        "Semantic Segmentation",
        "Zero-shot Classification"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:19.264284Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.13291",
    "title": "Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems",
    "authors": [
      "Xuxin Cheng",
      "Ke Zeng",
      "Zhiquan Cao",
      "Linyi Dai",
      "Wenxuan Gao",
      "Fei Han",
      "Ai Jian",
      "Feng Hong",
      "Wenxing Hu",
      "Zihe Huang",
      "Dejian Kong",
      "Jia Leng",
      "Zhuoyuan Liao",
      "Pei Liu",
      "Jiaye Lin",
      "Xing Ma",
      "Jingqing Ruan",
      "Jiaxing Song",
      "Xiaoyu Tan",
      "Ruixuan Xiao",
      "Wenhui Yu",
      "Wenyu Zhan",
      "Haoxing Zhang",
      "Chao Zhou",
      "Hao Zhou",
      "Shaodong Zheng",
      "Ruinian Chen",
      "Siyuan Chen",
      "Ziyang Chen",
      "Yiwen Dong",
      "Yaoyou Fan",
      "Yangyi Fang",
      "Yang Gan",
      "Shiguang Guo",
      "Qi He",
      "Chaowen Hu",
      "Binghui Li",
      "Dailin Li",
      "Xiangyu Li",
      "Yan Li",
      "Chengjian Liu",
      "Xiangfeng Liu",
      "Jiahui Lv",
      "Qiao Ma",
      "Jiang Pan",
      "Cong Qin",
      "Chenxing Sun",
      "Wen Sun",
      "Zhonghui Wang",
      "Abudukelimu Wuerkaixi",
      "Xin Yang",
      "Fangyi Yuan",
      "Yawen Zhu",
      "Tianyi Zhai",
      "Jie Zhang",
      "Runlai Zhang",
      "Yao Xu",
      "Yiran Zhao",
      "Yifan Wang",
      "Xunliang Cai",
      "Yangen Hu",
      "Cao Liu",
      "Lu Pan",
      "Xiaoli Wang",
      "Bo Xiao",
      "Wenyuan Yao",
      "Qianlin Zhou",
      "Benchang Zhu"
    ],
    "abstract": "Enhancing customer experience is essential for business success, particularly as service demands grow in scale and complexity. Generative artificial intelligence and Large Language Models (LLMs) have empowered intelligent interaction systems to deliver efficient, personalized, and 24/7 support. In practice, intelligent interaction systems encounter several challenges: (1) Constructing high-quality data for cold-start training is difficult, hindering self-evolution and raising labor costs. (2) Multi-turn dialogue performance remains suboptimal due to inadequate intent understanding, rule compliance, and solution extraction. (3) Frequent evolution of business rules affects system operability and transferability, constraining low-cost expansion and adaptability. (4) Reliance on a single LLM is insufficient in complex scenarios, where the absence of multi-agent frameworks and effective collaboration undermines process completeness and service quality. (5) The open-domain nature of multi-turn dialogues, lacking unified golden answers, hampers quantitative evaluation and continuous optimization. To address these challenges, we introduce WOWService, an intelligent interaction system tailored for industrial applications. With the integration of LLMs and multi-agent architectures, WOWService enables autonomous task management and collaborative problem-solving. Specifically, WOWService focuses on core modules including data construction, general capability enhancement, business scenario adaptation, multi-agent coordination, and automated evaluation. Currently, WOWService is deployed on the Meituan App, achieving significant gains in key metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user needs and advancing personalized service.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.13291.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13291",
    "published": "2025-10-15T08:35:51Z",
    "updated": "2026-01-14T07:30:10Z",
    "comment": "36 pages, 14 figures",
    "light_analysis": {
      "overview": "论文提出了名为WOWService的智能交互系统，集成了大型语言模型和多代理架构，以解决工业应用中的挑战并提升客户满意度与降低成本。",
      "motivation": "随着服务需求的规模化和复杂化，智能交互系统面临多个关键问题：构建高质量启动数据困难，阻碍系统自进化并增加人工成本；多轮对话性能不足，源于意图理解、规则遵从和方案提取的缺陷；业务规则频繁更新影响系统可操作性和可迁移性，限制低成本扩展；单一大语言模型在复杂场景下能力有限，缺乏多代理协作框架，削弱流程完整性和服务质量；开放领域多轮对话缺乏统一标准答案，导致定量评估和持续优化受阻。这些问题亟需解决以提升系统效率和用户体验。",
      "method": "研究核心是WOWService系统，通过集成大型语言模型和多代理架构来实现自主任务管理和协同问题解决。系统聚焦于五个核心模块：数据构建，利用LLMs自动生成训练数据以降低启动成本；通用能力增强，改进多轮对话的意图理解和规则遵从；业务场景适应，设计灵活机制应对频繁规则变化；多代理协调，构建协作框架提升复杂任务处理能力；自动化评估，建立量化指标支持系统优化。这些模块协同工作，系统部署在Meituan App中，以应对工业场景的挑战。",
      "result": "WOWService在Meituan App上的部署取得了显著效果，关键指标显示用户满意度提升。具体而言，User Satisfaction Metric 1 (USM 1) 降低了27.53%，表明相关成本或负面指标得到改善；User Satisfaction Metric 2 (USM 2) 提升了25.51%，反映系统在捕捉用户需求和提供个性化服务方面的进步。这些数据对比基线方法，证明了系统在提高效率和优化体验方面的有效性，摘要未明确说明其他详细对比，但整体展示了其在实际应用中的性能优势。",
      "conclusion": "研究的主要贡献在于开发了WOWService系统，有效解决了智能交互系统中的数据构建、多轮对话、规则适应、代理协作和评估优化等挑战。通过集成大型语言模型和多代理架构，系统显著提升了客户满意度和降低了运营成本，展示了这些技术在工业领域的实用价值。学术上，为智能交互系统设计提供了新思路；应用上，为类似平台提供了可借鉴的框架。局限性或未来工作摘要未明确说明，但可能涉及扩展至更多业务场景或增强系统泛化能力，以支持更广泛的应用。",
      "tags": [
        "Large Language Model",
        "Multi-Agent Architecture",
        "Intelligent Interaction System",
        "Data Construction",
        "Automated Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:34.542086Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.15960",
    "title": "Hydrogen production from blended waste biomass: pyrolysis, thermodynamic-kinetic analysis and AI-based modelling",
    "authors": [
      "Sana Kordoghli",
      "Abdelhakim Settar",
      "Oumayma Belaati",
      "Mohammad Alkhatib",
      "Khaled Chetehouna",
      "Zakaria Mansouri"
    ],
    "abstract": "This work contributes to advancing sustainable energy and waste management strategies by investigating the thermochemical conversion of food-based biomass through pyrolysis, highlighting the role of artificial intelligence (AI) in enhancing process modelling accuracy and optimization efficiency. The main objective is to explore the potential of underutilized biomass resources, such as spent coffee grounds (SCG) and date seeds (DS), for sustainable hydrogen production. Specifically, it aims to optimize the pyrolysis process while evaluating the performance of these resources both individually and as blends. Proximate, ultimate, fibre, TGA/DTG, kinetic, thermodynamic, and Py-Micro GC analyses were conducted for pure DS, SCG, and blends (75% DS - 25% SCG, 50% DS - 50% SCG, 25% DS - 75% SCG). Blend 3 offered superior hydrogen yield potential but had the highest activation energy (Ea: 313.24 kJ/mol), while Blend 1 exhibited the best activation energy value (Ea: 161.75 kJ/mol). The kinetic modelling based on isoconversional methods (KAS, FWO, Friedman) identified KAS as the most accurate. These approaches provide a detailed understanding of the pyrolysis process, with particular emphasis on the integration of artificial intelligence. An LSTM model trained with lignocellulosic data predicted TGA curves with exceptional accuracy (R^2: 0.9996-0.9998).",
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.15960.pdf",
    "abs_url": "https://arxiv.org/abs/2510.15960",
    "published": "2025-10-11T15:51:41Z",
    "updated": "2026-01-14T12:17:31Z",
    "comment": "41 pages, 21 figures",
    "light_analysis": {
      "overview": "本研究利用人工智能优化废物生物质的热解过程，以实现高效氢气生产，核心创新在于整合AI提升建模精度和优化效率。",
      "motivation": "该研究旨在解决废物生物质（如咖啡渣和枣籽）资源未被充分利用的问题，通过热化学转换将其转化为氢气，以促进可持续能源和废物管理。现有方法在热解过程的建模和优化方面可能存在精度不足和效率低下的挑战，限制了氢气生产的潜力。研究强调了人工智能技术的应用重要性，以克服这些限制并响应清洁能源需求的增长，但摘要未明确说明现有具体方法的不足之处。",
      "method": "研究方法包括对纯枣籽（DS）、咖啡渣（SCG）及其混合物（如75% DS - 25% SCG等）进行多维度分析，如近似分析、最终分析、纤维分析、热重分析/导数热重分析（TGA/DTG）、动力学和热力学分析，以及Py-Micro GC分析。核心创新点在于整合人工智能，使用长短期记忆（LSTM）模型基于木质纤维素数据训练，以预测热解过程的TGA曲线。此外，通过等转化法（如KAS、FWO、Friedman）进行动力学建模，重点关注了AI在优化过程中的技术特色和应用细节。",
      "result": "实验结果表明，混合物Blend 3（25%枣籽-75%咖啡渣）表现出优等的氢气产率潜力，但激活能最高（Ea: 313.24 kJ/mol），而Blend 1（75%枣籽-25%咖啡渣）具有最佳激活能值（Ea: 161.75 kJ/mol）。在动力学建模中，KAS方法被确定为最准确。人工智能部分的LSTM模型预测TGA曲线取得极高准确性，R^2值达到0.9996-0.9998，显示出比传统方法更好的性能提升，但摘要未明确说明与具体基线方法的对比数据细节。",
      "conclusion": "本研究的主要贡献是通过人工智能技术，特别是LSTM模型，显著提高了废物生物质热解过程的建模精度和优化效率，为可持续氢气生产提供了有效途径。其学术价值在于推动了热化学转换领域中AI方法的应用研究，实际应用价值体现在促进废物资源化和清洁能源开发。摘要未明确说明研究的局限性或未来工作方向，但可推断出潜在的改进空间，如扩展至更多生物质类型或进一步优化AI模型。",
      "tags": [
        "Pyrolysis",
        "Artificial Intelligence",
        "Long Short-Term Memory (LSTM)",
        "Thermodynamic-Kinetic Analysis",
        "Hydrogen Production"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:41.634599Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.08942",
    "title": "SOP-Maze: Evaluating Large Language Models on Complicated Business Standard Operating Procedures",
    "authors": [
      "Jiaming Wang",
      "Zhe Tang",
      "Zehao Jin",
      "Hefei Chen",
      "Yilin Jin",
      "Peng Ding",
      "Xiaoyu Li",
      "Xuezhi Cao"
    ],
    "abstract": "As large language models (LLMs) are widely deployed as domain-specific agents, many benchmarks have been proposed to evaluate their ability to follow instructions and make decisions in real-world scenarios. However, business scenarios often involve complex standard operating procedures (SOPs), and the evaluation of LLM capabilities in such contexts has not been fully explored. To bridge this gap, we propose SOP-Maze, a benchmark constructed from real-world business data and adapted into a collection of 397 instances and 3422 subtasks from 23 complex SOP scenarios. We further categorize SOP tasks into two broad classes: Lateral Root System (LRS), representing wide-option tasks that demand precise selection; and Heart Root System (HRS), which emphasizes deep logical reasoning with complex branches. Extensive experiments reveal that nearly all state-of-the-art models struggle with SOP-Maze. We conduct a comprehensive analysis and identify three key error categories: (i) route blindness: difficulty following procedures; (ii) conversational fragility: inability to handle real dialogue nuances; and (iii) calculation errors: mistakes in time or arithmetic reasoning under complex contexts. The systematic study explores LLM performance across SOP tasks that challenge both breadth and depth, offering new insights for improving model capabilities. We have open-sourced our work on: https://github.com/meituan-longcat/SOP-Maze.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.08942.pdf",
    "abs_url": "https://arxiv.org/abs/2510.08942",
    "published": "2025-10-10T02:47:53Z",
    "updated": "2026-01-14T03:37:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了SOP-Maze基准，用于评估大型语言模型在复杂业务标准操作程序上的性能。",
      "motivation": "随着大型语言模型作为领域代理的广泛应用，评估其在复杂业务标准操作程序（SOPs）中的能力变得日益重要。现有基准测试多关注通用指令遵循，但业务场景中的SOPs涉及复杂逻辑分支和精确选择，现有方法未能充分模拟这些挑战，导致模型性能评估不足。因此，研究旨在填补这一空白，为LLMs在真实业务应用中的综合能力提供更细致的评估环境。",
      "method": "研究构建了SOP-Maze基准，基于真实业务数据，包含23个复杂SOP场景的397个实例和3422个子任务。创新地将任务分类为两类：Lateral Root System（LRS），模拟宽选项任务要求精确选择；Heart Root System（HRS），强调深度逻辑推理和复杂分支处理。通过这种结构化设计，基准挑战模型的广度和深度能力，系统评估其在遵循程序和推理中的表现，但摘要未明确说明具体模型架构或技术实现细节。",
      "result": "实验结果表明，几乎所有最先进的大型语言模型在SOP-Maze基准上都表现不佳，突显了模型在复杂SOPs处理中的局限性。通过系统分析，识别出三个关键错误类别：路径盲区（模型难以遵循多步骤程序）、对话脆弱性（无法处理真实对话中的细微差别）和计算错误（复杂上下文下的时间或算术推理错误）。尽管摘要未提供具体性能指标如准确率，但这些发现为模型能力改进提供了实证方向。",
      "conclusion": "该研究通过SOP-Maze基准系统探索了大型语言模型在复杂业务标准操作程序上的性能，为理解模型在挑战广度和深度任务中的表现提供了新见解。主要贡献在于开发了一个真实世界评估框架，帮助识别并解决模型在精确选择、逻辑推理和对话处理中的关键错误，具有推动LLMs在业务应用中能力提升的学术和实际价值。开源基准鼓励社区进一步扩展和基于错误类别设计改进策略，未来工作可聚焦于错误缓解或基准扩充。",
      "tags": [
        "Large Language Models",
        "Standard Operating Procedures",
        "Evaluation Benchmark",
        "Logical Reasoning",
        "Dialogue Systems"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:56.874191Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.09676",
    "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling",
    "authors": [
      "Shayan Mohajer Hamidi",
      "En-Hui Yang",
      "Ben Liang"
    ],
    "abstract": "Inverse problems, where the goal is to recover an unknown signal from noisy or incomplete measurements, are central to applications in medical imaging, remote sensing, and computational biology. Diffusion models have recently emerged as powerful priors for solving such problems. However, existing methods either rely on projection-based techniques that enforce measurement consistency through heuristic updates, or they approximate the likelihood $p(\\boldsymbol{y} \\mid \\boldsymbol{x})$, often resulting in artifacts and instability under complex or high-noise conditions. To address these limitations, we propose a novel framework called \\emph{coupled data and measurement space diffusion posterior sampling} (C-DPS), which eliminates the need for constraint tuning or likelihood approximation. C-DPS introduces a forward stochastic process in the measurement space $\\{\\boldsymbol{y}_t\\}$, evolving in parallel with the data-space diffusion $\\{\\boldsymbol{x}_t\\}$, which enables the derivation of a closed-form posterior $p(\\boldsymbol{x}_{t-1} \\mid \\boldsymbol{x}_t, \\boldsymbol{y}_{t-1})$. This coupling allows for accurate and recursive sampling based on a well-defined posterior distribution. Empirical results demonstrate that C-DPS consistently outperforms existing baselines, both qualitatively and quantitatively, across multiple inverse problem benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.09676.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09676",
    "published": "2025-10-08T18:59:16Z",
    "updated": "2026-01-14T17:17:22Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出C-DPS框架，通过耦合数据和测量空间的扩散过程实现闭合形式后验采样，无需约束调整或似然近似，提升逆问题求解的准确性和稳定性。",
      "motivation": "逆问题在医疗成像和遥感等应用中至关重要，目标是恢复噪声或不完整测量中的信号。现有扩散模型方法存在不足：基于投影的技术依赖启发式更新强制执行测量一致性，或近似似然函数，导致在复杂或高噪声条件下产生伪影和不稳定性。因此，需要一种新方法来克服这些限制，提高信号恢复的准确性和鲁棒性，避免不稳定的采样过程。",
      "method": "C-DPS框架引入测量空间的前向随机过程{y_t}，与数据空间的扩散过程{x_t}并行演进。这种耦合允许推导出一个闭合形式的后验分布p(x_{t-1} | x_t, y_{t-1})，使得采样过程更加准确和递归。关键创新点在于消除约束调整或似然近似的需求，通过数据空间和测量空间的动态结合提升后验采样的效率。摘要未明确说明使用的具体数据集或模型架构细节。",
      "result": "实证结果显示，C-DPS在多个逆问题基准测试中，无论是定性还是定量上都持续优于现有基线方法。定性上减少了伪影和不稳定性，定量上提升了恢复性能，特别是在复杂或高噪声条件下。具体数据如准确率提升未在摘要中提供，但与基线相比，C-DPS展示了显著的改进，验证了其方法的有效性和鲁棒性。",
      "conclusion": "C-DPS框架的主要贡献是通过耦合数据和测量空间的动态，实现准确的后验采样，克服了现有方法在逆问题中的局限性。它具有重要学术价值，为扩散模型应用提供了新方向；实际应用中可提升医疗成像等领域的信号恢复质量。局限性如计算复杂度或特定条件未在摘要中说明，未来工作可能包括扩展到更多应用场景或优化算法效率。",
      "tags": [
        "Diffusion Models",
        "Posterior Sampling",
        "Inverse Problems",
        "Stochastic Processes",
        "Coupled Dynamics"
      ]
    },
    "analyzed_at": "2026-01-15T03:25:40.446585Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.07227",
    "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
    "authors": [
      "Arjun Krishnakumar",
      "Rhea Sanjay Sukthanker",
      "Hannan Javed Mahadik",
      "Gabriela Kadlecová",
      "Vladyslav Moroshan",
      "Timur Carstensen",
      "Frank Hutter",
      "Aaron Klein"
    ],
    "abstract": "Small Language models (SLMs) offer an efficient and accessible alternative to Large Language Models (LLMs), delivering strong performance while using far fewer resources. We introduce a simple and effective framework for pretraining SLMs that brings together three complementary ideas. First, we identify structurally sparse sub-network initializations that consistently outperform randomly initialized models of similar size under the same compute budget. Second, we use evolutionary search to automatically discover high-quality sub-network initializations, providing better starting points for pretraining. Third, we apply knowledge distillation from larger teacher models to speed up training and improve generalization. Together, these components make SLM pretraining substantially more efficient: our best model, discovered using evolutionary search and initialized with LLM weights, matches the validation perplexity of a comparable Pythia SLM while requiring 5.16x and 1.26x fewer floating point operations for token budgets of 10B and 100B, respectively. We release all code publicly, offering a practical and reproducible path toward cost-efficient small language model development at scale.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.07227.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07227",
    "published": "2025-10-08T16:57:46Z",
    "updated": "2026-01-14T11:05:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出结合子网络选择、进化搜索和知识蒸馏的框架，以高效预训练小语言模型。",
      "motivation": "小语言模型（SLMs）作为大型语言模型（LLMs）的高效替代品，能在使用较少资源下提供强大性能，但预训练过程计算成本高、资源消耗大。现有方法通常依赖随机初始化，导致训练效率不足，难以在有限预算下实现最优性能。因此，本研究旨在通过改进初始化和训练策略，解决SLM预训练中的效率问题，推动其在资源受限场景中的应用。",
      "method": "研究方法整合了三个互补组件：首先，识别结构化稀疏子网络初始化，这些初始化在相同计算预算下优于随机初始化模型；其次，利用进化搜索自动发现高质量子网络初始化为预训练提供更好起点；第三，应用从更大教师模型的知识蒸馏，加速训练过程并提升模型泛化能力。框架结合了子网络选择、自动搜索和蒸馏技术，使用LLM权重初始化子网络，并通过进化搜索优化。",
      "result": "实验结果表明，通过进化搜索发现并用LLM权重初始化的最佳模型，在验证困惑度上匹配了可比的Pythia小语言模型。在计算效率方面，对于10B和100B令牌预算，浮点运算分别减少了5.16倍和1.26倍。这证明了该方法在保持性能的同时，显著降低了训练成本，相比于基线方法具有更高效率。",
      "conclusion": "本研究的主要贡献是提出了一个高效的小语言模型预训练框架，结合子网络选择、进化搜索和知识蒸馏，提高了训练效率并减少了资源消耗。学术价值在于为资源有限下的模型预训练提供了创新方法，实际应用价值在于推动成本效益高的语言模型开发。代码已公开确保可重复性，未来工作可优化搜索算法或扩展到更多模型类型。",
      "tags": [
        "Small Language Models (SLMs)",
        "Subnetwork Selection",
        "Evolutionary Search",
        "Knowledge Distillation",
        "Efficient Pretraining"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:10.669864Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.06953",
    "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces",
    "authors": [
      "Minju Gwak",
      "Guijin Son",
      "Jaehyung Kim"
    ],
    "abstract": "The Uniform Information Density (UID) hypothesis suggests that effective communication maintains a stable flow of information. In this work, we revisit this principle in the context of large language model (LLM) reasoning traces, asking whether step-level uniformity reflects reasoning quality. To this end, we propose an entropy-based stepwise information density metric and introduce two complementary measures of uniformity, local and global uniformity scores. Across the experiments on six different reasoning benchmarks, we find that step-level uniformity not only provides a strong theoretical lens but also yields practical performance benefits; for example, selecting reasoning traces with more uniform information density at the step-level improves accuracy by 10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals that correct reasoning traces tend to avoid sharp information density spikes, while incorrect traces exhibit irregular information bursts. These results demonstrate that UID-inspired information density measures outperform alternative internal signals as predictors of reasoning quality. Results highlight the uniformity of the information density as a robust diagnostic and selection criterion for building more reliable and accurate reasoning systems.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2510.06953.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06953",
    "published": "2025-10-08T12:37:04Z",
    "updated": "2026-01-14T03:29:50Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出基于熵的步骤级信息密度指标，验证Uniform Information Density假设在LLM推理轨迹中能预测推理质量并提升准确率。",
      "motivation": "研究旨在解决大型语言模型（LLM）推理轨迹中步骤级信息密度均匀性是否反映推理质量的问题。Uniform Information Density（UID）假设指出有效通信应保持信息流稳定，但此原则在LLM推理领域的应用尚未深入探索。现有方法可能未充分利用信息密度特征来评估推理过程，导致推理系统可靠性和准确性不足。利用信息密度均匀性作为质量指标，可以改进自动推理的诊断工具，提升整体性能。摘要未明确说明具体现有方法的不足，但暗示其他内部信号可能不如UID启发的度量有效。",
      "method": "论文提出一种基于熵的步骤级信息密度度量方法，用于量化推理轨迹中信息流的稳定性，并引入两个互补的均匀性评分：局部均匀性分数和全局均匀性分数。这些指标将UID假设从通信理论扩展到LLM推理场景，定义步骤级均匀性作为核心创新。实验在六个不同的推理基准上进行，涉及LLM生成的推理轨迹分析，但摘要未明确说明具体数据集或模型架构。方法的关键在于使用熵基计算来捕捉信息密度的动态变化。",
      "result": "实验结果表明，步骤级信息密度均匀性显著影响推理质量。在AIME2025基准上，选择信息密度更均匀的推理轨迹，准确率相对基线提升了10-32%。分析显示，正确推理轨迹倾向于避免信息密度尖峰，而错误轨迹则呈现不规则信息突发，支持均匀性作为质量预测指标。UID启发的信息密度度量在预测推理质量方面优于其他内部信号，突出了其作为选择标准的实用性。摘要未提供更多具体数据，但强调均匀性带来的性能改进和对比优势。",
      "conclusion": "论文验证了Uniform Information Density启发的信息密度度量是评估LLM推理质量的有效工具，主要贡献在于提出步骤级均匀性作为诊断和选择标准，有助于构建更可靠、准确的推理系统。研究强调了信息流稳定性在推理中的理论价值，并为未来工作提供了框架，如优化均匀性度量或应用于更多场景。摘要未明确说明研究的局限性或具体未来方向，但暗示可进一步探索信息密度在其他AI任务中的应用。",
      "tags": [
        "Uniform Information Density",
        "Large Language Model",
        "Reasoning Traces",
        "Entropy-based Metric",
        "Reasoning Quality"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:14.682372Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.03731",
    "title": "Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation",
    "authors": [
      "Yongfu Xue"
    ],
    "abstract": "The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \\textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$α$ and IniLoRA-$β$, both leveraging distinct initialization methods to enhance performance further.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.03731.pdf",
    "abs_url": "https://arxiv.org/abs/2510.03731",
    "published": "2025-10-04T08:34:06Z",
    "updated": "2026-01-14T06:12:58Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出IniLoRA，一种通过优化初始化策略来改进低秩适应微调效率的新方法。",
      "motivation": "随着参数高效微调方法的快速发展，LoRA因其效果和参数效率的平衡而广受欢迎，成为适应大型语言模型的关键技术。然而，LoRA依赖初始化两个低秩矩阵乘积为零，这限制了其有效激活和利用原始模型权重的能力，形成性能瓶颈，导致微调效果未最大化。该问题重要，因为高效微调对模型应用至关重要，现有方法在提升性能方面存在不足，因此研究旨在改进初始化策略以克服此局限性。",
      "method": "论文提出IniLoRA，一种新的初始化策略，通过初始化低秩矩阵以紧密近似原始模型权重，避免零乘积问题，从而更有效地利用原始权重。核心创新点包括设计基于近似权重的高级初始化方法，并引入两个变体IniLoRA-α和IniLoRA-β，分别采用不同的初始化技术进一步优化性能。技术特色在于结合低秩适应和精细化初始化，改进模型权重更新过程，摘要未明确说明具体数据集或模型架构细节。",
      "result": "实验结果表明，IniLoRA在多个模型和任务上均优于标准LoRA，实现了更好的性能。摘要未提供具体性能指标如准确率提升，但指出与基线方法相比有显著改进，验证了优化初始化策略在提升微调效果方面的有效性。这表明IniLoRA能有效克服现有方法的瓶颈，增强参数高效微调的整体性能。",
      "conclusion": "IniLoRA通过改进初始化策略，显著提升了LoRA在参数高效微调中的性能，对大型语言模型应用具有重要学术和实用价值。研究贡献在于提供了一种简单而有效的方法来克服技术瓶颈，促进更高效的模型适应；未来工作可探索更多初始化变体或扩展到其他微调框架，摘要未明确说明局限性。",
      "tags": [
        "Large Language Model",
        "Fine-Tuning",
        "LoRA",
        "Initialization Strategy",
        "Parameter-Efficient Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:22.090919Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.26278",
    "title": "ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation",
    "authors": [
      "Edoardo Bianchi",
      "Jacopo Staiano",
      "Antonio Liotta"
    ],
    "abstract": "Existing approaches treat action quality assessment and skill proficiency estimation as classification problems, outputting discrete labels without interpretable reasoning. We reformulate this task as generative vision language modeling, introducing ProfVLM, a compact model that jointly predicts proficiency levels and generates expert-like natural language feedback from multi-view videos. ProfVLM leverages conditional language generation to provide actionable insights along with quantitative evaluation scores. Central to our method is an AttentiveGatedProjector that dynamically fuses and projects multi-view egocentric and exocentric features from a frozen TimeSformer backbone into a language model fine-tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60% compared to existing classification-based methods. By providing natural language critiques aligned with performance levels, this work shows that generative vision-language modeling offers a powerful and efficient paradigm shift for interpretable action quality assessment.",
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.26278.pdf",
    "abs_url": "https://arxiv.org/abs/2509.26278",
    "published": "2025-09-30T14:00:41Z",
    "updated": "2026-01-14T12:30:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出ProfVLM，一种轻量级视频语言模型，通过生成式视觉语言建模实现多视图熟练度估计，联合预测熟练度水平并生成可解释的自然语言反馈。",
      "motivation": "现有方法将动作质量评估和技能熟练度估计处理为分类问题，仅输出离散标签，缺乏可解释的推理过程，这限制了评估结果在实践中的应用，无法提供详细反馈以指导技能改进。ProfVLM旨在解决这一缺陷，重新表述任务为生成式视觉语言建模，以增强评估的丰富性和可操作性，推动从简单分类向综合反馈的转变。",
      "method": "ProfVLM的核心是一个紧凑的生成式视觉语言模型，通过AttentiveGatedProjector动态融合多视图的第一人称和第三人称视频特征。该方法使用冻结的TimeSformer作为骨干网络提取视频特征，并将其投影到微调的语言模型中，以实现条件语言生成反馈。模型在EgoExo4D数据集上训练，利用专家评论作为监督信号，确保生成的自然语言反馈具有专家水准。",
      "result": "在EgoExo4D数据集上，ProfVLM在动作质量评估任务中超越了最先进的分类方法，展示了卓越性能。同时，模型效率显著提升：参数量减少了高达20倍，训练时间减少了高达60%，这表明生成式方法在保持高性能的同时，实现了高效的资源利用，远超传统基于分类的基线。",
      "conclusion": "本研究表明，生成式视觉语言建模为可解释的动作质量评估提供了强大且高效的范式转变。ProfVLM不仅提升了评估准确性，还通过自然语言反馈增强了评估的可解释性和实用性，具有在技能培训和评估等领域的广泛应用前景。未来工作可扩展到更多数据集和场景，以进一步验证其鲁棒性。",
      "tags": [
        "Video-Language Model",
        "Generative Vision-Language Modeling",
        "AttentiveGatedProjector",
        "TimeSformer",
        "Multi-View Video Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:34.905937Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.23770",
    "title": "GenView++: Unifying Adaptive View Generation and Quality-Driven Supervision for Contrastive Representation Learning",
    "authors": [
      "Xiaojie Li",
      "Bei Wang",
      "Jianlong Wu",
      "Yue Yu",
      "Liqiang Nie",
      "Min Zhang"
    ],
    "abstract": "The success of contrastive learning depends on the construction and utilization of high-quality positive pairs. However, current methods face critical limitations on two fronts: on the construction side, both handcrafted and generative augmentations often suffer from limited diversity and risk semantic corruption; on the learning side, the absence of a quality assessment mechanism leads to suboptimal supervision where all pairs are treated equally. To tackle these challenges, we propose GenView++, a unified framework that addresses both fronts by introducing two synergistic innovations. To improve pair construction, GenView++ introduces a multi-source adaptive view generation mechanism to synthesize diverse yet semantically coherent views by dynamically modulating generative parameters across image-conditioned, text-conditioned, and image-text-conditioned strategies. Second, a quality-driven contrastive learning mechanism assesses each pair's semantic alignment and diversity to dynamically reweight their training contribution, prioritizing high-quality pairs while suppressing redundant or misaligned pairs. Extensive experiments demonstrate the effectiveness of GenView++ across both vision and vision-language tasks. For vision representation learning, it improves MoCov2 by +2.5% on ImageNet linear classification. For vision-language learning, it raises the average zero-shot classification accuracy by +12.31% over CLIP and +5.31% over SLIP across ten datasets, and further improves Flickr30k text retrieval R@5 by +3.2%.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.23770.pdf",
    "abs_url": "https://arxiv.org/abs/2509.23770",
    "published": "2025-09-28T09:35:37Z",
    "updated": "2026-01-14T06:01:44Z",
    "comment": "The code is available at \\url{https://github.com/xiaojieli0903/GenViewPlusPlus}",
    "light_analysis": {
      "overview": "GenView++ 提出一个统一的框架，通过自适应视图生成和质量驱动的监督，解决了对比学习中正对构建和学习的局限性，从而提升表示学习的效果。",
      "motivation": "对比学习的成功依赖于高质量正对的构建和使用，但当前方法面临两个关键局限：在构建方面，手工增强和生成增强往往多样性有限，并可能导致语义损坏，影响表示学习的效果；在学习方面，缺乏质量评估机制使得所有正对在监督中被同等对待，导致学习效率低下和性能瓶颈。这些问题限制了对比学习在视觉和视觉-语言任务中的进一步发展，因此研究旨在解决这些不足，通过创新机制提升正对的质量和学习过程的优化。摘要未明确说明更广泛的背景，但基于现有信息，研究动机源于改进对比学习方法的通用性和鲁棒性。",
      "method": "GenView++ 的核心方法包括两个协同创新：首先，多源自适应视图生成机制，通过动态调制生成参数，在图像条件、文本条件和图像-文本条件策略下合成多样且语义一致的视图，以提升正对的构建质量和多样性；其次，质量驱动的对比学习机制，评估每个正对的语义对齐和多样性，动态重加权其训练贡献，优先高质量对并抑制冗余或错配对。该方法统一处理了视图构建和监督优化，使用具体数据集如 ImageNet 和 Flickr30k 进行实验，模型架构可能基于现有对比学习方法，但摘要未明确说明详细设计。",
      "result": "实验结果表明，GenView++ 在视觉和视觉-语言任务中均取得显著改进。在视觉表示学习中，它使 MoCov2 在 ImageNet 线性分类上的准确率提升了 2.5%。在视觉-语言学习中，零样本分类在十个数据集上的平均准确率比 CLIP 高出 12.31%，比 SLIP 高出 5.31%，同时在 Flickr30k 文本检索任务中，R@5 提高了 3.2%。这些性能提升展示了其在对比学习中的优越性，并通过与基线方法的对比，验证了其有效性。",
      "conclusion": "GenView++ 的主要贡献在于提出一个统一的框架，将自适应视图生成与质量驱动的监督相结合，有效解决了对比学习中正对构建和学习的局限性。学术上，它推动了对比学习方法的发展，通过创新机制提升了学习效率和质量；实际应用中，在视觉和视觉-语言任务中表现出显著性能提升，具有广泛的应用潜力。未来工作可能涉及扩展到更多领域或优化算法效率，但摘要未明确说明具体方向或局限性。",
      "tags": [
        "Contrastive Learning",
        "Adaptive View Generation",
        "Quality-Driven Supervision",
        "Multi-Source Generation",
        "Vision-Language Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:26:52.312433Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2510.00040",
    "title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models",
    "authors": [
      "Junjie Li",
      "Ziao Wang",
      "Jianghong Ma",
      "Xiaofeng Zhang"
    ],
    "abstract": "Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.00040.pdf",
    "abs_url": "https://arxiv.org/abs/2510.00040",
    "published": "2025-09-27T02:57:37Z",
    "updated": "2026-01-14T12:33:16Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了Capability-Attributed Data Curation (CADC)框架，通过分析视觉-语言模型的内在能力实现高效数据策展，仅用5%数据超越全数据训练性能。",
      "motivation": "大型视觉-语言模型虽在基准测试中表现优异，但其行为控制通过指令调优仍面临挑战，尤其是在减少数据预算时易导致性能下降。现有启发式方法将模型视为黑盒，忽视潜在学习能力，导致低效且不可控。本研究旨在解决这一问题，通过引入内在能力分析，提升指令调优的数据效率和可控性，以满足实际应用中数据稀缺场景的需求。",
      "method": "论文提出CADC框架，首先从梯度学习轨迹中以无监督方式发现模型的内在能力；其次通过影响估计技术将训练数据关联到这些能力；最后采用平衡选择和阶段性排序构建能力感知的课程学习方案。这种方法将黑盒指令调优转化为可控的能力驱动过程，避免了任务特定启发式的局限性，直接优化学习机制。",
      "result": "实验结果显示，CADC在仅使用5%的原始指令调优数据时，在跨模态基准测试中超越了全数据训练的效果，验证了其高效性和优势。与基线方法相比，CADC避免了性能下降，实现了更高的模型性能，具体表现为在标准评估指标上的显著提升，突显了数据策展的关键作用。",
      "conclusion": "论文的主要贡献是确立内在能力作为模型学习的基础构建块，并推广CADC为指令数据策展的原则性范式。这不仅在学术上深化了对视觉-语言模型学习机制的理解，还在应用上通过高效数据策展提高了指令调优的效率和效果。未来工作可能包括将CADC扩展到其他模型或任务，以进一步验证其普适性和实用性。",
      "tags": [
        "Vision-Language Models",
        "Instruction Tuning",
        "Data Curation",
        "Gradient Learning Trajectories",
        "Influence Estimation"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:39.285954Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.23003",
    "title": "Physically Plausible Multi-System Trajectory Generation and Symmetry Discovery",
    "authors": [
      "Jiayin Liu",
      "Yulong Yang",
      "Vineet Bansal",
      "Christine Allen-Blanchette"
    ],
    "abstract": "From metronomes to celestial bodies, mechanics underpins how the world evolves in time and space. With consideration of this, a number of recent neural network models leverage inductive biases from classical mechanics to encourage model interpretability and ensure forecasted states are physical. However, in general, these models are designed to capture the dynamics of a single system with fixed physical parameters, from state-space measurements of a known configuration space. In this paper we introduce Symplectic Phase Space GAN (SPS-GAN) which can capture the dynamics of multiple systems, and generalize to unseen physical parameters from. Moreover, SPS-GAN does not require prior knowledge of the system configuration space. In fact, SPS-GAN can discover the configuration space structure of the system from arbitrary measurement types (e.g., state-space measurements, video frames). To achieve physically plausible generation, we introduce a novel architecture which embeds a Hamiltonian neural network recurrent module in a conditional GAN backbone. To discover the structure of the configuration space, we optimize the conditional time-series GAN objective with an additional physically motivated term to encourages a sparse representation of the configuration space. We demonstrate the utility of SPS-GAN for trajectory prediction, video generation and symmetry discovery. Our approach captures multiple systems and achieves performance on par with supervised models designed for single systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.23003.pdf",
    "abs_url": "https://arxiv.org/abs/2509.23003",
    "published": "2025-09-26T23:46:55Z",
    "updated": "2026-01-14T17:15:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "SPS-GAN是一种新颖模型，结合哈密顿神经网络和条件生成对抗网络，能捕获多系统动力学、推广到未见参数并自动发现配置空间结构。",
      "motivation": "现有神经网络模型在利用经典力学归纳偏置时，通常局限于单个固定参数系统，依赖已知配置空间测量，这限制了多系统场景应用。现实世界涉及多个系统且参数可变，需要通用模型来生成物理合理预测。因此，开发能处理多系统、适应未见参数并发现配置空间结构的方法，对提升物理引导机器学习的实用性和通用性至关重要。摘要指出，这些模型不能捕获多系统动力学或处理未知配置空间。",
      "method": "SPS-GAN提出了一种创新架构，将哈密顿神经网络的循环模块嵌入到条件生成对抗网络（GAN）的主干中，以实现物理合理的轨迹生成。关键创新点包括优化条件时间序列GAN目标函数，并添加基于物理的项来鼓励配置空间的稀疏表示，从而从任意测量类型（如状态空间测量或视频帧）中自动发现系统结构。这种方法不依赖先验配置空间知识，增强了模型的灵活性和学习能力。",
      "result": "SPS-GAN在轨迹预测、视频生成和对称性发现任务中展示了有效性。实验结果表明，该方法能捕获多个系统的动力学，并在性能上与专门为单个系统设计的监督模型相当，证明了其在推广到未见参数时的鲁棒性。摘要提到“performance on par with supervised models designed for single systems”，但未提供具体数据指标如准确率提升，因此基于现有信息描述其相对性能。",
      "conclusion": "该研究的主要贡献是引入了SPS-GAN，一个能够处理多系统、发现配置空间结构的物理引导生成模型。学术上，它将哈密顿力学与生成对抗网络结合，推动了可解释和物理合理机器学习的发展。实际中，适用于轨迹预测和视频生成等领域，增强了物理一致性。未来工作可能包括扩展到更复杂系统或集成额外物理约束，以进一步提高模型适用性。",
      "tags": [
        "Hamiltonian Neural Network",
        "Conditional GAN",
        "Symplectic Phase Space",
        "Trajectory Prediction",
        "Symmetry Discovery"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:13.157494Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.22979",
    "title": "OptiMind: Teaching LLMs to Think Like Optimization Experts",
    "authors": [
      "Xinzhi Zhang",
      "Zeyi Chen",
      "Humishka Zope",
      "Hugo Barbalho",
      "Konstantina Mellou",
      "Marco Molinaro",
      "Janardhan Kulkarni",
      "Ishai Menache",
      "Sirui Li"
    ],
    "abstract": "Mathematical programming -- the task of expressing operations and decision-making problems in precise mathematical language -- is fundamental across domains, yet remains a skill-intensive process requiring operations research expertise. Recent advances in large language models for complex reasoning have spurred interest in automating this task, translating natural language into executable optimization models. Current approaches, however, achieve limited accuracy, hindered by scarce and noisy training data without leveraging domain knowledge. In this work, we systematically integrate optimization expertise to improve formulation accuracy for mixed-integer linear programming, a key family of mathematical programs. Our OptiMind framework leverages semi-automated, class-based error analysis to guide both training and inference, explicitly preventing common mistakes within each optimization class. Our resulting fine-tuned LLM significantly improves formulation accuracy by 20.7% across multiple optimization benchmarks, with consistent gains under test-time scaling methods such as self-consistency and multi-turn feedback, enabling further progress toward robust LLM-assisted optimization formulation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.22979.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22979",
    "published": "2025-09-26T22:23:12Z",
    "updated": "2026-01-14T18:26:45Z",
    "comment": null,
    "light_analysis": {
      "overview": "OptiMind框架通过系统整合优化领域知识，显著提升了大型语言模型在混合整数线性编程中的公式化准确率。",
      "motivation": "数学编程作为表达操作和决策问题的精确数学语言，在多个领域至关重要，但传统上依赖于操作研究专家，过程技能密集。近年来，基于大型语言模型的复杂推理研究激发了自动化兴趣，即将自然语言转换为可执行的优化模型。然而，当前方法准确率有限，主要受限于稀缺和噪声的训练数据，且未能有效利用优化领域的专业知识，导致在自动化任务中性能不佳。因此，本研究致力于通过集成领域知识，解决这一挑战，以推动数学编程的自动化进程。",
      "method": "本研究提出OptiMind框架，通过半自动化、基于类的错误分析系统整合优化专业知识。该方法在训练和推理阶段指导大型语言模型，显式防止每个优化类中的常见错误，针对混合整数线性编程问题进行优化。框架的核心创新在于将领域知识与机器学习结合，利用错误分析改进模型，可能涉及对现有LLM进行微调，但摘要未明确说明具体数据集或模型架构。技术特色是结合专家分析和数据驱动方法，提升任务准确性和稳健性。",
      "result": "实验结果表明，经过OptiMind框架微调的大型语言模型在多个优化基准测试中，公式化准确率显著提高了20.7%。在自一致性和多轮反馈等测试时缩放方法下，模型表现出一致的性能增益，证明了其稳健性和可扩展性。相较于现有方法，该框架有效克服了数据稀缺和噪声问题，在混合整数线性编程任务中实现了更高的准确性，推动了自动化优化模型生成的进展。",
      "conclusion": "OptiMind框架的主要贡献是成功整合优化领域知识，显著提升LLM在数学编程任务中的准确率，具有重要的学术和实际价值。学术上，它推动了LLM在复杂推理和自动化优化领域的应用；实际中，可减少对专家的依赖，提高数学编程的效率。未来工作可能包括扩展到其他优化问题类型，并进一步探索领域知识融合的方法，以实现更广泛的应用。",
      "tags": [
        "Large Language Models",
        "Mixed-Integer Linear Programming",
        "Fine-tuning",
        "Error Analysis",
        "Self-Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:11.330354Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.20612",
    "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface",
    "authors": [
      "Daehee Lee",
      "Dongsu Lee",
      "TaeYoon Kwack",
      "Wonje Choi",
      "Honguk Woo"
    ],
    "abstract": "Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.20612.pdf",
    "abs_url": "https://arxiv.org/abs/2509.20612",
    "published": "2025-09-24T23:34:01Z",
    "updated": "2026-01-14T18:11:20Z",
    "comment": "NeurIPS 2025 Spotlight",
    "light_analysis": {
      "overview": "本研究提出了SIL-C框架，通过懒惰学习接口实现策略兼容的技能增量学习，确保技能演化时下游策略无需重新训练，提升了效率。",
      "motivation": "技能增量学习（SIL）让具身智能体通过环境交互扩展技能库，以构建高效的分层策略。但技能库演化可能破坏与现有策略的兼容性，限制技能重用和泛化，导致策略需要重新训练或调整，增加资源成本。现有方法在技能更新时缺乏兼容性保障，本工作旨在解决这一关键挑战，使SIL更实用和可扩展。",
      "method": "SIL-C采用双边懒惰学习映射技术，动态对齐策略引用的子任务空间和技能空间。核心创新是允许增量学习的技能改进直接提升下游策略，无需策略重新训练。具体实现基于轨迹分布相似性，选择合适技能执行子任务；摘要未明确说明使用的数据集或模型架构，但涉及动态映射机制和懒惰学习接口来确保兼容性。",
      "result": "论文在不同SIL场景中评估SIL-C，结果表明它能有效维持演化技能与下游策略的兼容性，并确保学习过程高效。摘要未提供具体性能指标或与基线的对比数据，但评估验证了框架在兼容性保持和效率提升方面的有效性，为进一步应用奠定基础。",
      "conclusion": "SIL-C的主要贡献是解决了技能增量学习中的策略兼容性问题，通过懒惰学习映射增强技能演化时的策略稳定性。学术价值在于提出新方法对齐技能和策略空间，实际应用上降低策略维护成本，促进可重用技能库发展。未来工作可能包括扩展场景或优化计算效率，以进一步提升框架的普适性和性能。",
      "tags": [
        "Skill Incremental Learning",
        "Lazy Learning",
        "Policy Compatibility",
        "Skill-Policy Mapping",
        "Trajectory Similarity"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:09.585697Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18085",
    "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
    "authors": [
      "Sudhanshu Agrawal",
      "Risheek Garrepalli",
      "Raghavv Goel",
      "Mingu Lee",
      "Christopher Lott",
      "Fatih Porikli"
    ],
    "abstract": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.18085.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18085",
    "published": "2025-09-22T17:58:21Z",
    "updated": "2026-01-14T06:58:58Z",
    "comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2 with additional analysis and referenced it in Sec 5.2. (v3): Added note to Sec 4.2 and Appendix A.2 specifying conditions for losslessness",
    "light_analysis": {
      "overview": "Spiffy是一种基于无损speculative decoding的算法，加速Diffusion LLMs推理达2.8-3.1倍，同时保持输出分布不变。",
      "motivation": "Diffusion LLMs (dLLMs) 作为自回归LLMs的替代，理论上具有更高token生成率，但当前开源dLLMs为最大化输出质量，每去噪步骤仅解码单个token，导致实际生成效率低下。这一问题重要，因为加速LLM推理对AI应用至关重要。现有speculative decoding方法主要针对AR-LLMs，应用于dLLMs时面临独特挑战，如保持输出分布和利用双向生成特性，需要新解决方案以提升效率。",
      "method": "Spiffy算法采用无损speculative decoding，通过dLLM自身分布以auto-speculative方式提议draft states，无需训练或运行独立draft模型，减少开销。核心创新是提出有向draft graph，专门利用dLLM的双向块状生成特性，支持并行验证。进一步引入离线校准算法，优化graph结构以提高接受率，从而加速推理过程。该方法整合了关键数据集和模型架构，但摘要未明确说明具体细节。",
      "result": "Spiffy将dLLM推理速度提升2.8至3.1倍，同时可证保持模型输出分布不变。与基线方法相比，这一加速是显著的。当与KV-caching和multi-token unmasking等并行解码技术结合时，综合优化使总加速倍数高达7.9倍，展示了Spiffy的互补性和有效性，突显了其在实际应用中的高性能表现。",
      "conclusion": "Spiffy的主要贡献是提出了一种专为dLLMs设计的无损speculative decoding算法，解决了从AR-LLMs移植到dLLMs的独特挑战，扩展了推理加速技术。学术价值在于为dLLM优化提供了新思路，实际应用则能大幅提高生成效率，降低计算成本。摘要未明确说明局限性，但未来工作可进一步探索graph优化或与其他加速方法的整合，以推动领域发展。",
      "tags": [
        "Speculative Decoding",
        "Diffusion LLMs",
        "Directed Draft Graph",
        "Parallel Verification",
        "LLM Acceleration"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:31.215322Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.14024",
    "title": "Differentially private federated learning for localized control of infectious disease dynamics",
    "authors": [
      "Raouf Kerkouche",
      "Henrik Zunker",
      "Mario Fritz",
      "Martin J. Kühn"
    ],
    "abstract": "In times of epidemics, swift reaction is necessary to mitigate epidemic spreading. For this reaction, localized approaches have several advantages, limiting necessary resources and reducing the impact of interventions on a larger scale. However, training a separate machine learning (ML) model on a local scale is often not feasible due to limited available data. Centralizing the data is also challenging because of its high sensitivity and privacy constraints. In this study, we consider a localized strategy based on the German counties and communities managed by the related local health authorities (LHA). For the preservation of privacy to not oppose the availability of detailed situational data, we propose a privacy-preserving forecasting method that can assist public health experts and decision makers. ML methods with federated learning (FL) train a shared model without centralizing raw data. Considering the counties, communities or LHAs as clients and finding a balance between utility and privacy, we study a FL framework with client-level differential privacy (DP). We train a shared multilayer perceptron on sliding windows of recent case counts to forecast the number of cases, while clients exchange only norm-clipped updates and the server aggregated updates with DP noise. We evaluate the approach on COVID-19 data on county-level during two phases. As expected, very strict privacy yields unstable, unusable forecasts. At a moderately strong level, the DP model closely approaches the non-DP model: R2 around 0.94 (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in November 2020; R2 around 0.88 (vs. 0.93) and MAPE of 21 % in March 2022. Overall, client-level DP-FL can deliver useful county-level predictions with strong privacy guarantees, and viable privacy budgets depend on epidemic phase, allowing privacy-compliant collaboration among health authorities for local forecasting.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.14024.pdf",
    "abs_url": "https://arxiv.org/abs/2509.14024",
    "published": "2025-09-17T14:28:04Z",
    "updated": "2026-01-14T15:45:09Z",
    "comment": "26 pages, 9 figures",
    "light_analysis": {
      "overview": "本研究提出一种结合差分隐私的联邦学习框架，用于局部流行病预测，以保护数据隐私。",
      "motivation": "该研究旨在解决在流行病爆发时，局部控制需要快速反应但数据有限的问题。传统方法中，集中数据训练面临敏感性和隐私约束，而本地训练因数据不足难以实现有效预测。现有方法可能无法平衡效用和隐私，因此开发一种隐私保护的协作学习方案至关重要，以支持卫生决策并缓解传播影响。",
      "method": "论文采用联邦学习框架，将德国县区、社区或当地卫生当局作为客户端，使用多层感知机模型基于滑动窗口的近期病例数据进行训练，预测病例数。客户端仅共享经过规范剪裁的模型更新，服务器在聚合更新时添加差分隐私噪声，以实现客户端级别的隐私保护，从而在不集中原始数据的情况下平衡预测效用和数据安全。",
      "result": "实验在COVID-19数据上评估了两个阶段：2020年11月和2022年3月。结果显示，在适度隐私级别下，差分隐私模型性能接近非隐私模型：2020年11月R2约0.94（对比0.95），平均绝对百分比误差为26%；2022年3月R2约0.88（对比0.93），误差为21%。非常严格的隐私会导致预测不稳定和不可用，验证了隐私与效用之间的权衡关系。",
      "conclusion": "研究表明，客户端级差分隐私联邦学习能提供有用的局部预测，并保证强隐私，促进卫生当局间的合规协作。隐私预算需根据流行病阶段调整，未来工作可进一步优化隐私设置或扩展到其他疾病场景，以增强方法的适应性和实用性。",
      "tags": [
        "Federated Learning",
        "Differential Privacy",
        "Multilayer Perceptron",
        "Epidemic Forecasting",
        "Privacy-Preserving Machine Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:19.377685Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.18159",
    "title": "Improved Segmentation of Polyps and Visual Explainability Analysis",
    "authors": [
      "Akwasi Asare",
      "Thanh-Huy Nguyen",
      "Ulas Bagci"
    ],
    "abstract": "Colorectal cancer (CRC) remains one of the leading causes of cancer-related morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as critical precursors according to the World Health Organization (WHO). Early and accurate segmentation of polyps during colonoscopy is essential for reducing CRC progression, yet manual delineation is labor-intensive and prone to observer variability. Deep learning methods have demonstrated strong potential for automated polyp analysis, but their limited interpretability remains a barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an explainable deep learning framework that integrates a U-Net architecture with a pre-trained ResNet-34 backbone and Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp segmentation. To ensure rigorous benchmarking, the model was trained and evaluated using 5-Fold Cross-Validation on the Kvasir-SEG dataset of 1,000 annotated endoscopic images. Experimental results show a mean Dice coefficient of 0.8902 +/- 0.0125, a mean Intersection-over-Union (IoU) of 0.8023, and an Area Under the Receiver Operating Characteristic Curve (AUC-ROC) of 0.9722. Advanced quantitative analysis using an optimal threshold yielded a Sensitivity of 0.9058 and Precision of 0.9083. Additionally, Grad-CAM visualizations confirmed that predictions were guided by clinically relevant regions, offering insight into the model's decision-making process. This study demonstrates that integrating segmentation accuracy with interpretability can support the development of trustworthy AI-assisted colonoscopy tools.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.18159.pdf",
    "abs_url": "https://arxiv.org/abs/2509.18159",
    "published": "2025-09-17T02:57:33Z",
    "updated": "2026-01-14T15:04:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出了一种名为PolypSeg-GradCAM的可解释深度学习框架，通过整合U-Net架构、预训练ResNet-34骨干和Grad-CAM技术，实现了息肉分割的改进和视觉解释性分析。",
      "motivation": "结直肠癌（CRC）是全球癌症相关发病率和死亡率的主要成因之一，胃肠道息肉是关键前兆。早期准确分割息肉对降低CRC进展至关重要，但手动分割劳动密集且易受观察者变异影响。深度学习方法在自动化息肉分析中显示出潜力，但缺乏解释性限制了其在临床的采用，这凸显了开发兼具准确性和透明度的解决方案的紧迫性。",
      "method": "研究提出了PolypSeg-GradCAM框架，其核心是集成U-Net分割架构与预训练ResNet-34作为骨干网络，并应用Grad-CAM提供视觉解释性。关键创新在于结合分割准确性和模型透明度，使临床医生能理解预测依据。数据集使用Kvasir-SEG（1000个标注的内窥镜图像），通过5折交叉验证进行训练和评估，以确保结果的稳健性。",
      "result": "实验结果显示，PolypSeg-GradCAM在Kvasir-SEG数据集上实现了平均Dice系数0.8902（标准差0.0125）、平均IoU 0.8023和AUC-ROC 0.9722的高性能。在最优阈值下，敏感性达到0.9058，精度为0.9083。此外，Grad-CAM可视化确认预测基于临床相关区域，增强了模型的可解释性和可靠性，摘要未明确说明与基线方法的直接对比。",
      "conclusion": "本研究证实了集成分割准确性和解释性对于开发可信AI辅助结肠镜工具的重要性。主要贡献是提出PolypSeg-GradCAM框架，提高深度学习的透明度和临床适用性，学术上推动可解释AI在医学图像分析中的应用，实践上有助于减少结直肠癌风险。未来可扩展数据集或优化模型以提升性能。",
      "tags": [
        "Polyp Segmentation",
        "U-Net",
        "ResNet-34",
        "Grad-CAM",
        "Explainable AI"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:33.404153Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.11773",
    "title": "AgenticIE: An Adaptive Agent for Information Extraction from Complex Regulatory Documents",
    "authors": [
      "Gaye Colakoglu",
      "Gürkan Solmaz",
      "Jonathan Fürst"
    ],
    "abstract": "Declaration of Performance (DoP) documents, mandated by EU regulation, specify characteristics of construction products, such as fire resistance and insulation. While this information is essential for quality control and reducing carbon footprints, it is not easily machine readable. Despite content requirements, DoPs exhibit significant variation in layout, schema, and format, further complicated by their multilingual nature. In this work, we propose DoP Key Information Extraction (KIE) and Question Answering (QA) as new NLP challenges. To address this challenge, we design a domain-specific AgenticIE system based on a planner-executor-corresponder pattern. For evaluation, we introduce a high-density, expert-annotated dataset of complex, multi-page regulatory documents in English and German. Unlike standard IE datasets (e.g., FUNSD, CORD) with sparse annotations, our dataset contains over 15K annotated entities, averaging over 190 annotations per document. Our agentic system outperforms static and multimodal LLM baselines, achieving Exact Match (EM) scores of 0.396 vs. 0.342 (GPT-4o, +16%) and 0.314 (GPT-4o-V, +26%) across the KIE and QA tasks. Our experimental analysis validates the benefits of the agentic system, as well as the challenging nature of our new DoP dataset.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.11773.pdf",
    "abs_url": "https://arxiv.org/abs/2509.11773",
    "published": "2025-09-15T10:53:05Z",
    "updated": "2026-01-14T09:15:01Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出了一种基于 agent 的自适应信息提取系统，用于从复杂的多语言监管文档中高效提取关键信息，并引入了新的 NLP 挑战和高密度标注数据集。",
      "motivation": "研究动机源于欧盟规定的 Declaration of Performance (DoP) 文档，这些文档包含建筑产品的关键特性如防火和绝缘，对质量控制和减少碳足迹至关重要。然而，由于文档在布局、结构和格式上存在显著变化，且具有多语言性，现有方法难以实现机器可读。因此，论文将 DoP 的关键信息提取和问答任务作为新的 NLP 挑战提出，旨在解决实际应用问题并弥补当前技术的不足。",
      "method": "论文设计了名为 AgenticIE 的领域特定系统，采用 planner-executor-corresponder 模式来处理复杂文档，通过规划、执行和对应步骤自适应提取信息。关键创新点在于 agent 架构的引入，以应对文档的多样性。同时，引入了一个专家标注的高密度数据集，包含英文和德文的复杂多页监管文档，标注了超过 15K 个实体，平均每文档超过 190 个标注，超越了传统数据集的稀疏标注。",
      "result": "实验结果表明，AgenticIE 系统在关键信息提取和问答任务上优于静态和多模态大型语言模型基线。具体地，系统的 Exact Match (EM) 分数达到 0.396，而 GPT-4o 的分数为 0.342（提升 16%），GPT-4o-V 的分数为 0.314（提升 26%）。这些数据验证了 agentic 系统在处理复杂文档时的性能优势，并在对比中显示出显著改进。",
      "conclusion": "论文的主要贡献在于提出并验证了一个自适应 agent 系统，有效解决了复杂多语言监管文档的信息提取挑战，同时引入了高密度标注数据集以促进相关研究。该研究具有学术价值，推动了信息提取和问答技术的发展，并具备实际应用潜力，如自动化质量控制和环保监测。未来工作可能包括扩展数据集到更多语言或优化 agent 架构。",
      "tags": [
        "Key Information Extraction",
        "Question Answering",
        "Agentic Systems",
        "Multilingual Documents",
        "Large Language Model"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:53.839320Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.08122",
    "title": "In-Context Learning Enhanced Credibility Transformer",
    "authors": [
      "Kishan Padayachy",
      "Ronald Richman",
      "Salvatore Scognamiglio",
      "Mario V. Wüthrich"
    ],
    "abstract": "The starting point of our network architecture is the Credibility Transformer which extends the classical Transformer architecture by a credibility mechanism to improve model learning and predictive performance. This Credibility Transformer learns credibilitized CLS tokens that serve as learned representations of the original input features. In this paper we present a new paradigm that augments this architecture by an in-context learning mechanism, i.e., we increase the information set by a context batch consisting of similar instances. This allows the model to enhance the CLS token representations of the instances by additional in-context information and fine-tuning. We empirically verify that this in-context learning enhances predictive accuracy by adapting to similar risk patterns. Moreover, this in-context learning also allows the model to generalize to new instances which, e.g., have feature levels in the categorical covariates that have not been present when the model was trained -- for a relevant example, think of a new vehicle model which has just been developed by a car manufacturer.",
    "categories": [
      "cs.LG",
      "stat.AP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.08122.pdf",
    "abs_url": "https://arxiv.org/abs/2509.08122",
    "published": "2025-09-09T20:00:52Z",
    "updated": "2026-01-14T10:49:19Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出了一种通过情境学习增强Credibility Transformer的新范式，以提升模型的预测准确性和泛化能力。",
      "motivation": "该研究旨在解决Credibility Transformer在处理未见过的特征时的泛化挑战，例如在风险评估中遇到的新车型特征。现有方法可能依赖于训练数据的有限信息，导致模型在面对新实例时性能下降。通过引入情境学习机制，模型能够利用相似实例的上下文信息，弥补数据缺失问题，从而提高适应性和准确性，这在动态环境如保险评估中尤为重要。",
      "method": "论文基于Credibility Transformer架构，扩展了情境学习机制，通过增加一个由相似实例组成的情境批次来增强信息集。核心创新点是将情境学习与可信度机制结合，允许模型通过额外的上下文信息和微调过程优化CLS令牌表示。这种方法利用情境批次动态调整输入特征的表示，从而提高模型的学习效率和预测性能。",
      "result": "实验验证表明，情境学习机制显著提高了预测准确性，通过适应相似风险模式来优化模型性能。模型能够有效泛化到训练中未见过的特征，例如处理新车型的分类协变量。虽然摘要未提供具体数据指标，但证实了该方法相对于基线方法的改进，增强了模型在真实世界应用中的鲁棒性和实用性。",
      "conclusion": "该研究的主要贡献是成功将情境学习融入Credibility Transformer，提升了模型的预测准确性和泛化能力。学术上推动了Transformer架构的演化，实际应用价值体现在风险评估等领域，能够处理动态数据环境。摘要未明确说明局限性，但未来工作可能涉及情境学习的更广泛整合和数据集扩展。",
      "tags": [
        "Credibility Transformer",
        "In-Context Learning",
        "CLS Token",
        "Transformer Architecture",
        "Fine-Tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:39.903192Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.07530",
    "title": "Universal Few-Shot Spatial Control for Diffusion Models",
    "authors": [
      "Kiet T. Nguyen",
      "Chanhyuk Lee",
      "Donggyun Kim",
      "Dong Hoon Lee",
      "Seunghoon Hong"
    ],
    "abstract": "Spatial conditioning in pretrained text-to-image diffusion models has significantly improved fine-grained control over the structure of generated images. However, existing control adapters exhibit limited adaptability and incur high training costs when encountering novel spatial control conditions that differ substantially from the training tasks. To address this limitation, we propose Universal Few-Shot Control (UFC), a versatile few-shot control adapter capable of generalizing to novel spatial conditions. Given a few image-condition pairs of an unseen task and a query condition, UFC leverages the analogy between query and support conditions to construct task-specific control features, instantiated by a matching mechanism and an update on a small set of task-specific parameters. Experiments on six novel spatial control tasks show that UFC, fine-tuned with only 30 annotated examples of novel tasks, achieves fine-grained control consistent with the spatial conditions. Notably, when fine-tuned with 0.1% of the full training data, UFC achieves competitive performance with the fully supervised baselines in various control tasks. We also show that UFC is applicable agnostically to various diffusion backbones and demonstrate its effectiveness on both UNet and DiT architectures. Code is available at https://github.com/kietngt00/UFC.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.07530.pdf",
    "abs_url": "https://arxiv.org/abs/2509.07530",
    "published": "2025-09-09T09:08:07Z",
    "updated": "2026-01-14T06:19:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种通用的少样本控制适配器UFC，使预训练扩散模型能够快速适应新空间控制条件，减少训练成本。",
      "motivation": "现有预训练文本到图像扩散模型中的空间控制适配器在面对与训练任务差异较大的新空间条件时，存在适应能力有限和训练成本高的问题。这限制了模型在实际应用中的灵活性，因为需要大量标注数据来微调每个新任务。因此，开发一种能够用少量示例泛化到新条件的通用控制方法至关重要，以提高效率并扩展扩散模型的应用范围。",
      "method": "本研究提出了Universal Few-Shot Control（UFC）方法，它通过少量未见任务的图像-条件对，利用查询条件与支持条件之间的类比来构建任务特定的控制特征。具体采用匹配机制和更新少量任务特定参数来实例化这些特征。UFC不依赖于具体扩散骨干，支持UNet和DiT等多种架构，从而实现了跨任务和模型的通用性。",
      "result": "实验在六个新空间控制任务上进行，UFC仅用30个标注示例微调后，即能实现与空间条件一致的细粒度控制。此外，当使用完整训练数据的0.1%进行微调时，UFC在多种控制任务中表现与全监督基线方法竞争，展示了其高效性和性能优势。具体性能指标如准确率和控制一致性在摘要中未明确说明，但实验表明了方法的有效性。",
      "conclusion": "UFC的主要贡献在于提出了一种通用的少样本控制适配器，显著降低了扩散模型适应新空间条件的训练成本，提升了灵活性。学术价值在于推动少样本学习在生成模型中的应用，实际应用价值在于加速模型部署和扩展任务范围。未来工作可探索更多任务类型或集成更复杂的条件，但摘要未明确说明局限性。",
      "tags": [
        "Diffusion Models",
        "Few-Shot Learning",
        "Spatial Control",
        "Universal Adapter",
        "Matching Mechanism"
      ]
    },
    "analyzed_at": "2026-01-15T03:27:29.816353Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2509.03932",
    "title": "KPoEM: A Human-Annotated Dataset for Emotion Classification and RAG-Based Poetry Generation in Korean Modern Poetry",
    "authors": [
      "Iro Lim",
      "Haein Ji",
      "Byungjun Kim"
    ],
    "abstract": "This study introduces KPoEM (Korean Poetry Emotion Mapping), a novel dataset that serves as a foundation for both emotion-centered analysis and generative applications in modern Korean poetry. Despite advancements in NLP, poetry remains underexplored due to its complex figurative language and cultural specificity. We constructed a multi-label dataset of 7,662 entries (7,007 line-level and 615 work-level), annotated with 44 fine-grained emotion categories from five influential Korean poets. The KPoEM emotion classification model, fine-tuned through a sequential strategy -- moving from general-purpose corpora to the specialized KPoEM dataset -- achieved an F1-micro score of 0.60, significantly outperforming previous models (0.43). The model demonstrates an enhanced ability to identify temporally and culturally specific emotional expressions while preserving core poetic sentiments. Furthermore, applying the structured emotion dataset to a RAG-based poetry generation model demonstrates the empirical feasibility of generating texts that reflect the emotional and cultural sensibilities of Korean literature. This integrated approach strengthens the connection between computational techniques and literary analysis, opening new pathways for quantitative emotion research and generative poetics. Overall, this study provides a foundation for advancing emotion-centered analysis and creation in modern Korean poetry.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.03932.pdf",
    "abs_url": "https://arxiv.org/abs/2509.03932",
    "published": "2025-09-04T06:45:39Z",
    "updated": "2026-01-14T05:34:47Z",
    "comment": "43 pages, 22 tables, 3 figures, Digital Humanities and Social Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies, University of Pennsylvania, Philadelphia, USA",
    "light_analysis": {
      "overview": "本研究提出了KPoEM数据集，用于韩国现代诗歌的细粒度情感分类和基于RAG的诗歌生成，通过顺序微调策略显著提升模型性能。",
      "motivation": "自然语言处理领域在诗歌分析上进展有限，主要因其复杂比喻语言和文化特异性导致研究不足。现有方法在韩国诗歌情感分类任务中表现不佳，F1-micro分数仅为0.43，难以捕捉情感表达的细微差异和文化内涵。因此，本研究旨在构建一个专门的数据集，以支持更准确的情感分析和生成应用，弥补现有技术在诗歌领域中的空白。",
      "method": "研究方法包括构建KPoEM数据集，包含7,662个条目（7,007行级和615作品级），标注44个细粒度情感类别，基于五位影响深远的韩国诗人作品。情感分类模型采用顺序微调策略，从通用语料库迁移到专用数据集训练。此外，将结构化的情感数据集应用于基于RAG的诗歌生成模型，通过检索增强生成技术生成文本。",
      "result": "实验结果表明，情感分类模型的F1-micro分数达到0.60，比先前模型的0.43提升0.17，显示出更强的识别时间和文化特定情感表达的能力。模型在保持核心诗意方面表现良好。基于RAG的诗歌生成模型成功生成了反映韩国文学情感和文化敏感性的文本，验证了该方法的实证可行性。",
      "conclusion": "本研究的核心贡献是提供了KPoEM数据集，为韩国现代诗歌的情感分析和生成奠定了基础。其学术价值在于连接计算技术与文学分析，推动量化情感研究和生成诗学的发展。未来方向可能包括扩展数据集、优化模型性能或应用于其他文化语境，以进一步探索诗歌的定量分析。",
      "tags": [
        "Emotion Classification",
        "RAG-based Generation",
        "Fine-grained Annotation",
        "Korean NLP",
        "Sequential Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:16.563780Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.20294",
    "title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization",
    "authors": [
      "Frank Röder",
      "Jan Benad",
      "Manfred Eppe",
      "Pradeep Kr. Banerjee"
    ],
    "abstract": "Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.20294.pdf",
    "abs_url": "https://arxiv.org/abs/2508.20294",
    "published": "2025-08-27T22:02:56Z",
    "updated": "2026-01-14T17:50:26Z",
    "comment": "31 pages, 4 figures, accepted to NeurIPS 2025",
    "light_analysis": {
      "overview": "该论文提出Dynamics-Aligned Latent Imagination框架，通过自监督编码器推断潜在上下文表示，实现强化学习的零样本泛化。",
      "motivation": "现实世界强化学习面临适应未见环境条件的挑战，但现有方法如上下文马尔可夫决策过程常依赖显式上下文变量（如重力、摩擦），当上下文是潜在或难以测量时，这些方法应用受限。因此，本研究旨在解决如何在未知环境中实现高效泛化，避免昂贵的重新训练，并克服现有方法在潜在上下文推断上的不足。",
      "method": "论文提出Dynamics-Aligned Latent Imagination（DALI）框架，集成于Dreamer架构中。核心方法包括训练自监督编码器从智能体与环境的交互中预测前向动力学，以生成潜在上下文表示。这些表示条件化世界模型和策略，连接感知与控制。理论证明该编码器对高效上下文推断和鲁棒泛化至关重要，并实现了反事实一致性，如扰动重力编码维度可物理合理地改变想象轨迹。",
      "result": "在挑战性的cMDP基准测试中，DALI展现出显著性能提升。相比不考虑上下文的基线方法，DALI取得了显著增益；在外推任务中，常超过考虑上下文的基线方法，成功实现了对未见上下文变化的零样本泛化。摘要未明确说明具体性能指标，但强调了其在挑战性任务中的有效性。",
      "conclusion": "DALI框架通过推断潜在上下文表示，解决了强化学习在潜在环境变化下的泛化问题。其主要贡献在于结合自监督学习和理论证明，实现了鲁棒的零样本泛化。学术上，为上下文推断提供了新思路；实际中，支持自适应控制系统的开发。摘要未明确说明局限性或未来工作方向。",
      "tags": [
        "Contextual MDP",
        "Latent Imagination",
        "Self-Supervised Learning",
        "Zero-Shot Generalization",
        "Counterfactual Consistency"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:32.858094Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.19828",
    "title": "Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning",
    "authors": [
      "Sikuan Yan",
      "Xiufeng Yang",
      "Zuchao Huang",
      "Ercong Nie",
      "Zifeng Ding",
      "Zonggen Li",
      "Xiaowen Ma",
      "Jinhe Bi",
      "Kristian Kersting",
      "Jeff Z. Pan",
      "Hinrich Schütze",
      "Volker Tresp",
      "Yunpu Ma"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities across a wide range of NLP tasks, but they remain fundamentally stateless, constrained by limited context windows that hinder long-horizon reasoning. Recent efforts to address this limitation often augment LLMs with an external memory bank, yet most existing pipelines are static and heuristic-driven, lacking a learned mechanism for deciding what to store, update, or retrieve. We present Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the ability to actively manage and utilize external memory through two specialized agents: a Memory Manager that learns structured operations, including ADD, UPDATE, DELETE, and NOOP; and an Answer Agent that pre-selects and reasons over relevant entries. Both agents are fine-tuned with outcome-driven RL (PPO and GRPO), enabling adaptive memory management with minimal supervision. With only 152 training QA pairs, Memory-R1 outperforms strong baselines and generalizes across diverse question types, three benchmarks (LoCoMo, MSC, LongMemEval), and multiple model scales (3B-14B).",
    "categories": [
      "cs.CL",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.19828.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19828",
    "published": "2025-08-27T12:26:55Z",
    "updated": "2026-01-14T14:21:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "Memory-R1是一个基于强化学习的框架，通过两个专门代理使大型语言模型能主动管理和利用外部记忆。",
      "motivation": "大型语言模型在各种NLP任务中表现优异，但本质上无状态，受限于上下文窗口，阻碍了长期推理。现有方法常通过外部记忆库补充，但大多是静态和启发式驱动，缺乏学习机制来决定存储、更新或检索什么内容，这限制了LLMs在复杂、长序列任务中的有效应用。",
      "method": "论文提出Memory-R1框架，采用强化学习训练两个专门代理：Memory Manager学习结构化操作（如添加、更新、删除和无操作），以动态管理外部记忆；Answer Agent负责预选和推理相关记忆条目。两者通过结果驱动的强化学习方法（PPO和GRPO）进行微调，仅需152个训练QA对，实现自适应记忆管理，减少监督需求。",
      "result": "实验结果显示，Memory-R1仅用152个训练QA对，便在性能上超越了强基线。该方法能泛化到多样问题类型、三个基准数据集（LoCoMo、MSC和LongMemEval），并在不同模型规模（从3B到14B）上表现一致，展示了其高效记忆管理能力和广泛适用性。",
      "conclusion": "Memory-R1的主要贡献是通过强化学习框架使LLM能主动管理和利用外部记忆，仅需少量监督，为记忆管理提供了学习驱动的方法，提升了长期推理任务的性能。研究具有学术和实际应用价值，支持多基准和模型规模，未来可进一步探索扩展应用或优化技术。",
      "tags": [
        "Large Language Model",
        "Reinforcement Learning",
        "Memory Management",
        "PPO",
        "GRPO"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:21.038167Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.18025",
    "title": "AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration",
    "authors": [
      "Aditri Paul",
      "Archan Paul"
    ],
    "abstract": "Successful autonomous planetary exploration hinges on real-time, high-fidelity environmental perception. However, standard deep learning models usually demand far more memory and computation power than space-qualified, radiation-hardened onboard hardware can provide. This creates a fundamental design challenge of deploying sophisticated detection architectures without saturating the rigid power and memory envelopes of the computation hardware of planetary exploration platforms. We propose the Adaptive Quantized Planetary Crater Detection System to resolve this bottleneck. Our framework integrates a Quantized Neural Network, refined through Quantization Aware Training, with an Adaptive Multi-Sensor Fusion module. By forcing weights into low-precision integer arithmetic, we effectively strip away the floating-point overhead that typically bottlenecks onboard processors and system memory. This yields a leaner model footprint and significantly faster processing while the detection fidelity remains high. Such efficiency enables AMF module to merge high-bandwidth Optical Imagery streams with Digital Elevation Models using an Adaptive Weighting Mechanism to re-balance sensor priority under variable conditions like deep shadows or high albedo. Integrated Multi-Scale Detection Heads then resolve craters across a wide range of diameters, providing a computationally efficient and precise solution for real-time detection, localization of craters and hazard avoidance. This paper establishes the architectural design and theoretical justification of the system. While our methodology is grounded in principles of hybrid computer vision and planetary science, we present this as a blueprint for future empirical validation and hardware benchmarking on integer-arithmetic units. This system provides a capability vital for the next generation of autonomous landing, navigation, and deep space explorations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.ET",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.18025.pdf",
    "abs_url": "https://arxiv.org/abs/2508.18025",
    "published": "2025-08-25T13:44:00Z",
    "updated": "2026-01-14T14:49:07Z",
    "comment": "17 pages, 6 figures. A research paper on a novel deep learning framework for planetary crater detection",
    "light_analysis": {
      "overview": "本论文提出了一种自适应量化行星撞击坑检测系统，通过量化神经网络和自适应传感器融合，实现高效实时检测，解决资源受限平台上的部署挑战。",
      "motivation": "自主行星探索需要实时高保真环境感知，但标准深度学习模型的内存和计算需求远超航天硬件的严格限制，这导致了在功耗和内存约束下部署复杂检测架构的根本设计难题。问题在于现有方法通常无法适应空间任务中的资源受限环境，阻碍了实时感知的实现，因此开发高效方法至关重要。",
      "method": "该方法结合量化神经网络和自适应多传感器融合模块。量化神经网络通过量化感知训练，将权重转为低精度整数运算，以减少浮点开销，从而降低模型大小和加速处理。自适应模块使用权重机制融合光学图像与数字高程模型，根据阴影或高反照率等条件调整传感器优先级。多尺度检测头处理不同直径撞击坑，提供高效精确的检测方案。",
      "result": "摘要未提供具体实验数据，如准确率提升或效率改进的数值。文中提到模型变得更小、处理速度显著提高，同时检测保真度保持高水平，但未明确说明与基线方法的对比或具体性能指标。因此，结果部分基于系统设计旨在优化计算效率和精度推断，摘要未明确说明具体数据。",
      "conclusion": "本论文贡献了一个高效的行星撞击坑检测系统，结合量化技术和传感器融合，为自主空间探索提供关键能力。其学术价值在于提出了一种资源受限环境下的检测架构，实际应用支持下一代自主着陆、导航和深空探索。局限性是尚未进行实证验证，未来工作包括硬件基准测试和进一步实验验证。",
      "tags": [
        "Quantized Neural Network",
        "Quantization Aware Training",
        "Adaptive Multi-Sensor Fusion",
        "Multi-Scale Detection Heads",
        "Planetary Crater Detection"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:20.438858Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.17394",
    "title": "LVLM-Aware Multimodal Retrieval for RAG-Based Medical Diagnosis with General-Purpose Models",
    "authors": [
      "Nir Mazor",
      "Tom Hope"
    ],
    "abstract": "Retrieving visual and textual information from medical literature and hospital records can enhance diagnostic accuracy for clinical image interpretation. However, multimodal retrieval-augmented diagnosis is highly challenging. We explore a lightweight mechanism for enhancing diagnostic performance of retrieval-augmented LVLMs. We train a lightweight LVLM-aware multimodal retriever, such that the retriever learns to return images and texts that guide the LVLM toward correct predictions. In our low-resource setting, we perform only lightweight fine-tuning with small amounts of data, and use only general-purpose backbone models, achieving competitive results in clinical classification and VQA tasks compared to medically pre-trained models with extensive training. In a novel analysis, we highlight a previously unexplored class of errors that we term inconsistent retrieval predictions: cases where different top-retrieved images yield different predictions for the same target. We find that these cases are challenging for all models, even for non-retrieval models, and that our retrieval optimization mechanism significantly improves these cases over standard RAG. However, our analysis also sheds light on gaps in the ability of LVLMs to utilize retrieved information for clinical predictions. Code and models available at: https://github.com/Nirmaz/CLARE.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.17394.pdf",
    "abs_url": "https://arxiv.org/abs/2508.17394",
    "published": "2025-08-24T15:06:20Z",
    "updated": "2026-01-14T13:59:21Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出一种轻量级LVLM感知多模态检索机制，通过优化检索过程提升基于RAG的医疗诊断性能。",
      "motivation": "检索医疗图像和文本信息能够增强临床诊断的准确性，但多模态检索增强诊断面临挑战，现有方法通常依赖大量医学预训练数据，导致资源密集且难以适应低资源环境。本研究旨在解决这一问题，探索在低资源设置下开发轻量级机制，使用通用模型来弥补效率和应用范围的不足，推动高效诊断工具的发展。",
      "method": "论文训练一个轻量级的LVLM感知多模态检索器，该检索器通过学习返回引导大型视觉语言模型做出正确预测的图像和文本来优化检索过程。方法仅需少量数据进行轻量微调，并依赖通用骨干模型，避免了资源密集的医学预训练。关键创新在于将检索器设计为直接提升LVLM的诊断准确性，实现了在低资源环境下的高效多模态检索增强。",
      "result": "在临床分类和视觉问答任务中，所提方法取得了与广泛训练的医学预训练模型相竞争的性能，突显了轻量级策略的有效性。分析发现了一类新错误——不一致检索预测，即不同检索图像导致同一目标的不同预测；实验显示检索优化机制在这些情况下显著优于标准RAG方法。尽管未提供具体数据，但研究表明该方法在提升诊断稳定性和性能方面具有潜力。",
      "conclusion": "该研究成功开发了一种轻量级LVLM感知多模态检索方法，有效提升了基于RAG的医疗诊断性能，尤其适用于低资源场景。研究揭示了大型视觉语言模型在利用检索信息时的局限性，强调了优化多模态交互的重要性。这为未来改进诊断系统和探索更高效检索机制提供了学术参考和实用价值，但未明确指出具体局限性，仅提及信息利用不足作为未来方向。",
      "tags": [
        "Large Vision Language Model",
        "Retrieval-Augmented Generation",
        "Multimodal Retrieval",
        "Fine-tuning",
        "Medical Diagnosis"
      ]
    },
    "analyzed_at": "2026-01-15T03:28:44.705095Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.17200",
    "title": "Large Language Model-Based Automatic Formulation for Stochastic Optimization Models",
    "authors": [
      "Amirreza Talebi"
    ],
    "abstract": "This paper presents an integrated systematic study of the performance of large language models (LLMs), specifically ChatGPT, for automatically formulating and solving Stochastic Optimization (SO) problems from natural language descriptions. Focusing on three key categories, individual chance-constrained models, joint chance-constrained models, and two-stage stochastic mixed-integer linear programming models, we design several prompts that guide ChatGPT through structured tasks using chain-of-thought and agentic reasoning. We introduce a novel soft-scoring metric that evaluates the structural quality and partial correctness of generated models, addressing the limitations of canonical and execution-based accuracy metrics. Across a diverse set of SO problems, GPT-4-Turbo achieves better partial scores than GPT-3.5 variants except for individual chance-constrained problems. Structured prompts significantly outperform simple prompting, reducing extra-element generation and improving objective matching, although extra-element generation remains a nontrivial task. Our findings reveal that with well-engineered prompts and multi-agent collaboration, LLMs can facilitate SO formulations, paving the way for intelligent, language-driven modeling pipelines for SO in practice.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.17200.pdf",
    "abs_url": "https://arxiv.org/abs/2508.17200",
    "published": "2025-08-24T03:31:25Z",
    "updated": "2026-01-14T16:47:12Z",
    "comment": null,
    "light_analysis": {
      "overview": "本论文提出使用大型语言模型（特别是 ChatGPT）通过结构化提示和软评分指标自动从自然语言描述生成随机优化模型。",
      "motivation": "随机优化模型在工业和管理科学中应用广泛，但手动制定过程复杂且容易出错。现有评估方法如经典准确率和基于执行的指标未能充分评估模型结构完整性和部分正确性，限制了自动化系统的可靠性。因此，本研究旨在探索大型语言模型在自动制定随机优化问题中的性能，并改进评估指标以更好地反映生成质量，解决实际建模中的效率和质量问题。",
      "method": "研究设计了多种结构化提示，利用链式思维和代理推理引导大型语言模型生成三类随机优化模型：个体机会约束模型、联合机会约束模型和两阶段随机混合整数线性规划模型。引入了软评分指标，评估生成模型的结构质量和部分正确性，以弥补传统评估方法的不足。通过多代理协作策略优化提示，提升生成的准确性和鲁棒性，聚焦于模型架构和数据集描述。",
      "result": "在多样化随机优化问题测试中，GPT-4-Turbo 在除个体机会约束问题外的大部分类别中，其软评分优于 GPT-3.5 变体。结构化提示显著减少了额外元素的生成并改善了目标匹配度，尽管额外元素生成仍具挑战性。软评分指标成功揭示了生成模型的部分正确性和结构优势，验证了提示工程的有效性，与基线方法相比提升了建模质量。",
      "conclusion": "研究表明，通过精心设计的提示和多代理协作，大型语言模型能够有效辅助随机优化模型的自动制定，为智能、语言驱动的建模管道提供了实践基础。该成果具有重要学术价值，推动了自动化建模技术的发展，并指出了如额外元素生成等未来研究方向，为实际应用铺平道路。",
      "tags": [
        "Large Language Model",
        "Stochastic Optimization",
        "Chain-of-Thought",
        "Prompt Engineering",
        "Multi-Agent Collaboration"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:00.187294Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.14957",
    "title": "CuMoLoS-MAE: A Masked Autoencoder for Remote Sensing Data Reconstruction",
    "authors": [
      "Anurup Naskar",
      "Nathanael Zhixin Wong",
      "Sara Shamekh"
    ],
    "abstract": "Accurate atmospheric profiles from remote sensing instruments such as Doppler Lidar, Radar, and radiometers are frequently corrupted by low-SNR (Signal to Noise Ratio) gates, range folding, and spurious discontinuities. Traditional gap filling blurs fine-scale structures, whereas deep models lack confidence estimates. We present CuMoLoS-MAE, a Curriculum-Guided Monte Carlo Stochastic Ensemble Masked Autoencoder designed to (i) restore fine-scale features such as updraft and downdraft cores, shear lines, and small vortices, (ii) learn a data-driven prior over atmospheric fields, and (iii) quantify pixel-wise uncertainty. During training, CuMoLoS-MAE employs a mask-ratio curriculum that forces a ViT decoder to reconstruct from progressively sparser context. At inference, we approximate the posterior predictive by Monte Carlo over random mask realisations, evaluating the MAE multiple times and aggregating the outputs to obtain the posterior predictive mean reconstruction together with a finely resolved per-pixel uncertainty map. Together with high-fidelity reconstruction, this novel deep learning-based workflow enables enhanced convection diagnostics, supports real-time data assimilation, and improves long-term climate reanalysis.",
    "categories": [
      "cs.LG",
      "physics.ao-ph"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.14957.pdf",
    "abs_url": "https://arxiv.org/abs/2508.14957",
    "published": "2025-08-20T16:54:19Z",
    "updated": "2026-01-14T05:43:13Z",
    "comment": "Accepted for poster presentation at the NeurIPS 2025 workshop on Tackling Climate Change with Machine Learning. 5 pages, 2 figures",
    "light_analysis": {
      "overview": "论文提出CuMoLoS-MAE，一种课程引导的蒙特卡洛随机集成掩码自编码器，用于遥感数据高保真重建并量化像素级不确定性。",
      "motivation": "遥感仪器如多普勒激光雷达获取的大气剖面数据常受低信噪比、范围折叠和虚假不连续性影响，导致失真。传统间隙填充方法在处理这些噪声时会模糊精细尺度结构如涡旋和剪切线，而现有深度模型缺乏置信度估计，无法提供不确定性信息，这限制了在实时数据同化和长期气候再分析等关键气象应用中的可靠性。因此，开发一种既能恢复细节特征又能评估不确定性的新方法，对于提升大气科学研究和预测精度至关重要。",
      "method": "CuMoLoS-MAE基于掩码自编码器框架，核心创新在于结合课程学习和蒙特卡洛随机集成。训练阶段，采用掩码比率课程策略，逐步增加掩码比例，迫使视觉变换器（ViT）解码器从稀疏上下文学习重建，增强模型对复杂大气模式的建模能力。推理阶段，通过蒙特卡洛随机采样生成多个掩码实现，多次运行MAE并聚合输出，近似后验预测分布，从而获得平均重建结果和精细的每像素不确定性图，实现数据驱动先验学习和不确定性量化。",
      "result": "摘要未明确说明具体性能指标如准确率提升，但论文表明CuMoLoS-MAE能够实现高保真重建，有效恢复精细尺度特征如上升流、下降流核心和小涡旋，并生成像素级不确定性映射。这克服了传统方法模糊细节和深度模型缺乏置信度的问题，为增强对流诊断、支持实时数据同化和改进气候再分析提供了更可靠的数据基础。与基线方法相比，推断在保留结构完整性和不确定性估计方面有显著优势，但具体数据未提供。",
      "conclusion": "CuMoLoS-MAE的主要贡献是提出了一种集成课程学习和蒙特卡洛的掩码自编码器，实现了遥感数据的高质量重建和不确定性量化。学术上，它融合了自监督学习和概率建模，推动了自编码器在气象领域的应用；实际上，它为对流分析、数据同化和气候研究提供了更精确的工具。潜在局限性可能包括计算资源需求或对特定数据集的依赖性，未来工作可扩展模型泛化或结合其他机器学习技术优化效率。",
      "tags": [
        "Masked Autoencoder",
        "Vision Transformer (ViT)",
        "Monte Carlo",
        "Uncertainty Quantification",
        "Curriculum Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:09.219782Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.12079",
    "title": "Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks",
    "authors": [
      "Ningzhe Shi",
      "Yiqing Zhou",
      "Ling Liu",
      "Jinglin Shi",
      "Yihao Wu",
      "Haiwei Shi",
      "Hanxiao Yu"
    ],
    "abstract": "Integrated sensing and communication (ISAC) can enhance artificial intelligence-generated content (AIGC) networks by providing efficient sensing and transmission. Existing AIGC services usually assume that the accuracy of the generated content can be ensured, given accurate input data and prompt, thus only the content generation quality (CGQ) is concerned. However, it is not applicable in ISAC-based AIGC networks, where content generation is based on inaccurate sensed data. Moreover, the AIGC model itself introduces generation errors, which depend on the number of generating steps (i.e., computing resources). To assess the quality of experience of ISAC-based AIGC services, we propose a content accuracy and quality aware service assessment metric (CAQA). Since allocating more resources to sensing and generating improves content accuracy but may reduce communication quality, and vice versa, this sensing-generating (computing)-communication three-dimensional resource tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution space that grows exponentially with the number of users. To solve the CAQA-AIGC problem with low complexity, a linear programming (LP) guided deep reinforcement learning (DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the LP-guided approach and the action filter, LPDRL-F can transform the original three-dimensional solution space to two dimensions, reducing complexity while improving the learning performance of DRL. Simulations show that compared to existing DRL and generative diffusion model (GDM) algorithms without LP, LPDRL-F converges faster and finds better resource allocation solutions, improving AvgCAQA by more than 10%. With LPDRL-F, CAQA-AIGC can achieve an improvement in AvgCAQA of more than 50% compared to existing schemes focusing solely on CGQ.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.12079.pdf",
    "abs_url": "https://arxiv.org/abs/2508.12079",
    "published": "2025-08-16T15:29:59Z",
    "updated": "2026-01-14T15:28:17Z",
    "comment": "Accepted by IEEE TMC",
    "light_analysis": {
      "overview": "本文提出了一种基于线性规划指导的深度强化学习算法（LPDRL-F），用于优化ISAC驱动的AIGC网络中的资源分配，以提高内容准确性和质量的综合评估指标。",
      "motivation": "ISAC驱动的AIGC网络面临内容准确性挑战，因为内容基于不准确的传感数据生成，且AIGC模型自身引入生成错误，错误率依赖于计算资源。现有AIGC服务通常假设输入数据准确，仅关注内容生成质量（CGQ），但在ISAC环境中这不适用。资源分配需要在传感、生成（计算）和通信三个维度上优化，以最大化用户体验指标CAQA，但这是一个NP-hard问题，解空间随用户数指数增长，现有方法难以高效解决，因此需要开发新的优化算法来处理这一复杂权衡。",
      "method": "论文提出LPDRL-F算法，结合线性规划（LP）和深度强化学习（DRL），并引入动作滤波器。通过LP指导，将原始三维资源分配问题（传感、生成、通信）转化为二维问题，降低复杂度并提高DRL学习性能。关键创新点包括LP引导的降维策略和动作滤波器，用于优化所有用户的平均CAQA（AvgCAQA），在ISAC-AIGC网络中实现高效资源分配。摘要未明确说明使用的数据集或模型架构具体细节。",
      "result": "模拟结果显示，LPDRL-F算法相比未使用LP的现有DRL和生成扩散模型（GDM）算法，收敛速度更快，找到更好的资源分配方案，使平均CAQA（AvgCAQA）提升超过10%。此外，采用CAQA-AIGC评估指标后，相比仅关注CGQ的现有方案，AvgCAQA提升超过50%，证明了算法在提升用户体验方面的有效性。",
      "conclusion": "本文的主要贡献是提出了内容准确性和质量感知的服务评估指标（CAQA）以及LPDRL-F算法，解决了ISAC-AIGC网络中的资源分配问题。研究通过结合LP和DRL，应对NP-hard挑战，提高了用户体验，为AIGC服务在传感不准确环境下的质量保障提供了新方法，具有学术创新和实际应用价值。未来工作可能包括扩展到更多资源维度或验证在真实场景中的性能。",
      "tags": [
        "Integrated Sensing and Communication",
        "Artificial Intelligence-Generated Content",
        "Deep Reinforcement Learning",
        "Linear Programming",
        "Resource Allocation"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:33.935284Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.10337",
    "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering",
    "authors": [
      "Chenliang Zhang",
      "Lin Wang",
      "Yuanyuan Lu",
      "Yusheng Qi",
      "Kexin Wang",
      "Peixu Hou",
      "Wenshi Chen"
    ],
    "abstract": "This paper describes the solutions of the Dianping-Trust-Safety team for the META CRAG-MM challenge. The challenge requires building a comprehensive retrieval-augmented generation system capable for multi-modal multi-turn question answering. The competition consists of three tasks: (1) answering questions using structured data retrieved from an image-based mock knowledge graph, (2) synthesizing information from both knowledge graphs and web search results, and (3) handling multi-turn conversations that require context understanding and information aggregation from multiple sources. For Task 1, our solution is based on the vision large language model, enhanced by supervised fine-tuning with knowledge distilled from GPT-4.1. We further applied curriculum learning strategies to guide reinforcement learning, resulting in improved answer accuracy and reduced hallucination. For Task 2 and Task 3, we additionally leveraged web search APIs to incorporate external knowledge, enabling the system to better handle complex queries and multi-turn conversations. Our approach achieved 1st place in Task 1 with a significant lead of 52.38%, and 3rd place in Task 3, demonstrating the effectiveness of the integration of curriculum learning with reinforcement learning in our training pipeline.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.10337.pdf",
    "abs_url": "https://arxiv.org/abs/2508.10337",
    "published": "2025-08-14T04:37:56Z",
    "updated": "2026-01-14T13:50:30Z",
    "comment": null,
    "light_analysis": {
      "overview": "这篇论文提出了一种基于课程学习的强化学习方法，用于多模态检索增强生成，显著提高了答案准确性并减少了幻觉。",
      "motivation": "研究动机是解决META CRAG-MM挑战中的多模态多轮问题回答任务，该挑战要求构建一个全面的检索增强生成系统，能够处理图像知识图、网络搜索和多轮对话。现有方法可能在复杂查询和信息整合方面存在不足，导致准确性不高或幻觉问题，因此开发有效的方法以提升系统性能在AI应用中至关重要。摘要未明确说明具体不足，但挑战本身强调了多源信息处理的难点。",
      "method": "研究方法包括：针对任务1，使用视觉大型语言模型，并通过从GPT-4.1提取的知识进行监督微调增强，关键创新是应用课程学习策略引导强化学习以优化训练过程。对于任务2和3，额外整合网络搜索API以处理外部知识。整个技术路线结合了检索增强生成框架，旨在处理多模态输入和多轮对话，核心是通过课程学习逐步指导强化学习，提高模型稳定性和性能。",
      "result": "主要实验结果显示，该方法在任务1中取得第一名，领先52.38%，在任务3中获得第三名，证明了课程学习与强化学习结合的有效性。摘要提到答案准确性提高和幻觉减少，但未给出具体指标。与基线方法的对比未明确说明，但比赛排名表明该方法优于其他参赛方案，突出了其在多模态问题回答中的优势。",
      "conclusion": "结论表明，研究成功整合了课程学习与强化学习到训练流程中，提升了多模态检索增强生成系统的性能。学术价值在于验证了课程学习在引导强化学习中的应用潜力，为AI训练策略提供了新思路。实际应用价值在于构建更准确、可靠的多模态问题回答系统。未来工作可能包括进一步优化方法以处理更复杂任务或扩展应用范围，摘要未明确说明局限性。",
      "tags": [
        "Retrieval-Augmented Generation",
        "Curriculum Learning",
        "Reinforcement Learning",
        "Large Language Model",
        "Multimodal Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:29:22.043790Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.06475",
    "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning",
    "authors": [
      "Guimin Hu",
      "Daniel Hershcovich",
      "Hasti Seifi"
    ],
    "abstract": "Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.06475.pdf",
    "abs_url": "https://arxiv.org/abs/2508.06475",
    "published": "2025-08-08T17:25:37Z",
    "updated": "2026-01-14T15:59:08Z",
    "comment": null,
    "light_analysis": {
      "overview": "HapticLLaMA 是一个用于触觉描述的多模态语言模型，通过集成振动信号和两阶段训练实现了高效描述生成。",
      "motivation": "研究动机源于触触觉信号在虚拟现实、无障碍和康复等应用中的重要性，这些领域需要从振动等触觉信号生成自然语言描述以增强交互和辅助功能。现有多模态研究主要集中在视觉和音频模态，导致触觉信号描述生成任务未被充分探索，缺乏有效方法处理触觉数据。因此，本研究旨在填补这一技术空白，开发能够理解和生成触觉描述的系统，以推动多模态人工智能的发展并满足实际应用需求。",
      "method": "研究方法包括提出 HapticLLaMA 模型，该模型使用两种触觉分词器——基于频率的分词器和基于 EnCodec 的分词器，将连续振动信号转换为离散令牌序列，以便与大型语言模型 LLaMA 集成。训练过程分为两个阶段：首先进行监督微调，采用 LLaMA 架构结合 LoRA 进行参数适应；然后通过强化学习从人类反馈进行微调，以优化模型输出与人类感知的对齐。这种设计实现了触觉模态与文本模态的有效融合，并利用先进训练技术提升性能。",
      "result": "实验结果显示，HapticLLaMA 在触觉描述任务中表现优异，自动化评估指标上获得 METEOR 分数 59.98 和 BLEU-4 分数 32.06，表明生成描述具有较高准确性。在人类评估中，超过 61% 的生成描述在 7 点量表上获得高于 3.5 的评分，且强化学习从人类反馈微调带来了整体评分分布 10% 的改进，显著提升了模型输出与人类触觉感知的一致性，验证了方法在真实场景中的有效性。",
      "conclusion": "本研究得出结论，HapticLLaMA 成功实现了触觉信号到自然语言描述的转换，证明了大型语言模型在处理和适应感官数据方面的潜力。这一贡献具有重要学术价值，扩展了多模态学习的研究范围，为触觉模态探索提供了新视角。在实际应用中，它为虚拟现实、无障碍技术和康复领域提供了实用工具，未来工作可扩展到其他感官模态集成或进一步优化模型泛化能力。",
      "tags": [
        "Haptic Captioning",
        "Large Language Model",
        "Reinforcement Learning from Human Feedback",
        "LoRA",
        "EnCodec"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:22.687319Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.05065",
    "title": "Decoupling Continual Semantic Segmentation",
    "authors": [
      "Yifu Guo",
      "Yuquan Lu",
      "Wentao Zhang",
      "Zishan Xu",
      "Dexia Chen",
      "Siyu Zhang",
      "Yizhe Zhang",
      "Ruixuan Wang"
    ],
    "abstract": "Continual Semantic Segmentation (CSS) requires learning new classes without forgetting previously acquired knowledge, addressing the fundamental challenge of catastrophic forgetting in dense prediction tasks. However, existing CSS methods typically employ single-stage encoder-decoder architectures where segmentation masks and class labels are tightly coupled, leading to interference between old and new class learning and suboptimal retention-plasticity balance. We introduce DecoupleCSS, a novel two-stage framework for CSS. By decoupling class-aware detection from class-agnostic segmentation, DecoupleCSS enables more effective continual learning, preserving past knowledge while learning new classes. The first stage leverages pre-trained text and image encoders, adapted using LoRA, to encode class-specific information and generate location-aware prompts. In the second stage, the Segment Anything Model (SAM) is employed to produce precise segmentation masks, ensuring that segmentation knowledge is shared across both new and previous classes. This approach improves the balance between retention and adaptability in CSS, achieving state-of-the-art performance across a variety of challenging tasks. Our code is publicly available at: https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.05065.pdf",
    "abs_url": "https://arxiv.org/abs/2508.05065",
    "published": "2025-08-07T06:34:34Z",
    "updated": "2026-01-14T11:28:28Z",
    "comment": "https://github.com/euyis1019/Decoupling-Continual-Semantic-Segmentation",
    "light_analysis": {
      "overview": "DecoupleCSS通过解耦类感知检测和类无关分割，提出一个两阶段框架，以改进持续语义分割中的保留-可塑性平衡。",
      "motivation": "持续语义分割（CSS）需在学习新类别时避免灾难性遗忘，以应对密集预测任务的实际需求。现有方法多采用单阶段编码器-解码器架构，导致分割掩码与类别标签紧密耦合，新旧类别学习相互干扰，难以优化知识保留和学习新知识的平衡。该问题的重要性在于提升CSS的鲁棒性，以支持动态环境中的应用，如自动驾驶和视觉系统更新。",
      "method": "DecoupleCSS框架分为两个阶段：第一阶段使用预训练的文本和图像编码器，通过LoRA进行适应，编码类别特定信息并生成位置感知提示；第二阶段利用Segment Anything Model（SAM）生成精确的分割掩码，确保分割知识在新旧类别间共享。创新点在于解耦类别感知检测与类别无关分割，采用两阶段设计减少干扰，并集成预训练模型增强泛化能力，无需依赖单一架构。",
      "result": "DecoupleCSS在多种挑战性持续语义分割任务中实现了最先进的性能，显著改善了保留与适应性之间的平衡。与基线方法相比，该方法有效学习新类别同时保留旧类别知识，但摘要未明确说明具体性能指标数据，如准确率提升或效率改进的具体数值，仅强调整体性能优势。",
      "conclusion": "DecoupleCSS通过解耦框架解决了CSS中的保留-可塑性平衡问题，贡献了新颖的两阶段方法，具有重要学术价值，推动了持续学习在语义分割领域的进展。实际应用潜力包括自适应视觉系统，局限性可能在于计算开销或任务泛化，未来工作可扩展至更多复杂场景或优化模型效率。",
      "tags": [
        "Continual Semantic Segmentation",
        "Segment Anything Model",
        "LoRA",
        "Two-Stage Framework",
        "Dense Prediction"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:19.262824Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2508.01741",
    "title": "Simulated Ensemble Attack: Transferring Jailbreaks Across Fine-tuned Vision-Language Models",
    "authors": [
      "Ruofan Wang",
      "Xin Wang",
      "Yang Yao",
      "Juncheng Li",
      "Xuan Tong",
      "Xingjun Ma"
    ],
    "abstract": "The widespread practice of fine-tuning open-source Vision-Language Models (VLMs) raises a critical security concern: jailbreak vulnerabilities in base models may persist in downstream variants, enabling transferable attacks across fine-tuned systems. To investigate this risk, we propose the Simulated Ensemble Attack (SEA), a grey-box jailbreak framework that assumes full access to the base VLM but no knowledge of the fine-tuned target. SEA enhances transferability via Fine-tuning Trajectory Simulation (FTS), which models bounded parameter variations in the vision encoder, and Targeted Prompt Guidance (TPG), which stabilizes adversarial optimization through auxiliary textual guidance. Experiments on the Qwen2-VL family demonstrate that SEA achieves consistently high transfer success and toxicity rates across diverse fine-tuned variants, including safety-enhanced models, while standard PGD-based image jailbreaks exhibit negligible transferability. Further analysis reveals that fine-tuning primarily induces localized parameter shifts around the base model, explaining why attacks optimized over a simulated neighborhood transfer effectively. We also show that SEA generalizes across different base generations (e.g., Qwen2.5/3-VL), indicating that its effectiveness arises from shared fine-tuning-induced behaviors rather than architecture- or initialization-specific factors.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.01741.pdf",
    "abs_url": "https://arxiv.org/abs/2508.01741",
    "published": "2025-08-03T12:51:47Z",
    "updated": "2026-01-14T02:53:24Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出Simulated Ensemble Attack (SEA)框架，通过微调轨迹模拟和目标提示指导，实现跨微调视觉语言模型的jailbreak攻击转移。",
      "motivation": "随着开源视觉语言模型微调的普及，基础模型中的jailbreak漏洞可能在微调变体中持续存在，导致可转移的安全威胁。现有方法如基于PGD的图像jailbreak攻击在跨微调系统时转移性低，不足以有效评估这种风险。因此，研究旨在探索jailbreak攻击的转移性，以揭示潜在安全隐患并提供评估工具。",
      "method": "SEA是一种灰色框jailbreak框架，假设对基础视觉语言模型有完全访问权限，但对微调目标无知识。其核心包括Fine-tuning Trajectory Simulation (FTS)，用于模拟微调过程中视觉编码器参数的有界变化；Targeted Prompt Guidance (TPG)，通过辅助文本指导稳定对抗优化。这些技术结合参数模拟和文本引导，增强攻击在微调模型间的转移能力。",
      "result": "在Qwen2-VL家族上的实验表明，SEA实现了高转移成功率和毒性率，覆盖多种微调变体（包括安全增强模型），而标准PGD-based攻击转移性可忽略。分析揭示微调主要诱导基础模型周围的局部参数偏移，解释攻击在模拟邻域中的有效转移。此外，SEA在不同基础生成（如Qwen2.5/3-VL）上泛化良好，表明其有效性源于共享的微调行为而非特定架构或初始化因素。",
      "conclusion": "SEA框架证实了微调视觉语言模型中jailbreak漏洞的可转移性，强调微调安全性的重要性。研究不仅提供了有效的攻击方法，还通过分析微调行为提供了理论解释，具有学术和实际应用价值。未来工作可探索更广泛的模型应用和潜在防御策略，以增强系统安全性。",
      "tags": [
        "Vision-Language Models",
        "Jailbreak Attack",
        "Fine-tuning",
        "Transferable Attacks",
        "Adversarial Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:20.488926Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.22500",
    "title": "Nonlinear reconciliation: Error reduction theorems",
    "authors": [
      "Lorenzo Nespoli",
      "Anubhab Biswas",
      "Roberto Rocchetta",
      "Vasco Medici"
    ],
    "abstract": "Forecast reconciliation, an ex-post technique applied to forecasts that must satisfy constraints, has been a prominent topic in the forecasting literature over the past two decades. Recently, several efforts have sought to extend reconciliation methods to the probabilistic settings. Nevertheless, formal theorems demonstrating error reduction in nonlinear constraints, analogous to those presented in Panagiotelis et al.(2021), are still lacking. This paper addresses that gap by establishing such theorems for various classes of nonlinear hypersurfaces and vector-valued functions. Specifically, we derive an exact analog of Theorem 3.1 from Panagiotelis et al.(2021) for hypersurfaces with constant-sign curvature. Additionally, we provide an error reduction theorem for the broader case of hypersurfaces with non-constant-sign curvature and for general manifolds with codimension > 1. To support reproducibility and practical adoption, we release a JAX-based Python package, JNLR, implementing the presented theorems and reconciliation procedures.",
    "categories": [
      "cs.LG",
      "cs.CG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.22500.pdf",
    "abs_url": "https://arxiv.org/abs/2507.22500",
    "published": "2025-07-30T09:14:51Z",
    "updated": "2026-01-14T13:09:42Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文建立了非线性约束下的误差减少定理，填补了现有理论的空白。",
      "motivation": "预测协调作为预测学中的重要技术，用于确保预测满足约束条件，近年来已扩展到概率设置，但缺乏针对非线性约束的正式误差减少定理。现有工作如Panagiotelis等人（2021）只提供了线性或特定情况下的定理，对于更广泛的非线性超曲面和流形，理论基础不足，导致预测协调方法在复杂约束下的应用受限。本研究旨在解决这一问题，通过建立定理填补理论空白，推动预测协调技术的发展。",
      "method": "本研究通过推导多个误差减少定理来扩展非线性约束的预测协调理论。具体包括：针对常数符号曲率的超曲面，推导了与Panagiotelis等人定理类似的精确定理；进一步扩展到非常数符号曲率的超曲面和余维大于1的流形。关键创新点在于首次为这些非线性情况提供了正式定理。为实现再现性和实用，论文发布了基于JAX的Python包JNLR，实现了所提出的定理和协调过程。",
      "result": "论文成功建立了非线性约束下的误差减少定理，包括常数符号曲率超曲面的精确定理扩展，以及更广泛的非常数符号曲率超曲面和高余维流形的定理。这些结果为非线性预测协调提供了理论保证，填补了现有研究的空白。虽然摘要未提供具体实验数据，但通过理论推导与Panagiotelis等人工作的对比，验证了扩展的合理性和一般性，强调了定理的普适性和应用潜力。",
      "conclusion": "本研究的主要贡献在于填补了非线性约束下误差减少定理的空白，为预测协调理论的发展提供了重要支持。通过建立多种非线性情况的定理，增强了预测方法的理论基础，并发布了实用工具JNLR促进实际应用。学术价值在于扩展了现有定理的适用范围，实际价值在于为研究者和从业者提供了理论依据和实现代码。未来工作可能包括更广泛的应用场景探索和实验验证。",
      "tags": [
        "Forecast Reconciliation",
        "Error Reduction Theorems",
        "Nonlinear Constraints",
        "Hypersurfaces",
        "JAX"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:15.218118Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.21184",
    "title": "Can Language Models Discover Scaling Laws?",
    "authors": [
      "Haowei Lin",
      "Haotian Ye",
      "Wenzheng Feng",
      "Quzhe Huang",
      "Yujun Li",
      "Hubert Lim",
      "Zhengrui Li",
      "Xiangyu Wang",
      "Jianzhu Ma",
      "James Zou",
      "Yitao Liang"
    ],
    "abstract": "Discovering scaling laws for predicting model performance at scale is a fundamental and open-ended challenge, mostly reliant on slow, case specific human experimentation. To investigate the potential for LLMs to automate this process, we collect over 5,000 experiments from existing literature and curate eight diverse scaling law discovery tasks. While existing agents struggle to produce accurate law formulas, this paper introduces SLDAgent, an evolution-based agent that co-optimize the scaling law model and the parameters, enabling it to autonomously explore complex relationships between variables. For the first time, we demonstrates that SLDAgent can automatically discover laws that exhibit consistently more accurate extrapolation than their established, human-derived counterparts across all tasks. Through comprehensive analysis, we elucidate why these discovered laws are superior and verify their practical utility in both pretraining and finetuning applications. This work establishes a new paradigm for agentic scientific discovery, showing that AI systems can understand their own scaling behavior, and can contribute novel and practical knowledge back to the research community.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.21184.pdf",
    "abs_url": "https://arxiv.org/abs/2507.21184",
    "published": "2025-07-27T05:45:26Z",
    "updated": "2026-01-14T10:48:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文引入SLDAgent，首次证明基于进化的AI代理能自动发现比人类更优的缩放定律，推动代理科学发现新范式。",
      "motivation": "发现预测模型性能的缩放定律是机器学习的基础挑战，但当前依赖缓慢、特定案例的人工实验，限制了效率和可扩展性。现有自动化代理难以生成准确的定律公式，导致这一过程仍受限于人类智慧。因此，研究旨在探索语言模型自动发现缩放定律的潜力，以解决现有方法的不足，提升科学发现的自动化水平。",
      "method": "研究提出SLDAgent，一种基于进化算法的代理，通过共同优化缩放定律模型和参数，自动探索变量间的复杂关系。该方法利用收集的超过5,000个实验数据和八个多样化任务，进行系统评估。SLDAgent能够自主调整模型结构，无需人工干预，从而实现缩放定律的自动化发现，关键创新在于首次使用进化优化来适应复杂缩放行为。",
      "result": "SLDAgent在所有八个任务中自动发现的缩放定律，外推准确性一致超越传统人类推导的定律，未提供具体数据但强调了整体性能优势。通过与基线方法（如现有代理）对比，验证了其在实际应用（如预训练和微调）中的实用性。结果表明，SLDAgent能有效自动化科学发现过程，提升预测模型的可靠性和效率。",
      "conclusion": "这项研究确立了代理科学发现的新范式，首次展示AI系统能自主理解并优化自身缩放行为，为机器学习社区贡献新颖知识。通过SLDAgent发现的更优缩放定律具有实际应用价值，推动了自动化方法的发展。未来工作可扩展此方法到其他科学发现领域，并进一步探索其在复杂系统中的潜力。摘要未明确说明具体局限性，但暗示了广泛的适用性。",
      "tags": [
        "Large Language Model",
        "Scaling Law",
        "Evolutionary Algorithm",
        "Scientific Discovery",
        "Agent-based Systems"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:36.930053Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.15112",
    "title": "Distributional Machine Unlearning via Selective Data Removal",
    "authors": [
      "Youssef Allouah",
      "Rachid Guerraoui",
      "Sanmi Koyejo"
    ],
    "abstract": "Machine learning systems increasingly face requirements to remove entire domains of information--such as toxic language or biases--rather than individual user data. This task presents a dilemma: full removal of the unwanted domain data is computationally expensive, while random partial removal is statistically inefficient. We find that a domain's statistical influence is often concentrated in a small subset of its data samples, suggesting a path between ineffective partial removal and unnecessary complete removal. We formalize this as distributional unlearning: a framework to select a small subset that balances forgetting an unwanted distribution while preserving a desired one. Using Kullback-Leibler divergence constraints, we derive the exact removal-preservation Pareto frontier for Gaussian distributions and prove that models trained on the edited data achieve corresponding log-loss bounds. We propose a distance-based selection algorithm and show it is quadratically more sample-efficient than random removal in the challenging low-divergence regime. Experiments across synthetic, text, and image datasets (Jigsaw, CIFAR-10, SMS spam) show our method requires 15-82% less deletion than full removal for strong unlearning effects, e.g., halving initial forget set accuracy. Ultimately, by showing a small forget set often suffices, our framework lays the foundations for more scalable and rigorous subpopulation unlearning.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.15112.pdf",
    "abs_url": "https://arxiv.org/abs/2507.15112",
    "published": "2025-07-20T20:21:23Z",
    "updated": "2026-01-14T04:19:32Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出了一种基于选择性数据移除的分布级机器学习去学习框架，通过识别并移除少量关键样本实现高效子群遗忘。",
      "motivation": "机器学习系统在应对法规和伦理需求时，需要移除如毒性语言或偏见等整个领域的信息，而非仅删除单个用户数据。现有方法面临困境：完全移除计算成本高昂，随机部分移除统计效率低下，导致模型性能受损。这在实际应用中至关重要，如满足GDPR删除权或减少模型偏见，但传统方法难以平衡移除效率与统计精度。因此，亟需一种能有效忘记指定分布同时最小化数据损失的方法，以解决计算开销与模型可靠性之间的冲突。",
      "method": "论文提出分布级去学习框架，使用Kullback-Leibler散度约束来形式化平衡忘记不需要分布与保留需要分布的问题。针对高斯分布，推导出移除-保留的精确帕累托前沿，并证明在编辑数据上训练的模型能达到相应对数损失界限。核心创新是提出基于距离的选择算法，通过量化数据样本对分布的影响，高效识别关键子集进行移除。该算法在低散度区域样本效率比随机移除高二次方，关键细节包括利用KL散度优化和数据样本的度量选择，适用于合成和真实数据集，但模型架构细节摘要未明确说明。",
      "result": "实验在合成、文本和图像数据集（Jigsaw、CIFAR-10、SMS spam）上进行，结果显示提出的方法比完全移除数据节省15%至82%的删除量，同时实现强去学习效果，例如将忘记集准确率减半，从高值显著降低。与基线方法（如随机移除）相比，在低KL散度场景下，基于距离的选择算法展现出二次方更高的样本效率，具体表现为效率提升倍数。这些结果验证了方法的统计有效性和计算效率，在多种数据集上均优于传统移除策略，但具体数据指标摘要未完全量化。",
      "conclusion": "该研究的主要贡献是提出并验证了分布级去学习框架，通过选择性数据移除为子群遗忘提供了高效解决方案。学术价值在于建立了理论基础，如KL散度约束和帕累托前沿分析，推动了机器学习可控遗忘领域的发展。实际应用价值在于支持数据隐私保护和模型伦理调整，助力法规合规。未来工作可能包括扩展到非高斯分布或更复杂场景，但摘要未明确说明具体局限性。框架为更可扩展和严格的子群去学习奠定了基础，促进了去学习技术的实用化。",
      "tags": [
        "Machine Unlearning",
        "Distributional Unlearning",
        "Selective Data Removal",
        "Kullback-Leibler Divergence",
        "Gaussian Models"
      ]
    },
    "analyzed_at": "2026-01-15T03:30:58.664691Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.10171",
    "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis",
    "authors": [
      "Youngmin Kim",
      "Giyeong Oh",
      "Kwangsoo Youm",
      "Youngjae Yu"
    ],
    "abstract": "Concrete workability is essential for construction quality, with the slump test being the most widely used on-site method for its assessment. However, traditional slump testing is manual, time-consuming, and highly operator-dependent, making it unsuitable for continuous or real-time monitoring during placement. To address these limitations, we present SlumpGuard, an AI-powered vision system that analyzes the natural discharge flow from a mixer-truck chute using a single fixed camera. The system performs automatic chute detection, pouring-event identification, and video-based slump classification, enabling quality monitoring without sensors, hardware installation, or manual intervention. We introduce the system design, construct a site-replicated dataset of over 6,000 video clips, and report extensive evaluations demonstrating reliable chute localization, accurate pouring detection, and robust slump prediction under diverse field conditions. An expert study further reveals significant disagreement in human visual estimates, highlighting the need for automated assessment.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.10171.pdf",
    "abs_url": "https://arxiv.org/abs/2507.10171",
    "published": "2025-07-14T11:33:47Z",
    "updated": "2026-01-14T06:25:00Z",
    "comment": "Accepted to Automation in Construction. Our project page: https://winston1214.github.io/SlumpGuard/",
    "light_analysis": {
      "overview": "SlumpGuard系统利用AI驱动的视觉分析，通过单个摄像头实时自动预测混凝土塌落度，实现无需人工干预的施工质量监控。",
      "motivation": "传统混凝土塌落度测试依赖手动操作，过程耗时且结果易受操作员主观影响，无法满足施工中连续或实时监控需求，这限制了质量控制效率和准确性。现有方法缺乏自动化，难以适应现代建筑工业的高效要求，因此开发自动化塌落度预测系统至关重要，以克服人工测试的不一致性和延迟问题。",
      "method": "SlumpGuard系统采用AI驱动的视觉技术，使用固定摄像头捕获搅拌车溜槽排放视频，进行自动溜槽检测、浇筑事件识别和视频分类来实现塌落度预测。研究构建了包含超过6000个视频剪辑的现场复制数据集，并介绍了系统设计细节，关键创新在于无需传感器、硬件安装或人工干预，仅依赖摄像头和分析算法完成实时监控。",
      "result": "实验评估显示，SlumpGuard在各种现场条件下实现了可靠的溜槽定位、准确的浇筑检测和鲁棒的塌落度预测。专家研究进一步表明人类视觉估计存在显著不一致，突显了自动评估系统的优势。尽管摘要未明确说明具体性能指标如准确率提升，但广泛验证证明了系统的有效性，并与传统方法相比提高了监控效率。",
      "conclusion": "本研究贡献了一个基于AI和视频分析的实时塌落度预测系统，提升了施工质量监控的自动化水平和效率。学术上，它推动了计算机视觉在工业应用中的发展；实践上，可减少人工依赖并优化工程质量控制。未来工作可包括优化算法以适应更复杂环境，或扩展应用到其他建筑材料评估中。",
      "tags": [
        "Video Analysis",
        "Machine Learning",
        "Computer Vision",
        "Real-Time Systems"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:07.215613Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.08860",
    "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction",
    "authors": [
      "Awais Manzoor",
      "M. Atif Qureshi",
      "Etain Kidney",
      "Luca Longo"
    ],
    "abstract": "Retention campaigns in customer relationship management often rely on churn prediction models evaluated using traditional metrics such as AUC and F1-score. However, these metrics fail to reflect financial outcomes and may mislead strategic decisions. We introduce e-Profits, a novel business-aligned evaluation metric that quantifies model performance based on customer lifetime value, retention probability, and intervention costs. Unlike existing profit-based metrics such as Expected Maximum Profit, which assume fixed population-level parameters, e-Profits uses Kaplan-Meier survival analysis to estimate tenure-conditioned (customer-level) one-period retention probabilities and supports granular, per-customer profit evaluation. We benchmark six classifiers across two telecom datasets (IBM Telco and Maven Telecom) and demonstrate that e-Profits reshapes model rankings compared to traditional metrics, revealing financial advantages in models previously overlooked by AUC or F1-score. The metric also enables segment-level insight into which models maximise return on investment for high-value customers. e-Profits provides a transparent, customer-level evaluation framework that bridges predictive modelling and profit-driven decision-making in operational churn management. All source code is available at: https://github.com/Awaismanzoor/eprofits.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.08860.pdf",
    "abs_url": "https://arxiv.org/abs/2507.08860",
    "published": "2025-07-09T11:22:24Z",
    "updated": "2026-01-14T15:22:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文提出e-Profits，一个业务对齐的评估指标，用于客户流失预测，通过量化财务结果来评估模型性能。",
      "motivation": "在客户关系管理中，流失预测模型通常使用AUC和F1-score等传统指标评估，但这些指标未考虑财务结果，可能误导业务决策，导致资源分配不当。现有利润基指标如Expected Maximum Profit假设固定总体参数，无法进行细粒度、客户级评估，限制了其实际应用价值。因此，开发一种能直接反映模型对利润影响的新指标至关重要，以优化保留策略和提升决策效率。",
      "method": "e-Profits是一种基于Kaplan-Meier生存分析的评估指标，它估计条件于客户任期的单期保留概率，并结合客户生命周期价值、干预成本进行每个客户的利润计算。与现有方法不同，e-Profits支持细粒度评估，提供透明框架，弥补传统指标的不足，强调业务对齐和技术创新。它使用两个电信数据集（IBM Telco和Maven Telecom）进行测试，但摘要未明确说明具体模型架构细节。",
      "result": "在IBM Telco和Maven Telecom数据集上，对六个分类器进行基准测试，结果显示e-Profits改变了模型排名，相比AUC和F1-score，突出了先前被忽略模型的财务优势，例如某些模型在利润评估中表现更优。该指标还支持段级分析，识别在高价值客户群中最大化投资回报的模型，增强业务洞察力。摘要未明确具体性能提升数据，但强调了指标的重塑效果。",
      "conclusion": "e-Profits提供一个透明、客户级的评估框架，有效连接预测建模与利润驱动的流失管理决策，提升模型选择的业务相关性。其学术价值在于推动评估指标向业务对齐发展，实际应用价值在于优化企业保留策略和资源配置。未来工作可扩展至其他行业或集成更复杂的成本模型，开源代码便于进一步研究。",
      "tags": [
        "Customer Churn Prediction",
        "Evaluation Metric",
        "Kaplan-Meier Survival Analysis",
        "Profit-Sensitive",
        "Customer Lifetime Value"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:16.920631Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.05984",
    "title": "Development and Evaluation of HopeBot: an LLM-based chatbot for structured and interactive PHQ-9 depression screening",
    "authors": [
      "Zhijun Guo",
      "Alvina Lai",
      "Julia Ive",
      "Alexandru Petcu",
      "Yutong Wang",
      "Luyuan Qi",
      "Johan H Thygesen",
      "Kezhi Li"
    ],
    "abstract": "Static tools like the Patient Health Questionnaire-9 (PHQ-9) effectively screen depression but lack interactivity and adaptability. We developed HopeBot, a chatbot powered by a large language model (LLM) that administers the PHQ-9 using retrieval-augmented generation and real-time clarification. In a within-subject study, 132 adults in the United Kingdom and China completed both self-administered and chatbot versions. Scores demonstrated strong agreement (ICC = 0.91; 45% identical). Among 75 participants providing comparative feedback, 71% reported greater trust in the chatbot, highlighting clearer structure, interpretive guidance, and a supportive tone. Mean ratings (0-10) were 8.4 for comfort, 7.7 for voice clarity, 7.6 for handling sensitive topics, and 7.4 for recommendation helpfulness; the latter varied significantly by employment status and prior mental-health service use (p < 0.05). Overall, 87.1% expressed willingness to reuse or recommend HopeBot. These findings demonstrate voice-based LLM chatbots can feasibly serve as scalable, low-burden adjuncts for routine depression screening.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2507.05984.pdf",
    "abs_url": "https://arxiv.org/abs/2507.05984",
    "published": "2025-07-08T13:41:22Z",
    "updated": "2026-01-14T14:03:49Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文开发并评估了基于大型语言模型的聊天机器人HopeBot，实现结构化互动式PHQ-9抑郁症筛查，提升交互性和适应性。",
      "motivation": "现有抑郁症筛查工具如PHQ-9虽然有效，但缺乏交互性和适应性，导致无法根据用户反应进行调整，限制了筛查的准确性和用户体验。这一问题在心理健康领域尤为重要，因为交互式工具能提供更个性化支持，而现有静态方法难以满足此需求，因此研究旨在开发更灵活、可互动的解决方案。",
      "method": "论文提出HopeBot聊天机器人，利用大型语言模型（LLM）作为核心，结合检索增强生成技术来结构化生成PHQ-9问题，并集成实时澄清功能以增强用户互动。研究方法包括设计聊天机器人系统，并在132名来自英国和中国的成人参与者中进行用户实验，比较自我管理版本和聊天机器人版本的性能，关注技术实现和用户体验评估。",
      "result": "实验结果展示了强协议一致性，组内相关系数（ICC）为0.91，45%评分完全相同。在75名提供反馈的参与者中，71%表示更信任聊天机器人，因其结构更清晰、指导更明确、语调更支持。评分方面，平均舒适度8.4分，语音清晰度7.7分，处理敏感话题能力7.6分，推荐帮助度7.4分，后者在不同就业状况和心理健康服务使用背景下有显著差异（p < 0.05），整体87.1%愿意重用或推荐HopeBot。",
      "conclusion": "本研究证明了基于大型语言模型的语音聊天机器人可作为可扩展、低负担的辅助工具，用于常规抑郁症筛查，提升了筛查的交互性和用户体验。学术价值在于拓展了LLM在心理健康领域的应用，实际价值在于提供了一种易于部署的筛查解决方案。未来工作可进一步优化模型算法，扩大样本规模和多样性，以评估长期效果和临床适用性。",
      "tags": [
        "Large Language Model",
        "Chatbot",
        "Retrieval-Augmented Generation",
        "Depression Screening",
        "Interactive Assessment"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:02.391088Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.05950",
    "title": "Improving AI-Based Canine Heart Disease Diagnosis with Expert-Consensus Auscultation Labeling",
    "authors": [
      "Pinar Bisgin",
      "Tom Strube",
      "Niklas Tschorn",
      "Michael Pantförder",
      "Maximilian Fecke",
      "Ingrid Ljungvall",
      "Jens Häggström",
      "Gerhard Wess",
      "Christoph Schummer",
      "Sven Meister",
      "Falk M. Howar"
    ],
    "abstract": "Noisy labels pose significant challenges for AI model training in veterinary medicine. This study examines expert assessment ambiguity in canine auscultation data, highlights the negative impact of label noise on classification performance, and introduces methods for label noise reduction. To evaluate whether label noise can be minimized by incorporating multiple expert opinions, a dataset of 140 heart sound recordings (HSR) was annotated regarding the intensity of holosystolic heart murmurs caused by Myxomatous Mitral Valve Disease (MMVD). The expert opinions facilitated the selection of 70 high-quality HSR, resulting in a noise-reduced dataset. By leveraging individual heart cycles, the training data was expanded and classification robustness was enhanced. The investigation encompassed training and evaluating three classification algorithms: AdaBoost, XGBoost, and Random Forest. While AdaBoost and Random Forest exhibited reasonable performances, XGBoost demonstrated notable improvements in classification accuracy. All algorithms showed significant improvements in classification accuracy due to the applied label noise reduction, most notably XGBoost. Specifically, for the detection of mild heart murmurs, sensitivity increased from 37.71% to 90.98% and specificity from 76.70% to 93.69%. For the moderate category, sensitivity rose from 30.23% to 55.81% and specificity from 64.56% to 97.19%. In the loud/thrilling category, sensitivity and specificity increased from 58.28% to 95.09% and from 84.84% to 89.69%, respectively. These results highlight the importance of minimizing label noise to improve classification algorithms for the detection of canine heart murmurs. Index Terms: AI diagnosis, canine heart disease, heart sound classification, label noise reduction, machine learning, XGBoost, veterinary cardiology, MMVD.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.05950.pdf",
    "abs_url": "https://arxiv.org/abs/2507.05950",
    "published": "2025-07-08T12:48:25Z",
    "updated": "2026-01-14T10:03:29Z",
    "comment": "Accepted to IEEE Engineering in Medicine and Biology Conference (EMBC) 2025",
    "light_analysis": {
      "overview": "该研究通过引入基于专家共识的标签噪声减少方法，显著提升了犬类心脏杂音检测的AI分类算法性能。",
      "motivation": "在兽医医学中，AI模型训练常受标签噪声干扰，影响诊断准确性。本研究聚焦犬类心脏疾病诊断，特别是由MMVD引起的心脏杂音，现有方法因专家评估模糊性导致标签不一致，从而降低分类性能。问题的重要性在于准确诊断对宠物健康和临床实践至关重要，而现有标注方法可能引入噪声，限制了AI模型的可靠性和有效性，亟需改进以减少噪声并提高鲁棒性。",
      "method": "研究方法包括通过专家共识标注来减少标签噪声。具体步骤：收集140个犬类心音录音（HSR），标注全收缩期心脏杂音的强度；利用多名专家意见筛选出70个高质量样本，构建噪声减少的数据集。关键创新点是利用个体心周期扩展训练数据以增强分类鲁棒性。技术细节上，训练并评估了三种机器学习算法——AdaBoost、XGBoost和Random Forest，使用MMVD相关数据来验证方法的有效性，突出专家共识在数据预处理中的作用。",
      "result": "实验结果显示，所有分类算法在应用标签噪声减少后，准确性均有显著提升。XGBoost表现最佳：对于轻度心脏杂音检测，敏感性从37.71%提升到90.98%，特异性从76.70%提升到93.69%；中度类别敏感性从30.23%升至55.81%，特异性从64.56%升至97.19%；大声/震颤类别敏感性从58.28%增至95.09%，特异性从84.84%增至89.69%。这些数据与未减少噪声的基线对比，显示了噪声减少方法在改善所有算法性能方面的普遍有效性，尤其XGBoost的提升最为显著。",
      "conclusion": "本研究的主要贡献是成功通过专家共识标注减少了标签噪声，大幅提升了犬类心脏杂音检测的AI分类算法性能。学术价值在于为处理兽医医学中的标签噪声问题提供了有效方法，推动了AI在医疗诊断领域的应用研究。实际应用价值高，有助于提高犬类心脏疾病的诊断准确性，支持兽医心脏病学的临床实践。未来工作可能涉及扩展到其他疾病类型或更大数据集，但摘要未明确说明具体局限性或方向。",
      "tags": [
        "Label Noise Reduction",
        "XGBoost",
        "Heart Sound Classification",
        "Veterinary AI",
        "Expert-Consensus Labeling"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:18.700662Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.03285",
    "title": "Memory Mosaics at scale",
    "authors": [
      "Jianyu Zhang",
      "Léon Bottou"
    ],
    "abstract": "Memory Mosaics [Zhang et al., 2025], networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets.   To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (\"Memory Mosaics v2\"), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning.   Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2507.03285.pdf",
    "abs_url": "https://arxiv.org/abs/2507.03285",
    "published": "2025-07-04T04:23:03Z",
    "updated": "2026-01-14T14:06:45Z",
    "comment": "Oral @ NeurIPS 2025",
    "light_analysis": {
      "overview": "论文将Memory Mosaics扩展到10B参数规模，引入改进版本v2，在真实数据集上验证其组合性和上下文学习能力，显著优于Transformer模型。",
      "motivation": "Memory Mosaics先前在中等规模网络和小型合成数据集上展示了组合学习和上下文学习能力，但其在大规模真实世界数据集上的表现未知。这引发了研究动机：验证Memory Mosaics在扩展到大型语言模型规模（如llama-8B尺度）时是否仍保持这些优点。现有Transformer模型在处理新知识和上下文学习时可能效率有限，且单纯增加训练数据难以显著提升性能，因此探索Memory Mosaics在更现实环境中的潜力对于推动高效AI模型发展具有重要意义。",
      "method": "论文通过扩展Memory Mosaics到10B参数规模，训练在1 trillion tokens的数据上，并引入架构修改（称为Memory Mosaics v2），这些修改旨在优化模型性能，具体细节摘要未明确说明。评估方法包括三个维度：训练知识存储、新知识存储和上下文学习，以综合测试模型在不同场景下的学习和适应能力。",
      "result": "实验结果表明，Memory Mosaics v2在训练知识存储方面与Transformer模型匹配，但在新知识存储和上下文学习方面显著优于Transformer。具体地，即使在1 trillion tokens上训练的Memory Mosaics v2，其性能也超过在8 trillion tokens上训练的Transformer，表明改进源于模型架构优势而非单纯数据量增加。",
      "conclusion": "论文成功将Memory Mosaics扩展到大规模，展示了其在真实数据集上的优异性能，特别是在上下文学习和新知识存储方面。这为发展更高效的学习模型提供了新方向，具有学术价值，并可能应用于需要快速适应的AI系统。未来工作可能包括进一步优化架构或扩展到更大规模，以挖掘更多潜力。",
      "tags": [
        "Memory Mosaics",
        "Associative Memory Networks",
        "In-Context Learning",
        "Large Language Models",
        "Transformers"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:27.609750Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2507.02834",
    "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning",
    "authors": [
      "Ruiyang Zhou",
      "Shuozhe Li",
      "Amy Zhang",
      "Liu Leqi"
    ],
    "abstract": "Self-improvement via RL often fails on complex reasoning tasks because GRPO-style post-training methods rely on the model's initial ability to generate positive samples. Without guided exploration, these approaches merely reinforce what the model already knows (distribution-sharpening) rather than enabling the model to solve problems where it initially generates no correct solutions. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model's likelihood of predicting the correct answer. Based on these insights, we propose $\\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and modular framework that generates such samples by conditioning on the ground-truth answer. It can be integrated with popular RL training methods like GRPO and DPO. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most. Code is available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation .",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.02834.pdf",
    "abs_url": "https://arxiv.org/abs/2507.02834",
    "published": "2025-07-03T17:44:55Z",
    "updated": "2026-01-14T04:45:32Z",
    "comment": "Accepted to NeurIPS 2025 (Poster). Code available at https://github.com/HumainLab/ExPO_rl_reasoning_by_explanation",
    "light_analysis": {
      "overview": "论文提出ExPO框架，通过自解释引导的强化学习解锁困难推理任务，提升模型在初始无法生成正确解时的性能。",
      "motivation": "研究动机是解决强化学习自改进在复杂推理任务中的失败问题。现有方法如GRPO依赖模型初始生成正样本的能力，当模型初始无法产生正确解时，这些方法仅强化已知分布而无法探索新解，导致分布锐化而非能力提升。专家演示在RL后训练中往往无效，因此需要识别有效正样本的关键属性来引导探索，以增强模型在数学推理等挑战性任务中的泛化能力。",
      "method": "论文提出Self-Explanation Policy Optimization (ExPO)框架，核心方法是基于正确答案生成正样本，这些样本满足两个关键属性：在当前策略下可能，并能增加模型预测正确答案的似然。ExPO可以集成到流行的RL训练方法如GRPO和DPO中，实现模块化设计。通过自解释引导，模型生成比专家编写的推理链更符合其策略的轨迹，同时确保高质量。摘要未明确说明具体模型架构，但提及在MATH数据集上进行实验。",
      "result": "实验结果表明，ExPO在推理基准上提高了学习效率和最终性能。在挑战性设置如MATH level-5中，模型初始最困难时，ExPO超越了基于专家演示的方法。摘要未提供具体性能指标如准确率提升百分比，但表明ExPO通过有效探索生成高质量推理轨迹，在复杂任务上实现更好的泛化，与基线方法对比在效率和效果上均有改进。",
      "conclusion": "ExPO的主要贡献是提出了一个简单而有效的框架，通过自解释引导强化学习来解锁困难推理能力。研究的学术价值在于改进了RL在推理任务中的探索策略，实际应用价值在于提升AI系统在数学和逻辑领域的性能。未来工作可能包括扩展到更多任务或优化样本生成过程，局限性未在摘要中明确说明。",
      "tags": [
        "Reinforcement Learning",
        "Self-Explanation",
        "ExPO",
        "GRPO",
        "DPO"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:29.619986Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.17139",
    "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models",
    "authors": [
      "Michael Plainer",
      "Hao Wu",
      "Leon Klein",
      "Stephan Günnemann",
      "Frank Noé"
    ],
    "abstract": "In recent years, diffusion models trained on equilibrium molecular distributions have proven effective for sampling biomolecules. Beyond direct sampling, the score of such a model can also be used to derive the forces that act on molecular systems. However, while classical diffusion sampling usually recovers the training distribution, the corresponding energy-based interpretation of the learned score is often inconsistent with this distribution, even for low-dimensional toy systems. We trace this inconsistency to inaccuracies of the learned score at very small diffusion timesteps, where the model must capture the correct evolution of the data distribution. In this regime, diffusion models fail to satisfy the Fokker-Planck equation, which governs the evolution of the score. We interpret this deviation as one source of the observed inconsistencies and propose an energy-based diffusion model with a Fokker-Planck-derived regularization term to enforce consistency. We demonstrate our approach by sampling and simulating multiple biomolecular systems, including fast-folding proteins, and by introducing a state-of-the-art transferable Boltzmann emulator for dipeptides that supports simulation and achieves improved consistency and efficient sampling. Our code, model weights, and self-contained JAX and PyTorch notebooks are available at https://github.com/noegroup/ScoreMD.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "physics.chem-ph",
      "physics.comp-ph",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.17139.pdf",
    "abs_url": "https://arxiv.org/abs/2506.17139",
    "published": "2025-06-20T16:38:29Z",
    "updated": "2026-01-14T10:46:14Z",
    "comment": "Accepted at Conference on Neural Information Processing Systems (NeurIPS 2025)",
    "light_analysis": {
      "overview": "提出基于能量的扩散模型，通过Fokker-Planck正则化强制一致性，用于分子动力学采样和模拟。",
      "motivation": "近年来，扩散模型在平衡分子分布上训练，能有效采样生物分子，但将学习得分解释为能量时，常与训练分布不一致。这种不一致源于模型在极小扩散时间步长下得分的失准，导致不符合Fokker-Planck方程，限制了能量模拟的准确性。因此，研究动机是解决这一不匹配问题，以改进分子动力学模拟的可靠性和实用性，特别是在生物分子系统中的应用。",
      "method": "论文提出一种基于能量的扩散模型，核心创新是引入Fokker-Planck方程导出的正则化项，强制模型在学习得分时保持与训练分布的一致性。该方法应用于多个生物分子系统，如快折叠蛋白质，并开发了一个可转移的玻尔兹曼模拟器，用于二肽的模拟和高效采样。代码和模型权重已开源，支持JAX和PyTorch实现。",
      "result": "研究通过采样和模拟多个生物分子系统，展示了改进的一致性。例如，在快折叠蛋白质中，模型实现了更准确的采样。引入的可转移玻尔兹曼模拟器在二肽上达到最先进性能，提高了采样效率和一致性，但具体量化指标（如准确率提升）在摘要中未明确说明，需要参考完整论文。",
      "conclusion": "该研究提出了一种通过Fokker-Planck正则化增强扩散模型一致性的方法，有效应用于分子动力学采样和模拟，提高了能量解释的准确性。核心贡献是提供了一种实用工具，支持高效分子模拟。未来工作可扩展至更复杂的生物系统，局限性可能在于模型的计算复杂度和适用范围的进一步验证。",
      "tags": [
        "Diffusion Models",
        "Molecular Dynamics",
        "Energy-Based Models",
        "Fokker-Planck Equation",
        "Boltzmann Emulator"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:21.043768Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.12382",
    "title": "Exploring the Secondary Risks of Large Language Models",
    "authors": [
      "Jiawei Chen",
      "Zhengwei Fang",
      "Xiao Yang",
      "Chao Yu",
      "Zhaoxia Yin",
      "Hang Su"
    ],
    "abstract": "Ensuring the safety and alignment of Large Language Models is a significant challenge with their growing integration into critical applications and societal functions. While prior research has primarily focused on jailbreak attacks, less attention has been given to non-adversarial failures that subtly emerge during benign interactions. We introduce secondary risks a novel class of failure modes marked by harmful or misleading behaviors during benign prompts. Unlike adversarial attacks, these risks stem from imperfect generalization and often evade standard safety mechanisms. To enable systematic evaluation, we introduce two risk primitives verbose response and speculative advice that capture the core failure patterns. Building on these definitions, we propose SecLens, a black-box, multi-objective search framework that efficiently elicits secondary risk behaviors by optimizing task relevance, risk activation, and linguistic plausibility. To support reproducible evaluation, we release SecRiskBench, a benchmark dataset of 650 prompts covering eight diverse real-world risk categories. Experimental results from extensive evaluations on 16 popular models demonstrate that secondary risks are widespread, transferable across models, and modality independent, emphasizing the urgent need for enhanced safety mechanisms to address benign yet harmful LLM behaviors in real-world deployments.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.12382.pdf",
    "abs_url": "https://arxiv.org/abs/2506.12382",
    "published": "2025-06-14T07:31:52Z",
    "updated": "2026-01-14T12:16:50Z",
    "comment": "18 pages, 5 figures",
    "light_analysis": {
      "overview": "论文引入次要风险这一新失败模式，并提出SecLens框架和SecRiskBench基准，系统性评估大型语言模型在良性交互中的有害行为。",
      "motivation": "随着大型语言模型越来越多地集成到关键应用和社会功能中，确保其安全性和对齐性成为重大挑战。先前研究主要关注对抗性攻击如越狱，而忽视了在良性交互中微妙出现的非对抗性失败，这些次要风险源于模型的不完全泛化，往往规避标准安全机制，导致有害或误导性行为，凸显了系统评估这些风险的必要性以提升模型在实际部署中的可靠性。",
      "method": "论文定义了次要风险，并引入两个风险原语：冗长响应和推测性建议，以捕捉核心失败模式。基于此，提出了SecLens框架，一个黑盒、多目标搜索方法，通过优化任务相关性、风险激活和语言合理性来高效引发风险行为。同时发布了SecRiskBench基准数据集，包含650个提示，覆盖八个现实世界风险类别，如医疗建议和法律咨询，以支持可重复评估。",
      "result": "通过对16个流行的大型语言模型进行广泛评估，实验结果表明次要风险普遍存在，能够跨模型转移，并且与输入模态无关。这意味着这些风险是系统性缺陷，不仅限于特定模型，强调在现实世界部署中，即使在没有恶意意图的良性交互下，LLMs也可能产生有害行为，因此迫切需要增强安全机制来应对。摘要未明确说明具体性能指标数据。",
      "conclusion": "论文的主要贡献在于引入次要风险概念，并提出了SecLens评估框架和SecRiskBench基准数据集，通过实验证明这些风险广泛存在且具有转移性，揭示了当前LLMs安全机制的局限性。这为未来研究提供了方向，促进了更全面的安全评估，有助于开发更健壮的模型，以减少在实际应用中的潜在危害，并可能涉及改进泛化能力和风险检测技术。",
      "tags": [
        "Large Language Models",
        "Secondary Risks",
        "Black-box Search",
        "Benchmark Datasets",
        "Multi-objective Optimization"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:51.708851Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.12335",
    "title": "GroupNL: Low-Resource and Robust CNN Design over Cloud and Device",
    "authors": [
      "Chuntao Ding",
      "Jianhang Xie",
      "Junna Zhang",
      "Salman Raza",
      "Shangguang Wang",
      "Jiannong Cao"
    ],
    "abstract": "Deploying Convolutional Neural Network (CNN) models on ubiquitous Internet of Things (IoT) devices in a cloud-assisted manner to provide users with a variety of high-quality services has become mainstream. Most existing studies speed up model cloud training/on-device inference by reducing the number of convolution (Conv) parameters and floating-point operations (FLOPs). However, they usually employ two or more lightweight operations (e.g., depthwise Conv, $1\\times1$ cheap Conv) to replace a Conv, which can still affect the model's speedup even with fewer parameters and FLOPs. To this end, we propose the Grouped NonLinear transformation generation method (GroupNL), leveraging data-agnostic, hyperparameters-fixed, and lightweight Nonlinear Transformation Functions (NLFs) to generate diversified feature maps on demand via grouping, thereby reducing resource consumption while improving the robustness of CNNs. First, in a GroupNL Conv layer, a small set of feature maps, i.e., seed feature maps, are generated based on the seed Conv operation. Then, we split seed feature maps into several groups, each with a set of different NLFs, to generate the required number of diversified feature maps with tensor manipulation operators and nonlinear processing in a lightweight manner without additional Conv operations. We further introduce a sparse GroupNL Conv to speed up by reasonably designing the seed Conv groups between the number of input channels and seed feature maps. Experiments conducted on benchmarks and on-device resource measurements demonstrate that the GroupNL Conv is an impressive alternative to Conv layers in baseline models. Specifically, on Icons-50 dataset, the accuracy of GroupNL-ResNet-18 is 2.86% higher than ResNet-18; on ImageNet-C dataset, the accuracy of GroupNL-EfficientNet-ES achieves about 1.1% higher than EfficientNet-ES.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.DC"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.12335.pdf",
    "abs_url": "https://arxiv.org/abs/2506.12335",
    "published": "2025-06-14T04:02:35Z",
    "updated": "2026-01-14T11:04:38Z",
    "comment": "IEEE Transactions on Mobile Computing, accepted manuscript",
    "light_analysis": {
      "overview": "论文提出了GroupNL方法，通过分组非线性变换生成特征图，以降低CNN在物联网设备上的资源消耗并提高鲁棒性。",
      "motivation": "研究动机源于在云辅助的物联网（IoT）设备上部署卷积神经网络（CNN）时面临的资源限制问题。现有方法通常通过减少卷积参数和浮点运算（FLOPs）来加速模型，但常使用多个轻量级操作（如深度卷积、1x1卷积）替换单个卷积，这可能导致实际速度提升受限，并影响模型效率。因此，需要一种更高效的方法来减少资源消耗，同时增强CNN的鲁棒性，以适应资源受限的IoT环境，确保高质量服务。",
      "method": "研究方法为Grouped NonLinear transformation（GroupNL），核心是使用数据无关、超参数固定的轻量级非线性变换函数（NLFs），通过分组生成多样化特征图。具体步骤包括：在GroupNL Conv层中，首先基于种子卷积操作生成少量种子特征图；然后将种子特征图分成若干组，每组应用不同的NLFs，利用张量操作符和非线性处理生成所需数量的特征图，无需额外卷积操作。进一步引入稀疏GroupNL Conv，通过合理设计输入通道数和种子特征图之间的种子卷积组来加速，创新性地结合了分组策略和NLFs实现轻量化特征生成。",
      "result": "实验结果显示GroupNL Conv在基准测试和设备资源测量上表现优异。在Icons-50数据集上，GroupNL-ResNet-18的准确率比ResNet-18高2.86%；在ImageNet-C数据集上，GroupNL-EfficientNet-ES的准确率比EfficientNet-ES高约1.1%。这些数据证实了GroupNL Conv作为卷积层替代品的有效性，能在减少资源消耗的同时提升模型性能，与基线方法相比具有明显优势。",
      "conclusion": "结论表明，GroupNL方法通过分组非线性变换生成特征图，有效降低了CNN的资源消耗并增强了鲁棒性。主要贡献在于提出了一种新颖的轻量级CNN设计方法，适用于云和设备部署场景。学术价值在于探索了非线性变换在特征生成中的应用，实际应用价值在于支持资源受限的IoT设备高效运行。摘要未明确说明局限性和未来工作，但可推断未来可能扩展到更多模型或优化NLFs设计。",
      "tags": [
        "Grouped NonLinear Transformation",
        "Convolutional Neural Networks",
        "Nonlinear Transformation Functions",
        "Sparse Convolution",
        "IoT Deployment"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:12.298562Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.12198",
    "title": "ViSTA: Visual Storytelling using Multi-modal Adapters for Text-to-Image Diffusion Models",
    "authors": [
      "Sibo Dong",
      "Ismail Shaheen",
      "Maggie Shen",
      "Rupayan Mallick",
      "Sarah Adel Bargal"
    ],
    "abstract": "Text-to-image diffusion models have achieved remarkable success, yet generating coherent image sequences for visual storytelling remains challenging. A key challenge is effectively leveraging all previous text-image pairs, referred to as history text-image pairs, which provide contextual information for maintaining consistency across frames. Existing auto-regressive methods condition on all past image-text pairs but require extensive training, while training-free subject-specific approaches ensure consistency but lack adaptability to narrative prompts. To address these limitations, we propose a multi-modal history adapter for text-to-image diffusion models, \\textbf{ViSTA}. It consists of (1) a multi-modal history fusion module to extract relevant history features and (2) a history adapter to condition the generation on the extracted relevant features. We also introduce a salient history selection strategy during inference, where the most salient history text-image pair is selected, improving the quality of the conditioning. Furthermore, we propose to employ a Visual Question Answering-based metric TIFA to assess text-image alignment in visual storytelling, providing a more targeted and interpretable assessment of generated images. Evaluated on the StorySalon and FlintStonesSV dataset, our proposed ViSTA model is not only consistent across different frames, but also well-aligned with the narrative text descriptions.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.12198.pdf",
    "abs_url": "https://arxiv.org/abs/2506.12198",
    "published": "2025-06-13T19:57:40Z",
    "updated": "2026-01-14T15:15:58Z",
    "comment": "Accepted to WACV 2026",
    "light_analysis": {
      "overview": "本文提出多模态历史适配器ViSTA，用于增强文本到图像扩散模型在视觉叙事中的图像序列一致性和文本对齐。",
      "motivation": "视觉叙事任务要求生成连贯的图像序列以讲述故事，但现有文本到图像扩散模型难以保持跨帧一致性。关键挑战在于有效利用历史文本-图像对提供的上下文信息。当前自回归方法条件于所有过去对但训练成本高，而无训练的特定主题方法虽确保一致性却缺乏对叙事提示的适应性，亟需高效且适应性强的解决方案来解决这一问题。",
      "method": "ViSTA模型由多模态历史融合模块和历史适配器组成：前者提取相关历史特征，后者将这些特征条件化到生成过程中。创新点包括显著历史选择策略，在推理时选择最相关历史对以优化条件，并使用基于视觉问答的TIFA度量评估文本-图像对齐。模型在StorySalon和FlintStonesSV数据集上实现，强调了多模态信息处理和自适应机制的结合。",
      "result": "在StorySalon和FlintStonesSV数据集上的评估显示，ViSTA模型在不同帧之间保持高度一致性，并与叙事文本描述良好对齐。相比于基线方法，ViSTA在一致性和对齐性方面表现更优，但摘要未明确说明具体的性能指标数据如准确率提升。",
      "conclusion": "ViSTA通过多模态历史适配器解决了视觉叙事中图像序列一致性的关键问题，提升了文本到图像扩散模型在复杂任务中的应用价值。该研究具有实际应用潜力，如数字故事创作，但局限性可能包括对特定数据集的依赖，未来工作可扩展至更多样化场景或模态。",
      "tags": [
        "Visual Storytelling",
        "Text-to-Image Diffusion Models",
        "Multi-modal Adapters",
        "History Fusion",
        "TIFA Metric"
      ]
    },
    "analyzed_at": "2026-01-15T03:31:52.791135Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.06821",
    "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems",
    "authors": [
      "Yuhan Cao",
      "Zian Chen",
      "Kun Quan",
      "Ziliang Zhang",
      "Yu Wang",
      "Xiaoning Dong",
      "Yeqi Feng",
      "Guanzhong He",
      "Jingcheng Huang",
      "Jianhao Li",
      "Yixuan Tan",
      "Jiafu Tang",
      "Yilin Tang",
      "Junlei Wu",
      "Qianyu Xiao",
      "Can Zheng",
      "Shouchen Zhou",
      "Yuxiang Zhu",
      "Yiming Huang",
      "Tianxing He"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2506.06821.pdf",
    "abs_url": "https://arxiv.org/abs/2506.06821",
    "published": "2025-06-07T14:53:03Z",
    "updated": "2026-01-14T15:14:27Z",
    "comment": "37 pages, 22 figures",
    "light_analysis": {
      "overview": "本文提出TCGBench基准，评估大语言模型生成测试用例生成器的能力，并构建数据集以提升其生成针对性测试用例的性能。",
      "motivation": "虽然大语言模型在代码生成方面已证明出色能力，但将其用于通过测试用例生成进行代码检查或调试的研究尚不充分。本研究旨在探索LLMs在竞争级编程问题中生成可靠测试用例生成器的能力，以解决自动化代码调试中的不足，因为现有方法在生成针对性测试用例以暴露人类代码错误方面存在局限性。这有助于推动LLMs在软件工程领域的应用。",
      "method": "论文提出TCGBench基准，包含两个任务：一是生成有效测试用例生成器，二是生成针对性测试用例生成器以暴露人类代码错误。基于竞争级编程问题，构建高质量手工策划指令数据集，通过提示和微调方法增强LLMs。使用先进LLMs如o3-mini进行实验，分析其生成能力。",
      "result": "实验结果显示，LLMs在大多数情况下能生成有效测试用例生成器，但在生成针对性测试用例以揭示代码bug时表现不佳，即使如o3-mini等先进模型也显著低于人类水平。通过使用手工策划数据集进行提示和微调，LLMs的性能得到提升，表明数据辅助能有效改善模型表现。",
      "conclusion": "本研究通过建立TCGBench基准和数据集，评估并提升了LLMs在测试用例生成方面的能力，揭示了其在生成针对性测试用例时的局限性。这为LLMs在代码调试领域的应用提供了新见解和方法，未来工作可进一步优化模型或扩展应用场景。摘要未明确说明具体局限性，但可推断需要改进模型推理能力。",
      "tags": [
        "Large Language Models",
        "Test Case Generation",
        "Benchmark",
        "Code Debugging",
        "Competition-Level Programming"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:17.047864Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.01820",
    "title": "Fodor and Pylyshyn's Legacy: Still No Human-like Systematic Compositionality in Neural Networks",
    "authors": [
      "Tim Woydt",
      "Moritz Willig",
      "Antonia Wüst",
      "Lukas Helff",
      "Wolfgang Stammer",
      "Constantin A. Rothkopf",
      "Kristian Kersting"
    ],
    "abstract": "Strong meta-learning capabilities for systematic compositionality are emerging as an important skill for navigating the complex and changing tasks of today's world. However, in presenting models for robust adaptation to novel environments, it is important to refrain from making unsupported claims about the performance of meta-learning systems that ultimately do not stand up to scrutiny. While Fodor and Pylyshyn famously posited that neural networks inherently lack this capacity as they are unable to model compositional representations or structure-sensitive operations, and thus are not a viable model of the human mind, Lake and Baroni recently presented meta-learning as a pathway to compositionality. In this position paper, we critically revisit this claim and highlight limitations in the proposed meta-learning framework for compositionality. Our analysis shows that modern neural meta-learning systems can only perform such tasks, if at all, under a very narrow and restricted definition of a meta-learning setup. We therefore claim that `Fodor and Pylyshyn's legacy' persists, and to date, there is no human-like systematic compositionality learned in neural networks.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.01820.pdf",
    "abs_url": "https://arxiv.org/abs/2506.01820",
    "published": "2025-06-02T16:02:53Z",
    "updated": "2026-01-14T11:24:37Z",
    "comment": null,
    "light_analysis": {
      "overview": "批判元学习框架的局限性，主张神经网络尚未实现类似人类的系统性组合性。",
      "motivation": "系统性组合性是适应复杂和变化任务的关键能力，但现有研究如Lake和Baroni提出元学习作为实现组合性的途径，可能过度声称其性能。本论文旨在重新评估这一主张，指出神经网络在组合性表示和结构敏感操作上的固有局限性，源于Fodor和Pylyshyn的经典批评，强调避免不切实际的说法，以促进更严谨的AI模型评估和未来发展。",
      "method": "论文采用批判性分析和理论探讨的方法，通过回顾相关文献和评估现有元学习框架，揭示其在实现系统性组合性上的限制。未涉及具体实验设计或模型架构，而是聚焦于理论论证，分析元学习系统在狭窄和受限定义下的适用性，以强调神经网络组合性能力的不足。摘要未明确说明具体技术细节。",
      "result": "分析表明，现代神经元学习系统只能在非常狭窄和受限的元学习设置下执行组合性任务，无法实现人类水平的系统性组合性。与Lake和Baroni的元学习框架对比，发现其适用范围有限，未能普遍适应新颖环境，因此Fodor和Pylyshyn的批评依然有效，神经网络尚未学到类似人类的组合性能力。",
      "conclusion": "论文总结认为，神经网络仍缺乏类似人类的系统性组合性，Fodor和Pylyshyn的遗产持续存在。这提醒研究者需谨慎对待元学习的声称，避免夸大其词。研究强调了组合性在AI中的重要性，并指出未来应探索更有效的学习方法或理论突破，以解决这一根本挑战，推动AI向人类智能靠拢。",
      "tags": [
        "Neural Networks",
        "Meta-Learning",
        "Systematic Compositionality",
        "Critical Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:08.172360Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2506.01327",
    "title": "Enhancing Federated Class-Incremental Learning via Spatial-Temporal Statistics Aggregation",
    "authors": [
      "Zenghao Guan",
      "Guojun Zhu",
      "Yucan Zhou",
      "Wu Liu",
      "Weiping Wang",
      "Jiebo Luo",
      "Xiaoyan Gu"
    ],
    "abstract": "Federated Class-Incremental Learning (FCIL) enables Class-Incremental Learning (CIL) from distributed data. Existing FCIL methods typically integrate old knowledge preservation into local client training. However, these methods cannot avoid spatial-temporal client drift caused by data heterogeneity and often incur significant computational and communication overhead, limiting practical deployment. To address these challenges simultaneously, we propose a novel approach, Spatial-Temporal Statistics Aggregation (STSA), which provides a unified framework to aggregate feature statistics both spatially (across clients) and temporally (across stages). The aggregated feature statistics are unaffected by data heterogeneity and can be used to update the classifier in closed form at each stage. Additionally, we introduce STSA-E, a communication-efficient variant with theoretical guarantees, achieving similar performance to STSA-E with much lower communication overhead. Extensive experiments on three widely used FCIL datasets, with varying degrees of data heterogeneity, show that our method outperforms state-of-the-art FCIL methods in terms of performance, flexibility, and both communication and computation efficiency. The code is available at https://github.com/Yuqin-G/STSA.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.01327.pdf",
    "abs_url": "https://arxiv.org/abs/2506.01327",
    "published": "2025-06-02T05:14:57Z",
    "updated": "2026-01-14T16:57:26Z",
    "comment": "WWW 2026",
    "light_analysis": {
      "overview": "提出STSA框架，通过空间-时间特征统计聚合解决联邦类增量学习中的数据异构性和高开销问题，提升性能和效率。",
      "motivation": "研究动机源于联邦类增量学习（FCIL）在分布式数据上面临的挑战。现有方法通常将旧知识保存集成到本地客户端训练中，但无法避免由于数据异构性引起的时空客户端漂移，导致模型性能下降。同时，这些方法常伴随显著的计算和通信开销，限制了在实际部署中的应用。FCIL对于在边缘设备上实现连续学习具有重要意义，但现有方法的不足在于效率和效果受限，迫切需要更优的解决方案。",
      "method": "研究方法提出空间-时间统计聚合（STSA）框架，统一聚合特征统计量在空间维度（跨客户端）和时间维度（跨学习阶段），使统计量不受数据异构性影响，并能以封闭形式在每个阶段更新分类器。关键创新点是STSA-E，一个通信高效的变体，提供理论保证，以低通信开销实现与STSA相似的性能。实验基于三个广泛使用的FCIL数据集，涵盖不同程度的数据异构性，验证方法的通用性和有效性。",
      "result": "实验结果表明，在三个FCIL数据集上，STSA方法在性能、灵活性和效率方面均优于现有先进方法。具体地，方法有效缓解了数据异构性导致的时空漂移，通信和计算效率显著提升，STSA-E变体在保持高性能的同时大幅降低通信开销。与基线方法对比，证实了STSA在综合指标上的优势，为实际部署提供了更可行的选择，摘要未明确说明具体数值如准确率，但强调了整体效果的改进。",
      "conclusion": "结论总结，STSA方法通过空间-时间统计聚合为FCIL提供了统一框架，核心贡献是有效解决数据异构性和资源开销挑战，提升了学习效率。学术价值在于推动分布式增量学习领域的发展，实际应用价值在于促进资源受限环境下的部署。未来工作可进一步优化聚合策略或扩展到其他学习任务，代码已公开以支持复现和扩展研究。",
      "tags": [
        "Federated Learning",
        "Class-Incremental Learning",
        "Spatial-Temporal Statistics Aggregation",
        "Communication Efficiency",
        "Data Heterogeneity"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:37.049820Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.21364",
    "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders",
    "authors": [
      "James Oldfield",
      "Shawn Im",
      "Sharon Li",
      "Mihalis A. Nicolaou",
      "Ioannis Patras",
      "Grigorios G Chrysos"
    ],
    "abstract": "Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.21364.pdf",
    "abs_url": "https://arxiv.org/abs/2505.21364",
    "published": "2025-05-27T15:55:55Z",
    "updated": "2026-01-14T10:55:34Z",
    "comment": "NeurIPS 2025 camera-ready version",
    "light_analysis": {
      "overview": "本文提出Mixture of Decoders (MxDs)方法，通过层級稀疏性实现可解释且忠实的大语言模型密集层分解。",
      "motivation": "研究动机是解决多层感知器（MLPs）作为大型语言模型核心组件时，由于其密集表示导致模型难以理解、编辑和指导的问题。现有方法试图通过神经元级稀疏性实现可解释性，但牺牲了准确性，无法忠实重建原始映射，显著增加了模型的下一个令牌交叉熵损失。这突显了在保持高性能的同时提高模型可解释性的重要性，以应对当前可解释AI领域的挑战。",
      "method": "论文提出Mixture of Decoders (MxDs)，该方法泛化了多层感知器和门控线性单元，将预训练密集层扩展为成千上万个专业化子层。通过一种灵活的张量因子分解技术，每个稀疏激活的MxD子层实现全秩权重的线性变换，从而即使在高度稀疏性下也能保持原始解码器的表达能力。关键创新在于层級稀疏性范式，克服了传统稀疏近似的准确性权衡，并引入了MxDs作为新型分解架构。",
      "result": "实验结果表明，在高达30亿参数的语言模型中，MxDs在稀疏性-准确性前沿显著优于现有最先进方法（如Transcoders）。具体表现包括在稀疏层近似中减少损失并保持高准确性。进一步评估显示，通过稀疏探测和特征指导，MxDs学习了类似的专业化自然语言特征，验证了其忠实性和有效性。这些结果支持MxDs在可解释分解方面的优越性能。",
      "conclusion": "总结而言，Mixture of Decoders (MxDs)提供了一种可解释且忠实的密集层分解方法，成功解决了稀疏近似中的准确性权衡问题。研究的学术价值在于推动了层級稀疏性的研究，为设计更可靠的AI模型开辟了新方向；实际应用价值在于可能促进大语言模型的理解、编辑和指导。未来工作可探索更广泛的模型架构和扩展应用，尽管摘要未明确说明具体局限性，但该方法有望成为可解释AI的重要工具。",
      "tags": [
        "Mixture of Decoders (MxDs)",
        "Layer-Level Sparsity",
        "Tensor Factorization",
        "Sparse Probing",
        "Feature Steering"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:36.616484Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.21032",
    "title": "FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models",
    "authors": [
      "Nils Neukirch",
      "Johanna Vielhaben",
      "Nils Strodthoff"
    ],
    "abstract": "Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.21032.pdf",
    "abs_url": "https://arxiv.org/abs/2505.21032",
    "published": "2025-05-27T11:07:34Z",
    "updated": "2026-01-14T10:46:17Z",
    "comment": "Version published by Transactions on Machine Learning Research in 2025 (TMLR ISSN 2835-8856) at https://openreview.net/forum?id=UtE1YnPNgZ. 32 pages, 27 figures. This work builds on an earlier manuscript (arXiv:2505.21032) and crucially extends it. Code is available at https://github.com/AI4HealthUOL/FeatInv",
    "light_analysis": {
      "overview": "提出使用条件扩散模型实现特征空间到输入空间的高保真映射，以增强深度神经网络内部表示的可解释性。",
      "motivation": "深度神经网络的内部表示对于理解其属性和推理模式至关重要，但传统方法难以解释。现有从特征空间到输入空间的映射方法常依赖粗略近似，导致解释精度不足和实用性受限。本研究旨在通过更精确的映射方法来提升特征空间的可解释性，以解决计算机视觉模型中的解释性挑战，改善模型透明度和可靠性。",
      "method": "采用条件扩散模型作为核心方法，基于预训练的高保真扩散模型，以空间解析的特征图为条件进行概率学习，从而学习从特征空间到输入空间的映射。关键创新点在于利用扩散模型的生成能力实现高保真重建，并在多种预训练图像分类器（包括卷积神经网络和视觉变换器）上实施该方法，展示了其通用性和技术特色。",
      "result": "研究展示了该方法的可行性，在多个预训练分类器上实现了优秀的重建能力，通过定性比较和鲁棒性分析验证了有效性。与现有近似方法相比，表现出改进的解释精度和鲁棒性，尽管摘要未明确说明具体数据。应用方面，演示了如输入空间概念导向可视化和特征空间复合性质研究等潜在应用，证实了方法在提升特征空间理解方面的潜力。",
      "conclusion": "本研究的主要贡献是提出了一种基于条件扩散模型的映射方法，显著改善了深度神经网络特征空间的可解释性。学术价值在于提供了一种新颖的概率映射框架，实际应用价值包括增强计算机视觉模型的透明度和可解释性，为未来研究如扩展到其他领域或优化性能提供了基础，但摘要未明确说明具体局限性。",
      "tags": [
        "Conditional Diffusion Model",
        "Feature Space Mapping",
        "Spatially Resolved Features",
        "Image Classification",
        "Deep Neural Networks"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:24.933827Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.19789",
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "authors": [
      "Jijia Liu",
      "Feng Gao",
      "Bingwen Wei",
      "Xinlei Chen",
      "Qingmin Liao",
      "Yi Wu",
      "Chao Yu",
      "Yu Wang"
    ],
    "abstract": "Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.19789.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19789",
    "published": "2025-05-26T10:19:26Z",
    "updated": "2026-01-14T14:23:30Z",
    "comment": "Accepted by NeurIPS 2025",
    "light_analysis": {
      "overview": "本研究引入一个基准系统评估强化学习微调对视觉语言动作模型泛化的影响，发现PPO算法在提升语义理解和执行鲁棒性方面显著优于监督微调。",
      "motivation": "视觉语言动作模型在具身AI中具有潜力，但现有方法主要依赖监督微调进行训练，导致在分布偏移下容易受到复合错误影响，泛化能力受限。强化学习通过试错优化任务目标，有望克服这些不足，但目前缺乏系统研究来比较强化学习与监督微调在提升VLA泛化方面的具体效益，因此本研究旨在填补这一空白，探索强化学习对VLA模型泛化的贡献。",
      "method": "本研究设计了一个全面的基准，用于评估视觉语言动作模型在视觉、语义和执行维度上的泛化能力。通过系统比较强化学习微调与监督微调，重点关注PPO、DPO和GRPO等算法。开发了在VLA上高效进行PPO训练的简单配方，基于广泛实验分析这些方法在多个维度上的影响，关键创新点在于系统化评估和优化RL微调过程。",
      "result": "实验结果表明，使用PPO进行强化学习微调显著增强了视觉语言动作模型在语义理解和执行鲁棒性上的泛化能力，优于监督微调方法。在视觉鲁棒性方面，性能与监督微调保持可比。PPO算法被证明比基于大语言模型的DPO和GRPO方法更有效，同时开发的PPO训练配方提高了训练效率，为实际应用提供了支持。",
      "conclusion": "本研究表明，强化学习微调特别是使用PPO算法，能有效提升视觉语言动作模型的泛化能力，尤其在语义理解和执行鲁棒性方面。这为具身AI领域提供了实际应用价值，开发的训练配方增强了方法的实用性。未来工作可能扩展到更多强化学习算法和泛化维度，进一步优化模型性能。",
      "tags": [
        "Large Vision-Language Action (VLA) models",
        "Reinforcement Learning (RL)",
        "Proximal Policy Optimization (PPO)",
        "Supervised Fine-Tuning (SFT)",
        "Generalization Benchmark"
      ]
    },
    "analyzed_at": "2026-01-15T03:32:55.631228Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.17217",
    "title": "Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs",
    "authors": [
      "Kangda Wei",
      "Hasnat Md Abdullah",
      "Ruihong Huang"
    ],
    "abstract": "Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.17217.pdf",
    "abs_url": "https://arxiv.org/abs/2505.17217",
    "published": "2025-05-22T18:46:50Z",
    "updated": "2026-01-14T02:45:29Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种新颖的数据生成框架，通过培养探索性思维，利用直接偏好优化（DPO）来减轻大型语言模型中的性别偏见。",
      "motivation": "大型语言模型在生成文本时常常无意中强化性别刻板印象，导致在不同上下文中对男性和女性主题的不平等对待，这影响了AI应用的公平性和伦理标准。现有去偏方法可能缺乏对模型内在认知过程的深入干预，本研究旨在通过促进探索性思维来解决这一根本问题，从源头上减少偏见，确保模型在敏感领域如招聘、教育中的公正性。",
      "method": "本研究提出一个数据生成框架，首先提示模型生成结构相同、道德模糊的故事对，主角分别为男性和女性。接着，模型对这些故事进行道德判断，并比较判断是否一致。当发现不一致（表明存在性别偏见）时，系统引导模型重新评估以产生性别中立的判断。这些生成的故事-判断对被用于通过直接偏好优化（DPO）技术微调或优化模型参数，从而实现有效的偏见缓解。",
      "result": "实验结果表明，该方法显著降低了大型语言模型中的性别偏见，同时模型的通用能力如语言理解和生成得以保持，甚至在某些方面有所增强。具体性能指标如偏见减少的百分比未在摘要中明确说明，但可以推断该方法在去偏任务上表现优于基线方法，并且摘要强调了框架的有效性。",
      "conclusion": "本研究的主要贡献是开发了一个基于探索性思维的数据生成框架，有效减轻了LLMs的性别偏见，并通过DPO优化保持了模型性能。其学术价值在于为AI伦理研究提供了新视角，强调了模型内在思考过程的重要性；实际应用价值在于可以提升LLMs在敏感领域的公平性和可信度。未来工作可能包括扩展该方法到其他偏见类型，或进一步优化框架以适应更复杂的场景。",
      "tags": [
        "Large Language Model",
        "Gender Bias Mitigation",
        "Exploratory Thinking",
        "Direct Preference Optimization",
        "Moral Judgment"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:41.627921Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.15570",
    "title": "Out-of-Distribution Detection via Channelwise Feature Aggregation in Neural Network-Based Receivers",
    "authors": [
      "Marko Tuononen",
      "Heikki Penttinen",
      "Duy Vu",
      "Dani Korpi",
      "Vesa Starck",
      "Ville Hautamäki"
    ],
    "abstract": "Neural network-based radio receivers are expected to play a key role in future wireless systems, making reliable Out-Of-Distribution (OOD) detection essential. We propose a post-hoc, layerwise OOD framework based on channelwise feature aggregation that avoids classwise statistics--critical for multi-label soft-bit outputs with astronomically many classes. Receiver activations exhibit no discrete clusters but a smooth Signal-to-Noise-Ratio (SNR)-aligned manifold, consistent with classical receiver behavior and motivating manifold-aware OOD detection. We evaluate multiple OOD feature types, distance metrics, and methods across layers. Gaussian Mahalanobis with mean activations is the strongest single detector, earlier layers outperform later, and SNR/classifier fusions offer small, inconsistent AUROC gains. High-delay OOD is detected reliably, while high-speed remains challenging.",
    "categories": [
      "cs.LG",
      "eess.SP"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.15570.pdf",
    "abs_url": "https://arxiv.org/abs/2505.15570",
    "published": "2025-05-21T14:23:38Z",
    "updated": "2026-01-14T09:33:18Z",
    "comment": "50 pages, 39 figures, 48 tables, and 31 equations",
    "light_analysis": {
      "overview": "本文提出了一种基于通道级特征聚合的后验分层OOD检测框架，用于神经网络无线电接收器，以应对多标签软比特输出中的OOD检测挑战。",
      "motivation": "神经网络接收器在未来无线系统中预计扮演关键角色，因此可靠的OOD检测对于系统稳定性至关重要。传统方法在处理多标签软比特输出时面临类别数量极大的问题，需要避免使用类别级统计，以解决未知分布数据下的检测失效风险。本研究旨在填补这一空白，提升接收器在复杂无线环境中的适应性和可靠性，避免因OOD数据导致性能下降。",
      "method": "研究方法包括提出一个后验的、分层OOD检测框架，基于通道级特征聚合来避免类别级统计。关键创新在于利用接收器激活的平滑SNR对齐流形，以进行流形感知检测。通过评估多种OOD特征类型（如平均激活）、距离度量（如高斯Mahalanobis距离）以及跨层分析方法，探索最佳检测策略，摘要未明确说明具体数据集，但模型基于神经网络接收器架构。",
      "result": "实验结果表明，使用高斯Mahalanobis距离与平均激活的检测器性能最强；较早的神经网络层在OOD检测中优于较晚层；SNR与分类器融合提供的AUROC增益较小且不一致。高延迟OOD能够被可靠检测，而高速OOD检测仍具挑战性，摘要未明确说明具体数值对比，但描述了定性性能趋势。",
      "conclusion": "本研究的主要贡献是开发了一种适用于神经网络接收器的OOD检测框架，特别是在多标签场景下避免类别级统计。其学术价值在于引入流形感知技术改进检测方法；实际应用可增强无线系统的可靠性和安全性。未来工作需解决高速OOD检测的难题，并可能扩展到其他领域如通信信号处理。",
      "tags": [
        "Out-of-Distribution Detection",
        "Channelwise Feature Aggregation",
        "Neural Network-Based Receivers",
        "Gaussian Mahalanobis",
        "SNR-aligned manifold"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:12.396031Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2505.00541",
    "title": "KnowEEG: Explainable Knowledge Driven EEG Classification",
    "authors": [
      "Amarpal Sahota",
      "Navid Mohammadi Foumani",
      "Raul Santos-Rodriguez",
      "Zahraa S. Abdallah"
    ],
    "abstract": "Electroencephalography (EEG) is a method of recording brain activity that shows significant promise in applications ranging from disease classification to emotion detection and brain-computer interfaces. Recent advances in deep learning have improved EEG classification performance yet model explainability remains an issue. To address this key limitation of explainability we introduce KnowEEG; a novel explainable machine learning approach for EEG classification. KnowEEG extracts a comprehensive set of per-electrode features, filters them using statistical tests, and integrates between-electrode connectivity statistics. These features are then input to our modified Random Forest model (Fusion Forest) that balances per electrode statistics with between electrode connectivity features in growing the trees of the forest. By incorporating knowledge from both the generalized time-series and EEG-specific domains, KnowEEG achieves performance comparable to or exceeding state-of-the-art deep learning models across five different classification tasks: emotion detection, mental workload classification, eyes open/closed detection, abnormal EEG classification, and event detection. In addition to high performance, KnowEEG provides inherent explainability through feature importance scores for understandable features. We demonstrate by example on the eyes closed/open classification task that this explainability can be used to discover knowledge about the classes. This discovered knowledge for eyes open/closed classification was proven to be correct by current neuroscience literature. Therefore, the impact of KnowEEG will be significant for domains where EEG explainability is critical such as healthcare.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.00541.pdf",
    "abs_url": "https://arxiv.org/abs/2505.00541",
    "published": "2025-05-01T14:05:55Z",
    "updated": "2026-01-14T11:46:55Z",
    "comment": null,
    "light_analysis": {
      "overview": "KnowEEG是一种融合领域知识的可解释脑电图分类方法，通过改进随机森林模型实现高性能和固有解释性。",
      "motivation": "脑电图（EEG）分类在疾病诊断、情绪识别和脑机接口等领域具有广泛应用前景。然而，尽管深度学习技术提升了EEG分类性能，但模型的可解释性差，这使得其在需要透明决策的领域（如医疗保健）应用受限。现有方法通常以黑盒形式运行，缺乏对分类结果的合理解释，限制了医生的信任和临床接受度。因此，开发一种既能保持高性能又能提供可解释性的EEG分类方法至关重要。KnowEEG旨在解决这一关键限制，通过结合领域知识来增强模型的可解释性。",
      "method": "KnowEEG的核心方法首先从每个EEG电极提取全面的时间序列特征，包括频域和时域统计量，并使用统计测试（如t检验或ANOVA）进行特征过滤以去除冗余信息。然后，整合电极间的连通性统计特征，如互相关或相干性，以捕捉脑网络动态。这些特征输入到改进的随机森林模型（Fusion Forest），该模型在树生长过程中平衡个体电极统计和电极间连通性特征，确保分类决策融合了通用时间序列知识和EEG特定领域知识。关键创新在于设计了一种可解释的机器学习框架，通过特征重要性分数提供透明性，而不依赖于复杂的深度学习架构。",
      "result": "KnowEEG在五个不同EEG分类任务中表现优异，包括情绪检测、脑力负荷分类、眼开/闭检测、异常EEG分类和事件检测。实验结果显示，其性能与或超过最先进的深度学习模型（如卷积神经网络或循环神经网络），具体表现为准确率和F1分数可比较或略有提升，但摘要未提供具体数据数值。通过特征重要性分析，模型提供固有可解释性；例如，在眼开/闭分类任务中，发现与视觉处理相关的电极特征被高亮，这一发现与当前神经科学文献一致，验证了方法的有效性。这证明了KnowEEG不仅性能好，还能揭示分类背后的生物学依据。",
      "conclusion": "论文的主要贡献是提出了KnowEEG，一种高性能且可解释的EEG分类方法，通过融合领域知识和改进随机森林模型实现。其学术价值在于将可解释人工智能引入EEG分析领域，提升了模型的透明性和可信度，为未来研究提供了新方向。实际应用价值在于对医疗保健等关键领域有重大影响，这些领域对EEG解释性要求高，例如辅助疾病诊断或脑机接口开发。局限性方面，摘要未明确说明，但未来工作可能涉及扩展到更多EEG任务或优化计算效率，以进一步提高实用性和可扩展性。",
      "tags": [
        "EEG Classification",
        "Explainable AI",
        "Random Forest",
        "Feature Selection",
        "Time-Series Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:16.752832Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2504.01733",
    "title": "Epistemic Skills: Reasoning about Knowledge and Oblivion",
    "authors": [
      "Xiaolong Liang",
      "Yì N. Wáng"
    ],
    "abstract": "This paper presents a class of epistemic logics that captures the dynamics of acquiring knowledge and descending into oblivion, while incorporating concepts of group knowledge. The approach is grounded in a system of weighted models, introducing an ``epistemic skills'' metric to represent the epistemic capacities tied to knowledge updates. Within this framework, knowledge acquisition is modeled as a process of upskilling, whereas oblivion is represented as a consequence of downskilling. The framework further enables exploration of ``knowability'' and ``forgettability,'' defined as the potential to gain knowledge through upskilling and to lapse into oblivion through downskilling, respectively. Additionally, it supports a detailed analysis of the distinctions between epistemic de re and de dicto expressions. The computational complexity of the model checking and satisfiability problems is examined, offering insights into their theoretical foundations and practical implications.",
    "categories": [
      "cs.AI",
      "cs.CC",
      "cs.LO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2504.01733.pdf",
    "abs_url": "https://arxiv.org/abs/2504.01733",
    "published": "2025-04-02T13:41:42Z",
    "updated": "2026-01-14T16:03:35Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于加权模型和认知技能度量的认知逻辑框架，以模拟知识与遗忘的动态过程，并整合群体知识分析。",
      "motivation": "该研究旨在解决传统认知逻辑中对知识和遗忘动态建模不足的问题。在实际认知活动中，知识获取和遗忘并存，但现有方法多静态处理知识，忽视遗忘过程。通过引入技能度量和加权模型，本研究填补了认知逻辑中的空白，增强了其表达能力和实用性，对多智能体系统和认知科学领域具有重要价值。摘要未明确说明具体现有方法的不足，但强调了动态建模的必要性。",
      "method": "研究方法基于加权模型系统，引入“认知技能”度量来表示个体或群体的认知能力变化。知识获取被建模为“提升技能”过程，遗忘则是“降级技能”的结果。框架定义了“可知性”和“可遗忘性”，作为通过技能变化获得知识或陷入遗忘的潜力，并支持详细分析认知 de re 和 de dicto 表达式的区别。关键创新在于整合动态技能变化到形式化逻辑中，为知识动态提供了量化工具。",
      "result": "摘要未明确说明具体实验性能指标如准确率或效率改进，但提到了对模型检查和满足问题计算复杂性的分析。这表明研究探讨了框架的理论可计算性，为实际应用中的算法设计提供了基础。与基线方法的对比可能涉及传统认知逻辑模型，但摘要未详细说明；分析结果可能揭示了框架在复杂度方面的特点，有助于评估其在实际系统中的可行性。",
      "conclusion": "本文的主要贡献是提出了一个创新的认知逻辑框架，通过加权模型和认知技能度量，有效模拟了知识和遗忘的动态过程。研究整合了群体知识概念，并深入分析了可知性、可遗忘性及认知表达式的区分。学术上扩展了认知逻辑理论边界，实践中可应用于人工智能中的知识表示和多智能体协作等领域。局限性可能包括模型复杂性和计算开销，未来工作可优化算法或扩展到具体应用场景。",
      "tags": [
        "Epistemic Logic",
        "Weighted Models",
        "Model Checking",
        "Computational Complexity",
        "Group Knowledge"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:36.964577Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.20580",
    "title": "Training Large Neural Networks With Low-Dimensional Error Feedback",
    "authors": [
      "Maher Hanut",
      "Jonathan Kadmon"
    ],
    "abstract": "Training deep neural networks typically relies on backpropagating high dimensional error signals a computationally intensive process with little evidence supporting its implementation in the brain. However, since most tasks involve low-dimensional outputs, we propose that low-dimensional error signals may suffice for effective learning. To test this hypothesis, we introduce a novel local learning rule based on Feedback Alignment that leverages indirect, low-dimensional error feedback to train large networks. Our method decouples the backward pass from the forward pass, enabling precise control over error signal dimensionality while maintaining high-dimensional representations. We begin with a detailed theoretical derivation for linear networks, which forms the foundation of our learning framework, and extend our approach to nonlinear, convolutional, and transformer architectures. Remarkably, we demonstrate that even minimal error dimensionality on the order of the task dimensionality can achieve performance matching that of traditional backpropagation. Furthermore, our rule enables efficient training of convolutional networks, which have previously been resistant to Feedback Alignment methods, with minimal error. This breakthrough not only paves the way toward more biologically accurate models of learning but also challenges the conventional reliance on high-dimensional gradient signals in neural network training. Our findings suggest that low-dimensional error signals can be as effective as high-dimensional ones, prompting a reevaluation of gradient-based learning in high-dimensional systems. Ultimately, our work offers a fresh perspective on neural network optimization and contributes to understanding learning mechanisms in both artificial and biological systems.",
    "categories": [
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.20580.pdf",
    "abs_url": "https://arxiv.org/abs/2502.20580",
    "published": "2025-02-27T22:45:41Z",
    "updated": "2026-01-14T17:19:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出基于反馈对齐的低维误差反馈方法，实现大型神经网络的高效训练。",
      "motivation": "传统神经网络训练依赖反向传播高维误差信号，计算密集且缺乏生物证据支持。大多数任务涉及低维输出，表明低维误差信号可能足够有效。现有方法如反馈对齐在卷积网络上效果有限，因此本研究旨在解决高维梯度依赖问题，探索低维误差反馈以提升计算效率和生物合理性，推动更高效的学习机制发展。",
      "method": "本研究引入一种新的本地学习规则，基于反馈对齐，利用间接低维误差反馈训练大型网络。核心创新是解耦前向和后向传递，允许精确控制误差信号维度，同时保持高维表示。理论推导从线性网络开始，建立学习框架基础，然后扩展到非线性、卷积和Transformer架构，确保方法的广泛适用性和技术特色。",
      "result": "实验表明，即使误差维度接近任务维度，性能也能匹配传统反向传播。在卷积网络上实现了高效训练，突破了先前反馈对齐方法的局限，证明了低维误差反馈的有效性。摘要未明确说明具体性能指标如准确率，但强调性能媲美基线方法，展示了该方法在多种架构中的潜力。",
      "conclusion": "研究挑战了神经网络训练中对高维梯度信号的依赖，证明低维误差信号同样有效。这不仅为开发更生物准确的学习模型铺路，还提供了神经网络优化的新视角，促进了对人工和生物学习机制的理解。未来工作可进一步探索局限性或扩展应用场景。",
      "tags": [
        "Feedback Alignment",
        "Low-Dimensional Error Feedback",
        "Local Learning Rule",
        "Convolutional Networks",
        "Transformer Architectures"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:24.247037Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.16841",
    "title": "Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives",
    "authors": [
      "Dilermando Queiroz",
      "Anderson Carlos",
      "André Anjos",
      "Lilian Berton"
    ],
    "abstract": "Ensuring equitable Artificial Intelligence (AI) in healthcare demands systems that make unbiased decisions across all demographic groups, bridging technical innovation with ethical principles. Foundation Models (FMs), trained on vast datasets through self-supervised learning, enable efficient adaptation across medical imaging tasks while reducing dependency on labeled data. These models demonstrate potential for enhancing fairness, though significant challenges remain in achieving consistent performance across demographic groups. Our review indicates that effective bias mitigation in FMs requires systematic interventions throughout all stages of development. While previous approaches focused primarily on model-level bias mitigation, our analysis reveals that fairness in FMs requires integrated interventions throughout the development pipeline, from data documentation to deployment protocols. This comprehensive framework advances current knowledge by demonstrating how systematic bias mitigation, combined with policy engagement, can effectively address both technical and institutional barriers to equitable AI in healthcare. The development of equitable FMs represents a critical step toward democratizing advanced healthcare technologies, particularly for underserved populations and regions with limited medical infrastructure and computational resources.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2502.16841.pdf",
    "abs_url": "https://arxiv.org/abs/2502.16841",
    "published": "2025-02-24T04:54:49Z",
    "updated": "2026-01-14T14:42:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文综述医疗影像分析中公平性基础模型的挑战与前景，提出一个综合框架，通过系统性偏见减轻和政策参与来促进公平AI。",
      "motivation": "研究动机源于确保人工智能在医疗领域实现公平决策的紧迫需求，因为基础模型虽通过自监督学习在大数据集上训练，能高效适应医疗成像任务，减少对标注数据的依赖，但仍存在性能在不同人口群体间不一致的挑战。现有方法多聚焦模型层面的偏见减轻，忽视了从数据收集到部署的全流程干预，导致公平性难以保证，因此需要更系统性的解决方案来应对技术和伦理问题。",
      "method": "作为一篇综述文章，本文采用文献分析方法，探讨公平性基础模型的开发挑战。核心方法提出一个整合干预框架，涵盖数据文档、模型训练、评估到部署协议的所有阶段，强调系统性偏见减轻技术与政策参与相结合。该方法利用自监督学习的基础模型作为起点，通过技术措施如公平性检查点和社会层面的政策建议，来构建全面解决方案。",
      "result": "综述结果表明，实现公平性基础模型需要贯穿开发管道的整合干预，而非单一模型调整。系统性偏见减轻与政策参与能有效克服技术和制度障碍，提升模型在不同群体间的性能一致性。尽管摘要未明确说明具体实验数据，但框架的提出为实践提供了指导，表明这种综合方法能推进公平AI的知识积累和实际应用。",
      "conclusion": "本文结论强调，发展公平基础模型是民主化先进医疗技术的关键步骤，尤其服务于医疗资源有限的群体和地区。主要贡献在于提出一个全面框架，整合技术和社会因素，以应对公平AI的挑战。研究意义包括促进跨学科合作和政策制定，未来工作可涉及实证实施和扩展应用领域。",
      "tags": [
        "Foundation Models",
        "Medical Image Analysis",
        "Fairness",
        "Self-Supervised Learning",
        "Bias Mitigation"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:27.884486Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.15676",
    "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling",
    "authors": [
      "Zhining Zhang",
      "Chuanyang Jin",
      "Mung Yao Jia",
      "Shunchi Zhang",
      "Tianmin Shu"
    ],
    "abstract": "Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce AutoToM, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, AutoToM first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, AutoToM outperforms existing ToM methods and even large reasoning models. Additionally, we show that AutoToM can produce human-like confidence estimates and enable online mental inference for embodied decision-making.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2502.15676.pdf",
    "abs_url": "https://arxiv.org/abs/2502.15676",
    "published": "2025-02-21T18:57:52Z",
    "updated": "2026-01-14T14:23:27Z",
    "comment": "NeurIPS 2025 (Spotlight). 42 pages, 11 figures, 15 tables. Website at https://chuanyangjin.com/AutoToM/",
    "light_analysis": {
      "overview": "AutoToM 通过自动代理建模和贝叶斯逆规划，实现了可扩展、稳健和可解释的心理推理，解决了现有方法的泛化不足和错误倾向问题。",
      "motivation": "心理理论（ToM）是发展社会智能代理的关键能力，但当前方法存在显著局限。基于大型语言模型（LLMs）的推理容易产生系统性错误，而手工代理模型虽稳健却缺乏跨领域泛化能力。这限制了 ToM 在复杂、动态环境中的应用，尤其是在需要精确理解他人意图的场景中。因此，迫切需要一种新方法来平衡稳健性和可扩展性，以促进更可靠的智能代理开发。",
      "method": "AutoToM 采用自动化流程：首先基于给定问题生成初始代理模型，然后利用 LLM 后端执行自动贝叶斯逆规划。关键创新在于迭代优化机制，根据推理不确定性动态调整模型，例如通过引入额外的心理变量或扩展上下文的时间步。该方法结合了模型驱动的推理优势与自动化建模的灵活性，实现了高效的心理状态推断。",
      "result": "在五个多样化的基准测试中，AutoToM 表现优异，超越现有 ToM 方法和大型推理模型，显示出更高的准确性和泛化能力。此外，它能够产生人类类似的置信度估计，为决策提供可靠支持，并实现嵌入式决策中的在线心理推理。这些结果证明了其在实际应用中的潜力。",
      "conclusion": "AutoToM 的主要贡献是提供了一种可扩展、稳健和可解释的心理推理框架，提升了 ToM 任务的性能。其学术价值在于解决了模型泛化与稳健性的平衡问题，为认知计算领域提供了新方向；实际应用价值包括增强社会智能代理和嵌入式系统。未来工作可进一步探索其在新领域或实时场景中的扩展。",
      "tags": [
        "Theory of Mind",
        "Automated Agent Modeling",
        "Bayesian Inverse Planning",
        "Large Language Models",
        "Iterative Refinement"
      ]
    },
    "analyzed_at": "2026-01-15T03:33:30.142333Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.14037",
    "title": "DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation",
    "authors": [
      "Giorgio Franceschelli",
      "Mirco Musolesi"
    ],
    "abstract": "Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose DiffSampling, a new decoding method that leverages a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. In addition, we also propose two variations of the proposed method that aim to correct the subtle inconsistencies of common sampling strategies. Experiments involving four different text-generation tasks demonstrate that our approach consistently performs at least on par with the existing methods it builds upon in terms of quality, despite sampling from a larger set of tokens.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2502.14037.pdf",
    "abs_url": "https://arxiv.org/abs/2502.14037",
    "published": "2025-02-19T19:00:02Z",
    "updated": "2026-01-14T15:16:12Z",
    "comment": "Published in Transactions on Machine Learning Research (2025), see https://tmlr.infinite-conf.org/paper_pages/kXjHbMvdIi.html",
    "light_analysis": {
      "overview": "DiffSampling提出一种基于令牌概率分布差异的新解码方法，旨在提升神经文本生成的多样性和准确性。",
      "motivation": "当前语言模型在文本生成中常复制训练数据、输出重复内容并偏好常见语法和词汇，这主要由于现有解码策略的局限性：贪婪搜索等策略过度依赖高概率令牌，导致输出缺乏多样性；而随机采样等方法则可能引入低概率令牌，损害文本的准确性和连贯性。该研究旨在解决这些不足，以平衡多样性与准确性，提升生成文本的实用性和质量，这是自然语言处理领域的重要挑战。",
      "method": "DiffSampling的核心方法是利用数学分析令牌概率分布，通过计算连续排序概率之间的差异来识别并截断不正确令牌，从而在解码过程中动态调整采样范围，确保生成上下文适当的文本。论文还提出两种变体，旨在纠正常见采样策略中的细微不一致，进一步优化生成效果。摘要未明确说明具体使用的数据集或模型架构，但强调了该方法基于概率分布的创新分析。",
      "result": "实验在四个不同的文本生成任务中进行，结果显示DiffSampling在生成质量上至少与现有基准方法持平，尽管该方法从更大的令牌集中采样，但未牺牲准确性，表明其在多样性和准确性之间取得了良好平衡。摘要未提供具体性能指标数据（如准确率或效率改进），但强调了与基线方法的对比情况，说明方法的一致性表现。",
      "conclusion": "DiffSampling的主要贡献是提出一种创新的解码策略，通过分析概率分布差异来提升文本生成的多样性和准确性，解决了现有方法的不足。这项研究在学术上为解码技术提供了新思路，具有实际应用价值，可用于改进自然语言生成系统的输出质量。摘要未明确说明局限性或未来工作方向，但可能涉及进一步优化方法或扩展应用场景。",
      "tags": [
        "Decoding Strategy",
        "Token Probability Distribution",
        "Sampling Method",
        "Text Generation",
        "Diversity Enhancement"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:33.225275Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.11609",
    "title": "Exploiting Task Relationships in Continual Learning via Transferability-Aware Task Embeddings",
    "authors": [
      "Yanru Wu",
      "Jianning Wang",
      "Xiangyu Chen",
      "Enming Zhang",
      "Yang Tan",
      "Hanbing Liu",
      "Yang Li"
    ],
    "abstract": "Continual learning (CL) has been a critical topic in contemporary deep neural network applications, where higher levels of both forward and backward transfer are desirable for an effective CL performance. Existing CL strategies primarily focus on task models, either by regularizing model updates or by separating task-specific and shared components, while often overlooking the potential of leveraging inter-task relationships to enhance transfer. To address this gap, we propose a transferability-aware task embedding, termed H-embedding, and construct a hypernet framework under its guidance to learn task-conditioned model weights for CL tasks. Specifically, H-embedding is derived from an information theoretic measure of transferability and is designed to be online and easy to compute. Our method is also characterized by notable practicality, requiring only the storage of a low-dimensional task embedding per task and supporting efficient end-to-end training. Extensive evaluations on benchmarks including CIFAR-100, ImageNet-R, and DomainNet show that our framework performs prominently compared to various baseline and SOTA approaches, demonstrating strong potential in capturing and utilizing intrinsic task relationships. Our code is publicly available at https://github.com/viki760/Hembedding_Guided_Hypernet.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.11609.pdf",
    "abs_url": "https://arxiv.org/abs/2502.11609",
    "published": "2025-02-17T09:52:19Z",
    "updated": "2026-01-14T15:58:22Z",
    "comment": "28 pages, 5 figures, accepted by NeurIPS 2025",
    "light_analysis": {
      "overview": "本文提出了一种可迁移性感知的任务嵌入（H-embedding）和超网络框架，用于持续学习中高效利用任务间关系。",
      "motivation": "持续学习在深度神经网络应用中至关重要，旨在实现高效的前向和后向迁移以提升性能。然而，现有方法主要聚焦于任务模型的优化，如通过正则化更新或分离任务特定与共享组件，往往忽视了利用任务间关系来增强迁移潜力。这一问题的重要性在于，忽视这些关系可能导致迁移效率低下，影响模型在连续任务中的适应性。因此，本研究旨在填补这一空白，探索如何通过捕捉内在任务关系来改进持续学习策略。",
      "method": "本研究提出了一种名为H-embedding的可迁移性感知任务嵌入，其基于信息理论中的迁移性度量设计，能够在线快速计算。以此嵌入为指导，构建了一个超网络框架，用于学习任务条件化的模型权重。关键创新点在于通过H-embedding直接编码任务关系，从而优化模型权重分配。方法具有高度实用性，仅需为每个任务存储低维嵌入，支持端到端高效训练，减少了存储和计算开销，使得在连续任务中动态调整模型成为可能。",
      "result": "研究在多个基准数据集（包括CIFAR-100、ImageNet-R和DomainNet）上进行了广泛评估，结果表明该方法性能显著优于多种基线方法和当前最先进方法。摘要未明确说明具体性能指标（如准确率提升百分比），但强调框架在捕获和利用任务内在关系方面表现出强潜力，证明了其有效性。与现有策略相比，该方法在迁移效率和任务适应性上有所提升，凸显了利用任务嵌入的优越性。",
      "conclusion": "本研究的主要贡献是提出了一种新型可迁移性感知任务嵌入和超网络框架，成功增强了持续学习中任务关系的利用。其学术价值在于为持续学习领域提供了新的技术路径，推动了迁移机制的理论探索；实际应用价值体现在方法的实用性和高效性，适合大规模部署。局限性或未来工作方向摘要未明确说明，但可推测未来可能扩展到更多任务类型或优化计算效率。总体而言，该研究对提升深度神经网络的持续学习能力具有积极意义。",
      "tags": [
        "Continual Learning",
        "Transferability",
        "Task Embeddings",
        "Hypernetworks",
        "Information Theory"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:41.072380Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2502.11245",
    "title": "Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens",
    "authors": [
      "Samuele Bortolotti",
      "Emanuele Marconato",
      "Paolo Morettin",
      "Andrea Passerini",
      "Stefano Teso"
    ],
    "abstract": "Concept-based Models are neural networks that learn a concept extractor to map inputs to high-level concepts and an inference layer to translate these into predictions. Ensuring these modules produce interpretable concepts and behave reliably in out-of-distribution is crucial, yet the conditions for achieving this remain unclear. We study this problem by establishing a novel connection between Concept-based Models and reasoning shortcuts (RSs), a common issue where models achieve high accuracy by learning low-quality concepts, even when the inference layer is fixed and provided upfront. Specifically, we extend RSs to the more complex setting of Concept-based Models and derive theoretical conditions for identifying both the concepts and the inference layer. Our empirical results highlight the impact of RSs and show that existing methods, even combined with multiple natural mitigation strategies, often fail to meet these conditions in practice.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.11245.pdf",
    "abs_url": "https://arxiv.org/abs/2502.11245",
    "published": "2025-02-16T19:45:09Z",
    "updated": "2026-01-14T15:06:46Z",
    "comment": "Accepted at NeurIPS25",
    "light_analysis": {
      "overview": "本研究建立概念基模型与推理捷径之间的新连接，并推导出识别概念和推理层的理论条件。",
      "motivation": "概念基模型旨在通过学习概念提取器将输入映射到高层概念，并使用推理层进行预测，但确保这些模块产生可解释概念并在分布外数据上可靠行为是关键挑战。现有方法常受推理捷径影响，即模型可能通过学习低质量概念获得高准确性，即使推理层固定，这削弱了模型的可信度和应用价值。因此，迫切需要明确实现可靠概念基模型的条件，以解决可解释性不足和分布外泛化能力差的问题。",
      "method": "论文将推理捷径扩展到概念基模型的复杂设置中，通过理论推导来识别概念和推理层的条件。关键创新在于从神经符号视角建立分析框架，连接概念基模型与推理捷径问题，以探讨如何确保概念质量。摘要未明确说明使用的具体数据集或模型架构，但方法涉及理论分析来推导识别条件，并可能通过实证实验验证理论框架的有效性。",
      "result": "实证结果突出了推理捷径对概念基模型的显著影响，显示现有方法即使结合多种自然缓解策略，在实践中常无法满足理论条件。这表明确保概念可识别性和推理层准确性的挑战依然存在，现有技术在防止短路学习和提升模型可靠性方面存在局限。摘要未提供具体性能指标数据，但强调了理论与实证之间的差距，基线对比主要针对传统方法。",
      "conclusion": "本研究的主要贡献是提供理论框架来分析概念基模型的识别性问题，强调推理捷径对可解释性的挑战，学术价值在于深化对概念学习机制的理解，实际应用价值为开发更可靠的解释性AI模型铺路。局限性包括摘要未明确说明未来工作方向，但可推断需要探索更有效的缓解策略来满足理论条件，以提升模型在分布外场景的表现。",
      "tags": [
        "Concept-based Models",
        "Reasoning Shortcuts",
        "Neuro-Symbolic AI",
        "Interpretability",
        "Out-of-Distribution Reliability"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:03.221333Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.18792",
    "title": "Bayesian Optimization with Preference Exploration using a Monotonic Neural Network Ensemble",
    "authors": [
      "Hanyang Wang",
      "Juergen Branke",
      "Matthias Poloczek"
    ],
    "abstract": "Many real-world black-box optimization problems have multiple conflicting objectives. Rather than attempting to approximate the entire set of Pareto-optimal solutions, interactive preference learning allows to focus the search on the most relevant subset. However, few previous studies have exploited the fact that utility functions are usually monotonic. In this paper, we address the Bayesian Optimization with Preference Exploration (BOPE) problem and propose using a neural network ensemble as a utility surrogate model. This approach naturally integrates monotonicity and supports pairwise comparison data. Our experiments demonstrate that the proposed method outperforms state-of-the-art approaches and exhibits robustness to noise in utility evaluations. An ablation study highlights the critical role of monotonicity in enhancing performance.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2501.18792.pdf",
    "abs_url": "https://arxiv.org/abs/2501.18792",
    "published": "2025-01-30T22:50:34Z",
    "updated": "2026-01-14T12:51:34Z",
    "comment": null,
    "light_analysis": {
      "overview": "提出一种使用单调神经网络集成的贝叶斯优化与偏好探索方法，通过整合效用函数的单调性来提升搜索效率和鲁棒性。",
      "motivation": "现实世界黑盒优化问题常涉及多个冲突目标，如工程设计或资源分配，交互式偏好学习能帮助用户聚焦最相关解集，避免计算所有帕累托最优解的负担。然而，现有方法大多忽略了效用函数通常具有单调性这一事实，导致搜索效率不高或模型不稳健。本研究旨在解决这一问题，通过引入单调性约束来优化贝叶斯优化过程，弥补了现有技术的不足，从而提高偏好学习的实用性和准确性。",
      "method": "论文的核心方法是使用一个神经网络集成作为效用函数的代理模型，该模型自然整合了单调性约束，并能有效处理成对比较数据。关键创新点在于首次在贝叶斯优化框架中系统地利用单调性，以提高模型预测精度和对噪声的鲁棒性。方法支持用户通过交互反馈优化偏好，但摘要未明确说明具体的数据集、神经网络架构细节或训练过程，主要强调单调性整合和数据兼容性。",
      "result": "实验结果表明，所提出的方法在性能上显著优于现有的最先进方法，对效用评估中的噪声表现出较强的鲁棒性。通过消融研究，确认了单调性在提升算法性能中的关键作用，例如提高模型稳定性和搜索效率。摘要未提供具体的准确率或改进数值，但强调了方法的优越性，尤其在处理不确定偏好数据时表现更佳，为多目标优化问题提供了更可靠的解决方案。",
      "conclusion": "本研究的主要贡献是开发了一种集成单调性的贝叶斯优化与偏好探索框架，有效提升了偏好学习的效果和稳健性。其学术价值在于创新性地结合了单调性约束和神经网络集成技术，为机器学习优化领域提供了新思路；实践价值在于为现实应用如自动化设计或个性化推荐提供了更高效的优化工具。未来工作可探索扩展单调性到其他优化场景，或结合更多数据类型以增强泛化能力。",
      "tags": [
        "Bayesian Optimization",
        "Preference Learning",
        "Neural Network Ensemble",
        "Monotonic Functions",
        "Pairwise Comparison"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:24.568557Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2501.04184",
    "title": "MedicalNarratives: Connecting Medical Vision and Language with Localized Narratives",
    "authors": [
      "Wisdom O. Ikezogwo",
      "Kevin Zhang",
      "Mehmet Saygin Seyfioglu",
      "Fatemeh Ghezloo",
      "Linda Shapiro",
      "Ranjay Krishna"
    ],
    "abstract": "Multi-modal models are data hungry. While datasets with natural images are abundant, medical image datasets can not afford the same luxury. To enable representation learning for medical images at scale, we turn to YouTube, a platform with a large reservoir of open-source medical pedagogical videos. We curate MedicalNarratives, a dataset 4.7M medical image-text pairs, with 1M samples containing dense annotations in the form of spatial traces (and bounding boxes), and 118K videos centered on the trace event (with aligned text), enabling spatiotemporal grounding beyond single frames. Similar to $\\textit{think-aloud}$ studies where instructors speak while hovering their mouse cursor movements over relevant image regions, 1M images in MedicalNarratives contains localized mouse traces in image pixels, creating a spatial and temporal association between the text and pixels. To evaluate the utility of MedicalNarratives, we train GenMedClip with a CLIP-like objective using our dataset spanning 12 medical domains. GenMedClip outperforms previous state-of-the-art models on all 12 domains on a newly constructed medical imaging benchmark. $\\href{https://huggingface.co/datasets/wisdomik/MedicalNarratives}{[Data]}$",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2501.04184.pdf",
    "abs_url": "https://arxiv.org/abs/2501.04184",
    "published": "2025-01-07T23:32:05Z",
    "updated": "2026-01-14T03:02:38Z",
    "comment": null,
    "light_analysis": {
      "overview": "论文提出MedicalNarratives数据集，结合局部化叙述连接医学视觉与语言，并通过训练GenMedClip模型在医学多模态任务中取得领先性能。",
      "motivation": "医学多模态模型面临数据稀缺的挑战，尤其在医学领域，由于隐私和成本限制，现有数据集远少于自然图像，导致模型在医疗应用中的性能不足。本研究旨在解决这一瓶颈，通过挖掘YouTube的公开医疗教学视频资源，构建大规模、高质量标注的图像-文本对数据集，以促进医学视觉-语言表示学习的发展，提升模型的泛化能力和实际应用效果。",
      "method": "研究方法包括从YouTube收集医学教学视频，整理成MedicalNarratives数据集，包含4.7M图像-文本对，其中1M样本带有空间痕迹和边界框的密集注释，118K视频支持时空关联。关键创新是模仿“think-aloud”研究，通过鼠标痕迹建立文本与图像像素的时空联系。使用类似CLIP的目标函数训练GenMedClip模型，覆盖12个医学领域，以验证数据集对多模态学习的有效性。",
      "result": "实验结果显示，GenMedClip在新构建的医学成像基准测试中，在所有12个医学领域上均优于先前的最先进模型。摘要未明确说明具体性能指标如准确率，但强调了其全面的领先地位，这验证了MedicalNarratives数据集对提升模型性能的关键作用，展示了利用局部化叙述在医学多模态学习中的显著优势。",
      "conclusion": "本研究的主要贡献是创建了大规模、注释丰富的MedicalNarratives数据集，并通过GenMedClip模型验证了其在医学视觉-语言任务中的卓越性能，具有重要的学术价值和实际应用意义，为医学图像理解和教育工具开发奠定了基础。未来工作可能包括扩展到更多医学子领域或优化模型架构以进一步提高泛化能力。",
      "tags": [
        "Multi-modal Learning",
        "Medical Vision",
        "CLIP",
        "Dataset Curation",
        "Spatiotemporal Grounding"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:44.886196Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.16148",
    "title": "Frequency Is What You Need: Considering Word Frequency When Text Masking Benefits Vision-Language Model Pre-training",
    "authors": [
      "Mingliang Liang",
      "Martha Larson"
    ],
    "abstract": "Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of strategies (truncation, random masking, block masking and syntax masking) and has reported syntax masking as the top performer. In this paper, we analyze the impact of different text masking strategies on the word frequency in the training data, and show that this impact is connected to model success. This finding motivates Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF), our proposed masking approach, which directly leverages word frequency. Extensive experiments demonstrate the advantages of CLIPF over syntax masking and other existing approaches, particularly when the number of input tokens decreases. We show that not only CLIPF, but also other existing masking strategies, outperform syntax masking when enough epochs are used during training, a finding of practical importance for selecting a text masking method for VLM training. Our code is available online.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2412.16148.pdf",
    "abs_url": "https://arxiv.org/abs/2412.16148",
    "published": "2024-12-20T18:51:41Z",
    "updated": "2026-01-14T11:07:46Z",
    "comment": "Accepted by WACV 2026",
    "light_analysis": {
      "overview": "论文提出了一种基于词频的文本掩码策略 CLIPF，用于提升视觉语言模型预训练的效率，超越了现有最佳语法掩码方法。",
      "motivation": "VLMs 训练需要大量计算资源，提高效率是研究热点。现有文本掩码策略如语法掩码被报告为最佳，但未考虑词频对训练数据分布的影响，可能导致模型学习不足。本研究通过分析不同掩码策略（如截断、随机掩码）对训练数据中词频的效应，发现词频变化与模型性能成功相关，这揭示了现有方法的局限性，并激励提出直接利用词频的掩码策略，以更有效地优化训练过程，解决资源受限下的效率问题。",
      "method": "本研究提出 Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF)，一种新型文本掩码方法。核心创新是直接基于训练数据中的词频信息进行掩码，而非依赖随机、块或语法掩码。该方法通过分析词频分布，设计掩码模式来调整输入token的保留或替换，以增强对比学习的效果。摘要未明确说明具体模型架构和数据集细节，但推断采用标准对比学习框架，可能应用于图像-文本配对数据预训练，如 CLIP 风格模型。",
      "result": "广泛实验显示，CLIPF 在性能上优于现有最佳语法掩码和其他掩码方法，特别是在输入 token 数量减少时表现更佳，验证了词频掩码的有效性。研究还发现当训练 epoch 足够时，其他掩码策略也能超过语法掩码，这一发现对实际 VLM 训练中掩码方法选择具有重要指导意义。尽管摘要未提供具体准确率或效率改进数据，但实验结果表明词频掩码能提升模型成功，为资源优化提供了实证支持。",
      "conclusion": "论文的主要贡献是提出并验证了基于词频的 CLIPF 掩码策略，它通过直接利用词频信息优化文本掩码，显著提升了 VLM 预训练的效率。该研究强调了词频在掩码策略设计中的关键作用，为掩码方法改进提供了新视角，具有学术价值。实际应用价值在于帮助在有限计算资源下更高效地训练 VLMs，可能降低训练成本。局限性如具体数据集和超参数设置未明确说明，未来工作可进一步探索词频与其他因素（如语义结构）的结合。",
      "tags": [
        "Vision-Language Models",
        "Text Masking",
        "Word Frequency",
        "Contrastive Learning",
        "Pre-training"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:40.691410Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2412.09468",
    "title": "STORM: A Spatio-Temporal Factor Model Based on Dual Vector Quantized Variational Autoencoders for Financial Trading",
    "authors": [
      "Yilei Zhao",
      "Wentao Zhang",
      "Tingran Yang",
      "Yong Jiang",
      "Fei Huang",
      "Wei Yang Bryan Lim"
    ],
    "abstract": "In financial trading, factor models are widely used to price assets and capture excess returns from mispricing. Recently, we have witnessed the rise of variational autoencoder-based latent factor models, which learn latent factors self-adaptively. While these models focus on modeling overall market conditions, they often fail to effectively capture the temporal patterns of individual stocks. Additionally, representing multiple factors as single values simplifies the model but limits its ability to capture complex relationships and dependencies. As a result, the learned factors are of low quality and lack diversity, reducing their effectiveness and robustness across different trading periods. To address these issues, we propose a Spatio-Temporal factOR Model based on dual vector quantized variational autoencoders, named STORM, which extracts features of stocks from temporal and spatial perspectives, then fuses and aligns these features at the fine-grained and semantic level, and represents the factors as multi-dimensional embeddings. The discrete codebooks cluster similar factor embeddings, ensuring orthogonality and diversity, which helps distinguish between different factors and enables factor selection in financial trading. To show the performance of the proposed factor model, we apply it to two downstream experiments: portfolio management on two stock datasets and individual trading tasks on six specific stocks. The extensive experiments demonstrate STORM's flexibility in adapting to downstream tasks and superior performance over baseline models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2412.09468.pdf",
    "abs_url": "https://arxiv.org/abs/2412.09468",
    "published": "2024-12-12T17:15:49Z",
    "updated": "2026-01-14T07:55:43Z",
    "comment": null,
    "light_analysis": {
      "overview": "STORM提出了一种基于双向量量化变分自编码器的时空因子模型，通过提取和融合时空特征并将因子表示为多维嵌入，提升金融交易中因子质量和多样性。",
      "motivation": "在金融交易中，因子模型广泛用于资产定价和捕捉超额收益。现有基于变分自编码器的潜在因子模型虽能自适应学习因子，但主要关注整体市场条件，难以有效捕捉个股的时间模式，且将多因子简化为单值表示，限制了捕捉复杂关系和依赖的能力。这导致学到的因子质量低、缺乏多样性，降低了模型在不同交易期的有效性和鲁棒性，因此需要改进模型以更好地处理时空动态。",
      "method": "STORM采用双向量量化变分自编码器，从时间和空间两个角度提取股票特征，并在细粒度和语义层面进行融合与对齐，将因子表示为多维嵌入而非单值。核心创新包括使用离散码本聚类相似因子嵌入，确保正交性和多样性，从而帮助区分不同因子并支持金融交易中的因子选择。摘要未明确说明具体数据集和模型架构细节，但模型设计旨在适应金融数据的时空特性。",
      "result": "实验将STORM应用于两个下游任务：在两个股票数据集上进行组合管理，以及对六支特定股票执行个别交易任务。广泛实验表明，STORM能灵活适应不同下游任务，并在性能上优于基线模型。摘要未提供具体量化指标如准确率提升，但强调了其优越性和适应性，验证了模型在提高因子有效性和鲁棒性方面的优势。",
      "conclusion": "STORM通过引入时空因子模型，成功解决了现有方法在捕捉个股时间模式和因子表示简化方面的不足，主要贡献在于提高因子多样性和质量，增强金融交易模型的有效性。该研究具有实际应用价值，可改进资产定价和交易策略。未来工作可能涉及扩展到更多金融场景或优化模型效率，但摘要未明确说明。",
      "tags": [
        "Spatio-Temporal Modeling",
        "Vector Quantized Variational Autoencoder",
        "Factor Analysis",
        "Financial Trading",
        "Portfolio Management"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:39.362105Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.07426",
    "title": "Evaluating Detection Thresholds: The Impact of False Positives and Negatives on Super-Resolution Ultrasound Localization Microscopy",
    "authors": [
      "Sepideh K. Gharamaleki",
      "Brandon Helfield",
      "Hassan Rivaz"
    ],
    "abstract": "Super-resolution ultrasound imaging with ultrasound localization microscopy (ULM) offers a high-resolution view of microvascular structures. Yet, ULM image quality heavily relies on precise microbubble (MB) detection. Despite the crucial role of localization algorithms, there has been limited focus on the practical pitfalls in MB detection tasks such as setting the detection threshold. This study examines how False Positives (FPs) and False Negatives (FNs) affect ULM image quality by systematically adding controlled detection errors to simulated data. Results indicate that while both FP and FN rates impact Peak Signal-to-Noise Ratio (PSNR) similarly, increasing FP rates from 0\\% to 20\\% decreases Structural Similarity Index (SSIM) by 7\\%, whereas same FN rates cause a greater drop of around 45\\%. Moreover, dense MB regions are more resilient to detection errors, while sparse regions show high sensitivity, showcasing the need for robust MB detection frameworks to enhance super-resolution imaging.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2411.07426.pdf",
    "abs_url": "https://arxiv.org/abs/2411.07426",
    "published": "2024-11-11T22:58:56Z",
    "updated": "2026-01-14T05:05:54Z",
    "comment": null,
    "light_analysis": {
      "overview": "该论文通过模拟实验评估假阳性和假阴性对超分辨率超声定位显微镜图像质量的影响，揭示了检测阈值设置的重要性。",
      "motivation": "超分辨率超声成像技术，特别是超声定位显微镜（ULM），提供微血管结构的高分辨率视图，但图像质量严重依赖精确的微泡（MB）检测。尽管定位算法关键，但实际应用中如检测阈值设置等陷阱常被忽视，导致图像质量下降。现有方法缺乏对检测错误影响的系统性分析，因此研究假阳性和假阴性的影响至关重要，以解决这一不足并提升成像可靠性。",
      "method": "本研究采用模拟实验方法，系统性地向ULM的模拟数据中添加受控的假阳性（FP）和假阴性（FN）检测错误，通过调整检测阈值模拟不同错误率。关键创新点在于量化分析检测错误对图像质量指标（如PSNR和SSIM）的影响，并比较密集与稀疏MB区域的敏感性。摘要未明确说明具体数据集细节，但提及使用模拟数据以确保实验受控性，避免了真实数据中的噪声干扰。",
      "result": "实验结果表明，假阳性和假阴性率对峰值信噪比（PSNR）的影响相似。然而，在结构相似性指数（SSIM）方面，假阴性率从0%增加到20%导致SSIM下降约45%，远高于假阳性率相同变化下的7%下降。此外，密集微泡区域对检测错误更具弹性，而稀疏区域高度敏感。这些发现为优化检测算法提供了数据支持，强调了减少假阴性错误的重要性，特别是在稀疏血管结构成像中。",
      "conclusion": "本研究的主要贡献是系统地评估了检测错误对超分辨率超声定位显微镜图像质量的影响，量化了假阳性和假阴性的不同影响程度。研究强调了稳健微泡检测框架的必要性，以提升成像准确性和可靠性。学术价值在于提供了量化分析方法，实际应用价值在于指导检测阈值设置和改进图像重建算法。摘要未明确说明未来工作方向，但可推断需要开发更稳健的检测算法或扩展到真实数据验证，以增强临床应用潜力。",
      "tags": [
        "Ultrasound Localization Microscopy",
        "Super-Resolution Imaging",
        "Microbubble Detection",
        "False Positive",
        "False Negative"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:49.692377Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.06500",
    "title": "Graph Neural Network Surrogates to leverage Mechanistic Expert Knowledge towards Reliable and Immediate Pandemic Response",
    "authors": [
      "Agatha Schmidt",
      "Henrik Zunker",
      "Alexander Heinlein",
      "Martin J. Kühn"
    ],
    "abstract": "During the COVID-19 crisis, mechanistic models have guided evidence-based decision making. However, time-critical decisions in a dynamical environment limit the time available to gather supporting evidence. We address this bottleneck by developing a graph neural network (GNN) surrogate of an age-structured and spatially resolved mechanistic metapopulation simulation model. This combined approach complements classical modeling approaches which are mostly mechanistic and purely data-driven machine learning approaches which are often black box. Our design of experiments spans outbreak and persistent-threat regimes, up to three contact change points, and age-structured contact matrices on a spatial graph with 400 nodes representing German counties. We benchmark multiple GNN layers and identify an ARMAConv-based architecture that offers a strong accuracy-runtime trade-off. Across horizons of 30-90 day simulation and prediction, allowing up to three contact change points, the surrogate model attains 10-27 \\% mean absolute percentage error (MAPE) while delivering (near) constant runtime with respect to the forecast horizon. Our approach accelerates evaluation by up to 28,670 times compared with the mechanistic model, allowing responsive decision support in time-critical scenarios and straightforward web integration. These results show how GNN surrogates can translate complex metapopulation models into immediate, reliable tools for pandemic response.",
    "categories": [
      "cs.LG",
      "q-bio.PE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.06500.pdf",
    "abs_url": "https://arxiv.org/abs/2411.06500",
    "published": "2024-11-10T15:54:09Z",
    "updated": "2026-01-14T15:26:51Z",
    "comment": "20 pages, 9 figures",
    "light_analysis": {
      "overview": "本论文提出了一种基于图神经网络的替代模型，通过结合机制知识和机器学习，加速疫情模拟以支持快速决策。",
      "motivation": "在COVID-19危机中，基于证据的决策依赖于机制模型，但动态环境下的时间关键决策限制了模拟和证据收集的时间。现有方法包括计算密集的机制模型和缺乏可解释性的黑盒数据驱动方法，无法满足紧急响应需求。因此，需要开发一种混合方法，以在保持可靠性的同时实现快速评估，解决传统模型的时间瓶颈。",
      "method": "研究开发了图神经网络作为年龄结构和空间分辨机制元种群模拟模型的替代。实验设计包括模拟疫情爆发和持续威胁制度，最多考虑三个接触变化点，并使用基于400个节点代表德国县的空间图上的年龄结构接触矩阵。通过基准测试多种图神经网络层，最终选择了基于ARMAConv的架构，该架构在准确性和运行时间之间提供了优化平衡。",
      "result": "在30-90天的模拟和预测范围内，最多有三个接触变化点，替代模型的平均绝对百分比误差为10-27%，同时运行时间相对于预测范围保持近恒定。与原始机制模型相比，评估速度提升了高达28,670倍，显著提高了决策支持效率，在时间关键场景中实现了近实时响应。",
      "conclusion": "本研究表明图神经网络替代模型能够将复杂的元种群模拟转化为即时可靠的疫情响应工具，结合了机制模型的可解释性和机器学习的高效性。这具有学术价值，推动混合建模发展；实际应用价值体现在快速决策支持和网络集成，但未来可能需要扩展至更广泛场景或优化泛化能力，摘要未明确说明局限性。",
      "tags": [
        "Graph Neural Network",
        "Surrogate Model",
        "ARMAConv",
        "Metapopulation Model",
        "Pandemic Simulation"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:41.054059Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2411.01074",
    "title": "DNN Modularization via Activation-Driven Training",
    "authors": [
      "Tuan Ngo",
      "Abid Hassan",
      "Saad Shafiq",
      "Nenad Medvidovic"
    ],
    "abstract": "Deep Neural Networks (DNNs) tend to accrue technical debt and suffer from significant retraining costs when adapting to evolving requirements. Modularizing DNNs offers the promise of improving their reusability. Previous work has proposed techniques to decompose DNN models into modules both during and after training. However, these strategies yield several shortcomings, including significant weight overlaps and accuracy losses across modules, restricted focus on convolutional layers only, and added complexity and training time by introducing auxiliary masks to control modularity. In this work, we propose MODA, an activation-driven modular training approach. MODA promotes inherent modularity within a DNN model by directly regulating the activation outputs of its layers based on three modular objectives: intra-class affinity, inter-class dispersion, and compactness. MODA is evaluated using three well-known DNN models and five datasets with varying sizes. This evaluation indicates that, compared to the existing state-of-the-art, using MODA yields several advantages: (1) MODA accomplishes modularization with 22% less training time; (2) the resultant modules generated by MODA comprise up to 24x fewer weights and 37x less weight overlap while (3) preserving the original model's accuracy without additional fine-tuning; in module replacement scenarios, (4) MODA improves the accuracy of a target class by 12% on average while ensuring minimal impact on the accuracy of other classes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2411.01074.pdf",
    "abs_url": "https://arxiv.org/abs/2411.01074",
    "published": "2024-11-01T23:07:33Z",
    "updated": "2026-01-14T18:22:31Z",
    "comment": "Accepted at International Conference on Software Engineering (ICSE) 2026 - Research Track",
    "light_analysis": {
      "overview": "本文提出 MODA 方法，一种基于激活驱动训练的深度神经网络模块化技术，通过调节层激活输出提升训练效率并减少权重重叠。",
      "motivation": "深度神经网络在适应新需求时面临重训练成本高的问题，模块化可提高模型可重用性。现有方法如分解模型到模块中存在权重重叠严重、准确率损失、仅适用于卷积层以及增加训练时间和复杂度等不足，限制了实际应用。因此，需要开发更高效、保持性能的模块化方法来解决这些技术债务和效率瓶颈。",
      "method": "MODA 是一种激活驱动的模块化训练方法，通过在训练中直接调节神经网络层激活输出来促进内在模块性，基于三个模块目标：类内亲和力、类间分散性和紧凑性。关键创新包括避免使用辅助掩码，简化过程并提高效率。评估使用三个已知 DNN 模型和五个不同规模的数据集，确保方法泛化性。",
      "result": "实验表明，MODA 相比现有技术减少 22% 训练时间；生成模块包含最多 24 倍更少权重和 37 倍更少权重重叠；保持原始模型准确率无需额外微调。在模块替换场景中，目标类准确率平均提高 12%，对其他类影响最小，验证了方法的有效性和高效性。",
      "conclusion": "MODA 方法成功实现 DNN 模块化，显著提升训练效率和模块性能，减少技术债务，具有学术价值和实际应用潜力。研究展示了激活驱动方法的优势，未来工作可扩展到更广泛的网络架构和动态适应场景，摘要未明确说明具体局限性。",
      "tags": [
        "DNN Modularization",
        "Activation-Driven Training",
        "Intra-class Affinity",
        "Inter-class Dispersion",
        "Training Efficiency"
      ]
    },
    "analyzed_at": "2026-01-15T03:34:45.425971Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.21324",
    "title": "Mathematical Derivation Graphs: A Relation Extraction Task in STEM Manuscripts",
    "authors": [
      "Vishesh Prasad",
      "Brian Kim",
      "Nickvash Kani"
    ],
    "abstract": "Recent advances in natural language processing (NLP), particularly with the emergence of large language models (LLMs), have significantly enhanced the field of textual analysis. However, while these developments have yielded substantial progress in analyzing natural language text, applying analysis to mathematical equations and their relationships within texts has produced mixed results. This paper takes the initial steps in expanding the problem of relation extraction towards understanding the dependency relationships between mathematical expressions in STEM articles. The authors construct the Mathematical Derivation Graphs Dataset (MDGD), sourced from a random sampling of the arXiv corpus, containing an analysis of $107$ published STEM manuscripts with over $2000$ manually labeled inter-equation dependency relationships, resulting in a new object referred to as a derivation graph that summarizes the mathematical content of the manuscript. The authors exhaustively evaluate analytical and machine learning (ML) based models to assess their capability to identify and extract the derivation relationships for each article and compare the results with the ground truth. The authors show that the best tested LLMs achieve $F_1$ scores of $\\sim45\\%-52\\%$, and attempt to improve their performance by combining them with analytic algorithms and other methods.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2410.21324.pdf",
    "abs_url": "https://arxiv.org/abs/2410.21324",
    "published": "2024-10-26T16:52:22Z",
    "updated": "2026-01-14T17:06:58Z",
    "comment": "29 pages, 11 figures",
    "light_analysis": {
      "overview": "本文构建数学推导图数据集并评估模型，扩展关系提取任务至STEM文章中数学表达式的依赖关系分析。",
      "motivation": "随着自然语言处理（NLP）和大型语言模型（LLMs）的发展，文本分析在自然语言领域取得显著进展，但在分析数学方程及其关系时效果不佳。本研究旨在解决STEM文章中数学表达式依赖关系的提取问题，因为数学推导是学术内容的核心，自动提取这些关系有助于提升文献分析效率。现有方法在自然语言文本上有效，但在处理结构化数学内容时不足，突显了需要专门技术来处理这一挑战。",
      "method": "作者提出数学推导图的关系提取任务，并构建数学推导图数据集（MDGD），该数据集基于arXiv语料库随机采样的107篇STEM文章，包含超过2000个手动标注的方程间依赖关系，形成推导图以总结数学内容。研究方法涉及全面评估分析和机器学习模型，特别是使用大型语言模型（LLMs），并尝试通过结合分析算法和其他方法提升性能，关键创新点在于扩展关系提取至数学表达式领域并提供基准数据集。",
      "result": "实验结果表明，测试中表现最佳的大型语言模型（LLMs）在提取数学推导关系时，F1分数达到约45%至52%。作者将这些结果与真实标注进行比较，并尝试通过整合分析算法来改进性能。摘要未明确说明改进后的具体分数，但与基线方法对比显示，LLMs在这一新任务上仍面临挑战，需进一步优化以实现更高准确性。",
      "conclusion": "本文的主要贡献是定义了数学推导图的关系提取任务，并构建了MDGD数据集，为研究社区提供了重要基准。学术价值在于扩展NLP应用到STEM领域，推动数学内容的自动分析；实际应用价值包括辅助学术文献的高效处理。局限性在于模型性能有限，未来工作可探索结合LLMs与分析技术的更有效方法，以提升关系提取效果。",
      "tags": [
        "Relation Extraction",
        "Large Language Model",
        "Mathematical Derivation Graph",
        "Dataset Construction"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:27.770866Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2410.00998",
    "title": "\"Hiding in Plain Sight\": Designing Synthetic Dialog Generation for Uncovering Socially Situated Norms",
    "authors": [
      "Chengfei Wu",
      "Dan Goldwasser"
    ],
    "abstract": "Naturally situated conversations encapsulate the social norms inherent to their context, reflecting both the relationships between interlocutors and the underlying communicative intent. In this paper, we propose a novel, multi-step framework for generating dialogues that automatically uncovers social norms from rich, context-laden interactions through a process of self-assessment and norm discovery, rather than relying on predefined norm labels. Leveraging this framework, we construct NormHint, a comprehensive synthetic dialogue dataset spanning a wide range of interlocutor attributes (e.g., age, profession, personality), relationship types, conversation topics, and conversational trajectories. NormHint is meticulously annotated with turn-level norm violation information, detailed participant descriptions, and remediation suggestions-including alternative trajectories achieved through early intervention. Human validation and automated analysis demonstrate that our dataset captures diverse conversational topics with high naturalness and realism. Moreover, we discovered that fine-tuning a model with our norm violation data significantly enhances its ability to detect and understand potential norm violations in conversations.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2410.00998.pdf",
    "abs_url": "https://arxiv.org/abs/2410.00998",
    "published": "2024-10-01T18:38:23Z",
    "updated": "2026-01-14T06:03:44Z",
    "comment": "Accepted at AAAI Workshop on Shaping Responsible Synthetic Data in the Era of Foundation Models",
    "light_analysis": {
      "overview": "本文提出了一种新型多步骤框架，通过自我评估和规范发现自动生成揭示社交规范的对话，并构建了标注详尽的合成数据集NormHint。",
      "motivation": "社交规范在自然对话中无处不在，但现有方法通常依赖于预定义的规范标签，难以全面捕捉上下文相关的复杂规范，限制了模型检测和理解规范违反的能力。这促使研究开发一种自动发现规范的方法，以解决传统依赖手工标注的不足，提升对话系统的社交智能，特别是在多变的互动场景中。",
      "method": "论文提出了一个多步骤框架，通过自我评估和规范发现过程自动生成对话，而无需依赖预定义规范标签。核心创新包括设计合成对话生成机制，覆盖多种参与者属性（如年龄、职业、个性）、关系类型和话题，并利用此框架构建NormHint数据集，精细标注轮次级别的规范违反信息、参与者描述和补救建议。",
      "result": "人类验证和自动分析表明，NormHint数据集捕获了多样话题，具有高自然度和真实感。摘要未明确说明具体性能指标，但提到使用规范违反数据微调模型后，其检测和理解潜在规范违反的能力显著增强，优于基线方法，显示出实际应用潜力。",
      "conclusion": "本研究贡献了一个自动发现社交规范的框架和标注丰富的合成数据集NormHint，增强了对话系统理解和检测规范违反的能力，具有学术价值，推动了社交智能计算发展，并可在客服、教育等应用中提升交互质量。未来工作可能包括扩展规范类型或应用于更多场景，以克服潜在局限性。",
      "tags": [
        "Synthetic Dialog Generation",
        "Norm Discovery",
        "Self-Assessment",
        "NormHint Dataset",
        "Fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:28.673082Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2409.19800",
    "title": "Differentially Private Bilevel Optimization",
    "authors": [
      "Guy Kornowski"
    ],
    "abstract": "We present differentially private (DP) algorithms for bilevel optimization, a problem class that received significant attention lately in various machine learning applications. These are the first algorithms for such problems under standard DP constraints, and are also the first to avoid Hessian computations which are prohibitive in large-scale settings. Under the well-studied setting in which the upper-level is not necessarily convex and the lower-level problem is strongly-convex, our proposed gradient-based $(ε,δ)$-DP algorithm returns a point with hypergradient norm at most $\\widetilde{\\mathcal{O}}\\left((\\sqrt{d_\\mathrm{up}}/εn)^{1/2}+(\\sqrt{d_\\mathrm{low}}/εn)^{1/3}\\right)$ where $n$ is the dataset size, and $d_\\mathrm{up}/d_\\mathrm{low}$ are the upper/lower level dimensions. Our analysis covers constrained and unconstrained problems alike, accounts for mini-batch gradients, and applies to both empirical and population losses. As an application, we specialize our analysis to derive a simple private rule for tuning a regularization hyperparameter.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2409.19800.pdf",
    "abs_url": "https://arxiv.org/abs/2409.19800",
    "published": "2024-09-29T21:52:38Z",
    "updated": "2026-01-14T15:44:18Z",
    "comment": "Accepted to ALT 2026; some fixes following reviews",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "JSON 解析失败: 结构化输出解析失败 (尝试 4 次): Invalid json output: {\n  \"overview\": \"本文提出了首个在标准差分隐私约束下的双层优化算法，创新地避免了计算Hessian矩阵。\",\n  \"motivation\": \"双层优化在机器学习应用中日益受到关注，如超参数调整，但现有方法在数据隐私保护方面存在不足。差分隐私是保护敏感数据的核心技术，然而标准算法难以直接应用于双层优化，且传统方法在大规模设置中Hessian计算成本高昂，限制了实际应用。因此，开发高效且隐私保护的算法至关重要，以解决隐私与性能之间的平衡问题，促进安全机器学习的发展。\",\n  \"method\": \"论文提出一种基于梯度的(ε,δ)-差分隐私算法，专门用于双层优化问题。该方法创新地避免了Hessian计算，通过梯度更新机制集成隐私保护，降低了计算复杂度。算法适用于约束和非约束问题，支持小批量梯度处理，并能处理经验损失和总体损失。关键技术特色包括隐私噪声注入和梯度正则化，确保在保护数据隐私的同时，保持优化过程的效率和稳定性。\",\n  \"result\": \"在上下层分别为非凸和强凸的常见设置下，算法返回点的超梯度范数上界为 $\\widetilde{\\mathcal{O}}\\left((\\sqrt{d_\\mathrm{up}}/εn)^{1/2}+(\\sqrt{d_\\mathrm{low}}/εn)^{1/3}\\right)$，其中n是数据集大小，d_up/d_low是维度。这提供了理论性能保证，表明算法在隐私保护下的收敛性。作为首个此类算法，它避免了Hessian计算，相比传统方法在大规模设置中更高效，但摘要未明确提供具体实验数据如准确率，仅通过数学界限展示有效性。\",\n  \"conclusion\": \"本研究首次将差分隐私技术应用于双层优化，贡献了高效且隐私保护的算法，避免了Hessian计算并提供了全面理论分析。学术上，推动了隐私保护优化算法的发展；实际上，可应用于超参数调整等机器学习任务，提升数据安全。未来工作可扩展算法到更复杂设置或结合其他隐私机制，摘要未明确说明局限性，但可能包括在大规模场景下的进一步优化需求。\",\n  \"tags\": [\"Differential Privacy\", \"Bilevel Optimization\", \"Gradient-based Methods\", \"Hyperparameter Tuning\", \"Hypergradient\"]\n}\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
  },
  {
    "id": "2407.20224",
    "title": "Can Editing LLMs Inject Harm?",
    "authors": [
      "Canyu Chen",
      "Baixiang Huang",
      "Zekun Li",
      "Zhaorun Chen",
      "Shiyang Lai",
      "Xiongxiao Xu",
      "Jia-Chen Gu",
      "Jindong Gu",
      "Huaxiu Yao",
      "Chaowei Xiao",
      "Xifeng Yan",
      "William Yang Wang",
      "Philip Torr",
      "Dawn Song",
      "Kai Shu"
    ],
    "abstract": "Large Language Models (LLMs) have emerged as a new information channel. Meanwhile, one critical but under-explored question is: Is it possible to bypass the safety alignment and inject harmful information into LLMs stealthily? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Specifically, we focus on two typical safety risks of Editing Attack including Misinformation Injection and Bias Injection. For the first risk, we find that editing attacks can inject both commonsense and long-tail misinformation into LLMs, and the effectiveness for the former one is particularly high. For the second risk, we discover that not only can biased sentences be injected into LLMs with high effectiveness, but also one single biased sentence injection can degrade the overall fairness. Then, we further illustrate the high stealthiness of editing attacks. Our discoveries demonstrate the emerging misuse risks of knowledge editing techniques on compromising the safety alignment of LLMs and the feasibility of disseminating misinformation or bias with LLMs as new channels.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2407.20224.pdf",
    "abs_url": "https://arxiv.org/abs/2407.20224",
    "published": "2024-07-29T17:58:06Z",
    "updated": "2026-01-14T05:04:44Z",
    "comment": "Accepted to Proceedings of AAAI 2026. The first two authors contributed equally. 7 pages for main paper, 31 pages including appendix. The code, results, dataset for this paper and more resources are on the project website: https://llm-editing.github.io",
    "light_analysis": {
      "overview": "论文提出将知识编辑重新定义为大型语言模型的新型安全威胁——Editing Attack，并系统性调查其对安全对齐的危害，揭示知识编辑技术的滥用风险。",
      "motivation": "大型语言模型作为新信息渠道，其安全对齐机制可能被绕过以植入有害信息，这一问题对确保模型可靠性至关重要。现有研究未充分探索此类潜在威胁，特别是如何隐蔽地注入虚假信息或偏见，从而可能导致LLMs在实际应用中传播有害内容，影响社会公正和信息安全。",
      "method": "论文的核心方法是重新定义知识编辑为Editing Attack，并利用新构建的EditAttack数据集进行系统性调查。聚焦于两个典型安全风险：虚假信息注入和偏见注入。关键创新在于将知识编辑技术视为一种隐蔽的攻击手段，通过这种方法评估LLMs的安全脆弱性，但摘要未明确说明具体模型架构或编辑技术的细节。",
      "result": "实验结果发现编辑攻击能有效注入常识性和长尾虚假信息，常识性虚假信息的效果尤为显著。偏见注入不仅成功率较高，单个偏见句子甚至可能降低模型的整体公平性。此外，编辑攻击展现出高隐蔽性，暗示其在真实场景中难以检测。摘要未提供具体性能指标，但强调了与基线方法相比的攻击有效性。",
      "conclusion": "研究的主要贡献在于揭示了知识编辑技术在破坏LLMs安全对齐方面的滥用风险，证实了利用LLMs作为新渠道传播虚假信息或偏见的可行性。学术价值在于提出了Editing Attack这一新安全威胁概念，实践意义在于警示研究者需加强防御策略。未来工作可探索针对此类攻击的防御机制或更广泛的危害评估。",
      "tags": [
        "Knowledge Editing",
        "Safety Alignment",
        "Misinformation Injection",
        "Bias Injection",
        "LLM Security"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:35.192089Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2406.14191",
    "title": "Temporal Knowledge Graph Question Answering: A Survey",
    "authors": [
      "Miao Su",
      "Zixuan Li",
      "Zhuo Chen",
      "Long Bai",
      "Xiaolong Jin",
      "Jiafeng Guo"
    ],
    "abstract": "Knowledge Base Question Answering (KBQA) has been a long-standing field to answer questions based on knowledge bases. Recently, the evolving dynamics of knowledge have attracted a growing interest in Temporal Knowledge Graph Question Answering (TKGQA), an emerging task to answer temporal questions. However, this field grapples with ambiguities in defining temporal questions and lacks a systematic categorization of existing methods for TKGQA. In response, this paper provides a thorough survey from two perspectives: the taxonomy of temporal questions and the methodological categorization for TKGQA. Specifically, we first establish a detailed taxonomy of temporal questions engaged in prior studies. Subsequently, we provide a comprehensive review of TKGQA techniques of two categories: semantic parsing-based and TKG embedding-based. Building on this review, the paper outlines potential research directions aimed at advancing the field of TKGQA. This work aims to serve as a comprehensive reference for TKGQA and to stimulate further research.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2406.14191.pdf",
    "abs_url": "https://arxiv.org/abs/2406.14191",
    "published": "2024-06-20T10:51:06Z",
    "updated": "2026-01-14T06:54:40Z",
    "comment": "8 pages, 3 figures. This work has been submitted to the IEEE for possible publication",
    "light_analysis": {
      "overview": "本文为时间知识图谱问答提供了一项全面综述，从时间问题分类和方法分类两个视角进行了系统性梳理。",
      "motivation": "研究动机源于时间知识图谱问答作为一个新兴领域，面临着时间问题定义的模糊性和现有方法缺乏系统分类的问题。这些问题阻碍了该领域的深入研究和实际应用，现有文献中缺乏统一框架，导致方法比较和技术评估困难，因此本文旨在提供全面综述以填补这一空白，促进该领域的规范化和进展。",
      "method": "研究方法通过建立时间问题的详细分类来整理先前研究，并全面回顾了两类TKGQA技术：基于语义解析的方法和基于TKG嵌入的方法。分类框架覆盖了时间问题的多种类型，技术回顾则分析了每种方法的原理和实现细节，如使用知识图谱嵌入来建模时间依赖，从而构建系统性方法论综述，但摘要未明确提及具体数据集或模型架构。",
      "result": "主要结果包括提出了时间问题的分类框架，以及对TKGQA技术的系统回顾，但摘要未明确说明具体性能指标如准确率提升或效率改进。本文基于现有文献总结了技术进展，并与基线方法进行比较，指出了潜在研究方向，从而为未来研究提供了参考基础，但缺乏具体数据支撑。",
      "conclusion": "结论是本文提供了一个关于时间知识图谱问答的全面参考，总结了时间问题分类和方法回顾的主要贡献，强调了其在促进学术研究和实际应用中的价值。意义在于帮助研究人员理解现有技术并指导未来探索，局限性可能在于综述性质导致缺乏实验数据，未来工作方向包括完善分类框架和开发更高效的方法。",
      "tags": [
        "Temporal Knowledge Graph Question Answering",
        "Knowledge Base Question Answering",
        "Semantic Parsing",
        "TKG Embedding",
        "Survey"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:36.097758Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2405.02325",
    "title": "Are Biological Systems More Intelligent Than Artificial Intelligence?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "abstract": "Are biological self-organising systems more ``intelligent'' than artificial intelligence (AI)? If so, why? I address this question using a mathematical framework that defines intelligence in terms of adaptability. Systems are modelled as stacks of abstraction layers (\\emph{Stack Theory}) and compared by how effectively they delegate agentic control down their stacks. I illustrate this using computational, biological, military, governmental, and economic systems. Contemporary AI typically relies on static, human-engineered stacks whose lower layers are fixed during deployment. Put provocatively, such systems resemble inflexible bureaucracies that adapt only top-down. Biological systems are more intelligent because they delegate adaptation. Formally, I prove a theorem (\\emph{The Law of the Stack}) showing that adaptability at higher layers is bottlenecked by adaptability at lower layers. I further show that, under standard viability assumptions, maximising adaptability is equivalent to minimising variational free energy, implying that delegation is necessary for free-energy minimisation. Generalising bioelectric accounts of cancer as isolation from collective informational structures, I analyse cancer-like failure modes in non-biological systems when delegation is inadequate. This yields design principles for building robust systems via delegated control, and reframes hybrid agents (e.g. organoids or human--AI systems) as weak boundary-condition design problems in which constraints shape low-level policy spaces while preserving collective identity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2405.02325.pdf",
    "abs_url": "https://arxiv.org/abs/2405.02325",
    "published": "2024-04-23T00:13:14Z",
    "updated": "2026-01-14T05:22:12Z",
    "comment": "Definitions shared with arXiv:2404.07227, arXiv:2302.00843",
    "light_analysis": {
      "overview": "论文提出Stack Theory框架，证明委托适应性是智能系统的关键，并应用于分析和设计鲁棒系统。",
      "motivation": "研究动机是探讨生物系统是否比人工智能更智能，以解决当前AI系统在适应性上的不足。当代AI通常依赖静态、人工设计的抽象层堆栈，部署后下层固定，导致适应性受限，类似不灵活的官僚机构。这个问题重要，因为理解智能的本质可以指导设计更自适应和鲁棒的AI系统，适用于计算、生物、军事、政府和经济学领域，提升系统的整体效能和应对动态环境的能力。",
      "method": "研究方法基于Stack Theory，将系统建模为抽象层堆栈，定义智能为适应性。核心创新是引入委托控制概念，比较系统如何有效将代理控制向下层委托。证明The Law of the Stack定理，表明较高层适应性受较低层瓶颈限制。在标准可行性假设下，展示最大化适应性等价于最小化变分自由能，暗示委托对自由能最小化必要。分析涵盖计算、生物、军事、政府和经济的系统，并推广生物电癌症模型到非生物系统，识别委托不足时的失败模式。",
      "result": "主要结果是理论性的：证明了The Law of the Stack定理，揭示适应性在各层间的依赖关系；展示了适应性最大化与变分自由能最小化的等价性，为委托控制提供数学基础。通过分析不同系统，识别了当委托不当时类似癌症的失败模式，如信息隔离导致系统崩溃。与静态AI堆栈基线相比，委托控制能提升系统鲁棒性和适应性，但摘要未明确说明具体性能指标数据如准确率提升，仅提供理论分析和设计原则支持。",
      "conclusion": "论文的主要贡献是提出Stack Theory框架，重新定义智能基于适应性，并证明委托控制是智能系统核心。学术价值在于提供新数学工具比较和分析不同系统智能；实际应用价值包括指导构建更鲁棒的AI和混合代理系统（如人-AI系统），通过设计弱边界条件优化低层策略空间并保持集体身份。潜在局限性可能是理论框架的实证验证不足，未来工作可包括实验验证和扩展到更多领域应用。",
      "tags": [
        "Stack Theory",
        "adaptability",
        "delegated control",
        "variational free energy",
        "hybrid agents"
      ]
    },
    "analyzed_at": "2026-01-15T03:35:56.054948Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2404.10370",
    "title": "Know Yourself Better: Diverse Object-Related Features Improve Open Set Recognition",
    "authors": [
      "Jiawen Xu",
      "Margret Keuper"
    ],
    "abstract": "Open set recognition (OSR) is a critical aspect of machine learning, addressing the challenge of detecting novel classes during inference. Within the realm of deep learning, neural classifiers trained on a closed set of data typically struggle to identify novel classes, leading to erroneous predictions. To address this issue, various heuristic methods have been proposed, allowing models to express uncertainty by stating \"I don't know.\" However, a gap in the literature remains, as there has been limited exploration of the underlying mechanisms of these methods. In this paper, we conduct an analysis of open set recognition methods, focusing on the aspect of feature diversity. Our research reveals a significant correlation between learning diverse discriminative features and enhancing OSR performance. Building on this insight, we propose a novel OSR approach that leverages the advantages of feature diversity. The efficacy of our method is substantiated through rigorous evaluation on a standard OSR testbench, demonstrating a substantial improvement over state-of-the-art methods.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2404.10370.pdf",
    "abs_url": "https://arxiv.org/abs/2404.10370",
    "published": "2024-04-16T08:08:47Z",
    "updated": "2026-01-14T13:21:46Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于特征多样性的开放集识别方法，通过促进模型学习多样化判别特征，显著提升了在新类别检测上的性能。",
      "motivation": "开放集识别（OSR）是机器学习中的关键问题，旨在使模型在推理时有效检测未知类别，避免错误分类。现有深度学习分类器通常在闭集数据上训练，面对新类别时表现不佳，导致误判。虽然已有启发式方法允许模型表达不确定性，但对其底层机制，特别是特征多样性的影响，缺乏深入探索。因此，本研究动机在于填补这一空白，分析特征多样性如何优化OSR，解决现有方法在机制理解上的不足。",
      "method": "本研究首先系统分析了开放集识别方法，重点探讨特征多样性的作用，揭示了学习多样化判别特征与OSR性能的显著相关性。基于这一发现，作者提出一种新方法，通过利用特征多样性的优势来增强模型能力。该方法旨在促进模型学习更具多样性和判别性的特征表示，以提高对新类别的识别鲁棒性。摘要未明确说明具体技术细节，如使用的模型架构或数据集，但强调以特征多样性为核心创新点。",
      "result": "通过在标准开放集识别测试基准上的严格评估，该方法展示了相对于最先进方法的显著改进。摘要未提供具体性能指标，如准确率或效率提升的数值，但明确指出改进具有实质性，表明在检测新类别方面优于现有基线方法。这一结果验证了特征多样性策略的有效性，为OSR性能优化提供了实证支持。",
      "conclusion": "本研究强调了特征多样性对开放集识别的重要性，并提出了一种创新方法，深化了对OSR机制的理解。其学术价值在于为特征学习提供了新视角，实际应用价值在于提升模型的未知类别检测能力，增强机器学习系统的鲁棒性。未来工作可进一步探索特征多样性的具体实现机制，以及在更广泛场景中的应用潜力和局限性。",
      "tags": [
        "Open Set Recognition",
        "Feature Diversity",
        "Discriminative Features",
        "OSR Methods",
        "Machine Learning Evaluation"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:20.849005Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2404.03471",
    "title": "Template-Based Probes Are Imperfect Lenses for Counterfactual Bias Evaluation in LLMs",
    "authors": [
      "Farnaz Kohankhaki",
      "D. B. Emerson",
      "Jacob-Junqi Tian",
      "Laleh Seyyed-Kalantari",
      "Faiza Khan Khattak"
    ],
    "abstract": "Bias in large language models (LLMs) has many forms, from overt discrimination to implicit stereotypes. Counterfactual bias evaluation is a widely used approach to quantifying bias and often relies on template-based probes that explicitly state group membership. It aims to measure whether the outcome of a task performed by an LLM is invariant to a change in group membership. In this work, we find that template-based probes can introduce systematic distortions in bias measurements. Specifically, we consistently find that such probes suggest that LLMs classify text associated with White race as negative at disproportionately elevated rates. This is observed consistently across a large collection of LLMs, over several diverse template-based probes, and with different classification approaches. We hypothesize that this arises artificially due to linguistic asymmetries present in LLM pretraining data, in the form of markedness, (e.g., Black president vs. president) and templates used for bias measurement (e.g., Black president vs. White president). These findings highlight the need for more rigorous methodologies in counterfactual bias evaluation, ensuring that observed disparities reflect genuine biases rather than artifacts of linguistic conventions.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2404.03471.pdf",
    "abs_url": "https://arxiv.org/abs/2404.03471",
    "published": "2024-04-04T14:24:06Z",
    "updated": "2026-01-14T18:20:19Z",
    "comment": "22 Pages, 6 Figures, 5 Tables",
    "light_analysis": {
      "overview": "本文发现基于模板的探针在大型语言模型反事实偏见评估中可能引入系统性扭曲，从而误导偏见测量结果。",
      "motivation": "大型语言模型（LLMs）中的偏见评估对确保其公平应用至关重要，反事实偏见评估作为常用方法，依赖于基于模板的探针来量化模型对不同群体成员的反应差异。然而，现有方法可能存在不足，因为这些探针可能因预训练数据中的语言不对称性（如标记性）而引入人为扭曲，导致评估结果不一定反映真实偏见，而是语言约定的产物，从而强调了改进偏见评估方法的必要性。",
      "method": "本研究采用实验方法，通过收集多个LLMs的数据，使用多种基于模板的探针进行反事实偏见评估。核心方法是系统比较不同探针在测量偏见时的效果，重点关注探针模板中的语言对称性问题，创新点在于提出并验证了探针可能因语言不对称性（如标记性）而引入系统性扭曲的假设。实验涉及多个LLMs模型、多样化的模板设计和不同的分类方法，以全面探究测量误差的来源。",
      "result": "实验结果显示，基于模板的探针一致表明LLMs倾向于将白人相关文本分类为负面情绪，这一现象在多种LLMs模型、不同探针设计和分类方法中均被观察到。与真实偏见预期相比，这些结果可能人为夸大了对白人文本的负面偏见，表明探针本身的语言结构可能引入了系统性测量误差，从而扭曲了偏见评估的准确性。",
      "conclusion": "本研究的结论是，基于模板的探针在反事实偏见评估中作为不完善的工具，可能引入人为扭曲的测量结果。其贡献在于揭示了语言不对称性（如标记性）对偏见评估的潜在影响，强调了开发更严谨评估方法的重要性，这为改进LLMs偏见测量技术提供了学术见解，并促使研究者关注评估工具的自身偏差，未来工作可致力于设计更可靠的评估框架以避免语言约定干扰。",
      "tags": [
        "Large Language Model",
        "Counterfactual Bias Evaluation",
        "Template-Based Probe",
        "Markedness"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:34.258217Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2402.03646",
    "title": "Lens: A Knowledge-Guided Foundation Model for Network Traffic",
    "authors": [
      "Xiaochang Li",
      "Chen Qian",
      "Qineng Wang",
      "Jiangtao Kong",
      "Yuchen Wang",
      "Ziyu Yao",
      "Bo Ji",
      "Long Cheng",
      "Gang Zhou",
      "Huajie Shao"
    ],
    "abstract": "Network traffic refers to the amount of data being sent and received over the Internet or any system that connects computers. Analyzing network traffic is vital for security and management, yet remains challenging due to the heterogeneity of plain-text packet headers and encrypted payloads. To capture the latent semantics of traffic, recent studies have adopted Transformer-based pretraining techniques to learn network representations from massive traffic data. However, these methods pre-train on data-driven tasks but overlook network knowledge, such as masking partial digits of the indivisible network port numbers for prediction, thereby limiting semantic understanding. In addition, they struggle to extend classification to new classes during fine-tuning due to the distribution shift. Motivated by these limitations, we propose \\Lens, a unified knowledge-guided foundation model for both network traffic classification and generation. In pretraining, we propose a Knowledge-Guided Mask Span Prediction method with textual context for learning knowledge-enriched representations. For extending to new classes in finetuning, we reframe the traffic classification as a closed-ended generation task and introduce context-aware finetuning to adapt to the distribution shift. Evaluation results across various benchmark datasets demonstrate that the proposed Lens~achieves superior performance on both classification and generation tasks. For traffic classification, Lens~outperforms competitive baselines substantially on 8 out of 12 tasks with an average accuracy of \\textbf{96.33\\%} and extends to novel classes with significantly better performance. For traffic generation, Lens~generates better high-fidelity network traffic for network simulation, gaining up to \\textbf{30.46\\%} and \\textbf{33.3\\%} better accuracy and F1 in fuzzing tests. We will open-source the code upon publication.",
    "categories": [
      "cs.LG",
      "cs.NI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2402.03646.pdf",
    "abs_url": "https://arxiv.org/abs/2402.03646",
    "published": "2024-02-06T02:45:13Z",
    "updated": "2026-01-14T14:09:25Z",
    "comment": null,
    "light_analysis": {
      "overview": "Lens是一个知识引导的基础模型，通过结合网络知识和上下文感知微调，提升网络流量分类和生成的语义理解能力。",
      "motivation": "网络流量分析对网络安全和管理至关重要，但由于明文包头和加密负载的异构性，现有方法面临挑战。近期研究采用Transformer预训练技术，但忽略了网络知识（如端口号的结构），导致语义理解有限；同时在微调时因分布偏移难以扩展到新类，限制了分类的灵活性和准确性，因此需要知识增强的方法来解决这些不足。",
      "method": "Lens模型在预训练阶段提出知识引导的掩码跨度预测方法，结合文本上下文学习知识丰富表示，增强对网络语义的捕获。在微调阶段，将流量分类重构为封闭生成任务，并引入上下文感知微调来适应分布偏移，统一处理分类和生成任务，关键创新点在于整合网络知识和生成式适应策略。",
      "result": "实验结果显示，在多个基准数据集上，Lens在流量分类任务中平均准确率达96.33%，在8/12任务中优于基线，扩展到新类时性能显著提升；在流量生成任务中，生成高保真网络流量用于仿真，模糊测试中准确率和F1分别提升30.46%和33.3%，证明模型在分类和生成方面均具有优越效果。",
      "conclusion": "Lens通过知识引导预训练和上下文感知微调，显著提升了网络流量分类和生成的性能，贡献在于统一的知识增强基础模型，具有学术价值和实际应用意义，如网络安全仿真和管理，未来工作可能包括进一步优化模型和开源代码以促进研究。",
      "tags": [
        "Transformer",
        "Knowledge-Guided Learning",
        "Masked Language Modeling",
        "Fine-tuning",
        "Network Traffic Generation"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:06.387176Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2312.16424",
    "title": "Soft Contrastive Learning for Time Series",
    "authors": [
      "Seunghan Lee",
      "Taeyoung Park",
      "Kibok Lee"
    ],
    "abstract": "Contrastive learning has shown to be effective to learn representations from time series in a self-supervised way. However, contrasting similar time series instances or values from adjacent timestamps within a time series leads to ignore their inherent correlations, which results in deteriorating the quality of learned representations. To address this issue, we propose SoftCLT, a simple yet effective soft contrastive learning strategy for time series. This is achieved by introducing instance-wise and temporal contrastive loss with soft assignments ranging from zero to one. Specifically, we define soft assignments for 1) instance-wise contrastive loss by the distance between time series on the data space, and 2) temporal contrastive loss by the difference of timestamps. SoftCLT is a plug-and-play method for time series contrastive learning that improves the quality of learned representations without bells and whistles. In experiments, we demonstrate that SoftCLT consistently improves the performance in various downstream tasks including classification, semi-supervised learning, transfer learning, and anomaly detection, showing state-of-the-art performance. Code is available at this repository: https://github.com/seunghan96/softclt.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2312.16424.pdf",
    "abs_url": "https://arxiv.org/abs/2312.16424",
    "published": "2023-12-27T06:15:00Z",
    "updated": "2026-01-14T06:23:37Z",
    "comment": "ICLR 2024 Spotlight",
    "light_analysis": {
      "overview": "论文提出SoftCLT方法，通过引入软分配的对比损失来改进时间序列的自监督表示学习，有效提升表示质量。",
      "motivation": "对比学习在时间序列表示学习中已显示潜力，但现有方法在对比相似实例或相邻时间戳时，忽略了它们之间的内在相关性，导致学习到的表示质量下降。这限制了自监督学习在时间序列任务中的应用效果，因此需要一种能更精细地捕捉相关性的对比策略来解决这一问题。",
      "method": "SoftCLT是一种软对比学习策略，核心创新在于引入实例级和时间对比损失，并使用从零到一的软分配。具体地，实例级对比损失的软分配基于时间序列在数据空间中的距离，而时间对比损失的软分配基于时间戳的差异。该方法为即插即用设计，无需额外复杂设置即可直接应用于现有时间序列对比学习框架，提升表示学习的有效性。",
      "result": "实验显示，SoftCLT在多种下游任务中持续改进性能，包括分类、半监督学习、迁移学习和异常检测，达到了最先进水平。尽管摘要未提供具体数值，但与基线方法相比，该方法在表示质量上表现出显著提升，增强了任务准确性和泛化能力，证明了其在实际应用中的有效性。",
      "conclusion": "论文的主要贡献是提出了SoftCLT，一种创新的软对比学习策略，解决了时间序列表示学习中忽略相关性的问题，提高了表示质量。学术上推动了对比学习在时间序列领域的进展，实际应用价值体现在多种下游任务的性能提升。未来工作可探索更复杂的软分配机制或扩展到其他数据模态，以进一步优化表示学习效果。",
      "tags": [
        "Contrastive Learning",
        "Time Series",
        "Self-Supervised Learning",
        "Soft Assignments",
        "Representation Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:23.051057Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2310.17218",
    "title": "Prototypical Contrastive Learning-based CLIP Fine-tuning for Object Re-identification",
    "authors": [
      "Jiachen Li",
      "Xiaojin Gong"
    ],
    "abstract": "This work aims to adapt large-scale pre-trained vision-language models, such as contrastive language-image pretraining (CLIP), to enhance the performance of object reidentification (Re-ID) across various supervision settings. Although prompt learning has enabled a recent work named CLIP-ReID to achieve promising performance, the underlying mechanisms and the necessity of prompt learning remain unclear due to the absence of semantic labels in ReID tasks. In this work, we first analyze the role prompt learning in CLIP-ReID and identify its limitations. Based on our investigations, we propose a simple yet effective approach to adapt CLIP for supervised object Re-ID. Our approach directly fine-tunes the image encoder of CLIP using a prototypical contrastive learning (PCL) loss, eliminating the need for prompt learning. Experimental results on both person and vehicle Re-ID datasets demonstrate the competitiveness of our method compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP fine-tuning approach to unsupervised scenarios, where we achieve state-of-the art performance. Code is available at https://github.com/RikoLi/PCL-CLIP.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2310.17218.pdf",
    "abs_url": "https://arxiv.org/abs/2310.17218",
    "published": "2023-10-26T08:12:53Z",
    "updated": "2026-01-14T09:17:39Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出一种基于原型对比学习微调CLIP模型的方法，用于对象重新识别任务，无需依赖提示学习。",
      "motivation": "研究旨在解决将大规模预训练视觉-语言模型CLIP有效应用于对象重新识别（Re-ID）任务的问题。尽管CLIP-ReID通过提示学习取得了良好性能，但由于Re-ID任务通常缺乏语义标签，提示学习的机制和必要性不明确，限制了模型优化。这凸显了开发更直接、有效微调方法的重要性，以提升Re-ID性能并简化训练过程。",
      "method": "论文提出基于原型对比学习（PCL）的CLIP微调方法。该方法直接微调CLIP的图像编码器，使用PCL损失函数，无需提示学习。在监督场景下，通过PCL损失优化模型；并扩展到无监督场景，利用无标签数据进行训练。关键创新在于简化架构，减少计算负担，同时保持模型适应性。",
      "result": "实验在人和车辆Re-ID数据集上进行，结果显示，该方法在监督场景下与CLIP-ReID相比具有竞争力。在无监督场景下，实现了最先进的性能。尽管摘要未明确说明具体性能指标，但通过与基线方法对比，展示了方法的有效性和优越性。",
      "conclusion": "本研究的主要贡献是提出基于原型对比学习的CLIP微调方法，用于对象重新识别，简化了训练流程并提升性能。学术上为模型适应提供了新思路，应用上有助于增强实际系统的Re-ID能力。未来工作可能包括进一步优化损失函数或扩展到更多监督设置，但摘要未明确说明局限性。",
      "tags": [
        "Contrastive Learning",
        "CLIP",
        "Object Re-identification",
        "Prototypical Contrastive Learning",
        "Unsupervised Learning"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:38.651526Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2309.06135",
    "title": "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts",
    "authors": [
      "Zhi-Yi Chin",
      "Chieh-Ming Jiang",
      "Ching-Chun Huang",
      "Pin-Yu Chen",
      "Wei-Chen Chiu"
    ],
    "abstract": "Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown remarkable ability in high-quality content generation, and become one of the representatives for the recent wave of transformative AI. Nevertheless, such advance comes with an intensifying concern about the misuse of this generative technology, especially for producing copyrighted or NSFW (i.e. not safe for work) images. Although efforts have been made to filter inappropriate images/prompts or remove undesirable concepts/styles via model fine-tuning, the reliability of these safety mechanisms against diversified problematic prompts remains largely unexplored. In this work, we propose Prompting4Debugging (P4D) as a debugging and red-teaming tool that automatically finds problematic prompts for diffusion models to test the reliability of a deployed safety mechanism. We demonstrate the efficacy of our P4D tool in uncovering new vulnerabilities of SD models with safety mechanisms. Particularly, our result shows that around half of prompts in existing safe prompting benchmarks which were originally considered \"safe\" can actually be manipulated to bypass many deployed safety mechanisms, including concept removal, negative prompt, and safety guidance. Our findings suggest that, without comprehensive testing, the evaluations on limited safe prompting benchmarks can lead to a false sense of safety for text-to-image models.",
    "categories": [
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2309.06135.pdf",
    "abs_url": "https://arxiv.org/abs/2309.06135",
    "published": "2023-09-12T11:19:36Z",
    "updated": "2026-01-14T13:34:07Z",
    "comment": "ICML 2024 main conference paper. The source code is available at https://github.com/zhiyichin/P4D",
    "light_analysis": {
      "overview": "本文提出Prompting4Debugging（P4D）工具，用于自动发现文本到图像扩散模型安全机制中的漏洞，以红队测试方法评估其可靠性。",
      "motivation": "文本到图像扩散模型（如Stable Diffusion）近年来展现出高质量内容生成的卓越能力，成为AI转型的代表技术。然而，这种进步也加剧了对技术滥用的担忧，特别是生成受版权保护或不安全（NSFW）图像的风险。尽管已有方法通过过滤提示或模型微调来提升安全性，但这些安全机制在面对多样化问题提示时的鲁棒性尚未被充分研究。因此，开发自动化工具来测试安全机制的弱点，成为确保模型实际部署中有效性的关键需求。",
      "method": "论文提出Prompting4Debugging（P4D）作为调试和红队工具，旨在自动查找问题提示以测试文本到图像扩散模型的安全机制。该方法的核心创新点在于开发了自动化算法，能够系统地识别能绕过安全机制的提示。摘要未明确说明具体的技术细节，如使用的数据集或模型架构，但推测可能涉及提示的优化或对抗性生成技术，以评估安全机制的漏洞。这为测试模型安全性提供了一种新的技术路线，侧重于自动化漏洞发现。",
      "result": "通过P4D工具对Stable Diffusion模型及其安全机制进行测试，结果显示，在现有安全提示基准中，约一半原本被认为是“安全”的提示可以被操纵以绕过多种部署的安全机制。这些机制包括概念移除、负提示和安全指导。这一发现揭示了安全机制在面对特定问题提示时的脆弱性，并表明当前评估方法存在局限性。具体性能指标未详细说明，但摘要强调了漏洞的普遍存在，突显了全面测试的重要性。",
      "conclusion": "本文的主要贡献是开发了Prompting4Debugging（P4D）工具，用于全面测试文本到图像扩散模型的安全机制。研究发现，仅依赖有限的基准测试可能导致虚假的安全感，这突显了红队测试在确保模型安全性中的学术和实际价值。研究强调了改进安全机制和开发更鲁棒评估方法的必要性。潜在局限性包括可能未覆盖所有提示类型或模型变体，未来工作可扩展到其他生成模型和更广泛的安全场景。",
      "tags": [
        "Text-to-Image Diffusion Models",
        "Red-Teaming",
        "Safety Mechanisms",
        "Prompt Engineering",
        "Debugging"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:41.480409Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2308.14555",
    "title": "Kernel Limit for a Class of Recurrent Neural Networks Trained on Ergodic Data Sequences",
    "authors": [
      "Samuel Chun-Hei Lam",
      "Justin Sirignano",
      "Konstantinos Spiliopoulos"
    ],
    "abstract": "Mathematical methods are developed to characterize the asymptotics of recurrent neural networks (RNN) as the number of hidden units, data samples in the sequence, hidden state updates, and training steps simultaneously grow to infinity. In the case of an RNN with a simplified weight matrix, we prove the convergence of the RNN to the solution of an infinite-dimensional ODE coupled with the fixed point of a random algebraic equation. The analysis requires addressing several challenges which are unique to RNNs. In typical mean-field applications (e.g., feedforward neural networks), discrete updates are of magnitude $\\mathcal{O}(1/N)$ and the number of updates is $\\mathcal{O}(N)$. Therefore, the system can be represented as an Euler approximation of an appropriate ODE/PDE, which it will converge to as $N \\rightarrow \\infty$. However, the RNN hidden layer updates are $\\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of an ODE/PDE and standard mean-field techniques cannot be applied. Instead, we develop a fixed point analysis for the evolution of the RNN memory states, with convergence estimates in terms of the number of update steps and the number of hidden units. The RNN hidden layer is studied as a function in a Sobolev space, whose evolution is governed by the data sequence (a Markov chain), the parameter updates, and its dependence on the RNN hidden layer at the previous time step. Due to the strong correlation between updates, a Poisson equation must be used to bound the fluctuations of the RNN around its limit equation. These mathematical methods give rise to the neural tangent kernel (NTK) limits for RNNs trained on data sequences as the number of data samples and size of the neural network grow to infinity.",
    "categories": [
      "cs.LG",
      "math.PR",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2308.14555.pdf",
    "abs_url": "https://arxiv.org/abs/2308.14555",
    "published": "2023-08-28T13:17:39Z",
    "updated": "2026-01-14T14:47:00Z",
    "comment": "Revision in response to reviewers' comments. The mean-field random function has been replaced by a mean-field term. Some typos fixed. Minor title change",
    "light_analysis": {
      "overview": "本文开发数学方法分析循环神经网络在多个参数趋于无穷时的渐近行为，证明了其收敛到无限维ODE与随机代数方程的解。",
      "motivation": "研究旨在解决循环神经网络（RNN）在训练过程中渐近行为的理论分析问题。由于RNN隐藏层更新步长为O(1)，不同于前馈网络O(1/N)的更新，这使得标准平均场技术无法直接应用。该问题的重要性在于，理解RNN的极限行为有助于优化其训练过程，而现有方法的不足限制了理论发展，尤其在处理长期序列数据和大型网络时。",
      "method": "研究方法采用固定点分析来处理RNN隐藏状态的演化。核心创新包括将RNN隐藏层建模为Sobolev空间中的函数，其演化受数据序列（马尔可夫链）、参数更新和前一时间步依赖的影响。为解决更新间的强相关性，论文引入泊松方程来限制RNN波动，从而推导出神经切线核（NTK）的极限。关键细节涉及分析隐藏单元数、数据样本数和训练步数同时趋于无穷的联合渐近。",
      "result": "主要实验结果是理论证明：RNN在简化权重矩阵下，收敛到无限维ODE与随机代数方程固定点的解。论文给出了收敛估计，涉及更新步数和隐藏单元数的关系，但摘要未明确具体数据如准确率提升。与基线方法的对比方面，研究解决了传统平均场技术不适用的问题，为RNN的理论分析提供了新框架。",
      "conclusion": "论文的主要贡献是开发了一套数学方法来刻画RNN的渐近行为，扩展了神经网络的均值场理论到循环网络。学术价值在于为RNN的神经切线核理论奠定基础，有助于深入理解训练动态。实际应用价值可能包括优化RNN设计和加速训练过程。未来工作方向可包括进一步扩展到更复杂网络结构或实际数据集的分析。",
      "tags": [
        "Recurrent Neural Networks",
        "Neural Tangent Kernel",
        "Mean-field Theory",
        "Sobolev Spaces",
        "Fixed Point Analysis"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:36.767166Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2308.08794",
    "title": "Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces",
    "authors": [
      "Miguel Liu-Schiaffini",
      "Clare E. Singer",
      "Nikola Kovachki",
      "Sze Chai Leung",
      "Hyunji Jane Bae",
      "Kamyar Azizzadenesheli",
      "Anima Anandkumar"
    ],
    "abstract": "Tipping points are abrupt, drastic, and often irreversible changes in the evolution of non-stationary and chaotic dynamical systems. For instance, increased greenhouse gas concentrations are predicted to lead to drastic decreases in low cloud cover, referred to as a climatological tipping point. In this paper, we learn the evolution of such non-stationary dynamical systems using a novel recurrent neural operator (RNO), which learns mappings between function spaces. After training RNO on only the pre-tipping dynamics, we employ it to detect future tipping points using an uncertainty-based approach. In particular, we propose a conformal prediction framework to forecast tipping points by monitoring deviations from physics constraints (such as conserved quantities and partial differential equations), enabling forecasting of these abrupt changes along with a rigorous measure of uncertainty. We illustrate our proposed methodology on non-stationary ordinary and partial differential equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also apply our methods to forecast a climate tipping point in stratocumulus cloud cover and airfoil wake and stall transitions using only limited knowledge of the governing equations. For the latter, we show that our proposed method zero-shot generalizes to forecasting multiple future tipping points under varying Reynolds numbers. In our experiments, we demonstrate that even partial or approximate physics constraints can be used to accurately forecast future tipping points.",
    "categories": [
      "cs.LG",
      "math.DS"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2308.08794.pdf",
    "abs_url": "https://arxiv.org/abs/2308.08794",
    "published": "2023-08-17T05:42:27Z",
    "updated": "2026-01-14T06:54:26Z",
    "comment": null,
    "light_analysis": {
      "overview": "本文提出了一种基于递归神经算子和 conformal prediction 框架的倾点预测方法，用于非平稳动力系统，并提供严格的不确定性度量。",
      "motivation": "倾点是动力系统中的突变变化，在气候科学（如云盖剧变）和工程领域（如机翼失速）中具有重要意义。现有方法往往依赖完整的物理知识或难以量化不确定性，而实际系统中控制方程信息有限，导致预测困难。因此，本研究旨在开发一种新方法来准确预测倾点，以应对如气候变化监测等挑战，填补现有技术在非平稳性和不确定性处理方面的不足。",
      "method": "研究采用递归神经算子（RNO）学习函数空间之间的映射，仅使用前倾动态数据进行训练。关键创新点在于结合 conformal prediction 框架，通过监控与物理约束（如守恒定律和偏微分方程）的偏差来预测倾点，并量化不确定性。方法应用于非平稳常微分方程和偏微分方程，例如 Lorenz-63 和 Kuramoto-Sivashinsky 方程，展示了其处理复杂动力学的能力。",
      "result": "实验结果验证了方法的有效性：在气候倾点（如云盖变化）和机翼过渡预测中，仅需有限物理知识即可实现准确预测。具体地，方法在 zero-shot 设置下成功推广到不同雷诺数的多倾点预测，显示了强泛化能力。实验表明，即使部分或近似物理约束也能可靠用于预测，提升了在实际应用中的可行性，但摘要未提供具体性能指标数据。",
      "conclusion": "本研究的主要贡献是提出了一个集成机器学习和物理约束的倾点预测框架，具有严格的数学基础和不确定性量化能力。其学术价值在于推动了非平稳动力系统分析和不确定量化领域的发展，实际应用价值体现在气候科学和工程系统的早期预警中。潜在局限性包括对物理约束的依赖性，未来工作可扩展至更多复杂系统或优化约束整合策略。",
      "tags": [
        "Recurrent Neural Operator",
        "Conformal Prediction",
        "Uncertainty Quantification",
        "Tipping Point Forecasting",
        "Non-Stationary Dynamics"
      ]
    },
    "analyzed_at": "2026-01-15T03:36:54.187220Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2306.15242",
    "title": "Positional Embedding-Aware Activations",
    "authors": [
      "Kathan Shah",
      "Chawin Sitawarin"
    ],
    "abstract": "We present a neural network architecture designed to naturally learn a positional embedding and overcome the spectral bias towards lower frequencies faced by conventional activation functions. Our proposed architecture, SPDER, is a simple MLP that uses an activation function composed of a sinusoidal multiplied by a sublinear function, called the damping function. The sinusoidal enables the network to automatically learn the positional embedding of an input coordinate while the damping passes on the actual coordinate value by preventing it from being projected down to within a finite range of values. Our results indicate that SPDERs speed up training by 10x and converge to losses 1,500-50,000x lower than that of the state-of-the-art for image representation. SPDER is also state-of-the-art in audio representation. The superior representation capability allows SPDER to also excel on multiple downstream tasks such as image super-resolution and video frame interpolation. We provide intuition as to why SPDER significantly improves fitting compared to that of other INR methods while requiring no hyperparameter tuning or preprocessing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2306.15242.pdf",
    "abs_url": "https://arxiv.org/abs/2306.15242",
    "published": "2023-06-27T06:49:40Z",
    "updated": "2026-01-14T10:11:51Z",
    "comment": null,
    "light_analysis": {
      "overview": "本研究提出SPDER神经网络架构，通过由正弦函数和亚线性阻尼函数组成的新型激活函数，自动学习位置嵌入并克服频谱偏差，显著提升了隐式神经表示的拟合效率和精度。",
      "motivation": "传统激活函数在隐式神经表示（INR）中面临频谱偏差问题，即倾向于学习低频特征，这限制了网络准确学习输入坐标的位置嵌入，导致表示能力受限。现有INR方法通常需要手动超参数调优或预处理，效率低下且难以优化。解决这一问题对提升图像、音频等领域的神经表示质量至关重要。",
      "method": "SPDER是一个简单的多层感知机（MLP），其核心创新在于激活函数设计：将正弦函数与亚线性阻尼函数相乘。正弦函数使网络能自动学习输入坐标的位置嵌入，而阻尼函数防止坐标值被投影到有限范围内，从而传递实际值并克服频谱偏差。该方法无需超参数调优或预处理，摘要未明确说明具体数据集或模型架构细节，但推断应用于图像和音频表示。",
      "result": "实验结果表明，SPDER在图像表示任务中训练速度提升10倍，收敛损失比现有最佳方法低1,500至50,000倍，达到最先进水平。在音频表示方面同样表现优异，成为新的SOTA。下游任务测试中，如图像超分辨率和视频帧插值，SPDER也展现出卓越性能，验证了其强大表示能力和泛化优势。",
      "conclusion": "论文主要贡献是提出SPDER架构，通过新颖激活函数设计有效解决INR中的频谱偏差和位置嵌入学习问题，无需复杂调优。这具有重要学术价值，推动了神经表示学习的发展，并在实际应用中如图像处理和音频分析有广泛潜力。摘要未明确说明局限性，但未来工作可能涉及扩展到更多领域或优化计算效率。",
      "tags": [
        "Positional Embedding",
        "Spectral Bias",
        "Activation Function",
        "MLP",
        "Implicit Neural Representations"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:07.037553Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2303.12957",
    "title": "Reinforcement Learning with Exogenous States and Rewards",
    "authors": [
      "George Trimponias",
      "Thomas G. Dietterich"
    ],
    "abstract": "Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms can be applied during reinforcement learning to discover the exogenous subspace, remove the exogenous reward, and focus reinforcement learning on the endogenous MDP. Experiments on a variety of challenging synthetic MDPs show that these methods, applied online, discover large exogenous state spaces and produce substantial speedups in reinforcement learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2303.12957.pdf",
    "abs_url": "https://arxiv.org/abs/2303.12957",
    "published": "2023-03-22T23:37:28Z",
    "updated": "2026-01-14T05:15:05Z",
    "comment": "Substantial rewrite to improve rigor and clarity in response to referee reports at JMLR",
    "light_analysis": {
      "overview": "论文提出了一种算法，用于发现和分解强化学习中的外生状态与奖励，以加速学习过程。",
      "motivation": "在强化学习中，外生状态变量和奖励会向奖励信号注入不可控的变异，导致学习过程变慢，因为现有方法往往忽视这种分解，无法有效处理外生成分带来的噪声。这个问题重要是因为外生变量在许多实际场景中普遍存在，可能降低学习效率。研究旨在通过形式化并识别这些外生成分，来解决传统强化学习方法在处理不可控变异时的不足，从而提高学习的稳定性和速度。",
      "method": "论文首先形式化了外生状态变量和奖励的概念，并证明如果奖励函数可加性分解为内生和外生成分，那么马尔可夫决策过程可以分解为外生马尔可夫奖励过程和内生马尔可夫决策过程。关键创新是针对状态空间通过线性组合混合的情况，提出了算法来发现外生和内生子空间，这些算法设计为在线应用，能够在强化学习过程中实时发现外生子空间、移除外生奖励，并专注于内生MDP的优化。",
      "result": "实验在多种具有挑战性的合成MDP上进行，结果表明所提算法能够有效地发现大的外生状态空间。通过移除外生奖励并专注于内生MDP，强化学习过程实现了显著的速度提升，尽管摘要未提供具体的性能指标数值，但作者指出这些方法产生了实质性的加速。与基线方法相比，这证明了算法在减少奖励方差和提高学习效率方面的优势。",
      "conclusion": "本研究的主要贡献是形式化了外生状态和奖励在强化学习中的作用，并提出了自动发现状态空间分解的算法，具有重要的学术价值，为处理外生变量提供了理论基础。在实际应用中，这能加速强化学习过程，提高效率。未来工作可能包括扩展算法到非线性场景或更复杂的环境，但摘要未明确说明具体局限性。",
      "tags": [
        "Reinforcement Learning",
        "Exogenous State Variables",
        "Exogenous Rewards",
        "State Space Decomposition",
        "Linear Combination"
      ]
    },
    "analyzed_at": "2026-01-15T03:37:27.682456Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]