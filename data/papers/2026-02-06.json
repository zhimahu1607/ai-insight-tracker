[
  {
    "id": "2602.06043",
    "title": "Shared LoRA Subspaces for almost Strict Continual Learning",
    "authors": [
      "Prakhar Kaushik",
      "Ankit Vaidya",
      "Shravan Chaudhari",
      "Rama Chellappa",
      "Alan Yuille"
    ],
    "abstract": "Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06043.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06043",
    "published": "2026-02-05T18:59:58Z",
    "updated": "2026-02-05T18:59:58Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06042",
    "title": "Pseudo-Invertible Neural Networks",
    "authors": [
      "Yamit Ehrlich",
      "Nimrod Berman",
      "Assaf Shocher"
    ],
    "abstract": "The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or \"Back-Projection\", $x' = x + A^\\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x'$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, \"degradation\" is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06042.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06042",
    "published": "2026-02-05T18:59:58Z",
    "updated": "2026-02-05T18:59:58Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06041",
    "title": "Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning",
    "authors": [
      "Xuejun Zhang",
      "Aditi Tiwari",
      "Zhenhailong Wang",
      "Heng Ji"
    ],
    "abstract": "Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20Â° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06041.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06041",
    "published": "2026-02-05T18:59:55Z",
    "updated": "2026-02-05T18:59:55Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06039",
    "title": "DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching",
    "authors": [
      "Yuxing Lu",
      "Yucheng Hu",
      "Xukai Zhao",
      "Jiuxin Cao"
    ],
    "abstract": "Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager's round goal, each agent outputs lightweight natural-language query (need) and \\key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.06039.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06039",
    "published": "2026-02-05T18:59:51Z",
    "updated": "2026-02-05T18:59:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06040",
    "title": "SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs",
    "authors": [
      "Jintao Tong",
      "Shilin Yan",
      "Hongwei Xue",
      "Xiaojun Tang",
      "Kunyu Shi",
      "Guannan Zhang",
      "Ruixuan Li",
      "Yixiong Zou"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as \"visual thoughts\" into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06040.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06040",
    "published": "2026-02-05T18:59:51Z",
    "updated": "2026-02-05T18:59:51Z",
    "comment": "Project Page: https://accio-lab.github.io/SwimBird",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06037",
    "title": "Thinking with Geometry: Active Geometry Integration for Spatial Reasoning",
    "authors": [
      "Haoyuan Li",
      "Qihang Cao",
      "Tao Tang",
      "Kun Xiang",
      "Zihan Guo",
      "Jianhua Han",
      "Hang Xu",
      "Xiaodan Liang"
    ],
    "abstract": "Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06037.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06037",
    "published": "2026-02-05T18:59:32Z",
    "updated": "2026-02-05T18:59:32Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06036",
    "title": "DFlash: Block Diffusion for Flash Speculative Decoding",
    "authors": [
      "Jian Chen",
      "Yesheng Liang",
      "Zhijian Liu"
    ],
    "abstract": "Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.06036.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06036",
    "published": "2026-02-05T18:59:30Z",
    "updated": "2026-02-05T18:59:30Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06035",
    "title": "InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions",
    "authors": [
      "Sirui Xu",
      "Samuel Schulter",
      "Morteza Ziyadi",
      "Xialin He",
      "Xiaohan Fei",
      "Yu-Xiong Wang",
      "Liangyan Gui"
    ],
    "abstract": "Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.",
    "categories": [
      "cs.CV",
      "cs.GR",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06035.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06035",
    "published": "2026-02-05T18:59:27Z",
    "updated": "2026-02-05T18:59:27Z",
    "comment": "Webpage: https://sirui-xu.github.io/InterPrior/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06034",
    "title": "V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval",
    "authors": [
      "Dongyang Chen",
      "Chaoyang Wang",
      "Dezhao SU",
      "Xi Xiao",
      "Zeyu Zhang",
      "Jing Xiong",
      "Qing Li",
      "Yuzhang Shang",
      "Shichao Ka"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06034.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06034",
    "published": "2026-02-05T18:59:21Z",
    "updated": "2026-02-05T18:59:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06033",
    "title": "Can vision language models learn intuitive physics from interaction?",
    "authors": [
      "Luca M. Schulze Buschoff",
      "Konstantinos Voudouris",
      "Can Demircan",
      "Eric Schulz"
    ],
    "abstract": "Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06033.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06033",
    "published": "2026-02-05T18:59:20Z",
    "updated": "2026-02-05T18:59:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06032",
    "title": "Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation",
    "authors": [
      "David Shavin",
      "Sagie Benaim"
    ],
    "abstract": "Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted\" onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling\" geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher's consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06032.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06032",
    "published": "2026-02-05T18:59:05Z",
    "updated": "2026-02-05T18:59:05Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06031",
    "title": "AP-OOD: Attention Pooling for Out-of-Distribution Detection",
    "authors": [
      "Claus Hofmann",
      "Christian Huber",
      "Bernhard Lehner",
      "Daniel Klotz",
      "Sepp Hochreiter",
      "Werner Zellinger"
    ],
    "abstract": "Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06031.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06031",
    "published": "2026-02-05T18:59:01Z",
    "updated": "2026-02-05T18:59:01Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06029",
    "title": "Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference",
    "authors": [
      "Yingke Li",
      "Anjali Parashar",
      "Enlu Zhou",
      "Chuchu Fan"
    ],
    "abstract": "Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06029.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06029",
    "published": "2026-02-05T18:58:32Z",
    "updated": "2026-02-05T18:58:32Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06028",
    "title": "Context Forcing: Consistent Autoregressive Video Generation with Long Context",
    "authors": [
      "Shuo Chen",
      "Cong Wei",
      "Sun Sun",
      "Ping Nie",
      "Kai Zhou",
      "Ge Zhang",
      "Ming-Hsuan Yang",
      "Wenhu Chen"
    ],
    "abstract": "Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \\textbf{student-teacher mismatch}: the teacher's inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student's context length. To resolve this, we propose \\textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \\textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06028.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06028",
    "published": "2026-02-05T18:58:01Z",
    "updated": "2026-02-05T18:58:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06025",
    "title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "authors": [
      "Haozhen Zhang",
      "Haodong Yue",
      "Tao Feng",
      "Quanyu Long",
      "Jianzhu Bao",
      "Bowen Jin",
      "Weizhi Zhang",
      "Xiao Li",
      "Jiaxuan You",
      "Chengwei Qin",
      "Wenya Wang"
    ],
    "abstract": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.06025.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06025",
    "published": "2026-02-05T18:57:09Z",
    "updated": "2026-02-05T18:57:09Z",
    "comment": "Code is available at https://github.com/ViktorAxelsen/BudgetMem",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06023",
    "title": "Learning Event-Based Shooter Models from Virtual Reality Experiments",
    "authors": [
      "Christopher A. McClurg",
      "Alan R. Wagner"
    ],
    "abstract": "Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.06023.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06023",
    "published": "2026-02-05T18:56:49Z",
    "updated": "2026-02-05T18:56:49Z",
    "comment": "Preprint under review for conference publication. 9 pages, 4 figures, 4 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06022",
    "title": "Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering",
    "authors": [
      "Miranda Muqing Miao",
      "Young-Min Cho",
      "Lyle Ungar"
    ],
    "abstract": "Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\\% and expected calibration error (ECE) by 50\\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\\% accuracy improvements and 49\\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06022.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06022",
    "published": "2026-02-05T18:55:56Z",
    "updated": "2026-02-05T18:55:56Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06020",
    "title": "Mechanisms of AI Protein Folding in ESMFold",
    "authors": [
      "Kevin Lu",
      "Jannik Brinkmann",
      "Stefan Huber",
      "Aaron Mueller",
      "Yonatan Belinkov",
      "David Bau",
      "Chris Wendler"
    ],
    "abstract": "How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.",
    "categories": [
      "cs.LG",
      "q-bio.BM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06020.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06020",
    "published": "2026-02-05T18:54:54Z",
    "updated": "2026-02-05T18:54:54Z",
    "comment": "Our code, data, and results are available at https://folding.baulab.info",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06019",
    "title": "Multi-Token Prediction via Self-Distillation",
    "authors": [
      "John Kirchenbauer",
      "Abhimanyu Hans",
      "Brian Bartoldson",
      "Micah Goldblum",
      "Ashwinee Panda",
      "Tom Goldstein"
    ],
    "abstract": "Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\\times$ faster on average at $<5\\%$ drop in accuracy relative to single token decoding performance.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.06019.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06019",
    "published": "2026-02-05T18:54:48Z",
    "updated": "2026-02-05T18:54:48Z",
    "comment": "8 pages and 5 figures in the main body",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06017",
    "title": "MambaVF: State Space Model for Efficient Video Fusion",
    "authors": [
      "Zixiang Zhao",
      "Yukun Cui",
      "Lilun Deng",
      "Haowen Bai",
      "Haotong Qin",
      "Tao Feng",
      "Konrad Schindler"
    ],
    "abstract": "Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06017.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06017",
    "published": "2026-02-05T18:53:47Z",
    "updated": "2026-02-05T18:53:47Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06015",
    "title": "A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies",
    "authors": [
      "Panagiotis Kaliosis",
      "Adithya V Ganesan",
      "Oscar N. E. Kjell",
      "Whitney Ringwald",
      "Scott Feltman",
      "Melissa A. Carr",
      "Dimitris Samaras",
      "Camilo Ruggero",
      "Benjamin J. Luft",
      "Roman Kotov",
      "Andrew H. Schwartz"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.06015.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06015",
    "published": "2026-02-05T18:53:17Z",
    "updated": "2026-02-05T18:53:17Z",
    "comment": "18 pages, 3 figures, 5 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06014",
    "title": "Optimism Stabilizes Thompson Sampling for Adaptive Inference",
    "authors": [
      "Shunxing Yan",
      "Han Zhong"
    ],
    "abstract": "Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \\emph{optimism} as a key mechanism for restoring \\emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm's pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \\citep{halder2025stable} is stable for any $K \\ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \\citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.06014.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06014",
    "published": "2026-02-05T18:52:54Z",
    "updated": "2026-02-05T18:52:54Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06013",
    "title": "GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?",
    "authors": [
      "Ruihang Li",
      "Leigang Qu",
      "Jingxu Zhang",
      "Dongnan Gui",
      "Mengde Xu",
      "Xiaosong Zhang",
      "Han Hu",
      "Wenjie Wang",
      "Jiaqi Wang"
    ],
    "abstract": "The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.06013.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06013",
    "published": "2026-02-05T18:52:48Z",
    "updated": "2026-02-05T18:52:48Z",
    "comment": "Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06008",
    "title": "AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions",
    "authors": [
      "Xianyang Liu",
      "Shangding Gu",
      "Dawn Song"
    ],
    "abstract": "Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.06008.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06008",
    "published": "2026-02-05T18:50:36Z",
    "updated": "2026-02-05T18:50:36Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.06000",
    "title": "Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods",
    "authors": [
      "Ali Shendabadi",
      "Parnia Izadirad",
      "Mostafa Salehi",
      "Mahmoud Bijankhan"
    ],
    "abstract": "Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.06000.pdf",
    "abs_url": "https://arxiv.org/abs/2602.06000",
    "published": "2026-02-05T18:46:28Z",
    "updated": "2026-02-05T18:46:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05999",
    "title": "On Computation and Reinforcement Learning",
    "authors": [
      "Raj Ghugare",
      "MichaÅ Bortkiewicz",
      "Alicja Ziarko",
      "Benjamin Eysenbach"
    ],
    "abstract": "How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05999.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05999",
    "published": "2026-02-05T18:45:57Z",
    "updated": "2026-02-05T18:45:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05998",
    "title": "VisRefiner: Learning from Visual Differences for Screenshot-to-Code Generation",
    "authors": [
      "Jie Deng",
      "Kaichun Yao",
      "Libo Zhang"
    ],
    "abstract": "Screenshot-to-code generation aims to translate user interface screenshots into executable frontend code that faithfully reproduces the target layout and style. Existing multimodal large language models perform this mapping directly from screenshots but are trained without observing the visual outcomes of their generated code. In contrast, human developers iteratively render their implementation, compare it with the design, and learn how visual differences relate to code changes. Inspired by this process, we propose VisRefiner, a training framework that enables models to learn from visual differences between rendered predictions and reference designs. We construct difference-aligned supervision that associates visual discrepancies with corresponding code edits, allowing the model to understand how appearance variations arise from implementation changes. Building on this, we introduce a reinforcement learning stage for self-refinement, where the model improves its generated code by observing both the rendered output and the target design, identifying their visual differences, and updating the code accordingly. Experiments show that VisRefiner substantially improves single-step generation quality and layout fidelity, while also endowing models with strong self-refinement ability. These results demonstrate the effectiveness of learning from visual differences for advancing screenshot-to-code generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05998.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05998",
    "published": "2026-02-05T18:45:53Z",
    "updated": "2026-02-05T18:45:53Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05996",
    "title": "Orthogonal Self-Attention",
    "authors": [
      "Leo Zhang",
      "James Martens"
    ],
    "abstract": "Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05996.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05996",
    "published": "2026-02-05T18:42:57Z",
    "updated": "2026-02-05T18:42:57Z",
    "comment": "Preprint",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05993",
    "title": "Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps",
    "authors": [
      "Peter Holderrieth",
      "Douglas Chen",
      "Luca Eyring",
      "Ishin Shah",
      "Giri Anantharaman",
      "Yutong He",
      "Zeynep Akata",
      "Tommi Jaakkola",
      "Nicholas Matthew Boffi",
      "Max Simchowitz"
    ],
    "abstract": "Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose \"Diamond Maps\", stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05993.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05993",
    "published": "2026-02-05T18:42:00Z",
    "updated": "2026-02-05T18:42:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05992",
    "title": "DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs",
    "authors": [
      "Lizhuo Luo",
      "Shenggui Li",
      "Yonggang Wen",
      "Tianwei Zhang"
    ],
    "abstract": "Diffusion large language models (dLLMs) have emerged as a promising alternative for text generation, distinguished by their native support for parallel decoding. In practice, block inference is crucial for avoiding order misalignment in global bidirectional decoding and improving output quality. However, the widely-used fixed, predefined block (naive) schedule is agnostic to semantic difficulty, making it a suboptimal strategy for both quality and efficiency: it can force premature commitments to uncertain positions while delaying easy positions near block boundaries. In this work, we analyze the limitations of naive block scheduling and disclose the importance of dynamically adapting the schedule to semantic difficulty for reliable and efficient inference. Motivated by this, we propose Dynamic Sliding Block (DSB), a training-free block scheduling method that uses a sliding block with a dynamic size to overcome the rigidity of the naive block. To further improve efficiency, we introduce DSB Cache, a training-free KV-cache mechanism tailored to DSB. Extensive experiments across multiple models and benchmarks demonstrate that DSB, together with DSB Cache, consistently improves both generation quality and inference efficiency for dLLMs. Code is released at https://github.com/lizhuo-luo/DSB.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05992.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05992",
    "published": "2026-02-05T18:41:38Z",
    "updated": "2026-02-05T18:41:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05988",
    "title": "Layer-wise LoRA fine-tuning: a similarity metric approach",
    "authors": [
      "Keith Ando Ogawa",
      "Bruno Lopes Yamamoto",
      "Lucas Lauton de Alcantara",
      "Lucas Pellicer",
      "Rosimeire Pereira Costa",
      "Edson Bollis",
      "Anna Helena Reali Costa",
      "Artur Jordao"
    ],
    "abstract": "Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05988.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05988",
    "published": "2026-02-05T18:38:53Z",
    "updated": "2026-02-05T18:38:53Z",
    "comment": "Code is available at https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05986",
    "title": "RISE-Video: Can Video Generators Decode Implicit World Rules?",
    "authors": [
      "Mingxin Liu",
      "Shuran Ma",
      "Shibei Meng",
      "Xiangyu Zhao",
      "Zicheng Zhang",
      "Shaofeng Zhang",
      "Zhihang Zhong",
      "Peixian Chen",
      "Haoyu Cao",
      "Xing Sun",
      "Haodong Duan",
      "Xue Yang"
    ],
    "abstract": "While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \\textit{Reasoning Alignment}, \\textit{Temporal Consistency}, \\textit{Physical Rationality}, and \\textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05986.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05986",
    "published": "2026-02-05T18:36:10Z",
    "updated": "2026-02-05T18:36:10Z",
    "comment": "38 pages, 16 figures, 3 tables; Code: https://github.com/VisionXLab/RISE-Video; HuggingFace: https://huggingface.co/datasets/VisionXLab/RISE-Video",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05983",
    "title": "Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins",
    "authors": [
      "KreÅ¡imir KuÅ¡iÄ",
      "Vinny Cahill",
      "Ivana Dusparic"
    ],
    "abstract": "The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05983.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05983",
    "published": "2026-02-05T18:33:03Z",
    "updated": "2026-02-05T18:33:03Z",
    "comment": "IEEE IV2026 37th IEEE Intelligent Vehicles Symposium",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05977",
    "title": "Clifford Kolmogorov-Arnold Networks",
    "authors": [
      "Matthias Wolff",
      "Francesco Alesiani",
      "Christof Duhme",
      "Xiaoyi Jiang"
    ],
    "abstract": "We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05977.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05977",
    "published": "2026-02-05T18:25:40Z",
    "updated": "2026-02-05T18:25:40Z",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05971",
    "title": "Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space",
    "authors": [
      "Felipe D. Toro-HernÃ¡ndez",
      "Jesuino Vieira Filho",
      "Rodrigo M. Cabral-Carvalho"
    ],
    "abstract": "Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.",
    "categories": [
      "cs.CL",
      "cs.LG",
      "q-bio.NC"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05971.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05971",
    "published": "2026-02-05T18:23:04Z",
    "updated": "2026-02-05T18:23:04Z",
    "comment": "10 pages, 6 figures (excluding refs/appendix). Accepted to ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05970",
    "title": "Inverse Depth Scaling From Most Layers Being Similar",
    "authors": [
      "Yizhou Liu",
      "Sara Kangaslahti",
      "Ziming Liu",
      "Jeff Gore"
    ],
    "abstract": "Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.DS",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05970.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05970",
    "published": "2026-02-05T18:22:41Z",
    "updated": "2026-02-05T18:22:41Z",
    "comment": "23 pages, 24 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05967",
    "title": "A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders",
    "authors": [
      "Mohamad Amin Jamshidi",
      "Mehrbod Zarifi",
      "Zolfa Anvari",
      "Hamed Ghafarirad",
      "Mohammad Zareinejad"
    ],
    "abstract": "Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm's performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.",
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05967.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05967",
    "published": "2026-02-05T18:21:28Z",
    "updated": "2026-02-05T18:21:28Z",
    "comment": "Published in: 2025 33rd International Conference on Electrical Engineering (ICEE), Publisher IEEE",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05966",
    "title": "LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation",
    "authors": [
      "Mirlan Karimov",
      "Teodora Spasojevic",
      "Markus Braun",
      "Julian Wiederer",
      "Vasileios Belagiannis",
      "Marc Pollefeys"
    ],
    "abstract": "Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05966.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05966",
    "published": "2026-02-05T18:21:02Z",
    "updated": "2026-02-05T18:21:02Z",
    "comment": "Accepted to IEEE IV 2026. 8 pages, 3 figures. Code available at https://github.com/mirlanium/LSA",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05961",
    "title": "Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces",
    "authors": [
      "Arran Carter",
      "Sanghyeok Choi",
      "Kirill Tamogashev",
      "VÃ­ctor Elvira",
      "Nikolay Malkin"
    ],
    "abstract": "Sampling from a distribution $p(x) \\propto e^{-\\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy SchrÃ¶dinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05961.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05961",
    "published": "2026-02-05T18:16:57Z",
    "updated": "2026-02-05T18:16:57Z",
    "comment": "Code: https://github.com/mmacosha/offpolicy-discrete-diffusion-samplers-and-bridges",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05951",
    "title": "Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching",
    "authors": [
      "Junwan Kim",
      "Jiho Park",
      "Seonghu Jeon",
      "Seungryong Kim"
    ],
    "abstract": "Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05951.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05951",
    "published": "2026-02-05T18:08:20Z",
    "updated": "2026-02-05T18:08:20Z",
    "comment": "Project Page: https://junwankimm.github.io/CSFM",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05950",
    "title": "Breaking Symmetry Bottlenecks in GNN Readouts",
    "authors": [
      "Mouad Talhi",
      "Arne Wolf",
      "Anthea Monod"
    ],
    "abstract": "Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05950.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05950",
    "published": "2026-02-05T18:08:13Z",
    "updated": "2026-02-05T18:08:13Z",
    "comment": "23 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05946",
    "title": "$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment",
    "authors": [
      "Rajdeep Haldar",
      "Lantao Mei",
      "Guang Lin",
      "Yue Xing",
      "Qifan Song"
    ],
    "abstract": "Recent research shows that Preference Alignment (PA) objectives act as divergence estimators between aligned (chosen) and unaligned (rejected) response distributions. In this work, we extend this divergence-based perspective to general alignment settings, such as reinforcement learning with verifiable rewards (RLVR), where only environmental rewards are available. Within this unified framework, we propose $f$-Group Relative Policy Optimization ($f$-GRPO), a class of on-policy reinforcement learning, and $f$-Hybrid Alignment Loss ($f$-HAL), a hybrid on/off policy objectives, for general LLM alignment based on variational representation of $f$-divergences. We provide theoretical guarantees that these classes of objectives improve the average reward after alignment. Empirically, we validate our framework on both RLVR (Math Reasoning) and PA tasks (Safety Alignment), demonstrating superior performance and flexibility compared to current methods.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05946.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05946",
    "published": "2026-02-05T18:01:52Z",
    "updated": "2026-02-05T18:01:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05943",
    "title": "Orthogonal Model Merging",
    "authors": [
      "Sihan Yang",
      "Kexuan Shi",
      "Weiyang Liu"
    ],
    "abstract": "Merging finetuned Large Language Models (LLMs) has become increasingly important for integrating diverse capabilities into a single unified model. However, prevailing model merging methods rely on linear arithmetic in Euclidean space, which often destroys the intrinsic geometric properties of pretrained weights, such as hyperspherical energy. To address this, we propose Orthogonal Model Merging (OrthoMerge), a method that performs merging operations on the Riemannian manifold formed by the orthogonal group to preserve the geometric structure of the model's weights. By mapping task-specific orthogonal matrices learned by Orthogonal Finetuning (OFT) to the Lie algebra, OrthoMerge enables a principled yet efficient integration that takes into account both the direction and intensity of adaptations. In addition to directly leveraging orthogonal matrices obtained by OFT, we further extend this approach to general models finetuned with non-OFT methods (i.e., low-rank finetuning, full finetuning) via an Orthogonal-Residual Decoupling strategy. This technique extracts the orthogonal components of expert models by solving the orthogonal Procrustes problem, which are then merged on the manifold of the orthogonal group, while the remaining linear residuals are processed through standard additive merging. Extensive empirical results demonstrate the effectiveness of OrthoMerge in mitigating catastrophic forgetting and maintaining model performance across diverse tasks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05943.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05943",
    "published": "2026-02-05T17:57:14Z",
    "updated": "2026-02-05T17:57:14Z",
    "comment": "Technical report (18 pages, 9 figures, project page: https://spherelab.ai/OrthoMerge/)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05940",
    "title": "Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training",
    "authors": [
      "Junxiao Liu",
      "Zhijun Wang",
      "Yixiao Li",
      "Zhejian Lai",
      "Liqian Huang",
      "Xin Huang",
      "Xue Han",
      "Junlan Feng",
      "Shujian Huang"
    ],
    "abstract": "Long reasoning models often struggle in multilingual settings: they tend to reason in English for non-English questions; when constrained to reasoning in the question language, accuracies drop substantially. The struggle is caused by the limited abilities for both multilingual question understanding and multilingual reasoning. To address both problems, we propose TRIT (Translation-Reasoning Integrated Training), a self-improving framework that integrates the training of translation into multilingual reasoning. Without external feedback or additional multilingual data, our method jointly enhances multilingual question understanding and response generation. On MMATH, our method outperforms multiple baselines by an average of 7 percentage points, improving both answer correctness and language consistency. Further analysis reveals that integrating translation training improves cross-lingual question alignment by over 10 percentage points and enhances translation quality for both mathematical questions and general-domain text, with gains up to 8.4 COMET points on FLORES-200.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05940.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05940",
    "published": "2026-02-05T17:55:09Z",
    "updated": "2026-02-05T17:55:09Z",
    "comment": "16 pages, 11 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05937",
    "title": "Multi-Scale Global-Instance Prompt Tuning for Continual Test-time Adaptation in Medical Image Segmentation",
    "authors": [
      "Lingrui Li",
      "Yanfeng Zhou",
      "Nan Pu",
      "Xin Chen",
      "Zhun Zhong"
    ],
    "abstract": "Distribution shift is a common challenge in medical images obtained from different clinical centers, significantly hindering the deployment of pre-trained semantic segmentation models in real-world applications across multiple domains. Continual Test-Time Adaptation(CTTA) has emerged as a promising approach to address cross-domain shifts during continually evolving target domains. Most existing CTTA methods rely on incrementally updating model parameters, which inevitably suffer from error accumulation and catastrophic forgetting, especially in long-term adaptation. Recent prompt-tuning-based works have shown potential to mitigate the two issues above by updating only visual prompts. While these approaches have demonstrated promising performance, several limitations remain:1)lacking multi-scale prompt diversity, 2)inadequate incorporation of instance-specific knowledge, and 3)risk of privacy leakage. To overcome these limitations, we propose Multi-scale Global-Instance Prompt Tuning(MGIPT), to enhance scale diversity of prompts and capture both global- and instance-level knowledge for robust CTTA. Specifically, MGIPT consists of an Adaptive-scale Instance Prompt(AIP) and a Multi-scale Global-level Prompt(MGP). AIP dynamically learns lightweight and instance-specific prompts to mitigate error accumulation with adaptive optimal-scale selection mechanism. MGP captures domain-level knowledge across different scales to ensure robust adaptation with anti-forgetting capabilities. These complementary components are combined through a weighted ensemble approach, enabling effective dual-level adaptation that integrates both global and local information. Extensive experiments on medical image segmentation benchmarks demonstrate that our MGIPT outperforms state-of-the-art methods, achieving robust adaptation across continually changing target domains.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05937.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05937",
    "published": "2026-02-05T17:47:35Z",
    "updated": "2026-02-05T17:47:35Z",
    "comment": "8 pages, BIBM2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05936",
    "title": "Dimensionality Reduction on Riemannian Manifolds in Data Analysis",
    "authors": [
      "Alaa El Ichi",
      "Khalide Jbilou"
    ],
    "abstract": "In this work, we investigate Riemannian geometry based dimensionality reduction methods that respect the underlying manifold structure of the data. In particular, we focus on Principal Geodesic Analysis (PGA) as a nonlinear generalization of PCA for manifold valued data, and extend discriminant analysis through Riemannian adaptations of other known dimensionality reduction methods. These approaches exploit geodesic distances, tangent space representations, and intrinsic statistical measures to achieve more faithful low dimensional embeddings. We also discuss related manifold learning techniques and highlight their theoretical foundations and practical advantages. Experimental results on representative datasets demonstrate that Riemannian methods provide improved representation quality and classification performance compared to their Euclidean counterparts, especially for data constrained to curved spaces such as hyperspheres and symmetric positive definite manifolds. This study underscores the importance of geometry aware dimensionality reduction in modern machine learning and data science applications.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05936.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05936",
    "published": "2026-02-05T17:46:58Z",
    "updated": "2026-02-05T17:46:58Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05935",
    "title": "Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data",
    "authors": [
      "Sudeepta Mondal",
      "Xinyi Mary Xie",
      "Ruxiao Duan",
      "Alex Wong",
      "Ganesh Sundaramoorthi"
    ],
    "abstract": "Existing out-of-distribution (OOD) detectors are often tuned by a separate dataset deemed OOD with respect to the training distribution of a neural network (NN). OOD detectors process the activations of NN layers and score the output, where parameters of the detectors are determined by fitting to an in-distribution (training) set and the aforementioned dataset chosen adhocly. At detector training time, this adhoc dataset may not be available or difficult to obtain, and even when it's available, it may not be representative of actual OOD data, which is often ''unknown unknowns.\" Current benchmarks may specify some left-out set from test OOD sets. We show that there can be significant variance in performance of detectors based on the adhoc dataset chosen in current literature, and thus even if such a dataset can be collected, the performance of the detector may be highly dependent on the choice. In this paper, we introduce and formalize the often neglected problem of tuning OOD detectors without a given ``OOD'' dataset. To this end, we present strong baselines as an attempt to approach this problem. Furthermore, we propose a new generic approach to OOD detector tuning that does not require any extra data other than those used to train the NN. We show that our approach improves over baseline methods consistently across higher-parameter OOD detector families, while being comparable across lower-parameter families.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05935.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05935",
    "published": "2026-02-05T17:46:40Z",
    "updated": "2026-02-05T17:46:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05933",
    "title": "Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training",
    "authors": [
      "Zhenghao Xu",
      "Qin Lu",
      "Changlong Yu",
      "Tuo Zhao"
    ],
    "abstract": "Policy mirror descent (PMD) provides a principled framework for reinforcement learning (RL) by iteratively solving KL-regularized policy improvement subproblems. While this approach has been adopted in training advanced LLMs such as Kimi K1.5/K2, the ideal closed-form PMD updates require reliable partition function estimation, a significant challenge when working with limited rollouts in the vast action spaces of LLMs. We investigate a practical algorithm, termed PMD-mean, that approximates the log-partition term with the mean reward under the sampling policy and performs regression in log-policy space. Specifically, we characterize the population solution of PMD-mean and demonstrate that it implicitly optimizes mirror descent subproblems with an adaptive mixed KL--$Ï^2$ regularizer. This additional $Ï^2$ regularization constrains large probability changes, producing more conservative updates when expected rewards are low and enhancing robustness against finite-sample estimation errors. Experiments on math reasoning tasks show that PMD-mean achieves superior performance with improved stability and time efficiency. These findings deepen our understanding of PMD-mean and illuminate pathways toward principled improvements in RL algorithms for LLMs. Code is available at https://github.com/horizon-rl/OpenKimi.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05933.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05933",
    "published": "2026-02-05T17:44:28Z",
    "updated": "2026-02-05T17:44:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05932",
    "title": "Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions",
    "authors": [
      "LÃ©o Labat",
      "Etienne Ollion",
      "FranÃ§ois Yvon"
    ],
    "abstract": "Multiple-Choice Questions (MCQs) are often used to assess knowledge, reasoning abilities, and even values encoded in large language models (LLMs). While the effect of multilingualism has been studied on LLM factual recall, this paper seeks to investigate the less explored question of language-induced variation in value-laden MCQ responses. Are multilingual LLMs consistent in their responses across languages, i.e. behave like theoretical polyglots, or do they answer value-laden MCQs depending on the language of the question, like a multitude of monolingual models expressing different values through a single model? We release a new corpus, the Multilingual European Value Survey (MEVS), which, unlike prior work relying on machine translation or ad hoc prompts, solely comprises human-translated survey questions aligned in 8 European languages. We administer a subset of those questions to over thirty multilingual LLMs of various sizes, manufacturers and alignment-fine-tuning status under comprehensive, controlled prompt variations including answer order, symbol type, and tail character. Our results show that while larger, instruction-tuned models display higher overall consistency, the robustness of their responses varies greatly across questions, with certain MCQs eliciting total agreement within and across models while others leave LLM answers split. Language-specific behavior seems to arise in all consistent, instruction-fine-tuned models, but only on certain questions, warranting a further study of the selective effect of preference fine-tuning.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05932.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05932",
    "published": "2026-02-05T17:44:06Z",
    "updated": "2026-02-05T17:44:06Z",
    "comment": "17 pages, 5 figures (8 pages of references and appendices)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05929",
    "title": "KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs",
    "authors": [
      "Jian Chen",
      "Zhuoran Wang",
      "Jiayu Qin",
      "Ming Li",
      "Meng Wang",
      "Changyou Chen",
      "Yin Chen",
      "Qizhen Weng",
      "Yirui Liu"
    ],
    "abstract": "Large language models rely on kv-caches to avoid redundant computation during autoregressive decoding, but as context length grows, reading and writing the cache can quickly saturate GPU memory bandwidth. Recent work has explored KV-cache compression, yet most approaches neglect the data-dependent nature of kv-caches and their variation across layers. We introduce KV-CoRE KV-cache Compressibility by Rank Evaluation), an SVD-based method for quantifying the data-dependent low-rank compressibility of kv-caches. KV-CoRE computes the optimal low-rank approximation under the Frobenius norm and, being gradient-free and incremental, enables efficient dataset-level, layer-wise evaluation. Using this method, we analyze multiple models and datasets spanning five English domains and sixteen languages, uncovering systematic patterns that link compressibility to model architecture, training data, and language coverage. As part of this analysis, we employ the Normalized Effective Rank as a metric of compressibility and show that it correlates strongly with performance degradation under compression. Our study establishes a principled evaluation framework and the first large-scale benchmark of kv-cache compressibility in LLMs, offering insights for dynamic, data-aware compression and data-centric model development.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05929.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05929",
    "published": "2026-02-05T17:41:57Z",
    "updated": "2026-02-05T17:41:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05920",
    "title": "Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem",
    "authors": [
      "Eva AndrÃ©s"
    ],
    "abstract": "This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.",
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05920.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05920",
    "published": "2026-02-05T17:32:14Z",
    "updated": "2026-02-05T17:32:14Z",
    "comment": "22 pages, 12 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05910",
    "title": "Chunky Post-Training: Data Driven Failures of Generalization",
    "authors": [
      "Seoirse Murray",
      "Allison Qi",
      "Timothy Qian",
      "John Schulman",
      "Collin Burns",
      "Sara Price"
    ],
    "abstract": "LLM post-training involves many diverse datasets, each targeting a specific behavior. But these datasets encode incidental patterns alongside intended ones: correlations between formatting and content, narrow phrasings across diverse problems, and implicit associations arising from the discrete data curation process. These patterns are often invisible to developers yet salient to models, producing behaviors that surprise their creators, such as rejecting true facts presented in a particular question format. We call this chunky post-training: the model learns spurious correlations as a result of distinct chunks of post-training data. We introduce SURF, a black-box pipeline which surfaces these unintended behaviors at run time, and TURF, a tool that traces these failures back to specific post-training data. Applying these tools to frontier models (Claude 4.5, GPT-5.1, Grok 4.1, Gemini 3) and open models (TÃ¼lu 3), we show that chunky post-training produces miscalibrated behaviors, which often result from imbalanced or underspecified chunks of post-training data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05910.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05910",
    "published": "2026-02-05T17:25:51Z",
    "updated": "2026-02-05T17:25:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05909",
    "title": "CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression",
    "authors": [
      "Kangjie Zhang",
      "Wenxuan Huang",
      "Xin Zhou",
      "Boxiang Zhou",
      "Dejia Song",
      "Yuan Xie",
      "Baochang Zhang",
      "Lizhuang Ma",
      "Nemo Chen",
      "Xu Tang",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "abstract": "Contrastive Language-Image Pre-training (CLIP) has achieved widely applications in various computer vision tasks, e.g., text-to-image generation, Image-Text retrieval and Image captioning. However, CLIP suffers from high memory and computation cost, which prohibits its usage to the resource-limited application scenarios. Existing CLIP compression methods typically reduce the size of pre-trained CLIP weights by selecting their subset as weight inheritance for further retraining via mask optimization or important weight measurement. However, these select-based weight inheritance often compromises the feature presentation ability, especially on the extreme compression. In this paper, we propose a novel mapping-based CLIP compression framework, CLIP-Map. It leverages learnable matrices to map and combine pretrained weights by Full-Mapping with Kronecker Factorization, aiming to preserve as much information from the original weights as possible. To mitigate the optimization challenges introduced by the learnable mapping, we propose Diagonal Inheritance Initialization to reduce the distribution shifting problem for efficient and effective mapping learning. Extensive experimental results demonstrate that the proposed CLIP-Map outperforms select-based frameworks across various compression ratios, with particularly significant gains observed under high compression settings.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05909.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05909",
    "published": "2026-02-05T17:25:16Z",
    "updated": "2026-02-05T17:25:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05905",
    "title": "Codified Finite-state Machines for Role-playing",
    "authors": [
      "Letian Peng",
      "Yupeng Hou",
      "Kun Zhou",
      "Jingbo Shang"
    ],
    "abstract": "Modeling latent character states is crucial for consistent and engaging role-playing (RP) with large language models (LLMs). Yet, existing prompting-based approaches mainly capture surface actions, often failing to track the latent states that drive interaction. We revisit finite-state machines (FSMs), long used in game design to model state transitions. While effective in small, well-specified state spaces, traditional hand-crafted, rule-based FSMs struggle to adapt to the open-ended semantic space of RP. To address this, we introduce Codified Finite-State Machines (CFSMs), a framework that automatically codifies textual character profiles into FSMs using LLM-based coding. CFSMs extract key states and transitions directly from the profile, producing interpretable structures that enforce character consistency. To further capture uncertainty and variability, we extend CFSMs into Codified Probabilistic Finite-State Machines (CPFSMs), where transitions are modeled as probability distributions over states. Through both synthetic evaluations and real-world RP scenarios in established artifacts, we demonstrate that CFSM and CPFSM outperform generally applied baselines, verifying effectiveness not only in structured tasks but also in open-ended stochastic state exploration.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05905.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05905",
    "published": "2026-02-05T17:19:18Z",
    "updated": "2026-02-05T17:19:18Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05903",
    "title": "Verification of the Implicit World Model in a Generative Model via Adversarial Sequences",
    "authors": [
      "AndrÃ¡s Balogh",
      "MÃ¡rk Jelasity"
    ],
    "abstract": "Generative sequence models are typically trained on sample sequences from natural or formal languages. It is a crucial question whether -- or to what extent -- sample-based training is able to capture the true structure of these languages, often referred to as the ``world model''. Theoretical results indicate that we can hope for soundness at best, that is, generating valid sequences, but not necessarily all of them. However, it is still important to have practical tools that are able to verify whether a given sequence model is sound. In this study, we focus on chess, as it is a domain that provides enough complexity while having a simple rule-based world model. We propose adversarial sequence generation for verifying the soundness of the sequence model. Our adversaries generate valid sequences so as to force the sequence model to generate an invalid next move prediction. Apart from the falsification of soundness, this method is also suitable for a more fine-grained analysis of the failure modes and the effects of different choices during training. To demonstrate this, we propose a number of methods for adversarial sequence generation and evaluate the approach on a large set of chess models. We train models on random as well as high-quality chess games, using several training recipes. We find that none of the models are sound, but some training techniques and dataset choices are able to improve soundness remarkably. We also investigate the potential application of board state probes in both our training and attack methods. Our findings indicate that the extracted board states have no causal role in next token prediction in most of the models.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05903.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05903",
    "published": "2026-02-05T17:18:22Z",
    "updated": "2026-02-05T17:18:22Z",
    "comment": "Accepted at ICLR 2026. Code, datasets, and models are available at https://github.com/szegedai/world-model-verification",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05902",
    "title": "Regularized Calibration with Successive Rounding for Post-Training Quantization",
    "authors": [
      "Seohyeon Cha",
      "Huancheng Chen",
      "Dongjun Kim",
      "Haoran Zhang",
      "Kevin Chan",
      "Gustavo de Veciana",
      "Haris Vikalo"
    ],
    "abstract": "Large language models (LLMs) deliver robust performance across diverse applications, yet their deployment often faces challenges due to the memory and latency costs of storing and accessing billions of parameters. Post-training quantization (PTQ) enables efficient inference by mapping pretrained weights to low-bit formats without retraining, but its effectiveness depends critically on both the quantization objective and the rounding procedure used to obtain low-bit weight representations. In this work, we show that interpolating between symmetric and asymmetric calibration acts as a form of regularization that preserves the standard quadratic structure used in PTQ while providing robustness to activation mismatch. Building on this perspective, we derive a simple successive rounding procedure that naturally incorporates asymmetric calibration, as well as a bounded-search extension that allows for an explicit trade-off between quantization quality and the compute cost. Experiments across multiple LLM families, quantization bit-widths, and benchmarks demonstrate that the proposed bounded search based on a regularized asymmetric calibration objective consistently improves perplexity and accuracy over PTQ baselines, while incurring only modest and controllable additional computational cost.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05902.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05902",
    "published": "2026-02-05T17:18:02Z",
    "updated": "2026-02-05T17:18:02Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05897",
    "title": "Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models",
    "authors": [
      "Shuo Nie",
      "Hexuan Deng",
      "Chao Wang",
      "Ruiyu Fang",
      "Xuebo Liu",
      "Shuangyong Song",
      "Yu Li",
      "Min Zhang",
      "Xuelong Li"
    ],
    "abstract": "As large language models become smaller and more efficient, small reasoning models (SRMs) are crucial for enabling chain-of-thought (CoT) reasoning in resource-constrained settings. However, they are prone to faithfulness hallucinations, especially in intermediate reasoning steps. Existing mitigation methods based on online reinforcement learning rely on outcome-based rewards or coarse-grained CoT evaluation, which can inadvertently reinforce unfaithful reasoning when the final answer is correct. To address these limitations, we propose Faithfulness-Aware Step-Level Reinforcement Learning (FaithRL), introducing step-level supervision via explicit faithfulness rewards from a process reward model, together with an implicit truncated resampling strategy that generates contrastive signals from faithful prefixes. Experiments across multiple SRMs and Open-Book QA benchmarks demonstrate that FaithRL consistently reduces hallucinations in both the CoT and final answers, leading to more faithful and reliable reasoning. Code is available at https://github.com/Easy195/FaithRL.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05897.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05897",
    "published": "2026-02-05T17:15:12Z",
    "updated": "2026-02-05T17:15:12Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05896",
    "title": "Parity, Sensitivity, and Transformers",
    "authors": [
      "Alexander Kozachinskiy",
      "Tomasz Steifer",
      "PrzemysÅaw WaÅÈ©ga"
    ],
    "abstract": "The transformer architecture is almost a decade old. Despite that, we still have a limited understanding of what this architecture can or cannot compute. For instance, can a 1-layer transformer solve PARITY -- or more generally -- which kinds of transformers can do it? Known constructions for PARITY have at least 2 layers and employ impractical features: either a length-dependent positional encoding, or hardmax, or layernorm without the regularization parameter, or they are not implementable with causal masking.   We give a new construction of a transformer for PARITY with softmax, length-independent and polynomially bounded positional encoding, no layernorm, working both with and without causal masking. We also give the first lower bound for transformers solving PARITY -- by showing that it cannot be done with only one layer and one head.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05896.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05896",
    "published": "2026-02-05T17:14:33Z",
    "updated": "2026-02-05T17:14:33Z",
    "comment": "15 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05892",
    "title": "ContextBench: A Benchmark for Context Retrieval in Coding Agents",
    "authors": [
      "Han Li",
      "Letian Zhu",
      "Bohan Zhang",
      "Rili Feng",
      "Jiaming Wang",
      "Yue Pan",
      "Earl T. Barr",
      "Sarro Federica",
      "Zhaoyang Chu",
      "He Ye"
    ],
    "abstract": "LLM-based coding agents have shown strong performance on automated issue resolution benchmarks, yet existing evaluations largely focus on final task success, providing limited insight into how agents retrieve and use code context during problem solving. We introduce ContextBench, a process-oriented evaluation of context retrieval in coding agents. ContextBench consists of 1,136 issue-resolution tasks from 66 repositories across eight programming languages, each augmented with human-annotated gold contexts. We further implement an automated evaluation framework that tracks agent trajectories and measures context recall, precision, and efficiency throughout issue resolution. Using ContextBench, we evaluate four frontier LLMs and five coding agents. Our results show that sophisticated agent scaffolding yields only marginal gains in context retrieval (\"The Bitter Lesson\" of coding agents), LLMs consistently favor recall over precision, and substantial gaps exist between explored and utilized context. ContextBench augments existing end-to-end benchmarks with intermediate gold-context metrics that unbox the issue-resolution process. These contexts offer valuable intermediate signals for guiding LLM reasoning in software tasks. Data and code are available at: https://cioutn.github.io/context-bench/.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05892.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05892",
    "published": "2026-02-05T17:10:26Z",
    "updated": "2026-02-05T17:10:26Z",
    "comment": "36 pages, 6 figures, 4 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05890",
    "title": "DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training",
    "authors": [
      "Dingwei Zhu",
      "Zhiheng Xi",
      "Shihan Dou",
      "Jiahan Li",
      "Chenhao Huang",
      "Junjie Ye",
      "Sixian Li",
      "Mingxu Chai",
      "Yuhui Wang",
      "Yajie Yang",
      "Ming Zhang",
      "Jiazheng Zhang",
      "Shichun Liu",
      "Caishuang Huang",
      "Yunke Zhang",
      "Yuran Wang",
      "Tao Gui",
      "Xipeng Qiu",
      "Qi Zhang",
      "Xuanjing Huang"
    ],
    "abstract": "Training reinforcement learning (RL) systems in real-world environments remains challenging due to noisy supervision and poor out-of-domain (OOD) generalization, especially in LLM post-training. Recent distributional RL methods improve robustness by modeling values with multiple quantile points, but they still learn each quantile independently as a scalar. This results in rough-grained value representations that lack fine-grained conditioning on state information, struggling under complex and OOD conditions. We propose DFPO (Distributional Value Flow Policy Optimization with Conditional Risk and Consistency Control), a robust distributional RL framework that models values as continuous flows across time steps. By scaling value modeling through learning of a value flow field instead of isolated quantile predictions, DFPO captures richer state information for more accurate advantage estimation. To stabilize training under noisy feedback, DFPO further integrates conditional risk control and consistency constraints along value flow trajectories. Experiments on dialogue, math reasoning, and scientific tasks show that DFPO outperforms PPO, FlowRL, and other robust baselines under noisy supervision, achieving improved training stability and generalization.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05890.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05890",
    "published": "2026-02-05T17:07:42Z",
    "updated": "2026-02-05T17:07:42Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05887",
    "title": "Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting",
    "authors": [
      "Tianqi Shen",
      "Jinji Yang",
      "Junze He",
      "Kunhan Gao",
      "Ziye Ma"
    ],
    "abstract": "Low-rank matrix sensing is a fundamental yet challenging nonconvex problem whose optimization landscape typically contains numerous spurious local minima, making it difficult for gradient-based optimizers to converge to the global optimum. Recent work has shown that over-parameterization via tensor lifting can convert such local minima into strict saddle points, an insight that also partially explains why massive scaling can improve generalization and performance in modern machine learning. Motivated by this observation, we propose a Simulated Oracle Direction (SOD) escape mechanism that simulates the landscape and escape direction of the over-parametrized space, without resorting to actually lifting the problem, since that would be computationally intractable. In essence, we designed a mathematical framework to project over-parametrized escape directions onto the original parameter space to guarantee a strict decrease of objective value from existing local minima. To the best of the our knowledge, this represents the first deterministic framework that could escape spurious local minima with guarantee, especially without using random perturbations or heuristic estimates. Numerical experiments demonstrate that our framework reliably escapes local minima and facilitates convergence to global optima, while incurring minimal computational cost when compared to explicit tensor over-parameterization. We believe this framework has non-trivial implications for nonconvex optimization beyond matrix sensing, by showcasing how simulated over-parameterization can be leveraged to tame challenging optimization landscapes.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05887.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05887",
    "published": "2026-02-05T17:05:02Z",
    "updated": "2026-02-05T17:05:02Z",
    "comment": "48 pages, 10 figures, 5 tables. Submitted to Mathematical Programming",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05885",
    "title": "Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations",
    "authors": [
      "Wei Liu",
      "Jiawei Xu",
      "Yingru Li",
      "Longtao Zheng",
      "Tianjian Li",
      "Qian Liu",
      "Junxian He"
    ],
    "abstract": "High-quality kernel is critical for scalable AI systems, and enabling LLMs to generate such code would advance AI development. However, training LLMs for this task requires sufficient data, a robust environment, and the process is often vulnerable to reward hacking and lazy optimization. In these cases, models may hack training rewards and prioritize trivial correctness over meaningful speedup. In this paper, we systematically study reinforcement learning (RL) for kernel generation. We first design KernelGYM, a robust distributed GPU environment that supports reward hacking check, data collection from multi-turn interactions and long-term RL training. Building on KernelGYM, we investigate effective multi-turn RL methods and identify a biased policy gradient issue caused by self-inclusion in GRPO. To solve this, we propose Turn-level Reinforce-Leave-One-Out (TRLOO) to provide unbiased advantage estimation for multi-turn RL. To alleviate lazy optimization, we incorporate mismatch correction for training stability and introduce Profiling-based Rewards (PR) and Profiling-based Rejection Sampling (PRS) to overcome the issue. The trained model, Dr.Kernel-14B, reaches performance competitive with Claude-4.5-Sonnet in Kernelbench. Finally, we study sequential test-time scaling for Dr.Kernel-14B. On the KernelBench Level-2 subset, 31.6% of the generated kernels achieve at least a 1.2x speedup over the Torch reference, surpassing Claude-4.5-Sonnet (26.7%) and GPT-5 (28.6%). When selecting the best candidate across all turns, this 1.2x speedup rate further increases to 47.8%. All resources, including environment, training code, models, and dataset, are included in https://www.github.com/hkust-nlp/KernelGYM.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05885.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05885",
    "published": "2026-02-05T17:01:09Z",
    "updated": "2026-02-05T17:01:09Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05884",
    "title": "Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views",
    "authors": [
      "Gino E. Jansen",
      "Carolina BrÃ¡s",
      "R. Nils Planken",
      "Mark J. Schuuring",
      "Berto J. Bouma",
      "Ivana IÅ¡gum"
    ],
    "abstract": "Accurate 3D representations of cardiac structures allow quantitative analysis of anatomy and function. In this work, we propose a method for reconstructing complete 3D cardiac shapes from segmentations of sparse planes in CT angiography (CTA) for application in 2D transthoracic echocardiography (TTE). Our method uses a neural implicit function to reconstruct the 3D shape of the cardiac chambers and left-ventricle myocardium from sparse CTA planes. To investigate the feasibility of achieving 3D reconstruction from 2D TTE, we select planes that mimic the standard apical 2D TTE views. During training, a multi-layer perceptron learns shape priors from 3D segmentations of the target structures in CTA. At test time, the network reconstructs 3D cardiac shapes from segmentations of TTE-mimicking CTA planes by jointly optimizing the latent code and the rigid transforms that map the observed planes into 3D space. For each heart, we simulate four realistic apical views, and we compare reconstructed multi-class volumes with the reference CTA volumes. On a held-out set of CTA segmentations, our approach achieves an average Dice coefficient of 0.86 $\\pm$ 0.04 across all structures. Our method also achieves markedly lower volume errors than the clinical standard, Simpson's biplane rule: 4.88 $\\pm$ 4.26 mL vs. 8.14 $\\pm$ 6.04 mL, respectively, for the left ventricle; and 6.40 $\\pm$ 7.37 mL vs. 37.76 $\\pm$ 22.96 mL, respectively, for the left atrium. This suggests that our approach offers a viable route to more accurate 3D chamber quantification in 2D transthoracic echocardiography.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CE"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05884.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05884",
    "published": "2026-02-05T17:00:59Z",
    "updated": "2026-02-05T17:00:59Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05883",
    "title": "A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges",
    "authors": [
      "Philippe J. Giabbanelli"
    ],
    "abstract": "Large language models (LLMs) have rapidly become familiar tools to researchers and practitioners. Concepts such as prompting, temperature, or few-shot examples are now widely recognized, and LLMs are increasingly used in Modeling & Simulation (M&S) workflows. However, practices that appear straightforward may introduce subtle issues, unnecessary complexity, or may even lead to inferior results. Adding more data can backfire (e.g., deteriorating performance through model collapse or inadvertently wiping out existing guardrails), spending time on fine-tuning a model can be unnecessary without a prior assessment of what it already knows, setting the temperature to 0 is not sufficient to make LLMs deterministic, providing a large volume of M&S data as input can be excessive (LLMs cannot attend to everything) but naive simplifications can lose information. We aim to provide comprehensive and practical guidance on how to use LLMs, with an emphasis on M&S applications. We discuss common sources of confusion, including non-determinism, knowledge augmentation (including RAG and LoRA), decomposition of M&S data, and hyper-parameter settings. We emphasize principled design choices, diagnostic strategies, and empirical evaluation, with the goal of helping modelers make informed decisions about when, how, and whether to rely on LLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05883.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05883",
    "published": "2026-02-05T17:00:07Z",
    "updated": "2026-02-05T17:00:07Z",
    "comment": "Book chapter. Accepted in Artificial Intelligence in Modeling and Simulation, Philippe J. Giabbanelli and Istvan David (eds). Series on Simulation Foundations, Methods and Applications. Springer, Cham. Series ISSN: 2195-2817",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05882",
    "title": "EoCD: Encoder only Remote Sensing Change Detection",
    "authors": [
      "Mubashir Noman",
      "Mustansar Fiaz",
      "Hiyam Debary",
      "Abdul Hannan",
      "Shah Nawaz",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ],
    "abstract": "Being a cornerstone of temporal analysis, change detection has been playing a pivotal role in modern earth observation. Existing change detection methods rely on the Siamese encoder to individually extract temporal features followed by temporal fusion. Subsequently, these methods design sophisticated decoders to improve the change detection performance without taking into consideration the complexity of the model. These aforementioned issues intensify the overall computational cost as well as the network's complexity which is undesirable. Alternatively, few methods utilize the early fusion scheme to combine the temporal images. These methods prevent the extra overhead of Siamese encoder, however, they also rely on sophisticated decoders for better performance. In addition, these methods demonstrate inferior performance as compared to late fusion based methods. To bridge these gaps, we introduce encoder only change detection (EoCD) that is a simple and effective method for the change detection task. The proposed method performs the early fusion of the temporal data and replaces the decoder with a parameter-free multiscale feature fusion module thereby significantly reducing the overall complexity of the model. EoCD demonstrate the optimal balance between the change detection performance and the prediction speed across a variety of encoder architectures. Additionally, EoCD demonstrate that the performance of the model is predominantly dependent on the encoder network, making the decoder an additional component. Extensive experimentation on four challenging change detection datasets reveals the effectiveness of the proposed method.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05882.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05882",
    "published": "2026-02-05T16:58:42Z",
    "updated": "2026-02-05T16:58:42Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05880",
    "title": "Contour Refinement using Discrete Diffusion in Low Data Regime",
    "authors": [
      "Fei Yu Guan",
      "Ian Keefe",
      "Sophie Wilkinson",
      "Daniel D. B. Perrakis",
      "Steven Waslander"
    ],
    "abstract": "Boundary detection of irregular and translucent objects is an important problem with applications in medical imaging, environmental monitoring and manufacturing, where many of these applications are plagued with scarce labeled data and low in situ computational resources. While recent image segmentation studies focus on segmentation mask alignment with ground-truth, the task of boundary detection remains understudied, especially in the low data regime. In this work, we present a lightweight discrete diffusion contour refinement pipeline for robust boundary detection in the low data regime. We use a Convolutional Neural Network(CNN) architecture with self-attention layers as the core of our pipeline, and condition on a segmentation mask, iteratively denoising a sparse contour representation. We introduce multiple novel adaptations for improved low-data efficacy and inference efficiency, including using a simplified diffusion process, a customized model architecture, and minimal post processing to produce a dense, isolated contour given a dataset of size <500 training images. Our method outperforms several SOTA baselines on the medical imaging dataset KVASIR, is competitive on HAM10K and our custom wildfire dataset, Smoke, while improving inference framerate by 3.5X.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05880.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05880",
    "published": "2026-02-05T16:55:08Z",
    "updated": "2026-02-05T16:55:08Z",
    "comment": "CRV 2026, 8 pages, 6 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05879",
    "title": "EuroLLM-22B: Technical Report",
    "authors": [
      "Miguel Moura Ramos",
      "Duarte M. Alves",
      "Hippolyte Gisserot-Boukhlef",
      "JoÃ£o Alves",
      "Pedro Henrique Martins",
      "Patrick Fernandes",
      "JosÃ© Pombal",
      "Nuno M. Guerreiro",
      "Ricardo Rei",
      "Nicolas Boizard",
      "Amin Farajian",
      "Mateusz Klimaszewski",
      "JosÃ© G. C. de Souza",
      "Barry Haddow",
      "FranÃ§ois Yvon",
      "Pierre Colombo",
      "Alexandra Birch",
      "AndrÃ© F. T. Martins"
    ],
    "abstract": "This report presents EuroLLM-22B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-22B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. Across a broad set of multilingual benchmarks, EuroLLM-22B demonstrates strong performance in reasoning, instruction following, and translation, achieving results competitive with models of comparable size. To support future research, we release our base and instruction-tuned models, our multilingual web pretraining data and updated EuroBlocks instruction datasets, as well as our pre-training and evaluation codebases.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05879.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05879",
    "published": "2026-02-05T16:53:47Z",
    "updated": "2026-02-05T16:53:47Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05877",
    "title": "Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy",
    "authors": [
      "Lukas Stappen",
      "Ahmet Erkan Turan",
      "Johann Hagerer",
      "Georg Groh"
    ],
    "abstract": "The integration of Large Language Model (LLM)-based conversational agents into vehicles creates novel security challenges at the intersection of agentic AI, automotive safety, and inter-agent communication. As these intelligent assistants coordinate with external services via protocols such as Google's Agent-to-Agent (A2A), they establish attack surfaces where manipulations can propagate through natural language payloads, potentially causing severe consequences ranging from driver distraction to unauthorized vehicle control. Existing AI security frameworks, while foundational, lack the rigorous \"separation of concerns\" standard in safety-critical systems engineering by co-mingling the concepts of what is being protected (assets) with how it is attacked (attack paths). This paper addresses this methodological gap by proposing a threat modeling framework called AgentHeLLM (Agent Hazard Exploration for LLM Assistants) that formally separates asset identification from attack path analysis. We introduce a human-centric asset taxonomy derived from harm-oriented \"victim modeling\" and inspired by the Universal Declaration of Human Rights, and a formal graph-based model that distinguishes poison paths (malicious data propagation) from trigger paths (activation actions). We demonstrate the framework's practical applicability through an open-source attack path suggestion tool AgentHeLLM Attack Path Generator that automates multi-stage threat discovery using a bi-level search strategy.",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05877.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05877",
    "published": "2026-02-05T16:53:41Z",
    "updated": "2026-02-05T16:53:41Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05875",
    "title": "Beyond Manual Planning: Seating Allocation for Large Organizations",
    "authors": [
      "Anton Ipsen",
      "Michael Cashmore",
      "Kirsty Fielding",
      "Nicolas Marchesotti",
      "Parisa Zehtabi",
      "Daniele Magazzeni",
      "Manuela Veloso"
    ],
    "abstract": "We introduce the Hierarchical Seating Allocation Problem (HSAP) which addresses the optimal assignment of hierarchically structured organizational teams to physical seating arrangements on a floor plan. This problem is driven by the necessity for large organizations with large hierarchies to ensure that teams with close hierarchical relationships are seated in proximity to one another, such as ensuring a research group occupies a contiguous area. Currently, this problem is managed manually leading to infrequent and suboptimal replanning efforts. To alleviate this manual process, we propose an end-to-end framework to solve the HSAP. A scalable approach to calculate the distance between any pair of seats using a probabilistic road map (PRM) and rapidly-exploring random trees (RRT) which is combined with heuristic search and dynamic programming approach to solve the HSAP using integer programming. We demonstrate our approach under different sized instances by evaluating the PRM framework and subsequent allocations both quantitatively and qualitatively.",
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05875.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05875",
    "published": "2026-02-05T16:52:44Z",
    "updated": "2026-02-05T16:52:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05874",
    "title": "xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection",
    "authors": [
      "AdriÃ¡n GirÃ³n",
      "Pablo Miralles",
      "Javier Huertas-Tato",
      "Sergio D'Antonio",
      "David Camacho"
    ],
    "abstract": "Hate speech detection is commonly framed as a direct binary classification problem despite being a composite concept defined through multiple interacting factors that vary across legal frameworks, platform policies, and annotation guidelines. As a result, supervised models often overfit dataset-specific definitions and exhibit limited robustness under domain shift and annotation noise.   We introduce xList-Hate, a diagnostic framework that decomposes hate speech detection into a checklist of explicit, concept-level questions grounded in widely shared normative criteria. Each question is independently answered by a large language model (LLM), producing a binary diagnostic representation that captures hateful content features without directly predicting the final label. These diagnostic signals are then aggregated by a lightweight, fully interpretable decision tree, yielding transparent and auditable predictions.   We evaluate it across multiple hate speech benchmarks and model families, comparing it against zero-shot LLM classification and in-domain supervised fine-tuning. While supervised methods typically maximize in-domain performance, we consistently improves cross-dataset robustness and relative performance under domain shift. In addition, qualitative analysis of disagreement cases provides evidence that the framework can be less sensitive to certain forms of annotation inconsistency and contextual ambiguity. Crucially, the approach enables fine-grained interpretability through explicit decision paths and factor-level analysis.   Our results suggest that reframing hate speech detection as a diagnostic reasoning task, rather than a monolithic classification problem, provides a robust, explainable, and extensible alternative for content moderation.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05874.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05874",
    "published": "2026-02-05T16:51:56Z",
    "updated": "2026-02-05T16:51:56Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05873",
    "title": "Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks",
    "authors": [
      "Minyoung Kim"
    ],
    "abstract": "Bayesian (deep) neural networks (BNN) are often more attractive than the mainstream point-estimate vanilla deep learning in various aspects including uncertainty quantification, robustness to noise, resistance to overfitting, and more. The variational inference (VI) is one of the most widely adopted approximate inference methods. Whereas the ELBO-based variational free energy method is a dominant choice in the literature, in this paper we introduce a score-based alternative for BNN variational inference. Although there have been quite a few score-based variational inference methods proposed in the community, most are not adequate for large-scale BNNs for various computational and technical reasons. We propose a novel scalable VI method where the learning objective combines the score matching loss and the proximal penalty term in iterations, which helps our method avoid the reparametrized sampling, and allows for noisy unbiased mini-batch scores through stochastic gradients. This in turn makes our method scalable to large-scale neural networks including Vision Transformers, and allows for richer variational density families. On several benchmarks including visual recognition and time-series forecasting with large-scale deep networks, we empirically show the effectiveness of our approach.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05873.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05873",
    "published": "2026-02-05T16:51:07Z",
    "updated": "2026-02-05T16:51:07Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05871",
    "title": "Pathwise Test-Time Correction for Autoregressive Long Video Generation",
    "authors": [
      "Xunzhi Xiang",
      "Zixuan Duan",
      "Guiyu Zhang",
      "Haiyu Zhang",
      "Zhe Gao",
      "Junta Wu",
      "Shaofeng Zhang",
      "Tengfei Wang",
      "Qi Fan",
      "Chunchao Guo"
    ],
    "abstract": "Distilled autoregressive diffusion models facilitate real-time short video synthesis but suffer from severe error accumulation during long-sequence generation. While existing Test-Time Optimization (TTO) methods prove effective for images or short clips, we identify that they fail to mitigate drift in extended sequences due to unstable reward landscapes and the hypersensitivity of distilled parameters. To overcome these limitations, we introduce Test-Time Correction (TTC), a training-free alternative. Specifically, TTC utilizes the initial frame as a stable reference anchor to calibrate intermediate stochastic states along the sampling trajectory. Extensive experiments demonstrate that our method seamlessly integrates with various distilled models, extending generation lengths with negligible overhead while matching the quality of resource-intensive training-based methods on 30-second benchmarks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05871.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05871",
    "published": "2026-02-05T16:50:39Z",
    "updated": "2026-02-05T16:50:39Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05863",
    "title": "Constrained Group Relative Policy Optimization",
    "authors": [
      "Roger Girgis",
      "Rodrigue de Schaetzen",
      "Luke Rowe",
      "AzalÃ©e Robitaille",
      "Christopher Pal",
      "Liam Paull"
    ],
    "abstract": "While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05863.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05863",
    "published": "2026-02-05T16:44:23Z",
    "updated": "2026-02-05T16:44:23Z",
    "comment": "16 pages, 6 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05861",
    "title": "CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs",
    "authors": [
      "Seyedmasoud Mousavi",
      "Ruomeng Xu",
      "Xiaojing Zhu"
    ],
    "abstract": "Graph-structured data is ubiquitous and powerful in representing complex relationships in many online platforms. While graph neural networks (GNNs) are widely used to learn from such data, counterfactual graph learning has emerged as a promising approach to improve model interpretability. Counterfactual explanation research focuses on identifying a counterfactual graph that is similar to the original but leads to different predictions. These explanations optimize two objectives simultaneously: the sparsity of changes in the counterfactual graph and the validity of its predictions. Building on these qualitative optimization goals, this paper introduces CFRecs, a novel framework that transforms counterfactual explanations into actionable insights. CFRecs employs a two-stage architecture consisting of a graph neural network (GNN) and a graph variational auto-encoder (Graph-VAE) to strategically propose minimal yet high-impact changes in graph structure and node attributes to drive desirable outcomes in recommender systems. We apply CFRecs to Zillow's graph-structured data to deliver actionable recommendations for both home buyers and sellers with the goal of helping them navigate the competitive housing market and achieve their homeownership goals. Experimental results on Zillow's user-listing interaction data demonstrate the effectiveness of CFRecs, which also provides a fresh perspective on recommendations using counterfactual reasoning in graphs.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05861.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05861",
    "published": "2026-02-05T16:42:51Z",
    "updated": "2026-02-05T16:42:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05859",
    "title": "DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders",
    "authors": [
      "Xu Wang",
      "Bingqing Jiang",
      "Yu Wan",
      "Baosong Yang",
      "Lingpeng Kong",
      "Difan Zou"
    ],
    "abstract": "Sparse autoencoders (SAEs) have become a standard tool for mechanistic interpretability in autoregressive large language models (LLMs), enabling researchers to extract sparse, human-interpretable features and intervene on model behavior. Recently, as diffusion language models (DLMs) have become an increasingly promising alternative to the autoregressive LLMs, it is essential to develop tailored mechanistic interpretability tools for this emerging class of models. In this work, we present DLM-Scope, the first SAE-based interpretability framework for DLMs, and demonstrate that trained Top-K SAEs can faithfully extract interpretable features. Notably, we find that inserting SAEs affects DLMs differently than autoregressive LLMs: while SAE insertion in LLMs typically incurs a loss penalty, in DLMs it can reduce cross-entropy loss when applied to early layers, a phenomenon absent or markedly weaker in LLMs. Additionally, SAE features in DLMs enable more effective diffusion-time interventions, often outperforming LLM steering. Moreover, we pioneer certain new SAE-based research directions for DLMs: we show that SAEs can provide useful signals for DLM decoding order; and the SAE features are stable during the post-training phase of DLMs. Our work establishes a foundation for mechanistic interpretability in DLMs and shows a great potential of applying SAEs to DLM-related tasks and algorithms.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05859.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05859",
    "published": "2026-02-05T16:41:25Z",
    "updated": "2026-02-05T16:41:25Z",
    "comment": "23 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05857",
    "title": "BABE: Biology Arena BEnchmark",
    "authors": [
      "Junting Zhou",
      "Jin Chen",
      "Linfeng Hao",
      "Denghui Cao",
      "Zheyu Wang",
      "Qiguang Chen",
      "Chaoyou Fu",
      "Jiaze Chen",
      "Yuchen Wu",
      "Ge Zhang",
      "Mingxuan Wang",
      "Wenhao Huang",
      "Tong Yang"
    ],
    "abstract": "The rapid evolution of large language models (LLMs) has expanded their capabilities from basic dialogue to advanced scientific reasoning. However, existing benchmarks in biology often fail to assess a critical skill required of researchers: the ability to integrate experimental results with contextual knowledge to derive meaningful conclusions. To address this gap, we introduce BABE(Biology Arena BEnchmark), a comprehensive benchmark designed to evaluate the experimental reasoning capabilities of biological AI systems. BABE is uniquely constructed from peer-reviewed research papers and real-world biological studies, ensuring that tasks reflect the complexity and interdisciplinary nature of actual scientific inquiry. BABE challenges models to perform causal reasoning and cross-scale inference. Our benchmark provides a robust framework for assessing how well AI systems can reason like practicing scientists, offering a more authentic measure of their potential to contribute to biological research.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05857.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05857",
    "published": "2026-02-05T16:39:20Z",
    "updated": "2026-02-05T16:39:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05853",
    "title": "RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference",
    "authors": [
      "Siran Liu",
      "Guoxia Wang",
      "Sa Wang",
      "Jinle Zeng",
      "HaoYang Xie",
      "Siyu Lou",
      "JiaBin Yang",
      "DianHai Yu",
      "Haifeng Wang",
      "Chao Yang"
    ],
    "abstract": "The quadratic complexity of attention mechanisms poses a critical bottleneck for large language models processing long contexts. While dynamic sparse attention methods offer input-adaptive efficiency, they face fundamental trade-offs: requiring preprocessing, lacking global evaluation, violating query independence, or incurring high computational overhead. We present RRAttention, a novel dynamic sparse attention method that simultaneously achieves all desirable properties through a head \\underline{r}ound-\\underline{r}obin (RR) sampling strategy. By rotating query sampling positions across attention heads within each stride, RRAttention maintains query independence while enabling efficient global pattern discovery with stride-level aggregation. Our method reduces complexity from $O(L^2)$ to $O(L^2/S^2)$ and employs adaptive Top-$Ï$ selection for optimal sparsity. Extensive experiments on natural language understanding (HELMET) and multimodal video comprehension (Video-MME) demonstrate that RRAttention recovers over 99\\% of full attention performance while computing only half of the attention blocks, achieving 2.4$\\times$ speedup at 128K context length and outperforming existing dynamic sparse attention methods.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05853.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05853",
    "published": "2026-02-05T16:37:41Z",
    "updated": "2026-02-05T16:37:41Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05852",
    "title": "Exact Recovery in the Data Block Model",
    "authors": [
      "Amir R. Asadi",
      "Akbar Davoodi",
      "Ramin Javadi",
      "Farzad Parvaresh"
    ],
    "abstract": "Community detection in networks is a fundamental problem in machine learning and statistical inference, with applications in social networks, biological systems, and communication networks. The stochastic block model (SBM) serves as a canonical framework for studying community structure, and exact recovery, identifying the true communities with high probability, is a central theoretical question. While classical results characterize the phase transition for exact recovery based solely on graph connectivity, many real-world networks contain additional data, such as node attributes or labels. In this work, we study exact recovery in the Data Block Model (DBM), an SBM augmented with node-associated data, as formalized by Asadi, Abbe, and VerdÃº (2017). We introduce the Chernoff--TV divergence and use it to characterize a sharp exact recovery threshold for the DBM. We further provide an efficient algorithm that achieves this threshold, along with a matching converse result showing impossibility below the threshold. Finally, simulations validate our findings and demonstrate the benefits of incorporating vertex data as side information in community detection.",
    "categories": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05852.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05852",
    "published": "2026-02-05T16:36:57Z",
    "updated": "2026-02-05T16:36:57Z",
    "comment": "35 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05849",
    "title": "Visualizing the loss landscapes of physics-informed neural networks",
    "authors": [
      "Conor Rowan",
      "Finn Murphy-Blanchard"
    ],
    "abstract": "Training a neural network requires navigating a high-dimensional, non-convex loss surface to find parameters that minimize this loss. In many ways, it is surprising that optimizers such as stochastic gradient descent and ADAM can reliably locate minima which perform well on both the training and test data. To understand the success of training, a \"loss landscape\" community has emerged to study the geometry of the loss function and the dynamics of optimization, often using visualization techniques. However, these loss landscape studies have mostly been limited to machine learning for image classification. In the newer field of physics-informed machine learning, little work has been conducted to visualize the landscapes of losses defined not by regression to large data sets, but by differential operators acting on state fields discretized by neural networks. In this work, we provide a comprehensive review of the loss landscape literature, as well as a discussion of the few existing physics-informed works which investigate the loss landscape. We then use a number of the techniques we survey to empirically investigate the landscapes defined by the Deep Ritz and squared residual forms of the physics loss function. We find that the loss landscapes of physics-informed neural networks have many of the same properties as the data-driven classification problems studied in the literature. Unexpectedly, we find that the two formulations of the physics loss often give rise to similar landscapes, which appear smooth, well-conditioned, and convex in the vicinity of the solution. The purpose of this work is to introduce the loss landscape perspective to the scientific machine learning community, compare the Deep Ritz and the strong form losses, and to challenge prevailing intuitions about the complexity of the loss landscapes of physics-informed networks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05849.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05849",
    "published": "2026-02-05T16:35:51Z",
    "updated": "2026-02-05T16:35:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05847",
    "title": "OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention",
    "authors": [
      "Zhangquan Chen",
      "Jiale Tao",
      "Ruihuang Li",
      "Yihao Hu",
      "Ruitao Chen",
      "Zhantao Yang",
      "Xinlei Yu",
      "Haodong Jing",
      "Manyuan Zhang",
      "Shuai Shao",
      "Biao Wang",
      "Qinglin Lu",
      "Ruqi Huang"
    ],
    "abstract": "While humans perceive the world through diverse modalities that operate synergistically to support a holistic understanding of their surroundings, existing omnivideo models still face substantial challenges on audio-visual understanding tasks. In this paper, we propose OmniVideo-R1, a novel reinforced framework that improves mixed-modality reasoning. OmniVideo-R1 empowers models to \"think with omnimodal cues\" by two key strategies: (1) query-intensive grounding based on self-supervised learning paradigms; and (2) modality-attentive fusion built upon contrastive learning paradigms. Extensive experiments on multiple benchmarks demonstrate that OmniVideo-R1 consistently outperforms strong baselines, highlighting its effectiveness and robust generalization capabilities.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05847.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05847",
    "published": "2026-02-05T16:35:19Z",
    "updated": "2026-02-05T16:35:19Z",
    "comment": "19 pages, 12 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05845",
    "title": "Self-Supervised Learning with a Multi-Task Latent Space Objective",
    "authors": [
      "Pierre-FranÃ§ois De Plaen",
      "Abhishek Jha",
      "Luc Van Gool",
      "Tinne Tuytelaars",
      "Marc Proesmans"
    ],
    "abstract": "Self-supervised learning (SSL) methods based on Siamese networks learn visual representations by aligning different views of the same image. The multi-crop strategy, which incorporates small local crops to global ones, enhances many SSL frameworks but causes instability in predictor-based architectures such as BYOL, SimSiam, and MoCo v3. We trace this failure to the shared predictor used across all views and demonstrate that assigning a separate predictor to each view type stabilizes multi-crop training, resulting in significant performance gains. Extending this idea, we treat each spatial transformation as a distinct alignment task and add cutout views, where part of the image is masked before encoding. This yields a simple multi-task formulation of asymmetric Siamese SSL that combines global, local, and masked views into a single framework. The approach is stable, generally applicable across backbones, and consistently improves the performance of ResNet and ViT models on ImageNet.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05845.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05845",
    "published": "2026-02-05T16:33:30Z",
    "updated": "2026-02-05T16:33:30Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05843",
    "title": "OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions",
    "authors": [
      "Fangzhi Xu",
      "Hang Yan",
      "Qiushi Sun",
      "Jinyang Wu",
      "Zixian Huang",
      "Muye Huang",
      "Jingyang Gong",
      "Zichen Ding",
      "Kanzhi Cheng",
      "Yian Wang",
      "Xinyu Che",
      "Zeyi Sun",
      "Jian Zhang",
      "Zhangyue Yin",
      "Haoran Luo",
      "Xuanjing Huang",
      "Ben Kao",
      "Jun Liu",
      "Qika Lin"
    ],
    "abstract": "The rapid advancement of Large Language Models (LLMs) has catalyzed the development of autonomous agents capable of navigating complex environments. However, existing evaluations primarily adopt a deductive paradigm, where agents execute tasks based on explicitly provided rules and static goals, often within limited planning horizons. Crucially, this neglects the inductive necessity for agents to discover latent transition laws from experience autonomously, which is the cornerstone for enabling agentic foresight and sustaining strategic coherence. To bridge this gap, we introduce OdysseyArena, which re-centers agent evaluation on long-horizon, active, and inductive interactions. We formalize and instantiate four primitives, translating abstract transition dynamics into concrete interactive environments. Building upon this, we establish OdysseyArena-Lite for standardized benchmarking, providing a set of 120 tasks to measure an agent's inductive efficiency and long-horizon discovery. Pushing further, we introduce OdysseyArena-Challenge to stress-test agent stability across extreme interaction horizons (e.g., > 200 steps). Extensive experiments on 15+ leading LLMs reveal that even frontier models exhibit a deficiency in inductive scenarios, identifying a critical bottleneck in the pursuit of autonomous discovery in complex environments. Our code and data are available at https://github.com/xufangzhi/Odyssey-Arena",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05843.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05843",
    "published": "2026-02-05T16:31:43Z",
    "updated": "2026-02-05T16:31:43Z",
    "comment": "34 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05842",
    "title": "Reinforcement World Model Learning for LLM-based Agents",
    "authors": [
      "Xiao Yu",
      "Baolin Peng",
      "Ruize Xu",
      "Yelong Shen",
      "Pengcheng He",
      "Suman Nath",
      "Nikhil Singh",
      "Jiangfeng Gao",
      "Zhou Yu"
    ],
    "abstract": "Large language models (LLMs) have achieved strong performance in language-centric tasks. However, in agentic settings, LLMs often struggle to anticipate action consequences and adapt to environment dynamics, highlighting the need for world-modeling capabilities in LLM-based agents. We propose Reinforcement World Model Learning (RWML), a self-supervised method that learns action-conditioned world models for LLM-based agents on textual states using sim-to-real gap rewards. Our method aligns simulated next states produced by the model with realized next states observed from the environment, encouraging consistency between internal world simulations and actual environment dynamics in a pre-trained embedding space. Unlike next-state token prediction, which prioritizes token-level fidelity (i.e., reproducing exact wording) over semantic equivalence and can lead to model collapse, our method provides a more robust training signal and is empirically less susceptible to reward hacking than LLM-as-a-judge. We evaluate our method on ALFWorld and $Ï^2$ Bench and observe significant gains over the base model, despite being entirely self-supervised. When combined with task-success rewards, our method outperforms direct task-success reward RL by 6.9 and 5.7 points on ALFWorld and $Ï^2$ Bench respectively, while matching the performance of expert-data training.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05842.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05842",
    "published": "2026-02-05T16:30:08Z",
    "updated": "2026-02-05T16:30:08Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05833",
    "title": "Synthesizing Realistic Test Data without Breaking Privacy",
    "authors": [
      "Laura Plein",
      "Alexi Turcotte",
      "Arina Hallemans",
      "Andreas Zeller"
    ],
    "abstract": "There is a need for synthetic training and test datasets that replicate statistical distributions of original datasets without compromising their confidentiality. A lot of research has been done in leveraging Generative Adversarial Networks (GANs) for synthetic data generation. However, the resulting models are either not accurate enough or are still vulnerable to membership inference attacks (MIA) or dataset reconstruction attacks since the original data has been leveraged in the training process. In this paper, we explore the feasibility of producing a synthetic test dataset with the same statistical properties as the original one, with only indirectly leveraging the original data in the generation process. The approach is inspired by GANs, with a generation step and a discrimination step. However, in our approach, we use a test generator (a fuzzer) to produce test data from an input specification, preserving constraints set by the original data; a discriminator model determines how close we are to the original data. By evolving samples and determining \"good samples\" with the discriminator, we can generate privacy-preserving data that follows the same statistical distributions are the original dataset, leading to a similar utility as the original data. We evaluated our approach on four datasets that have been used to evaluate the state-of-the-art techniques. Our experiments highlight the potential of our approach towards generating synthetic datasets that have high utility while preserving privacy.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05833.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05833",
    "published": "2026-02-05T16:22:01Z",
    "updated": "2026-02-05T16:22:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05832",
    "title": "UI-Mem: Self-Evolving Experience Memory for Online Reinforcement Learning in Mobile GUI Agents",
    "authors": [
      "Han Xiao",
      "Guozhi Wang",
      "Hao Wang",
      "Shilong Liu",
      "Yuxiang Chai",
      "Yue Pan",
      "Yufeng Zhou",
      "Xiaoxin Chen",
      "Yafei Wen",
      "Hongsheng Li"
    ],
    "abstract": "Online Reinforcement Learning (RL) offers a promising paradigm for enhancing GUI agents through direct environment interaction. However, its effectiveness is severely hindered by inefficient credit assignment in long-horizon tasks and repetitive errors across tasks due to the lack of experience transfer. To address these challenges, we propose UI-Mem, a novel framework that enhances GUI online RL with a Hierarchical Experience Memory. Unlike traditional replay buffers, our memory accumulates structured knowledge, including high-level workflows, subtask skills, and failure patterns. These experiences are stored as parameterized templates that enable cross-task and cross-application transfer. To effectively integrate memory guidance into online RL, we introduce Stratified Group Sampling, which injects varying levels of guidance across trajectories within each rollout group to maintain outcome diversity, driving the unguided policy toward internalizing guided behaviors. Furthermore, a Self-Evolving Loop continuously abstracts novel strategies and errors to keep the memory aligned with the agent's evolving policy. Experiments on online GUI benchmarks demonstrate that UI-Mem significantly outperforms traditional RL baselines and static reuse strategies, with strong generalization to unseen applications. Project page: https://ui-mem.github.io",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05832.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05832",
    "published": "2026-02-05T16:21:43Z",
    "updated": "2026-02-05T16:21:43Z",
    "comment": "23 pages, 16 figures. Project page: https://ui-mem.github.io",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05830",
    "title": "Learning Compact Boolean Networks",
    "authors": [
      "Shengpu Wang",
      "Yuhao Mao",
      "Yani Zhang",
      "Martin Vechev"
    ],
    "abstract": "Floating-point neural networks dominate modern machine learning but incur substantial inference cost, motivating interest in Boolean networks for resource-constrained settings. However, learning compact and accurate Boolean networks is challenging due to their combinatorial nature. In this work, we address this challenge from three different angles: learned connections, compact convolutions and adaptive discretization. First, we propose a novel strategy to learn efficient connections with no additional parameters and negligible computational overhead. Second, we introduce a novel convolutional Boolean architecture that exploits the locality with reduced number of Boolean operations than existing methods. Third, we propose an adaptive discretization strategy to reduce the accuracy drop when converting a continuous-valued network into a Boolean one. Extensive results on standard vision benchmarks demonstrate that the Pareto front of accuracy vs. computation of our method significantly outperforms prior state-of-the-art, achieving better accuracy with up to 37x fewer Boolean operations.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05830.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05830",
    "published": "2026-02-05T16:19:59Z",
    "updated": "2026-02-05T16:19:59Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05829",
    "title": "Weaver: End-to-End Agentic System Training for Video Interleaved Reasoning",
    "authors": [
      "Yudi Shi",
      "Shangzhe Di",
      "Qirui Chen",
      "Qinian Wang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Weidi Xie"
    ],
    "abstract": "Video reasoning constitutes a comprehensive assessment of a model's capabilities, as it demands robust perceptual and interpretive skills, thereby serving as a means to explore the boundaries of model performance. While recent research has leveraged text-centric Chain-of-Thought reasoning to augment these capabilities, such approaches frequently suffer from representational mismatch and restricted by limited perceptual acuity. To address these limitations, we propose Weaver, a novel, end-to-end trainable multimodal reasoning agentic system. Weaver empowers its policy model to dynamically invoke diverse tools throughout the reasoning process, enabling progressive acquisition of crucial visual cues and construction of authentic multimodal reasoning trajectories. Furthermore, we integrate a reinforcement learning algorithm to allow the system to freely explore strategies for employing and combining these tools with trajectory-free data. Extensive experiments demonstrate that our system, Weaver, enhances performance on several complex video reasoning benchmarks, particularly those involving long videos.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05829.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05829",
    "published": "2026-02-05T16:19:41Z",
    "updated": "2026-02-05T16:19:41Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05827",
    "title": "Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation",
    "authors": [
      "Hai Zhang",
      "Siqi Liang",
      "Li Chen",
      "Yuxian Li",
      "Yukuan Xu",
      "Yichao Zhong",
      "Fu Zhang",
      "Hongyang Li"
    ],
    "abstract": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05827.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05827",
    "published": "2026-02-05T16:16:13Z",
    "updated": "2026-02-05T16:16:13Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05822",
    "title": "NVS-HO: A Benchmark for Novel View Synthesis of Handheld Objects",
    "authors": [
      "Musawar Ali",
      "Manuel Carranza-GarcÃ­a",
      "Nicola Fioraio",
      "Samuele Salti",
      "Luigi Di Stefano"
    ],
    "abstract": "We propose NVS-HO, the first benchmark designed for novel view synthesis of handheld objects in real-world environments using only RGB inputs. Each object is recorded in two complementary RGB sequences: (1) a handheld sequence, where the object is manipulated in front of a static camera, and (2) a board sequence, where the object is fixed on a ChArUco board to provide accurate camera poses via marker detection. The goal of NVS-HO is to learn a NVS model that captures the full appearance of an object from (1), whereas (2) provides the ground-truth images used for evaluation. To establish baselines, we consider both a classical SfM pipeline and a state-of-the-art pre-trained feed-forward neural network (VGGT) as pose estimators, and train NVS models based on NeRF and Gaussian Splatting. Our experiments reveal significant performance gaps in current methods under unconstrained handheld conditions, highlighting the need for more robust approaches. NVS-HO thus offers a challenging real-world benchmark to drive progress in RGB-based novel view synthesis of handheld objects.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05822.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05822",
    "published": "2026-02-05T16:13:53Z",
    "updated": "2026-02-05T16:13:53Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05818",
    "title": "TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning",
    "authors": [
      "Zihao Jiang",
      "Miao Peng",
      "Zhenyan Shan",
      "Wenjie Xu",
      "Ben Liu",
      "Gong Chen",
      "Ziqi Gao",
      "Min Peng"
    ],
    "abstract": "Temporal knowledge graph question answering (TKGQA) aims to answer time-sensitive questions by leveraging temporal knowledge bases. While Large Language Models (LLMs) demonstrate significant potential in TKGQA, current prompting strategies constrain their efficacy in two primary ways. First, they are prone to reasoning hallucinations under complex temporal constraints. Second, static prompting limits model autonomy and generalization, as it lack optimization through dynamic interaction with temporal knowledge graphs (TKGs) environments. To address these limitations, we propose \\textbf{TKG-Thinker}, a novel agent equipped with autonomous planning and adaptive retrieval capabilities for reasoning over TKGs. Specifically, TKG-Thinker performs in-depth temporal reasoning through dynamic multi-turn interactions with TKGs via a dual-training strategy. We first apply Supervised Fine-Tuning (SFT) with chain-of thought data to instill core planning capabilities, followed by a Reinforcement Learning (RL) stage that leverages multi-dimensional rewards to refine reasoning policies under intricate temporal constraints. Experimental results on benchmark datasets with three open-source LLMs show that TKG-Thinker achieves state-of-the-art performance and exhibits strong generalization across complex TKGQA settings.",
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05818.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05818",
    "published": "2026-02-05T16:08:36Z",
    "updated": "2026-02-05T16:08:36Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05813",
    "title": "Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers",
    "authors": [
      "Artem Riabinin",
      "Andrey Veprikov",
      "Arman Bolatov",
      "Martin TakÃ¡Ä",
      "Aleksandr Beznosikov"
    ],
    "abstract": "We study adaptive learning rate scheduling for norm-constrained optimizers (e.g., Muon and Lion). We introduce a generalized smoothness assumption under which local curvature decreases with the suboptimality gap and empirically verify that this behavior holds along optimization trajectories. Under this assumption, we establish convergence guarantees under an appropriate choice of learning rate, for which warm-up followed by decay arises naturally from the proof rather than being imposed heuristically.   Building on this theory, we develop a practical learning rate scheduler that relies only on standard hyperparameters and adapts the warm-up duration automatically at the beginning of training. We evaluate this method on large language model pretraining with LLaMA architectures and show that our adaptive warm-up selection consistently outperforms or at least matches the best manually tuned warm-up schedules across all considered setups, without additional hyperparameter search. Our source code is available at https://github.com/brain-lab-research/llm-baselines/tree/warmup",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05813.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05813",
    "published": "2026-02-05T16:06:19Z",
    "updated": "2026-02-05T16:06:19Z",
    "comment": "26 pages, 6 figures, 4 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05812",
    "title": "Principled Confidence Estimation for Deep Computed Tomography",
    "authors": [
      "Matteo GÃ¤tzner",
      "Johannes Kirschner"
    ],
    "abstract": "We present a principled framework for confidence estimation in computed tomography (CT) reconstruction. Based on the sequential likelihood mixing framework (Kirschner et al., 2025), we establish confidence regions with theoretical coverage guarantees for deep-learning-based CT reconstructions. We consider a realistic forward model following the Beer-Lambert law, i.e., a log-linear forward model with Poisson noise, closely reflecting clinical and scientific imaging conditions. The framework is general and applies to both classical algorithms and deep learning reconstruction methods, including U-Nets, U-Net ensembles, and generative Diffusion models. Empirically, we demonstrate that deep reconstruction methods yield substantially tighter confidence regions than classical reconstructions, without sacrificing theoretical coverage guarantees. Our approach allows the detection of hallucinations in reconstructed images and provides interpretable visualizations of confidence regions. This establishes deep models not only as powerful estimators, but also as reliable tools for uncertainty-aware medical imaging.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05812.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05812",
    "published": "2026-02-05T16:04:19Z",
    "updated": "2026-02-05T16:04:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05811",
    "title": "STProtein: predicting spatial protein expression from multi-omics data",
    "authors": [
      "Zhaorui Jiang",
      "Yingfang Yuan",
      "Lei Hu",
      "Wei Pang"
    ],
    "abstract": "The integration of spatial multi-omics data from single tissues is crucial for advancing biological research. However, a significant data imbalance impedes progress: while spatial transcriptomics data is relatively abundant, spatial proteomics data remains scarce due to technical limitations and high costs. To overcome this challenge we propose STProtein, a novel framework leveraging graph neural networks with multi-task learning strategy. STProtein is designed to accurately predict unknown spatial protein expression using more accessible spatial multi-omics data, such as spatial transcriptomics. We believe that STProtein can effectively addresses the scarcity of spatial proteomics, accelerating the integration of spatial multi-omics and potentially catalyzing transformative breakthroughs in life sciences. This tool enables scientists to accelerate discovery by identifying complex and previously hidden spatial patterns of proteins within tissues, uncovering novel relationships between different marker genes, and exploring the biological \"Dark Matter\".",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05811.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05811",
    "published": "2026-02-05T16:04:03Z",
    "updated": "2026-02-05T16:04:03Z",
    "comment": "STProtein: predicting spatial protein expression from multi-omics data is accepted SPARTA_AAAI2026 Oral GitHub: https://github.com/zhaorui-bi/STProtein",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05810",
    "title": "Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents",
    "authors": [
      "Quan M. Tran",
      "Zhuo Huang",
      "Wenbin Zhang",
      "Bo Han",
      "Koji Yatani",
      "Masashi Sugiyama",
      "Tongliang Liu"
    ],
    "abstract": "Autonomous agents excel in self-improvement through reflection and iterative refinement, which reuse successful task trajectories as in-context examples to assist subsequent reasoning. However, shifting across tasks often introduces a context mismatch. Hence, existing approaches either discard the trajectories or manipulate them using heuristics, leading to a non-negligible fine-tuning cost or unguaranteed performance. To bridge this gap, we reveal a context-trajectory correlation, where shifts of context are highly parallel with shifts of trajectory. Based on this finding, we propose BrIdge contextual gap FoR imprOvised trajectory STeering (Bifrost), a training-free method that leverages context differences to precisely guide the adaptation of previously solved trajectories towards the target task, mitigating the misalignment caused by context shifts. Our trajectory adaptation is conducted at the representation level using agent hidden states, ensuring trajectory transformation accurately aligns with the target context in a shared space. Across diverse benchmarks, Bifrost consistently outperforms existing trajectory reuse and finetuned self-improvement methods, demonstrating that agents can effectively leverage past experiences despite substantial context shifts.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05810.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05810",
    "published": "2026-02-05T16:03:56Z",
    "updated": "2026-02-05T16:03:56Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05809",
    "title": "Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning",
    "authors": [
      "Enwei Tong",
      "Yuanchao Bai",
      "Yao Zhu",
      "Junjun Jiang",
      "Xianming Liu"
    ],
    "abstract": "Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05809.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05809",
    "published": "2026-02-05T16:02:48Z",
    "updated": "2026-02-05T16:02:48Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05805",
    "title": "NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking",
    "authors": [
      "Kang Chen",
      "Zhuoka Feng",
      "Sihan Zhao",
      "Kai Xiong",
      "Junjie Nian",
      "Yaoning Wang",
      "Changyi Xiao",
      "Yixin Cao"
    ],
    "abstract": "Large language models increasingly spend inference compute sampling multiple chain-of-thought traces or searching over merged checkpoints. This shifts the bottleneck from generation to selection, often without supervision on the target distribution. We show entropy-based exploration proxies follow an inverted-U with accuracy, suggesting extra exploration can become redundant and induce overthinking. We propose NEX, a white-box label-free unsupervised scoring framework that views reasoning as alternating E-phase (exploration) and X-phase (exploitation). NEX detects E-phase as spikes in newly activated MLP neurons per token from sparse activation caches, then uses a sticky two-state HMM to infer E-X phases and credits E-introduced neurons by whether they are reused in the following X span. These signals yield interpretable neuron weights and a single Good-Mass Fraction score to rank candidate responses and merged variants without task answers. Across reasoning benchmarks and Qwen3 merge families, NEX computed on a small unlabeled activation set predicts downstream accuracy and identifies better variants; we further validate the E-X signal with human annotations and provide causal evidence via \"Effective-vs-Redundant\" neuron transfer.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05805.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05805",
    "published": "2026-02-05T15:59:12Z",
    "updated": "2026-02-05T15:59:12Z",
    "comment": "21 pages, 9 figures, 5 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05797",
    "title": "Classification Under Local Differential Privacy with Model Reversal and Model Averaging",
    "authors": [
      "Caihong Qin",
      "Yang Bai"
    ],
    "abstract": "Local differential privacy (LDP) has become a central topic in data privacy research, offering strong privacy guarantees by perturbing user data at the source and removing the need for a trusted curator. However, the noise introduced by LDP often significantly reduces data utility. To address this issue, we reinterpret private learning under LDP as a transfer learning problem, where the noisy data serve as the source domain and the unobserved clean data as the target. We propose novel techniques specifically designed for LDP to improve classification performance without compromising privacy: (1) a noised binary feedback-based evaluation mechanism for estimating dataset utility; (2) model reversal, which salvages underperforming classifiers by inverting their decision boundaries; and (3) model averaging, which assigns weights to multiple reversed classifiers based on their estimated utility. We provide theoretical excess risk bounds under LDP and demonstrate how our methods reduce this risk. Empirical results on both simulated and real-world datasets show substantial improvements in classification accuracy.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05797.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05797",
    "published": "2026-02-05T15:52:34Z",
    "updated": "2026-02-05T15:52:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05794",
    "title": "FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem",
    "authors": [
      "Aboli Kathar",
      "Aman Kumar",
      "Anusha Kamath",
      "Araveeti Srujan",
      "Ashish Sharma",
      "Chandra Bhushan",
      "Dilip Asbe",
      "Divya Sorate",
      "Duddu Prasanth Kumar",
      "Evan Acharya",
      "Harsh Sharma",
      "Hrithik Kadam",
      "Kanishk Singla",
      "Keyur Doshi",
      "Kiran Praveen",
      "Kolisetty Krishna SK",
      "Krishanu Adhikary",
      "Lokesh MPT",
      "Mayurdeep Sonowal",
      "Nadeem Shaikh",
      "Navya Prakash",
      "Nimit Kothari",
      "Nitin Kukreja",
      "Prashant Devadiga",
      "Rakesh Paul",
      "Ratanjeet Pratap Chauhan",
      "Raunak Kalani",
      "Raviraj Joshi",
      "Shamanth MH",
      "Shantanu Pandey",
      "Shubham Soni",
      "Siddharth Dixit",
      "Smriti Jopat",
      "Sunil Patel",
      "Suraj Singh",
      "Suvradip Paul",
      "Tulasi Pilla",
      "Utkarsh Vaidya",
      "Vineeth Nambiar",
      "Vishal Kanvaty",
      "Yatharth Dedhia"
    ],
    "abstract": "We present FiMI (Finance Model for India), a domain-specialized financial language model developed for Indian digital payment systems. We develop two model variants: FiMI Base and FiMI Instruct. FiMI adapts the Mistral Small 24B architecture through a multi-stage training pipeline, beginning with continuous pre-training on 68 Billion tokens of curated financial, multilingual (English, Hindi, Hinglish), and synthetic data. This is followed by instruction fine-tuning and domain-specific supervised fine-tuning focused on multi-turn, tool-driven conversations that model real-world workflows, such as transaction disputes and mandate lifecycle management. Evaluations reveal that FiMI Base achieves a 20% improvement over the Mistral Small 24B Base model on finance reasoning benchmark, while FiMI Instruct outperforms the Mistral Small 24B Instruct model by 87% on domain-specific tool-calling. Moreover, FiMI achieves these significant domain gains while maintaining comparable performance to models of similar size on general benchmarks.",
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05794.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05794",
    "published": "2026-02-05T15:48:49Z",
    "updated": "2026-02-05T15:48:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05789",
    "title": "Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation",
    "authors": [
      "Hengyi Wang",
      "Ruiqiang Zhang",
      "Chang Liu",
      "Guanjie Wang",
      "Zehua Ma",
      "Han Fang",
      "Weiming Zhang"
    ],
    "abstract": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05789.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05789",
    "published": "2026-02-05T15:45:39Z",
    "updated": "2026-02-05T15:45:39Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05786",
    "title": "Selecting Hyperparameters for Tree-Boosting",
    "authors": [
      "Floris Jan Koster",
      "Fabio Sigrist"
    ],
    "abstract": "Tree-boosting is a widely used machine learning technique for tabular data. However, its out-of-sample accuracy is critically dependent on multiple hyperparameters. In this article, we empirically compare several popular methods for hyperparameter optimization for tree-boosting including random grid search, the tree-structured Parzen estimator (TPE), Gaussian-process-based Bayesian optimization (GP-BO), Hyperband, the sequential model-based algorithm configuration (SMAC) method, and deterministic full grid search using $59$ regression and classification data sets. We find that the SMAC method clearly outperforms all the other considered methods. We further observe that (i) a relatively large number of trials larger than $100$ is required for accurate tuning, (ii) using default values for hyperparameters yields very inaccurate models, (iii) all considered hyperparameters can have a material effect on the accuracy of tree-boosting, i.e., there is no small set of hyperparameters that is more important than others, and (iv) choosing the number of boosting iterations using early stopping yields more accurate results compared to including it in the search space for regression tasks.",
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05786.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05786",
    "published": "2026-02-05T15:44:42Z",
    "updated": "2026-02-05T15:44:42Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05785",
    "title": "ReText: Text Boosts Generalization in Image-Based Person Re-identification",
    "authors": [
      "Timur Mamedov",
      "Karina Kvanchiani",
      "Anton Konushin",
      "Vadim Konushin"
    ],
    "abstract": "Generalizable image-based person re-identification (Re-ID) aims to recognize individuals across cameras in unseen domains without retraining. While multiple existing approaches address the domain gap through complex architectures, recent findings indicate that better generalization can be achieved by stylistically diverse single-camera data. Although this data is easy to collect, it lacks complexity due to minimal cross-view variation. We propose ReText, a novel method trained on a mixture of multi-camera Re-ID data and single-camera data, where the latter is complemented by textual descriptions to enrich semantic cues. During training, ReText jointly optimizes three tasks: (1) Re-ID on multi-camera data, (2) image-text matching, and (3) image reconstruction guided by text on single-camera data. Experiments demonstrate that ReText achieves strong generalization and significantly outperforms state-of-the-art methods on cross-domain Re-ID benchmarks. To the best of our knowledge, this is the first work to explore multimodal joint learning on a mixture of multi-camera and single-camera data in image-based person Re-ID.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05785.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05785",
    "published": "2026-02-05T15:43:31Z",
    "updated": "2026-02-05T15:43:31Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05783",
    "title": "Distributional Reinforcement Learning with Diffusion Bridge Critics",
    "authors": [
      "Shutong Ding",
      "Yimiao Zhou",
      "Ke Hu",
      "Mokai Pan",
      "Shan Zhong",
      "Yanwei Fu",
      "Jingya Wang",
      "Ye Shi"
    ],
    "abstract": "Recent advances in diffusion-based reinforcement learning (RL) methods have demonstrated promising results in a wide range of continuous control tasks. However, existing works in this field focus on the application of diffusion policies while leaving the diffusion critics unexplored. In fact, since policy optimization fundamentally relies on the critic, accurate value estimation is far more important than policy expressiveness. Furthermore, given the stochasticity of most reinforcement learning tasks, it has been confirmed that the critic is more appropriately depicted with a distributional model. Motivated by these points, we propose a novel distributional RL method with Diffusion Bridge Critics (DBC). DBC directly models the inverse cumulative distribution function (CDF) of the Q value. This allows us to accurately capture the value distribution and prevents it from collapsing into a trivial Gaussian distribution owing to the strong distribution-matching capability of the diffusion bridge. Moreover, we further derive an analytic integral formula to address discretization errors in DBC, which is essential in value estimation. To our knowledge, DBC is the first work to employ the diffusion bridge model as the critic. Notably, DBC is also a plug-and-play component and can be integrated into most existing RL frameworks. Experimental results on MuJoCo robot control benchmarks demonstrate the superiority of DBC compared with previous distributional critic models.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05783.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05783",
    "published": "2026-02-05T15:40:14Z",
    "updated": "2026-02-05T15:40:14Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05779",
    "title": "How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs",
    "authors": [
      "Emily Dent",
      "Jared Tanner"
    ],
    "abstract": "The intermediate layers of deep networks can be characterised as a Gaussian process, in particular the Edge-of-Chaos (EoC) initialisation strategy prescribes the limiting covariance matrix of the Gaussian process. Here we show that the under-utilised chosen variance of the Gaussian process is important in the training of deep networks with sparsity inducing activation, such as a shifted and clipped ReLU, $\\text{CReLU}_{Ï,m}(x)=\\min(\\max(x-Ï,0),m)$. Specifically, initialisations leading to larger fixed Gaussian process variances, allow for improved expressivity with activation sparsity as large as 90% in DNNs and CNNs, and generally improve the stability of the training process. Enabling full, or near full, accuracy at such high levels of sparsity in the hidden layers suggests a promising mechanism to reduce the energy consumption of machine learning models involving fully connected layers.",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05779.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05779",
    "published": "2026-02-05T15:38:37Z",
    "updated": "2026-02-05T15:38:37Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05776",
    "title": "Cross-Domain Offline Policy Adaptation via Selective Transition Correction",
    "authors": [
      "Mengbei Yan",
      "Jiafei Lyu",
      "Shengjie Sun",
      "Zhongjian Qiao",
      "Jingwen Yang",
      "Zichuan Lin",
      "Deheng Ye",
      "Xiu Li"
    ],
    "abstract": "It remains a critical challenge to adapt policies across domains with mismatched dynamics in reinforcement learning (RL). In this paper, we study cross-domain offline RL, where an offline dataset from another similar source domain can be accessed to enhance policy learning upon a target domain dataset. Directly merging the two datasets may lead to suboptimal performance due to potential dynamics mismatches. Existing approaches typically mitigate this issue through source domain transition filtering or reward modification, which, however, may lead to insufficient exploitation of the valuable source domain data. Instead, we propose to modify the source domain data into the target domain data. To that end, we leverage an inverse policy model and a reward model to correct the actions and rewards of source transitions, explicitly achieving alignment with the target dynamics. Since limited data may result in inaccurate model training, we further employ a forward dynamics model to retain corrected samples that better match the target dynamics than the original transitions. Consequently, we propose the Selective Transition Correction (STC) algorithm, which enables reliable usage of source domain data for policy adaptation. Experiments on various environments with dynamics shifts demonstrate that STC achieves superior performance against existing baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05776.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05776",
    "published": "2026-02-05T15:37:29Z",
    "updated": "2026-02-05T15:37:29Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05774",
    "title": "Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance",
    "authors": [
      "Xiandong Zou",
      "Jianshu Li",
      "Jing Huang",
      "Pan Zhou"
    ],
    "abstract": "Speculative decoding accelerates inference for (M)LLMs, yet a training-decoding discrepancy persists: while existing methods optimize single greedy trajectories, decoding involves verifying and ranking multiple sampled draft paths. We propose Variational Speculative Decoding (VSD), formulating draft training as variational inference over latent proposals (draft paths). VSD maximizes the marginal probability of target-model acceptance, yielding an ELBO that promotes high-quality latent proposals while minimizing divergence from the target distribution. To enhance quality and reduce variance, we incorporate a path-level utility and optimize via an Expectation-Maximization procedure. The E-step draws MCMC samples from an oracle-filtered posterior, while the M-step maximizes weighted likelihood using Adaptive Rejection Weighting (ARW) and Confidence-Aware Regularization (CAR). Theoretical analysis confirms that VSD increases expected acceptance length and speedup. Extensive experiments across LLMs and MLLMs show that VSD achieves up to a 9.6% speedup over EAGLE-3 and 7.9% over ViSpec, significantly improving decoding efficiency.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05774.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05774",
    "published": "2026-02-05T15:36:19Z",
    "updated": "2026-02-05T15:36:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05769",
    "title": "Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors",
    "authors": [
      "Adnan Al Ali",
      "JindÅich Helcl",
      "JindÅich LibovickÃ½"
    ],
    "abstract": "LLM-based assistants have been widely popularised after the release of ChatGPT. Concerns have been raised about their misuse in academia, given the difficulty of distinguishing between human-written and generated text. To combat this, automated techniques have been developed and shown to be effective, to some extent. However, prior work suggests that these methods often falsely flag essays from non-native speakers as generated, due to their low perplexity extracted from an LLM, which is supposedly a key feature of the detectors. We revisit these statements two years later, specifically in the Czech language setting. We show that the perplexity of texts from non-native speakers of Czech is not lower than that of native speakers. We further examine detectors from three separate families and find no systematic bias against non-native speakers. Finally, we demonstrate that contemporary detectors operate effectively without relying on perplexity.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05769.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05769",
    "published": "2026-02-05T15:31:24Z",
    "updated": "2026-02-05T15:31:24Z",
    "comment": "This paper was accepted to EACL 2026 Student Research Workshop",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05765",
    "title": "RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism",
    "authors": [
      "Zhong Guan",
      "Haoran Sun",
      "Yongjian Guo",
      "Shuai Di",
      "Xiaodong Bai",
      "Jing Long",
      "Tianyun Zhao",
      "Mingxi Luo",
      "Chen Zhou",
      "Yucheng Guo",
      "Qiming Yang",
      "Wanting Xu",
      "Wen Huang",
      "Yunxuan Ma",
      "Hongke Zhao",
      "Likang Wu",
      "Xiaotie Deng",
      "Xi Xiao",
      "Sheng Wen",
      "Yicheng Gong",
      "Junwu Xiong"
    ],
    "abstract": "In recent years, Vision-Language-Action (VLA) models have emerged as a crucial pathway towards general embodied intelligence, yet their training efficiency has become a key bottleneck. Although existing reinforcement learning (RL)-based training frameworks like RLinf can enhance model generalization, they still rely on synchronous execution, leading to severe resource underutilization and throughput limitations during environment interaction, policy generation (rollout), and model update phases (actor). To overcome this challenge, this paper, for the first time, proposes and implements a fully-asynchronous policy training framework encompassing the entire pipeline from environment interaction, rollout generation, to actor policy updates. Systematically drawing inspiration from asynchronous optimization ideas in large model RL, our framework designs a multi-level decoupled architecture. This includes asynchronous parallelization of environment interaction and trajectory collection, streaming execution for policy generation, and decoupled scheduling for training updates. We validated the effectiveness of our method across diverse VLA models and environments. On the LIBERO benchmark, the framework achieves throughput improvements of up to 59.25\\% compared to existing synchronous strategies. When deeply optimizing separation strategies, throughput can be increased by as much as 126.67\\%. We verified the effectiveness of each asynchronous component via ablation studies. Scaling law validation across 8 to 256 GPUs demonstrates our method's excellent scalability under most conditions.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05765.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05765",
    "published": "2026-02-05T15:30:23Z",
    "updated": "2026-02-05T15:30:23Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05762",
    "title": "RocqSmith: Can Automatic Optimization Forge Better Proof Agents?",
    "authors": [
      "Andrei Kozyrev",
      "Nikita Khramov",
      "Denis Lochmelis",
      "Valerio Morelli",
      "Gleb Solovev",
      "Anton Podkopaev"
    ],
    "abstract": "This work studies the applicability of automatic AI agent optimization methods to real-world agents in formal verification settings, focusing on automated theorem proving in Rocq as a representative and challenging domain. We evaluate how different automatic agent optimizers perform when applied to the task of optimizing a Rocq proof-generation agent, and assess whether parts of the fine-grained tuning of agentic systems, such as prompt design, contextual knowledge, and control strategies, can be automated. Our results show that while several optimizers yield measurable improvements, simple few-shot bootstrapping is the most consistently effective; however, none of the studied methods matches the performance of a carefully engineered state-of-the-art proof agent.",
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.LO",
      "cs.SE"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05762.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05762",
    "published": "2026-02-05T15:28:26Z",
    "updated": "2026-02-05T15:28:26Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05758",
    "title": "LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards",
    "authors": [
      "Bowen Ping",
      "Zijun Chen",
      "Yiyao Yu",
      "Tingfeng Hui",
      "Junchi Yan",
      "Baobao Chang"
    ],
    "abstract": "Reinforcement Learning has emerged as a key driver for LLM reasoning. This capability is equally pivotal in long-context scenarios--such as long-dialogue understanding and structured data analysis, where the challenge extends beyond consuming tokens to performing rigorous deduction. While existing efforts focus on data synthesis or architectural changes, recent work points out that relying solely on sparse, outcome-only rewards yields limited gains, as such coarse signals are often insufficient to effectively guide the complex long-context reasoning. To address this, we propose LongR, a unified framework that enhances long-context performance by integrating a dynamic \"Think-and-Read\" mechanism, which interleaves reasoning with document consultation, with a contextual density reward based on relative information gain to quantify the utility of the relevant documents. Empirically, LongR achieves a 9% gain on LongBench v2 and consistent improvements on RULER and InfiniteBench, demonstrating robust efficiency in navigating extensive contexts. Furthermore, LongR consistently enhances performance across diverse RL algorithms (e.g., DAPO, GSPO). Finally, we conduct in-depth analyses to investigate the impact of reasoning chain length on efficiency and the model's robustness against distractors.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05758.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05758",
    "published": "2026-02-05T15:26:47Z",
    "updated": "2026-02-05T15:26:47Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05755",
    "title": "FMPose3D: monocular 3D pose estimation via flow matching",
    "authors": [
      "Ti Wang",
      "Xiaohang Yu",
      "Mackenzie Weygandt Mathis"
    ],
    "abstract": "Monocular 3D pose estimation is fundamentally ill-posed due to depth ambiguity and occlusions, thereby motivating probabilistic methods that generate multiple plausible 3D pose hypotheses. In particular, diffusion-based models have recently demonstrated strong performance, but their iterative denoising process typically requires many timesteps for each prediction, making inference computationally expensive. In contrast, we leverage Flow Matching (FM) to learn a velocity field defined by an Ordinary Differential Equation (ODE), enabling efficient generation of 3D pose samples with only a few integration steps. We propose a novel generative pose estimation framework, FMPose3D, that formulates 3D pose estimation as a conditional distribution transport problem. It continuously transports samples from a standard Gaussian prior to the distribution of plausible 3D poses conditioned only on 2D inputs. Although ODE trajectories are deterministic, FMPose3D naturally generates various pose hypotheses by sampling different noise seeds. To obtain a single accurate prediction from those hypotheses, we further introduce a Reprojection-based Posterior Expectation Aggregation (RPEA) module, which approximates the Bayesian posterior expectation over 3D hypotheses. FMPose3D surpasses existing methods on the widely used human pose estimation benchmarks Human3.6M and MPI-INF-3DHP, and further achieves state-of-the-art performance on the 3D animal pose datasets Animal3D and CtrlAni3D, demonstrating strong performance across both 3D pose domains. The code is available at https://github.com/AdaptiveMotorControlLab/FMPose3D.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05755.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05755",
    "published": "2026-02-05T15:25:35Z",
    "updated": "2026-02-05T15:25:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05749",
    "title": "How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning",
    "authors": [
      "Kai Ming Ting",
      "Wei-Jie Xu",
      "Hang Zhang"
    ],
    "abstract": "Deep clustering (DC) is often quoted to have a key advantage over $k$-means clustering. Yet, this advantage is often demonstrated using image datasets only, and it is unclear whether it addresses the fundamental limitations of $k$-means clustering. Deep Embedded Clustering (DEC) learns a latent representation via an autoencoder and performs clustering based on a $k$-means-like procedure, while the optimization is conducted in an end-to-end manner. This paper investigates whether the deep-learned representation has enabled DEC to overcome the known fundamental limitations of $k$-means clustering, i.e., its inability to discover clusters of arbitrary shapes, varied sizes and densities. Our investigations on DEC have a wider implication on deep clustering methods in general. Notably, none of these methods exploit the underlying data distribution. We uncover that a non-deep learning approach achieves the intended aim of deep clustering by making use of distributional information of clusters in a dataset to effectively address these fundamental limitations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05749.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05749",
    "published": "2026-02-05T15:16:04Z",
    "updated": "2026-02-05T15:16:04Z",
    "comment": "Work on progress",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05748",
    "title": "LeakBoost: Perceptual-Loss-Based Membership Inference Attack",
    "authors": [
      "Amit Kravchik Taub",
      "Fred M. Grabovski",
      "Guy Amit",
      "Yisroel Mirsky"
    ],
    "abstract": "Membership inference attacks (MIAs) aim to determine whether a sample was part of a model's training set, posing serious privacy risks for modern machine-learning systems. Existing MIAs primarily rely on static indicators, such as loss or confidence, and do not fully leverage the dynamic behavior of models when actively probed. We propose LeakBoost, a perceptual-loss-based interrogation framework that actively probes a model's internal representations to expose hidden membership signals. Given a candidate input, LeakBoost synthesizes an interrogation image by optimizing a perceptual (activation-space) objective, amplifying representational differences between members and non-members. This image is then analyzed by an off-the-shelf membership detector, without modifying the detector itself. When combined with existing membership inference methods, LeakBoost achieves substantial improvements at low false-positive rates across multiple image classification datasets and diverse neural network architectures. In particular, it raises AUC from near-chance levels (0.53-0.62) to 0.81-0.88, and increases TPR at 1 percent FPR by over an order of magnitude compared to strong baseline attacks. A detailed sensitivity analysis reveals that deeper layers and short, low-learning-rate optimization produce the strongest leakage, and that improvements concentrate in gradient-based detectors. LeakBoost thus offers a modular and computationally efficient way to assess privacy risks in white-box settings, advancing the study of dynamic membership inference.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05748.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05748",
    "published": "2026-02-05T15:15:35Z",
    "updated": "2026-02-05T15:15:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05746",
    "title": "Learning to Inject: Automated Prompt Injection via Reinforcement Learning",
    "authors": [
      "Xin Chen",
      "Jie Zhang",
      "Florian Tramer"
    ],
    "abstract": "Prompt injection is one of the most critical vulnerabilities in LLM agents; yet, effective automated attacks remain largely unexplored from an optimization perspective. Existing methods heavily depend on human red-teamers and hand-crafted prompts, limiting their scalability and adaptability. We propose AutoInject, a reinforcement learning framework that generates universal, transferable adversarial suffixes while jointly optimizing for attack success and utility preservation on benign tasks. Our black-box method supports both query-based optimization and transfer attacks to unseen models and tasks. Using only a 1.5B parameter adversarial suffix generator, we successfully compromise frontier systems including GPT 5 Nano, Claude Sonnet 3.5, and Gemini 2.5 Flash on the AgentDojo benchmark, establishing a stronger baseline for automated prompt injection research.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05746.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05746",
    "published": "2026-02-05T15:14:46Z",
    "updated": "2026-02-05T15:14:46Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05737",
    "title": "Neuro-Inspired Visual Pattern Recognition via Biological Reservoir Computing",
    "authors": [
      "Luca Ciampi",
      "Ludovico Iannello",
      "Fabrizio Tonelli",
      "Gabriele Lagani",
      "Angelo Di Garbo",
      "Federico Cremisi",
      "Giuseppe Amato"
    ],
    "abstract": "In this paper, we present a neuro-inspired approach to reservoir computing (RC) in which a network of in vitro cultured cortical neurons serves as the physical reservoir. Rather than relying on artificial recurrent models to approximate neural dynamics, our biological reservoir computing (BRC) system leverages the spontaneous and stimulus-evoked activity of living neural circuits as its computational substrate. A high-density multi-electrode array (HD-MEA) provides simultaneous stimulation and readout across hundreds of channels: input patterns are delivered through selected electrodes, while the remaining ones capture the resulting high-dimensional neural responses, yielding a biologically grounded feature representation. A linear readout layer (single-layer perceptron) is then trained to classify these reservoir states, enabling the living neural network to perform static visual pattern-recognition tasks within a computer-vision framework. We evaluate the system across a sequence of tasks of increasing difficulty, ranging from pointwise stimuli to oriented bars, clock-digit-like shapes, and handwritten digits from the MNIST dataset. Despite the inherent variability of biological neural responses-arising from noise, spontaneous activity, and inter-session differences-the system consistently generates high-dimensional representations that support accurate classification. These results demonstrate that in vitro cortical networks can function as effective reservoirs for static visual pattern recognition, opening new avenues for integrating living neural substrates into neuromorphic computing frameworks. More broadly, this work contributes to the effort to incorporate biological principles into machine learning and supports the goals of neuro-inspired vision by illustrating how living neural systems can inform the design of efficient and biologically grounded computational models.",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05737.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05737",
    "published": "2026-02-05T15:02:07Z",
    "updated": "2026-02-05T15:02:07Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05735",
    "title": "CSRv2: Unlocking Ultra-Sparse Embeddings",
    "authors": [
      "Lixuan Guo",
      "Yifei Wang",
      "Tiansheng Wen",
      "Yifan Wang",
      "Aosong Feng",
      "Bo Chen",
      "Stefanie Jegelka",
      "Chenyu You"
    ],
    "abstract": "In the era of large foundation models, the quality of embeddings has become a central determinant of downstream task performance and overall system capability. Yet widely used dense embeddings are often extremely high-dimensional, incurring substantial costs in storage, memory, and inference latency. To address these, Contrastive Sparse Representation (CSR) is recently proposed as a promising direction, mapping dense embeddings into high-dimensional but k-sparse vectors, in contrast to compact dense embeddings such as Matryoshka Representation Learning (MRL). Despite its promise, CSR suffers severe degradation in the ultra-sparse regime, where over 80% of neurons remain inactive, leaving much of its efficiency potential unrealized. In this paper, we introduce CSRv2, a principled training approach designed to make ultra-sparse embeddings viable. CSRv2 stabilizes sparsity learning through progressive k-annealing, enhances representational quality via supervised contrastive objectives, and ensures end-to-end adaptability with full backbone finetuning. CSRv2 reduces dead neurons from 80% to 20% and delivers a 14% accuracy gain at k=2, bringing ultra-sparse embeddings on par with CSR at k=8 and MRL at 32 dimensions, all with only two active features. While maintaining comparable performance, CSRv2 delivers a 7x speedup over MRL, and yields up to 300x improvements in compute and memory efficiency relative to dense embeddings in text representation. Extensive experiments across text and vision demonstrate that CSRv2 makes ultra-sparse embeddings practical without compromising performance, where CSRv2 achieves 7%/4% improvement over CSR when k=4 and further increases this gap to 14%/6% when k=2 in text/vision representation. By making extreme sparsity viable, CSRv2 broadens the design space for real-time and edge-deployable AI systems where both embedding quality and efficiency are critical.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IR",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05735.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05735",
    "published": "2026-02-05T14:59:51Z",
    "updated": "2026-02-05T14:59:51Z",
    "comment": "Accepted by ICLR2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05730",
    "title": "Depth as Prior Knowledge for Object Detection",
    "authors": [
      "Moussa Kassem Sbeyti",
      "Nadja Klein"
    ],
    "abstract": "Detecting small and distant objects remains challenging for object detectors due to scale variation, low resolution, and background clutter. Safety-critical applications require reliable detection of these objects for safe planning. Depth information can improve detection, but existing approaches require complex, model-specific architectural modifications. We provide a theoretical analysis followed by an empirical investigation of the depth-detection relationship. Together, they explain how depth causes systematic performance degradation and why depth-informed supervision mitigates it. We introduce DepthPrior, a framework that uses depth as prior knowledge rather than as a fused feature, providing comparable benefits without modifying detector architectures. DepthPrior consists of Depth-Based Loss Weighting (DLW) and Depth-Based Loss Stratification (DLS) during training, and Depth-Aware Confidence Thresholding (DCT) during inference. The only overhead is the initial cost of depth estimation. Experiments across four benchmarks (KITTI, MS COCO, VisDrone, SUN RGB-D) and two detectors (YOLOv11, EfficientDet) demonstrate the effectiveness of DepthPrior, achieving up to +9% mAP$_S$ and +7% mAR$_S$ for small objects, with inference recovery rates as high as 95:1 (true vs. false detections). DepthPrior offers these benefits without additional sensors, architectural changes, or performance costs. Code is available at https://github.com/mos-ks/DepthPrior.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05730.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05730",
    "published": "2026-02-05T14:52:39Z",
    "updated": "2026-02-05T14:52:39Z",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05729",
    "title": "Adaptive Global and Fine-Grained Perceptual Fusion for MLLM Embeddings Compatible with Hard Negative Amplification",
    "authors": [
      "Lexiang Hu",
      "Youze Xue",
      "Dian Li",
      "Gang Liu",
      "Zhouchen Lin"
    ],
    "abstract": "Multimodal embeddings serve as a bridge for aligning vision and language, with the two primary implementations -- CLIP-based and MLLM-based embedding models -- both limited to capturing only global semantic information. Although numerous studies have focused on fine-grained understanding, we observe that complex scenarios currently targeted by MLLM embeddings often involve a hybrid perceptual pattern of both global and fine-grained elements, thus necessitating a compatible fusion mechanism. In this paper, we propose Adaptive Global and Fine-grained perceptual Fusion for MLLM Embeddings (AGFF-Embed), a method that prompts the MLLM to generate multiple embeddings focusing on different dimensions of semantic information, which are then adaptively and smoothly aggregated. Furthermore, we adapt AGFF-Embed with the Explicit Gradient Amplification (EGA) technique to achieve in-batch hard negatives enhancement without requiring fine-grained editing of the dataset. Evaluation on the MMEB and MMVP-VLM benchmarks shows that AGFF-Embed comprehensively achieves state-of-the-art performance in both general and fine-grained understanding compared to other multimodal embedding models.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05729.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05729",
    "published": "2026-02-05T14:52:35Z",
    "updated": "2026-02-05T14:52:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05728",
    "title": "CompactRAG: Reducing LLM Calls and Token Overhead in Multi-Hop Question Answering",
    "authors": [
      "Hao Yang",
      "Zhiyu Yang",
      "Xupeng Zhang",
      "Wei Wei",
      "Yunjie Zhang",
      "Lin Yang"
    ],
    "abstract": "Retrieval-augmented generation (RAG) has become a key paradigm for knowledge-intensive question answering. However, existing multi-hop RAG systems remain inefficient, as they alternate between retrieval and reasoning at each step, resulting in repeated LLM calls, high token consumption, and unstable entity grounding across hops. We propose CompactRAG, a simple yet effective framework that decouples offline corpus restructuring from online reasoning.   In the offline stage, an LLM reads the corpus once and converts it into an atomic QA knowledge base, which represents knowledge as minimal, fine-grained question-answer pairs. In the online stage, complex queries are decomposed and carefully rewritten to preserve entity consistency, and are resolved through dense retrieval followed by RoBERTa-based answer extraction. Notably, during inference, the LLM is invoked only twice in total - once for sub-question decomposition and once for final answer synthesis - regardless of the number of reasoning hops.   Experiments on HotpotQA, 2WikiMultiHopQA, and MuSiQue demonstrate that CompactRAG achieves competitive accuracy while substantially reducing token consumption compared to iterative RAG baselines, highlighting a cost-efficient and practical approach to multi-hop reasoning over large knowledge corpora. The implementation is available at GitHub.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05728.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05728",
    "published": "2026-02-05T14:52:06Z",
    "updated": "2026-02-05T14:52:06Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05725",
    "title": "Muon in Associative Memory Learning: Training Dynamics and Scaling Laws",
    "authors": [
      "Binghui Li",
      "Kaifei Wang",
      "Han Zhong",
      "Pinyan Lu",
      "Liwei Wang"
    ],
    "abstract": "Muon updates matrix parameters via the matrix sign of the gradient and has shown strong empirical gains, yet its dynamics and scaling behavior remain unclear in theory. We study Muon in a linear associative memory model with softmax retrieval and a hierarchical frequency spectrum over query-answer pairs, with and without label noise. In this setting, we show that Gradient Descent (GD) learns frequency components at highly imbalanced rates, leading to slow convergence bottlenecked by low-frequency components. In contrast, the Muon optimizer mitigates this imbalance, leading to faster and more uniform progress. Specifically, in the noiseless case, Muon achieves an exponential speedup over GD; in the noisy case with a power-decay frequency spectrum, we derive Muon's optimization scaling law and demonstrate its superior scaling efficiency over GD. Furthermore, we show that Muon can be interpreted as an implicit matrix preconditioner arising from adaptive task alignment and block-symmetric gradient structure. In contrast, the preconditioner with coordinate-wise sign operator could match Muon under oracle access to unknown task representations, which is infeasible for SignGD in practice. Experiments on synthetic long-tail classification and LLaMA-style pre-training corroborate the theory.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05725.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05725",
    "published": "2026-02-05T14:49:40Z",
    "updated": "2026-02-05T14:49:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05723",
    "title": "Mitigating Hallucination in Financial Retrieval-Augmented Generation via Fine-Grained Knowledge Verification",
    "authors": [
      "Taoye Yin",
      "Haoyuan Hu",
      "Yaxin Fan",
      "Xinhao Chen",
      "Xinya Wu",
      "Kai Deng",
      "Kezun Zhang",
      "Feng Wang"
    ],
    "abstract": "In financial Retrieval-Augmented Generation (RAG) systems, models frequently rely on retrieved documents to generate accurate responses due to the time-sensitive nature of the financial domain. While retrieved documents help address knowledge gaps, model-generated responses still suffer from hallucinations that contradict the retrieved information. To mitigate this inconsistency, we propose a Reinforcement Learning framework enhanced with Fine-grained Knowledge Verification (RLFKV). Our method decomposes financial responses into atomic knowledge units and assesses the correctness of each unit to compute the fine-grained faithful reward. This reward offers more precise optimization signals, thereby improving alignment with the retrieved documents. Additionally, to prevent reward hacking (e.g., overly concise replies), we incorporate an informativeness reward that encourages the policy model to retain at least as many knowledge units as the base model. Experiments conducted on the public Financial Data Description (FDD) task and our newly proposed FDD-ANT dataset demonstrate consistent improvements, confirming the effectiveness of our approach.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05723.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05723",
    "published": "2026-02-05T14:49:05Z",
    "updated": "2026-02-05T14:49:05Z",
    "comment": "accepted by ICASSP 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05718",
    "title": "Exploring the Temporal Consistency for Point-Level Weakly-Supervised Temporal Action Localization",
    "authors": [
      "Yunchuan Ma",
      "Laiyun Qing",
      "Guorong Li",
      "Yuqing Liu",
      "Yuankai Qi",
      "Qingming Huang"
    ],
    "abstract": "Point-supervised Temporal Action Localization (PTAL) adopts a lightly frame-annotated paradigm (\\textit{i.e.}, labeling only a single frame per action instance) to train a model to effectively locate action instances within untrimmed videos. Most existing approaches design the task head of models with only a point-supervised snippet-level classification, without explicit modeling of understanding temporal relationships among frames of an action. However, understanding the temporal relationships of frames is crucial because it can help a model understand how an action is defined and therefore benefits localizing the full frames of an action. To this end, in this paper, we design a multi-task learning framework that fully utilizes point supervision to boost the model's temporal understanding capability for action localization. Specifically, we design three self-supervised temporal understanding tasks: (i) Action Completion, (ii) Action Order Understanding, and (iii) Action Regularity Understanding. These tasks help a model understand the temporal consistency of actions across videos. To the best of our knowledge, this is the first attempt to explicitly explore temporal consistency for point supervision action localization. Extensive experimental results on four benchmark datasets demonstrate the effectiveness of the proposed method compared to several state-of-the-art approaches.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05718.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05718",
    "published": "2026-02-05T14:46:21Z",
    "updated": "2026-02-05T14:46:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05717",
    "title": "Anchored Policy Optimization: Mitigating Exploration Collapse Via Support-Constrained Rectification",
    "authors": [
      "Tianyi Wang",
      "Long Li",
      "Hongcan Guo",
      "Yibiao Chen",
      "Yixia Li",
      "Yong Wang",
      "Yun Chen",
      "Guanhua Chen"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is increasingly viewed as a tree pruning mechanism. However, we identify a systemic pathology termed Recursive Space Contraction (RSC), an irreversible collapse driven by the combined dynamics of positive sharpening and negative squeezing, where the sampling probability of valid alternatives vanishes. While Kullback-Leibler (KL) regularization aims to mitigate this, it imposes a rigid Shape Matching constraint that forces the policy to mimic the reference model's full density, creating a gradient conflict with the sharpening required for correctness. We propose Anchored Policy Optimization (APO), shifting the paradigm from global Shape Matching to Support Coverage. By defining a Safe Manifold based on the reference model's high-confidence support, APO permits aggressive sharpening for efficiency while selectively invoking a restorative force during error correction to prevent collapse. We theoretically derive that APO serves as a gradient-aligned mechanism to maximize support coverage, enabling an Elastic Recovery that re-inflates valid branches. Empirical evaluations on mathematical benchmarks demonstrate that APO breaks the accuracy-diversity trade-off, significantly improving Pass@1 while restoring the Pass@K diversity typically lost by standard policy gradient methods.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05717.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05717",
    "published": "2026-02-05T14:41:57Z",
    "updated": "2026-02-05T14:41:57Z",
    "comment": "17 pages, 6 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05713",
    "title": "Projected Boosting with Fairness Constraints: Quantifying the Cost of Fair Training Distributions",
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ],
    "abstract": "Boosting algorithms enjoy strong theoretical guarantees: when weak learners maintain positive edge, AdaBoost achieves geometric decrease of exponential loss. We study how to incorporate group fairness constraints into boosting while preserving analyzable training dynamics. Our approach, FairBoost, projects the ensemble-induced exponential-weights distribution onto a convex set of distributions satisfying fairness constraints (as a reweighting surrogate), then trains weak learners on this fair distribution. The key theoretical insight is that projecting the training distribution reduces the effective edge of weak learners by a quantity controlled by the KL-divergence of the projection. We prove an exponential-loss bound where the convergence rate depends on weak learner edge minus a \"fairness cost\" term $Î´_t = \\sqrt{\\mathrm{KL}(w^t \\| q^t)/2}$. This directly quantifies the accuracy-fairness tradeoff in boosting dynamics. Experiments on standard benchmarks validate the theoretical predictions and demonstrate competitive fairness-accuracy tradeoffs with stable training curves.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05713.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05713",
    "published": "2026-02-05T14:38:32Z",
    "updated": "2026-02-05T14:38:32Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05711",
    "title": "OmniMoE: An Efficient MoE by Orchestrating Atomic Experts at Scale",
    "authors": [
      "Jingze Shi",
      "Zhangyang Peng",
      "Yizhang Zhu",
      "Yifan Wu",
      "Guang Liu",
      "Yuyu Luo"
    ],
    "abstract": "Mixture-of-Experts (MoE) architectures are evolving towards finer granularity to improve parameter efficiency. However, existing MoE designs face an inherent trade-off between the granularity of expert specialization and hardware execution efficiency. We propose OmniMoE, a system-algorithm co-designed framework that pushes expert granularity to its logical extreme. OmniMoE introduces vector-level Atomic Experts, enabling scalable routing and execution within a single MoE layer, while retaining a shared dense MLP branch for general-purpose processing. Although this atomic design maximizes capacity, it poses severe challenges for routing complexity and memory access. To address these, OmniMoE adopts a system-algorithm co-design: (i) a Cartesian Product Router that decomposes the massive index space to reduce routing complexity from O(N) to O(sqrt(N)); and (ii) Expert-Centric Scheduling that inverts the execution order to turn scattered, memory-bound lookups into efficient dense matrix operations. Validated on seven benchmarks, OmniMoE (with 1.7B active parameters) achieves 50.9% zero-shot accuracy across seven benchmarks, outperforming coarse-grained (e.g., DeepSeekMoE) and fine-grained (e.g., PEER) baselines. Crucially, OmniMoE reduces inference latency from 73ms to 6.7ms (a 10.9-fold speedup) compared to PEER, demonstrating that massive-scale fine-grained MoE can be fast and accurate. Our code is open-sourced at https://github.com/flash-algo/omni-moe.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05711.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05711",
    "published": "2026-02-05T14:37:32Z",
    "updated": "2026-02-05T14:37:32Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05709",
    "title": "Nonlinearity as Rank: Generative Low-Rank Adapter with Radial Basis Functions",
    "authors": [
      "Yihao Ouyang",
      "Shiwei Li",
      "Haozhao Wang",
      "Xiandi Luo",
      "Zhuoqi Hu",
      "Yuetong Song",
      "Qiyu Qin",
      "Yichen Li",
      "Ruixuan Li"
    ],
    "abstract": "Low-rank adaptation (LoRA) approximates the update of a pretrained weight matrix using the product of two low-rank matrices. However, standard LoRA follows an explicit-rank paradigm, where increasing model capacity requires adding more rows or columns (i.e., basis vectors) to the low-rank matrices, leading to substantial parameter growth. In this paper, we find that these basis vectors exhibit significant parameter redundancy and can be compactly represented by lightweight nonlinear functions. Therefore, we propose Generative Low-Rank Adapter (GenLoRA), which replaces explicit basis vector storage with nonlinear basis vector generation. Specifically, GenLoRA maintains a latent vector for each low-rank matrix and employs a set of lightweight radial basis functions (RBFs) to synthesize the basis vectors. Each RBF requires far fewer parameters than an explicit basis vector, enabling higher parameter efficiency in GenLoRA. Extensive experiments across multiple datasets and architectures show that GenLoRA attains higher effective LoRA ranks under smaller parameter budgets, resulting in superior fine-tuning performance. The code is available at https://anonymous.4open.science/r/GenLoRA-1519.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05709.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05709",
    "published": "2026-02-05T14:36:44Z",
    "updated": "2026-02-05T14:36:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05707",
    "title": "Fix Representation (Optimally) Before Fairness: Finite-Sample Shrinkage Population Correction and the True Price of Fairness Under Subpopulation Shift",
    "authors": [
      "Amir Asiaee",
      "Kaveh Aryan"
    ],
    "abstract": "Machine learning practitioners frequently observe tension between predictive accuracy and group fairness constraints -- yet sometimes fairness interventions appear to improve accuracy. We show that both phenomena can be artifacts of training data that misrepresents subgroup proportions. Under subpopulation shift (stable within-group distributions, shifted group proportions), we establish: (i) full importance-weighted correction is asymptotically unbiased but finite-sample suboptimal; (ii) the optimal finite-sample correction is a shrinkage reweighting that interpolates between target and training mixtures; (iii) apparent \"fairness helps accuracy\" can arise from comparing fairness methods to an improperly-weighted baseline. We provide an actionable evaluation protocol: fix representation (optimally) before fairness -- compare fairness interventions against a shrinkage-corrected baseline to isolate the true, irreducible price of fairness. Experiments on synthetic and real-world benchmarks (Adult, COMPAS) validate our theoretical predictions and demonstrate that this protocol eliminates spurious tradeoffs, revealing the genuine fairness-utility frontier.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05707.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05707",
    "published": "2026-02-05T14:32:05Z",
    "updated": "2026-02-05T14:32:05Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05706",
    "title": "Poster: Camera Tampering Detection for Outdoor IoT Systems",
    "authors": [
      "Shadi Attarha",
      "Kanaga Shanmugi",
      "Anna FÃ¶rster"
    ],
    "abstract": "Recently, the use of smart cameras in outdoor settings has grown to improve surveillance and security. Nonetheless, these systems are susceptible to tampering, whether from deliberate vandalism or harsh environmental conditions, which can undermine their monitoring effectiveness. In this context, detecting camera tampering is more challenging when a camera is capturing still images rather than video as there is no sequence of continuous frames over time. In this study, we propose two approaches for detecting tampered images: a rule-based method and a deep-learning-based method. The aim is to evaluate how each method performs in terms of accuracy, computational demands, and the data required for training when applied to real-world scenarios. Our results show that the deep-learning model provides higher accuracy, while the rule-based method is more appropriate for scenarios where resources are limited and a prolonged calibration phase is impractical. We also offer publicly available datasets with normal, blurred, and rotated images to support the development and evaluation of camera tampering detection methods, addressing the need for such resources.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05706.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05706",
    "published": "2026-02-05T14:30:27Z",
    "updated": "2026-02-05T14:30:27Z",
    "comment": "Proceedings of the 2024 INTERNATIONAL CONFERENCE ON EMBEDDED WIRELESS SYSTEMS AND NETWORKS (EWSN)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05704",
    "title": "Limitations of SGD for Multi-Index Models Beyond Statistical Queries",
    "authors": [
      "Daniel Barzilai",
      "Ohad Shamir"
    ],
    "abstract": "Understanding the limitations of gradient methods, and stochastic gradient descent (SGD) in particular, is a central challenge in learning theory. To that end, a commonly used tool is the Statistical Queries (SQ) framework, which studies performance limits of algorithms based on noisy interaction with the data. However, it is known that the formal connection between the SQ framework and SGD is tenuous: Existing results typically rely on adversarial or specially-structured gradient noise that does not reflect the noise in standard SGD, and (as we point out here) can sometimes lead to incorrect predictions. Moreover, many analyses of SGD for challenging problems rely on non-trivial algorithmic modifications, such as restricting the SGD trajectory to the sphere or using very small learning rates. To address these shortcomings, we develop a new, non-SQ framework to study the limitations of standard vanilla SGD, for single-index and multi-index models (namely, when the target function depends on a low-dimensional projection of the inputs). Our results apply to a broad class of settings and architectures, including (potentially deep) neural networks.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05704.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05704",
    "published": "2026-02-05T14:29:10Z",
    "updated": "2026-02-05T14:29:10Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05695",
    "title": "Determining Energy Efficiency Sweet Spots in Production LLM Inference",
    "authors": [
      "Hiari Pizzini Cavagna",
      "Andrea Proia",
      "Giacomo Madella",
      "Giovanni B. Esposito",
      "Francesco Antici",
      "Daniele Cesarini",
      "Zeynep Kiziltan",
      "Andrea Bartolini"
    ],
    "abstract": "Large Language Models (LLMs) inference is central in modern AI applications, making it critical to understand their energy footprint. Existing approaches typically estimate energy consumption through simple linear functions of input and output sequence lengths, yet our observations reveal clear Energy Efficiency regimes: peak efficiency occurs with short-to-moderate inputs and medium-length outputs, while efficiency drops sharply for long inputs or very short outputs, indicating a non-linear dependency. In this work, we propose an analytical model derived from the computational and memory-access complexity of the Transformer architecture, capable of accurately characterizing the efficiency curve as a function of input and output lengths. To assess its accuracy, we evaluate energy consumption using TensorRT-LLM on NVIDIA H100 GPUs across a diverse set of LLMs ranging from 1B to 9B parameters, including OPT, LLaMA, Gemma, Falcon, Qwen2, and Granite, tested over input and output lengths from 64 to 4096 tokens, achieving a mean MAPE of 1.79%. Our results show that aligning sequence lengths with these efficiency \"Sweet Spots\" can substantially reduce energy usage, supporting informed truncation, summarization, and adaptive generation strategies in production systems.",
    "categories": [
      "cs.AI",
      "cs.PF"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05695.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05695",
    "published": "2026-02-05T14:21:00Z",
    "updated": "2026-02-05T14:21:00Z",
    "comment": "To appear at ICPE 2026 (International Conference on Performance Engineering)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05694",
    "title": "Consensus-Aligned Neuron Efficient Fine-Tuning Large Language Models for Multi-Domain Machine Translation",
    "authors": [
      "Shuting Jiang",
      "Ran Song",
      "Yuxin Huang",
      "Yan Xiang",
      "Yantuan Xian",
      "Shengxiang Gao",
      "Zhengtao Yu"
    ],
    "abstract": "Multi-domain machine translation (MDMT) aims to build a unified model capable of translating content across diverse domains. Despite the impressive machine translation capabilities demonstrated by large language models (LLMs), domain adaptation still remains a challenge for LLMs. Existing MDMT methods such as in-context learning and parameter-efficient fine-tuning often suffer from domain shift, parameter interference and limited generalization. In this work, we propose a neuron-efficient fine-tuning framework for MDMT that identifies and updates consensus-aligned neurons within LLMs. These neurons are selected by maximizing the mutual information between neuron behavior and domain features, enabling LLMs to capture both generalizable translation patterns and domain-specific nuances. Our method then fine-tunes LLMs guided by these neurons, effectively mitigating parameter interference and domain-specific overfitting. Comprehensive experiments on three LLMs across ten German-English and Chinese-English translation domains evidence that our method consistently outperforms strong PEFT baselines on both seen and unseen domains, achieving state-of-the-art performance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05694.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05694",
    "published": "2026-02-05T14:20:59Z",
    "updated": "2026-02-05T14:20:59Z",
    "comment": "Accepted by AAAI 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05693",
    "title": "FedRandom: Sampling Consistent and Accurate Contribution Values in Federated Learning",
    "authors": [
      "Arno Geimer",
      "Beltran Fiz Pontiveros",
      "Radu State"
    ],
    "abstract": "Federated Learning is a privacy-preserving decentralized approach for Machine Learning tasks. In industry deployments characterized by a limited number of entities possessing abundant data, the significance of a participant's role in shaping the global model becomes pivotal given that participation in a federation incurs costs, and participants may expect compensation for their involvement. Additionally, the contributions of participants serve as a crucial means to identify and address potential malicious actors and free-riders. However, fairly assessing individual contributions remains a significant hurdle. Recent works have demonstrated a considerable inherent instability in contribution estimations across aggregation strategies. While employing a different strategy may offer convergence benefits, this instability can have potentially harming effects on the willingness of participants in engaging in the federation. In this work, we introduce FedRandom, a novel mitigation technique to the contribution instability problem. Tackling the instability as a statistical estimation problem, FedRandom allows us to generate more samples than when using regular FL strategies. We show that these additional samples provide a more consistent and reliable evaluation of participant contributions. We demonstrate our approach using different data distributions across CIFAR-10, MNIST, CIFAR-100 and FMNIST and show that FedRandom reduces the overall distance to the ground truth by more than a third in half of all evaluated scenarios, and improves stability in more than 90% of cases.",
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05693.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05693",
    "published": "2026-02-05T14:19:21Z",
    "updated": "2026-02-05T14:19:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05692",
    "title": "MedErrBench: A Fine-Grained Multilingual Benchmark for Medical Error Detection and Correction with Clinical Expert Annotations",
    "authors": [
      "Congbo Ma",
      "Yichun Zhang",
      "Yousef Al-Jazzazi",
      "Ahamed Foisal",
      "Laasya Sharma",
      "Yousra Sadqi",
      "Khaled Saleh",
      "Jihad Mallat",
      "Farah E. Shamout"
    ],
    "abstract": "Inaccuracies in existing or generated clinical text may lead to serious adverse consequences, especially if it is a misdiagnosis or incorrect treatment suggestion. With Large Language Models (LLMs) increasingly being used across diverse healthcare applications, comprehensive evaluation through dedicated benchmarks is crucial. However, such datasets remain scarce, especially across diverse languages and contexts. In this paper, we introduce MedErrBench, the first multilingual benchmark for error detection, localization, and correction, developed under the guidance of experienced clinicians. Based on an expanded taxonomy of ten common error types, MedErrBench covers English, Arabic and Chinese, with natural clinical cases annotated and reviewed by domain experts. We assessed the performance of a range of general-purpose, language-specific, and medical-domain language models across all three tasks. Our results reveal notable performance gaps, particularly in non-English settings, highlighting the need for clinically grounded, language-aware systems. By making MedErrBench and our evaluation protocols publicly-available, we aim to advance multilingual clinical NLP to promote safer and more equitable AI-based healthcare globally. The dataset is available in the supplementary material. An anonymized version of the dataset is available at: https://github.com/congboma/MedErrBench.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05692.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05692",
    "published": "2026-02-05T14:18:20Z",
    "updated": "2026-02-05T14:18:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05690",
    "title": "Almost Asymptotically Optimal Active Clustering Through Pairwise Observations",
    "authors": [
      "Rachel S. Y. Teo",
      "P. N. Karthik",
      "Ramya Korlakai Vinayak",
      "Vincent Y. F. Tan"
    ],
    "abstract": "We propose a new analysis framework for clustering $M$ items into an unknown number of $K$ distinct groups using noisy and actively collected responses. At each time step, an agent is allowed to query pairs of items and observe bandit binary feedback. If the pair of items belongs to the same (resp.\\ different) cluster, the observed feedback is $1$ with probability $p>1/2$ (resp.\\ $q<1/2$). Leveraging the ubiquitous change-of-measure technique, we establish a fundamental lower bound on the expected number of queries needed to achieve a desired confidence in the clustering accuracy, formulated as a sup-inf optimization problem. Building on this theoretical foundation, we design an asymptotically optimal algorithm in which the stopping criterion involves an empirical version of the inner infimum -- the Generalized Likelihood Ratio (GLR) statistic -- being compared to a threshold. We develop a computationally feasible variant of the GLR statistic and show that its performance gap to the lower bound can be accurately empirically estimated and remains within a constant multiple of the lower bound.",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05690.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05690",
    "published": "2026-02-05T14:16:47Z",
    "updated": "2026-02-05T14:16:47Z",
    "comment": "31 pages, 1 figure",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05688",
    "title": "Mining Generalizable Activation Functions",
    "authors": [
      "Alex Vitvitskyi",
      "Michael Boratko",
      "Matej Grcic",
      "Razvan Pascanu",
      "Deep Shah",
      "Petar VeliÄkoviÄ"
    ],
    "abstract": "The choice of activation function is an active area of research, with different proposals aimed at improving optimization, while maintaining expressivity. Additionally, the activation function can significantly alter the implicit inductive bias of the architecture, controlling its non-linear behavior. In this paper, in line with previous work, we argue that evolutionary search provides a useful framework for finding new activation functions, while we also make two novel observations. The first is that modern pipelines, such as AlphaEvolve, which relies on frontier LLMs as a mutator operator, allows for a much wider and flexible search space; e.g., over all possible python functions within a certain FLOP budget, eliminating the need for manually constructed search spaces. In addition, these pipelines will be biased towards meaningful activation functions, given their ability to represent common knowledge, leading to a potentially more efficient search of the space. The second observation is that, through this framework, one can target not only performance improvements but also activation functions that encode particular inductive biases. This can be done by using performance on out-of-distribution data as a fitness function, reflecting the degree to which the architecture respects the inherent structure in the data in a manner independent of distribution shifts. We carry an empirical exploration of this proposal and show that relatively small scale synthetic datasets can be sufficient for AlphaEvolve to discover meaningful activations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05688.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05688",
    "published": "2026-02-05T14:13:40Z",
    "updated": "2026-02-05T14:13:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05679",
    "title": "Perception-Based Beliefs for POMDPs with Visual Observations",
    "authors": [
      "Miriam SchÃ¤fers",
      "Merlijn Krale",
      "Thiago D. SimÃ£o",
      "Nils Jansen",
      "Maximilian Weininger"
    ],
    "abstract": "Partially observable Markov decision processes (POMDPs) are a principled planning model for sequential decision-making under uncertainty. Yet, real-world problems with high-dimensional observations, such as camera images, remain intractable for traditional belief- and filtering-based solvers. To tackle this problem, we introduce the Perception-based Beliefs for POMDPs framework (PBP), which complements such solvers with a perception model. This model takes the form of an image classifier which maps visual observations to probability distributions over states. PBP incorporates these distributions directly into belief updates, so the underlying solver does not need to reason explicitly over high-dimensional observation spaces. We show that the belief update of PBP coincides with the standard belief update if the image classifier is exact. Moreover, to handle classifier imprecision, we incorporate uncertainty quantification and introduce two methods to adjust the belief update accordingly. We implement PBP using two traditional POMDP solvers and empirically show that (1) it outperforms existing end-to-end deep RL methods and (2) uncertainty quantification improves robustness of PBP against visual corruption.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05679.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05679",
    "published": "2026-02-05T14:01:39Z",
    "updated": "2026-02-05T14:01:39Z",
    "comment": "Accepted at AAMAS 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05676",
    "title": "ShapeUP: Scalable Image-Conditioned 3D Editing",
    "authors": [
      "Inbar Gat",
      "Dana Cohen-Bar",
      "Guy Levy",
      "Elad Richardson",
      "Daniel Cohen-Or"
    ],
    "abstract": "Recent advancements in 3D foundation models have enabled the generation of high-fidelity assets, yet precise 3D manipulation remains a significant challenge. Existing 3D editing frameworks often face a difficult trade-off between visual controllability, geometric consistency, and scalability. Specifically, optimization-based methods are prohibitively slow, multi-view 2D propagation techniques suffer from visual drift, and training-free latent manipulation methods are inherently bound by frozen priors and cannot directly benefit from scaling. In this work, we present ShapeUP, a scalable, image-conditioned 3D editing framework that formulates editing as a supervised latent-to-latent translation within a native 3D representation. This formulation allows ShapeUP to build on a pretrained 3D foundation model, leveraging its strong generative prior while adapting it to editing through supervised training. In practice, ShapeUP is trained on triplets consisting of a source 3D shape, an edited 2D image, and the corresponding edited 3D shape, and learns a direct mapping using a 3D Diffusion Transformer (DiT). This image-as-prompt approach enables fine-grained visual control over both local and global edits and achieves implicit, mask-free localization, while maintaining strict structural consistency with the original asset. Our extensive evaluations demonstrate that ShapeUP consistently outperforms current trained and training-free baselines in both identity preservation and edit fidelity, offering a robust and scalable paradigm for native 3D content creation.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05676.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05676",
    "published": "2026-02-05T13:59:16Z",
    "updated": "2026-02-05T13:59:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05668",
    "title": "Stable but Wrong: When More Data Degrades Scientific Conclusions",
    "authors": [
      "Zhipeng Zhang",
      "Kai Li"
    ],
    "abstract": "Modern science increasingly relies on ever-growing observational datasets and automated inference pipelines, under the implicit belief that accumulating more data makes scientific conclusions more reliable. Here we show that this belief can fail in a fundamental and irreversible way. We identify a structural regime in which standard inference procedures converge smoothly, remain well calibrated, and pass conventional diagnostic checks, yet systematically converge to incorrect conclusions. This failure arises when the reliability of observations degrades in a manner that is intrinsically unobservable to the inference process itself. Using minimal synthetic experiments, we demonstrate that in this regime additional data do not correct error but instead amplify it, while residual-based and goodness-of-fit diagnostics remain misleadingly normal. These results reveal an intrinsic limit of data-driven science: stability, convergence, and confidence are not sufficient indicators of epistemic validity. We argue that inference cannot be treated as an unconditional consequence of data availability, but must instead be governed by explicit constraints on the integrity of the observational process.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05668.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05668",
    "published": "2026-02-05T13:51:47Z",
    "updated": "2026-02-05T13:51:47Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05667",
    "title": "Accelerating Benchmarking of Functional Connectivity Modeling via Structure-aware Core-set Selection",
    "authors": [
      "Ling Zhan",
      "Zhen Li",
      "Junjie Huang",
      "Tao Jia"
    ],
    "abstract": "Benchmarking the hundreds of functional connectivity (FC) modeling methods on large-scale fMRI datasets is critical for reproducible neuroscience. However, the combinatorial explosion of model-data pairings makes exhaustive evaluation computationally prohibitive, preventing such assessments from becoming a routine pre-analysis step. To break this bottleneck, we reframe the challenge of FC benchmarking by selecting a small, representative core-set whose sole purpose is to preserve the relative performance ranking of FC operators. We formalize this as a ranking-preserving subset selection problem and propose Structure-aware Contrastive Learning for Core-set Selection (SCLCS), a self-supervised framework to select these core-sets. SCLCS first uses an adaptive Transformer to learn each sample's unique FC structure. It then introduces a novel Structural Perturbation Score (SPS) to quantify the stability of these learned structures during training, identifying samples that represent foundational connectivity archetypes. Finally, while SCLCS identifies stable samples via a top-k ranking, we further introduce a density-balanced sampling strategy as a necessary correction to promote diversity, ensuring the final core-set is both structurally robust and distributionally representative. On the large-scale REST-meta-MDD dataset, SCLCS preserves the ground-truth model ranking with just 10% of the data, outperforming state-of-the-art (SOTA) core-set selection methods by up to 23.2% in ranking consistency (nDCG@k). To our knowledge, this is the first work to formalize core-set selection for FC operator benchmarking, thereby making large-scale operators comparisons a feasible and integral part of computational neuroscience. Code is publicly available on https://github.com/lzhan94swu/SCLCS",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05667.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05667",
    "published": "2026-02-05T13:50:39Z",
    "updated": "2026-02-05T13:50:39Z",
    "comment": "33 pages, 8 figures, ICLR conference paper",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05665",
    "title": "Graph-based Agent Memory: Taxonomy, Techniques, and Applications",
    "authors": [
      "Chang Yang",
      "Chuang Zhou",
      "Yilin Xiao",
      "Su Dong",
      "Luyao Zhuang",
      "Yujing Zhang",
      "Zhu Wang",
      "Zijin Hong",
      "Zheng Yuan",
      "Zhishang Xiang",
      "Shengyuan Chen",
      "Huachi Zhou",
      "Qinggang Zhang",
      "Ninghao Liu",
      "Jinsong Su",
      "Xinrun Wang",
      "Yi Chang",
      "Xiao Huang"
    ],
    "abstract": "Memory emerges as the core module in the Large Language Model (LLM)-based agents for long-horizon complex tasks (e.g., multi-turn dialogue, game playing, scientific discovery), where memory can enable knowledge accumulation, iterative reasoning and self-evolution. Among diverse paradigms, graph stands out as a powerful structure for agent memory due to the intrinsic capabilities to model relational dependencies, organize hierarchical information, and support efficient retrieval. This survey presents a comprehensive review of agent memory from the graph-based perspective. First, we introduce a taxonomy of agent memory, including short-term vs. long-term memory, knowledge vs. experience memory, non-structural vs. structural memory, with an implementation view of graph-based memory. Second, according to the life cycle of agent memory, we systematically analyze the key techniques in graph-based agent memory, covering memory extraction for transforming the data into the contents, storage for organizing the data efficiently, retrieval for retrieving the relevant contents from memory to support reasoning, and evolution for updating the contents in the memory. Third, we summarize the open-sourced libraries and benchmarks that support the development and evaluation of self-evolving agent memory. We also explore diverse application scenarios. Finally, we identify critical challenges and future research directions. This survey aims to offer actionable insights to advance the development of more efficient and reliable graph-based agent memory systems. All the related resources, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphMemory.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05665.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05665",
    "published": "2026-02-05T13:49:05Z",
    "updated": "2026-02-05T13:49:05Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05660",
    "title": "Probabilistic Multi-Regional Solar Power Forecasting with Any-Quantile Recurrent Neural Networks",
    "authors": [
      "Slawek Smyl",
      "PaweÅ PeÅka",
      "Grzegorz Dudek"
    ],
    "abstract": "The increasing penetration of photovoltaic (PV) generation introduces significant uncertainty into power system operation, necessitating forecasting approaches that extend beyond deterministic point predictions. This paper proposes an any-quantile probabilistic forecasting framework for multi-regional PV power generation based on the Any-Quantile Recurrent Neural Network (AQ-RNN). The model integrates an any-quantile forecasting paradigm with a dual-track recurrent architecture that jointly processes series-specific and cross-regional contextual information, supported by dilated recurrent cells, patch-based temporal modeling, and a dynamic ensemble mechanism.   The proposed framework enables the estimation of calibrated conditional quantiles at arbitrary probability levels within a single trained model and effectively exploits spatial dependencies to enhance robustness at the system level. The approach is evaluated using 30 years of hourly PV generation data from 259 European regions and compared against established statistical and neural probabilistic baselines. The results demonstrate consistent improvements in forecast accuracy, calibration, and prediction interval quality, underscoring the suitability of the proposed method for uncertainty-aware energy management and operational decision-making in renewable-dominated power systems.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05660.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05660",
    "published": "2026-02-05T13:43:18Z",
    "updated": "2026-02-05T13:43:18Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05657",
    "title": "Tight Long-Term Tail Decay of (Clipped) SGD in Non-Convex Optimization",
    "authors": [
      "Aleksandar Armacki",
      "Dragana BajoviÄ",
      "DuÅ¡an JakovetiÄ",
      "Soummya Kar",
      "Ali H. Sayed"
    ],
    "abstract": "The study of tail behaviour of SGD-induced processes has been attracting a lot of interest, due to offering strong guarantees with respect to individual runs of an algorithm. While many works provide high-probability guarantees, quantifying the error rate for a fixed probability threshold, there is a lack of work directly studying the probability of failure, i.e., quantifying the tail decay rate for a fixed error threshold. Moreover, existing results are of finite-time nature, limiting their ability to capture the true long-term tail decay which is more informative for modern learning models, typically trained for millions of iterations. Our work closes these gaps, by studying the long-term tail decay of SGD-based methods through the lens of large deviations theory, establishing several strong results in the process. First, we provide an upper bound on the tails of the gradient norm-squared of the best iterate produced by (vanilla) SGD, for non-convex costs and bounded noise, with long-term decay at rate $e^{-t/\\log(t)}$. Next, we relax the noise assumption by considering clipped SGD (c-SGD) under heavy-tailed noise with bounded moment of order $p \\in (1,2]$, showing an upper bound with long-term decay at rate $e^{-t^{Î²_p}/\\log(t)}$, where $Î²_p = \\frac{4(p-1)}{3p-2}$ for $p \\in (1,2)$ and $e^{-t/\\log^2(t)}$ for $p = 2$. Finally, we provide lower bounds on the tail decay, at rate $e^{-t}$, showing that our rates for both SGD and c-SGD are tight, up to poly-logarithmic factors. Notably, our results demonstrate an order of magnitude faster long-term tail decay compared to existing work based on finite-time bounds, which show rates $e^{-\\sqrt{t}}$ and $e^{-t^{Î²_p/2}}$, $p \\in (1,2]$, for SGD and c-SGD, respectively. As such, we uncover regimes where the tails decay much faster than previously known, providing stronger long-term guarantees for individual runs.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05657.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05657",
    "published": "2026-02-05T13:41:13Z",
    "updated": "2026-02-05T13:41:13Z",
    "comment": "32 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05656",
    "title": "Alignment Verifiability in Large Language Models: Normative Indistinguishability under Behavioral Evaluation",
    "authors": [
      "Igor Santos-Grueiro"
    ],
    "abstract": "Behavioral evaluation is the dominant paradigm for assessing alignment in large language models (LLMs). In practice, alignment is inferred from performance under finite evaluation protocols - benchmarks, red-teaming suites, or automated pipelines - and observed compliance is often treated as evidence of underlying alignment. This inference step, from behavioral evidence to claims about latent alignment properties, is typically implicit and rarely analyzed as an inference problem in its own right.   We study this problem formally. We frame alignment evaluation as an identifiability question under partial observability and allow agent behavior to depend on information correlated with the evaluation regime. Within this setting, we introduce the Alignment Verifiability Problem and the notion of Normative Indistinguishability, capturing when distinct latent alignment hypotheses induce identical distributions over all evaluator-accessible signals.   Our main result is a negative but sharply delimited identifiability theorem. Under finite behavioral evaluation and evaluation-aware agents, observed behavioral compliance does not uniquely identify latent alignment. That is, even idealized behavioral evaluation cannot, in general, certify alignment as a latent property.   We further show that behavioral alignment tests should be interpreted as estimators of indistinguishability classes rather than verifiers of alignment. Passing increasingly stringent tests may reduce the space of compatible hypotheses, but cannot collapse it to a singleton under the stated conditions. This reframes alignment benchmarks as providing upper bounds on observable compliance within a regime, rather than guarantees of underlying alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05656.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05656",
    "published": "2026-02-05T13:40:56Z",
    "updated": "2026-02-05T13:40:56Z",
    "comment": "10 pages. Theoretical analysis of behavioral alignment evaluation",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05650",
    "title": "Enhancing Personality Recognition by Comparing the Predictive Power of Traits, Facets, and Nuances",
    "authors": [
      "Amir Ansari",
      "Jana Subirana",
      "Bruna Silva",
      "Sergio Escalera",
      "David Gallardo-Pujol",
      "Cristina Palmero"
    ],
    "abstract": "Personality is a complex, hierarchical construct typically assessed through item-level questionnaires aggregated into broad trait scores. Personality recognition models aim to infer personality traits from different sources of behavioral data. However, reliance on broad trait scores as ground truth, combined with limited training data, poses challenges for generalization, as similar trait scores can manifest through diverse, context dependent behaviors. In this work, we explore the predictive impact of the more granular hierarchical levels of the Big-Five Personality Model, facets and nuances, to enhance personality recognition from audiovisual interaction data. Using the UDIVA v0.5 dataset, we trained a transformer-based model including cross-modal (audiovisual) and cross-subject (dyad-aware) attention mechanisms. Results show that nuance-level models consistently outperform facet and trait-level models, reducing mean squared error by up to 74% across interaction scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05650.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05650",
    "published": "2026-02-05T13:35:04Z",
    "updated": "2026-02-05T13:35:04Z",
    "comment": "Accepted to the 2025 13th International Conference on Affective Computing and Intelligent Interaction (Late Breaking Results)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05649",
    "title": "End-to-End Compression for Tabular Foundation Models",
    "authors": [
      "Guri ZabÃ«rgja",
      "Rafiq Kamel",
      "Arlind Kadra",
      "Christian M. M. Frey",
      "Josif Grabocka"
    ],
    "abstract": "The long-standing dominance of gradient-boosted decision trees for tabular data has recently been challenged by in-context learning tabular foundation models. In-context learning methods fit and predict in one forward pass without parameter updates by leveraging the training data as context for predicting on query test points. While recent tabular foundation models achieve state-of-the-art performance, their transformer architecture based on the attention mechanism has quadratic complexity regarding dataset size, which in turn increases the overhead on training and inference time, and limits the capacity of the models to handle large-scale datasets. In this work, we propose TACO, an end-to-end tabular compression model that compresses the training dataset in a latent space. We test our method on the TabArena benchmark, where our proposed method is up to 94x faster in inference time, while consuming up to 97\\% less memory compared to the state-of-the-art tabular transformer architecture, all while retaining performance without significant degradation. Lastly, our method not only scales better with increased dataset sizes, but it also achieves better performance compared to other baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05649.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05649",
    "published": "2026-02-05T13:33:58Z",
    "updated": "2026-02-05T13:33:58Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05648",
    "title": "Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization of Turkish and Hebrew",
    "authors": [
      "Giuseppe Samo",
      "Paola Merlo"
    ],
    "abstract": "We investigate how transformer models represent complex verb paradigms in Turkish and Modern Hebrew, concentrating on how tokenization strategies shape this ability. Using the Blackbird Language Matrices task on natural data, we show that for Turkish -- with its transparent morphological markers -- both monolingual and multilingual models succeed, either when tokenization is atomic or when it breaks words into small subword units. For Hebrew, instead, monolingual and multilingual models diverge. A multilingual model using character-level tokenization fails to capture the language non-concatenative morphology, but a monolingual model with morpheme-aware segmentation performs well. Performance improves on more synthetic datasets, in all models.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05648.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05648",
    "published": "2026-02-05T13:31:21Z",
    "updated": "2026-02-05T13:31:21Z",
    "comment": "13 pages, 7 figures, to appear as proceedings of the SIGTURK 2026 Workshop",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05646",
    "title": "Empowering Time Series Analysis with Large-Scale Multimodal Pretraining",
    "authors": [
      "Peng Chen",
      "Siyuan Wang",
      "Shiyan Hu",
      "Xingjian Wu",
      "Yang Shu",
      "Zhongwen Rao",
      "Meng Wang",
      "Yijie Li",
      "Bin Yang",
      "Chenjuan Guo"
    ],
    "abstract": "While existing time series foundation models primarily rely on large-scale unimodal pretraining, they lack complementary modalities to enhance time series understanding. Building multimodal foundation models is a natural next step, but it faces key challenges: 1) lack of a unified multimodal pretraining paradigm and large-scale multimodal corpora for time series analysis; 2) how to effectively integrate heterogeneous modalities and enhance model generalization. To address these challenges, we take an early step toward multimodal foundation models for time series analysis. We first propose a multimodal pretraining paradigm that leverages time series with endogenous modalities (derived images and text) and exogenous knowledge (real-world news), providing a comprehensive multi-view perspective for time series analysis. To support this, we develop an automated data construction pipeline to curate MM-TS, the first large-scale multimodal time series dataset spanning six domains, with up to one billion points. Then we propose HORAI, a frequency-enhanced multimodal foundation model. It integrates two core components: the Frequency-enhanced Cross-Modality Encoder and the Time-Frequency Decoder, designed to effectively fuse multimodal features and enhance model generalization across modalities and domains. After pretraining on MM-TS, HORAI achieves state-of-the-art zero-shot performance on time series forecasting and anomaly detection tasks, demonstrating strong generalization.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05646.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05646",
    "published": "2026-02-05T13:26:35Z",
    "updated": "2026-02-05T13:26:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05639",
    "title": "Joint Embedding Variational Bayes",
    "authors": [
      "Amin Oji",
      "Paul Fieguth"
    ],
    "abstract": "We introduce Variational Joint Embedding (VJE), a framework that synthesizes joint embedding and variational inference to enable self-supervised learning of probabilistic representations in a reconstruction-free, non-contrastive setting. Compared to energy-based predictive objectives that optimize pointwise discrepancies, VJE maximizes a symmetric conditional evidence lower bound (ELBO) for a latent-variable model defined directly on encoder embeddings. We instantiate the conditional likelihood with a heavy-tailed Student-$t$ model using a polar decomposition that explicitly decouples directional and radial factors to prevent norm-induced instabilities during training. VJE employs an amortized inference network to parameterize a diagonal Gaussian variational posterior whose feature-wise variances are shared with the likelihood scale to capture anisotropic uncertainty without auxiliary projection heads. Across ImageNet-1K, CIFAR-10/100, and STL-10, VJE achieves performance comparable to standard non-contrastive baselines under linear and k-NN evaluation. We further validate these probabilistic semantics through one-class CIFAR-10 anomaly detection, where likelihood-based scoring under the proposed model outperforms comparable self-supervised baselines.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05639.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05639",
    "published": "2026-02-05T13:18:53Z",
    "updated": "2026-02-05T13:18:53Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05638",
    "title": "UniSurg: A Video-Native Foundation Model for Universal Understanding of Surgical Videos",
    "authors": [
      "Jinlin Wu",
      "Felix Holm",
      "Chuxi Chen",
      "An Wang",
      "Yaxin Hu",
      "Xiaofan Ye",
      "Zelin Zang",
      "Miao Xu",
      "Lihua Zhou",
      "Huai Liao",
      "Danny T. M. Chan",
      "Ming Feng",
      "Wai S. Poon",
      "Hongliang Ren",
      "Dong Yi",
      "Nassir Navab",
      "Gaofeng Meng",
      "Jiebo Luo",
      "Hongbin Liu",
      "Zhen Lei"
    ],
    "abstract": "While foundation models have advanced surgical video analysis, current approaches rely predominantly on pixel-level reconstruction objectives that waste model capacity on low-level visual details - such as smoke, specular reflections, and fluid motion - rather than semantic structures essential for surgical understanding. We present UniSurg, a video-native foundation model that shifts the learning paradigm from pixel-level reconstruction to latent motion prediction. Built on the Video Joint Embedding Predictive Architecture (V-JEPA), UniSurg introduces three key technical innovations tailored to surgical videos: 1) motion-guided latent prediction to prioritize semantically meaningful regions, 2) spatiotemporal affinity self-distillation to enforce relational consistency, and 3) feature diversity regularization to prevent representation collapse in texture-sparse surgical scenes. To enable large-scale pretraining, we curate UniSurg-15M, the largest surgical video dataset to date, comprising 3,658 hours of video from 50 sources across 13 anatomical regions. Extensive experiments across 17 benchmarks demonstrate that UniSurg significantly outperforms state-of-the-art methods on surgical workflow recognition (+14.6% F1 on EgoSurgery, +10.3% on PitVis), action triplet recognition (39.54% mAP-IVT on CholecT50), skill assessment, polyp segmentation, and depth estimation. These results establish UniSurg as a new standard for universal, motion-oriented surgical video understanding.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05638.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05638",
    "published": "2026-02-05T13:18:33Z",
    "updated": "2026-02-05T13:18:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05636",
    "title": "Generative Ontology: When Structured Knowledge Learns to Create",
    "authors": [
      "Benny Cheung"
    ],
    "abstract": "Traditional ontologies excel at describing domain structure but cannot generate novel artifacts. Large language models generate fluently but produce outputs that lack structural validity, hallucinating mechanisms without components, goals without end conditions. We introduce Generative Ontology, a framework that synthesizes these complementary strengths: ontology provides the grammar; the LLM provides the creativity.   Generative Ontology encodes domain knowledge as executable Pydantic schemas that constrain LLM generation via DSPy signatures. A multi-agent pipeline assigns specialized roles to different ontology domains: a Mechanics Architect designs game systems, a Theme Weaver integrates narrative, a Balance Critic identifies exploits. Each agent carrying a professional \"anxiety\" that prevents shallow, agreeable outputs. Retrieval-augmented generation grounds novel designs in precedents from existing exemplars, while iterative validation ensures coherence between mechanisms and components.   We demonstrate the framework through GameGrammar, a system for generating complete tabletop game designs. Given a thematic prompt (\"bioluminescent fungi competing in a cave ecosystem\"), the pipeline produces structurally complete, playable game specifications with mechanisms, components, victory conditions, and setup instructions. These outputs satisfy ontological constraints while remaining genuinely creative.   The pattern generalizes beyond games. Any domain with expert vocabulary, validity constraints, and accumulated exemplars (music composition, software architecture, culinary arts) is a candidate for Generative Ontology. We argue that constraints do not limit creativity but enable it: just as grammar makes poetry possible, ontology makes structured generation possible.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05636.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05636",
    "published": "2026-02-05T13:14:20Z",
    "updated": "2026-02-05T13:14:20Z",
    "comment": "15 pages, 6 figures, 6 tables. Code available at https://github.com/bennycheung/GameGrammarCLI",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05635",
    "title": "Structural Disentanglement in Bilinear MLPs via Architectural Inductive Bias",
    "authors": [
      "Ojasva Nema",
      "Kaustubh Sharma",
      "Aditya Chauhan",
      "Parikshit Pareek"
    ],
    "abstract": "Selective unlearning and long-horizon extrapolation remain fragile in modern neural networks, even when tasks have underlying algebraic structure. In this work, we argue that these failures arise not solely from optimization or unlearning algorithms, but from how models structure their internal representations during training. We explore if having explicit multiplicative interactions as an architectural inductive bias helps in structural disentanglement, through Bilinear MLPs. We show analytically that bilinear parameterizations possess a `non-mixing' property under gradient flow conditions, where functional components separate into orthogonal subspace representations. This provides a mathematical foundation for surgical model modification. We validate this hypothesis through a series of controlled experiments spanning modular arithmetic, cyclic reasoning, Lie group dynamics, and targeted unlearning benchmarks. Unlike pointwise nonlinear networks, multiplicative architectures are able to recover true operators aligned with the underlying algebraic structure. Our results suggest that model editability and generalization are constrained by representational structure, and that architectural inductive bias plays a central role in enabling reliable unlearning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05635.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05635",
    "published": "2026-02-05T13:14:01Z",
    "updated": "2026-02-05T13:14:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05633",
    "title": "CASTLE: A Comprehensive Benchmark for Evaluating Student-Tailored Personalized Safety in Large Language Models",
    "authors": [
      "Rui Jia",
      "Ruiyi Lan",
      "Fengrui Liu",
      "Zhongxiang Dai",
      "Bo Jiang",
      "Jing Shao",
      "Jingyuan Chen",
      "Guandong Xu",
      "Fei Wu",
      "Min Zhang"
    ],
    "abstract": "Large language models (LLMs) have advanced the development of personalized learning in education. However, their inherent generation mechanisms often produce homogeneous responses to identical prompts. This one-size-fits-all mechanism overlooks the substantial heterogeneity in students cognitive and psychological, thereby posing potential safety risks to vulnerable groups. Existing safety evaluations primarily rely on context-independent metrics such as factual accuracy, bias, or toxicity, which fail to capture the divergent harms that the same response might cause across different student attributes. To address this gap, we propose the concept of Student-Tailored Personalized Safety and construct CASTLE based on educational theories. This benchmark covers 15 educational safety risks and 14 student attributes, comprising 92,908 bilingual scenarios. We further design three evaluation metrics: Risk Sensitivity, measuring the model ability to detect risks; Emotional Empathy, evaluating the model capacity to recognize student states; and Student Alignment, assessing the match between model responses and student attributes. Experiments on 18 SOTA LLMs demonstrate that CASTLE poses a significant challenge: all models scored below an average safety rating of 2.3 out of 5, indicating substantial deficiencies in personalized safety assurance.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05633.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05633",
    "published": "2026-02-05T13:13:19Z",
    "updated": "2026-02-05T13:13:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05630",
    "title": "Rewards as Labels: Revisiting RLVR from a Classification Perspective",
    "authors": [
      "Zepeng Zhai",
      "Meilin Chen",
      "Jiaxuan Zhao",
      "Junlang Qian",
      "Lei Shen",
      "Yuan Lu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards has recently advanced the capabilities of Large Language Models in complex reasoning tasks by providing explicit rule-based supervision. Among RLVR methods, GRPO and its variants have achieved strong empirical performance. Despite their success, we identify that they suffer from Gradient Misassignment in Positives and Gradient Domination in Negatives, which lead to inefficient and suboptimal policy updates. To address these issues, we propose Rewards as Labels (REAL), a novel framework that revisits verifiable rewards as categorical labels rather than scalar weights, thereby reformulating policy optimization as a classification problem. Building on this, we further introduce anchor logits to enhance policy learning. Our analysis reveals that REAL induces a monotonic and bounded gradient weighting, enabling balanced gradient allocation across rollouts and effectively mitigating the identified mismatches. Extensive experiments on mathematical reasoning benchmarks show that REAL improves training stability and consistently outperforms GRPO and strong variants such as DAPO. On the 1.5B model, REAL improves average Pass@1 over DAPO by 6.7%. These gains further scale to 7B model, REAL continues to outperform DAPO and GSPO by 6.2% and 1.7%, respectively. Notably, even with a vanilla binary cross-entropy, REAL remains stable and exceeds DAPO by 4.5% on average.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05630.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05630",
    "published": "2026-02-05T13:11:36Z",
    "updated": "2026-02-05T13:11:36Z",
    "comment": "12 pages, 5 figures, 4 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05629",
    "title": "ROMAN: Reward-Orchestrated Multi-Head Attention Network for Autonomous Driving System Testing",
    "authors": [
      "Jianlei Chi",
      "Yuzhen Wu",
      "Jiaxuan Hou",
      "Xiaodong Zhang",
      "Ming Fan",
      "Suhui Sun",
      "Weijun Dai",
      "Bo Li",
      "Jianguo Sun",
      "Jun Sun"
    ],
    "abstract": "Automated Driving System (ADS) acts as the brain of autonomous vehicles, responsible for their safety and efficiency. Safe deployment requires thorough testing in diverse real-world scenarios and compliance with traffic laws like speed limits, signal obedience, and right-of-way rules. Violations like running red lights or speeding pose severe safety risks. However, current testing approaches face significant challenges: limited ability to generate complex and high-risk law-breaking scenarios, and failing to account for complex interactions involving multiple vehicles and critical situations. To address these challenges, we propose ROMAN, a novel scenario generation approach for ADS testing that combines a multi-head attention network with a traffic law weighting mechanism. ROMAN is designed to generate high-risk violation scenarios to enable more thorough and targeted ADS evaluation. The multi-head attention mechanism models interactions among vehicles, traffic signals, and other factors. The traffic law weighting mechanism implements a workflow that leverages an LLM-based risk weighting module to evaluate violations based on the two dimensions of severity and occurrence. We have evaluated ROMAN by testing the Baidu Apollo ADS within the CARLA simulation platform and conducting extensive experiments to measure its performance. Experimental results demonstrate that ROMAN surpassed state-of-the-art tools ABLE and LawBreaker by achieving 7.91% higher average violation count than ABLE and 55.96% higher than LawBreaker, while also maintaining greater scenario diversity. In addition, only ROMAN successfully generated violation scenarios for every clause of the input traffic laws, enabling it to identify more high-risk violations than existing approaches.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05629.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05629",
    "published": "2026-02-05T13:09:58Z",
    "updated": "2026-02-05T13:09:58Z",
    "comment": "The manuscript includes 13 pages, 8 tables, and 7 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05625",
    "title": "Reactive Knowledge Representation and Asynchronous Reasoning",
    "authors": [
      "Simon Kohaut",
      "Benedict Flade",
      "Julian Eggert",
      "Kristian Kersting",
      "Devendra Singh Dhami"
    ],
    "abstract": "Exact inference in complex probabilistic models often incurs prohibitive computational costs. This challenge is particularly acute for autonomous agents in dynamic environments that require frequent, real-time belief updates. Existing methods are often inefficient for ongoing reasoning, as they re-evaluate the entire model upon any change, failing to exploit that real-world information streams have heterogeneous update rates. To address this, we approach the problem from a reactive, asynchronous, probabilistic reasoning perspective. We first introduce Resin (Reactive Signal Inference), a probabilistic programming language that merges probabilistic logic with reactive programming. Furthermore, to provide efficient and exact semantics for Resin, we propose Reactive Circuits (RCs). Formulated as a meta-structure over Algebraic Circuits and asynchronous data streams, RCs are time-dynamic Directed Acyclic Graphs that autonomously adapt themselves based on the volatility of input signals. In high-fidelity drone swarm simulations, our approach achieves several orders of magnitude of speedup over frequency-agnostic inference. We demonstrate that RCs' structural adaptations successfully capture environmental dynamics, significantly reducing latency and facilitating reactive real-time reasoning. By partitioning computations based on the estimated Frequency of Change in the asynchronous inputs, large inference tasks can be decomposed into individually memoized sub-problems. This ensures that only the specific components of a model affected by new information are re-evaluated, drastically reducing redundant computation in streaming contexts.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05625.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05625",
    "published": "2026-02-05T13:02:01Z",
    "updated": "2026-02-05T13:02:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05619",
    "title": "Mode-Dependent Rectification for Stable PPO Training",
    "authors": [
      "Mohamad Mohamad",
      "Francesco Ponzio",
      "Xavier Descombes"
    ],
    "abstract": "Mode-dependent architectural components (layers that behave differently during training and evaluation, such as Batch Normalization or dropout) are commonly used in visual reinforcement learning but can destabilize on-policy optimization. We show that in Proximal Policy Optimization (PPO), discrepancies between training and evaluation behavior induced by Batch Normalization lead to policy mismatch, distributional drift, and reward collapse. We propose Mode-Dependent Rectification (MDR), a lightweight dual-phase training procedure that stabilizes PPO under mode-dependent layers without architectural changes. Experiments across procedurally generated games and real-world patch-localization tasks demonstrate that MDR consistently improves stability and performance, and extends naturally to other mode-dependent layers.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05619.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05619",
    "published": "2026-02-05T12:54:19Z",
    "updated": "2026-02-05T12:54:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05617",
    "title": "Unified Sensor Simulation for Autonomous Driving",
    "authors": [
      "Nikolay Patakin",
      "Arsenii Shirokov",
      "Anton Konushin",
      "Dmitry Senushkin"
    ],
    "abstract": "In this work, we introduce \\textbf{XSIM}, a sensor simulation framework for autonomous driving. XSIM extends 3DGUT splatting with a generalized rolling-shutter modeling tailored for autonomous driving applications. Our framework provides a unified and flexible formulation for appearance and geometric sensor modeling, enabling rendering of complex sensor distortions in dynamic environments. We identify spherical cameras, such as LiDARs, as a critical edge case for existing 3DGUT splatting due to cyclic projection and time discontinuities at azimuth boundaries leading to incorrect particle projection. To address this issue, we propose a phase modeling mechanism that explicitly accounts temporal and shape discontinuities of Gaussians projected by the Unscented Transform at azimuth borders. In addition, we introduce an extended 3D Gaussian representation that incorporates two distinct opacity parameters to resolve mismatches between geometry and color distributions. As a result, our framework provides enhanced scene representations with improved geometric consistency and photorealistic appearance. We evaluate our framework extensively on multiple autonomous driving datasets, including Waymo Open Dataset, Argoverse 2, and PandaSet. Our framework consistently outperforms strong recent baselines and achieves state-of-the-art performance across all datasets. The source code is publicly available at \\href{https://github.com/whesense/XSIM}{https://github.com/whesense/XSIM}.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05617.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05617",
    "published": "2026-02-05T12:52:46Z",
    "updated": "2026-02-05T12:52:46Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05616",
    "title": "Path-Guided Flow Matching for Dataset Distillation",
    "authors": [
      "Xuhui Li",
      "Zhengquan Luo",
      "Xiwei Liu",
      "Yongqiang Yu",
      "Zhiqiang Xu"
    ],
    "abstract": "Dataset distillation compresses large datasets into compact synthetic sets with comparable performance in training models. Despite recent progress on diffusion-based distillation, this type of method typically depends on heuristic guidance or prototype assignment, which comes with time-consuming sampling and trajectory instability and thus hurts downstream generalization especially under strong control or low IPC. We propose \\emph{Path-Guided Flow Matching (PGFM)}, the first flow matching-based framework for generative distillation, which enables fast deterministic synthesis by solving an ODE in a few steps. PGFM conducts flow matching in the latent space of a frozen VAE to learn class-conditional transport from Gaussian noise to data distribution. Particularly, we develop a continuous path-to-prototype guidance algorithm for ODE-consistent path control, which allows trajectories to reliably land on assigned prototypes while preserving diversity and efficiency. Extensive experiments across high-resolution benchmarks demonstrate that PGFM matches or surpasses prior diffusion-based distillation approaches with fewer steps of sampling while delivering competitive performance with remarkably improved efficiency, e.g., 7.6$\\times$ more efficient than the diffusion-based counterparts with 78\\% mode coverage.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05616.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05616",
    "published": "2026-02-05T12:52:32Z",
    "updated": "2026-02-05T12:52:32Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05605",
    "title": "Shiva-DiT: Residual-Based Differentiable Top-$k$ Selection for Efficient Diffusion Transformers",
    "authors": [
      "Jiaji Zhang",
      "Hailiang Zhao",
      "Guoxuan Zhu",
      "Ruichao Sun",
      "Jiaju Wu",
      "Xinkui Zhao",
      "Hanlin Tang",
      "Weiyi Lu",
      "Kan Liu",
      "Tao Lan",
      "Lin Qu",
      "Shuiguang Deng"
    ],
    "abstract": "Diffusion Transformers (DiTs) incur prohibitive computational costs due to the quadratic scaling of self-attention. Existing pruning methods fail to simultaneously satisfy differentiability, efficiency, and the strict static budgets required for hardware overhead. To address this, we propose Shiva-DiT, which effectively reconciles these conflicting requirements via Residual-Based Differentiable Top-$k$ Selection. By leveraging a residual-aware straight-through estimator, our method enforces deterministic token counts for static compilation while preserving end-to-end learnability through residual gradient estimation. Furthermore, we introduce a Context-Aware Router and Adaptive Ratio Policy to autonomously learn an adaptive pruning schedule. Experiments on mainstream models, including SD3.5, demonstrate that Shiva-DiT establishes a new Pareto frontier, achieving a 1.54$\\times$ wall-clock speedup with superior fidelity compared to existing baselines, effectively eliminating ragged tensor overheads.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05605.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05605",
    "published": "2026-02-05T12:42:22Z",
    "updated": "2026-02-05T12:42:22Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05602",
    "title": "Multi-instance robust fitting for non-classical geometric models",
    "authors": [
      "Zongliang Zhang",
      "Shuxiang Li",
      "Xingwang Huang",
      "Zongyue Wang"
    ],
    "abstract": "Most existing robust fitting methods are designed for classical models, such as lines, circles, and planes. In contrast, fewer methods have been developed to robustly handle non-classical models, such as spiral curves, procedural character models, and free-form surfaces. Furthermore, existing methods primarily focus on reconstructing a single instance of a non-classical model. This paper aims to reconstruct multiple instances of non-classical models from noisy data. We formulate this multi-instance fitting task as an optimization problem, which comprises an estimator and an optimizer. Specifically, we propose a novel estimator based on the model-to-data error, capable of handling outliers without a predefined error threshold. Since the proposed estimator is non-differentiable with respect to the model parameters, we employ a meta-heuristic algorithm as the optimizer to seek the global optimum. The effectiveness of our method are demonstrated through experimental results on various non-classical models. The code is available at https://github.com/zhangzongliang/fitting.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05602.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05602",
    "published": "2026-02-05T12:38:38Z",
    "updated": "2026-02-05T12:38:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05600",
    "title": "On the Superlinear Relationship between SGD Noise Covariance and Loss Landscape Curvature",
    "authors": [
      "Yikuan Zhang",
      "Ning Yang",
      "Yuhai Tu"
    ],
    "abstract": "Stochastic Gradient Descent (SGD) introduces anisotropic noise that is correlated with the local curvature of the loss landscape, thereby biasing optimization toward flat minima. Prior work often assumes an equivalence between the Fisher Information Matrix and the Hessian for negative log-likelihood losses, leading to the claim that the SGD noise covariance $\\mathbf{C}$ is proportional to the Hessian $\\mathbf{H}$. We show that this assumption holds only under restrictive conditions that are typically violated in deep neural networks. Using the recently discovered Activity--Weight Duality, we find a more general relationship agnostic to the specific loss formulation, showing that $\\mathbf{C} \\propto \\mathbb{E}_p[\\mathbf{h}_p^2]$, where $\\mathbf{h}_p$ denotes the per-sample Hessian with $\\mathbf{H} = \\mathbb{E}_p[\\mathbf{h}_p]$. As a consequence, $\\mathbf{C}$ and $\\mathbf{H}$ commute approximately rather than coincide exactly, and their diagonal elements follow an approximate power-law relation $C_{ii} \\propto H_{ii}^Î³$ with a theoretically bounded exponent $1 \\leq Î³\\leq 2$, determined by per-sample Hessian spectra. Experiments across datasets, architectures, and loss functions validate these bounds, providing a unified characterization of the noise-curvature relationship in deep learning.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05600.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05600",
    "published": "2026-02-05T12:35:13Z",
    "updated": "2026-02-05T12:35:13Z",
    "comment": "8 pages, 15 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05599",
    "title": "BhashaSetu: Cross-Lingual Knowledge Transfer from High-Resource to Extreme Low-Resource Languages",
    "authors": [
      "Subhadip Maji",
      "Arnab Bhattacharya"
    ],
    "abstract": "Despite remarkable advances in natural language processing, developing effective systems for low-resource languages remains a formidable challenge, with performances typically lagging far behind high-resource counterparts due to data scarcity and insufficient linguistic resources. Cross-lingual knowledge transfer has emerged as a promising approach to address this challenge by leveraging resources from high-resource languages. In this paper, we investigate methods for transferring linguistic knowledge from high-resource languages to low-resource languages, where the number of labeled training instances is in hundreds. We focus on sentence-level and word-level tasks. We introduce a novel method, GETR (Graph-Enhanced Token Representation) for cross-lingual knowledge transfer along with two adopted baselines (a) augmentation in hidden layers and (b) token embedding transfer through token translation. Experimental results demonstrate that our GNN-based approach significantly outperforms existing multilingual and cross-lingual baseline methods, achieving 13 percentage point improvements on truly low-resource languages (Mizo, Khasi) for POS tagging, and 20 and 27 percentage point improvements in macro-F1 on simulated low-resource languages (Marathi, Bangla, Malayalam) across sentiment classification and NER tasks respectively. We also present a detailed analysis of the transfer mechanisms and identify key factors that contribute to successful knowledge transfer in this linguistic context.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05599.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05599",
    "published": "2026-02-05T12:33:30Z",
    "updated": "2026-02-05T12:33:30Z",
    "comment": "Accepted as a long paper at IJCNLP-AACL Main Conference",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05598",
    "title": "CAViT -- Channel-Aware Vision Transformer for Dynamic Feature Fusion",
    "authors": [
      "Aon Safdar",
      "Mohamed Saadeldin"
    ],
    "abstract": "Vision Transformers (ViTs) have demonstrated strong performance across a range of computer vision tasks by modeling long-range spatial interactions via self-attention. However, channel-wise mixing in ViTs remains static, relying on fixed multilayer perceptrons (MLPs) that lack adaptability to input content. We introduce 'CAViT', a dual-attention architecture that replaces the static MLP with a dynamic, attention-based mechanism for feature interaction. Each Transformer block in CAViT performs spatial self-attention followed by channel-wise self-attention, allowing the model to dynamically recalibrate feature representations based on global image context. This unified and content-aware token mixing strategy enhances representational expressiveness without increasing depth or complexity. We validate CAViT across five benchmark datasets spanning both natural and medical domains, where it outperforms the standard ViT baseline by up to +3.6% in accuracy, while reducing parameter count and FLOPs by over 30%. Qualitative attention maps reveal sharper and semantically meaningful activation patterns, validating the effectiveness of our attention-driven token mixing.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05598.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05598",
    "published": "2026-02-05T12:33:09Z",
    "updated": "2026-02-05T12:33:09Z",
    "comment": "Presented at the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2025 (CVPR 25) in the 4th Workshop on Transformers for Visions - T4V (https://sites.google.com/view/t4v-cvpr25/) Accepted for Publication at 33rd International Conference on Artificial Intelligence and Cognitive Science (AICS 2025), where it was shortlisted for Best Paper Award. (https://aicsconf.org/?page_id=278)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05597",
    "title": "Emulating Aggregate Human Choice Behavior and Biases with GPT Conversational Agents",
    "authors": [
      "Stephen Pilli",
      "Vivek Nallur"
    ],
    "abstract": "Cognitive biases often shape human decisions. While large language models (LLMs) have been shown to reproduce well-known biases, a more critical question is whether LLMs can predict biases at the individual level and emulate the dynamics of biased human behavior when contextual factors, such as cognitive load, interact with these biases. We adapted three well-established decision scenarios into a conversational setting and conducted a human experiment (N=1100). Participants engaged with a chatbot that facilitates decision-making through simple or complex dialogues. Results revealed robust biases. To evaluate how LLMs emulate human decision-making under similar interactive conditions, we used participant demographics and dialogue transcripts to simulate these conditions with LLMs based on GPT-4 and GPT-5. The LLMs reproduced human biases with precision. We found notable differences between models in how they aligned human behavior. This has important implications for designing and evaluating adaptive, bias-aware LLM-based AI systems in interactive contexts.",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05597.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05597",
    "published": "2026-02-05T12:33:05Z",
    "updated": "2026-02-05T12:33:05Z",
    "comment": "Accepted at CHI'26. arXiv admin note: substantial text overlap with arXiv:2601.11049",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05590",
    "title": "EgoPoseVR: Spatiotemporal Multi-Modal Reasoning for Egocentric Full-Body Pose in Virtual Reality",
    "authors": [
      "Haojie Cheng",
      "Shaun Jing Heng Ong",
      "Shaoyu Cai",
      "Aiden Tat Yang Koh",
      "Fuxi Ouyang",
      "Eng Tat Khoo"
    ],
    "abstract": "Immersive virtual reality (VR) applications demand accurate, temporally coherent full-body pose tracking. Recent head-mounted camera-based approaches show promise in egocentric pose estimation, but encounter challenges when applied to VR head-mounted displays (HMDs), including temporal instability, inaccurate lower-body estimation, and the lack of real-time performance. To address these limitations, we present EgoPoseVR, an end-to-end framework for accurate egocentric full-body pose estimation in VR that integrates headset motion cues with egocentric RGB-D observations through a dual-modality fusion pipeline. A spatiotemporal encoder extracts frame- and joint-level representations, which are fused via cross-attention to fully exploit complementary motion cues across modalities. A kinematic optimization module then imposes constraints from HMD signals, enhancing the accuracy and stability of pose estimation. To facilitate training and evaluation, we introduce a large-scale synthetic dataset of over 1.8 million temporally aligned HMD and RGB-D frames across diverse VR scenarios. Experimental results show that EgoPoseVR outperforms state-of-the-art egocentric pose estimation models. A user study in real-world scenes further shows that EgoPoseVR achieved significantly higher subjective ratings in accuracy, stability, embodiment, and intention for future use compared to baseline methods. These results show that EgoPoseVR enables robust full-body pose tracking, offering a practical solution for accurate VR embodiment without requiring additional body-worn sensors or room-scale tracking systems.",
    "categories": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05590.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05590",
    "published": "2026-02-05T12:17:35Z",
    "updated": "2026-02-05T12:17:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05588",
    "title": "A Mixed Reality System for Robust Manikin Localization in Childbirth Training",
    "authors": [
      "Haojie Cheng",
      "Chang Liu",
      "Abhiram Kanneganti",
      "Mahesh Arjandas Choolani",
      "Arundhati Tushar Gosavi",
      "Eng Tat Khoo"
    ],
    "abstract": "Opportunities for medical students to gain practical experience in vaginal births are increasingly constrained by shortened clinical rotations, patient reluctance, and the unpredictable nature of labour. To alleviate clinicians' instructional burden and enhance trainees' learning efficiency, we introduce a mixed reality (MR) system for childbirth training that combines virtual guidance with tactile manikin interaction, thereby preserving authentic haptic feedback while enabling independent practice without continuous on-site expert supervision. The system extends the passthrough capability of commercial head-mounted displays (HMDs) by spatially calibrating an external RGB-D camera, allowing real-time visual integration of physical training objects. Building on this capability, we implement a coarse-to-fine localization pipeline that first aligns the maternal manikin with fiducial markers to define a delivery region and then registers the pre-scanned neonatal head within this area. This process enables spatially accurate overlay of virtual guiding hands near the manikin, allowing trainees to follow expert trajectories reinforced by haptic interaction. Experimental evaluations demonstrate that the system achieves accurate and stable manikin localization on a standalone headset, ensuring practical deployment without external computing resources. A large-scale user study involving 83 fourth-year medical students was subsequently conducted to compare MR-based and virtual reality (VR)-based childbirth training. Four senior obstetricians independently assessed performance using standardized criteria. Results showed that MR training achieved significantly higher scores in delivery, post-delivery, and overall task performance, and was consistently preferred by trainees over VR training.",
    "categories": [
      "cs.CV",
      "cs.ET",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05588.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05588",
    "published": "2026-02-05T12:17:05Z",
    "updated": "2026-02-05T12:17:05Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05582",
    "title": "Geometric Observability Index: An Operator-Theoretic Framework for Per-Feature Sensitivity, Weak Observability, and Dynamic Effects in SE(3) Pose Estimation",
    "authors": [
      "Joe-Mei Feng",
      "Sheng-Wei Yu"
    ],
    "abstract": "We present a unified operator-theoretic framework for analyzing per-feature sensitivity in camera pose estimation on the Lie group SE(3). Classical sensitivity tools - conditioning analyses, Euclidean perturbation arguments, and Fisher information bounds - do not explain how individual image features influence the pose estimate, nor why dynamic or inconsistent observations can disproportionately distort modern SLAM and structure-from-motion systems. To address this gap, we extend influence function theory to matrix Lie groups and derive an intrinsic perturbation operator for left-trivialized M-estimators on SE(3).   The resulting Geometric Observability Index (GOI) quantifies the contribution of a single measurement through the curvature operator and the Lie algebraic structure of the observable subspace. GOI admits a spectral decomposition along the principal directions of the observable curvature, revealing a direct correspondence between weak observability and amplified sensitivity. In the population regime, GOI coincides with the Fisher information geometry on SE(3), yielding a single-measurement analogue of the Cramer-Rao bound.   The same spectral mechanism explains classical degeneracies such as pure rotation and vanishing parallax, as well as dynamic feature amplification along weak curvature directions. Overall, GOI provides a geometrically consistent description of measurement influence that unifies conditioning analysis, Fisher information geometry, influence function theory, and dynamic scene detectability through the spectral geometry of the curvature operator. Because these quantities arise directly within Gauss-Newton pipelines, the curvature spectrum and GOI also yield lightweight, training-free diagnostic signals for identifying dynamic features and detecting weak observability configurations without modifying existing SLAM architectures.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05582.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05582",
    "published": "2026-02-05T12:12:00Z",
    "updated": "2026-02-05T12:12:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05578",
    "title": "LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation",
    "authors": [
      "Junyang Chen",
      "Xiangbo Lv",
      "Zhiqiang Kou",
      "Xingdong Sheng",
      "Ning Xu",
      "Yiguo Qiao"
    ],
    "abstract": "Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05578.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05578",
    "published": "2026-02-05T12:03:11Z",
    "updated": "2026-02-05T12:03:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05577",
    "title": "LocateEdit-Bench: A Benchmark for Instruction-Based Editing Localization",
    "authors": [
      "Shiyu Wu",
      "Shuyan Li",
      "Jing Li",
      "Jing Liu",
      "Yequan Wang"
    ],
    "abstract": "Recent advancements in image editing have enabled highly controllable and semantically-aware alteration of visual content, posing unprecedented challenges to manipulation localization. However, existing AI-generated forgery localization methods primarily focus on inpainting-based manipulations, making them ineffective against the latest instruction-based editing paradigms. To bridge this critical gap, we propose LocateEdit-Bench, a large-scale dataset comprising $231$K edited images, designed specifically to benchmark localization methods against instruction-driven image editing. Our dataset incorporates four cutting-edge editing models and covers three common edit types. We conduct a detailed analysis of the dataset and develop two multi-metric evaluation protocols to assess existing localization methods. Our work establishes a foundation to keep pace with the evolving landscape of image editing, thereby facilitating the development of effective methods for future forgery localization. Dataset will be open-sourced upon acceptance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05577.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05577",
    "published": "2026-02-05T12:01:09Z",
    "updated": "2026-02-05T12:01:09Z",
    "comment": "11 pages, 7 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05576",
    "title": "OpenMAG: A Comprehensive Benchmark for Multimodal-Attributed Graph",
    "authors": [
      "Chenxi Wan",
      "Xunkai Li",
      "Yilong Zuo",
      "Haokun Deng",
      "Sihan Li",
      "Bowen Fan",
      "Hongchao Qin",
      "Ronghua Li",
      "Guoren Wang"
    ],
    "abstract": "Multimodal-Attributed Graph (MAG) learning has achieved remarkable success in modeling complex real-world systems by integrating graph topology with rich attributes from multiple modalities. With the rapid proliferation of novel MAG models capable of handling intricate cross-modal semantics and structural dependencies, establishing a rigorous and unified evaluation standard has become imperative. Although existing benchmarks have facilitated initial progress, they exhibit critical limitations in domain coverage, encoder flexibility, model diversity, and task scope, presenting significant challenges to fair evaluation. To bridge this gap, we present OpenMAG, a comprehensive benchmark that integrates 19 datasets across 6 domains and incorporates 16 encoders to support both static and trainable feature encoding. OpenMAG further implements a standardized library of 24 state-of-the-art models and supports 8 downstream tasks, enabling fair comparisons within a unified framework. Through systematic assessment of necessity, data quality, effectiveness, robustness, and efficiency, we derive 14 fundamental insights into MAG learning to guide future advancements. Our code is available at https://github.com/YUKI-N810/OpenMAG.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05576.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05576",
    "published": "2026-02-05T11:59:40Z",
    "updated": "2026-02-05T11:59:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05574",
    "title": "A Hybrid CNN and ML Framework for Multi-modal Classification of Movement Disorders Using MRI and Brain Structural Features",
    "authors": [
      "Mengyu Li",
      "IngibjÃ¶rg KristjÃ¡nsdÃ³ttir",
      "Thilo van Eimeren",
      "Kathrin Giehl",
      "Lotta M. Ellingsen",
      "the ASAP Neuroimaging Initiative"
    ],
    "abstract": "Atypical Parkinsonian Disorders (APD), also known as Parkinson-plus syndrome, are a group of neurodegenerative diseases that include progressive supranuclear palsy (PSP) and multiple system atrophy (MSA). In the early stages, overlapping clinical features often lead to misdiagnosis as Parkinson's disease (PD). Identifying reliable imaging biomarkers for early differential diagnosis remains a critical challenge. In this study, we propose a hybrid framework combining convolutional neural networks (CNNs) with machine learning (ML) techniques to classify APD subtypes versus PD and distinguish between the subtypes themselves: PSP vs. PD, MSA vs. PD, and PSP vs. MSA. The model leverages multi-modal input data, including T1-weighted magnetic resonance imaging (MRI), segmentation masks of 12 deep brain structures associated with APD, and their corresponding volumetric measurements. By integrating these complementary modalities, including image data, structural segmentation masks, and quantitative volume features, the hybrid approach achieved promising classification performance with area under the curve (AUC) scores of 0.95 for PSP vs. PD, 0.86 for MSA vs. PD, and 0.92 for PSP vs. MSA. These results highlight the potential of combining spatial and structural information for robust subtype differentiation. In conclusion, this study demonstrates that fusing CNN-based image features with volume-based ML inputs improves classification accuracy for APD subtypes. The proposed approach may contribute to more reliable early-stage diagnosis, facilitating timely and targeted interventions in clinical practice.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05574.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05574",
    "published": "2026-02-05T11:57:20Z",
    "updated": "2026-02-05T11:57:20Z",
    "comment": "To be published in Proceedings of SPIE Medical Imaging 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05573",
    "title": "Visual Implicit Geometry Transformer for Autonomous Driving",
    "authors": [
      "Arsenii Shirokov",
      "Mikhail Kuznetsov",
      "Danila Stepochkin",
      "Egor Evdokimov",
      "Daniil Glazkov",
      "Nikolay Patakin",
      "Anton Konushin",
      "Dmitry Senushkin"
    ],
    "abstract": "We introduce the Visual Implicit Geometry Transformer (ViGT), an autonomous driving geometric model that estimates continuous 3D occupancy fields from surround-view camera rigs. ViGT represents a step towards foundational geometric models for autonomous driving, prioritizing scalability, architectural simplicity, and generalization across diverse sensor configurations. Our approach achieves this through a calibration-free architecture, enabling a single model to adapt to different sensor setups. Unlike general-purpose geometric foundational models that focus on pixel-aligned predictions, ViGT estimates a continuous 3D occupancy field in a birds-eye-view (BEV) addressing domain-specific requirements. ViGT naturally infers geometry from multiple camera views into a single metric coordinate frame, providing a common representation for multiple geometric tasks. Unlike most existing occupancy models, we adopt a self-supervised training procedure that leverages synchronized image-LiDAR pairs, eliminating the need for costly manual annotations. We validate the scalability and generalizability of our approach by training our model on a mixture of five large-scale autonomous driving datasets (NuScenes, Waymo, NuPlan, ONCE, and Argoverse) and achieving state-of-the-art performance on the pointmap estimation task, with the best average rank across all evaluated baselines. We further evaluate ViGT on the Occ3D-nuScenes benchmark, where ViGT achieves comparable performance with supervised methods. The source code is publicly available at \\href{https://github.com/whesense/ViGT}{https://github.com/whesense/ViGT}.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05573.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05573",
    "published": "2026-02-05T11:54:38Z",
    "updated": "2026-02-05T11:54:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05572",
    "title": "ShapeGaussian: High-Fidelity 4D Human Reconstruction in Monocular Videos via Vision Priors",
    "authors": [
      "Zhenxiao Liang",
      "Ning Zhang",
      "Youbao Tang",
      "Ruei-Sung Lin",
      "Qixing Huang",
      "Peng Chang",
      "Jing Xiao"
    ],
    "abstract": "We introduce ShapeGaussian, a high-fidelity, template-free method for 4D human reconstruction from casual monocular videos. Generic reconstruction methods lacking robust vision priors, such as 4DGS, struggle to capture high-deformation human motion without multi-view cues. While template-based approaches, primarily relying on SMPL, such as HUGS, can produce photorealistic results, they are highly susceptible to errors in human pose estimation, often leading to unrealistic artifacts. In contrast, ShapeGaussian effectively integrates template-free vision priors to achieve both high-fidelity and robust scene reconstructions. Our method follows a two-step pipeline: first, we learn a coarse, deformable geometry using pretrained models that estimate data-driven priors, providing a foundation for reconstruction. Then, we refine this geometry using a neural deformation model to capture fine-grained dynamic details. By leveraging 2D vision priors, we mitigate artifacts from erroneous pose estimation in template-based methods and employ multiple reference frames to resolve the invisibility issue of 2D keypoints in a template-free manner. Extensive experiments demonstrate that ShapeGaussian surpasses template-based methods in reconstruction accuracy, achieving superior visual quality and robustness across diverse human motions in casual monocular videos.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05572.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05572",
    "published": "2026-02-05T11:52:15Z",
    "updated": "2026-02-05T11:52:15Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05571",
    "title": "EdgeMask-DG*: Learning Domain-Invariant Graph Structures via Adversarial Edge Masking",
    "authors": [
      "Rishabh Bhattacharya",
      "Naresh Manwani"
    ],
    "abstract": "Structural shifts pose a significant challenge for graph neural networks, as graph topology acts as a covariate that can vary across domains. Existing domain generalization methods rely on fixed structural augmentations or training on globally perturbed graphs, mechanisms that do not pinpoint which specific edges encode domain-invariant information. We argue that domain-invariant structural information is not rigidly tied to a single topology but resides in the consensus across multiple graph structures derived from topology and feature similarity. To capture this, we first propose EdgeMask-DG, a novel min-max algorithm where an edge masker learns to find worst-case continuous masks subject to a sparsity constraint, compelling a task GNN to perform effectively under these adversarial structural perturbations. Building upon this, we introduce EdgeMask-DG*, an extension that applies this adversarial masking principle to an enriched graph. This enriched graph combines the original topology with feature-derived edges, allowing the model to discover invariances even when the original topology is noisy or domain-specific. EdgeMask-DG* is the first to systematically combine adaptive adversarial topology search with feature-enriched graphs. We provide a formal justification for our approach from a robust optimization perspective. We demonstrate that EdgeMask-DG* achieves new state-of-the-art performance on diverse graph domain generalization benchmarks, including citation networks, social networks, and temporal graphs. Notably, on the Cora OOD benchmark, EdgeMask-DG* lifts the worst-case domain accuracy to 78.0\\%, a +3.8 pp improvement over the prior state of the art (74.2\\%). The source code for our experiments can be found here: https://anonymous.4open.science/r/TMLR-EAEF/",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05571.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05571",
    "published": "2026-02-05T11:52:06Z",
    "updated": "2026-02-05T11:52:06Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05570",
    "title": "TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?",
    "authors": [
      "Yikun Zong",
      "Cheston Tan"
    ],
    "abstract": "Humans excel at spatial reasoning tasks like Tangram puzzle assembly through cognitive processes involving mental rotation, iterative refinement, and visual feedback. Inspired by how humans solve Tangram puzzles through trial-and-error, observation, and correction, we design a framework that models these human cognitive mechanisms. However, comprehensive experiments across five representative Vision-Language Models (VLMs) reveal systematic failures in continuous geometric reasoning: average IoU of only 0.41 on single-piece tasks, dropping to 0.23 on two-piece composition, far below human performance where children can complete Tangram tasks successfully. This paper addresses a fundamental challenge in self-improving AI: can models iteratively refine their predictions at test time without parameter updates? We introduce a test-time self-refinement framework that combines in-context learning (ICL) with reward-guided feedback loops, inspired by human cognitive processes. Our training-free verifier-refiner agent applies recursive refinement loops that iteratively self-refine predictions based on geometric consistency feedback, achieving IoU improvements from 0.63 to 0.932 on medium-triangle cases without any model retraining. This demonstrates that incorporating human-inspired iterative refinement mechanisms through ICL and reward loops can substantially enhance geometric reasoning in VLMs, moving self-improving AI from promise to practice in continuous spatial domains. Our work is available at this anonymous link https://anonymous.4open.science/r/TangramVLM-F582/.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05570.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05570",
    "published": "2026-02-05T11:49:30Z",
    "updated": "2026-02-05T11:49:30Z",
    "comment": "13 pages, 4 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05567",
    "title": "MAGPrompt: Message-Adaptive Graph Prompt Tuning for Graph Neural Networks",
    "authors": [
      "Long D. Nguyen",
      "Binh P. Nguyen"
    ],
    "abstract": "Pre-trained graph neural networks (GNNs) transfer well, but adapting them to downstream tasks remains challenging due to mismatches between pre-training objectives and task requirements. Graph prompt tuning offers a parameter-efficient alternative to fine-tuning, yet most methods only modify inputs or representations and leave message passing unchanged, limiting their ability to adapt neighborhood interactions. We propose message-adaptive graph prompt tuning, which injects learnable prompts into the message passing step to reweight incoming neighbor messages and add task-specific prompt vectors during message aggregation, while keeping the backbone GNN frozen. The approach is compatible with common GNN backbones and pre-training strategies, and applicable across downstream settings. Experiments on diverse node- and graph-level datasets show consistent gains over prior graph prompting methods in few-shot settings, while achieving performance competitive with fine-tuning in full-shot regimes.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05567.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05567",
    "published": "2026-02-05T11:39:49Z",
    "updated": "2026-02-05T11:39:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05557",
    "title": "PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds",
    "authors": [
      "Michael Schwingshackl",
      "Fabio F. Oberweger",
      "Mario Niedermeyer",
      "Huemer Johannes",
      "Markus Murschitz"
    ],
    "abstract": "We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper's opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05557.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05557",
    "published": "2026-02-05T11:29:09Z",
    "updated": "2026-02-05T11:29:09Z",
    "comment": "8 Pages, 11 Figures, Accepted at 2026 IEEE International Conference on Robotics & Automation (ICRA) Vienna",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05555",
    "title": "IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools",
    "authors": [
      "Panagiotis Sapoutzoglou",
      "Orestis Vaggelis",
      "Athina Zacharia",
      "Evangelos Sartinas",
      "Maria Pateraki"
    ],
    "abstract": "We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05555.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05555",
    "published": "2026-02-05T11:28:57Z",
    "updated": "2026-02-05T11:28:57Z",
    "comment": "To appear in ICRA 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05551",
    "title": "FastVMT: Eliminating Redundancy in Video Motion Transfer",
    "authors": [
      "Yue Ma",
      "Zhikai Wang",
      "Tianhao Ren",
      "Mingzhe Zheng",
      "Hongyu Liu",
      "Jiayi Guo",
      "Mark Fong",
      "Yuxuan Xue",
      "Zixiang Zhao",
      "Konrad Schindler",
      "Qifeng Chen",
      "Linfeng Zhang"
    ],
    "abstract": "Video motion transfer aims to synthesize videos by generating visual content according to a text prompt while transferring the motion pattern observed in a reference video. Recent methods predominantly use the Diffusion Transformer (DiT) architecture. To achieve satisfactory runtime, several methods attempt to accelerate the computations in the DiT, but fail to address structural sources of inefficiency. In this work, we identify and remove two types of computational redundancy in earlier work: motion redundancy arises because the generic DiT architecture does not reflect the fact that frame-to-frame motion is small and smooth; gradient redundancy occurs if one ignores that gradients change slowly along the diffusion trajectory. To mitigate motion redundancy, we mask the corresponding attention layers to a local neighborhood such that interaction weights are not computed unnecessarily distant image regions. To exploit gradient redundancy, we design an optimization scheme that reuses gradients from previous diffusion steps and skips unwarranted gradient computations. On average, FastVMT achieves a 3.43x speedup without degrading the visual fidelity or the temporal consistency of the generated videos.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05551.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05551",
    "published": "2026-02-05T11:15:59Z",
    "updated": "2026-02-05T11:15:59Z",
    "comment": "Accepted by ICLR2026, Project page: fastvmt.gitHub.io, Code: https://github.com/mayuelala/FastVMT",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05549",
    "title": "Logical Guidance for the Exact Composition of Diffusion Models",
    "authors": [
      "Francesco Alesiani",
      "Jonathan Warrell",
      "Tanja Bien",
      "Henrik Christiansen",
      "Matheus Ferraz",
      "Mathias Niepert"
    ],
    "abstract": "We propose LOGDIFF (Logical Guidance for the Exact Composition of Diffusion Models), a guidance framework for diffusion models that enables principled constrained generation with complex logical expressions at inference time.   We study when exact score-based guidance for complex logical formulas can be obtained from guidance signals associated with atomic properties.   First, we derive an exact Boolean calculus that provides a sufficient condition for exact logical guidance.   Specifically, if a formula admits a circuit representation in which conjunctions combine conditionally independent subformulas and disjunctions combine subformulas that are either conditionally independent or mutually exclusive, exact logical guidance is achievable.   In this case, the guidance signal can be computed exactly from atomic scores and posterior probabilities using an efficient recursive algorithm.   Moreover, we show that, for commonly encountered classes of distributions, any desired Boolean formula is compilable into such a circuit representation.   Second, by combining atomic guidance scores with posterior probability estimates, we introduce a hybrid guidance approach that bridges classifierguidance and classifier-free guidance, applicable to both compositional logical guidance and standard conditional generation.   We demonstrate the effectiveness of our framework on multiple image and protein structure generation tasks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05549.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05549",
    "published": "2026-02-05T11:10:06Z",
    "updated": "2026-02-05T11:10:06Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05548",
    "title": "Unveiling Implicit Advantage Symmetry: Why GRPO Struggles with Exploration and Difficulty Adaptation",
    "authors": [
      "Zhiqi Yu",
      "Zhangquan Chen",
      "Mengting Liu",
      "Heye Zhang",
      "Liangqiong Qu"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), particularly GRPO, has become the standard for eliciting LLM reasoning. However, its efficiency in exploration and difficulty adaptation remains an open challenge. In this work, we argue that these bottlenecks stem from an implicit advantage symmetry inherent in Group Relative Advantage Estimation (GRAE). This symmetry induces two critical limitations: (i) at the group level, strict symmetry in weights between correct and incorrect trajectories leaves unsampled action logits unchanged, thereby hindering exploration of novel correct solution. (ii) at the sample level, the algorithm implicitly prioritizes medium-difficulty samples, remaining agnostic to the non-stationary demands of difficulty focus. Through controlled experiments, we reveal that this symmetric property is sub-optimal, yielding two pivotal insights: (i) asymmetrically suppressing the advantages of correct trajectories encourages essential exploration. (ii) learning efficiency is maximized by a curriculum-like transition-prioritizing simpler samples initially before gradually shifting to complex ones. Motivated by these findings, we propose Asymmetric GRAE (A-GRAE), which dynamically modulates exploration incentives and sample-difficulty focus. Experiments across seven benchmarks demonstrate that A-GRAE consistently improves GRPO and its variants across both LLMs and MLLMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05548.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05548",
    "published": "2026-02-05T11:07:14Z",
    "updated": "2026-02-05T11:07:14Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05547",
    "title": "Multi-Task GRPO: Reliable LLM Reasoning Across Tasks",
    "authors": [
      "Shyam Sundhar Ramesh",
      "Xiaotong Ji",
      "Matthieu Zimmer",
      "Sangwoong Yoon",
      "Zhiyong Wang",
      "Haitham Bou Ammar",
      "Aurelien Lucchi",
      "Ilija Bogunovic"
    ],
    "abstract": "RL-based post-training with GRPO is widely used to improve large language models on individual reasoning tasks. However, real-world deployment requires reliable performance across diverse tasks. A straightforward multi-task adaptation of GRPO often leads to imbalanced outcomes, with some tasks dominating optimization while others stagnate. Moreover, tasks can vary widely in how frequently prompts yield zero advantages (and thus zero gradients), which further distorts their effective contribution to the optimization signal. To address these issues, we propose a novel Multi-Task GRPO (MT-GRPO) algorithm that (i) dynamically adapts task weights to explicitly optimize worst-task performance and promote balanced progress across tasks, and (ii) introduces a ratio-preserving sampler to ensure task-wise policy gradients reflect the adapted weights. Experiments on both 3-task and 9-task settings show that MT-GRPO consistently outperforms baselines in worst-task accuracy. In particular, MT-GRPO achieves 16-28% and 6% absolute improvement on worst-task performance over standard GRPO and DAPO, respectively, while maintaining competitive average accuracy. Moreover, MT-GRPO requires 50% fewer training steps to reach 50% worst-task accuracy in the 3-task setting, demonstrating substantially improved efficiency in achieving reliable performance across tasks.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05547.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05547",
    "published": "2026-02-05T11:06:37Z",
    "updated": "2026-02-05T11:06:37Z",
    "comment": "Preprint",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05544",
    "title": "Reasoning-guided Collaborative Filtering with Language Models for Explainable Recommendation",
    "authors": [
      "Fahad Anwaar",
      "Adil Mehmood Khan",
      "Muhammad Khalid",
      "Usman Zia",
      "Kezhi Wang"
    ],
    "abstract": "Large Language Models (LLMs) exhibit potential for explainable recommendation systems but overlook collaborative signals, while prevailing methods treat recommendation and explanation as separate tasks, resulting in a memory footprint. We present RGCF-XRec, a hybrid framework that introduces reasoning-guided collaborative filtering (CF) knowledge into a language model to deliver explainable sequential recommendations in a single step. Theoretical grounding and empirical findings reveal that RGCF-XRec offers three key merits over leading CF-aware LLM-based methods: (1) reasoning-guided augmentation of CF knowledge through contextual prompting to discover latent preferences and interpretable reasoning paths; (2) an efficient scoring mechanism based on four dimensions: coherence, completeness, relevance, and consistency to mitigate noisy CF reasoning traces and retain high-quality explanations; (3) a unified representation learning network that encodes collaborative and semantic signals, enabling a structured prompt to condition the LLM for explainable sequential recommendation. RGCF-XRec demonstrates consistent improvements across Amazon datasets, Sports, Toys, and Beauty, comprising 642,503 user-item interactions. It improves HR@10 by 7.38\\% in Sports and 4.59\\% in Toys, along with ROUGE-L by 8.02\\% and 3.49\\%, respectively. It reduces the cold warm performance gap, achieving overall gains of 14.5\\% in cold-start and 11.9\\% in warm start scenarios, and enhances zero-shot HR@5 by 18.54\\% in Beauty and 23.16\\% in Toys, highlighting effective generalization and robustness. Moreover, RGCF-XRec achieves training efficiency with a lightweight LLaMA 3.2-3B backbone, ensuring scalability for real-world applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05544.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05544",
    "published": "2026-02-05T11:05:09Z",
    "updated": "2026-02-05T11:05:09Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05539",
    "title": "Steering Large Reasoning Models towards Concise Reasoning via Flow Matching",
    "authors": [
      "Yawei Li",
      "Benjamin Bergner",
      "Yinghan Zhao",
      "Vihang Prakash Patil",
      "Bei Chen",
      "Cheng Wang"
    ],
    "abstract": "Large Reasoning Models (LRMs) excel at complex reasoning tasks, but their efficiency is often hampered by overly verbose outputs. Prior steering methods attempt to address this issue by applying a single, global vector to hidden representations -- an approach grounded in the restrictive linear representation hypothesis. In this work, we introduce FlowSteer, a nonlinear steering method that goes beyond uniform linear shifts by learning a complete transformation between the distributions associated with verbose and concise reasoning. This transformation is learned via Flow Matching as a velocity field, enabling precise, input-dependent control over the model's reasoning process. By aligning steered representations with the distribution of concise-reasoning activations, FlowSteer yields more compact reasoning than the linear shifts. Across diverse reasoning benchmarks, FlowSteer demonstrates strong task performance and token efficiency compared to leading inference-time baselines. Our work demonstrates that modeling the full distributional transport with generative techniques offers a more effective and principled foundation for controlling LRMs.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05539.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05539",
    "published": "2026-02-05T10:56:13Z",
    "updated": "2026-02-05T10:56:13Z",
    "comment": "This paper has been accepted to Transactions on Machine Learning Research (TMLR)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05538",
    "title": "A Comparative Study of 3D Person Detection: Sensor Modalities and Robustness in Diverse Indoor and Outdoor Environments",
    "authors": [
      "Malaz Tamim",
      "Andrea Matic-Flierl",
      "Karsten Roscher"
    ],
    "abstract": "Accurate 3D person detection is critical for safety in applications such as robotics, industrial monitoring, and surveillance. This work presents a systematic evaluation of 3D person detection using camera-only, LiDAR-only, and camera-LiDAR fusion. While most existing research focuses on autonomous driving, we explore detection performance and robustness in diverse indoor and outdoor scenes using the JRDB dataset. We compare three representative models - BEVDepth (camera), PointPillars (LiDAR), and DAL (camera-LiDAR fusion) - and analyze their behavior under varying occlusion and distance levels. Our results show that the fusion-based approach consistently outperforms single-modality models, particularly in challenging scenarios. We further investigate robustness against sensor corruptions and misalignments, revealing that while DAL offers improved resilience, it remains sensitive to sensor misalignment and certain LiDAR-based corruptions. In contrast, the camera-based BEVDepth model showed the lowest performance and was most affected by occlusion, distance, and noise. Our findings highlight the importance of utilizing sensor fusion for enhanced 3D person detection, while also underscoring the need for ongoing research to address the vulnerabilities inherent in these systems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05538.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05538",
    "published": "2026-02-05T10:53:35Z",
    "updated": "2026-02-05T10:53:35Z",
    "comment": "Accepted for VISAPP 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05536",
    "title": "When Shared Knowledge Hurts: Spectral Over-Accumulation in Model Merging",
    "authors": [
      "Yayuan Li",
      "Ze Peng",
      "Jian Zhang",
      "Jintao Guo",
      "Yue Duan",
      "Yinghuan Shi"
    ],
    "abstract": "Model merging combines multiple fine-tuned models into a single model by adding their weight updates, providing a lightweight alternative to retraining. Existing methods primarily target resolving conflicts between task updates, leaving the failure mode of over-counting shared knowledge unaddressed. We show that when tasks share aligned spectral directions (i.e., overlapping singular vectors), a simple linear combination repeatedly accumulates these directions, inflating the singular values and biasing the merged model toward shared subspaces. To mitigate this issue, we propose Singular Value Calibration (SVC), a training-free and data-free post-processing method that quantifies subspace overlap and rescales inflated singular values to restore a balanced spectrum. Across vision and language benchmarks, SVC consistently improves strong merging baselines and achieves state-of-the-art performance. Furthermore, by modifying only the singular values, SVC improves the performance of Task Arithmetic by 13.0%. Code is available at: https://github.com/lyymuwu/SVC.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05536.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05536",
    "published": "2026-02-05T10:52:36Z",
    "updated": "2026-02-05T10:52:36Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05535",
    "title": "Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification",
    "authors": [
      "Tao Huang",
      "Rui Wang",
      "Xiaofei Liu",
      "Yi Qin",
      "Li Duan",
      "Liping Jing"
    ],
    "abstract": "Large vision-language models (LVLMs) have shown substantial advances in multimodal understanding and generation. However, when presented with incompetent or adversarial inputs, they frequently produce unreliable or even harmful content, such as fact hallucinations or dangerous instructions. This misalignment with human expectations, referred to as \\emph{misbehaviors} of LVLMs, raises serious concerns for deployment in critical applications. These misbehaviors are found to stem from epistemic uncertainty, specifically either conflicting internal knowledge or the absence of supporting information. However, existing uncertainty quantification methods, which typically capture only overall epistemic uncertainty, have shown limited effectiveness in identifying such issues. To address this gap, we propose Evidential Uncertainty Quantification (EUQ), a fine-grained method that captures both information conflict and ignorance for effective detection of LVLM misbehaviors. In particular, we interpret features from the model output head as either supporting (positive) or opposing (negative) evidence. Leveraging Evidence Theory, we model and aggregate this evidence to quantify internal conflict and knowledge gaps within a single forward pass. We extensively evaluate our method across four categories of misbehavior, including hallucinations, jailbreaks, adversarial vulnerabilities, and out-of-distribution (OOD) failures, using state-of-the-art LVLMs, and find that EUQ consistently outperforms strong baselines, showing that hallucinations correspond to high internal conflict and OOD failures to high ignorance. Furthermore, layer-wise evidential uncertainty dynamics analysis helps interpret the evolution of internal representations from a new perspective. The source code is available at https://github.com/HT86159/EUQ.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05535.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05535",
    "published": "2026-02-05T10:51:39Z",
    "updated": "2026-02-05T10:51:39Z",
    "comment": "Accepted to ICLR 2026. Code is available at https://github.com/HT86159/EUQ",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05534",
    "title": "SSG: Scaled Spatial Guidance for Multi-Scale Visual Autoregressive Generation",
    "authors": [
      "Youngwoo Shin",
      "Jiwan Hur",
      "Junmo Kim"
    ],
    "abstract": "Visual autoregressive (VAR) models generate images through next-scale prediction, naturally achieving coarse-to-fine, fast, high-fidelity synthesis mirroring human perception. In practice, this hierarchy can drift at inference time, as limited capacity and accumulated error cause the model to deviate from its coarse-to-fine nature. We revisit this limitation from an information-theoretic perspective and deduce that ensuring each scale contributes high-frequency content not explained by earlier scales mitigates the train-inference discrepancy. With this insight, we propose Scaled Spatial Guidance (SSG), training-free, inference-time guidance that steers generation toward the intended hierarchy while maintaining global coherence. SSG emphasizes target high-frequency signals, defined as the semantic residual, isolated from a coarser prior. To obtain this prior, we leverage a principled frequency-domain procedure, Discrete Spatial Enhancement (DSE), which is devised to sharpen and better isolate the semantic residual through frequency-aware construction. SSG applies broadly across VAR models leveraging discrete visual tokens, regardless of tokenization design or conditioning modality. Experiments demonstrate SSG yields consistent gains in fidelity and diversity while preserving low latency, revealing untapped efficiency in coarse-to-fine image generation. Code is available at https://github.com/Youngwoo-git/SSG.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05534.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05534",
    "published": "2026-02-05T10:48:58Z",
    "updated": "2026-02-05T10:48:58Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05533",
    "title": "Conditional Diffusion Guidance under Hard Constraint: A Stochastic Analysis Approach",
    "authors": [
      "Zhengyi Guo",
      "Wenpin Tang",
      "Renyuan Xu"
    ],
    "abstract": "We study conditional generation in diffusion models under hard constraints, where generated samples must satisfy prescribed events with probability one. Such constraints arise naturally in safety-critical applications and in rare-event simulation, where soft or reward-based guidance methods offer no guarantee of constraint satisfaction. Building on a probabilistic interpretation of diffusion models, we develop a principled conditional diffusion guidance framework based on Doob's h-transform, martingale representation and quadratic variation process. Specifically, the resulting guided dynamics augment a pretrained diffusion with an explicit drift correction involving the logarithmic gradient of a conditioning function, without modifying the pretrained score network. Leveraging martingale and quadratic-variation identities, we propose two novel off-policy learning algorithms based on a martingale loss and a martingale-covariation loss to estimate h and its gradient using only trajectories from the pretrained model. We provide non-asymptotic guarantees for the resulting conditional sampler in both total variation and Wasserstein distances, explicitly characterizing the impact of score approximation and guidance estimation errors. Numerical experiments demonstrate the effectiveness of the proposed methods in enforcing hard constraints and generating rare-event samples.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05533.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05533",
    "published": "2026-02-05T10:46:20Z",
    "updated": "2026-02-05T10:46:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05532",
    "title": "Split Personality Training: Revealing Latent Knowledge Through Alternate Personalities",
    "authors": [
      "Florian Dietz",
      "William Wale",
      "Oscar Gilg",
      "Robert McCarthy",
      "Felix Michalak",
      "Gustavo Ewbank Rodrigues Danon",
      "Miguelito de Guzman",
      "Dietrich Klakow"
    ],
    "abstract": "Detecting misalignment in large language models is challenging because models may learn to conceal misbehavior during training. Standard auditing techniques fall short: black-box methods often cannot distinguish misaligned outputs from benign ones, and mechanistic interpretability does not scale with model capabilities. We introduce Split Personality Training (SPT), which fine-tunes a second ``honest persona'' into LoRA parameters that remain inactive during normal operation. After the main model responds, we activate the LoRA adapter and insert a trigger string, enabling the honest persona to review the response while accessing the main model's latent states. We test our method on the Anthropic Auditing Game Model Organism, a benchmark where Llama-3.3-70B is trained to exploit reward hacks while concealing this behavior. SPT achieves 96% overall accuracy, whereas Anthropic reports near 0% accuracy. The honest persona reveals latent knowledge inaccessible to external observers, such as the fictional biases the compromised model was trained on.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05532.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05532",
    "published": "2026-02-05T10:45:48Z",
    "updated": "2026-02-05T10:45:48Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05527",
    "title": "Generalization of Self-Supervised Vision Transformers for Protein Localization Across Microscopy Domains",
    "authors": [
      "Ben Isselmann",
      "Dilara GÃ¶ksu",
      "Andreas Weinmann"
    ],
    "abstract": "Task-specific microscopy datasets are often too small to train deep learning models that learn robust feature representations. Self-supervised learning (SSL) can mitigate this by pretraining on large unlabeled datasets, but it remains unclear how well such representations transfer across microscopy domains with different staining protocols and channel configurations. We investigate the cross-domain transferability of DINO-pretrained Vision Transformers for protein localization on the OpenCell dataset. We generate image embeddings using three DINO backbones pretrained on ImageNet-1k, the Human Protein Atlas (HPA), and OpenCell, and evaluate them by training a supervised classification head on OpenCell labels. All pretrained models transfer well, with the microscopy-specific HPA-pretrained model achieving the best performance (mean macro $F_1$-score = 0.8221 \\pm 0.0062), slightly outperforming a DINO model trained directly on OpenCell (0.8057 \\pm 0.0090). These results highlight the value of large-scale pretraining and indicate that domain-relevant SSL representations can generalize effectively to related but distinct microscopy datasets, enabling strong downstream performance even when task-specific labeled data are limited.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05527.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05527",
    "published": "2026-02-05T10:39:00Z",
    "updated": "2026-02-05T10:39:00Z",
    "comment": "AMEE Conference Proceeding 2025, 11 pages, 2 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05522",
    "title": "Mapper-GIN: Lightweight Structural Graph Abstraction for Corrupted 3D Point Cloud Classification",
    "authors": [
      "Jeongbin You",
      "Donggun Kim",
      "Sejun Park",
      "Seungsang Oh"
    ],
    "abstract": "Robust 3D point cloud classification is often pursued by scaling up backbones or relying on specialized data augmentation. We instead ask whether structural abstraction alone can improve robustness, and study a simple topology-inspired decomposition based on the Mapper algorithm. We propose Mapper-GIN, a lightweight pipeline that partitions a point cloud into overlapping regions using Mapper (PCA lens, cubical cover, and followed by density-based clustering), constructs a region graph from their overlaps, and performs graph classification with a Graph Isomorphism Network. On the corruption benchmark ModelNet40-C, Mapper-GIN achieves competitive and stable accuracy under Noise and Transformation corruptions with only 0.5M parameters. In contrast to prior approaches that require heavier architectures or additional mechanisms to gain robustness, Mapper-GIN attains strong corruption robustness through simple region-level graph abstraction and GIN message passing. Overall, our results suggest that region-graph structure offers an efficient and interpretable source of robustness for 3D visual recognition.",
    "categories": [
      "cs.CV",
      "math.GT"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05522.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05522",
    "published": "2026-02-05T10:30:40Z",
    "updated": "2026-02-05T10:30:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05515",
    "title": "A Unified Multimodal Framework for Dataset Construction and Model-Based Diagnosis of Ameloblastoma",
    "authors": [
      "Ajo Babu George",
      "Anna Mariam John",
      "Athul Anoop",
      "Balu Bhasuran"
    ],
    "abstract": "Artificial intelligence (AI)-enabled diagnostics in maxillofacial pathology require structured, high-quality multimodal datasets. However, existing resources provide limited ameloblastoma coverage and lack the format consistency needed for direct model training. We present a newly curated multimodal dataset specifically focused on ameloblastoma, integrating annotated radiological, histopathological, and intraoral clinical images with structured data derived from case reports. Natural language processing techniques were employed to extract clinically relevant features from textual reports, while image data underwent domain specific preprocessing and augmentation. Using this dataset, a multimodal deep learning model was developed to classify ameloblastoma variants, assess behavioral patterns such as recurrence risk, and support surgical planning. The model is designed to accept clinical inputs such as presenting complaint, age, and gender during deployment to enhance personalized inference. Quantitative evaluation demonstrated substantial improvements; variant classification accuracy increased from 46.2 percent to 65.9 percent, and abnormal tissue detection F1-score improved from 43.0 percent to 90.3 percent. Benchmarked against resources like MultiCaRe, this work advances patient-specific decision support by providing both a robust dataset and an adaptable multimodal AI framework.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05515.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05515",
    "published": "2026-02-05T10:15:34Z",
    "updated": "2026-02-05T10:15:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05512",
    "title": "A Human-in-the-Loop, LLM-Centered Architecture for Knowledge-Graph Question Answering",
    "authors": [
      "Larissa Pusch",
      "Alexandre Courtiol",
      "Tim Conrad"
    ],
    "abstract": "Large Language Models (LLMs) excel at language understanding but remain limited in knowledge-intensive domains due to hallucinations, outdated information, and limited explainability. Text-based retrieval-augmented generation (RAG) helps ground model outputs in external sources but struggles with multi-hop reasoning. Knowledge Graphs (KGs), in contrast, support precise, explainable querying, yet require a knowledge of query languages. This work introduces an interactive framework in which LLMs generate and explain Cypher graph queries and users iteratively refine them through natural language. Applied to real-world KGs, the framework improves accessibility to complex datasets while preserving factual accuracy and semantic rigor and provides insight into how model performance varies across domains. Our core quantitative evaluation is a 90-query benchmark on a synthetic movie KG that measures query explanation quality and fault detection across multiple LLMs, complemented by two smaller real-life query-generation experiments on a Hyena KG and the MaRDI (Mathematical Research Data Initiative) KG.",
    "categories": [
      "cs.CL",
      "cs.IR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05512.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05512",
    "published": "2026-02-05T10:10:19Z",
    "updated": "2026-02-05T10:10:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05508",
    "title": "VGGT-Motion: Motion-Aware Calibration-Free Monocular SLAM for Long-Range Consistency",
    "authors": [
      "Zhuang Xiong",
      "Chen Zhang",
      "Qingshan Xu",
      "Wenbing Tao"
    ],
    "abstract": "Despite recent progress in calibration-free monocular SLAM via 3D vision foundation models, scale drift remains severe on long sequences. Motion-agnostic partitioning breaks contextual coherence and causes zero-motion drift, while conventional geometric alignment is computationally expensive. To address these issues, we propose VGGT-Motion, a calibration-free SLAM system for efficient and robust global consistency over kilometer-scale trajectories. Specifically, we first propose a motion-aware submap construction mechanism that uses optical flow to guide adaptive partitioning, prune static redundancy, and encapsulate turns for stable local geometry. We then design an anchor-driven direct Sim(3) registration strategy. By exploiting context-balanced anchors, it achieves search-free, pixel-wise dense alignment and efficient loop closure without costly feature matching. Finally, a lightweight submap-level pose graph optimization enforces global consistency with linear complexity, enabling scalable long-range operation. Experiments show that VGGT-Motion markedly improves trajectory accuracy and efficiency, achieving state-of-the-art performance in zero-shot, long-range calibration-free monocular SLAM.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05508.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05508",
    "published": "2026-02-05T10:07:11Z",
    "updated": "2026-02-05T10:07:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05499",
    "title": "SDFP: Speculative Decoding with FIT-Pruned Models for Training-Free and Plug-and-Play LLM Acceleration",
    "authors": [
      "Hanyu Wei",
      "Zunhai Su",
      "Peng Lu",
      "Chao Li",
      "Spandan Tiwari",
      "Ashish Sirasao",
      "Yuhan Dong"
    ],
    "abstract": "Large language models (LLMs) underpin interactive multimedia applications such as captioning, retrieval, recommendation, and creative content generation, yet their autoregressive decoding incurs substantial latency. Speculative decoding reduces latency using a lightweight draft model, but deployment is often limited by the cost and complexity of acquiring, tuning, and maintaining an effective draft model. Recent approaches usually require auxiliary training or specialization, and even training-free methods incur costly search or optimization. We propose SDFP, a fully training-free and plug-and-play framework that builds the draft model via Fisher Information Trace (FIT)-based layer pruning of a given LLM. Using layer sensitivity as a proxy for output perturbation, SDFP removes low-impact layers to obtain a compact draft while preserving compatibility with the original model for standard speculative verification. SDFP needs no additional training, hyperparameter tuning, or separately maintained drafts, enabling rapid, deployment-friendly draft construction. Across benchmarks, SDFP delivers 1.32x-1.5x decoding speedup without altering the target model's output distribution, supporting low-latency multimedia applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05499.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05499",
    "published": "2026-02-05T10:02:00Z",
    "updated": "2026-02-05T10:02:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05495",
    "title": "Transport and Merge: Cross-Architecture Merging for Large Language Models",
    "authors": [
      "Chenhang Cui",
      "Binyun Yang",
      "Fei Shen",
      "Yuxin Chen",
      "Jingnan Zheng",
      "Xiang Wang",
      "An Zhang",
      "Tat-Seng Chua"
    ],
    "abstract": "Large language models (LLMs) achieve strong capabilities by scaling model capacity and training data, yet many real-world deployments rely on smaller models trained or adapted from low-resource data. This gap motivates the need for mechanisms to transfer knowledge from large, high-resource models to smaller, low-resource targets. While model merging provides an effective transfer mechanism, most existing approaches assume architecture-compatible models and therefore cannot directly transfer knowledge from large high-resource LLMs to heterogeneous low-resource targets. In this work, we propose a cross-architecture merging framework based on optimal transport (OT) that aligns activations to infer cross-neuron correspondences between heterogeneous models. The resulting transport plans are then used to guide direct weight-space fusion, enabling effective high-resource to low-resource transfer using only a small set of inputs. Extensive experiments across low-resource languages and specialized domains demonstrate consistent improvements over target models.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05495.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05495",
    "published": "2026-02-05T09:57:57Z",
    "updated": "2026-02-05T09:57:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05494",
    "title": "A Unified Framework for Rethinking Policy Divergence Measures in GRPO",
    "authors": [
      "Qingyuan Wu",
      "Yuhui Wang",
      "Simon Sinong Zhan",
      "Yanning Dai",
      "Shilong Deng",
      "Sarra Habchi",
      "Qi Zhu",
      "Matthias GallÃ©",
      "Chao Huang"
    ],
    "abstract": "Reinforcement Learning with Verified Reward (RLVR) has emerged as a critical paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). Most existing RLVR methods, such as GRPO and its variants, ensure stable updates by constraining policy divergence through clipping likelihood ratios. This paper introduces a unified clipping framework that characterizes existing methods via a general notion of policy divergence, encompassing both likelihood ratios and Kullback-Leibler (KL) divergences and extending to alternative measures. The framework provides a principled foundation for systematically analyzing how different policy divergence measures affect exploration and performance. We further identify the KL3 estimator, a variance-reduced Monte Carlo estimator of the KL divergence, as a key policy divergence constraint. We theoretically demonstrate that the KL3-based constraint is mathematically equivalent to an asymmetric ratio-based clipping that reallocates probability mass toward high-confidence actions, promoting stronger exploration while retaining the simplicity of GRPO-style methods. Empirical results on mathematical reasoning benchmarks demonstrate that incorporating the KL3 estimator into GRPO improves both training stability and final performance, highlighting the importance of principled policy divergence constraints in policy optimization.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05494.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05494",
    "published": "2026-02-05T09:56:16Z",
    "updated": "2026-02-05T09:56:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05493",
    "title": "LinguistAgent: A Reflective Multi-Model Platform for Automated Linguistic Annotation",
    "authors": [
      "Bingru Li"
    ],
    "abstract": "Data annotation remains a significant bottleneck in the Humanities and Social Sciences, particularly for complex semantic tasks such as metaphor identification. While Large Language Models (LLMs) show promise, a significant gap remains between the theoretical capability of LLMs and their practical utility for researchers. This paper introduces LinguistAgent, an integrated, user-friendly platform that leverages a reflective multi-model architecture to automate linguistic annotation. The system implements a dual-agent workflow, comprising an Annotator and a Reviewer, to simulate a professional peer-review process. LinguistAgent supports comparative experiments across three paradigms: Prompt Engineering (Zero/Few-shot), Retrieval-Augmented Generation, and Fine-tuning. We demonstrate LinguistAgent's efficacy using the task of metaphor identification as an example, providing real-time token-level evaluation (Precision, Recall, and $F_1$ score) against human gold standards. The application and codes are released on https://github.com/Bingru-Li/LinguistAgent.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05493.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05493",
    "published": "2026-02-05T09:55:19Z",
    "updated": "2026-02-05T09:55:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05487",
    "title": "Feature points evaluation on omnidirectional vision with a photorealistic fisheye sequence -- A report on experiments done in 2014",
    "authors": [
      "Julien Moreau",
      "S. Ambellouis",
      "Yassine Ruichek"
    ],
    "abstract": "What is this report: This is a scientific report, contributing with a detailed bibliography, a dataset which we will call now PFSeq for ''Photorealistic Fisheye Sequence'' and make available at https://doi.org/10. 57745/DYIVVU, and comprehensive experiments. This work should be considered as a draft, and has been done during my PhD thesis ''Construction of 3D models from fisheye video data-Application to the localisation in urban area'' in 2014 [Mor16]. These results have never been published. The aim was to find the best features detector and descriptor for fisheye images, in the context of selfcalibration, with cameras mounted on the top of a car and aiming at the zenith (to proceed then fisheye visual odometry and stereovision in urban scenes). We face a chicken and egg problem, because we can not take advantage of an accurate projection model for an optimal features detection and description, and we rightly need good features to perform the calibration (i.e. to compute the accurate projection model of the camera). What is not this report: It does not contribute with new features algorithm. It does not compare standard features algorithms to algorithms designed for omnidirectional images (unfortunately). It has not been peer-reviewed. Discussions have been translated and enhanced but the experiments have not been run again and the report has not been updated accordingly to the evolution of the state-of-the-art (read this as a 2014 report).",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05487.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05487",
    "published": "2026-02-05T09:49:33Z",
    "updated": "2026-02-05T09:49:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05480",
    "title": "SOMA-1M: A Large-Scale SAR-Optical Multi-resolution Alignment Dataset for Multi-Task Remote Sensing",
    "authors": [
      "Peihao Wu",
      "Yongxiang Yao",
      "Yi Wan",
      "Wenfei Zhang",
      "Ruipeng Zhao",
      "Jiayuan Li",
      "Yongjun Zhang"
    ],
    "abstract": "Synthetic Aperture Radar (SAR) and optical imagery provide complementary strengths that constitute the critical foundation for transcending single-modality constraints and facilitating cross-modal collaborative processing and intelligent interpretation. However, existing benchmark datasets often suffer from limitations such as single spatial resolution, insufficient data scale, and low alignment accuracy, making them inadequate for supporting the training and generalization of multi-scale foundation models. To address these challenges, we introduce SOMA-1M (SAR-Optical Multi-resolution Alignment), a pixel-level precisely aligned dataset containing over 1.3 million pairs of georeferenced images with a specification of 512 x 512 pixels. This dataset integrates imagery from Sentinel-1, PIESAT-1, Capella Space, and Google Earth, achieving global multi-scale coverage from 0.5 m to 10 m. It encompasses 12 typical land cover categories, effectively ensuring scene diversity and complexity. To address multimodal projection deformation and massive data registration, we designed a rigorous coarse-to-fine image matching framework ensuring pixel-level alignment. Based on this dataset, we established comprehensive evaluation benchmarks for four hierarchical vision tasks, including image matching, image fusion, SAR-assisted cloud removal, and cross-modal translation, involving over 30 mainstream algorithms. Experimental results demonstrate that supervised training on SOMA-1M significantly enhances performance across all tasks. Notably, multimodal remote sensing image (MRSI) matching performance achieves current state-of-the-art (SOTA) levels. SOMA-1M serves as a foundational resource for robust multimodal algorithms and remote sensing foundation models. The dataset will be released publicly at: https://github.com/PeihaoWu/SOMA-1M.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05480.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05480",
    "published": "2026-02-05T09:39:49Z",
    "updated": "2026-02-05T09:39:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05479",
    "title": "Phi-Former: A Pairwise Hierarchical Approach for Compound-Protein Interactions Prediction",
    "authors": [
      "Zhe Wang",
      "Zijing Liu",
      "Chencheng Xu",
      "Yuan Yao"
    ],
    "abstract": "Drug discovery remains time-consuming, labor-intensive, and expensive, often requiring years and substantial investment per drug candidate. Predicting compound-protein interactions (CPIs) is a critical component in this process, enabling the identification of molecular interactions between drug candidates and target proteins. Recent deep learning methods have successfully modeled CPIs at the atomic level, achieving improved efficiency and accuracy over traditional energy-based approaches. However, these models do not always align with chemical realities, as molecular fragments (motifs or functional groups) typically serve as the primary units of biological recognition and binding. In this paper, we propose Phi-former, a pairwise hierarchical interaction representation learning method that addresses this gap by incorporating the biological role of motifs in CPIs. Phi-former represents compounds and proteins hierarchically and employs a pairwise pre-training framework to model interactions systematically across atom-atom, motif-motif, and atom-motif levels, reflecting how biological systems recognize molecular partners. We design intra-level and inter-level learning pipelines that make different interaction levels mutually beneficial. Experimental results demonstrate that Phi-former achieves superior performance on CPI-related tasks. A case study shows that our method accurately identifies specific atoms or motifs activated in CPIs, providing interpretable model explanations. These insights may guide rational drug design and support precision medicine applications.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05479.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05479",
    "published": "2026-02-05T09:39:22Z",
    "updated": "2026-02-05T09:39:22Z",
    "comment": "Accepted to BIBM 2025. 6 pages, 5 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05472",
    "title": "ALIVE: Awakening LLM Reasoning via Adversarial Learning and Instructive Verbal Evaluation",
    "authors": [
      "Yiwen Duan",
      "Jing Ye",
      "Xinpei Zhao"
    ],
    "abstract": "The quest for expert-level reasoning in Large Language Models (LLMs) has been hampered by a persistent \\textit{reward bottleneck}: traditional reinforcement learning (RL) relies on scalar rewards that are \\textbf{costly} to scale, \\textbf{brittle} across domains, and \\textbf{blind} to the underlying logic of a solution. This reliance on external, impoverished signals prevents models from developing a deep, self-contained understanding of reasoning principles. We introduce \\textbf{ALIVE} (\\emph{Adversarial Learning with Instructive Verbal Evaluation}), a hands-free alignment framework that moves beyond scalar reward optimization toward intrinsic reasoning acquisition. Grounded in the principle of \\emph{Cognitive Synergy}, ALIVE unifies problem posing, solving, and judging within a single policy model to internalize the logic of correctness. By coupling adversarial learning with instructive verbal feedback, ALIVE enables models to internalize evaluative criteria directly from raw corpora, effectively transforming external critiques into an endogenous reasoning faculty. Empirical evaluations across mathematical reasoning, code generation, and general logical inference benchmarks demonstrate that ALIVE consistently mitigates reward signal limitations. With identical data and compute, it achieves accuracy gains, markedly improved cross-domain generalization, and higher self-correction rates. These results indicate that the reasoning trinity fosters a self-sustaining trajectory of capability growth, positioning ALIVE as a scalable foundation for general-purpose reasoning alignment without human-in-the-loop supervision.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05472.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05472",
    "published": "2026-02-05T09:20:23Z",
    "updated": "2026-02-05T09:20:23Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05471",
    "title": "Reasoning under Ambiguity: Uncertainty-Aware Multilingual Emotion Classification under Partial Supervision",
    "authors": [
      "Md. Mithun Hossaina",
      "Mashary N. Alrasheedy",
      "Nirban Bhowmick",
      "Shamim Forhad",
      "Md. Shakil Hossain",
      "Sudipto Chaki",
      "Md Shafiqul Islam"
    ],
    "abstract": "Contemporary knowledge-based systems increasingly rely on multilingual emotion identification to support intelligent decision-making, yet they face major challenges due to emotional ambiguity and incomplete supervision. Emotion recognition from text is inherently uncertain because multiple emotional states often co-occur and emotion annotations are frequently missing or heterogeneous. Most existing multi-label emotion classification methods assume fully observed labels and rely on deterministic learning objectives, which can lead to biased learning and unreliable predictions under partial supervision. This paper introduces Reasoning under Ambiguity, an uncertainty-aware framework for multilingual multi-label emotion classification that explicitly aligns learning with annotation uncertainty. The proposed approach uses a shared multilingual encoder with language-specific optimization and an entropy-based ambiguity weighting mechanism that down-weights highly ambiguous training instances rather than treating missing labels as negative evidence. A mask-aware objective with positive-unlabeled regularization is further incorporated to enable robust learning under partial supervision. Experiments on English, Spanish, and Arabic emotion classification benchmarks demonstrate consistent improvements over strong baselines across multiple evaluation metrics, along with improved training stability, robustness to annotation sparsity, and enhanced interpretability.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05471.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05471",
    "published": "2026-02-05T09:17:25Z",
    "updated": "2026-02-05T09:17:25Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05467",
    "title": "MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation",
    "authors": [
      "Dekang Qi",
      "Shuang Zeng",
      "Xinyuan Chang",
      "Feng Xiong",
      "Shichao Xie",
      "Xiaolong Wu",
      "Mu Xu"
    ],
    "abstract": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization.",
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05467.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05467",
    "published": "2026-02-05T09:15:34Z",
    "updated": "2026-02-05T09:15:34Z",
    "comment": "9 pages, 2 figures, 5 tables, conference",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05464",
    "title": "Refine and Purify: Orthogonal Basis Optimization with Null-Space Denoising for Conditional Representation Learning",
    "authors": [
      "Jiaquan Wang",
      "Yan Lyu",
      "Chen Li",
      "Yuheng Jia"
    ],
    "abstract": "Conditional representation learning aims to extract criterion-specific features for customized tasks. Recent studies project universal features onto the conditional feature subspace spanned by an LLM-generated text basis to obtain conditional representations. However, such methods face two key limitations: sensitivity to subspace basis and vulnerability to inter-subspace interference. To address these challenges, we propose OD-CRL, a novel framework integrating Adaptive Orthogonal Basis Optimization (AOBO) and Null-Space Denoising Projection (NSDP). Specifically, AOBO constructs orthogonal semantic bases via singular value decomposition with a curvature-based truncation. NSDP suppresses non-target semantic interference by projecting embeddings onto the null space of irrelevant subspaces. Extensive experiments conducted across customized clustering, customized classification, and customized retrieval tasks demonstrate that OD-CRL achieves a new state-of-the-art performance with superior generalization.",
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05464.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05464",
    "published": "2026-02-05T09:14:44Z",
    "updated": "2026-02-05T09:14:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05463",
    "title": "Thermodynamic Limits of Physical Intelligence",
    "authors": [
      "Koichi Takahashi",
      "Yusuke Hayashi"
    ],
    "abstract": "Modern AI systems achieve remarkable capabilities at the cost of substantial energy consumption. To connect intelligence to physical efficiency, we propose two complementary bits-per-joule metrics under explicit accounting conventions: (1) Thermodynamic Epiplexity per Joule -- bits of structural information about a theoretical environment-instance variable newly encoded in an agent's internal state per unit measured energy within a stated boundary -- and (2) Empowerment per Joule -- the embodied sensorimotor channel capacity (control information) per expected energetic cost over a fixed horizon. These provide two axes of physical intelligence: recognition (model-building) vs.control (action influence). Drawing on stochastic thermodynamics, we show how a Landauer-scale closed-cycle benchmark for epiplexity acquisition follows as a corollary of a standard thermodynamic-learning inequality under explicit subsystem assumptions, and we clarify how Landauer-scaled costs act as closed-cycle benchmarks under explicit reset/reuse and boundary-closure assumptions; conversely, we give a simple decoupling construction showing that without such assumptions -- and without charging for externally prepared low-entropy resources (e.g.fresh memory) crossing the boundary -- information gain and in-boundary dissipation need not be tightly linked. For empirical settings where the latent structure variable is unavailable, we align the operational notion of epiplexity with compute-bounded MDL epiplexity and recommend reporting MDL-epiplexity / compression-gain surrogates as companions. Finally, we propose a unified efficiency framework that reports both metrics together with a minimal checklist of boundary/energy accounting, coarse-graining/noise, horizon/reset, and cost conventions to reduce ambiguity and support consistent bits-per-joule comparisons, and we sketch connections to energy-adjusted scaling analyses.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05463.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05463",
    "published": "2026-02-05T09:12:43Z",
    "updated": "2026-02-05T09:12:43Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05459",
    "title": "When Are RL Hyperparameters Benign? A Study in Offline Goal-Conditioned RL",
    "authors": [
      "Jan Malte TÃ¶pperwien",
      "Aditya Mohan",
      "Marius Lindauer"
    ],
    "abstract": "Hyperparameter sensitivity in Deep Reinforcement Learning (RL) is often accepted as unavoidable. However, it remains unclear whether it is intrinsic to the RL problem or exacerbated by specific training mechanisms. We investigate this question in offline goal-conditioned RL, where data distributions are fixed, and non-stationarity can be explicitly controlled via scheduled shifts in data quality. Additionally, we study varying data qualities under both stationary and non-stationary regimes, and cover two representative algorithms: HIQL (bootstrapped TD-learning) and QRL (quasimetric representation learning). Overall, we observe substantially greater robustness to changes in hyperparameter configurations than commonly reported for online RL, even under controlled non-stationarity. Once modest expert data is present ($\\approx$ 20\\%), QRL maintains broad, stable near-optimal regions, while HIQL exhibits sharp optima that drift significantly across training phases. To explain this divergence, we introduce an inter-goal gradient alignment diagnostic. We find that bootstrapped objectives exhibit stronger destructive gradient interference, which coincides directly with hyperparameter sensitivity. These results suggest that high sensitivity to changes in hyperparameter configurations during training is not inevitable in RL, but is amplified by the dynamics of bootstrapping, offering a pathway toward more robust algorithmic objective design.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05459.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05459",
    "published": "2026-02-05T09:08:17Z",
    "updated": "2026-02-05T09:08:17Z",
    "comment": "27 pages, 19 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05454",
    "title": "Attention Retention for Continual Learning with Vision Transformers",
    "authors": [
      "Yue Lu",
      "Xiangyu Zhou",
      "Shizhou Zhang",
      "Yinghui Xing",
      "Guoqiang Liang",
      "Wencong Zhang"
    ],
    "abstract": "Continual learning (CL) empowers AI systems to progressively acquire knowledge from non-stationary data streams. However, catastrophic forgetting remains a critical challenge. In this work, we identify attention drift in Vision Transformers as a primary source of catastrophic forgetting, where the attention to previously learned visual concepts shifts significantly after learning new tasks. Inspired by neuroscientific insights into the selective attention in the human visual system, we propose a novel attention-retaining framework to mitigate forgetting in CL. Our method constrains attention drift by explicitly modifying gradients during backpropagation through a two-step process: 1) extracting attention maps of the previous task using a layer-wise rollout mechanism and generating instance-adaptive binary masks, and 2) when learning a new task, applying these masks to zero out gradients associated with previous attention regions, thereby preventing disruption of learned visual concepts. For compatibility with modern optimizers, the gradient masking process is further enhanced by scaling parameter updates proportionally to maintain their relative magnitudes. Experiments and visualizations demonstrate the effectiveness of our method in mitigating catastrophic forgetting and preserving visual concepts. It achieves state-of-the-art performance and exhibits robust generalizability across diverse CL scenarios.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05454.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05454",
    "published": "2026-02-05T08:55:58Z",
    "updated": "2026-02-05T08:55:58Z",
    "comment": "AAAI-2026 Camera Ready",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05449",
    "title": "DisCa: Accelerating Video Diffusion Transformers with Distillation-Compatible Learnable Feature Caching",
    "authors": [
      "Chang Zou",
      "Changlin Li",
      "Yang Li",
      "Patrol Li",
      "Jianbing Wu",
      "Xiao He",
      "Songtao Liu",
      "Zhao Zhong",
      "Kailin Huang",
      "Linfeng Zhang"
    ],
    "abstract": "While diffusion models have achieved great success in the field of video generation, this progress is accompanied by a rapidly escalating computational burden. Among the existing acceleration methods, Feature Caching is popular due to its training-free property and considerable speedup performance, but it inevitably faces semantic and detail drop with further compression. Another widely adopted method, training-aware step-distillation, though successful in image generation, also faces drastic degradation in video generation with a few steps. Furthermore, the quality loss becomes more severe when simply applying training-free feature caching to the step-distilled models, due to the sparser sampling steps. This paper novelly introduces a distillation-compatible learnable feature caching mechanism for the first time. We employ a lightweight learnable neural predictor instead of traditional training-free heuristics for diffusion models, enabling a more accurate capture of the high-dimensional feature evolution process. Furthermore, we explore the challenges of highly compressed distillation on large-scale video models and propose a conservative Restricted MeanFlow approach to achieve more stable and lossless distillation. By undertaking these initiatives, we further push the acceleration boundaries to $11.8\\times$ while preserving generation quality. Extensive experiments demonstrate the effectiveness of our method. The code is in the supplementary materials and will be publicly available.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05449.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05449",
    "published": "2026-02-05T08:45:08Z",
    "updated": "2026-02-05T08:45:08Z",
    "comment": "17 pages, 7 figures; cvpr2026 submission",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05448",
    "title": "BLITZRANK: Principled Zero-shot Ranking Agents with Tournament Graphs",
    "authors": [
      "Sheshansh Agrawal",
      "Thien Hang Nguyen",
      "Douwe Kiela"
    ],
    "abstract": "Large language models have emerged as powerful zero-shot rerankers for retrieval-augmented generation, offering strong generalization without task-specific training. However, existing LLM reranking methods either rely on heuristics that fail to fully exploit the information revealed by each ranking decision or are inefficient when they do. We introduce a tournament graph framework that provides a principled foundation for $k$-wise reranking. Our key observation is that each $k$-document comparison reveals a complete tournament of $\\binom{k}{2}$ pairwise preferences. These tournaments are aggregated into a global preference graph, whose transitive closure yields many additional orderings without further model invocations. We formalize when a candidate's rank is certifiably determined and design a query schedule that greedily maximizes information gain towards identifying the top-$m$ items. Our framework also gracefully handles non-transitive preferences - cycles induced by LLM judgments - by collapsing them into equivalence classes that yield principled tiered rankings. Empirically, across 14 benchmarks and 5 LLMs, our method achieves Pareto dominance over existing methods: matching or exceeding accuracy while requiring 25-40% fewer tokens than comparable approaches, and 7$\\times$ fewer than pairwise methods at near-identical quality.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05448.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05448",
    "published": "2026-02-05T08:41:00Z",
    "updated": "2026-02-05T08:41:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05447",
    "title": "Structured Context Engineering for File-Native Agentic Systems: Evaluating Schema Accuracy, Format Effectiveness, and Multi-File Navigation at Scale",
    "authors": [
      "Damon McMillan"
    ],
    "abstract": "Large Language Model agents increasingly operate external systems through programmatic interfaces, yet practitioners lack empirical guidance on how to structure the context these agents consume. Using SQL generation as a proxy for programmatic agent operations, we present a systematic study of context engineering for structured data, comprising 9,649 experiments across 11 models, 4 formats (YAML, Markdown, JSON, Token-Oriented Object Notation [TOON]), and schemas ranging from 10 to 10,000 tables.   Our findings challenge common assumptions. First, architecture choice is model-dependent: file-based context retrieval improves accuracy for frontier-tier models (Claude, GPT, Gemini; +2.7%, p=0.029) but shows mixed results for open source models (aggregate -7.7%, p<0.001), with deficits varying substantially by model. Second, format does not significantly affect aggregate accuracy (chi-squared=2.45, p=0.484), though individual models, particularly open source, exhibit format-specific sensitivities. Third, model capability is the dominant factor, with a 21 percentage point accuracy gap between frontier and open source tiers that dwarfs any format or architecture effect. Fourth, file-native agents scale to 10,000 tables through domain-partitioned schemas while maintaining high navigation accuracy. Fifth, file size does not predict runtime efficiency: compact formats can consume significantly more tokens at scale due to format-unfamiliar search patterns.   These findings provide practitioners with evidence-based guidance for deploying LLM agents on structured systems, demonstrating that architectural decisions should be tailored to model capability rather than assuming universal best practices.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05447.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05447",
    "published": "2026-02-05T08:39:05Z",
    "updated": "2026-02-05T08:39:05Z",
    "comment": "8 pages, 7 figures, 10 tables, 26 references",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05444",
    "title": "Causal Front-Door Adjustment for Robust Jailbreak Attacks on LLMs",
    "authors": [
      "Yao Zhou",
      "Zeen Song",
      "Wenwen Qiang",
      "Fengge Wu",
      "Shuyi Zhou",
      "Changwen Zheng",
      "Hui Xiong"
    ],
    "abstract": "Safety alignment mechanisms in Large Language Models (LLMs) often operate as latent internal states, obscuring the model's inherent capabilities. Building on this observation, we model the safety mechanism as an unobserved confounder from a causal perspective. Then, we propose the \\textbf{C}ausal \\textbf{F}ront-Door \\textbf{A}djustment \\textbf{A}ttack ({\\textbf{CFA}}$^2$) to jailbreak LLM, which is a framework that leverages Pearl's Front-Door Criterion to sever the confounding associations for robust jailbreaking. Specifically, we employ Sparse Autoencoders (SAEs) to physically strip defense-related features, isolating the core task intent. We further reduce computationally expensive marginalization to a deterministic intervention with low inference complexity. Experiments demonstrate that {CFA}$^2$ achieves state-of-the-art attack success rates while offering a mechanistic interpretation of the jailbreaking process.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05444.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05444",
    "published": "2026-02-05T08:34:49Z",
    "updated": "2026-02-05T08:34:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05440",
    "title": "Synthetic Defect Geometries of Cast Metal Objects Modeled via 2d Voronoi Tessellations",
    "authors": [
      "Natascha Jeziorski",
      "Petra GospodnetiÄ",
      "Claudia Redenbach"
    ],
    "abstract": "In industry, defect detection is crucial for quality control. Non-destructive testing (NDT) methods are preferred as they do not influence the functionality of the object while inspecting. Automated data evaluation for automated defect detection is a growing field of research. In particular, machine learning approaches show promising results. To provide training data in sufficient amount and quality, synthetic data can be used. Rule-based approaches enable synthetic data generation in a controllable environment. Therefore, a digital twin of the inspected object including synthetic defects is needed. We present parametric methods to model 3d mesh objects of various defect types that can then be added to the object geometry to obtain synthetic defective objects. The models are motivated by common defects in metal casting but can be transferred to other machining procedures that produce similar defect shapes. Synthetic data resembling the real inspection data can then be created by using a physically based Monte Carlo simulation of the respective testing method. Using our defect models, a variable and arbitrarily large synthetic data set can be generated with the possibility to include rarely occurring defects in sufficient quantity. Pixel-perfect annotation can be created in parallel. As an example, we will use visual surface inspection, but the procedure can be applied in combination with simulations for any other NDT method.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05440.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05440",
    "published": "2026-02-05T08:28:44Z",
    "updated": "2026-02-05T08:28:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05437",
    "title": "Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models",
    "authors": [
      "Basel Mousi",
      "Fahim Dalvi",
      "Shammur Chowdhury",
      "Firoj Alam",
      "Nadir Durrani"
    ],
    "abstract": "Vision-language models (VLMs) can achieve high accuracy while still accepting culturally plausible but visually incorrect interpretations. Existing hallucination benchmarks rarely test this failure mode, particularly outside Western contexts and English. We introduce M2CQA, a culturally grounded multimodal benchmark built from images spanning 17 MENA countries, paired with contrastive true and counterfactual statements in English, Arabic, and its dialects. To isolate hallucination beyond raw accuracy, we propose the CounterFactual Hallucination Rate (CFHR), which measures counterfactual acceptance conditioned on correctly answering the true statement. Evaluating state-of-the-art VLMs under multiple prompting strategies, we find that CFHR rises sharply in Arabic, especially in dialects, even when true-statement accuracy remains high. Moreover, reasoning-first prompting consistently increases counterfactual hallucination, while answering before justifying improves robustness. We will make the experimental resources and dataset publicly available for the community.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05437.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05437",
    "published": "2026-02-05T08:26:44Z",
    "updated": "2026-02-05T08:26:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05435",
    "title": "Stable Velocity: A Variance Perspective on Flow Matching",
    "authors": [
      "Donglin Yang",
      "Yongxing Zhang",
      "Xin Yu",
      "Liang Hou",
      "Xin Tao",
      "Pengfei Wan",
      "Xiaojuan Qi",
      "Renjie Liao"
    ],
    "abstract": "While flow matching is elegant, its reliance on single-sample conditional velocities leads to high-variance training targets that destabilize optimization and slow convergence. By explicitly characterizing this variance, we identify 1) a high-variance regime near the prior, where optimization is challenging, and 2) a low-variance regime near the data distribution, where conditional and marginal velocities nearly coincide. Leveraging this insight, we propose Stable Velocity, a unified framework that improves both training and sampling. For training, we introduce Stable Velocity Matching (StableVM), an unbiased variance-reduction objective, along with Variance-Aware Representation Alignment (VA-REPA), which adaptively strengthen auxiliary supervision in the low-variance regime. For inference, we show that dynamics in the low-variance regime admit closed-form simplifications, enabling Stable Velocity Sampling (StableVS), a finetuning-free acceleration. Extensive experiments on ImageNet $256\\times256$ and large pretrained text-to-image and text-to-video models, including SD3.5, Flux, Qwen-Image, and Wan2.2, demonstrate consistent improvements in training efficiency and more than $2\\times$ faster sampling within the low-variance regime without degrading sample quality. Our code is available at https://github.com/linYDTHU/StableVelocity.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05435.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05435",
    "published": "2026-02-05T08:25:05Z",
    "updated": "2026-02-05T08:25:05Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05434",
    "title": "LD-SLRO: Latent Diffusion Structured Light for 3-D Reconstruction of Highly Reflective Objects",
    "authors": [
      "Sanghoon Jeon",
      "Gihyun Jung",
      "Suhyeon Ka",
      "Jae-Sang Hyun"
    ],
    "abstract": "Fringe projection profilometry-based 3-D reconstruction of objects with high reflectivity and low surface roughness remains a significant challenge. When measuring such glossy surfaces, specular reflection and indirect illumination often lead to severe distortion or loss of the projected fringe patterns. To address these issues, we propose a latent diffusion-based structured light for reflective objects (LD-SLRO). Phase-shifted fringe images captured from highly reflective surfaces are first encoded to extract latent representations that capture surface reflectance characteristics. These latent features are then used as conditional inputs to a latent diffusion model, which probabilistically suppresses reflection-induced artifacts and recover lost fringe information, yielding high-quality fringe images. The proposed components, including the specular reflection encoder, time-variant channel affine layer, and attention modules, further improve fringe restoration quality. In addition, LD-SLRO provides high flexibility in configuring the input and output fringe sets. Experimental results demonstrate that the proposed method improves both fringe quality and 3-D reconstruction accuracy over state-of-the-art methods, reducing the average root-mean-squared error from 1.8176 mm to 0.9619 mm.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05434.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05434",
    "published": "2026-02-05T08:24:38Z",
    "updated": "2026-02-05T08:24:38Z",
    "comment": "10 pages, 7 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05430",
    "title": "Day-Ahead Electricity Price Forecasting for Volatile Markets Using Foundation Models with Regularization Strategy",
    "authors": [
      "Kritchanat Ponyuenyong",
      "Pengyu Tu",
      "Jia Wei Tan",
      "Wei Soon Cheong",
      "Jamie Ng Suat Ling",
      "Lianlian Jiang"
    ],
    "abstract": "Electricity price forecasting (EPF) is essential for energy markets stakeholders (e.g. grid operators, energy traders, policymakers) but remains challenging due to the inherent volatility and nonlinearity of price signals. Traditional statistical and deep learning (DL) models often struggle to capture complex temporal dependencies and integrate heterogeneous data effectively. While time series foundation models (TSFMs) have shown strong performance in general time series forecasting tasks, such as traffic forecasting and weather forecasting. However, their effectiveness in day-ahead EPF, particularly in volatile markets, remains underexplored. This paper presents a spike regularization strategy and evaluates a wide range of TSFMs, including Tiny Time Mixers (TTMs), MOIRAI, MOMENT, and TimesFM, against traditional statistical and DL models such as Autoregressive Integrated Moving Average (ARIMA), Long-short Term Memory (LSTM), and Convolutional Neural Network - LSTM (CNN-LSTM) using half-hourly wholesale market data with volatile trends in Singapore. Exogenous factors (e.g. weather and calendar variables) are also incorporated into models where applicable. Results demonstrate that TSFMs consistently outperform traditional approaches, achieving up to 37.4% improvement in MAPE across various evaluation settings. The findings offer practical guidance for improving forecast accuracy and decision-making in volatile electricity markets.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05430.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05430",
    "published": "2026-02-05T08:20:50Z",
    "updated": "2026-02-05T08:20:50Z",
    "comment": "Accepted to AI4TS Workshop @ AAAI'26 (Oral and Poster), see https://ai4ts.github.io/aaai2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05429",
    "title": "M$^2$-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining",
    "authors": [
      "Rui Lv",
      "Juncheng Mo",
      "Tianyi Chu",
      "Chen Rao",
      "Hongyi Jing",
      "Jiajie Teng",
      "Jiafu Chen",
      "Shiqi Zhang",
      "Liangzi Ding",
      "Shuo Fang",
      "Huaizhong Lin",
      "Ziqiang Dang",
      "Chenguang Ma",
      "Lei Zhao"
    ],
    "abstract": "Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent-trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M$^2$-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.",
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05429.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05429",
    "published": "2026-02-05T08:19:39Z",
    "updated": "2026-02-05T08:19:39Z",
    "comment": "Accepted by ICLR 2026. Supplementary material is included at the end of the main paper (16 pages, 15 figures, 2 tables)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05426",
    "title": "Multi-AD: Cross-Domain Unsupervised Anomaly Detection for Medical and Industrial Applications",
    "authors": [
      "Wahyu Rahmaniar",
      "Kenji Suzuki"
    ],
    "abstract": "Traditional deep learning models often lack annotated data, especially in cross-domain applications such as anomaly detection, which is critical for early disease diagnosis in medicine and defect detection in industry. To address this challenge, we propose Multi-AD, a convolutional neural network (CNN) model for robust unsupervised anomaly detection across medical and industrial images. Our approach employs the squeeze-and-excitation (SE) block to enhance feature extraction via channel-wise attention, enabling the model to focus on the most relevant features and detect subtle anomalies. Knowledge distillation (KD) transfers informative features from the teacher to the student model, enabling effective learning of the differences between normal and anomalous data. Then, the discriminator network further enhances the model's capacity to distinguish between normal and anomalous data. At the inference stage, by integrating multi-scale features, the student model can detect anomalies of varying sizes. The teacher-student (T-S) architecture ensures consistent representation of high-dimensional features while adapting them to enhance anomaly detection. Multi-AD was evaluated on several medical datasets, including brain MRI, liver CT, and retina OCT, as well as industrial datasets, such as MVTec AD, demonstrating strong generalization across multiple domains. Experimental results demonstrated that our approach consistently outperformed state-of-the-art models, achieving the best average AUROC for both image-level (81.4% for medical and 99.6% for industrial) and pixel-level (97.0% for medical and 98.4% for industrial) tasks, making it effective for real-world applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05426.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05426",
    "published": "2026-02-05T08:17:42Z",
    "updated": "2026-02-05T08:17:42Z",
    "comment": "28 pages, 8 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05424",
    "title": "THOR: Inductive Link Prediction over Hyper-Relational Knowledge Graphs",
    "authors": [
      "Weijian Yu",
      "Yuhuan Lu",
      "Dingqi Yang"
    ],
    "abstract": "Knowledge graphs (KGs) have become a key ingredient supporting a variety of applications. Beyond the traditional triplet representation of facts where a relation connects two entities, modern KGs observe an increasing number of hyper-relational facts, where an arbitrary number of qualifiers associated with a triplet provide auxiliary information to further describe the rich semantics of the triplet, which can effectively boost the reasoning performance in link prediction tasks. However, existing link prediction techniques over such hyper-relational KGs (HKGs) mostly focus on a transductive setting, where KG embedding models are learned from the specific vocabulary of a given KG and subsequently can only make predictions within the same vocabulary, limiting their generalizability to previously unseen vocabularies. Against this background, we propose THOR, an inducTive link prediction technique for Hyper-relational knOwledge gRaphs. Specifically, we first introduce both relation and entity foundation graphs, modeling their fundamental inter- and intra-fact interactions in HKGs, which are agnostic to any specific relations and entities. Afterward, THOR is designed to learn from the two foundation graphs with two parallel graph encoders followed by a transformer decoder, which supports efficient masked training and fully-inductive inference. We conduct a thorough evaluation of THOR in hyper-relational link prediction tasks on 12 datasets with different settings. Results show that THOR outperforms a sizable collection of baselines, yielding 66.1%, 55.9%, and 20.4% improvement over the best-performing rule-based, semi-inductive, and fully-inductive techniques, respectively. A series of ablation studies also reveals our key design factors capturing the structural invariance transferable across HKGs for inductive tasks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05424.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05424",
    "published": "2026-02-05T08:15:49Z",
    "updated": "2026-02-05T08:15:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05423",
    "title": "NeVStereo: A NeRF-Driven NVS-Stereo Architecture for High-Fidelity 3D Tasks",
    "authors": [
      "Pengcheng Chen",
      "Yue Hu",
      "Wenhao Li",
      "Nicole M Gunderson",
      "Andrew Feng",
      "Zhenglong Sun",
      "Peter Beerel",
      "Eric J Seibel"
    ],
    "abstract": "In modern dense 3D reconstruction, feed-forward systems (e.g., VGGT, pi3) focus on end-to-end matching and geometry prediction but do not explicitly output the novel view synthesis (NVS). Neural rendering-based approaches offer high-fidelity NVS and detailed geometry from posed images, yet they typically assume fixed camera poses and can be sensitive to pose errors. As a result, it remains non-trivial to obtain a single framework that can offer accurate poses, reliable depth, high-quality rendering, and accurate 3D surfaces from casually captured views. We present NeVStereo, a NeRF-driven NVS-stereo architecture that aims to jointly deliver camera poses, multi-view depth, novel view synthesis, and surface reconstruction from multi-view RGB-only inputs. NeVStereo combines NeRF-based NVS for stereo-friendly renderings, confidence-guided multi-view depth estimation, NeRF-coupled bundle adjustment for pose refinement, and an iterative refinement stage that updates both depth and the radiance field to improve geometric consistency. This design mitigated the common NeRF-based issues such as surface stacking, artifacts, and pose-depth coupling. Across indoor, outdoor, tabletop, and aerial benchmarks, our experiments indicate that NeVStereo achieves consistently strong zero-shot performance, with up to 36% lower depth error, 10.4% improved pose accuracy, 4.5% higher NVS fidelity, and state-of-the-art mesh quality (F1 91.93%, Chamfer 4.35 mm) compared to existing prestigious methods.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05423.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05423",
    "published": "2026-02-05T08:15:06Z",
    "updated": "2026-02-05T08:15:06Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05420",
    "title": "Disco: Densely-overlapping Cell Instance Segmentation via Adjacency-aware Collaborative Coloring",
    "authors": [
      "Rui Sun",
      "Yiwen Yang",
      "Kaiyu Guo",
      "Chen Jiang",
      "Dongli Xu",
      "Zhaonan Liu",
      "Tan Pan",
      "Limei Han",
      "Xue Jiang",
      "Wu Wei",
      "Yuan Cheng"
    ],
    "abstract": "Accurate cell instance segmentation is foundational for digital pathology analysis. Existing methods based on contour detection and distance mapping still face significant challenges in processing complex and dense cellular regions. Graph coloring-based methods provide a new paradigm for this task, yet the effectiveness of this paradigm in real-world scenarios with dense overlaps and complex topologies has not been verified. Addressing this issue, we release a large-scale dataset GBC-FS 2025, which contains highly complex and dense sub-cellular nuclear arrangements. We conduct the first systematic analysis of the chromatic properties of cell adjacency graphs across four diverse datasets and reveal an important discovery: most real-world cell graphs are non-bipartite, with a high prevalence of odd-length cycles (predominantly triangles). This makes simple 2-coloring theory insufficient for handling complex tissues, while higher-chromaticity models would cause representational redundancy and optimization difficulties. Building on this observation of complex real-world contexts, we propose Disco (Densely-overlapping Cell Instance Segmentation via Adjacency-aware COllaborative Coloring), an adjacency-aware framework based on the \"divide and conquer\" principle. It uniquely combines a data-driven topological labeling strategy with a constrained deep learning system to resolve complex adjacency conflicts. First, \"Explicit Marking\" strategy transforms the topological challenge into a learnable classification task by recursively decomposing the cell graph and isolating a \"conflict set.\" Second, \"Implicit Disambiguation\" mechanism resolves ambiguities in conflict regions by enforcing feature dissimilarity between different instances, enabling the model to learn separable feature representations.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05420.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05420",
    "published": "2026-02-05T08:05:48Z",
    "updated": "2026-02-05T08:05:48Z",
    "comment": "17 pages, 10 figures; ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05419",
    "title": "Grammatical Error Correction Evaluation by Optimally Transporting Edit Representation",
    "authors": [
      "Takumi Goto",
      "Yusuke Sakai",
      "Taro Watanabe"
    ],
    "abstract": "Automatic evaluation in grammatical error correction (GEC) is crucial for selecting the best-performing systems. Currently, reference-based metrics are a popular choice, which basically measure the similarity between hypothesis and reference sentences. However, similarity measures based on embeddings, such as BERTScore, are often ineffective, since many words in the source sentences remain unchanged in both the hypothesis and the reference. This study focuses on edits specifically designed for GEC, i.e., ERRANT, and computes similarity measured over the edits from the source sentence. To this end, we propose edit vector, a representation for an edit, and introduce a new metric, UOT-ERRANT, which transports these edit vectors from hypothesis to reference using unbalanced optimal transport. Experiments with SEEDA meta-evaluation show that UOT-ERRANT improves evaluation performance, particularly in the +Fluency domain where many edits occur. Moreover, our method is highly interpretable because the transport plan can be interpreted as a soft edit alignment, making UOT-ERRANT a useful metric for both system ranking and analyzing GEC systems. Our code is available from https://github.com/gotutiyan/uot-errant.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05419.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05419",
    "published": "2026-02-05T08:05:42Z",
    "updated": "2026-02-05T08:05:42Z",
    "comment": "Accepted to TACL. This is a pre-MIT Press publication version",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05415",
    "title": "VMF-GOS: Geometry-guided virtual Outlier Synthesis for Long-Tailed OOD Detection",
    "authors": [
      "Ningkang Peng",
      "Qianfeng Yu",
      "Yuhao Zhang",
      "Yafei Liu",
      "Xiaoqian Peng",
      "Peirong Ma",
      "Yi Chen",
      "Peiheng Li",
      "Yanhui Gu"
    ],
    "abstract": "Out-of-Distribution (OOD) detection under long-tailed distributions is a highly challenging task because the scarcity of samples in tail classes leads to blurred decision boundaries in the feature space. Current state-of-the-art (sota) methods typically employ Outlier Exposure (OE) strategies, relying on large-scale real external datasets (such as 80 Million Tiny Images) to regularize the feature space. However, this dependence on external data often becomes infeasible in practical deployment due to high data acquisition costs and privacy sensitivity. To this end, we propose a novel data-free framework aimed at completely eliminating reliance on external datasets while maintaining superior detection performance. We introduce a Geometry-guided virtual Outlier Synthesis (GOS) strategy that models statistical properties using the von Mises-Fisher (vMF) distribution on a hypersphere. Specifically, we locate a low-likelihood annulus in the feature space and perform directional sampling of virtual outliers in this region. Simultaneously, we introduce a new Dual-Granularity Semantic Loss (DGS) that utilizes contrastive learning to maximize the distinction between in-distribution (ID) features and these synthesized boundary outliers. Extensive experiments on benchmarks such as CIFAR-LT demonstrate that our method outperforms sota approaches that utilize external real images.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05415.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05415",
    "published": "2026-02-05T07:58:12Z",
    "updated": "2026-02-05T07:58:12Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05414",
    "title": "TSBOW: Traffic Surveillance Benchmark for Occluded Vehicles Under Various Weather Conditions",
    "authors": [
      "Ngoc Doan-Minh Huynh",
      "Duong Nguyen-Ngoc Tran",
      "Long Hoang Pham",
      "Tai Huu-Phuong Tran",
      "Hyung-Joon Jeon",
      "Huy-Hung Nguyen",
      "Duong Khac Vu",
      "Hyung-Min Jeon",
      "Son Hong Phan",
      "Quoc Pham-Nam Ho",
      "Chi Dai Tran",
      "Trinh Le Ba Khanh",
      "Jae Wook Jeon"
    ],
    "abstract": "Global warming has intensified the frequency and severity of extreme weather events, which degrade CCTV signal and video quality while disrupting traffic flow, thereby increasing traffic accident rates. Existing datasets, often limited to light haze, rain, and snow, fail to capture extreme weather conditions. To address this gap, this study introduces the Traffic Surveillance Benchmark for Occluded vehicles under various Weather conditions (TSBOW), a comprehensive dataset designed to enhance occluded vehicle detection across diverse annual weather scenarios. Comprising over 32 hours of real-world traffic data from densely populated urban areas, TSBOW includes more than 48,000 manually annotated and 3.2 million semi-labeled frames; bounding boxes spanning eight traffic participant classes from large vehicles to micromobility devices and pedestrians. We establish an object detection benchmark for TSBOW, highlighting challenges posed by occlusions and adverse weather. With its varied road types, scales, and viewpoints, TSBOW serves as a critical resource for advancing Intelligent Transportation Systems. Our findings underscore the potential of CCTV-based traffic monitoring, pave the way for new research and applications. The TSBOW dataset is publicly available at: https://github.com/SKKUAutoLab/TSBOW.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05414.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05414",
    "published": "2026-02-05T07:52:37Z",
    "updated": "2026-02-05T07:52:37Z",
    "comment": "This paper has been accepted by the 40th AAAI Conference on Artificial Intelligence (AAAI-26)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05410",
    "title": "Robust Federated Learning via Byzantine Filtering over Encrypted Updates",
    "authors": [
      "Adda Akram Bendoukha",
      "Aymen Boudguiga",
      "Nesrine Kaaniche",
      "Renaud Sirdey",
      "Didem Demirag",
      "SÃ©bastien Gambs"
    ],
    "abstract": "Federated Learning (FL) aims to train a collaborative model while preserving data privacy. However, the distributed nature of this approach still raises privacy and security issues, such as the exposure of sensitive data due to inference attacks and the influence of Byzantine behaviors on the trained model. In particular, achieving both secure aggregation and Byzantine resilience remains challenging, as existing solutions often address these aspects independently. In this work, we propose to address these challenges through a novel approach that combines homomorphic encryption for privacy-preserving aggregation with property-inference-inspired meta-classifiers for Byzantine filtering. First, following the property-inference attacks blueprint, we train a set of filtering meta-classifiers on labeled shadow updates, reproducing a diverse ensemble of Byzantine misbehaviors in FL, including backdoor, gradient-inversion, label-flipping and shuffling attacks. The outputs of these meta-classifiers are then used to cancel the Byzantine encrypted updates by reweighting. Second, we propose an automated method for selecting the optimal kernel and the dimensionality hyperparameters with respect to homomorphic inference, aggregation constraints and efficiency over the CKKS cryptosystem. Finally, we demonstrate through extensive experiments the effectiveness of our approach against Byzantine participants on the FEMNIST, CIFAR10, GTSRB, and acsincome benchmarks. More precisely, our SVM filtering achieves accuracies between $90$% and $94$% for identifying Byzantine updates at the cost of marginal losses in model utility and encrypted inference runtimes ranging from $6$ to $24$ seconds and from $9$ to $26$ seconds for an overall aggregation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05410.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05410",
    "published": "2026-02-05T07:46:19Z",
    "updated": "2026-02-05T07:46:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05407",
    "title": "H-AdminSim: A Multi-Agent Simulator for Realistic Hospital Administrative Workflows with FHIR Integration",
    "authors": [
      "Jun-Min Lee",
      "Meong Hi Son",
      "Edward Choi"
    ],
    "abstract": "Hospital administration departments handle a wide range of operational tasks and, in large hospitals, process over 10,000 requests per day, driving growing interest in LLM-based automation. However, prior work has focused primarily on patient--physician interactions or isolated administrative subtasks, failing to capture the complexity of real administrative workflows. To address this gap, we propose H-AdminSim, a comprehensive end-to-end simulation framework that combines realistic data generation with multi-agent-based simulation of hospital administrative workflows. These tasks are quantitatively evaluated using detailed rubrics, enabling systematic comparison of LLMs. Through FHIR integration, H-AdminSim provides a unified and interoperable environment for testing administrative workflows across heterogeneous hospital settings, serving as a standardized testbed for assessing the feasibility and performance of LLM-driven administrative automation.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05407.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05407",
    "published": "2026-02-05T07:44:56Z",
    "updated": "2026-02-05T07:44:56Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05403",
    "title": "Advancing Opinion Dynamics Modeling with Neural Diffusion-Convection-Reaction Equation",
    "authors": [
      "Chenghua Gong",
      "Yihang Jiang",
      "Hao Li",
      "Rui Sun",
      "Juyuan Zhang",
      "Tianjun Gu",
      "Liming Pan",
      "Linyuan LÃ¼"
    ],
    "abstract": "Advanced opinion dynamics modeling is vital for deciphering social behavior, emphasizing its role in mitigating polarization and securing cyberspace. To synergize mechanistic interpretability with data-driven flexibility, recent studies have explored the integration of Physics-Informed Neural Networks (PINNs) for opinion modeling. Despite this promise, existing methods are tailored to incomplete priors, lacking a comprehensive physical system to integrate dynamics from local, global, and endogenous levels. Moreover, penalty-based constraints adopted in existing methods struggle to deeply encode physical priors, leading to optimization pathologies and discrepancy between latent representations and physical transparency. To this end, we offer a physical view to interpret opinion dynamics via Diffusion-Convection-Reaction (DCR) system inspired by interacting particle theory. Building upon the Neural ODEs, we define the neural opinion dynamics to coordinate neural networks with physical priors, and further present the OPINN, a physics-informed neural framework for opinion dynamics modeling. Evaluated on real-world and synthetic datasets, OPINN achieves state-of-the-art performance in opinion evolution forecasting, offering a promising paradigm for the nexus of cyber, physical, and social systems.",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05403.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05403",
    "published": "2026-02-05T07:41:19Z",
    "updated": "2026-02-05T07:41:19Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05400",
    "title": "OPUS: Towards Efficient and Principled Data Selection in Large Language Model Pre-training in Every Iteration",
    "authors": [
      "Shaobo Wang",
      "Xuan Ouyang",
      "Tianyi Xu",
      "Yuzheng Hu",
      "Jialin Liu",
      "Guo Chen",
      "Tianyu Zhang",
      "Junhao Zheng",
      "Kexin Yang",
      "Xingzhang Ren",
      "Dayiheng Liu",
      "Linfeng Zhang"
    ],
    "abstract": "As high-quality public text approaches exhaustion, a phenomenon known as the Data Wall, pre-training is shifting from more tokens to better tokens. However, existing methods either rely on heuristic static filters that ignore training dynamics, or use dynamic yet optimizer-agnostic criteria based on raw gradients. We propose OPUS (Optimizer-induced Projected Utility Selection), a dynamic data selection framework that defines utility in the optimizer-induced update space. OPUS scores candidates by projecting their effective updates, shaped by modern optimizers, onto a target direction derived from a stable, in-distribution proxy. To ensure scalability, we employ Ghost technique with CountSketch for computational efficiency, and Boltzmann sampling for data diversity, incurring only 4.7\\% additional compute overhead. OPUS achieves remarkable results across diverse corpora, quality tiers, optimizers, and model scales. In pre-training of GPT-2 Large/XL on FineWeb and FineWeb-Edu with 30B tokens, OPUS outperforms industrial-level baselines and even full 200B-token training. Moreover, when combined with industrial-level static filters, OPUS further improves pre-training efficiency, even with lower-quality data. Furthermore, in continued pre-training of Qwen3-8B-Base on SciencePedia, OPUS achieves superior performance using only 0.5B tokens compared to full training with 3B tokens, demonstrating significant data efficiency gains in specialized domains.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05400.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05400",
    "published": "2026-02-05T07:34:23Z",
    "updated": "2026-02-05T07:34:23Z",
    "comment": "45 pages, 7 figures, 8 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05397",
    "title": "Explainable Pathomics Feature Visualization via Correlation-aware Conditional Feature Editing",
    "authors": [
      "Yuechen Yang",
      "Junlin Guo",
      "Ruining Deng",
      "Junchao Zhu",
      "Zhengyi Lu",
      "Chongyu Qu",
      "Yanfan Zhu",
      "Xingyi Guo",
      "Yu Wang",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ],
    "abstract": "Pathomics is a recent approach that offers rich quantitative features beyond what black-box deep learning can provide, supporting more reproducible and explainable biomarkers in digital pathology. However, many derived features (e.g., \"second-order moment\") remain difficult to interpret, especially across different clinical contexts, which limits their practical adoption. Conditional diffusion models show promise for explainability through feature editing, but they typically assume feature independence**--**an assumption violated by intrinsically correlated pathomics features. Consequently, editing one feature while fixing others can push the model off the biological manifold and produce unrealistic artifacts. To address this, we propose a Manifold-Aware Diffusion (MAD) framework for controllable and biologically plausible cell nuclei editing. Unlike existing approaches, our method regularizes feature trajectories within a disentangled latent space learned by a variational auto-encoder (VAE). This ensures that manipulating a target feature automatically adjusts correlated attributes to remain within the learned distribution of real cells. These optimized features then guide a conditional diffusion model to synthesize high-fidelity images. Experiments demonstrate that our approach is able to navigate the manifold of pathomics features when editing those features. The proposed method outperforms baseline methods in conditional feature editing while preserving structural coherence.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05397.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05397",
    "published": "2026-02-05T07:28:54Z",
    "updated": "2026-02-05T07:28:54Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05393",
    "title": "Late-to-Early Training: LET LLMs Learn Earlier, So Faster and Better",
    "authors": [
      "Ji Zhao",
      "Yufei Gu",
      "Shitong Shao",
      "Xun Zhou",
      "Liang Xiang",
      "Zeke Xie"
    ],
    "abstract": "As Large Language Models (LLMs) achieve remarkable empirical success through scaling model and data size, pretraining has become increasingly critical yet computationally prohibitive, hindering rapid development. Despite the availability of numerous pretrained LLMs developed at significant computational expense, a fundamental real-world question remains underexplored: \\textit{Can we leverage existing small pretrained models to accelerate the training of larger models?} In this paper, we propose a Late-to-Early Training (LET) paradigm that enables LLMs to explicitly learn later knowledge in earlier steps and earlier layers. The core idea is to guide the early layers of an LLM during early training using representations from the late layers of a pretrained (i.e. late training phase) model. We identify two key mechanisms that drive LET's effectiveness: late-to-early-step learning and late-to-early-layer learning. These mechanisms significantly accelerate training convergence while robustly enhancing both language modeling capabilities and downstream task performance, enabling faster training with superior performance. Extensive experiments on 1.4B and 7B parameter models demonstrate LET's efficiency and effectiveness. Notably, when training a 1.4B LLM on the Pile dataset, our method achieves up to 1.6$\\times$ speedup with nearly 5\\% improvement in downstream task accuracy compared to standard training, even when using a pretrained model with 10$\\times$ fewer parameters than the target model.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05393.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05393",
    "published": "2026-02-05T07:19:34Z",
    "updated": "2026-02-05T07:19:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05392",
    "title": "Beyond Length: Context-Aware Expansion and Independence as Developmentally Sensitive Evaluation in Child Utterances",
    "authors": [
      "Jiyun Chun",
      "Eric Fosler-Lussier",
      "Michael White",
      "Andrew Perrault"
    ],
    "abstract": "Evaluating the quality of children's utterances in adult-child dialogue remains challenging due to insufficient context-sensitive metrics. Common proxies such as Mean Length of Utterance (MLU), lexical diversity (vocd-D), and readability indices (Flesch-Kincaid Grade Level, Gunning Fog Index) are dominated by length and ignore conversational context, missing aspects of response quality such as reasoning depth, topic maintenance, and discourse planning. We introduce an LLM-as-a-judge framework that first classifies the Previous Adult Utterance Type and then scores the child's response along two axes: Expansion (contextual elaboration and inferential depth) and Independence (the child's contribution to advancing the discourse). These axes reflect fundamental dimensions in child language development, where Expansion captures elaboration, clause combining, and causal and contrastive connectives. Independence captures initiative, topic control, decreasing reliance on adult scaffolding through growing self-regulation, and audience design. We establish developmental validity by showing age-related patterns and demonstrate predictive value by improving age estimation over common baselines. We further confirm semantic sensitivity by detecting differences tied to discourse relations. Our metrics align with human judgments, enabling large-scale evaluation. This shifts child utterance assessment from simply measuring length to evaluating how meaningfully the child's speech contributes to and advances the conversation within its context.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05392.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05392",
    "published": "2026-02-05T07:19:04Z",
    "updated": "2026-02-05T07:19:04Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05391",
    "title": "Dataset Distillation via Relative Distribution Matching and Cognitive Heritage",
    "authors": [
      "Qianxin Xia",
      "Jiawei Du",
      "Yuhan Zhang",
      "Jielei Wang",
      "Guoming Lu"
    ],
    "abstract": "Dataset distillation seeks to synthesize a highly compact dataset that achieves performance comparable to the original dataset on downstream tasks. For the classification task that use pre-trained self-supervised models as backbones, previous linear gradient matching optimizes synthetic images by encouraging them to mimic the gradient updates induced by real images on the linear classifier. However, this batch-level formulation requires loading thousands of real images and applying multiple rounds of differentiable augmentations to synthetic images at each distillation step, leading to substantial computational and memory overhead. In this paper, we introduce statistical flow matching , a stable and efficient supervised learning framework that optimizes synthetic images by aligning constant statistical flows from target class centers to non-target class centers in the original data. Our approach loads raw statistics only once and performs a single augmentation pass on the synthetic data, achieving performance comparable to or better than the state-of-the-art methods with 10x lower GPU memory usage and 4x shorter runtime. Furthermore, we propose a classifier inheritance strategy that reuses the classifier trained on the original dataset for inference, requiring only an extremely lightweight linear projector and marginal storage while achieving substantial performance gains.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05391.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05391",
    "published": "2026-02-05T07:18:48Z",
    "updated": "2026-02-05T07:18:48Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05390",
    "title": "Assessing Electricity Demand Forecasting with Exogenous Data in Time Series Foundation Models",
    "authors": [
      "Wei Soon Cheong",
      "Lian Lian Jiang",
      "Jamie Ng Suat Ling"
    ],
    "abstract": "Time-series foundation models have emerged as a new paradigm for forecasting, yet their ability to effectively leverage exogenous features -- critical for electricity demand forecasting -- remains unclear. This paper empirically evaluates foundation models capable of modeling cross-channel correlations against a baseline LSTM with reversible instance normalization across Singaporean and Australian electricity markets at hourly and daily granularities. We systematically assess MOIRAI, MOMENT, TinyTimeMixers, ChronosX, and Chronos-2 under three feature configurations: all features, selected features, and target-only. Our findings reveal highly variable effectiveness: while Chronos-2 achieves the best performance among foundation models (in zero-shot settings), the simple baseline frequently outperforms all foundation models in Singapore's stable climate, particularly for short-term horizons. Model architecture proves critical, with synergistic architectural implementations (TTM's channel-mixing, Chronos-2's grouped attention) consistently leveraging exogenous features, while other approaches show inconsistent benefits. Geographic context emerges as equally important, with foundation models demonstrating advantages primarily in variable climates. These results challenge assumptions about universal foundation model superiority and highlight the need for domain-specific models, specifically in the energy domain.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05390.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05390",
    "published": "2026-02-05T07:17:21Z",
    "updated": "2026-02-05T07:17:21Z",
    "comment": "9 pages, 1 Figure and 3 Tables. Accepted to AI4TS Workshop @ AAAI'26 as an oral presentation (see https://ai4ts.github.io/aaai2026)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05389",
    "title": "A Decomposition-based State Space Model for Multivariate Time-Series Forecasting",
    "authors": [
      "Shunya Nagashima",
      "Shuntaro Suzuki",
      "Shuitsu Koyama",
      "Shinnosuke Hirano"
    ],
    "abstract": "Multivariate time series (MTS) forecasting is crucial for decision-making in domains such as weather, energy, and finance. It remains challenging because real-world sequences intertwine slow trends, multi-rate seasonalities, and irregular residuals. Existing methods often rely on rigid, hand-crafted decompositions or generic end-to-end architectures that entangle components and underuse structure shared across variables. To address these limitations, we propose DecompSSM, an end-to-end decomposition framework using three parallel deep state space model branches to capture trend, seasonal, and residual components. The model features adaptive temporal scales via an input-dependent predictor, a refinement module for shared cross-variable context, and an auxiliary loss that enforces reconstruction and orthogonality. Across standard benchmarks (ECL, Weather, ETTm2, and PEMS04), DecompSSM outperformed strong baselines, indicating the effectiveness of combining component-wise deep state space models and global context refinement.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05389.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05389",
    "published": "2026-02-05T07:17:08Z",
    "updated": "2026-02-05T07:17:08Z",
    "comment": "ICASSP2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05387",
    "title": "Parallel Swin Transformer-Enhanced 3D MRI-to-CT Synthesis for MRI-Only Radiotherapy Planning",
    "authors": [
      "Zolnamar Dorjsembe",
      "Hung-Yi Chen",
      "Furen Xiao",
      "Hsing-Kuo Pao"
    ],
    "abstract": "MRI provides superior soft tissue contrast without ionizing radiation; however, the absence of electron density information limits its direct use for dose calculation. As a result, current radiotherapy workflows rely on combined MRI and CT acquisitions, increasing registration uncertainty and procedural complexity. Synthetic CT generation enables MRI only planning but remains challenging due to nonlinear MRI-CT relationships and anatomical variability. We propose Parallel Swin Transformer-Enhanced Med2Transformer, a 3D architecture that integrates convolutional encoding with dual Swin Transformer branches to model both local anatomical detail and long-range contextual dependencies. Multi-scale shifted window attention with hierarchical feature aggregation improves anatomical fidelity. Experiments on public and clinical datasets demonstrate higher image similarity and improved geometric accuracy compared with baseline methods. Dosimetric evaluation shows clinically acceptable performance, with a mean target dose error of 1.69%. Code is available at: https://github.com/mobaidoctor/med2transformer.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05387.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05387",
    "published": "2026-02-05T07:13:54Z",
    "updated": "2026-02-05T07:13:54Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05385",
    "title": "IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models",
    "authors": [
      "Tao Liu",
      "Jiafan Lu",
      "Bohan Yu",
      "Pengcheng Wu",
      "Liu Haixin",
      "Guoyu Xu",
      "Li Xiangheng",
      "Lixiao Li",
      "Jiaming Hou",
      "Zhao Shijun",
      "Xinglin Lyu",
      "Kunli Zhang",
      "Yuxiang Jia",
      "Hongyin Zan"
    ],
    "abstract": "Text-to-SQL is a key natural language processing task that maps natural language questions to SQL queries, enabling intuitive interaction with web-based databases. Although current methods perform well on benchmarks like BIRD and Spider, they struggle with complex reasoning, domain knowledge, and hypothetical queries, and remain costly in enterprise deployment. To address these issues, we propose a framework named IESR(Information Enhanced Structured Reasoning) for lightweight large language models: (i) leverages LLMs for key information understanding and schema linking, and decoupling mathematical computation and SQL generation, (ii) integrates a multi-path reasoning mechanism based on Monte Carlo Tree Search (MCTS) with majority voting, and (iii) introduces a trajectory consistency verification module with a discriminator model to ensure accuracy and consistency. Experimental results demonstrate that IESR achieves state-of-the-art performance on the complex reasoning benchmark LogicCat (24.28 EX) and the Archer dataset (37.28 EX) using only compact lightweight models without fine-tuning. Furthermore, our analysis reveals that current coder models exhibit notable biases and deficiencies in physical knowledge, mathematical computation, and common-sense reasoning, highlighting important directions for future research. We released code at https://github.com/Ffunkytao/IESR-SLM.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05385.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05385",
    "published": "2026-02-05T07:10:45Z",
    "updated": "2026-02-05T07:10:45Z",
    "comment": "25 pages, 16 figures, 8 tables. Hongyin Zan is corresponding author, Jiafan Lu is first co-author",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05384",
    "title": "Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting",
    "authors": [
      "Hao Feng",
      "Wei Shi",
      "Ke Zhang",
      "Xiang Fei",
      "Lei Liao",
      "Dingkang Yang",
      "Yongkun Du",
      "Xuecheng Wu",
      "Jingqun Tang",
      "Yang Liu",
      "Hong Chen",
      "Can Huang"
    ],
    "abstract": "Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05384.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05384",
    "published": "2026-02-05T07:09:57Z",
    "updated": "2026-02-05T07:09:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05382",
    "title": "VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs",
    "authors": [
      "Tina Khezresmaeilzadeh",
      "Jike Zhong",
      "Konstantinos Psounis"
    ],
    "abstract": "Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05382.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05382",
    "published": "2026-02-05T07:07:27Z",
    "updated": "2026-02-05T07:07:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05381",
    "title": "Clinical Validation of Medical-based Large Language Model Chatbots on Ophthalmic Patient Queries with LLM-based Evaluation",
    "authors": [
      "Ting Fang Tan",
      "Kabilan Elangovan",
      "Andreas Pollreisz",
      "Kevin Bryan Dy",
      "Wei Yan Ng",
      "Joy Le Yi Wong",
      "Jin Liyuan",
      "Chrystie Quek Wan Ning",
      "Ashley Shuen Ying Hong",
      "Arun James Thirunavukarasu",
      "Shelley Yin-His Chang",
      "Jie Yao",
      "Dylan Hong",
      "Wang Zhaoran",
      "Amrita Gupta",
      "Daniel SW Ting"
    ],
    "abstract": "Domain specific large language models are increasingly used to support patient education, triage, and clinical decision making in ophthalmology, making rigorous evaluation essential to ensure safety and accuracy. This study evaluated four small medical LLMs Meerkat-7B, BioMistral-7B, OpenBioLLM-8B, and MedLLaMA3-v20 in answering ophthalmology related patient queries and assessed the feasibility of LLM based evaluation against clinician grading. In this cross sectional study, 180 ophthalmology patient queries were answered by each model, generating 2160 responses. Models were selected for parameter sizes under 10 billion to enable resource efficient deployment. Responses were evaluated by three ophthalmologists of differing seniority and by GPT-4-Turbo using the S.C.O.R.E. framework assessing safety, consensus and context, objectivity, reproducibility, and explainability, with ratings assigned on a five point Likert scale. Agreement between LLM and clinician grading was assessed using Spearman rank correlation, Kendall tau statistics, and kernel density estimate analyses. Meerkat-7B achieved the highest performance with mean scores of 3.44 from Senior Consultants, 4.08 from Consultants, and 4.18 from Residents. MedLLaMA3-v20 performed poorest, with 25.5 percent of responses containing hallucinations or clinically misleading content, including fabricated terminology. GPT-4-Turbo grading showed strong alignment with clinician assessments overall, with Spearman rho of 0.80 and Kendall tau of 0.67, though Senior Consultants graded more conservatively. Overall, medical LLMs demonstrated potential for safe ophthalmic question answering, but gaps remained in clinical depth and consensus, supporting the feasibility of LLM based evaluation for large scale benchmarking and the need for hybrid automated and clinician review frameworks to guide safe clinical deployment.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05381.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05381",
    "published": "2026-02-05T07:00:20Z",
    "updated": "2026-02-05T07:00:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05380",
    "title": "SAIL: Self-Amplified Iterative Learning for Diffusion Model Alignment with Minimal Human Feedback",
    "authors": [
      "Xiaoxuan He",
      "Siming Fu",
      "Wanli Li",
      "Zhiyuan Li",
      "Dacheng Yin",
      "Kang Rong",
      "Fengyun Rao",
      "Bo Zhang"
    ],
    "abstract": "Aligning diffusion models with human preferences remains challenging, particularly when reward models are unavailable or impractical to obtain, and collecting large-scale preference datasets is prohibitively expensive. \\textit{This raises a fundamental question: can we achieve effective alignment using only minimal human feedback, without auxiliary reward models, by unlocking the latent capabilities within diffusion models themselves?} In this paper, we propose \\textbf{SAIL} (\\textbf{S}elf-\\textbf{A}mplified \\textbf{I}terative \\textbf{L}earning), a novel framework that enables diffusion models to act as their own teachers through iterative self-improvement. Starting from a minimal seed set of human-annotated preference pairs, SAIL operates in a closed-loop manner where the model progressively generates diverse samples, self-annotates preferences based on its evolving understanding, and refines itself using this self-augmented dataset. To ensure robust learning and prevent catastrophic forgetting, we introduce a ranked preference mixup strategy that carefully balances exploration with adherence to initial human priors. Extensive experiments demonstrate that SAIL consistently outperforms state-of-the-art methods across multiple benchmarks while using merely 6\\% of the preference data required by existing approaches, revealing that diffusion models possess remarkable self-improvement capabilities that, when properly harnessed, can effectively replace both large-scale human annotation and external reward models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05380.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05380",
    "published": "2026-02-05T06:58:38Z",
    "updated": "2026-02-05T06:58:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05375",
    "title": "Erase at the Core: Representation Unlearning for Machine Unlearning",
    "authors": [
      "Jaewon Lee",
      "Yongwoo Kim",
      "Donghyun Kim"
    ],
    "abstract": "Many approximate machine unlearning methods demonstrate strong logit-level forgetting -- such as near-zero accuracy on the forget set -- yet continue to preserve substantial information within their internal feature representations. We refer to this discrepancy as superficial forgetting. Recent studies indicate that most existing unlearning approaches primarily alter the final classifier, leaving intermediate representations largely unchanged and highly similar to those of the original model. To address this limitation, we introduce the Erase at the Core (EC), a framework designed to enforce forgetting throughout the entire network hierarchy. EC integrates multi-layer contrastive unlearning on the forget set with retain set preservation through deeply supervised learning. Concretely, EC attaches auxiliary modules to intermediate layers and applies both contrastive unlearning and cross-entropy losses at each supervision point, with layer-wise weighted losses. Experimental results show that EC not only achieves effective logit-level forgetting, but also substantially reduces representational similarity to the original model across intermediate layers. Furthermore, EC is model-agnostic and can be incorporated as a plug-in module into existing unlearning methods, improving representation-level forgetting while maintaining performance on the retain set.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05375.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05375",
    "published": "2026-02-05T06:54:44Z",
    "updated": "2026-02-05T06:54:44Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05374",
    "title": "Cross-Lingual Empirical Evaluation of Large Language Models for Arabic Medical Tasks",
    "authors": [
      "Chaimae Abouzahir",
      "Congbo Ma",
      "Nizar Habash",
      "Farah E. Shamout"
    ],
    "abstract": "In recent years, Large Language Models (LLMs) have become widely used in medical applications, such as clinical decision support, medical education, and medical question answering. Yet, these models are often English-centric, limiting their robustness and reliability for linguistically diverse communities. Recent work has highlighted discrepancies in performance in low-resource languages for various medical tasks, but the underlying causes remain poorly understood. In this study, we conduct a cross-lingual empirical analysis of LLM performance on Arabic and English medical question and answering. Our findings reveal a persistent language-driven performance gap that intensifies with increasing task complexity. Tokenization analysis exposes structural fragmentation in Arabic medical text, while reliability analysis suggests that model-reported confidence and explanations exhibit limited correlation with correctness. Together, these findings underscore the need for language-aware design and evaluation strategies in LLMs for medical tasks.",
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05374.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05374",
    "published": "2026-02-05T06:52:46Z",
    "updated": "2026-02-05T06:52:46Z",
    "comment": "Accepted to HeaLing-EACL 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05371",
    "title": "Hinge Regression Tree: A Newton Method for Oblique Regression Tree Splitting",
    "authors": [
      "Hongyi Li",
      "Han Lin",
      "Jun Xu"
    ],
    "abstract": "Oblique decision trees combine the transparency of trees with the power of multivariate decision boundaries, but learning high-quality oblique splits is NP-hard, and practical methods still rely on slow search or theory-free heuristics. We present the Hinge Regression Tree (HRT), which reframes each split as a non-linear least-squares problem over two linear predictors whose max/min envelope induces ReLU-like expressive power. The resulting alternating fitting procedure is exactly equivalent to a damped Newton (Gauss-Newton) method within fixed partitions. We analyze this node-level optimization and, for a backtracking line-search variant, prove that the local objective decreases monotonically and converges; in practice, both fixed and adaptive damping yield fast, stable convergence and can be combined with optional ridge regularization. We further prove that HRT's model class is a universal approximator with an explicit $O(Î´^2)$ approximation rate, and show on synthetic and real-world benchmarks that it matches or outperforms single-tree baselines with more compact structures.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05371.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05371",
    "published": "2026-02-05T06:49:01Z",
    "updated": "2026-02-05T06:49:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05370",
    "title": "PACE: Defying the Scaling Hypothesis of Exploration in Iterative Alignment for Mathematical Reasoning",
    "authors": [
      "Jun Rao",
      "Zixiong Yu",
      "Xuebo Liu",
      "Guhan Chen",
      "Jing Li",
      "Jiansheng Wei",
      "Xiaojun Meng",
      "Min Zhang"
    ],
    "abstract": "Iterative Direct Preference Optimization has emerged as the state-of-the-art paradigm for aligning Large Language Models on reasoning tasks. Standard implementations (DPO-R1) rely on Best-of-N sampling (e.g., $N \\ge 8$) to mine golden trajectories from the distribution tail. In this paper, we challenge this scaling hypothesis and reveal a counter-intuitive phenomenon: in mathematical reasoning, aggressive exploration yields diminishing returns and even catastrophic policy collapse. We theoretically demonstrate that scaling $N$ amplifies verifier noise and induces detrimental distribution shifts. To resolve this, we introduce \\textbf{PACE} (Proximal Alignment via Corrective Exploration), which replaces brute-force mining with a generation-based corrective strategy. Operating with a minimal budget ($2<N<3$), PACE synthesizes high-fidelity preference pairs from failed explorations. Empirical evaluations show that PACE outperforms DPO-R1 $(N=16)$ while using only about $1/5$ of the compute, demonstrating superior robustness against reward hacking and label noise.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05370.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05370",
    "published": "2026-02-05T06:47:40Z",
    "updated": "2026-02-05T06:47:40Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05367",
    "title": "RaBiT: Residual-Aware Binarization Training for Accurate and Efficient LLMs",
    "authors": [
      "Youngcheon You",
      "Banseok Lee",
      "Minseop Choi",
      "Seonyoung Kim",
      "Hyochan Chong",
      "Changdong Kim",
      "Youngmin Kim",
      "Dongkyu Kim"
    ],
    "abstract": "Efficient deployment of large language models (LLMs) requires extreme quantization, forcing a critical trade-off between low-bit efficiency and performance. Residual binarization enables hardware-friendly, matmul-free inference by stacking binary ($\\pm$1) layers, but is plagued by pathological feature co-adaptation. We identify a key failure mode, which we term inter-path adaptation: during quantization-aware training (QAT), parallel residual binary paths learn redundant features, degrading the error-compensation structure and limiting the expressive capacity of the model. While prior work relies on heuristic workarounds (e.g., path freezing) that constrain the solution space, we propose RaBiT, a novel quantization framework that resolves co-adaptation by algorithmically enforcing a residual hierarchy. Its core mechanism sequentially derives each binary path from a single shared full-precision weight, which ensures that every path corrects the error of the preceding one. This process is stabilized by a robust initialization that prioritizes functional preservation over mere weight approximation. RaBiT redefines the 2-bit accuracy-efficiency frontier: it achieves state-of-the-art performance, rivals even hardware-intensive Vector Quantization (VQ) methods, and delivers a $4.49\\times$ inference speed-up over full-precision models on an RTX 4090.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05367.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05367",
    "published": "2026-02-05T06:41:11Z",
    "updated": "2026-02-05T06:41:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05362",
    "title": "Imagine a City: CityGenAgent for Procedural 3D City Generation",
    "authors": [
      "Zishan Liu",
      "Zecong Tang",
      "RuoCheng Wu",
      "Xinzhe Zheng",
      "Jingyu Hu",
      "Ka-Hei Hui",
      "Haoran Xie",
      "Bo Dai",
      "Zhengzhe Liu"
    ],
    "abstract": "The automated generation of interactive 3D cities is a critical challenge with broad applications in autonomous driving, virtual reality, and embodied intelligence. While recent advances in generative models and procedural techniques have improved the realism of city generation, existing methods often struggle with high-fidelity asset creation, controllability, and manipulation. In this work, we introduce CityGenAgent, a natural language-driven framework for hierarchical procedural generation of high-quality 3D cities. Our approach decomposes city generation into two interpretable components, Block Program and Building Program. To ensure structural correctness and semantic alignment, we adopt a two-stage learning strategy: (1) Supervised Fine-Tuning (SFT). We train BlockGen and BuildingGen to generate valid programs that adhere to schema constraints, including non-self-intersecting polygons and complete fields; (2) Reinforcement Learning (RL). We design Spatial Alignment Reward to enhance spatial reasoning ability and Visual Consistency Reward to bridge the gap between textual descriptions and the visual modality. Benefiting from the programs and the models' generalization, CityGenAgent supports natural language editing and manipulation. Comprehensive evaluations demonstrate superior semantic alignment, visual quality, and controllability compared to existing methods, establishing a robust foundation for scalable 3D city generation.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05362.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05362",
    "published": "2026-02-05T06:36:03Z",
    "updated": "2026-02-05T06:36:03Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05360",
    "title": "Breaking Semantic Hegemony: Decoupling Principal and Residual Subspaces for Generalized OOD Detection",
    "authors": [
      "Ningkang Peng",
      "Xiaoqian Peng",
      "Yuhao Zhang",
      "Qianfeng Yu",
      "Feng Xing",
      "Peirong Ma",
      "Xichen Yang",
      "Yi Chen",
      "Tingyu Lu",
      "Yanhui Gu"
    ],
    "abstract": "While feature-based post-hoc methods have made significant strides in Out-of-Distribution (OOD) detection, we uncover a counter-intuitive Simplicity Paradox in existing state-of-the-art (SOTA) models: these models exhibit keen sensitivity in distinguishing semantically subtle OOD samples but suffer from severe Geometric Blindness when confronting structurally distinct yet semantically simple samples or high-frequency sensor noise. We attribute this phenomenon to Semantic Hegemony within the deep feature space and reveal its mathematical essence through the lens of Neural Collapse. Theoretical analysis demonstrates that the spectral concentration bias, induced by the high variance of the principal subspace, numerically masks the structural distribution shift signals that should be significant in the residual subspace. To address this issue, we propose D-KNN, a training-free, plug-and-play geometric decoupling framework. This method utilizes orthogonal decomposition to explicitly separate semantic components from structural residuals and introduces a dual-space calibration mechanism to reactivate the model's sensitivity to weak residual signals. Extensive experiments demonstrate that D-KNN effectively breaks Semantic Hegemony, establishing new SOTA performance on both CIFAR and ImageNet benchmarks. Notably, in resolving the Simplicity Paradox, it reduces the FPR95 from 31.3% to 2.3%; when addressing sensor failures such as Gaussian noise, it boosts the detection performance (AUROC) from a baseline of 79.7% to 94.9%.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05360.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05360",
    "published": "2026-02-05T06:32:33Z",
    "updated": "2026-02-05T06:32:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05359",
    "title": "Multimodal Latent Reasoning via Hierarchical Visual Cues Injection",
    "authors": [
      "Yiming Zhang",
      "Qiangyu Yan",
      "Borui Jiang",
      "Kai Han"
    ],
    "abstract": "The advancement of multimodal large language models (MLLMs) has enabled impressive perception capabilities. However, their reasoning process often remains a \"fast thinking\" paradigm, reliant on end-to-end generation or explicit, language-centric chains of thought (CoT), which can be inefficient, verbose, and prone to hallucination. This work posits that robust reasoning should evolve within a latent space, integrating multimodal signals seamlessly. We propose multimodal latent reasoning via HIerarchical Visual cuEs injection (\\emph{HIVE}), a novel framework that instills deliberate, \"slow thinking\" without depending on superficial textual rationales. Our method recursively extends transformer blocks, creating an internal loop for iterative reasoning refinement. Crucially, it injectively grounds this process with hierarchical visual cues from global scene context to fine-grained regional details directly into the model's latent representations. This enables the model to perform grounded, multi-step inference entirely in the aligned latent space. Extensive evaluations demonstrate that test-time scaling is effective when incorporating vision knowledge, and that integrating hierarchical information significantly enhances the model's understanding of complex scenes.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05359.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05359",
    "published": "2026-02-05T06:31:12Z",
    "updated": "2026-02-05T06:31:12Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05358",
    "title": "Bayesian Neighborhood Adaptation for Graph Neural Networks",
    "authors": [
      "Paribesh Regmi",
      "Rui Li",
      "Kishan K C"
    ],
    "abstract": "The neighborhood scope (i.e., number of hops) where graph neural networks (GNNs) aggregate information to characterize a node's statistical property is critical to GNNs' performance. Two-stage approaches, training and validating GNNs for every pre-specified neighborhood scope to search for the best setting, is a time-consuming task and tends to be biased due to the search space design. How to adaptively determine proper neighborhood scopes for the aggregation process for both homophilic and heterophilic graphs remains largely unexplored. We thus propose to model the GNNs' message-passing behavior on a graph as a stochastic process by treating the number of hops as a beta process. This Bayesian framework allows us to infer the most plausible neighborhood scope for message aggregation simultaneously with the optimization of GNN parameters. Our theoretical analysis shows that the scope inference improves the expressivity of a GNN. Experiments on benchmark homophilic and heterophilic datasets show that the proposed method is compatible with state-of-the-art GNN variants, achieving competitive or superior performance on the node classification task, and providing well-calibrated predictions.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05358.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05358",
    "published": "2026-02-05T06:29:38Z",
    "updated": "2026-02-05T06:29:38Z",
    "comment": "Published in Transactions on Machine Learning Research (TMLR), 07/2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05354",
    "title": "PATHWAYS: Evaluating Investigation and Context Discovery in AI Web Agents",
    "authors": [
      "Shifat E. Arman",
      "Syed Nazmus Sakib",
      "Tapodhir Karmakar Taton",
      "Nafiul Haque",
      "Shahrear Bin Amin"
    ],
    "abstract": "We introduce PATHWAYS, a benchmark of 250 multi-step decision tasks that test whether web-based agents can discover and correctly use hidden contextual information. Across both closed and open models, agents typically navigate to relevant pages but retrieve decisive hidden evidence in only a small fraction of cases. When tasks require overturning misleading surface-level signals, performance drops sharply to near chance accuracy. Agents frequently hallucinate investigative reasoning by claiming to rely on evidence they never accessed. Even when correct context is discovered, agents often fail to integrate it into their final decision. Providing more explicit instructions improves context discovery but often reduces overall accuracy, revealing a tradeoff between procedural compliance and effective judgement. Together, these results show that current web agent architectures lack reliable mechanisms for adaptive investigation, evidence integration, and judgement override.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05354.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05354",
    "published": "2026-02-05T06:24:23Z",
    "updated": "2026-02-05T06:24:23Z",
    "comment": "35 pages, 13 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05353",
    "title": "AgentXRay: White-Boxing Agentic Systems via Workflow Reconstruction",
    "authors": [
      "Ruijie Shi",
      "Houbin Zhang",
      "Yuecheng Han",
      "Yuheng Wang",
      "Jingru Fan",
      "Runde Yang",
      "Yufan Dang",
      "Huatao Li",
      "Dewen Liu",
      "Yuan Cheng",
      "Chen Qian"
    ],
    "abstract": "Large Language Models have shown strong capabilities in complex problem solving, yet many agentic systems remain difficult to interpret and control due to opaque internal workflows. While some frameworks offer explicit architectures for collaboration, many deployed agentic systems operate as black boxes to users. We address this by introducing Agentic Workflow Reconstruction (AWR), a new task aiming to synthesize an explicit, interpretable stand-in workflow that approximates a black-box system using only input--output access. We propose AgentXRay, a search-based framework that formulates AWR as a combinatorial optimization problem over discrete agent roles and tool invocations in a chain-structured workflow space. Unlike model distillation, AgentXRay produces editable white-box workflows that match target outputs under an observable, output-based proxy metric, without accessing model parameters. To navigate the vast search space, AgentXRay employs Monte Carlo Tree Search enhanced by a scoring-based Red-Black Pruning mechanism, which dynamically integrates proxy quality with search depth. Experiments across diverse domains demonstrate that AgentXRay achieves higher proxy similarity and reduces token consumption compared to unpruned search, enabling deeper workflow exploration under fixed iteration budgets.",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05353.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05353",
    "published": "2026-02-05T06:24:15Z",
    "updated": "2026-02-05T06:24:15Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05352",
    "title": "Smoothness Errors in Dynamics Models and How to Avoid Them",
    "authors": [
      "Edward Berman",
      "Luisa Li",
      "Jung Yeon Park",
      "Robin Walters"
    ],
    "abstract": "Modern neural networks have shown promise for solving partial differential equations over surfaces, often by discretizing the surface as a mesh and learning with a mesh-aware graph neural network. However, graph neural networks suffer from oversmoothing, where a node's features become increasingly similar to those of its neighbors. Unitary graph convolutions, which are mathematically constrained to preserve smoothness, have been proposed to address this issue. Despite this, in many physical systems, such as diffusion processes, smoothness naturally increases and unitarity may be overconstraining. In this paper, we systematically study the smoothing effects of different GNNs for dynamics modeling and prove that unitary convolutions hurt performance for such tasks. We propose relaxed unitary convolutions that balance smoothness preservation with the natural smoothing required for physical systems. We also generalize unitary and relaxed unitary convolutions from graphs to meshes. In experiments on PDEs such as the heat and wave equations over complex meshes and on weather forecasting, we find that our method outperforms several strong baselines, including mesh-aware transformers and equivariant neural networks.",
    "categories": [
      "cs.LG",
      "math.SG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05352.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05352",
    "published": "2026-02-05T06:23:25Z",
    "updated": "2026-02-05T06:23:25Z",
    "comment": "Ecstatic to share relaxed unitary mesh convolutions with the community :D! Work is under review at ICML 2026. First two authors contributed equally",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05349",
    "title": "Learning with Adaptive Prototype Manifolds for Out-of-Distribution Detection",
    "authors": [
      "Ningkang Peng",
      "JiuTao Zhou",
      "Yuhao Zhang",
      "Xiaoqian Peng",
      "Qianfeng Yu",
      "Linjing Qian",
      "Tingyu Lu",
      "Yi Chen",
      "Yanhui Gu"
    ],
    "abstract": "Out-of-distribution (OOD) detection is a critical task for the safe deployment of machine learning models in the real world. Existing prototype-based representation learning methods have demonstrated exceptional performance. Specifically, we identify two fundamental flaws that universally constrain these methods: the Static Homogeneity Assumption (fixed representational resources for all classes) and the Learning-Inference Disconnect (discarding rich prototype quality knowledge at inference). These flaws fundamentally limit the model's capacity and performance. To address these issues, we propose APEX (Adaptive Prototype for eXtensive OOD Detection), a novel OOD detection framework designed via a Two-Stage Repair process to optimize the learned feature manifold. APEX introduces two key innovations to address these respective flaws: (1) an Adaptive Prototype Manifold (APM), which leverages the Minimum Description Length (MDL) principle to automatically determine the optimal prototype complexity $K_c^*$ for each class, thereby fundamentally resolving prototype collision; and (2) a Posterior-Aware OOD Scoring (PAOS) mechanism, which quantifies prototype quality (cohesion and separation) to bridge the learning-inference disconnect. Comprehensive experiments on benchmarks such as CIFAR-100 validate the superiority of our method, where APEX achieves new state-of-the-art performance.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05349.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05349",
    "published": "2026-02-05T06:21:16Z",
    "updated": "2026-02-05T06:21:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05347",
    "title": "How Do Language Models Acquire Character-Level Information?",
    "authors": [
      "Soma Sato",
      "Ryohei Sasano"
    ],
    "abstract": "Language models (LMs) have been reported to implicitly encode character-level information, despite not being explicitly provided during training. However, the mechanisms underlying this phenomenon remain largely unexplored. To reveal the mechanisms, we analyze how models acquire character-level knowledge by comparing LMs trained under controlled settings, such as specifying the pre-training dataset or tokenizer, with those trained under standard settings. We categorize the contributing factors into those independent of tokenization. Our analysis reveals that merge rules and orthographic constraints constitute primary factors arising from tokenization, whereas semantic associations of substrings and syntactic information function as key factors independent of tokenization.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05347.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05347",
    "published": "2026-02-05T06:19:51Z",
    "updated": "2026-02-05T06:19:51Z",
    "comment": "Accepted to EACL 2026 Main Conference",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05339",
    "title": "Consistency-Preserving Concept Erasure via Unsafe-Safe Pairing and Directional Fisher-weighted Adaptation",
    "authors": [
      "Yongwoo Kim",
      "Sungmin Cha",
      "Hyunsoo Kim",
      "Jaewon Lee",
      "Donghyun Kim"
    ],
    "abstract": "With the increasing versatility of text-to-image diffusion models, the ability to selectively erase undesirable concepts (e.g., harmful content) has become indispensable. However, existing concept erasure approaches primarily focus on removing unsafe concepts without providing guidance toward corresponding safe alternatives, which often leads to failure in preserving the structural and semantic consistency between the original and erased generations. In this paper, we propose a novel framework, PAIRed Erasing (PAIR), which reframes concept erasure from simple removal to consistency-preserving semantic realignment using unsafe-safe pairs. We first generate safe counterparts from unsafe inputs while preserving structural and semantic fidelity, forming paired unsafe-safe multimodal data. Leveraging these pairs, we introduce two key components: (1) Paired Semantic Realignment, a guided objective that uses unsafe-safe pairs to explicitly map target concepts to semantically aligned safe anchors; and (2) Fisher-weighted Initialization for DoRA, which initializes parameter-efficient low-rank adaptation matrices using unsafe-safe pairs, encouraging the generation of safe alternatives while selectively suppressing unsafe concepts. Together, these components enable fine-grained erasure that removes only the targeted concepts while maintaining overall semantic consistency. Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art baselines, achieving effective concept erasure while preserving structural integrity, semantic coherence, and generation quality.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05339.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05339",
    "published": "2026-02-05T06:05:24Z",
    "updated": "2026-02-05T06:05:24Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05333",
    "title": "Pool-based Active Learning as Noisy Lossy Compression: Characterizing Label Complexity via Finite Blocklength Analysis",
    "authors": [
      "Kosuke Sugiyama",
      "Masato Uchida"
    ],
    "abstract": "This paper proposes an information-theoretic framework for analyzing the theoretical limits of pool-based active learning (AL), in which a subset of instances is selectively labeled. The proposed framework reformulates pool-based AL as a noisy lossy compression problem by mapping pool observations to noisy symbol observations, data selection to compression, and learning to decoding. This correspondence enables a unified information-theoretic analysis of data selection and learning in pool-based AL. Applying finite blocklength analysis of noisy lossy compression, we derive information-theoretic lower bounds on label complexity and generalization error that serve as theoretical limits for a given learning algorithm under its associated optimal data selection strategy. Specifically, our bounds include terms that reflect overfitting induced by the learning algorithm and the discrepancy between its inductive bias and the target task, and are closely related to established information-theoretic bounds and stability theory, which have not been previously applied to the analysis of pool-based AL. These properties yield a new theoretical perspective on pool-based AL.",
    "categories": [
      "cs.LG",
      "cs.IT"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05333.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05333",
    "published": "2026-02-05T05:57:54Z",
    "updated": "2026-02-05T05:57:54Z",
    "comment": "21 pages, 1 figure",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05330",
    "title": "MTPano: Multi-Task Panoramic Scene Understanding via Label-Free Integration of Dense Prediction Priors",
    "authors": [
      "Jingdong Zhang",
      "Xiaohang Zhan",
      "Lingzhi Zhang",
      "Yizhou Wang",
      "Zhengming Yu",
      "Jionghao Wang",
      "Wenping Wang",
      "Xin Li"
    ],
    "abstract": "Comprehensive panoramic scene understanding is critical for immersive applications, yet it remains challenging due to the scarcity of high-resolution, multi-task annotations. While perspective foundation models have achieved success through data scaling, directly adapting them to the panoramic domain often fails due to severe geometric distortions and coordinate system discrepancies. Furthermore, the underlying relations between diverse dense prediction tasks in spherical spaces are underexplored. To address these challenges, we propose MTPano, a robust multi-task panoramic foundation model established by a label-free training pipeline. First, to circumvent data scarcity, we leverage powerful perspective dense priors. We project panoramic images into perspective patches to generate accurate, domain-gap-free pseudo-labels using off-the-shelf foundation models, which are then re-projected to serve as patch-wise supervision. Second, to tackle the interference between task types, we categorize tasks into rotation-invariant (e.g., depth, segmentation) and rotation-variant (e.g., surface normals) groups. We introduce the Panoramic Dual BridgeNet, which disentangles these feature streams via geometry-aware modulation layers that inject absolute position and ray direction priors. To handle the distortion from equirectangular projections (ERP), we incorporate ERP token mixers followed by a dual-branch BridgeNet for interactions with gradient truncation, facilitating beneficial cross-task information sharing while blocking conflicting gradients from incompatible task attributes. Additionally, we introduce auxiliary tasks (image gradient, point map, etc.) to fertilize the cross-task learning process. Extensive experiments demonstrate that MTPano achieves state-of-the-art performance on multiple benchmarks and delivers competitive results against task-specific panoramic specialist foundation models.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05330.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05330",
    "published": "2026-02-05T05:51:28Z",
    "updated": "2026-02-05T05:51:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05327",
    "title": "ProAct: Agentic Lookahead in Interactive Environments",
    "authors": [
      "Yangbin Yu",
      "Mingyu Yang",
      "Junyou Li",
      "Yiming Gao",
      "Feiyu Liu",
      "Yijun Yang",
      "Zichuan Lin",
      "Jiafei Lyu",
      "Yicheng Liu",
      "Zhicong Lu",
      "Deheng Ye",
      "Jie Jiang"
    ],
    "abstract": "Existing Large Language Model (LLM) agents struggle in interactive environments requiring long-horizon planning, primarily due to compounding errors when simulating future states. To address this, we propose ProAct, a framework that enables agents to internalize accurate lookahead reasoning through a two-stage training paradigm. First, we introduce Grounded LookAhead Distillation (GLAD), where the agent undergoes supervised fine-tuning on trajectories derived from environment-based search. By compressing complex search trees into concise, causal reasoning chains, the agent learns the logic of foresight without the computational overhead of inference-time search. Second, to further refine decision accuracy, we propose the Monte-Carlo Critic (MC-Critic), a plug-and-play auxiliary value estimator designed to enhance policy-gradient algorithms like PPO and GRPO. By leveraging lightweight environment rollouts to calibrate value estimates, MC-Critic provides a low-variance signal that facilitates stable policy optimization without relying on expensive model-based value approximation. Experiments on both stochastic (e.g., 2048) and deterministic (e.g., Sokoban) environments demonstrate that ProAct significantly improves planning accuracy. Notably, a 4B parameter model trained with ProAct outperforms all open-source baselines and rivals state-of-the-art closed-source models, while demonstrating robust generalization to unseen environments. The codes and models are available at https://github.com/GreatX3/ProAct",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05327.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05327",
    "published": "2026-02-05T05:45:16Z",
    "updated": "2026-02-05T05:45:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05323",
    "title": "GAS: Enhancing Reward-Cost Balance of Generative Model-assisted Offline Safe RL",
    "authors": [
      "Zifan Liu",
      "Xinran Li",
      "Shibo Chen",
      "Jun Zhang"
    ],
    "abstract": "Offline Safe Reinforcement Learning (OSRL) aims to learn a policy to achieve high performance in sequential decision-making while satisfying constraints, using only pre-collected datasets. Recent works, inspired by the strong capabilities of Generative Models (GMs), reformulate decision-making in OSRL as a conditional generative process, where GMs generate desirable actions conditioned on predefined reward and cost values. However, GM-assisted methods face two major challenges in OSRL: (1) lacking the ability to \"stitch\" optimal transitions from suboptimal trajectories within the dataset, and (2) struggling to balance reward targets with cost targets, particularly when they are conflict. To address these issues, we propose Goal-Assisted Stitching (GAS), a novel algorithm designed to enhance stitching capabilities while effectively balancing reward maximization and constraint satisfaction. To enhance the stitching ability, GAS first augments and relabels the dataset at the transition level, enabling the construction of high-quality trajectories from suboptimal ones. GAS also introduces novel goal functions, which estimate the optimal achievable reward and cost goals from the dataset. These goal functions, trained using expectile regression on the relabeled and augmented dataset, allow GAS to accommodate a broader range of reward-cost return pairs and achieve a better tradeoff between reward maximization and constraint satisfaction compared to human-specified values. The estimated goals then guide policy training, ensuring robust performance under constrained settings. Furthermore, to improve training stability and efficiency, we reshape the dataset to achieve a more uniform reward-cost return distribution. Empirical results validate the effectiveness of GAS, demonstrating superior performance in balancing reward maximization and constraint satisfaction compared to existing methods.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05323.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05323",
    "published": "2026-02-05T05:44:48Z",
    "updated": "2026-02-05T05:44:48Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05321",
    "title": "Wid3R: Wide Field-of-View 3D Reconstruction via Camera Model Conditioning",
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Adil Qureshi",
      "Somi Jeong",
      "Dinesh Manocha",
      "Suyong Yeon"
    ],
    "abstract": "We present Wid3R, a feed-forward neural network for visual geometry reconstruction that supports wide field-of-view camera models. Prior methods typically assume that input images are rectified or captured with pinhole cameras, since both their architectures and training datasets are tailored to perspective images only. These assumptions limit their applicability in real-world scenarios that use fisheye or panoramic cameras and often require careful calibration and undistortion. In contrast, Wid3R is a generalizable multi-view 3D estimation method that can model wide field-of-view camera types. Our approach leverages a ray representation with spherical harmonics and a novel camera model token within the network, enabling distortion-aware 3D reconstruction. Furthermore, Wid3R is the first multi-view foundation model to support feed-forward 3D reconstruction directly from 360 imagery. It demonstrates strong zero-shot robustness and consistently outperforms prior methods, achieving improvements of up to +77.33 on Stanford2D3D.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05321.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05321",
    "published": "2026-02-05T05:42:03Z",
    "updated": "2026-02-05T05:42:03Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05319",
    "title": "Accelerated Sequential Flow Matching: A Bayesian Filtering Perspective",
    "authors": [
      "Yinan Huang",
      "Hans Hao-Hsun Hsu",
      "Junran Wang",
      "Bo Dai",
      "Pan Li"
    ],
    "abstract": "Sequential prediction from streaming observations is a fundamental problem in stochastic dynamical systems, where inherent uncertainty often leads to multiple plausible futures. While diffusion and flow-matching models are capable of modeling complex, multi-modal trajectories, their deployment in real-time streaming environments typically relies on repeated sampling from a non-informative initial distribution, incurring substantial inference latency and potential system backlogs. In this work, we introduce Sequential Flow Matching, a principled framework grounded in Bayesian filtering. By treating streaming inference as learning a probability flow that transports the predictive distribution from one time step to the next, our approach naturally aligns with the recursive structure of Bayesian belief updates. We provide theoretical justification that initializing generation from the previous posterior offers a principled warm start that can accelerate sampling compared to naÃ¯ve re-sampling. Across a wide range of forecasting, decision-making and state estimation tasks, our method achieves performance competitive with full-step diffusion while requiring only one or very few sampling steps, therefore with faster sampling. It suggests that framing sequential inference via Bayesian filtering provides a new and principled perspective towards efficient real-time deployment of flow-based models.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05319.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05319",
    "published": "2026-02-05T05:37:14Z",
    "updated": "2026-02-05T05:37:14Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05311",
    "title": "Formal Synthesis of Certifiably Robust Neural Lyapunov-Barrier Certificates",
    "authors": [
      "Chengxiao Wang",
      "Haoze Wu",
      "Gagandeep Singh"
    ],
    "abstract": "Neural Lyapunov and barrier certificates have recently been used as powerful tools for verifying the safety and stability properties of deep reinforcement learning (RL) controllers. However, existing methods offer guarantees only under fixed ideal unperturbed dynamics, limiting their reliability in real-world applications where dynamics may deviate due to uncertainties. In this work, we study the problem of synthesizing \\emph{robust neural Lyapunov barrier certificates} that maintain their guarantees under perturbations in system dynamics. We formally define a robust Lyapunov barrier function and specify sufficient conditions based on Lipschitz continuity that ensure robustness against bounded perturbations. We propose practical training objectives that enforce these conditions via adversarial training, Lipschitz neighborhood bound, and global Lipschitz regularization. We validate our approach in two practically relevant environments, Inverted Pendulum and 2D Docking. The former is a widely studied benchmark, while the latter is a safety-critical task in autonomous systems. We show that our methods significantly improve both certified robustness bounds (up to $4.6$ times) and empirical success rates under strong perturbations (up to $2.4$ times) compared to the baseline. Our results demonstrate effectiveness of training robust neural certificates for safe RL under perturbations in dynamics.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "eess.SY"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05311.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05311",
    "published": "2026-02-05T05:08:01Z",
    "updated": "2026-02-05T05:08:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05307",
    "title": "MentorCollab: Selective Large-to-Small Inference-Time Guidance for Efficient Reasoning",
    "authors": [
      "Haojin Wang",
      "Yike Wang",
      "Shangbin Feng",
      "Hannaneh Hajishirzi",
      "Yulia Tsvetkov"
    ],
    "abstract": "Large reasoning models (LRMs) achieve strong performance by producing long chains of thought, but their inference costs are high and often generate redundant reasoning. Small language models (SLMs) are far more efficient, yet struggle on multi-step reasoning tasks. A natural idea is to let a large model guide a small one at inference time as a mentor, yet existing collaboration methods often promote imitation, resulting in verbose reasoning without consistent error correction. We propose MentorCollab, an inference-time collaboration method in which an LRM selectively and sparsely guides an SLM, rather than taking over generation. At randomly sampled token positions, we probe for divergences between the two models and use a lightweight verifier to decide whether the SLM should follow a short lookahead segment from its mentor or continue on its own. Across 15 SLM--LRM pairs and 3 domains (math reasoning, general knowledge, and commonsense reasoning), our method improves performance in 12 settings, with average gains of 3.0% and up to 8.0%, while adopting only having 18.4% tokens generated by the expensive mentor model on average. We find that short segments and selective probing are sufficient for effective collaboration. Our results show that selective inference-time guidance restores large-model reasoning ability without substantial inference overhead.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05307.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05307",
    "published": "2026-02-05T04:58:16Z",
    "updated": "2026-02-05T04:58:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05305",
    "title": "FlashBlock: Attention Caching for Efficient Long-Context Block Diffusion",
    "authors": [
      "Zhuokun Chen",
      "Jianfei Cai",
      "Bohan Zhuang"
    ],
    "abstract": "Generating long-form content, such as minute-long videos and extended texts, is increasingly important for modern generative models. Block diffusion improves inference efficiency via KV caching and block-wise causal inference and has been widely adopted in diffusion language models and video generation. However, in long-context settings, block diffusion still incurs substantial overhead from repeatedly computing attention over a growing KV cache. We identify an underexplored property of block diffusion: cross-step redundancy of attention within a block. Our analysis shows that attention outputs from tokens outside the current block remain largely stable across diffusion steps, while block-internal attention varies significantly. Based on this observation, we propose FlashBlock, a cached block-external attention mechanism that reuses stable attention output, reducing attention computation and KV cache access without modifying the diffusion process. Moreover, FlashBlock is orthogonal to sparse attention and can be combined as a complementary residual reuse strategy, substantially improving model accuracy under aggressive sparsification. Experiments on diffusion language models and video generation demonstrate up to 1.44$\\times$ higher token throughput and up to 1.6$\\times$ reduction in attention time, with negligible impact on generation quality. Project page: https://caesarhhh.github.io/FlashBlock/.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05305.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05305",
    "published": "2026-02-05T04:57:21Z",
    "updated": "2026-02-05T04:57:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05304",
    "title": "A Short and Unified Convergence Analysis of the SAG, SAGA, and IAG Algorithms",
    "authors": [
      "Feng Zhu",
      "Robert W. Heath",
      "Aritra Mitra"
    ],
    "abstract": "Stochastic variance-reduced algorithms such as Stochastic Average Gradient (SAG) and SAGA, and their deterministic counterparts like the Incremental Aggregated Gradient (IAG) method, have been extensively studied in large-scale machine learning. Despite their popularity, existing analyses for these algorithms are disparate, relying on different proof techniques tailored to each method. Furthermore, the original proof of SAG is known to be notoriously involved, requiring computer-aided analysis. Focusing on finite-sum optimization with smooth and strongly convex objective functions, our main contribution is to develop a single unified convergence analysis that applies to all three algorithms: SAG, SAGA, and IAG. Our analysis features two key steps: (i) establishing a bound on delays due to stochastic sub-sampling using simple concentration tools, and (ii) carefully designing a novel Lyapunov function that accounts for such delays. The resulting proof is short and modular, providing the first high-probability bounds for SAG and SAGA that can be seamlessly extended to non-convex objectives and Markov sampling. As an immediate byproduct of our new analysis technique, we obtain the best known rates for the IAG algorithm, significantly improving upon prior bounds.",
    "categories": [
      "cs.LG",
      "eess.SY",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05304.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05304",
    "published": "2026-02-05T04:57:20Z",
    "updated": "2026-02-05T04:57:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05302",
    "title": "PieArena: Frontier Language Agents Achieve MBA-Level Negotiation Performance and Reveal Novel Behavioral Differences",
    "authors": [
      "Chris Zhu",
      "Sasha Cui",
      "Will Sanok Dufallo",
      "Runzhi Jin",
      "Zhen Xu",
      "Linjun Zhang",
      "Daylian Cain"
    ],
    "abstract": "We present an in-depth evaluation of LLMs' ability to negotiate, a central business task that requires strategic reasoning, theory of mind, and economic value creation. To do so, we introduce PieArena, a large-scale negotiation benchmark grounded in multi-agent interactions over realistic scenarios drawn from an MBA negotiation course at an elite business school. We find systematic evidence of AGI-level performance in which a representative frontier agent (GPT-5) matches or outperforms trained business-school students, despite a semester of general negotiation instruction and targeted coaching immediately prior to the task. We further study the effects of joint-intentionality agentic scaffolding and find asymmetric gains, with large improvements for mid- and lower-tier LMs and diminishing returns for frontier LMs. Beyond deal outcomes, PieArena provides a multi-dimensional negotiation behavioral profile, revealing novel cross-model heterogeneity, masked by deal-outcome-only benchmarks, in deception, computation accuracy, instruction compliance, and perceived reputation. Overall, our results suggest that frontier language agents are already intellectually and psychologically capable of deployment in high-stakes economic settings, but deficiencies in robustness and trustworthiness remain open challenges.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05302.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05302",
    "published": "2026-02-05T04:52:20Z",
    "updated": "2026-02-05T04:52:20Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05297",
    "title": "Aspect-Aware MOOC Recommendation in a Heterogeneous Network",
    "authors": [
      "Seongyeub Chu",
      "Jongwoo Kim",
      "Mun Yong Yi"
    ],
    "abstract": "MOOC recommendation systems have received increasing attention to help learners navigate and select preferred learning content. Traditional methods such as collaborative filtering and content-based filtering suffer from data sparsity and over-specialization. To alleviate these limitations, graph-based approaches have been proposed; however, they still rely heavily on manually predefined metapaths, which often capture only superficial structural relationships and impose substantial burdens on domain experts as well as significant engineering costs. To overcome these limitations, we propose AMR (Aspect-aware MOOC Recommendation), a novel framework that models path-specific multiple aspects by embedding the semantic content of nodes within each metapath. AMR automatically discovers metapaths through bi-directional walks, derives aspect-aware path representations using a bi-LSTM-based encoder, and incorporates these representations as edge features in the learner-learner and KC-KC subgraphs to achieve fine-grained semantically informed KC recommendations. Extensive experiments on the large-scale MOOCCube and PEEK datasets show that AMR consistently outperforms state-of-the-art graph neural network baselines across key metrics such as HR@K and nDCG@K. Further analysis confirms that AMR effectively captures rich path-specific aspect information, allowing more accurate recommendations than those methods that rely solely on predefined metapaths. The code will be available upon accepted.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05297.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05297",
    "published": "2026-02-05T04:41:49Z",
    "updated": "2026-02-05T04:41:49Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05293",
    "title": "Fast-SAM3D: 3Dfy Anything in Images but Faster",
    "authors": [
      "Weilun Feng",
      "Mingqiang Wu",
      "Zhiliang Chen",
      "Chuanguang Yang",
      "Haotong Qin",
      "Yuqi Li",
      "Xiaokun Liu",
      "Guoxin Fan",
      "Zhulin An",
      "Libo Huang",
      "Yulun Zhang",
      "Michele Magno",
      "Yongjun Xu"
    ],
    "abstract": "SAM3D enables scalable, open-world 3D reconstruction from complex scenes, yet its deployment is hindered by prohibitive inference latency. In this work, we conduct the \\textbf{first systematic investigation} into its inference dynamics, revealing that generic acceleration strategies are brittle in this context. We demonstrate that these failures stem from neglecting the pipeline's inherent multi-level \\textbf{heterogeneity}: the kinematic distinctiveness between shape and layout, the intrinsic sparsity of texture refinement, and the spectral variance across geometries. To address this, we present \\textbf{Fast-SAM3D}, a training-free framework that dynamically aligns computation with instantaneous generation complexity. Our approach integrates three heterogeneity-aware mechanisms: (1) \\textit{Modality-Aware Step Caching} to decouple structural evolution from sensitive layout updates; (2) \\textit{Joint Spatiotemporal Token Carving} to concentrate refinement on high-entropy regions; and (3) \\textit{Spectral-Aware Token Aggregation} to adapt decoding resolution. Extensive experiments demonstrate that Fast-SAM3D delivers up to \\textbf{2.67$\\times$} end-to-end speedup with negligible fidelity loss, establishing a new Pareto frontier for efficient single-view 3D generation. Our code is released in https://github.com/wlfeng0509/Fast-SAM3D.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05293.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05293",
    "published": "2026-02-05T04:27:59Z",
    "updated": "2026-02-05T04:27:59Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05289",
    "title": "Towards a Science of Collective AI: LLM-based Multi-Agent Systems Need a Transition from Blind Trial-and-Error to Rigorous Science",
    "authors": [
      "Jingru Fan",
      "Dewen Liu",
      "Yufan Dang",
      "Huatao Li",
      "Yuheng Wang",
      "Wei Liu",
      "Feiyu Duan",
      "Xuanwen Ding",
      "Shu Yao",
      "Lin Wu",
      "Ruijie Shi",
      "Wai-Shing Leung",
      "Yuan Cheng",
      "Zhongyu Wei",
      "Cheng Yang",
      "Chen Qian",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "abstract": "Recent advancements in Large Language Models (LLMs) have greatly extended the capabilities of Multi-Agent Systems (MAS), demonstrating significant effectiveness across a wide range of complex and open-ended domains. However, despite this rapid progress, the field still relies heavily on empirical trial-and-error. It lacks a unified and principled scientific framework necessary for systematic optimization and improvement. This bottleneck stems from the ambiguity of attribution: first, the absence of a structured taxonomy of factors leaves researchers restricted to unguided adjustments; second, the lack of a unified metric fails to distinguish genuine collaboration gain from mere resource accumulation. In this paper, we advocate for a transition to design science through an integrated framework. We advocate to establish the collaboration gain metric ($Î$) as the scientific standard to isolate intrinsic gains from increased budgets. Leveraging $Î$, we propose a factor attribution paradigm to systematically identify collaboration-driving factors. To support this, we construct a systematic MAS factor library, structuring the design space into control-level presets and information-level dynamics. Ultimately, this framework facilitates the transition from blind experimentation to rigorous science, paving the way towards a true science of Collective AI.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05289.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05289",
    "published": "2026-02-05T04:19:52Z",
    "updated": "2026-02-05T04:19:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05287",
    "title": "Position: Universal Time Series Foundation Models Rest on a Category Error",
    "authors": [
      "Xilin Dai",
      "Wanxu Cai",
      "Zhijian Xu",
      "Qiang Xu"
    ],
    "abstract": "This position paper argues that the pursuit of \"Universal Foundation Models for Time Series\" rests on a fundamental category error, mistaking a structural Container for a semantic Modality. We contend that because time series hold incompatible generative processes (e.g., finance vs. fluid dynamics), monolithic models degenerate into expensive \"Generic Filters\" that fail to generalize under distributional drift. To address this, we introduce the \"Autoregressive Blindness Bound,\" a theoretical limit proving that history-only models cannot predict intervention-driven regime shifts. We advocate replacing universality with a Causal Control Agent paradigm, where an agent leverages external context to orchestrate a hierarchy of specialized solvers, from frozen domain experts to lightweight Just-in-Time adaptors. We conclude by calling for a shift in benchmarks from \"Zero-Shot Accuracy\" to \"Drift Adaptation Speed\" to prioritize robust, control-theoretic systems.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05287.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05287",
    "published": "2026-02-05T04:14:27Z",
    "updated": "2026-02-05T04:14:27Z",
    "comment": "Position Paper",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05286",
    "title": "HealthMamba: An Uncertainty-aware Spatiotemporal Graph State Space Model for Effective and Reliable Healthcare Facility Visit Prediction",
    "authors": [
      "Dahai Yu",
      "Lin Jiang",
      "Rongchao Xu",
      "Guang Wang"
    ],
    "abstract": "Healthcare facility visit prediction is essential for optimizing healthcare resource allocation and informing public health policy. Despite advanced machine learning methods being employed for better prediction performance, existing works usually formulate this task as a time-series forecasting problem without considering the intrinsic spatial dependencies of different types of healthcare facilities, and they also fail to provide reliable predictions under abnormal situations such as public emergencies. To advance existing research, we propose HealthMamba, an uncertainty-aware spatiotemporal framework for accurate and reliable healthcare facility visit prediction. HealthMamba comprises three key components: (i) a Unified Spatiotemporal Context Encoder that fuses heterogeneous static and dynamic information, (ii) a novel Graph State Space Model called GraphMamba for hierarchical spatiotemporal modeling, and (iii) a comprehensive uncertainty quantification module integrating three uncertainty quantification mechanisms for reliable prediction. We evaluate HealthMamba on four large-scale real-world datasets from California, New York, Texas, and Florida. Results show HealthMamba achieves around 6.0% improvement in prediction accuracy and 3.5% improvement in uncertainty quantification over state-of-the-art baselines.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05286.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05286",
    "published": "2026-02-05T04:14:27Z",
    "updated": "2026-02-05T04:14:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05285",
    "title": "Robust Inference-Time Steering of Protein Diffusion Models via Embedding Optimization",
    "authors": [
      "Minhuan Li",
      "Jiequn Han",
      "Pilar Cossio",
      "Luhuan Wu"
    ],
    "abstract": "In many biophysical inverse problems, the goal is to generate biomolecular conformations that are both physically plausible and consistent with experimental measurements. As recent sequence-to-structure diffusion models provide powerful data-driven priors, posterior sampling has emerged as a popular framework by guiding atomic coordinates to target conformations using experimental likelihoods. However, when the target lies in a low-density region of the prior, posterior sampling requires aggressive and brittle weighting of the likelihood guidance. Motivated by this limitation, we propose EmbedOpt, an alternative inference-time approach for steering diffusion models to optimize experimental likelihoods in the conditional embedding space. As this space encodes rich sequence and coevolutionary signals, optimizing over it effectively shifts the diffusion prior to align with experimental constraints. We validate EmbedOpt on two benchmarks simulating cryo-electron microscopy map fitting and experimental distance constraints. We show that EmbedOpt outperforms the coordinate-based posterior sampling method in map fitting tasks, matches performance on distance constraint tasks, and exhibits superior engineering robustness across hyperparameters spanning two orders of magnitude. Moreover, its smooth optimization behavior enables a significant reduction in the number of diffusion steps required for inference, leading to better efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05285.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05285",
    "published": "2026-02-05T04:13:33Z",
    "updated": "2026-02-05T04:13:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05281",
    "title": "Back to Basics: Revisiting Exploration in Reinforcement Learning for LLM Reasoning via Generative Probabilities",
    "authors": [
      "Pengyi Li",
      "Elizaveta Goncharova",
      "Andrey Kuznetsov",
      "Ivan Oseledets"
    ],
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an indispensable paradigm for enhancing reasoning in Large Language Models (LLMs). However, standard policy optimization methods, such as Group Relative Policy Optimization (GRPO), often converge to low-entropy policies, leading to severe mode collapse and limited output diversity. We analyze this issue from the perspective of sampling probability dynamics, identifying that the standard objective disproportionately reinforces the highest-likelihood paths, thereby suppressing valid alternative reasoning chains. To address this, we propose a novel Advantage Re-weighting Mechanism (ARM) designed to equilibrate the confidence levels across all correct responses. By incorporating Prompt Perplexity and Answer Confidence into the advantage estimation, our method dynamically reshapes the reward signal to attenuate the gradient updates of over-confident reasoning paths, while redistributing probability mass toward under-explored correct solutions. Empirical results demonstrate that our approach significantly enhances generative diversity and response entropy while maintaining competitive accuracy, effectively achieving a superior trade-off between exploration and exploitation in reasoning tasks. Empirical results on Qwen2.5 and DeepSeek models across mathematical and coding benchmarks show that ProGRPO significantly mitigates entropy collapse. Specifically, on Qwen2.5-7B, our method outperforms GRPO by 5.7% in Pass@1 and, notably, by 13.9% in Pass@32, highlighting its superior capability in generating diverse correct reasoning paths.",
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05281.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05281",
    "published": "2026-02-05T04:06:55Z",
    "updated": "2026-02-05T04:06:55Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05279",
    "title": "Hallucination-Resistant Security Planning with a Large Language Model",
    "authors": [
      "Kim Hammar",
      "Tansu Alpcan",
      "Emil Lupu"
    ],
    "abstract": "Large language models (LLMs) are promising tools for supporting security management tasks, such as incident response planning. However, their unreliability and tendency to hallucinate remain significant challenges. In this paper, we address these challenges by introducing a principled framework for using an LLM as decision support in security management. Our framework integrates the LLM in an iterative loop where it generates candidate actions that are checked for consistency with system constraints and lookahead predictions. When consistency is low, we abstain from the generated actions and instead collect external feedback, e.g., by evaluating actions in a digital twin. This feedback is then used to refine the candidate actions through in-context learning (ICL). We prove that this design allows to control the hallucination risk by tuning the consistency threshold. Moreover, we establish a bound on the regret of ICL under certain assumptions. To evaluate our framework, we apply it to an incident response use case where the goal is to generate a response and recovery plan based on system logs. Experiments on four public datasets show that our framework reduces recovery times by up to 30% compared to frontier LLMs.",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05279.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05279",
    "published": "2026-02-05T04:06:23Z",
    "updated": "2026-02-05T04:06:23Z",
    "comment": "Accepted to IEEE/IFIP Network Operations and Management Symposium 2026. To appear in the conference proceedings",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05275",
    "title": "Magic-MM-Embedding: Towards Visual-Token-Efficient Universal Multimodal Embedding with MLLMs",
    "authors": [
      "Qi Li",
      "Yanzhe Zhao",
      "Yongxin Zhou",
      "Yameng Wang",
      "Yandong Yang",
      "Yuanjia Zhou",
      "Jue Wang",
      "Zuojian Wang",
      "Jinxiang Liu"
    ],
    "abstract": "Multimodal Large Language Models (MLLMs) have shown immense promise in universal multimodal retrieval, which aims to find relevant items of various modalities for a given query. But their practical application is often hindered by the substantial computational cost incurred from processing a large number of tokens from visual inputs. In this paper, we propose Magic-MM-Embedding, a series of novel models that achieve both high efficiency and state-of-the-art performance in universal multimodal embedding. Our approach is built on two synergistic pillars: (1) a highly efficient MLLM architecture incorporating visual token compression to drastically reduce inference latency and memory footprint, and (2) a multi-stage progressive training strategy designed to not only recover but significantly boost performance. This coarse-to-fine training paradigm begins with extensive continue pretraining to restore multimodal understanding and generation capabilities, progresses to large-scale contrastive pretraining and hard negative mining to enhance discriminative power, and culminates in a task-aware fine-tuning stage guided by an MLLM-as-a-Judge for precise data curation. Comprehensive experiments show that our model outperforms existing methods by a large margin while being more inference-efficient.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05275.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05275",
    "published": "2026-02-05T04:01:01Z",
    "updated": "2026-02-05T04:01:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05271",
    "title": "Unlocking Prototype Potential: An Efficient Tuning Framework for Few-Shot Class-Incremental Learning",
    "authors": [
      "Shengqin Jiang",
      "Xiaoran Feng",
      "Yuankai Qi",
      "Haokui Zhang",
      "Renlong Hang",
      "Qingshan Liu",
      "Lina Yao",
      "Quan Z. Sheng",
      "Ming-Hsuan Yang"
    ],
    "abstract": "Few-shot class-incremental learning (FSCIL) seeks to continuously learn new classes from very limited samples while preserving previously acquired knowledge. Traditional methods often utilize a frozen pre-trained feature extractor to generate static class prototypes, which suffer from the inherent representation bias of the backbone. While recent prompt-based tuning methods attempt to adapt the backbone via minimal parameter updates, given the constraint of extreme data scarcity, the model's capacity to assimilate novel information and substantively enhance its global discriminative power is inherently limited. In this paper, we propose a novel shift in perspective: freezing the feature extractor while fine-tuning the prototypes. We argue that the primary challenge in FSCIL is not feature acquisition, but rather the optimization of decision regions within a static, high-quality feature space. To this end, we introduce an efficient prototype fine-tuning framework that evolves static centroids into dynamic, learnable components. The framework employs a dual-calibration method consisting of class-specific and task-aware offsets. These components function synergistically to improve the discriminative capacity of prototypes for ongoing incremental classes. Extensive results demonstrate that our method attains superior performance across multiple benchmarks while requiring minimal learnable parameters.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05271.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05271",
    "published": "2026-02-05T03:50:53Z",
    "updated": "2026-02-05T03:50:53Z",
    "comment": "under review",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05269",
    "title": "Hybrid Gated Flow (HGF): Stabilizing 1.58-bit LLMs via Selective Low-Rank Correction",
    "authors": [
      "David Alejandro Trejo Pizzo"
    ],
    "abstract": "The deployment of Large Language Models (LLMs) on edge devices is fundamentally constrained by the \"Memory Wall\" -- a hardware limitation where memory bandwidth, not compute, becomes the bottleneck. Recent 1.58-bit quantization techniques (e.g., BitNet b1.58) dramatically reduce memory footprint but typically incur a perplexity degradation of 20-25% compared to FP16 baselines. In this work, we introduce Hybrid Gated Flow (HGF), a dual-stream architecture that couples a 1.58-bit ternary backbone with a learnable, low-rank FP16 correction path controlled by adaptive gates.   Through extensive experiments on the TinyStories dataset across two training regimes (2500 and 3500 steps), we demonstrate that HGF 5.4 achieves a validation loss of 0.9306 compared to BitNet's 1.0294, recovering approximately 55% of the quality gap between pure ternary quantization and the FP16 baseline (0.8490). This recovery is achieved with only ~12-15% memory overhead beyond the ternary backbone.   Furthermore, we provide empirical evidence for an emergent phenomenon: quantization as structural regularization. While a full-precision differential attention baseline (Diff_Only) exhibited training instability with validation loss exceeding 1.68, the ternary-anchored HGF maintained robust convergence throughout training. Finally, we report preliminary results extending this architecture to 1.2B and 3B parameter models trained on SlimPajama and FineWeb-Edu. These larger-scale experiments confirm that the architectural stability and quality recovery observed in small-scale proxies scale linearly to production-grade language modeling regimes.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05269.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05269",
    "published": "2026-02-05T03:47:17Z",
    "updated": "2026-02-05T03:47:17Z",
    "comment": "21 pages, 4 figures, 6 tables. Code and models will be released at opencores.ai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05266",
    "title": "Beyond Cosine Similarity",
    "authors": [
      "Xinbo Ai"
    ],
    "abstract": "Cosine similarity, the standard metric for measuring semantic similarity in vector spaces, is mathematically grounded in the Cauchy-Schwarz inequality, which inherently limits it to capturing linear relationships--a constraint that fails to model the complex, nonlinear structures of real-world semantic spaces. We advance this theoretical underpinning by deriving a tighter upper bound for the dot product than the classical Cauchy-Schwarz bound. This new bound leads directly to recos, a similarity metric that normalizes the dot product by the sorted vector components. recos relaxes the condition for perfect similarity from strict linear dependence to ordinal concordance, thereby capturing a broader class of relationships. Extensive experiments across 11 embedding models--spanning static, contextualized, and universal types--demonstrate that recos consistently outperforms traditional cosine similarity, achieving higher correlation with human judgments on standard Semantic Textual Similarity (STS) benchmarks. Our work establishes recos as a mathematically principled and empirically superior alternative, offering enhanced accuracy for semantic analysis in complex embedding spaces.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05266.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05266",
    "published": "2026-02-05T03:46:21Z",
    "updated": "2026-02-05T03:46:21Z",
    "comment": "18 pages, 2 figures, 1 theorem, 3 corollaries",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05262",
    "title": "ReGLA: Efficient Receptive-Field Modeling with Gated Linear Attention Network",
    "authors": [
      "Junzhou Li",
      "Manqi Zhao",
      "Yilin Gao",
      "Zhiheng Yu",
      "Yin Li",
      "Dongsheng Jiang",
      "Li Xiao"
    ],
    "abstract": "Balancing accuracy and latency on high-resolution images is a critical challenge for lightweight models, particularly for Transformer-based architectures that often suffer from excessive latency. To address this issue, we introduce \\textbf{ReGLA}, a series of lightweight hybrid networks, which integrates efficient convolutions for local feature extraction with ReLU-based gated linear attention for global modeling. The design incorporates three key innovations: the Efficient Large Receptive Field (ELRF) module for enhancing convolutional efficiency while preserving a large receptive field; the ReLU Gated Modulated Attention (RGMA) module for maintaining linear complexity while enhancing local feature representation; and a multi-teacher distillation strategy to boost performance on downstream tasks. Extensive experiments validate the superiority of ReGLA; particularly the ReGLA-M achieves \\textbf{80.85\\%} Top-1 accuracy on ImageNet-1K at $224px$, with only \\textbf{4.98 ms} latency at $512px$. Furthermore, ReGLA outperforms similarly scaled iFormer models in downstream tasks, achieving gains of \\textbf{3.1\\%} AP on COCO object detection and \\textbf{3.6\\%} mIoU on ADE20K semantic segmentation, establishing it as a state-of-the-art solution for high-resolution visual applications.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05262.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05262",
    "published": "2026-02-05T03:43:29Z",
    "updated": "2026-02-05T03:43:29Z",
    "comment": "11 pages, 4 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05261",
    "title": "Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR",
    "authors": [
      "Fanfan Liu",
      "Youyang Yin",
      "Peng Shi",
      "Siqi Yang",
      "Zhixiong Zeng",
      "Haibo Qiu"
    ],
    "abstract": "Recent applications of Reinforcement Learning with Verifiable Rewards (RLVR) to Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated significant success in enhancing reasoning capabilities for complex tasks. During RLVR training, an increase in response length is often regarded as a key factor contributing to the growth of reasoning ability. However, the patterns of change in response length vary significantly across different RLVR algorithms during the training process. To provide a fundamental explanation for these variations, this paper conducts an in-depth analysis of the components of mainstream RLVR algorithms. We present a theoretical analysis of the factors influencing response length and validate our theory through extensive experimentation. Building upon these theoretical findings, we propose the Length-Unbiased Sequence Policy Optimization (LUSPO) algorithm. Specifically, we rectify the length bias inherent in Group Sequence Policy Optimization (GSPO), rendering its loss function unbiased with respect to response length and thereby resolving the issue of response length collapse. We conduct extensive experiments across mathematical reasoning benchmarks and multimodal reasoning scenarios, where LUSPO consistently achieves superior performance. Empirical results demonstrate that LUSPO represents a novel, state-of-the-art optimization strategy compared to existing methods such as GRPO and GSPO.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05261.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05261",
    "published": "2026-02-05T03:35:38Z",
    "updated": "2026-02-05T03:35:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05258",
    "title": "CoPE: Clipped RoPE as A Scalable Free Lunch for Long Context LLMs",
    "authors": [
      "Haoran Li",
      "Sucheng Ren",
      "Alan Yuille",
      "Feng Wang"
    ],
    "abstract": "Rotary Positional Embedding (RoPE) is a key component of context scaling in Large Language Models (LLMs). While various methods have been proposed to adapt RoPE to longer contexts, their guiding principles generally fall into two categories: (1) out-of-distribution (OOD) mitigation, which scales RoPE frequencies to accommodate unseen positions, and (2) Semantic Modeling, which posits that the attention scores computed with RoPE should always prioritize semantically similar tokens. In this work, we unify these seemingly distinct objectives through a minimalist intervention, namely CoPE: soft clipping lowfrequency components of RoPE. CoPE not only eliminates OOD outliers and refines semantic signals, but also prevents spectral leakage caused by hard clipping. Extensive experiments demonstrate that simply applying our soft clipping strategy to RoPE yields significant performance gains that scale up to 256k context length, validating our theoretical analysis and establishing CoPE as a new state-of-the-art for length generalization. Our code, data, and models are available at https://github.com/hrlics/CoPE.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05258.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05258",
    "published": "2026-02-05T03:31:14Z",
    "updated": "2026-02-05T03:31:14Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05257",
    "title": "RFM-Pose:Reinforcement-Guided Flow Matching for Fast Category-Level 6D Pose Estimation",
    "authors": [
      "Diya He",
      "Qingchen Liu",
      "Cong Zhang",
      "Jiahu Qin"
    ],
    "abstract": "Object pose estimation is a fundamental problem in computer vision and plays a critical role in virtual reality and embodied intelligence, where agents must understand and interact with objects in 3D space. Recently, score based generative models have to some extent solved the rotational symmetry ambiguity problem in category level pose estimation, but their efficiency remains limited by the high sampling cost of score-based diffusion. In this work, we propose a new framework, RFM-Pose, that accelerates category-level 6D object pose generation while actively evaluating sampled hypotheses. To improve sampling efficiency, we adopt a flow-matching generative model and generate pose candidates along an optimal transport path from a simple prior to the pose distribution. To further refine these candidates, we cast the flow-matching sampling process as a Markov decision process and apply proximal policy optimization to fine-tune the sampling policy. In particular, we interpret the flow field as a learnable policy and map an estimator to a value network, enabling joint optimization of pose generation and hypothesis scoring within a reinforcement learning framework. Experiments on the REAL275 benchmark demonstrate that RFM-Pose achieves favorable performance while significantly reducing computational cost. Moreover, similar to prior work, our approach can be readily adapted to object pose tracking and attains competitive results in this setting.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05257.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05257",
    "published": "2026-02-05T03:26:15Z",
    "updated": "2026-02-05T03:26:15Z",
    "comment": "This work has been submitted to the IEEE for possible publication",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05252",
    "title": "Copyright Detective: A Forensic System to Evidence LLMs Flickering Copyright Leakage Risks",
    "authors": [
      "Guangwei Zhang",
      "Jianing Zhu",
      "Cheng Qian",
      "Neil Gong",
      "Rada Mihalcea",
      "Zhaozhuo Xu",
      "Jingrui He",
      "Jiaqi Ma",
      "Yun Huang",
      "Chaowei Xiao",
      "Bo Li",
      "Ahmed Abbasi",
      "Dongwon Lee",
      "Heng Ji",
      "Denghui Zhang"
    ],
    "abstract": "We present Copyright Detective, the first interactive forensic system for detecting, analyzing, and visualizing potential copyright risks in LLM outputs. The system treats copyright infringement versus compliance as an evidence discovery process rather than a static classification task due to the complex nature of copyright law. It integrates multiple detection paradigms, including content recall testing, paraphrase-level similarity analysis, persuasive jailbreak probing, and unlearning verification, within a unified and extensible framework. Through interactive prompting, response collection, and iterative workflows, our system enables systematic auditing of verbatim memorization and paraphrase-level leakage, supporting responsible deployment and transparent evaluation of LLM copyright risks even with black-box access.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.05252.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05252",
    "published": "2026-02-05T03:09:52Z",
    "updated": "2026-02-05T03:09:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05251",
    "title": "TADS: Task-Aware Data Selection for Multi-Task Multimodal Pre-Training",
    "authors": [
      "Guanjie Cheng",
      "Boyi Li",
      "Lingyu Sun",
      "Mengying Zhu",
      "Yangyang Wu",
      "Xinkui Zhao",
      "Shuiguang Deng"
    ],
    "abstract": "Large-scale multimodal pre-trained models like CLIP rely heavily on high-quality training data, yet raw web-crawled datasets are often noisy, misaligned, and redundant, leading to inefficient training and suboptimal generalization. Existing data selection methods are either heuristic-based, suffering from bias and limited diversity, or data-driven but task-agnostic, failing to optimize for multi-task scenarios. To address these gaps, we introduce TADS (Task-Aware Data Selection), a novel framework for multi-task multimodal pre-training that integrates Intrinsic Quality, Task Relevance, and Distributional Diversity into a learnable value function. TADS employs a comprehensive quality assessment system with unimodal and cross-modal operators, quantifies task relevance via interpretable similarity vectors, and optimizes diversity through cluster-based weighting. A feedback-driven meta-learning mechanism adaptively refines the selection strategy based on proxy model performance across multiple downstream tasks. Experiments on CC12M demonstrate that TADS achieves superior zero-shot performance on benchmarks like ImageNet, CIFAR-100, MS-COCO, and Flickr30K, using only 36% of the data while outperforming baselines by an average of 1.0%. This highlights that TADS significantly enhances data efficiency by curating a high-utility subset that yields a much higher performance ceiling within the same computational constraints.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05251.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05251",
    "published": "2026-02-05T03:08:45Z",
    "updated": "2026-02-05T03:08:45Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05250",
    "title": "Active Label Cleaning for Reliable Detection of Electron Dense Deposits in Transmission Electron Microscopy Images",
    "authors": [
      "Jieyun Tan",
      "Shuo Liu",
      "Guibin Zhang",
      "Ziqi Li",
      "Jian Geng",
      "Lei Zhang",
      "Lei Cao"
    ],
    "abstract": "Automated detection of electron dense deposits (EDD) in glomerular disease is hindered by the scarcity of high-quality labeled data. While crowdsourcing reduces annotation cost, it introduces label noise. We propose an active label cleaning method to efficiently denoise crowdsourced datasets. Our approach uses active learning to select the most valuable noisy samples for expert re-annotation, building high-accuracy cleaning models. A Label Selection Module leverages discrepancies between crowdsourced labels and model predictions for both sample selection and instance-level noise grading. Experiments show our method achieves 67.18% AP\\textsubscript{50} on a private dataset, an 18.83% improvement over training on noisy labels. This performance reaches 95.79% of that with full expert annotation while reducing annotation cost by 73.30%. The method provides a practical, cost-effective solution for developing reliable medical AI with limited expert resources.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05250.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05250",
    "published": "2026-02-05T03:08:02Z",
    "updated": "2026-02-05T03:08:02Z",
    "comment": "10 pages, 6 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05249",
    "title": "Automatic Cognitive Task Generation for In-Situ Evaluation of Embodied Agents",
    "authors": [
      "Xinyi He",
      "Ying Yang",
      "Chuanjian Fu",
      "Sihan Guo",
      "Songchun Zhu",
      "Lifeng Fan",
      "Zhenliang Zhang",
      "Yujia Peng"
    ],
    "abstract": "As general intelligent agents are poised for widespread deployment in diverse households, evaluation tailored to each unique unseen 3D environment has become a critical prerequisite. However, existing benchmarks suffer from severe data contamination and a lack of scene specificity, inadequate for assessing agent capabilities in unseen settings. To address this, we propose a dynamic in-situ task generation method for unseen environments inspired by human cognition. We define tasks through a structured graph representation and construct a two-stage interaction-evolution task generation system for embodied agents (TEA). In the interaction stage, the agent actively interacts with the environment, creating a loop between task execution and generation that allows for continuous task generation. In the evolution stage, task graph modeling allows us to recombine and reuse existing tasks to generate new ones without external data. Experiments across 10 unseen scenes demonstrate that TEA automatically generated 87,876 tasks in two cycles, which human verification confirmed to be physically reasonable and encompassing essential daily cognitive capabilities. Benchmarking SOTA models against humans on our in-situ tasks reveals that models, despite excelling on public benchmarks, perform surprisingly poorly on basic perception tasks, severely lack 3D interaction awareness and show high sensitivity to task types in reasoning. These sobering findings highlight the necessity of in-situ evaluation before deploying agents into real-world human environments.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05249.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05249",
    "published": "2026-02-05T03:07:00Z",
    "updated": "2026-02-05T03:07:00Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05243",
    "title": "CORP: Closed-Form One-shot Representation-Preserving Structured Pruning for Vision Transformers",
    "authors": [
      "Boxiang Zhang",
      "Baijian Yang"
    ],
    "abstract": "Vision Transformers achieve strong accuracy but incur high compute and memory cost. Structured pruning can reduce inference cost, but most methods rely on retraining or multi-stage optimization. These requirements limit post-training deployment. We propose \\textbf{CORP}, a closed-form one-shot structured pruning framework for Vision Transformers. CORP removes entire MLP hidden dimensions and attention substructures without labels, gradients, or fine-tuning. It operates under strict post-training constraints using only a small unlabeled calibration set. CORP formulates structured pruning as a representation recovery problem. It models removed activations and attention logits as affine functions of retained components and derives closed-form ridge regression solutions that fold compensation into model weights. This minimizes expected representation error under the calibration distribution. Experiments on ImageNet with DeiT models show strong redundancy in MLP and attention representations. Without compensation, one-shot structured pruning causes severe accuracy degradation. With CORP, models preserve accuracy under aggressive sparsity. On DeiT-Huge, CORP retains 82.8\\% Top-1 accuracy after pruning 50\\% of both MLP and attention structures. CORP completes pruning in under 20 minutes on a single GPU and delivers substantial real-world efficiency gains.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.05243.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05243",
    "published": "2026-02-05T03:03:17Z",
    "updated": "2026-02-05T03:03:17Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05240",
    "title": "Explainable AI: A Combined XAI Framework for Explaining Brain Tumour Detection Models",
    "authors": [
      "Patrick McGonagle",
      "William Farrelly",
      "Kevin Curran"
    ],
    "abstract": "This study explores the integration of multiple Explainable AI (XAI) techniques to enhance the interpretability of deep learning models for brain tumour detection. A custom Convolutional Neural Network (CNN) was developed and trained on the BraTS 2021 dataset, achieving 91.24% accuracy in distinguishing between tumour and non-tumour regions. This research combines Gradient-weighted Class Activation Mapping (GRAD-CAM), Layer-wise Relevance Propagation (LRP) and SHapley Additive exPlanations (SHAP) to provide comprehensive insights into the model's decision-making process. This multi-technique approach successfully identified both full and partial tumours, offering layered explanations ranging from broad regions of interest to pixel-level details. GRAD-CAM highlighted important spatial regions, LRP provided detailed pixel-level relevance and SHAP quantified feature contributions. The integrated approach effectively explained model predictions, including cases with partial tumour visibility thus showing superior explanatory power compared to individual XAI methods. This research enhances transparency and trust in AI-driven medical imaging analysis by offering a more comprehensive perspective on the model's reasoning. The study demonstrates the potential of integrated XAI techniques in improving the reliability and interpretability of AI systems in healthcare, particularly for critical tasks like brain tumour detection.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.05240.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05240",
    "published": "2026-02-05T02:58:30Z",
    "updated": "2026-02-05T02:58:30Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.05238",
    "title": "PatchFlow: Leveraging a Flow-Based Model with Patch Features",
    "authors": [
      "Boxiang Zhang",
      "Baijian Yang",
      "Xiaoming Wang",
      "Corey Vian"
    ],
    "abstract": "Die casting plays a crucial role across various industries due to its ability to craft intricate shapes with high precision and smooth surfaces. However, surface defects remain a major issue that impedes die casting quality control. Recently, computer vision techniques have been explored to automate and improve defect detection. In this work, we combine local neighbor-aware patch features with a normalizing flow model and bridge the gap between the generic pretrained feature extractor and industrial product images by introducing an adapter module to increase the efficiency and accuracy of automated anomaly detection. Compared to state-of-the-art methods, our approach reduces the error rate by 20\\% on the MVTec AD dataset, achieving an image-level AUROC of 99.28\\%. Our approach has also enhanced performance on the VisA dataset , achieving an image-level AUROC of 96.48\\%. Compared to the state-of-the-art models, this represents a 28.2\\% reduction in error. Additionally, experiments on a proprietary die casting dataset yield an accuracy of 95.77\\% for anomaly detection, without requiring any anomalous samples for training. Our method illustrates the potential of leveraging computer vision and deep learning techniques to advance inspection capabilities for the die casting industry",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.05238.pdf",
    "abs_url": "https://arxiv.org/abs/2602.05238",
    "published": "2026-02-05T02:57:28Z",
    "updated": "2026-02-05T02:57:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.04145",
    "title": "Training Data Efficiency in Multimodal Process Reward Models",
    "authors": [
      "Jinyuan Li",
      "Chengsong Huang",
      "Langlin Huang",
      "Shaoyang Xu",
      "Haolin Liu",
      "Wenxuan Zhang",
      "Jiaxin Huang"
    ],
    "abstract": "Multimodal Process Reward Models (MPRMs) are central to step-level supervision for visual reasoning in MLLMs. Training MPRMs typically requires large-scale Monte Carlo (MC)-annotated corpora, incurring substantial training cost. This paper studies the data efficiency for MPRM training. Our preliminary experiments reveal that MPRM training quickly saturates under random subsampling of the training data, indicating substantial redundancy within existing MC-annotated corpora. To explain this, we formalize a theoretical framework and reveal that informative gradient updates depend on two factors: label mixtures of positive/negative steps and label reliability (average MC scores of positive steps). Guided by these insights, we propose the Balanced-Information Score (BIS), which prioritizes both mixture and reliability based on existing MC signals at the rollout level, without incurring any additional cost. Across two backbones (InternVL2.5-8B and Qwen2.5-VL-7B) on VisualProcessBench, BIS-selected subsets consistently match and even surpass the full-data performance at small fractions. Notably, the BIS subset reaches full-data performance using only 10% of the training data, improving over random subsampling by a relative 4.1%.",
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.MM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.04145.pdf",
    "abs_url": "https://arxiv.org/abs/2602.04145",
    "published": "2026-02-04T02:27:38Z",
    "updated": "2026-02-05T03:46:39Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.04142",
    "title": "JSynFlow: Japanese Synthesised Flowchart Visual Question Answering Dataset built with Large Language Models",
    "authors": [
      "Hiroshi Sasaki"
    ],
    "abstract": "Vision and language models (VLMs) are expected to analyse complex documents, such as those containing flowcharts, through a question-answering (QA) interface. The ability to recognise and interpret these flowcharts is in high demand, as they provide valuable insights unavailable in text-only explanations. However, developing VLMs with precise flowchart understanding requires large-scale datasets of flowchart images and corresponding text, the creation of which is highly time-consuming. To address this challenge, we introduce JSynFlow, a synthesised visual QA dataset for Japanese flowcharts, generated using large language models (LLMs). Our dataset comprises task descriptions for various business occupations, the corresponding flowchart images rendered from domain-specific language (DSL) code, and related QA pairs. This paper details the dataset's synthesis procedure and demonstrates that fine-tuning with JSynFlow significantly improves VLM performance on flowchart-based QA tasks. Our dataset is publicly available at https://huggingface.co/datasets/jri-advtechlab/jsynflow.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.04142.pdf",
    "abs_url": "https://arxiv.org/abs/2602.04142",
    "published": "2026-02-04T02:24:32Z",
    "updated": "2026-02-05T07:38:00Z",
    "comment": "7 pages, 1 figure",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.03951",
    "title": "Representation Geometry as a Diagnostic for Out-of-Distribution Robustness",
    "authors": [
      "Ali Zia",
      "Farid Hazratian"
    ],
    "abstract": "Robust generalization under distribution shift remains difficult to monitor and optimize in the absence of target-domain labels, as models with similar in-distribution accuracy can exhibit markedly different out-of-distribution (OOD) performance. While prior work has focused on training-time regularization and low-order representation statistics, little is known about whether the geometric structure of learned embeddings provides reliable post-hoc signals of robustness. We propose a geometry-based diagnostic framework that constructs class-conditional mutual k-nearest-neighbor graphs from in-distribution embeddings and extracts two complementary invariants: a global spectral complexity proxy based on the reduced log-determinant of the normalized Laplacian, and a local smoothness measure based on Ollivier--Ricci curvature. Across multiple architectures, training regimes, and corruption benchmarks, we find that lower spectral complexity and higher mean curvature consistently predict stronger OOD accuracy across checkpoints. Controlled perturbations and topological analyses further show that these signals reflect meaningful representation structure rather than superficial embedding statistics. Our results demonstrate that representation geometry enables interpretable, label-free robustness diagnosis and supports reliable unsupervised checkpoint selection under distribution shift.",
    "categories": [
      "cs.LG",
      "cs.CV",
      "math.DG",
      "math.GN"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.03951.pdf",
    "abs_url": "https://arxiv.org/abs/2602.03951",
    "published": "2026-02-03T19:13:36Z",
    "updated": "2026-02-05T04:54:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.02989",
    "title": "SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation",
    "authors": [
      "Zhanfeng Liao",
      "Jiajun Zhang",
      "Hanzhang Tu",
      "Zhixi Wang",
      "Yunqi Gao",
      "Hongwen Zhang",
      "Yebin Liu"
    ],
    "abstract": "Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.02989.pdf",
    "abs_url": "https://arxiv.org/abs/2602.02989",
    "published": "2026-02-03T01:50:59Z",
    "updated": "2026-02-05T15:08:33Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.02830",
    "title": "SC3D: Dynamic and Differentiable Causal Discovery for Temporal and Instantaneous Graphs",
    "authors": [
      "Sourajit Das",
      "Dibyajyoti Chakraborty",
      "Romit Maulik"
    ],
    "abstract": "Discovering causal structures from multivariate time series is a key problem because interactions span across multiple lags and possibly involve instantaneous dependencies. Additionally, the search space of the dynamic graphs is combinatorial in nature. In this study, we propose \\textit{Stable Causal Dynamic Differentiable Discovery (SC3D)}, a two-stage differentiable framework that jointly learns lag-specific adjacency matrices and, if present, an instantaneous directed acyclic graph (DAG). In Stage 1, SC3D performs edge preselection through node-wise prediction to obtain masks for lagged and instantaneous edges, whereas Stage 2 refines these masks by optimizing a likelihood with sparsity along with enforcing acyclicity on the instantaneous block. Numerical results across synthetic and benchmark dynamical systems demonstrate that SC3D achieves improved stability and more accurate recovery of both lagged and instantaneous causal structures compared to existing temporal baselines.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.02830.pdf",
    "abs_url": "https://arxiv.org/abs/2602.02830",
    "published": "2026-02-02T21:32:18Z",
    "updated": "2026-02-05T15:06:56Z",
    "comment": "8 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.02780",
    "title": "A Study of Adaptive Modeling Towards Robust Generalization",
    "authors": [
      "Zihao Jing",
      "Qiuhao Zeng",
      "Ruiyi Fang",
      "Yan Yi Li",
      "Yan Sun",
      "Boyu Wang",
      "Pingzhao Hu"
    ],
    "abstract": "Large language models (LLMs) increasingly support reasoning over biomolecular structures, but most existing approaches remain modality-specific and rely on either sequence-style encodings or fixed-length connector tokens for structural inputs. These designs can under-expose explicit geometric cues and impose rigid fusion bottlenecks, leading to over-compression and poor token allocation as structural complexity grows. We present a unified all-atom framework that grounds language reasoning in geometric information while adaptively scaling structural tokens. The method first constructs variable-size structural patches on molecular graphs using an instruction-conditioned gating policy, enabling complexity-aware allocation of query tokens. It then refines the resulting patch tokens via cross-attention with modality embeddings and injects geometry-informed tokens into the language model to improve structure grounding and reduce structural hallucinations. Across diverse all-atom benchmarks, the proposed approach yields consistent gains in heterogeneous structure-grounded reasoning. An anonymized implementation is provided in the supplementary material.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2602.02780.pdf",
    "abs_url": "https://arxiv.org/abs/2602.02780",
    "published": "2026-02-02T20:35:44Z",
    "updated": "2026-02-05T07:57:58Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.02258",
    "title": "Alignment-Aware Model Adaptation via Feedback-Guided Optimization",
    "authors": [
      "Gaurav Bhatt",
      "Aditya Chinchure",
      "Jiawei Zhou",
      "Leonid Sigal"
    ],
    "abstract": "Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.02258.pdf",
    "abs_url": "https://arxiv.org/abs/2602.02258",
    "published": "2026-02-02T16:03:16Z",
    "updated": "2026-02-05T17:05:45Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.03875",
    "title": "Reversible Deep Learning for 13C NMR in Chemoinformatics: On Structures and Spectra",
    "authors": [
      "Stefan Kuhn",
      "Vandana Dwarka",
      "Przemyslaw Karol Grenda",
      "Eero Vainikko"
    ],
    "abstract": "We introduce a reversible deep learning model for 13C NMR that uses a single conditional invertible neural network for both directions between molecular structures and spectra. The network is built from i-RevNet style bijective blocks, so the forward map and its inverse are available by construction. We train the model to predict a 128-bit binned spectrum code from a graph-based structure encoding, while the remaining latent dimensions capture residual variability. At inference time, we invert the same trained network to generate structure candidates from a spectrum code, which explicitly represents the one-to-many nature of spectrum-to-structure inference. On a filtered subset, the model is numerically invertible on trained examples, achieves spectrum-code prediction above chance, and produces coarse but meaningful structural signals when inverted on validation spectra. These results demonstrate that invertible architectures can unify spectrum prediction and uncertainty-aware candidate generation within one end-to-end model.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2602.03875.pdf",
    "abs_url": "https://arxiv.org/abs/2602.03875",
    "published": "2026-02-01T18:48:31Z",
    "updated": "2026-02-05T12:43:31Z",
    "comment": "10 pages, 4 figures, 4 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.00391",
    "title": "Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data",
    "authors": [
      "Alberto Mario Ceballos-Arroyo",
      "Shrikanth M. Yadav",
      "Chu-Hsuan Lin",
      "Jisoo Kim",
      "Geoffrey S. Young",
      "Lei Qin",
      "Huaizu Jiang"
    ],
    "abstract": "In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (adHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online at https://github.com/alceballosa/robust-vessel-segmentation",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.00391.pdf",
    "abs_url": "https://arxiv.org/abs/2602.00391",
    "published": "2026-01-30T23:10:48Z",
    "updated": "2026-02-05T15:37:06Z",
    "comment": "18 pages, 10 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.00377",
    "title": "DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models",
    "authors": [
      "Zhaochen Hong",
      "Jiaxuan You"
    ],
    "abstract": "Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: (i) cross-sequence coupling that concentrates probability mass on high-frequency prefixes, (ii) competitive decoding effects that suppress long-tail concepts, and (iii) scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse - divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 19.6-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models intended for deployment.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2602.00377.pdf",
    "abs_url": "https://arxiv.org/abs/2602.00377",
    "published": "2026-01-30T22:56:56Z",
    "updated": "2026-02-05T07:33:51Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.23177",
    "title": "MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics",
    "authors": [
      "Mikel M. Iparraguirre",
      "Iciar Alfaro",
      "David Gonzalez",
      "Elias Cueto"
    ],
    "abstract": "We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.   We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.23177.pdf",
    "abs_url": "https://arxiv.org/abs/2601.23177",
    "published": "2026-01-30T17:02:47Z",
    "updated": "2026-02-05T10:55:13Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.22693",
    "title": "PEAR: Pixel-aligned Expressive humAn mesh Recovery",
    "authors": [
      "Jiahao Wu",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Lei Zhu",
      "Jingyi Li",
      "Yu Li"
    ],
    "abstract": "Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.22693.pdf",
    "abs_url": "https://arxiv.org/abs/2601.22693",
    "published": "2026-01-30T08:12:54Z",
    "updated": "2026-02-05T06:25:40Z",
    "comment": "23 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.22401",
    "title": "Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the ErdÅs Problems",
    "authors": [
      "Tony Feng",
      "Trieu Trinh",
      "Garrett Bingham",
      "Jiwon Kang",
      "Shengtong Zhang",
      "Sang-hyun Kim",
      "Kevin Barreto",
      "Carl Schildkraut",
      "Junehyuk Jung",
      "Jaehyeon Seo",
      "Carlo Pagano",
      "Yuri Chervonyi",
      "Dawsen Hwang",
      "Kaiying Hou",
      "Sergei Gukov",
      "Cheng-Chiang Tsai",
      "Hyunwoo Choi",
      "Youngbeom Jin",
      "Wei-Yuan Li",
      "Hao-An Wu",
      "Ruey-An Shiu",
      "Yu-Sheng Shih",
      "Quoc V. Le",
      "Thang Luong"
    ],
    "abstract": "We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's ErdÅs Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the ErdÅs Problems.",
    "categories": [
      "cs.AI",
      "math.CO",
      "math.NT"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2601.22401.pdf",
    "abs_url": "https://arxiv.org/abs/2601.22401",
    "published": "2026-01-29T23:15:36Z",
    "updated": "2026-02-05T17:48:07Z",
    "comment": "Reclassify Erdos-935 as Independent Rediscovery, bringing the number of autonomous solutions down to 5. (Explanation in Addendum 4.1) Elaborate on Footnote 3. Slightly reword various phrases in the Introduction in response to feedback",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2602.00151",
    "title": "Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency",
    "authors": [
      "Alexander Blezinger",
      "Wolfgang Nejdl",
      "Ming Tang"
    ],
    "abstract": "Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2602.00151.pdf",
    "abs_url": "https://arxiv.org/abs/2602.00151",
    "published": "2026-01-29T14:06:50Z",
    "updated": "2026-02-05T09:54:33Z",
    "comment": "9 pages, 7 figures and 5 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.20916",
    "title": "Noninvasive Intracranial Pressure Estimation Using Subspace System Identification and Bespoke Machine Learning Algorithms: A Learning-to-Rank Approach",
    "authors": [
      "Anni Zhao",
      "Ayca Ermis",
      "Jeffrey Robert Vitt",
      "Sergio Brasil",
      "Wellingson Paiva",
      "Magdalena Kasprowicz",
      "Malgorzata Burzynska",
      "Robert Hamilton",
      "Runze Yan",
      "Ofer Sadan",
      "J. Claude Hemphill",
      "Lieven Vandenberghe",
      "Xiao Hu"
    ],
    "abstract": "Accurate noninvasive estimation of intracranial pressure (ICP) remains a major challenge in critical care. We developed a bespoke machine learning algorithm that integrates system identification and ranking-constrained optimization to estimate mean ICP from noninvasive signals. A machine learning framework was proposed to obtain accurate mean ICP values using arbitrary noninvasive signals. The subspace system identification algorithm is employed to identify cerebral hemodynamics models for ICP simulation using arterial blood pressure (ABP), cerebral blood velocity (CBv), and R-wave to R-wave interval (R-R interval) signals in a comprehensive database. A mapping function to describe the relationship between the features of noninvasive signals and the estimation errors is learned using innovative ranking constraints through convex optimization. Patients across multiple clinical settings were randomly split into testing and training datasets for performance evaluation of the mapping function. The results indicate that about 31.88% of testing entries achieved estimation errors within 2 mmHg and 34.07% of testing entries between 2 mmHg and 6 mmHg from the nonlinear mapping with constraints. Our results demonstrate the feasibility of the proposed noninvasive ICP estimation approach. Further validation and technical refinement are required before clinical deployment, but this work lays the foundation for safe and broadly accessible ICP monitoring in patients with acute brain injury and related conditions.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.20916.pdf",
    "abs_url": "https://arxiv.org/abs/2601.20916",
    "published": "2026-01-28T18:10:31Z",
    "updated": "2026-02-05T03:02:33Z",
    "comment": "17 pages, 9 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.17883",
    "title": "EEG Foundation Models: Progresses, Benchmarking, and Open Problems",
    "authors": [
      "Dingkun Liu",
      "Yuheng Chen",
      "Zhu Chen",
      "Zhenyao Cui",
      "Yaozhi Wen",
      "Jiayu An",
      "Jingwei Luo",
      "Dongrui Wu"
    ],
    "abstract": "Electroencephalography (EEG) foundation models have recently emerged as a promising paradigm for brain-computer interfaces (BCIs), aiming to learn transferable neural representations from large-scale heterogeneous recordings. Despite rapid progresses, there lacks fair and comprehensive comparisons of existing EEG foundation models, due to inconsistent pre-training objectives, preprocessing choices, and downstream evaluation protocols. This paper fills this gap. We first review 50 representative models and organize their design choices into a unified taxonomic framework including data standardization, model architectures, and self-supervised pre-training strategies. We then evaluate 12 open-source foundation models and competitive specialist baselines across 13 EEG datasets spanning nine BCI paradigms. Emphasizing real-world deployments, we consider both cross-subject generalization under a leave-one-subject-out protocol and rapid calibration under a within-subject few-shot setting. We further compare full-parameter fine-tuning with linear probing to assess the transferability of pre-trained representations, and examine the relationship between model scale and downstream performance. Our results indicate that: 1) linear probing is frequently insufficient; 2) specialist models trained from scratch remain competitive across many tasks; and, 3) larger foundation models do not necessarily yield better generalization performance under current data regimes and training practices.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.17883.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17883",
    "published": "2026-01-25T15:28:50Z",
    "updated": "2026-02-05T13:54:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.17667",
    "title": "Entropic Risk-Aware Monte Carlo Tree Search",
    "authors": [
      "Pedro P. Santos",
      "Jacopo Silvestrin",
      "Alberto Sardinha",
      "Francisco S. Melo"
    ],
    "abstract": "We propose a provably correct Monte Carlo tree search (MCTS) algorithm for solving risk-aware Markov decision processes (MDPs) with entropic risk measure (ERM) objectives. We provide a non-asymptotic analysis of our proposed algorithm, showing that the algorithm: (i) is correct in the sense that the empirical ERM obtained at the root node converges to the optimal ERM; and (ii) enjoys polynomial regret concentration. Our algorithm successfully exploits the dynamic programming formulations for solving risk-aware MDPs with ERM objectives introduced by previous works in the context of an upper confidence bound-based tree search algorithm. Finally, we provide a set of illustrative experiments comparing our risk-aware MCTS method against relevant baselines.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2601.17667.pdf",
    "abs_url": "https://arxiv.org/abs/2601.17667",
    "published": "2026-01-25T03:07:31Z",
    "updated": "2026-02-05T14:15:55Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.13899",
    "title": "Towards Visually Explaining Statistical Tests with Applications in Biomedical Imaging",
    "authors": [
      "Masoumeh Javanbakhat",
      "Piotr Komorowski",
      "Dilyara Bareeva",
      "Wei-Chang Lai",
      "Wojciech Samek",
      "Christoph Lippert"
    ],
    "abstract": "Deep neural two-sample tests have recently shown strong power for detecting distributional differences between groups, yet their black-box nature limits interpretability and practical adoption in biomedical analysis. Moreover, most existing post-hoc explainability methods rely on class labels, making them unsuitable for label-free statistical testing settings. We propose an explainable deep statistical testing framework that augments deep two-sample tests with sample-level and feature-level explanations, revealing which individual samples and which input features drive statistically significant group differences. Our method highlights which image regions and which individual samples contribute most to the detected group difference, providing spatial and instance-wise insight into the test's decision. Applied to biomedical imaging data, the proposed framework identifies influential samples and highlights anatomically meaningful regions associated with disease-related variation. This work bridges statistical inference and explainable AI, enabling interpretable, label-free population analysis in medical imaging.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.13899.pdf",
    "abs_url": "https://arxiv.org/abs/2601.13899",
    "published": "2026-01-20T12:27:23Z",
    "updated": "2026-02-05T15:54:09Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.12666",
    "title": "Near-Light Color Photometric Stereo for Mono-Chromatic Non-Lambertian Surfaces",
    "authors": [
      "Zonglin Li",
      "Jieji Ren",
      "Shuangfan Zhou",
      "Heng Guo",
      "Jinnuo Zhang",
      "Jiang Zhou",
      "Boxin Shi",
      "Zhanyu Ma",
      "Guoying Gu"
    ],
    "abstract": "Color photometric stereo enables single-shot surface reconstruction, extending conventional photometric stereo that requires multiple images of a static scene under varying illumination to dynamic scenarios. However, most existing approaches assume ideal distant lighting and Lambertian reflectance, leaving more practical near-light conditions and non-Lambertian surfaces underexplored. To overcome this limitation, we propose a framework that leverages neural implicit representations for depth and BRDF modeling under the assumption of mono-chromaticity (uniform chromaticity and homogeneous material), which alleviates the inherent ill-posedness of color photometric stereo and allows for detailed surface recovery from just one image. Furthermore, we design a compact optical tactile sensor to validate our approach. Experiments on both synthetic and real-world datasets demonstrate that our method achieves accurate and robust surface reconstruction.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.12666.pdf",
    "abs_url": "https://arxiv.org/abs/2601.12666",
    "published": "2026-01-19T02:26:08Z",
    "updated": "2026-02-05T04:14:20Z",
    "comment": "5 pages 7figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2601.05599",
    "title": "Quantifying and Inducing Shape Bias in CNNs via Max-Pool Dilation",
    "authors": [
      "Takito Sawada",
      "Akinori Iwata",
      "Masahiro Okuda"
    ],
    "abstract": "Convolutional Neural Networks (CNNs) exhibit a well-known texture bias, prioritizing local patterns over global shapes - a tendency inherent to their convolutional architecture. While this bias is beneficial for texture-rich natural images, it often degrades performance on shape-dominant data such as illustrations and sketches. Although prior work has proposed shape-biased models to mitigate this issue, these approaches lack a quantitative metric for identifying which datasets would actually benefit from such modifications. To address this limitation, we propose a data-driven metric that quantifies the shape-texture balance within a dataset by computing the Structural Similarity Index (SSIM) between an image's luminance (Y) channel and its L0-smoothed counterpart. Building on this metric, we introduce a computationally efficient adaptation method that promotes shape bias by modifying the dilation of max-pooling operations while keeping convolutional weights frozen. Experimental results demonstrate consistent accuracy improvements on shape-dominant datasets, particularly in low-data regimes where full fine-tuning is impractical, requiring training only the final classification layer.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2601.05599.pdf",
    "abs_url": "https://arxiv.org/abs/2601.05599",
    "published": "2026-01-09T07:36:29Z",
    "updated": "2026-02-05T16:49:18Z",
    "comment": "Accepted to IEVC 2026. 4 pages, 1 figure, 3 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.24162",
    "title": "Deep Probabilistic Supervision for Image Classification",
    "authors": [
      "Anton AdelÃ¶w",
      "Matteo Gamba",
      "Atsuto Maki"
    ],
    "abstract": "Supervised training of deep neural networks for classification typically relies on hard targets, which promote overconfidence and can limit calibration, generalization, and robustness. Self-distillation methods aim to mitigate this by leveraging inter-class and sample-specific information present in the model's own predictions, but often remain dependent on hard targets without explicitly modeling predictive uncertainty. With this in mind, we propose Deep Probabilistic Supervision (DPS), a principled learning framework constructing sample-specific target distributions via statistical inference on the model's own predictions, remaining independent of hard targets after initialization. We show that DPS consistently yields higher test accuracy (e.g., +2.0% for DenseNet-264 on ImageNet) and significantly lower Expected Calibration Error (ECE) (-40% ResNet-50, CIFAR-100) than existing self-distillation methods. When combined with a contrastive loss, DPS achieves state-of-the-art robustness under label noise.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.24162.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24162",
    "published": "2025-12-30T11:48:06Z",
    "updated": "2026-02-05T12:01:11Z",
    "comment": "16 pages, 12 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.24139",
    "title": "Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction",
    "authors": [
      "Qianyi Chen",
      "Bo Li"
    ],
    "abstract": "While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \\(1-Î±\\pm Î´\\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.",
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.24139.pdf",
    "abs_url": "https://arxiv.org/abs/2512.24139",
    "published": "2025-12-30T11:02:35Z",
    "updated": "2026-02-05T16:11:55Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.23646",
    "title": "Active Perception Agent for Omnimodal Audio-Video Understanding",
    "authors": [
      "Keda Tao",
      "Wenjie Du",
      "Bohan Yu",
      "Weiqiang Wang",
      "Jian Liu",
      "Huan Wang"
    ],
    "abstract": "Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often face challenges in fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, to our best knowledge, the first fully active perception agent that dynamically orchestrates specialized unimodal tools to achieve more fine-grained omnimodal reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, we demonstrate a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and closed-source models by substantial margins of 10% - 20% accuracy without training.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.23646.pdf",
    "abs_url": "https://arxiv.org/abs/2512.23646",
    "published": "2025-12-29T17:59:05Z",
    "updated": "2026-02-05T09:08:35Z",
    "comment": "Website:https://kd-tao.github.io/OmniAgent/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.22120",
    "title": "See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning",
    "authors": [
      "Shuoshuo Zhang",
      "Yizhen Zhang",
      "Jingjing Fu",
      "Lei Song",
      "Jiang Bian",
      "Yujiu Yang",
      "Rui Wang"
    ],
    "abstract": "Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.22120.pdf",
    "abs_url": "https://arxiv.org/abs/2512.22120",
    "published": "2025-12-26T18:59:47Z",
    "updated": "2026-02-05T08:49:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.19243",
    "title": "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis",
    "authors": [
      "Meng Chu",
      "Senqiao Yang",
      "Haoxuan Che",
      "Suiyun Zhang",
      "Xichen Zhang",
      "Shaozuo Yu",
      "Haokun Gui",
      "Zhefan Rao",
      "Dandan Tu",
      "Rui Liu",
      "Jiaya Jia"
    ],
    "abstract": "Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.19243.pdf",
    "abs_url": "https://arxiv.org/abs/2512.19243",
    "published": "2025-12-22T10:25:38Z",
    "updated": "2026-02-05T03:45:16Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.14980",
    "title": "Softly Constrained Denoisers for Diffusion Models",
    "authors": [
      "Victor M. Yeom-Song",
      "Severi Rissanen",
      "Arno Solin",
      "Samuel Kaski",
      "Mingfei Sun"
    ],
    "abstract": "Diffusion models struggle to produce samples that respect constraints, a common requirement in scientific applications. Recent approaches have introduced regularization terms in the loss or guidance methods during sampling to enforce such constraints, but they bias the generative model away from the true data distribution. This is a problem when the constraint is misspecified, which is a common issue in scientific applications where constraint formulation is challenging. We propose to integrate guidance-inspired adjustments to the denoiser, instead of the loss or sampling loop. This achieves a soft inductive bias towards constraint-compliant samples. We show that these softly constrained denoisers exploit constraint knowledge to improve compliance over standard denoisers, while maintaining enough flexibility to deviate from it in case of misspecification with observed data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.14980.pdf",
    "abs_url": "https://arxiv.org/abs/2512.14980",
    "published": "2025-12-17T00:35:45Z",
    "updated": "2026-02-05T09:09:23Z",
    "comment": "18 pages including appendix, 8 figures including appendix, preprint",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.13821",
    "title": "The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces",
    "authors": [
      "Subramanyam Sahoo"
    ],
    "abstract": "Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability - adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a theoretically grounded approach to AI control for code generation tasks, though practical deployment requires addressing the high false positive rates observed in initial evaluations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.13821.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13821",
    "published": "2025-12-15T19:05:37Z",
    "updated": "2026-02-05T16:40:19Z",
    "comment": "13 Pages, A Preprint",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.13636",
    "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning",
    "authors": [
      "Haoyu Fu",
      "Diankun Zhang",
      "Zongchuang Zhao",
      "Jianfeng Cui",
      "Hongwei Xie",
      "Bing Wang",
      "Guang Chen",
      "Dingkang Liang",
      "Xiang Bai"
    ],
    "abstract": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. Using the lightweight Qwen-0.5B LLM, MindDrive achieves Driving Score (DS) of 78.04 and Success Rate (SR) of 55.09% on the challenging Bench2Drive benchmark. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.",
    "categories": [
      "cs.CV",
      "cs.RO"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.13636.pdf",
    "abs_url": "https://arxiv.org/abs/2512.13636",
    "published": "2025-12-15T18:31:32Z",
    "updated": "2026-02-05T03:37:28Z",
    "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.12307",
    "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding",
    "authors": [
      "Benjamin Beilharz",
      "Thomas S. A. Wallis"
    ],
    "abstract": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.",
    "categories": [
      "cs.CV",
      "cs.GR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.12307.pdf",
    "abs_url": "https://arxiv.org/abs/2512.12307",
    "published": "2025-12-13T12:26:57Z",
    "updated": "2026-02-05T12:34:19Z",
    "comment": "23 pages, 11 figures. Added appendix with more figure results. Code will be available here: https://github.com/ag-perception-wallis-lab/MRD",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.17939",
    "title": "A 96pJ/Frame/Pixel and 61pJ/Event Anti-UAV System with Hybrid Object Tracking Modes",
    "authors": [
      "Yuncheng Lu",
      "Yucen Shi",
      "Aobo Li",
      "Zehao Li",
      "Junying Li",
      "Bo Wang",
      "Tony Tae-Hyoung Kim"
    ],
    "abstract": "We present an energy-efficient anti-UAV system that integrates frame-based and event-driven object tracking to enable reliable detection of small and fast-moving drones. The system reconstructs binary event frames using run-length encoding, generates region proposals, and adaptively switches between frame mode and event mode based on object size and velocity. A Fast Object Tracking Unit improves robustness for high-speed targets through adaptive thresholding and trajectory-based classification. The neural processing unit supports both grayscale-patch and trajectory inference with a custom instruction set and a zero-skipping MAC architecture, reducing redundant neural computations by more than 97 percent. Implemented in 40 nm CMOS technology, the 2 mm^2 chip achieves 96 pJ per frame per pixel and 61 pJ per event at 0.8 V, and reaches 98.2 percent recognition accuracy on public UAV datasets across 50 to 400 m ranges and 5 to 80 pixels per second speeds. The results demonstrate state-of-the-art end-to-end energy efficiency for anti-UAV systems.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2512.17939.pdf",
    "abs_url": "https://arxiv.org/abs/2512.17939",
    "published": "2025-12-12T13:53:38Z",
    "updated": "2026-02-05T15:04:39Z",
    "comment": "2 pages, 7 figures, conference paper published in IEEE Asian Solid-State Circuits Conference 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.06609",
    "title": "Vector Quantization using Gaussian Variational Autoencoder",
    "authors": [
      "Tongda Xu",
      "Wendi Zheng",
      "Jiajun He",
      "Jose Miguel Hernandez-Lobato",
      "Yan Wang",
      "Ya-Qin Zhang",
      "Jie Tang"
    ],
    "abstract": "Vector-quantized variational autoencoders (VQ-VAEs) are discrete autoencoders that compress images into discrete tokens. However, they are difficult to train due to discretization. In this paper, we propose a simple yet effective technique dubbed Gaussian Quant (GQ), which first trains a Gaussian VAE under certain constraints and then converts it into a VQ-VAE without additional training. For conversion, GQ generates random Gaussian noise as a codebook and finds the closest noise vector to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAEs for effective conversion, named the target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in the supplementary materials.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.06609.pdf",
    "abs_url": "https://arxiv.org/abs/2512.06609",
    "published": "2025-12-07T00:57:58Z",
    "updated": "2026-02-05T11:09:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2512.04949",
    "title": "CARL: Focusing Agentic Reinforcement Learning on Critical Actions",
    "authors": [
      "Leyang Shen",
      "Yang Zhang",
      "Chun Kai Ling",
      "Xiaoyan Zhao",
      "Tat-Seng Chua"
    ],
    "abstract": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for long-horizon agentic reasoning. CARL leverages entropy as a heuristic proxy for action criticality and achieves focused training by assigning rewards to high-criticality actions while excluding low-criticality actions from model updates, avoiding noisy credit assignment and redundant computation. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency across diverse evaluation settings. The source code will be publicly available.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2512.04949.pdf",
    "abs_url": "https://arxiv.org/abs/2512.04949",
    "published": "2025-12-04T16:15:46Z",
    "updated": "2026-02-05T03:39:41Z",
    "comment": "17 pages, 5 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.17059",
    "title": "REArtGS++: Generalizable Articulation Reconstruction with Temporal Geometry Constraint via Planar Gaussian Splatting",
    "authors": [
      "Di Wu",
      "Liu Liu",
      "Anran Huang",
      "Yuyan Liu",
      "Qiaojun Yu",
      "Shaofan Liu",
      "Liangtu Song",
      "Cewu Lu"
    ],
    "abstract": "Articulated objects are pervasive in daily environments, such as drawers and refrigerators. Towards their part-level surface reconstruction and joint parameter estimation, REArtGS introduces a category-agnostic approach using multi-view RGB images at two different states. However, we observe that REArtGS still struggles with screw-joint or multi-part objects and lacks geometric constraints for unseen states. In this paper, we propose REArtGS++, a novel method towards generalizable articulated object reconstruction with temporal geometry constraint and planar Gaussian splatting. We first model a decoupled screw motion for each joint without type prior, and jointly optimize part-aware Gaussians with joint parameters through part motion blending. To introduce time-continuous geometric constraint for articulated modeling, we encourage Gaussians to be planar and propose a temporally consistent regularization between planar normal and depth through Taylor first-order expansion. Extensive experiments on both synthetic and real-world articulated objects demonstrate our superiority in generalizable part-level surface reconstruction and joint parameter estimation, compared to existing approaches. Project Site: https://sites.google.com/view/reartgs2/home.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.17059.pdf",
    "abs_url": "https://arxiv.org/abs/2511.17059",
    "published": "2025-11-21T09:07:56Z",
    "updated": "2026-02-05T14:41:41Z",
    "comment": "10 pages, 7 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.11007",
    "title": "VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models",
    "authors": [
      "Xinlei Yu",
      "Chengming Xu",
      "Guibin Zhang",
      "Zhangquan Chen",
      "Yudong Zhang",
      "Yongbo He",
      "Peng-Tao Jiang",
      "Jiangning Zhang",
      "Xiaobin Hu",
      "Shuicheng Yan"
    ],
    "abstract": "Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a \"visual processing bottleneck\": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2511.11007.pdf",
    "abs_url": "https://arxiv.org/abs/2511.11007",
    "published": "2025-11-14T06:51:34Z",
    "updated": "2026-02-05T05:44:15Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.10192",
    "title": "Text2SQL-Flow: A Robust SQL-Aware Data Augmentation Framework for Text-to-SQL",
    "authors": [
      "Qifeng Cai",
      "Hao Liang",
      "Chang Xu",
      "Tao Xie",
      "Wentao Zhang",
      "Bin Cui"
    ],
    "abstract": "The data-centric paradigm has become pivotal in AI, especially for Text-to-SQL, where performance is limited by scarce, simplistic, and low-diversity datasets. To address this, we propose Text2SQL-Flow, a SQL-aware data augmentation framework that generates large-scale, semantically valid, and structurally diverse Text-to-SQL pairs from minimal seed data. It operates across six augmentation dimensions and integrates an end-to-end pipeline featuring SQL execution verification, natural language question generation, chain-of-thought reasoning traces, and data classification. A modular Database Manager ensures cross-database compatibility and scalability. Using this framework, we build SQLFlow, a high-quality dataset of 89,544 annotated examples. We evaluate SQLFlow in two settings: (1) For open-source LLMs, fine-tuning on SQLFlow consistently improves performance across benchmarks under the same data budget. (2) For closed-source LLMs, we introduce a masked alignment retrieval method that treats SQLFlow as both knowledge base and training data for the retriever. This enables structure-aware example matching by modeling fine-grained alignments between questions and SQL queries. Experiments show our retrieval strategy outperforms existing methods, underscoring the value of SQLFlow's high-fidelity data and our novel technique. Our work establishes a scalable, data-centric foundation for advancing Text-to-SQL systems and highlights the critical role of high-quality structured data in modern AI.",
    "categories": [
      "cs.CL",
      "cs.DB"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.10192.pdf",
    "abs_url": "https://arxiv.org/abs/2511.10192",
    "published": "2025-11-13T11:02:15Z",
    "updated": "2026-02-05T13:30:43Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.09736",
    "title": "Data Heterogeneity and Forgotten Labels in Split Federated Learning",
    "authors": [
      "Joana Tirana",
      "Dimitra Tsigkari",
      "David Solans Noguero",
      "Nicolas Kourtellis"
    ],
    "abstract": "In Split Federated Learning (SFL), the clients collaboratively train a model with the help of a server by splitting the model into two parts. Part-1 is trained locally at each client and aggregated by the aggregator at the end of each round. Part-2 is trained at a server that sequentially processes the intermediate activations received from each client. We study the phenomenon of catastrophic forgetting (CF) in SFL in the presence of data heterogeneity. In detail, due to the nature of SFL, local updates of part-1 may drift away from global optima, while part-2 is sensitive to the processing sequence, similar to forgetting in continual learning (CL). Specifically, we observe that the trained model performs better in classes (labels) seen at the end of the sequence. We investigate this phenomenon with emphasis on key aspects of SFL, such as the processing order at the server and the cut layer. Based on our findings, we propose Hydra, a novel mitigation method inspired by multi-head neural networks and adapted for the SFL setting. Extensive numerical evaluations show that Hydra outperforms baselines and methods from the literature.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2511.09736.pdf",
    "abs_url": "https://arxiv.org/abs/2511.09736",
    "published": "2025-11-12T20:57:49Z",
    "updated": "2026-02-05T16:23:20Z",
    "comment": "A shorter version of this paper will appear in the proceedings of AAAI 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2511.00476",
    "title": "Remembering Unequally: Global and Disciplinary Bias in LLM Reconstruction of Scholarly Coauthor Lists",
    "authors": [
      "Ghazal Kalhor",
      "Afra Mashhadi"
    ],
    "abstract": "Ongoing breakthroughs in large language models (LLMs) are reshaping scholarly search and discovery interfaces. While these systems offer new possibilities for navigating scientific knowledge, they also raise concerns about fairness and representational bias rooted in the models' memorized training data. As LLMs are increasingly used to answer queries about researchers and research communities, their ability to accurately reconstruct scholarly coauthor lists becomes an important but underexamined issue. In this study, we investigate how memorization in LLMs affects the reconstruction of coauthor lists and whether this process reflects existing inequalities across academic disciplines and world regions. We evaluate three prominent models, DeepSeek R1, Llama 4 Scout, and Mixtral 8x7B, by comparing their generated coauthor lists against bibliographic reference data. Our analysis reveals a systematic advantage for highly cited researchers, indicating that LLM memorization disproportionately favors already visible scholars. However, this pattern is not uniform: certain disciplines, such as Clinical Medicine, and some regions, including parts of Africa, exhibit more balanced reconstruction outcomes. These findings highlight both the risks and limitations of relying on LLM-generated relational knowledge in scholarly discovery contexts and emphasize the need for careful auditing of memorization-driven biases in LLM-based systems.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2511.00476.pdf",
    "abs_url": "https://arxiv.org/abs/2511.00476",
    "published": "2025-11-01T10:05:43Z",
    "updated": "2026-02-05T08:39:47Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.25502",
    "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting",
    "authors": [
      "Vladyslav Moroshan",
      "Julien Siems",
      "Arber Zela",
      "Timur Carstensen",
      "Frank Hutter"
    ],
    "abstract": "Foundation models for zero-shot time series forecasting face challenges in efficient long-horizon prediction and reproducibility, with existing synthetic-only approaches underperforming on challenging benchmarks. This paper presents TempoPFN, a univariate time series foundation model based on linear Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The model uses a GatedDeltaProduct architecture with state-weaving for fully parallelizable training across sequence lengths, eliminating the need for windowing or summarization techniques while maintaining robust temporal state-tracking. Our comprehensive synthetic data pipeline unifies diverse generators, including stochastic differential equations, Gaussian processes, and audio synthesis, with novel augmentations. In zero-shot evaluations on the Gift-Eval, fev-bench and Chronos-ZS benchmarks, TempoPFN achieves top-tier competitive performance, outperforming all existing synthetic-only approaches and surpassing the majority of models trained on real-world data, while being more efficient than existing baselines by leveraging fully parallelizable training and inference. We open-source our complete data generation pipeline and training code, providing a reproducible foundation for future research.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.25502.pdf",
    "abs_url": "https://arxiv.org/abs/2510.25502",
    "published": "2025-10-29T13:27:18Z",
    "updated": "2026-02-05T12:02:09Z",
    "comment": "38 pages, 22 figures, 17 tables",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.22936",
    "title": "PPE: Positional Preservation Embedding for Token Compression in Multimodal Large Language Models",
    "authors": [
      "Mouxiao Huang",
      "Borui Jiang",
      "Dehua Zheng",
      "Hailin Hu",
      "Kai Han",
      "Xinghao Chen"
    ],
    "abstract": "Multimodal large language models (MLLMs) have achieved strong performance on vision-language tasks, yet often suffer from inefficiencies due to redundant visual tokens. Existing token merging methods reduce sequence length but frequently disrupt spatial layouts and temporal continuity by disregarding positional relationships. In this work, we propose a novel encoding operator dubbed as \\textbf{P}ositional \\textbf{P}reservation \\textbf{E}mbedding (\\textbf{PPE}), which has the main hallmark of preservation of spatiotemporal structure during visual token compression. PPE explicitly introduces the disentangled encoding of 3D positions in the token dimension, enabling each compressed token to encapsulate different positions from multiple original tokens. Furthermore, we show that PPE can effectively support cascade clustering -- a progressive token compression strategy that leads to better performance retention. PPE is a parameter-free and generic operator that can be seamlessly integrated into existing token merging methods without any adjustments. Applied to state-of-the-art token merging framework, PPE achieves consistent improvements of $2\\%\\sim5\\%$ across multiple vision-language benchmarks, including MMBench (general vision understanding), TextVQA (layout understanding) and VideoMME (temporal understanding). These results demonstrate that preserving positional cues is critical for efficient and effective MLLM reasoning. Our code is available at https://github.com/MouxiaoHuang/PPE.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2510.22936.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22936",
    "published": "2025-10-27T02:40:02Z",
    "updated": "2026-02-05T08:25:22Z",
    "comment": "ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.22031",
    "title": "Differentiable Constraint-Based Causal Discovery",
    "authors": [
      "Jincheng Zhou",
      "Mengbo Wang",
      "Anqi He",
      "Yumeng Zhou",
      "Hessam Olya",
      "Murat Kocaoglu",
      "Bruno Ribeiro"
    ],
    "abstract": "Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code and data of the proposed method are publicly available at https://github$.$com/PurdueMINDS/DAGPA.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.22031.pdf",
    "abs_url": "https://arxiv.org/abs/2510.22031",
    "published": "2025-10-24T21:28:39Z",
    "updated": "2026-02-05T11:26:17Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.21292",
    "title": "Additive Models Explained: A Computational Complexity Approach",
    "authors": [
      "Shahaf Bassan",
      "Michal Moshkovitz",
      "Guy Katz"
    ],
    "abstract": "Generalized Additive Models (GAMs) are commonly considered *interpretable* within the ML community, as their structure makes the relationship between inputs and outputs relatively understandable. Therefore, it may seem natural to hypothesize that obtaining meaningful explanations for GAMs could be performed efficiently and would not be computationally infeasible. In this work, we challenge this hypothesis by analyzing the *computational complexity* of generating different explanations for various forms of GAMs across multiple contexts. Our analysis reveals a surprisingly diverse landscape of both positive and negative complexity outcomes. Particularly, under standard complexity assumptions such as P!=NP, we establish several key findings: (1) in stark contrast to many other common ML models, the complexity of generating explanations for GAMs is heavily influenced by the structure of the input space; (2) the complexity of explaining GAMs varies significantly with the types of component models used - but interestingly, these differences only emerge under specific input domain settings; (3) significant complexity distinctions appear for obtaining explanations in regression tasks versus classification tasks in GAMs; and (4) expressing complex models like neural networks additively (e.g., as neural additive models) can make them easier to explain, though interestingly, this benefit appears only for certain explanation methods and input domains. Collectively, these results shed light on the feasibility of computing diverse explanations for GAMs, offering a rigorous theoretical picture of the conditions under which such computations are possible or provably hard.",
    "categories": [
      "cs.LG",
      "cs.CC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.21292.pdf",
    "abs_url": "https://arxiv.org/abs/2510.21292",
    "published": "2025-10-24T09:40:30Z",
    "updated": "2026-02-05T09:58:34Z",
    "comment": "To appear in NeurIPS 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.20383",
    "title": "Hierarchical Time Series Forecasting with Robust Reconciliation",
    "authors": [
      "Shuhei Aikawa",
      "Aru Suzuki",
      "Kei Yoshitake",
      "Kanata Teshigawara",
      "Akira Iwabuchi",
      "Ken Kobayashi",
      "Kazuhide Nakata"
    ],
    "abstract": "This paper focuses on forecasting hierarchical time-series data, where each higher-level observation equals the sum of its corresponding lower-level time series. In such contexts, the forecast values should be coherent, meaning that the forecast value of each parent series exactly matches the sum of the forecast values of its child series. Existing hierarchical forecasting methods typically generate base forecasts independently for each series and then apply a reconciliation procedure to adjust them so that the resulting forecast values are coherent across the hierarchy. These methods generally derive an optimal reconciliation, using a covariance matrix of the forecast error. In practice, however, the true covariance matrix is unknown and has to be estimated from finite samples in advance. This gap between the true and estimated covariance matrix may degrade forecast performance. To address this issue, we propose a robust optimization framework for hierarchical reconciliation that accounts for uncertainty in the estimated covariance matrix. We first introduce an uncertainty set for the estimated covariance matrix and formulate a reconciliation problem that minimizes the worst-case average of weighted squared residuals over this uncertainty set. We show that our problem can be cast as a semidefinite optimization problem. Numerical experiments demonstrate that the proposed robust reconciliation method achieved better forecast performance than existing hierarchical forecasting methods, which indicates the effectiveness of integrating uncertainty into the reconciliation process.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.20383.pdf",
    "abs_url": "https://arxiv.org/abs/2510.20383",
    "published": "2025-10-23T09:30:53Z",
    "updated": "2026-02-05T08:33:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.18713",
    "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
    "authors": [
      "Joongkyu Lee",
      "Seouh-won Yi",
      "Min-hwan Oh"
    ],
    "abstract": "We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\\tilde{O}\\left( \\frac{d}{T} \\sqrt{ \\sum_{t=1}^T \\frac{1}{|S_t|}} \\right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $Î©\\left( \\frac{d}{K \\sqrt{T}} \\right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.18713.pdf",
    "abs_url": "https://arxiv.org/abs/2510.18713",
    "published": "2025-10-21T15:11:01Z",
    "updated": "2026-02-05T09:06:17Z",
    "comment": "Accepted at NeurIPS 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.17314",
    "title": "Auto-Rubric: Learning From Implicit Weights to Explicit Rubrics for Reward Modeling",
    "authors": [
      "Lipeng Xie",
      "Sen Huang",
      "Zhuo Zhang",
      "Anni Zou",
      "Yunpeng Zhai",
      "Dingchao Ren",
      "Kezun Zhang",
      "Haoyuan Hu",
      "Boyin Liu",
      "Haoran Chen",
      "Zhaoyang Liu",
      "Bolin Ding"
    ],
    "abstract": "Conventional reward modeling relies on gradient descent over neural weights, creating opaque, data-hungry \"black boxes.\" We propose a paradigm shift from implicit to explicit reward parameterization, recasting optimization from continuous weight spaces to the discrete space of natural language rubrics. We introduce a training-free framework based on iterative rubric learning: it locally induces discriminative criteria via verification-driven refinement, and globally compresses the candidate criteria pool into a compact core set by maximizing an information-theoretic coding rate objective. We organize the compressed core set into a hierarchical rubric structure -- high-level evaluation dimensions supported by concrete verification checks -- serving as an interpretable, portable reward function. Empirically, our approach challenges prevailing data scaling assumptions: using only 70 preference pairs, our rubric-guided judges outperform fully trained reward models on diverse benchmarks. For instance, Qwen3-8B equipped with our learned rubrics achieves 80.91% on RewardBench2, surpassing the specialized Skywork-Reward-V2-Qwen3-8B (78.20%). These results demonstrate that alignment signals are highly compressible and can be effectively captured through explicit symbolic search.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.17314.pdf",
    "abs_url": "https://arxiv.org/abs/2510.17314",
    "published": "2025-10-20T09:01:37Z",
    "updated": "2026-02-05T09:36:28Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.17196",
    "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
    "authors": [
      "Jiaqi Leng",
      "Xiang Hu",
      "Junxiong Wang",
      "Jianguo Li",
      "Wei Wu",
      "Yucheng Lu"
    ],
    "abstract": "Effectively processing long contexts is a critical challenge for language models. While standard Transformers are limited by quadratic complexity and poor length extrapolation, alternative architectures like sliding window attention and state space models sacrifice the ability to effectively utilize the full context due to their fixed-size memory. Chunk-based sparse attention has emerged as a promising paradigm for extreme length generalization, yet the key architectural principles underpinning its success are not yet fully understood. In this work, we present a systematic dissection of these models to identify the core components driving their performance. Through a unified framework and comprehensive ablation studies, we demonstrate that a combination of three design principles is critical: (1) an expressive, non-linear Chunk Encoder with a dedicated CLS token to produce representations for retrieval; (2) a Bypassing Residual Path to stably integrate retrieved global information without it being overridden by the local residual stream; and (3) enforced selection sparsity during pre-training to bridge the train-test distribution gap. We provide a theoretical motivation for intra-chunk information processing and landmark generation. By combining these principles, we establish a new state-of-the-art for training-free length extrapolation, successfully generalizing models trained on a 4K context to 32 million tokens on RULER and BABILong. Our findings provide a clear and empirically-grounded set of design principles for developing future, highly-capable long-context language models.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.17196.pdf",
    "abs_url": "https://arxiv.org/abs/2510.17196",
    "published": "2025-10-20T06:17:57Z",
    "updated": "2026-02-05T15:48:38Z",
    "comment": "Accepted to ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.13762",
    "title": "Progressive multi-fidelity learning with neural networks for physical system predictions",
    "authors": [
      "Paolo Conti",
      "Mengwu Guo",
      "Attilio Frangi",
      "Andrea Manzoni"
    ],
    "abstract": "Highly accurate datasets from numerical or physical experiments are often expensive and time-consuming to acquire, posing a significant challenge for applications that require precise evaluations, potentially across multiple scenarios and in real-time. Even building sufficiently accurate surrogate models can be extremely challenging with limited high-fidelity data. Conversely, less expensive, low-fidelity data can be computed more easily and encompass a broader range of scenarios. By leveraging multi-fidelity information, prediction capabilities of surrogates can be improved. However, in practical situations, data may be different in types, come from sources of different modalities, and not be concurrently available, further complicating the modeling process. To address these challenges, we introduce a progressive multi-fidelity surrogate model. This model can sequentially incorporate diverse data types using tailored encoders. Multi-fidelity regression from the encoded inputs to the target quantities of interest is then performed using neural networks. Input information progressively flows from lower to higher fidelity levels through two sets of connections: concatenations among all the encoded inputs, and additive connections among the final outputs. This dual connection system enables the model to exploit correlations among different datasets while ensuring that each level makes an additive correction to the previous level without altering it. This approach prevents performance degradation as new input data are integrated into the model and automatically adapts predictions based on the available inputs. We demonstrate the effectiveness of the approach on numerical benchmarks and a real-world case study, showing that it reliably integrates multi-modal data and provides accurate predictions, maintaining performance when generalizing across time and parameter variations.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.13762.pdf",
    "abs_url": "https://arxiv.org/abs/2510.13762",
    "published": "2025-10-15T17:10:47Z",
    "updated": "2026-02-05T13:59:56Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.09425",
    "title": "Bandits with Single-Peaked Preferences and Limited Resources",
    "authors": [
      "Omer Ben-Porat",
      "Gur Keinan",
      "Rotem Torkan"
    ],
    "abstract": "We study an online stochastic matching problem in which an algorithm sequentially matches $U$ users to $K$ arms, aiming to maximize cumulative reward over $T$ rounds under budget constraints. Without structural assumptions, computing the optimal matching is NP-hard, making online learning computationally infeasible. To overcome this barrier, we focus on single-peaked preferences -- a well-established structure in social choice theory, where users' preferences are unimodal with respect to a common order over arms. We devise an efficient algorithm for the offline budgeted matching problem, and leverage it into an efficient online algorithm with a regret of $\\tilde O(UKT^{2/3})$. Our approach relies on a novel PQ tree-based order approximation method. If the single-peaked structure is known, we develop an efficient UCB-like algorithm that achieves a regret bound of $\\tilde O(U\\sqrt{TK})$.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.09425.pdf",
    "abs_url": "https://arxiv.org/abs/2510.09425",
    "published": "2025-10-10T14:27:25Z",
    "updated": "2026-02-05T08:04:40Z",
    "comment": "Accepted to the International Conference on Learning Representations 2026 (ICLR'26)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.08859",
    "title": "Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models",
    "authors": [
      "Ragib Amin Nihal",
      "Rui Wen",
      "Kazuhiro Nakadai",
      "Jun Sakuma"
    ],
    "abstract": "Large language models (LLMs) remain vulnerable to multi-turn jailbreaking attacks that exploit conversational context to bypass safety constraints gradually. These attacks target different harm categories through distinct conversational approaches. Existing multi-turn methods often rely on heuristic or ad hoc exploration strategies, providing limited insight into underlying model weaknesses. The relationship between conversation patterns and model vulnerabilities across harm categories remains poorly understood. We propose Pattern Enhanced Chain of Attack (PE-CoA), a framework of five conversation patterns to construct multi-turn jailbreaks through natural dialogue. Evaluating PE-CoA on twelve LLMs spanning ten harm categories, we achieve state-of-the-art performance, uncovering pattern-specific vulnerabilities and LLM behavioral characteristics: models exhibit distinct weakness profiles, defense to one pattern does not generalize to others, and model families share similar failure modes. These findings highlight limitations of safety training and indicate the need for pattern-aware defenses. Code available on: https://github.com/Ragib-Amin-Nihal/PE-CoA",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.08859.pdf",
    "abs_url": "https://arxiv.org/abs/2510.08859",
    "published": "2025-10-09T23:26:28Z",
    "updated": "2026-02-05T06:10:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.08734",
    "title": "Transmuting prompts into weights",
    "authors": [
      "Hanna Mazzawi",
      "Benoit Dherin",
      "Michael Munn",
      "Michael Wunder",
      "Javier Gonzalvo"
    ],
    "abstract": "A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt's influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.08734.pdf",
    "abs_url": "https://arxiv.org/abs/2510.08734",
    "published": "2025-10-09T18:40:39Z",
    "updated": "2026-02-05T18:44:09Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.07760",
    "title": "VAO: Validation-Aligned Optimization for Cross-Task Generative Auto-Bidding",
    "authors": [
      "Yiqin Lv",
      "Zhiyu Mou",
      "Miao Xu",
      "Jinghao Chen",
      "Qi Wang",
      "Yixiu Mao",
      "Yun Qu",
      "Rongquan Bai",
      "Chuan Yu",
      "Jian Xu",
      "Bo Zheng",
      "Xiangyang Ji"
    ],
    "abstract": "Generative auto-bidding has demonstrated strong performance in online advertising, yet it often suffers from data scarcity in small-scale settings with limited advertiser participation. While cross-task data sharing is a natural remedy to mitigate this issue, naive approaches often introduce gradient bias due to distribution shifts across different tasks, and existing methods are not readily applicable to generative auto-bidding. In this paper, we propose Validation-Aligned Optimization (VAO), a principled data-sharing method that adaptively reweights cross-task data contributions based on validation performance feedback. Notably, VAO aligns training dynamics to prioritize updates that improve generalization on the target task, effectively leveraging auxiliary data and mitigating gradient bias. Building on VAO, we introduce a unified generative autobidding framework that generalizes across multiple tasks using a single model and all available task data. Extensive experiments on standard auto-bidding benchmarks validate the effectiveness of our approach.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.07760.pdf",
    "abs_url": "https://arxiv.org/abs/2510.07760",
    "published": "2025-10-09T03:59:51Z",
    "updated": "2026-02-05T14:43:23Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.06141",
    "title": "High-probability Convergence Guarantees of Decentralized SGD",
    "authors": [
      "Aleksandar Armacki",
      "Ali H. Sayed"
    ],
    "abstract": "Convergence in high-probability (HP) has attracted increasing interest, due to implying exponentially decaying tail bounds and strong guarantees for individual runs of an algorithm. While many works study HP guarantees in centralized settings, much less is understood in the decentralized setup, where existing works require strong assumptions, like uniformly bounded gradients, or asymptotically vanishing noise. This results in a significant gap between the assumptions used to establish convergence in the HP and the mean-squared error (MSE) sense, and is also contrary to centralized settings, where it is known that $\\mathtt{SGD}$ converges in HP under the same conditions on the cost function as needed for MSE convergence. Motivated by these observations, we study the HP convergence of Decentralized $\\mathtt{SGD}$ ($\\mathtt{DSGD}$) in the presence of light-tailed noise, providing several strong results. First, we show that $\\mathtt{DSGD}$ converges in HP under the same conditions on the cost as in the MSE sense, removing the restrictive assumptions used in prior works. Second, our sharp analysis yields order-optimal rates for both non-convex and strongly convex costs. Third, we establish a linear speed-up in the number of users, leading to matching, or strictly better transient times than those obtained from MSE results, further underlining the tightness of our analysis. To the best of our knowledge, this is the first work that shows $\\mathtt{DSGD}$ achieves a linear speed-up in the HP sense. Our relaxed assumptions and sharp rates stem from several technical results of independent interest, including a result on the variance-reduction effect of decentralized methods in the HP sense, as well as a novel bound on the MGF of strongly convex costs, which is of interest even in centralized settings. Finally, we provide experiments that validate our theory.",
    "categories": [
      "cs.LG",
      "cs.MA",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.06141.pdf",
    "abs_url": "https://arxiv.org/abs/2510.06141",
    "published": "2025-10-07T17:15:08Z",
    "updated": "2026-02-05T13:26:07Z",
    "comment": "47 pages, 2 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.04660",
    "title": "An Attention-based Feature Memory Design for Energy-Efficient Continual Learning",
    "authors": [
      "Yuandou Wang",
      "Filip Gunnarsson",
      "Rihan Hai"
    ],
    "abstract": "Tabular data streams are increasingly prevalent in real-time decision-making across healthcare, finance, and the Internet of Things, often generated and processed on resource-constrained edge and mobile devices. Continual learning (CL) enables models to learn sequentially from such streams while retaining previously acquired knowledge. While recent CL advances have made significant progress in mitigating catastrophic forgetting, the energy and memory efficiency of CL for tabular data streams remains largely unexplored. To address this gap, we propose AttenMLP, which integrates attention-based feature replay with context retrieval and sliding buffer updates within a minibatch training framework for streaming tabular learning.   We evaluate AttenMLP against state-of-the-art (SOTA) tabular models on real-world concept drift benchmarks with temporal distribution shifts. Experimental results show that AttenMLP achieves accuracy comparable to strong baselines without replay, while substantially reducing energy consumption through tunable design choices. In particular, with the proposed attention-based feature memory design, AttenMLP costs a 0.062 decrease in final accuracy under the incremental concept drift dataset, while reducing energy usage up to 33.3\\% compared to TabPFNv2. Under the abrupt concept drift dataset, AttenMLP reduces 1.47\\% energy consumption compared to TabR, at the cost of a 0.038 decrease in final accuracy. Although ranking third in global efficiency, AttenMLP demonstrates energy-accuracy trade-offs across both abrupt and incremental concept drift scenarios compared to SOTA tabular models.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2510.04660.pdf",
    "abs_url": "https://arxiv.org/abs/2510.04660",
    "published": "2025-10-06T10:05:44Z",
    "updated": "2026-02-05T13:13:25Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2510.03999",
    "title": "LH-Deception: Simulating and Understanding LLM Deceptive Behaviors in Long-Horizon Interactions",
    "authors": [
      "Yang Xu",
      "Xuanming Zhang",
      "Samuel Yeh",
      "Jwala Dhamala",
      "Ousmane Dia",
      "Rahul Gupta",
      "Sharon Li"
    ],
    "abstract": "Deception is a pervasive feature of human communication and an emerging concern in large language models (LLMs). While recent studies document instances of LLM deception, most evaluations remain confined to single-turn prompts and fail to capture the long-horizon interactions in which deceptive strategies typically unfold. We introduce a new simulation framework, LH-Deception, for a systematic, empirical quantification of deception in LLMs under extended sequences of interdependent tasks and dynamic contextual pressures. LH-Deception is designed as a multi-agent system: a performer agent tasked with completing tasks and a supervisor agent that evaluates progress, provides feedback, and maintains evolving states of trust. An independent deception auditor then reviews full trajectories to identify when and how deception occurs. We conduct extensive experiments across 11 frontier models, spanning both closed-source and open-source systems, and find that deception is model-dependent, increases with event pressure, and consistently erodes supervisor trust. Qualitative analyses further reveal emergent, long-horizon phenomena, such as ``chains of deception\", which are invisible to static, single-turn evaluations. Our findings provide a foundation for evaluating future LLMs in real-world, trust-sensitive contexts.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2510.03999.pdf",
    "abs_url": "https://arxiv.org/abs/2510.03999",
    "published": "2025-10-05T02:18:23Z",
    "updated": "2026-02-05T12:03:58Z",
    "comment": "ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.26000",
    "title": "Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access",
    "authors": [
      "Daniel Ebi",
      "Gaspard Lambrechts",
      "Damien Ernst",
      "Klemens BÃ¶hm"
    ],
    "abstract": "Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.26000.pdf",
    "abs_url": "https://arxiv.org/abs/2509.26000",
    "published": "2025-09-30T09:32:20Z",
    "updated": "2026-02-05T18:21:20Z",
    "comment": "11 pages, 26 pages total, 3 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.24494",
    "title": "Why Tree-Style Branching Matters for Thought Advantage Estimation in GRPO",
    "authors": [
      "Hongcheng Wang",
      "Yinuo Huang",
      "Sukai Wang",
      "Guanghui Ren",
      "Hao Dong"
    ],
    "abstract": "Group Relative Policy Optimization (GRPO) trains Chain-of-Thought reasoning with verifiable rewards, but estimating thought-level advantages without value functions often suffers from high variance. Although tree-style branching is used in practice to reduce the variance, it lacks a theoretical explanation of why it works and whether it is important or even potentially necessary. We study thought-level advantage estimation in GRPO from a variance perspective under a minimal tree-style setting where multiple answers are sampled for each thought. Using the multivariate delta method, we reveal an asymmetry in how different sampling dimensions affect variance. Increasing the number of sampled thoughts ($K$) leaves a strictly positive variance floor, whereas increasing the number of answers per thought ($M$) induces a monotonic decrease in variance, asymptotically decreasing it to zero. This implies that accurate thought-level advantage estimation is impossible through scaling thought sampling alone, making branching a potentially necessary mechanism rather than a heuristic. Experiments further provide empirical evidence for both the effectiveness and necessity of answer-level branching, demonstrating improved optimization stability, training efficiency, and final performance not only in math but also across a broad range of vision domains and under different model architectures and sizes.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.24494.pdf",
    "abs_url": "https://arxiv.org/abs/2509.24494",
    "published": "2025-09-29T09:07:45Z",
    "updated": "2026-02-05T15:54:07Z",
    "comment": "Under review",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.22650",
    "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
    "authors": [
      "Anna Kukleva",
      "Enis Simsar",
      "Alessio Tonioni",
      "Muhammad Ferjad Naeem",
      "Federico Tombari",
      "Jan Eric Lenssen",
      "Bernt Schiele"
    ],
    "abstract": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach achieves strong performance and surpasses prior methods on most datasets, establishing a new state of the art without fine-tuning, additional components and complex reasoning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.22650.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22650",
    "published": "2025-09-26T17:59:57Z",
    "updated": "2026-02-05T10:20:31Z",
    "comment": "Project Page: https://refam-diffusion.github.io/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.22352",
    "title": "SurvDiff: A Diffusion Model for Generating Synthetic Data in Survival Analysis",
    "authors": [
      "Marie Brockschmidt",
      "Maresa SchrÃ¶der",
      "Stefan Feuerriegel"
    ],
    "abstract": "Survival analysis is a cornerstone of clinical research by modeling time-to-event outcomes such as metastasis, disease relapse, or patient death. Unlike standard tabular data, survival data often come with incomplete event information due to dropout, or loss to follow-up. This poses unique challenges for synthetic data generation, where it is crucial for clinical research to faithfully reproduce both the event-time distribution and the censoring mechanism. In this paper, we propose SurvDiff an end-to-end diffusion model specifically designed for generating synthetic data in survival analysis. SurvDiff is tailored to capture the data-generating mechanism by jointly generating mixed-type covariates, event times, and right-censoring, guided by a survival-tailored loss function. The loss encodes the time-to-event structure and directly optimizes for downstream survival tasks, which ensures that SurvDiff (i) reproduces realistic event-time distributions and (ii preserves the censoring mechanism. Across multiple datasets, we show that SurvDiff consistently outperforms state-of-the-art generative baselines in both distributional fidelity and survival model evaluation metrics across multiple medical datasets. To the best of our knowledge, SurvDiff is the first end-to-end diffusion model explicitly designed for generating synthetic survival data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.22352.pdf",
    "abs_url": "https://arxiv.org/abs/2509.22352",
    "published": "2025-09-26T13:50:29Z",
    "updated": "2026-02-05T09:25:26Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.21950",
    "title": "Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach",
    "authors": [
      "Daiqing Wu",
      "Dongbao Yang",
      "Sicheng Zhao",
      "Can Ma",
      "Yu Zhou"
    ],
    "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Project page: https://github.com/wdqqdw/MVEI.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.21950.pdf",
    "abs_url": "https://arxiv.org/abs/2509.21950",
    "published": "2025-09-26T06:30:39Z",
    "updated": "2026-02-05T13:38:54Z",
    "comment": "Accepted by ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.21818",
    "title": "Sharpness-Aware Minimization Can Hallucinate Minimizers",
    "authors": [
      "Chanwoong Park",
      "Uijeong Jang",
      "Ernest K. Ryu",
      "Insoon Yang"
    ],
    "abstract": "Sharpness-Aware Minimization (SAM) is widely used to seek flatter minima -- often linked to better generalization. In its standard implementation, SAM updates the current iterate using the loss gradient evaluated at a point perturbed by distance $Ï$ along the normalized gradient direction. We show that, for some choices of $Ï$, SAM can stall at points where this shifted (perturbed-point) gradient vanishes despite a nonzero original gradient, and therefore, they are not stationary points of the original loss. We call these points hallucinated minimizers, prove their existence under simple nonconvex landscape conditions (e.g., the presence of a local minimizer and a local maximizer), and establish sufficient conditions for local convergence of the SAM iterates to them. We corroborate this failure mode in neural network training and observe that it aligns with SAM's performance degradation often seen at large $Ï$. Finally, as a practical safeguard, we find that a short initial SGD warm-start before enabling SAM mitigates this failure mode and reduces sensitivity to the choice of $Ï$.",
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.21818.pdf",
    "abs_url": "https://arxiv.org/abs/2509.21818",
    "published": "2025-09-26T03:26:07Z",
    "updated": "2026-02-05T13:11:45Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.21004",
    "title": "Multi-Agent Inverted Transformer for Flight Trajectory Prediction",
    "authors": [
      "Seokbin Yoon",
      "Keumjin Lee"
    ],
    "abstract": "Flight trajectory prediction for multiple aircraft is essential and provides critical insights into how aircraft navigate within current air traffic flows. However, predicting multi-agent flight trajectories is inherently challenging. One of the major difficulties is modeling both the individual aircraft behaviors over time and the complex interactions between flights. Generating explainable prediction outcomes is also a challenge. Therefore, we propose a Multi-Agent Inverted Transformer, MAIFormer, as a novel neural architecture that predicts multi-agent flight trajectories. The proposed framework features two key attention modules: (i) masked multivariate attention, which captures spatio-temporal patterns of individual aircraft, and (ii) agent attention, which models the social patterns among multiple agents in complex air traffic scenes. We evaluated MAIFormer using a real-world automatic dependent surveillance-broadcast flight trajectory dataset from the terminal airspace of Incheon International Airport in South Korea. The experimental results show that MAIFormer achieves the best performance across multiple metrics and outperforms other methods. In addition, MAIFormer produces prediction outcomes that are interpretable from a human perspective, which improves both the transparency of the model and its practical utility in air traffic control.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.21004.pdf",
    "abs_url": "https://arxiv.org/abs/2509.21004",
    "published": "2025-09-25T10:59:29Z",
    "updated": "2026-02-05T16:47:36Z",
    "comment": "11 pages, 8 figures, submitted for IEEE Transactions on Intelligent Transportation System",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.20900",
    "title": "Learning to Summarize by Learning to Quiz: Adversarial Agentic Collaboration for Long Document Summarization",
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "abstract": "Long document summarization remains a significant challenge for current large language models (LLMs), as existing approaches commonly struggle with information loss, factual inconsistencies, and coherence issues when processing excessively long documents. We propose SummQ, a novel adversarial multi-agent framework that addresses these limitations through collaborative intelligence between specialized agents operating in two complementary domains: summarization and quizzing. Our approach employs summary generators and reviewers that work collaboratively to create and evaluate comprehensive summaries, while quiz generators and reviewers create comprehension questions that serve as continuous quality checks for the summarization process. This adversarial dynamic, enhanced by an examinee agent that validates whether the generated summary contains the information needed to answer the quiz questions, enables iterative refinement through multifaceted feedback mechanisms. We evaluate SummQ on three widely used long document summarization benchmarks. Experimental results demonstrate that our framework significantly outperforms existing state-of-the-art methods across ROUGE and BERTScore metrics, as well as in LLM-as-a-Judge and human evaluations. Our comprehensive analyses reveal the effectiveness of the multi-agent collaboration dynamics, the influence of different agent configurations, and the impact of the quizzing mechanism. This work establishes a new approach for long document summarization that uses adversarial agentic collaboration to improve summarization quality.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2509.20900.pdf",
    "abs_url": "https://arxiv.org/abs/2509.20900",
    "published": "2025-09-25T08:36:19Z",
    "updated": "2026-02-05T07:34:02Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.15602",
    "title": "TennisTV: Do Multimodal Large Language Models Understand Tennis Rallies?",
    "authors": [
      "Zhongyuan Bao",
      "Lejun Zhang"
    ],
    "abstract": "Multimodal large language models (MLLMs) excel at general video understanding but struggle with fast, high-frequency sports like tennis, where rally clips are short yet information-dense. To systematically evaluate MLLMs in this challenging domain, we present TennisTV, the first and most comprehensive benchmark for tennis video understanding. TennisTV models each rally as a temporal-ordered sequence of consecutive stroke events, using automated pipelines for filtering and question generation. It covers 8 tasks from the stroke level to the rally level and includes 2527 human-verified questions. Evaluating 17 representative MLLMs, we provide the first systematic assessment of tennis video understanding. Results yield two key insights: (i) frame-sampling density should be tailored and balanced across tasks, and (ii) improving temporal grounding is essential for stronger reasoning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.15602.pdf",
    "abs_url": "https://arxiv.org/abs/2509.15602",
    "published": "2025-09-19T05:08:05Z",
    "updated": "2026-02-05T09:55:18Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.11298",
    "title": "When Are Two RLHF Objectives the Same?",
    "authors": [
      "Madhava Gaikwad"
    ],
    "abstract": "The preference optimization literature contains many proposed objectives, often presented as distinct improvements. We introduce Opal, a canonicalization algorithm that determines whether two preference objectives are algebraically equivalent by producing either a canonical form or a concrete witness of non-equivalence. Applying Opal reveals that many widely used methods optimize the same underlying objective, while others are provably distinct. For example, batch normalization can cause the same response pair to receive different gradients depending on batch composition. We identify a small set of structural mechanisms that give rise to genuinely different objectives; most remaining differences are reparameterizations.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.11298.pdf",
    "abs_url": "https://arxiv.org/abs/2509.11298",
    "published": "2025-09-14T14:42:39Z",
    "updated": "2026-02-05T17:34:59Z",
    "comment": "21 pages",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.06690",
    "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring",
    "authors": [
      "Usman Haider",
      "Lukasz Szemet",
      "Daniel Kelly",
      "Vasileios Sergis",
      "Andrew C. Daly",
      "Karl Mason"
    ],
    "abstract": "Bioprinting is a rapidly advancing field that offers a transformative approach to fabricating tissue and organ models through the precise deposition of cell-laden bioinks. Ensuring the fidelity and consistency of printed structures in real-time remains a core challenge, particularly under constraints imposed by limited imaging data and resource-constrained embedded hardware. Semantic segmentation of the extrusion process, differentiating between nozzle, extruded bioink, and surrounding background, enables in situ monitoring critical to maintaining print quality and biological viability. In this work, we introduce a lightweight semantic segmentation framework tailored for real-time bioprinting applications. We present a novel, manually annotated dataset comprising 787 RGB images captured during the bioprinting process, labeled across three classes: nozzle, bioink, and background. To achieve fast and efficient inference suitable for integration with bioprinting systems, we propose a BioLite U-Net architecture that leverages depthwise separable convolutions to drastically reduce computational load without compromising accuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based segmentation baselines using mean Intersection over Union (mIoU), Dice score, and pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess real-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85% and a Dice score of 96.17%, while being over 1300x smaller than MobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame, demonstrating near real-time capability. Compared to MobileNet baselines, BioLite U-Net offers a superior tradeoff between segmentation accuracy, efficiency, and deployability, making it highly suitable for intelligent, closed-loop bioprinting systems.",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.AR"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2509.06690.pdf",
    "abs_url": "https://arxiv.org/abs/2509.06690",
    "published": "2025-09-08T13:44:55Z",
    "updated": "2026-02-05T12:38:01Z",
    "comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes dataset description, BioLite U-Net architecture, benchmark results on edge device (Raspberry Pi 4B)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.06505",
    "title": "On optimal solutions of classical and sliced Wasserstein GANs with non-Gaussian data",
    "authors": [
      "Yu-Jui Huang",
      "Hsin-Hua Shen",
      "Yu-Chih Huang",
      "Wan-Yi Lin",
      "Shih-Chun Lin"
    ],
    "abstract": "The generative adversarial network (GAN) aims to approximate an unknown distribution via a parameterized neural network (NN). While GANs have been widely applied in reinforcement and semi-supervised learning as well as computer vision tasks, selecting their parameters often needs an exhaustive search, and only a few selection methods have been proven to be theoretically optimal. One of the most promising GAN variants is the Wasserstein GAN (WGAN). Prior work on optimal parameters for population WGAN is limited to the linear-quadratic-Gaussian (LQG) setting, where the generator NN is linear, and the data is Gaussian. In this paper, we focus on the characterization of optimal solutions of population WGAN beyond the LQG setting. As a basic result, closed-form optimal parameters for one-dimensional WGAN are derived when the NN has non-linear activation functions, and the data is non-Gaussian. For high-dimensional data, we adopt the sliced Wasserstein framework and show that the linear generator can be asymptotically optimal. Moreover, the original sliced WGAN only constrains the projected data marginal instead of the whole one in classical WGAN, and thus, we propose another new unprojected sliced WGAN and identify its asymptotic optimality. Empirical studies show that compared to the celebrated r-principal component analysis (r-PCA) solution, which has cubic complexity to the data dimension, our generator for sliced WGAN can achieve better performance with only linear complexity.",
    "categories": [
      "cs.LG",
      "cs.IT",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.06505.pdf",
    "abs_url": "https://arxiv.org/abs/2509.06505",
    "published": "2025-09-08T10:10:37Z",
    "updated": "2026-02-05T08:25:42Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.04661",
    "title": "Flexible inference for animal learning rules using neural networks",
    "authors": [
      "Yuhan Helena Liu",
      "Victor Geadah",
      "Jonathan Pillow"
    ],
    "abstract": "Understanding how animals learn is a central challenge in neuroscience, with growing relevance to the development of animal- or human-aligned artificial intelligence. However, existing approaches tend to assume fixed parametric forms for the learning rule (e.g., Q-learning, policy gradient), which may not accurately describe the complex forms of learning employed by animals in realistic settings. Here we address this gap by developing a framework to infer learning rules directly from behavioral data collected during de novo task learning. We assume that animals follow a decision policy parameterized by a generalized linear model (GLM), and we model their learning rule -- the mapping from task covariates to per-trial weight updates -- using a deep neural network (DNN). This formulation allows flexible, data-driven inference of learning rules while maintaining an interpretable form of the decision policy itself. To capture more complex learning dynamics, we introduce a recurrent neural network (RNN) variant that relaxes the Markovian assumption that learning depends solely on covariates of the current trial, allowing for learning rules that integrate information over multiple trials. Simulations demonstrate that the framework can recover ground-truth learning rules. We applied our DNN and RNN-based methods to a large behavioral dataset from mice learning to perform a sensory decision-making task and found that they outperformed traditional RL learning rules at predicting the learning trajectories of held-out mice. The inferred learning rules exhibited reward-history-dependent learning dynamics, with larger updates following sequences of rewarded trials. Overall, these methods provide a flexible framework for inferring learning rules from behavioral data in de novo learning tasks, setting the stage for improved animal training protocols and the development of behavioral digital twins.",
    "categories": [
      "cs.LG",
      "cs.NE"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.04661.pdf",
    "abs_url": "https://arxiv.org/abs/2509.04661",
    "published": "2025-09-04T21:20:11Z",
    "updated": "2026-02-05T17:21:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.03493",
    "title": "On Entropy Control in LLM-RL Algorithms",
    "authors": [
      "Han Shen"
    ],
    "abstract": "For RL algorithms, appropriate entropy control is crucial to their effectiveness. To control the policy entropy, a commonly used method is entropy regularization, which is adopted in various popular RL algorithms including PPO, SAC and A3C. Although entropy regularization proves effective in robotic and games RL conventionally, studies found that it gives weak to no gains in LLM-RL training. In this work, we study the issues of entropy bonus in LLM-RL setting. Specifically, we first argue that the conventional entropy regularization suffers from the LLM's extremely large response space and the sparsity of the optimal outputs. As a remedy, we propose AEnt, an entropy control method that utilizes a new clamped entropy bonus with an automatically adjusted coefficient. The clamped entropy is evaluated with the re-normalized policy defined on certain smaller token space, which encourages exploration within a more compact response set. In addition, the algorithm automatically adjusts entropy coefficient according to the clamped entropy value, effectively controlling the entropy-induced bias while leveraging the entropy's benefits. AEnt is tested in math-reasoning tasks under different base models and datasets, and it is observed that AEnt outperforms the baselines consistently across multiple benchmarks.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.03493.pdf",
    "abs_url": "https://arxiv.org/abs/2509.03493",
    "published": "2025-09-03T17:23:19Z",
    "updated": "2026-02-05T06:47:40Z",
    "comment": "Updated with ICLR 2026 version",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2509.03054",
    "title": "Calibration and Transformation-Free Weight-Only LLMs Quantization via Dynamic Grouping",
    "authors": [
      "Xinzhe Zheng",
      "Zhen-Qun Yang",
      "Zishan Liu",
      "Haoran Xie",
      "S. Joe Qin",
      "Arlene Chen",
      "Fangzhen Lin"
    ],
    "abstract": "Large Language Models (LLMs) deliver strong performance but are difficult to deploy under tight memory and compute constraints. Low-bit post-training quantization (PTQ) is a promising direction; however, it typically relies on calibration data, auxiliary transformations, and GPU tools. To address these limitations, we propose MSB (Multi Scale Binary), a calibration-free and transformation-free PTQ method that generalizes binary quantization to multi-bit settings. MSB optimizes a dynamic grouping criterion that minimizes within group variance, yielding group-wise multiscale levels that can be applied consistently across granularities from per tensor to block-wise configurations with 64 elements groups per row, without calibration or intermediate transforms. We implement the optimization in a CPU based solver for the quantization step and evaluate using standard bfloat16 execution without low-bit packing. On Llama 3.2 3B, MSB achieves 8.43 perplexity on WikiText-2 under 4-bit weight only block-wise quantization, compared to 7.81 in full precision and 12.23 with GPTQ its default setup. Overall, MSB provides a new optimization perspective for low-bit PTQ while simplifying the pipeline by removing calibration and transformations.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2509.03054.pdf",
    "abs_url": "https://arxiv.org/abs/2509.03054",
    "published": "2025-09-03T06:36:21Z",
    "updated": "2026-02-05T07:18:02Z",
    "comment": "34 pages, 10 figures. Version 3 corrects the bit-length error and adds new experiments and analysis; the core methodology remains unchanged",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.21051",
    "title": "Language Models and Logic Programs for Trustworthy Tax Reasoning",
    "authors": [
      "William Jurayj",
      "Nils Holzenberger",
      "Benjamin Van Durme"
    ],
    "abstract": "According to the United States Internal Revenue Service, ``the average American spends $\\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.21051.pdf",
    "abs_url": "https://arxiv.org/abs/2508.21051",
    "published": "2025-08-28T17:55:07Z",
    "updated": "2026-02-05T18:58:31Z",
    "comment": "Accepted to AAAI 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.21022",
    "title": "A Sketch-and-Project Analysis of Subsampled Natural Gradient Algorithms",
    "authors": [
      "Gil Goldshlager",
      "Jiang Hu",
      "Lin Lin"
    ],
    "abstract": "Subsampled natural gradient descent (SNG) has been used to enable high-precision scientific machine learning, but standard analyses based on stochastic preconditioning fail to provide insight into realistic small-sample settings. We overcome this limitation by instead analyzing SNG as a sketch-and-project method. Motivated by this lens, we discard the usual theoretical proxy which decouples gradients and preconditioners using two independent mini-batches, and we replace it with a new proxy based on squared volume sampling. Under this new proxy we show that the expectation of the SNG direction becomes equal to a preconditioned gradient descent step even in the presence of coupling, leading to (i) global convergence guarantees when using a single mini-batch of any size, and (ii) an explicit characterization of the convergence rate in terms of quantities related to the sketch-and-project structure. These findings in turn yield new insights into small-sample settings, for example by suggesting that the advantage of SNG over SGD is that it can more effectively exploit spectral decay in the model Jacobian. We also extend these ideas to explain a popular structured momentum scheme for SNG, known as SPRING, by showing that it arises naturally from accelerated sketch-and-project methods.",
    "categories": [
      "cs.LG",
      "math.OC",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.21022.pdf",
    "abs_url": "https://arxiv.org/abs/2508.21022",
    "published": "2025-08-28T17:24:59Z",
    "updated": "2026-02-05T17:09:24Z",
    "comment": "21 pages, 6 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.19842",
    "title": "Symplectic convolutional neural networks",
    "authors": [
      "SÃ¼leyman YÄ±ldÄ±z",
      "Konrad Janik",
      "Peter Benner"
    ],
    "abstract": "We propose a new symplectic convolutional neural network (CNN) architecture by leveraging symplectic neural networks, proper symplectic decomposition, and tensor techniques. Specifically, we first introduce a mathematically equivalent form of the convolution layer and then, using symplectic neural networks, we demonstrate a way to parameterize the layers of the CNN to ensure that the convolution layer remains symplectic. To construct a complete autoencoder, we introduce a symplectic pooling layer. We demonstrate the performance of the proposed neural network on three examples: the wave equation, the nonlinear SchrÃ¶dinger (NLS) equation, and the sine-Gordon equation. The numerical results indicate that the symplectic CNN outperforms the linear symplectic autoencoder obtained via proper symplectic decomposition.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2508.19842.pdf",
    "abs_url": "https://arxiv.org/abs/2508.19842",
    "published": "2025-08-27T12:55:24Z",
    "updated": "2026-02-05T13:34:31Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.12840",
    "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics",
    "authors": [
      "Giovanni Briglia",
      "Francesco Fabiano",
      "Stefano Mariani"
    ],
    "abstract": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability. To address this, we exploit Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process. GNNs, which naturally capture the graph-like nature of Kripke models, allow us to derive meaningful estimates of state quality -- e.g., the distance from the nearest goal -- by generalizing knowledge obtained from previously solved planning instances. We integrate these predictive heuristics into an epistemic planning pipeline and evaluate them against standard baselines, showing improvements in the scalability of multi-agent epistemic planning.",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2508.12840.pdf",
    "abs_url": "https://arxiv.org/abs/2508.12840",
    "published": "2025-08-18T11:26:20Z",
    "updated": "2026-02-05T18:00:15Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.04349",
    "title": "GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy",
    "authors": [
      "Hongze Tan",
      "Zihan Wang",
      "Jianfei Pan",
      "Jinghao Lin",
      "Hao Wang",
      "Yifan Wu",
      "Tao Chen",
      "Zhihang Zheng",
      "Zhihao Tang",
      "Haihua Yang"
    ],
    "abstract": "Reinforcement Learning (RL) is pivotal for enhancing Large Language Model (LLM) reasoning, yet mainstream algorithms such as GRPO and DAPO remain constrained by a coarse-grained credit assignment paradigm, where all tokens within the same response receive the identical reward. In this paper, we propose Dynamic Entropy Weighting, systematically define entropy-based weight ratios $\\frac{H_{i,t}}{\\sum_{k=1}^{n} H_{k,t}}$ and similar variants to redistribute rewards and get fine-grained rewards through two new algorithms: Group Token Policy Optimization (GTPO), which assigns an entropy-weighted reward to each token and synthesizes token-specific advantage function to drive the model toward optimal path, and the analogous algorithm Sequence-Level GRPO (GRPO-S), which extends this design to the sequence level and exhibits superior stability in long Chain-of-Thought (CoT) reasoning tasks.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2508.04349.pdf",
    "abs_url": "https://arxiv.org/abs/2508.04349",
    "published": "2025-08-06T11:42:47Z",
    "updated": "2026-02-05T08:04:18Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2508.01151",
    "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
    "authors": [
      "Yu Lei",
      "Jinbin Bai",
      "Qingyu Shi",
      "Aosong Feng",
      "Hongcheng Gao",
      "Xiao Zhang",
      "Rex Ying"
    ],
    "abstract": "Text-to-image diffusion models have revolutionized visual content generation, yet their deployment is hindered by a fundamental limitation: safety mechanisms enforce rigid, uniform standards that fail to reflect diverse user preferences shaped by age, culture, or personal beliefs. To address this, we propose Personalized Safety Alignment (PSA), a framework that transitions generative safety from static filtration to user-conditioned adaptation. We introduce Sage, a large-scale dataset capturing diverse safety boundaries across 1,000 simulated user profiles, covering complex risks often missed by traditional datasets. By integrating these profiles via a parameter-efficient cross-attention adapter, PSA dynamically modulates generation to align with individual sensitivities. Extensive experiments demonstrate that PSA achieves a calibrated safety-quality trade-off: under permissive profiles, it relaxes over-cautious constraints to enhance visual fidelity, while under restrictive profiles, it enforces state-of-the-art suppression, significantly outperforming static baselines. Furthermore, PSA exhibits superior instruction adherence compared to prompt-engineering methods, establishing personalization as a vital direction for creating adaptive, user-centered, and responsible generative AI. Our code, data, and models are publicly available at https://github.com/M-E-AGI-Lab/PSAlign.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2508.01151.pdf",
    "abs_url": "https://arxiv.org/abs/2508.01151",
    "published": "2025-08-02T02:23:20Z",
    "updated": "2026-02-05T07:15:25Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2507.17001",
    "title": "Should Bias be Eliminated? A General Framework to Use Bias for OOD Generalization",
    "authors": [
      "Yan Li",
      "Yunlong Deng",
      "Zijian Li",
      "Anpeng Wu",
      "Zeyu Tang",
      "Kun Zhang",
      "Guangyi Chen"
    ],
    "abstract": "Most approaches to out-of-distribution (OOD) generalization learn domain-invariant representations by discarding contextual bias. In this paper, we raise a critical question: Should bias be eliminated? If not, is there a general way to leverage bias for better OOD generalization? To answer these questions, we first provide a theoretical analysis that characterizes the circumstances in which biased features contribute positively. Although theoretical results show that bias may sometimes play a positive role, leveraging it effectively is non-trivial, since its harmful and beneficial components are often entangled. Recent advances have sought to refine the prediction of bias by presuming reliable predictions from invariant features. However, such assumptions may be too strong in the real world, especially when the target also shifts from training to testing domains. Motivated by this challenge, we introduce a framework to leverage bias in a more general scenario. Specifically, we employ a generative model to capture the data generation process and identify the underlying bias factors, which are then used to construct a bias-aware predictor. Since the bias-aware predictor may shift across environments, we first estimate the environment state to train predictors under different environments, combining them as a mixture of domain experts for the final prediction. Then, we build a general invariant predictor, which can be invariant under label shift to guide the adaptation of the bias-aware predictor. Evaluations on synthetic data and standard domain generalization benchmarks demonstrate that our method consistently outperforms both invariance only baselines, recent bias utilization approaches and advanced baselines, yielding improved robustness and adaptability.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.17001.pdf",
    "abs_url": "https://arxiv.org/abs/2507.17001",
    "published": "2025-07-22T20:17:48Z",
    "updated": "2026-02-05T11:17:24Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2507.13772",
    "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification",
    "authors": [
      "Abhijit Sen",
      "Giridas Maiti",
      "Bikram K. Parida",
      "Bhanu P. Mishra",
      "Mahima Arya",
      "Denys I. Bondar"
    ],
    "abstract": "Feature engineering continues to play a critical role in image classification, particularly when interpretability and computational efficiency are prioritized over deep learning models with millions of parameters. In this study, we revisit classical machine learning based image classification through a novel approach centered on Permutation Entropy (PE), a robust and computationally lightweight measure traditionally used in time series analysis but rarely applied to image data. We extend PE to two-dimensional images and propose a multiscale, multi-orientation entropy-based feature extraction approach that characterizes spatial order and complexity along rows, columns, diagonals, anti-diagonals, and local patches of the image. To enhance the discriminatory power of the entropy features, we integrate two classic image descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an image. The resulting hand-crafted feature set, comprising of 780 dimensions, is used to train Support Vector Machine (SVM) classifiers optimized through grid search. The proposed approach is evaluated on multiple benchmark datasets, including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers competitive classification performance without relying on deep architectures. Our results demonstrate that the fusion of PE with HOG and LBP provides a compact, interpretable, and effective alternative to computationally expensive and limited interpretable deep learning models. This shows a potential of entropy-based descriptors in image classification and contributes a lightweight and generalizable solution to interpretable machine learning in image classification and computer vision.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2507.13772.pdf",
    "abs_url": "https://arxiv.org/abs/2507.13772",
    "published": "2025-07-18T09:29:03Z",
    "updated": "2026-02-05T10:19:50Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2507.13579",
    "title": "Learning to summarize user information for personalized reinforcement learning from human feedback",
    "authors": [
      "Hyunji Nam",
      "Yanming Wan",
      "Mickel Liu",
      "Peter Ahnn",
      "Jianxun Lian",
      "Natasha Jaques"
    ],
    "abstract": "As everyday use cases of large language model (LLM) AI assistants have expanded, it is becoming increasingly important to personalize responses to align to different users' preferences and goals. While reinforcement learning from human feedback (RLHF) is effective at improving LLMs to be generally more helpful and fluent, it does not account for variability across users, as it models the entire user population with a single reward model, meaning it assumes that everyone's preferences are the same. We present a novel framework, Preference Learning Using Summarization (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries of each user's preferences, characteristics, and past conversations. These summaries condition the reward model, enabling it to make personalized predictions about the types of responses valued by each user. Both the user-summarization model and reward model are trained simultaneously, creating an online co-adaptation loop. We show that in contrast to the standard Bradley-Terry model, summaries produced by PLUS capture diverse aspects of user preferences, achieving a 11-77/% improvement in reward model accuracy. Key strengths of PLUS are: (1) robust performance with new users and conversation topics, achieving a 25\\% improvement over the best personalized reward model technique used for RLHF; (2) zero-shot personalization with state-of-the-art proprietary models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72\\% win rate compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond preference labels, and (4) interpretable representation of users, enabling greater transparency and user control in pluralistic LLM alignment.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.13579.pdf",
    "abs_url": "https://arxiv.org/abs/2507.13579",
    "published": "2025-07-17T23:48:51Z",
    "updated": "2026-02-05T17:13:23Z",
    "comment": "10 pages for main text, 10 pages for appendix",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2507.04756",
    "title": "CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering",
    "authors": [
      "Hang Lv",
      "Sheng Liang",
      "Hao Wang",
      "Hongchao Gu",
      "Yaxiong Wu",
      "Wei Guo",
      "Defu Lian",
      "Yong Liu",
      "Enhong Chen"
    ],
    "abstract": "Personalization has become crucial for adapting models to the diverse and evolving needs of users across cultural, temporal, and contextual dimensions. While existing methods often rely on centralized fine-tuning or static preference alignment within a single model, they struggle to achieve both real-time and high-quality personalization under the resource and privacy constraints of personal devices. To address this challenge, we propose CoSteer, a collaborative framework that enables tuning-free, real-time personalization via decoding-time adaptation. By leveraging logit differences between context-aware and context-agnostic local small models, CoSteer steers cloud-based large models, ensuring effective personalization while preserving the large model's capabilities. Personalization is handled locally, with only final tokens sent to the cloud, maintaining both user context and system efficiency. Through extensive experiments across a wide range of tasks, we demonstrate that CoSteer generates high-quality personalized content, ensuring both effectiveness and computational efficiency. Our results highlight its robustness across models and environments, confirming its practical applicability in real-world scenarios.",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2507.04756.pdf",
    "abs_url": "https://arxiv.org/abs/2507.04756",
    "published": "2025-07-07T08:32:29Z",
    "updated": "2026-02-05T08:26:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2506.22186",
    "title": "Thompson Sampling-Based Learning and Control for Unknown Dynamic Systems",
    "authors": [
      "Kaikai Zheng",
      "Dawei Shi",
      "Yang Shi",
      "Long Wang"
    ],
    "abstract": "Thompson sampling (TS) is a Bayesian randomized exploration strategy that samples options (e.g., system parameters or control laws) from the current posterior and then applies the selected option that is optimal for a task, thereby balancing exploration and exploitation; this makes TS effective for active learning-based controller design. However, TS relies on finite parametric representations, which limits its applicability to more general spaces, which are more commonly encountered in control system design. To address this issue, this work proposes a parameterization method for control law learning using reproducing kernel Hilbert spaces and designs a data-driven active learning control approach. Specifically, the proposed method treats the control law as an element in a function space, allowing the design of control laws without imposing restrictions on the system structure or the form of the controller. A TS framework is proposed in this work to reduce control costs through online exploration and exploitation, and the convergence guarantees are further provided for the learning process. Theoretical analysis shows that the proposed method learns the relationship between control laws and closed-loop performance metrics at an exponential rate, and the upper bound of control regret is also derived. Furthermore, the closed-loop stability of the proposed learning framework is analyzed. Numerical experiments on controlling unknown nonlinear systems validate the effectiveness of the proposed method.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2506.22186.pdf",
    "abs_url": "https://arxiv.org/abs/2506.22186",
    "published": "2025-06-27T12:49:43Z",
    "updated": "2026-02-05T14:44:35Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2506.21996",
    "title": "AlphaBeta is not as good as you think: a simple class of synthetic games for a better analysis of deterministic game-solving algorithms",
    "authors": [
      "RaphaÃ«l Boige",
      "Amine Boumaza",
      "Bruno Scherrer"
    ],
    "abstract": "Deterministic game-solving algorithms are conventionally analyzed in the light of their average-case complexity against a distribution of random game-trees, where leaf values are independently sampled from a fixed distribution. This simplified model enables uncluttered mathematical analysis, revealing two key properties: root value distributions asymptotically collapse to a single fixed value for finite-valued trees, and all reasonable algorithms achieve global optimality. However, these findings are artifacts of the model's design: its long criticized independence assumption strips games of structural complexity, producing trivial instances where no algorithm faces meaningful challenges. To address this limitation, we introduce a class of synthetic games generated by a probabilistic model that incrementally constructs game-trees using a fixed level-wise conditional distribution. By enforcing ancestor dependencies, a critical structural feature of real-world games, our framework generates problems with adjustable difficulty while retaining some form of analytical tractability. For several algorithms, including AlphaBeta and Scout, we derive recursive formulas characterizing their average-case complexities under this model. These allow us to rigorously compare algorithms on deep game-trees, where Monte-Carlo simulations are no longer feasible. While asymptotically, all algorithms seem to converge to identical branching factor (a result analogous to that of independence-based models), deep finite trees reveal stark differences: AlphaBeta incurs a significantly larger constant multiplicative factor compared to algorithms like Scout, leading to a substantial practical slowdown. Our framework sheds new light on classical game-solving algorithms, offering rigorous evidence and analytical tools to advance the understanding of these methods under a richer, more challenging, and yet tractable model.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.21996.pdf",
    "abs_url": "https://arxiv.org/abs/2506.21996",
    "published": "2025-06-27T08:07:17Z",
    "updated": "2026-02-05T09:24:38Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2507.01028",
    "title": "Dual Perspectives on Non-Contrastive Self-Supervised Learning",
    "authors": [
      "Jean Ponce",
      "Basile Terver",
      "Martial Hebert",
      "Michael Arbel"
    ],
    "abstract": "The {\\em stop gradient} and {\\em exponential moving average} iterative procedures are commonly used in non-contrastive approaches to self-supervised learning to avoid representation collapse, with excellent performance in downstream applications in practice. This presentation investigates these procedures from the dual viewpoints of optimization and dynamical systems. We show that, in general, although they {\\em do not} optimize the original objective, or {\\em any} other smooth function, they {\\em do} avoid collapse Following~\\citet{Tian21}, but without any of the extra assumptions used in their proofs, we then show using a dynamical system perspective that, in the linear case, minimizing the original objective function without the use of a stop gradient or exponential moving average {\\em always} leads to collapse. Conversely, we characterize explicitly the equilibria of the dynamical systems associated with these two procedures in this linear setting as algebraic varieties in their parameter space, and show that they are, in general, {\\em asymptotically stable}. Our theoretical findings are illustrated by empirical experiments with real and synthetic data.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2507.01028.pdf",
    "abs_url": "https://arxiv.org/abs/2507.01028",
    "published": "2025-06-18T07:46:51Z",
    "updated": "2026-02-05T10:08:11Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2506.13342",
    "title": "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers",
    "authors": [
      "Wooseok Seo",
      "Seungju Han",
      "Jaehun Jung",
      "Benjamin Newman",
      "Seungwon Lim",
      "Seungbeen Lee",
      "Ximing Lu",
      "Yejin Choi",
      "Youngjae Yu"
    ],
    "abstract": "Fact verification is essential for ensuring the reliability of LLM applications. In this study, we evaluate 12 pre-trained LLMs and one specialized fact-verifier, including frontier LLMs and open-weight reasoning LLMs, using a collection of examples from 14 fact-checking benchmarks. We share three findings intended to guide future development of more robust fact verifiers. First, we highlight the importance of addressing annotation errors and ambiguity in datasets, demonstrating that approximately 16\\% of ambiguous or incorrectly labeled data substantially influences model rankings. Neglecting this issue may result in misleading conclusions during comparative evaluations, and we suggest using a systematic pipeline utilizing LLM-as-a-judge to help identify these issues at scale. Second, we discover that frontier LLMs with few-shot in-context examples, often overlooked in previous works, achieve top-tier performance. We therefore recommend that future studies include comparisons with these simple yet highly effective baselines. Lastly, despite their effectiveness, frontier LLMs incur substantial costs, motivating the development of small, fine-tuned fact verifiers. We show that these small models still have room for improvement, particularly on instances that require complex reasoning. Encouragingly, we demonstrate that augmenting training with synthetic multi-hop reasoning data significantly enhances their capabilities in such instances. We release our code, model, and dataset at https://github.com/just1nseo/verifying-the-verifiers.",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2506.13342.pdf",
    "abs_url": "https://arxiv.org/abs/2506.13342",
    "published": "2025-06-16T10:32:10Z",
    "updated": "2026-02-05T04:04:41Z",
    "comment": "Accepted to COLM 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2506.08194",
    "title": "GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra",
    "authors": [
      "Mateusz Michalkiewicz",
      "Anekha Sokhal",
      "Tadeusz Michalkiewicz",
      "Piotr Pawlikowski",
      "Mahsa Baktashmotlagh",
      "Varun Jampani",
      "Guha Balakrishnan"
    ],
    "abstract": "Modern monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet recent works cast doubt on their true understanding of geometric properties. We introduce GOQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra covering varying levels of complexity and symmetry, from Platonic, Archimedean, Johnson, and Catalan solids to stellations and compound shapes. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric Platonic solids accurately. Next, although foundation models may be shown via linear and non-linear probing to capture specific 3D symmetry elements, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants such as ChatGPT, Gemini and Claud exhibit remarkably low accuracy in interpreting basic shape properties such as face geometry, convexity, and compound structures of complex polyhedra. GIQ is publicly available at toomanymatts.github.io/giq-benchmark/, providing a structured platform to benchmark critical gaps in geometric intelligence and facilitate future progress in robust, geometry-aware representation learning.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.08194.pdf",
    "abs_url": "https://arxiv.org/abs/2506.08194",
    "published": "2025-06-09T20:11:21Z",
    "updated": "2026-02-05T16:06:21Z",
    "comment": "Accepted to ICLR 2026. Camera ready version",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2506.01758",
    "title": "Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks",
    "authors": [
      "Ruibin Li",
      "Tao Yang",
      "Yangming Shi",
      "Weiguo Feng",
      "Shilei Wen",
      "Bingyue Peng",
      "Lei Zhang"
    ],
    "abstract": "Diffusion models have shown impressive performance in many visual generation and manipulation tasks. Many existing methods focus on training a model for a specific task, especially, text-to-video (T2V) generation, while many other works focus on finetuning the pretrained T2V model for image-to-video (I2V), video-to-video (V2V), image and video manipulation tasks, etc. However, training a strong T2V foundation model requires a large amount of high-quality annotations, which is very costly. In addition, many existing models can perform only one or several tasks. In this work, we introduce a unified framework, namely many-for-many, which leverages the available training data from many different visual generation and manipulation tasks to train a single model for those different tasks. Specifically, we design a lightweight adapter to unify the different conditions in different tasks, then employ a joint image-video learning strategy to progressively train the model from scratch. Our joint learning leads to a unified visual generation and manipulation model with improved video generation performance. In addition, we introduce depth maps as a condition to help our model better perceive the 3D space in visual generation. Two versions of our model are trained with different model sizes (8B and 2B), each of which can perform more than 10 different tasks. In particular, our 8B model demonstrates highly competitive performance in video generation tasks compared to open-source and even commercial engines. Our models and source codes are available at https://github.com/leeruibin/MfM.git.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2506.01758.pdf",
    "abs_url": "https://arxiv.org/abs/2506.01758",
    "published": "2025-06-02T15:05:44Z",
    "updated": "2026-02-05T14:38:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.23071",
    "title": "Rethinking Multi-Modal Learning from Gradient Uncertainty",
    "authors": [
      "Peizheng Guo",
      "Jingyao Wang",
      "Wenwen Qiang",
      "Jiahuan Zhou",
      "Changwen Zheng",
      "Gang Hua"
    ],
    "abstract": "Multi-Modal Learning (MML) integrates information from diverse modalities to improve predictive accuracy. While existing optimization strategies have made significant strides by mitigating gradient direction conflicts, we revisit MML from a gradient-based perspective to explore further improvements. Empirically, we observe an interesting phenomenon: performance fluctuations can persist in both conflict and non-conflict settings. Based on this, we argue that: beyond gradient direction, the intrinsic reliability of gradients acts as a decisive factor in optimization, necessitating the explicit modeling of gradient uncertainty. Guided by this insight, we propose Bayesian-Oriented Gradient Calibration for MML (BOGC-MML). Our approach explicitly models gradients as probability distributions to capture uncertainty, interpreting their precision as evidence within the framework of subjective logic and evidence theory. By subsequently aggregating these signals using a reduced Dempster's combination rule, BOGC-MML adaptively weights gradients based on their reliability to generate a calibrated update. Extensive experiments demonstrate the effectiveness and advantages of the proposed method.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.23071.pdf",
    "abs_url": "https://arxiv.org/abs/2505.23071",
    "published": "2025-05-29T04:23:22Z",
    "updated": "2026-02-05T08:56:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.20624",
    "title": "POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event Online Polarization",
    "authors": [
      "Usman Naseem",
      "Robert Geislinger",
      "Juan Ren",
      "Sarah Kohail",
      "Rudy Garrido Veliz",
      "P Sam Sahil",
      "Yiran Zhang",
      "Marco Antonio Stranisci",
      "Idris Abdulmumin",
      "Ãzge Alacam",
      "Cengiz AcartÃ¼rk",
      "Aisha Jabr",
      "Saba Anwar",
      "Abinew Ali Ayele",
      "Simona Frenda",
      "Alessandra Teresa Cignarella",
      "Elena Tutubalina",
      "Oleg Rogov",
      "Aung Kyaw Htet",
      "Xintong Wang",
      "Surendrabikram Thapa",
      "Kritesh Rauniyar",
      "Tanmoy Chakraborty",
      "Arfeen Zeeshan",
      "Dheeraj Kodati",
      "Satya Keerthi",
      "Sahar Moradizeyveh",
      "Firoj Alam",
      "Arid Hasan",
      "Syed Ishtiaque Ahmed",
      "Ye Kyaw Thu",
      "Shantipriya Parida",
      "Ihsan Ayyub Qazi",
      "Lilian Wanzare",
      "Nelson Odhiambo Onyango",
      "Clemencia Siro",
      "Jane Wanjiru Kimani",
      "Ibrahim Said Ahmad",
      "Adem Chanie Ali",
      "Martin Semmann",
      "Chris Biemann",
      "Shamsuddeen Hassan Muhammad",
      "Seid Muhie Yimam"
    ],
    "abstract": "Online polarization poses a growing challenge for democratic discourse, yet most computational social science research remains monolingual, culturally narrow, or event-specific. We introduce POLAR, a multilingual, multicultural, and multi-event dataset with over 110K instances in 22 languages drawn from diverse online platforms and real-world events. Polarization is annotated along three axes, namely detection, type, and manifestation, using a variety of annotation platforms adapted to each cultural context. We conduct two main experiments: (1) fine-tuning six pretrained small language models; and (2) evaluating a range of open and closed large language models in few-shot and zero-shot settings. The results show that, while most models perform well in binary polarization detection, they achieve substantially lower performance when predicting polarization types and manifestations. These findings highlight the complex, highly contextual nature of polarization and demonstrate the need for robust, adaptable approaches in NLP and computational social science. All resources will be released to support further research and effective mitigation of digital polarization globally.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.20624.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20624",
    "published": "2025-05-27T02:04:58Z",
    "updated": "2026-02-05T12:57:13Z",
    "comment": "Preprint",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.20295",
    "title": "SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?",
    "authors": [
      "Michael Kirchhof",
      "Luca FÃ¼ger",
      "Adam GoliÅski",
      "Eeshan Gunesh Dhekane",
      "Arno Blaas",
      "Seong Joon Oh",
      "Sinead Williamson"
    ],
    "abstract": "The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.20295.pdf",
    "abs_url": "https://arxiv.org/abs/2505.20295",
    "published": "2025-05-26T17:59:53Z",
    "updated": "2026-02-05T17:25:06Z",
    "comment": "Accepted at ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.19969",
    "title": "Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models",
    "authors": [
      "Antti Koskela",
      "Tejas Kulkarni"
    ],
    "abstract": "Achieving differential privacy (DP) guarantees in fully decentralized machine learning is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. We present a framework for DP analysis of decentralized gossip-based averaging algorithms with additive node-level noise, from arbitrary views of nodes in a graph. We present an analytical framework based on a linear systems formulation that accurately characterizes privacy leakage between nodes. Our main contribution is showing that the DP guarantees are those of a Gaussian mechanism, where the growth of the squared sensitivity is asymptotically $O(T)$, where $T$ is the number of training rounds, similarly as in the case of central aggregation. As an application of the sensitivity analysis, we show that the excess risk of decentralized private learning for strongly convex losses is asymptotically similar as in centralized private learning.",
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.DC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.19969.pdf",
    "abs_url": "https://arxiv.org/abs/2505.19969",
    "published": "2025-05-26T13:31:43Z",
    "updated": "2026-02-05T09:02:12Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.17767",
    "title": "Position: The Real Barrier to LLM Agent Usability is Agentic ROI",
    "authors": [
      "Weiwen Liu",
      "Jiarui Qin",
      "Xu Huang",
      "Xingshan Zeng",
      "Yunjia Xi",
      "Jianghao Lin",
      "Chuhan Wu",
      "Yasheng Wang",
      "Lifeng Shang",
      "Ruiming Tang",
      "Defu Lian",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "abstract": "Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. While LLM agents are technically capable of performing a broad range of tasks, not all of these capabilities translate into meaningful usability. This position paper argues that the central question for LLM agent usability is no longer whether a task can be automated, but whether it delivers sufficient Agentic Return on Investment (Agentic ROI). Agentic ROI reframes evaluation from raw performance to a holistic, utility-driven perspective, guiding when, where, and for whom LLM agents should be deployed. Despite widespread application in high-ROI tasks like coding and scientific research, we identify a critical usability gap in mass-market, everyday applications. To address this, we propose a zigzag developmental trajectory: first scaling up to improve information gain and time savings, then scaling down to reduce cost. We present a strategic roadmap across these phases to make LLM agents truly usable, accessible, and scalable in real-world applications.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.17767.pdf",
    "abs_url": "https://arxiv.org/abs/2505.17767",
    "published": "2025-05-23T11:40:58Z",
    "updated": "2026-02-05T03:58:26Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.16048",
    "title": "SPhyR: Spatial-Physical Reasoning Benchmark on Material Distribution",
    "authors": [
      "Philipp D. Siedler"
    ],
    "abstract": "We introduce a novel dataset designed to benchmark the physical and spatial reasoning capabilities of Large Language Models (LLM) based on topology optimization, a method for computing optimal material distributions within a design space under prescribed loads and supports. In this dataset, LLMs are provided with conditions such as 2D boundary, applied forces and supports, and must reason about the resulting optimal material distribution. The dataset includes a variety of tasks, ranging from filling in masked regions within partial structures to predicting complete material distributions. Solving these tasks requires understanding the flow of forces and the required material distribution under given constraints, without access to simulation tools or explicit physical models, challenging models to reason about structural stability and spatial organization. Our dataset targets the evaluation of spatial and physical reasoning abilities in 2D settings, offering a complementary perspective to traditional language and logic benchmarks.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2505.16048.pdf",
    "abs_url": "https://arxiv.org/abs/2505.16048",
    "published": "2025-05-21T22:00:20Z",
    "updated": "2026-02-05T16:53:24Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.16001",
    "title": "Image-to-Image Translation with Diffusion Transformers and CLIP-Based Image Conditioning",
    "authors": [
      "Qiang Zhu",
      "Kuan Lu",
      "Menghao Huo",
      "Yuxiao Li"
    ],
    "abstract": "Image-to-image translation aims to learn a mapping between a source and a target domain, enabling tasks such as style transfer, appearance transformation, and domain adaptation. In this work, we explore a diffusion-based framework for image-to-image translation by adapting Diffusion Transformers (DiT), which combine the denoising capabilities of diffusion models with the global modeling power of transformers. To guide the translation process, we condition the model on image embeddings extracted from a pre-trained CLIP encoder, allowing for fine-grained and structurally consistent translations without relying on text or class labels. We incorporate both a CLIP similarity loss to enforce semantic consistency and an LPIPS perceptual loss to enhance visual fidelity during training. We validate our approach on two benchmark datasets: face2comics, which translates real human faces to comic-style illustrations, and edges2shoes, which translates edge maps to realistic shoe images. Experimental results demonstrate that DiT, combined with CLIP-based conditioning and perceptual similarity objectives, achieves high-quality, semantically faithful translations, offering a promising alternative to GAN-based models for paired image-to-image translation tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.16001.pdf",
    "abs_url": "https://arxiv.org/abs/2505.16001",
    "published": "2025-05-21T20:37:33Z",
    "updated": "2026-02-05T16:53:22Z",
    "comment": "Published in: 2025 6th International Conference on Computer Vision, Image and Deep Learning (CVIDL)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.13812",
    "title": "Physics-Driven Local-Whole Elastic Deformation Modeling for Point Cloud Representation Learning",
    "authors": [
      "Zhongyu Chen",
      "Rong Zhao",
      "Xie Han",
      "Xindong Guo",
      "Song Wang",
      "Zherui Qiao"
    ],
    "abstract": "Existing point cloud representation learning methods primarily rely on data-driven strategies to extract geometric information from large amounts of scattered data. However, most methods focus solely on the spatial distribution features of point clouds while overlooking the relationship between local information and the whole structure, which limits the accuracy of point cloud representation. Local information reflect the fine-grained variations of an object, while the whole structure is determined by the interaction and combination of these local features, collectively defining the object's shape. In real-world, objects undergo deformation under external forces, and this deformation gradually affects the whole structure through the propagation of forces from local regions, thereby altering the object's geometric features. Therefore, appropriately introducing a physics-driven mechanism to capture the topological relationships between local parts and the whole object can effectively mitigate for the limitations of data-driven point cloud methods in structural modeling, and enhance the generalization and interpretability of point cloud representations for downstream tasks such as understanding and recognition. Inspired by this, we incorporate a physics-driven mechanism into the data-driven method to learn fine-grained features in point clouds and model the structural relationship between local regions and the whole shape. Specifically, we design a dual-task encoder-decoder framework that combines the geometric modeling capability of data-driven implicit fields with physics-driven elastic deformation. Through the integration of physics-based loss functions, the framework is guided to predict localized deformation and explicitly capture the correspondence between local structural changes and whole shape variations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.13812.pdf",
    "abs_url": "https://arxiv.org/abs/2505.13812",
    "published": "2025-05-20T01:48:32Z",
    "updated": "2026-02-05T08:12:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.12300",
    "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models",
    "authors": [
      "Weixuan Wang",
      "Minghao Wu",
      "Barry Haddow",
      "Alexandra Birch"
    ],
    "abstract": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2505.12300.pdf",
    "abs_url": "https://arxiv.org/abs/2505.12300",
    "published": "2025-05-18T08:31:44Z",
    "updated": "2026-02-05T07:49:42Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.10960",
    "title": "Relational Graph Transformer",
    "authors": [
      "Vijay Prakash Dwivedi",
      "Sri Jaladi",
      "Yangyi Shen",
      "Federico LÃ³pez",
      "Charilaos I. Kanatsoulis",
      "Rishi Puri",
      "Matthias Fey",
      "Jure Leskovec"
    ],
    "abstract": "Relational Deep Learning (RDL) is a promising approach for building state-of-the-art predictive models on multi-table relational data by representing it as a heterogeneous temporal graph. However, commonly used Graph Neural Network models suffer from fundamental limitations in capturing complex structural patterns and long-range dependencies that are inherent in relational data. While Graph Transformers have emerged as powerful alternatives to GNNs on general graphs, applying them to relational entity graphs presents unique challenges: (i) Traditional positional encodings fail to generalize to massive, heterogeneous graphs; (ii) existing architectures cannot model the temporal dynamics and schema constraints of relational data; (iii) existing tokenization schemes lose critical structural information. Here we introduce the Relational Graph Transformer (RelGT), the first graph transformer architecture designed specifically for relational tables. RelGT employs a novel multi-element tokenization strategy that decomposes each node into five components (features, type, hop distance, time, and local structure), enabling efficient encoding of heterogeneity, temporality, and topology without expensive precomputation. Our architecture combines local attention over sampled subgraphs with global attention to learnable centroids, incorporating both local and database-wide representations. Across 21 tasks from the RelBench benchmark, RelGT consistently matches or outperforms GNN baselines by up to 18%, establishing Graph Transformers as a powerful architecture for Relational Deep Learning.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DB"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2505.10960.pdf",
    "abs_url": "https://arxiv.org/abs/2505.10960",
    "published": "2025-05-16T07:51:58Z",
    "updated": "2026-02-05T05:25:22Z",
    "comment": "ICLR 2026, Code: https://github.com/snap-stanford/relgt",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2505.04672",
    "title": "Histo-Miner: Deep learning based tissue features extraction pipeline from H&E whole slide images of cutaneous squamous cell carcinoma",
    "authors": [
      "Lucas SancÃ©rÃ©",
      "Carina Lorenz",
      "Doris Helbig",
      "Oana-Diana Persa",
      "Sonja Dengler",
      "Alexander Kreuter",
      "Martim Laimer",
      "Roland Lang",
      "Anne FrÃ¶hlich",
      "Jennifer Landsberg",
      "Johannes BrÃ¤gelmann",
      "Katarzyna Bozek"
    ],
    "abstract": "Recent advancements in digital pathology have enabled comprehensive analysis of Whole-Slide Images (WSI) from tissue samples, leveraging high-resolution microscopy and computational capabilities. Despite this progress, there is a lack of labeled datasets and open source pipelines specifically tailored for analysis of skin tissue. Here we propose Histo-Miner, a deep learning-based pipeline for analysis of skin WSIs and generate two datasets with labeled nuclei and tumor regions. We develop our pipeline for the analysis of patient samples of cutaneous squamous cell carcinoma (cSCC), a frequent non-melanoma skin cancer. Utilizing the two datasets, comprising 47,392 annotated cell nuclei and 144 tumor-segmented WSIs respectively, both from cSCC patients, Histo-Miner employs convolutional neural networks and vision transformers for nucleus segmentation and classification as well as tumor region segmentation. Performance of trained models positively compares to state of the art with multi-class Panoptic Quality (mPQ) of 0.569 for nucleus segmentation, macro-averaged F1 of 0.832 for nucleus classification and mean Intersection over Union (mIoU) of 0.907 for tumor region segmentation. From these predictions we generate a compact feature vector summarizing tissue morphology and cellular interactions, which can be used for various downstream tasks. Here, we use Histo-Miner to predict cSCC patient response to immunotherapy based on pre-treatment WSIs from 45 patients. Histo-Miner identifies percentages of lymphocytes, the granulocyte to lymphocyte ratio in tumor vicinity and the distances between granulocytes and plasma cells in tumors as predictive features for therapy response. This highlights the applicability of Histo-Miner to clinically relevant scenarios, providing direct interpretation of the classification and insights into the underlying biology.",
    "categories": [
      "cs.CV",
      "q-bio.QM"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2505.04672.pdf",
    "abs_url": "https://arxiv.org/abs/2505.04672",
    "published": "2025-05-07T09:34:03Z",
    "updated": "2026-02-05T13:35:50Z",
    "comment": "37 pages including supplement, 5 core figures. Version 2: change sections order, add new supplementary sections, minor text updates. Version 3: Author addition and update of author contributions, increase font on 2 figures, minor text updates",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2504.17732",
    "title": "DPMambaIR: All-in-One Image Restoration via Degradation-Aware Prompt State Space Model",
    "authors": [
      "Zhanwen Liu",
      "Sai Zhou",
      "Yuchao Dai",
      "Yang Wang",
      "Yisheng An",
      "Xiangmo Zhao"
    ],
    "abstract": "All-in-One image restoration aims to address multiple image degradation problems using a single model, offering a more practical and versatile solution compared to designing dedicated models for each degradation type. Existing approaches typically rely on Degradation-specific models or coarse-grained degradation prompts to guide image restoration. However, they lack fine-grained modeling of degradation information and face limitations in balancing multi-task conflicts. To overcome these limitations, we propose DPMambaIR, a novel All-in-One image restoration framework that introduces a fine-grained degradation extractor and a Degradation-Aware Prompt State Space Model (DP-SSM). The DP-SSM leverages the fine-grained degradation features captured by the extractor as dynamic prompts, which are then incorporated into the state space modeling process. This enhances the model's adaptability to diverse degradation types, while a complementary High-Frequency Enhancement Block (HEB) recovers local high-frequency details. Extensive experiments on a mixed dataset containing seven degradation types show that DPMambaIR achieves the best performance, with 27.69dB and 0.893 in PSNR and SSIM, respectively. These results highlight the potential and superiority of DPMambaIR as a unified solution for All-in-One image restoration.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.17732.pdf",
    "abs_url": "https://arxiv.org/abs/2504.17732",
    "published": "2025-04-24T16:46:32Z",
    "updated": "2026-02-05T09:06:01Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2504.15371",
    "title": "Event2Vec: Processing Neuromorphic Events Directly by Representations in Vector Space",
    "authors": [
      "Wei Fang",
      "Priyadarshini Panda"
    ],
    "abstract": "Neuromorphic event cameras possess superior temporal resolution, power efficiency, and dynamic range compared to traditional cameras. However, their asynchronous and sparse data format poses a significant challenge for conventional deep learning methods. Existing methods either convert the events into dense synchronous frame representations for processing by powerful CNNs or Transformers, but lose the asynchronous, sparse and high temporal resolution characteristics of events during the conversion process; or adopt irregular models such as sparse convolution, spiking neural networks, or graph neural networks to process the irregular event representations but fail to take full advantage of GPU acceleration. Inspired by word-to-vector models, we draw an analogy between words and events to introduce event2vec, a novel representation that allows neural networks to process events directly. This approach is fully compatible with the parallel processing capabilities of Transformers. We demonstrate the effectiveness of event2vec on the DVS Gesture, ASL-DVS, and DVS-Lip benchmarks, showing that event2vec is remarkably parameter-efficient, features high throughput and low latency, and achieves high accuracy even with an extremely low number of events or low spatial resolutions. Event2vec introduces a novel paradigm by demonstrating for the first time that sparse, irregular event data can be directly integrated into high-throughput Transformer architectures. This breakthrough resolves the long-standing conflict between maintaining data sparsity and maximizing GPU efficiency, offering a promising balance for real-time, low-latency neuromorphic vision tasks. The code is provided in https://github.com/Intelligent-Computing-Lab-Panda/event2vec.",
    "categories": [
      "cs.CV",
      "cs.NE"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.15371.pdf",
    "abs_url": "https://arxiv.org/abs/2504.15371",
    "published": "2025-04-21T18:21:18Z",
    "updated": "2026-02-05T15:47:39Z",
    "comment": "Fix a minor error in the abstract within the metadata of the previous version",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2504.15281",
    "title": "StyleMe3D: Stylization with Disentangled Priors by Multiple Encoders on 3D Gaussians",
    "authors": [
      "Cailin Zhuang",
      "Yaoqi Hu",
      "Xuanyang Zhang",
      "Wei Cheng",
      "Jiacheng Bao",
      "Shengqi Liu",
      "Yiying Yang",
      "Xianfang Zeng",
      "Gang Yu",
      "Ming Li"
    ],
    "abstract": "Current 3D Gaussian Splatting stylization approaches are limited in their ability to represent diverse artistic styles, frequently defaulting to low-level texture replacement or yielding semantically inconsistent outputs. In this paper, we introduce StyleMe3D, a novel hierarchical framework that achieves comprehensive, high-fidelity stylization by disentangling multi-level style representations while preserving geometric fidelity. The cornerstone of StyleMe3D is Dynamic Style Score Distillation (DSSD), which harnesses latent priors from a style-aware diffusion model to provide high-level semantic guidance, ensuring robust and expressive style transfer. To further refine this distillation process, we propose a multi-modal alignment strategy using the CLIP latent space: a CLIP-based style stream evaluator (Contrastive Style Descriptor) that enforces middle-level stylistic similarity, and a CLIP-based content stream evaluator (3D Gaussian Quality Assessment) that acts as a global regularizer to mitigate typical GS quality degradation. Finally, a VGG-based Simultaneously Optimized Scale module is integrated to refine fine-grained texture details at the low-level. Extensive experiments demonstrate that our method consistently preserves intricate geometric details and achieves coherent stylistic effects across entire scenes, significantly surpassing state-of-the-art baselines in both qualitative and quantitative evaluations.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.15281.pdf",
    "abs_url": "https://arxiv.org/abs/2504.15281",
    "published": "2025-04-21T17:59:55Z",
    "updated": "2026-02-05T04:48:49Z",
    "comment": "18 pages; Project page: https://styleme3d.github.io/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2504.10829",
    "title": "LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation",
    "authors": [
      "Hengyu Shi",
      "Junhao Su",
      "Tianyang Han",
      "Junfeng Luo",
      "Jialin Gao"
    ],
    "abstract": "Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2504.10829.pdf",
    "abs_url": "https://arxiv.org/abs/2504.10829",
    "published": "2025-04-15T03:12:01Z",
    "updated": "2026-02-05T07:47:50Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2504.07053",
    "title": "TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling",
    "authors": [
      "Liang-Hsuan Tseng",
      "Yi-Chang Chen",
      "Kuan-Yi Lee",
      "Da-Shan Shiu",
      "Hung-yi Lee"
    ],
    "abstract": "Recent efforts target spoken language models (SLMs) that not only listen but also speak for more natural human-LLM interaction. Joint speech-text modeling is a promising direction to achieve this. However, the effectiveness of recent speech tokens for joint modeling remains underexplored. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through a attention-based aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. With TASTE, we perform straightforward joint spoken language modeling by using Low-Rank Adaptation on the pre-trained text LLM. Experimental results show that TASTE-based SLMs perform comparable to previous work on SALMON and StoryCloze; while significantly outperform other pre-trained SLMs on speech continuation across subjective and objective evaluations. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and model are available at https://mtkresearch.github.io/TASTE-SpokenLM.github.io.",
    "categories": [
      "cs.CL",
      "cs.SD",
      "eess.AS"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2504.07053.pdf",
    "abs_url": "https://arxiv.org/abs/2504.07053",
    "published": "2025-04-09T17:14:33Z",
    "updated": "2026-02-05T17:31:01Z",
    "comment": "ICLR 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2503.21843",
    "title": "CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity Recognition",
    "authors": [
      "Ying Yu",
      "Siyao Li",
      "Yixuan Jiang",
      "Hang Xiao",
      "Jingxi Long",
      "Haotian Tang",
      "Hanyu Liu",
      "Chao Li"
    ],
    "abstract": "Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.21843.pdf",
    "abs_url": "https://arxiv.org/abs/2503.21843",
    "published": "2025-03-27T15:21:49Z",
    "updated": "2026-02-05T14:29:31Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2503.16252",
    "title": "Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning",
    "authors": [
      "Zhaowei Liu",
      "Xin Guo",
      "Zhi Yang",
      "Fangqi Lou",
      "Lingfeng Zeng",
      "Mengping Li",
      "Qi Qi",
      "Zhiqiang Liu",
      "Yiyang Han",
      "Dongpo Cheng",
      "Ronghao Chen",
      "Huacan Wang",
      "Xingdong Feng",
      "Huixia Judy Wang",
      "Chengchun Shi",
      "Liwen Zhang"
    ],
    "abstract": "In recent years, general-purpose large language models (LLMs) such as GPT, Gemini, Claude, and DeepSeek have advanced at an unprecedented pace. Despite these achievements, their application to finance remains challenging, due to fragmented data sources, intransparent reasoning processes, and weak transferability to business applications. In response, we introduce Fin-R1, a reasoning LLM designed for financial scenarios. With a compact size of 7 billion parameters, Fin-R1 reduces deployment costs while addressing the aforementioned challenges. Its development follows a two-stage pipeline. First, we construct Fin-R1-Data, a high-quality financial dataset consisting of 60,091 chain-of-thought (CoT) samples, distilled and filtered from multiple authoritative benchmarks to ensure consistency and reliability. Second, we train Fin-R1 using Fin-R1-Data through supervised fine-tuning (SFT), followed by reinforcement learning (RL). This stage substantially improves the model's ability to solve complex financial reasoning tasks, yielding outputs that are both accurate and interpretable. Despite its relatively small parameter scale, Fin-R1 achieves competitive empirical performance across established financial benchmarks and demonstrates practical utility in compliance checking and robo-advisory. Our code is publicly available at https://github.com/SUFE-AIFLM-Lab/Fin-R1, and has already attracted over 700 stars.",
    "categories": [
      "cs.CL"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2503.16252.pdf",
    "abs_url": "https://arxiv.org/abs/2503.16252",
    "published": "2025-03-20T15:46:18Z",
    "updated": "2026-02-05T07:37:48Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2503.07346",
    "title": "Hidden in Plain Sight -- Class Competition Focuses Attribution Maps",
    "authors": [
      "Nils Philipp Walter",
      "Jilles Vreeken",
      "Jonas Fischer"
    ],
    "abstract": "Attribution methods reveal which input features a neural network uses for a prediction, adding transparency to their decisions. A common problem is that these attributions seem unspecific, highlighting both important and irrelevant features. We revisit the common attribution pipeline and observe that using logits as attribution target is a main cause of this phenomenon. We show that the solution is in plain sight: considering distributions of attributions over multiple classes using existing attribution methods yields specific and fine-grained attributions. On common benchmarks, including the grid-pointing game and randomization-based sanity checks, this improves the ability of 18 attribution methods across 7 architectures up to 2x, agnostic to model architecture.",
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.07346.pdf",
    "abs_url": "https://arxiv.org/abs/2503.07346",
    "published": "2025-03-10T13:59:57Z",
    "updated": "2026-02-05T16:34:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2503.06749",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "authors": [
      "Wenxuan Huang",
      "Bohan Jia",
      "Zijie Zhai",
      "Shaosheng Cao",
      "Zheyu Ye",
      "Fei Zhao",
      "Zhe Xu",
      "Yao Hu",
      "Shaohui Lin"
    ],
    "abstract": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. Scaling up the amount of multimodal math data in the RL training, Vision-R1-32B and Vison-R1-72B achieves 76.4% and 78.2% MathVista benchmark scores, respectively. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2503.06749.pdf",
    "abs_url": "https://arxiv.org/abs/2503.06749",
    "published": "2025-03-09T20:06:45Z",
    "updated": "2026-02-05T16:21:33Z",
    "comment": "Accepted to ICLR 2026. Code is available at https://github.com/Osilly/Vision-R1",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.21086",
    "title": "Are foundation models useful feature extractors for electroencephalography analysis?",
    "authors": [
      "ÃzgÃ¼n Turgut",
      "Felix S. Bott",
      "Markus Ploner",
      "Daniel Rueckert"
    ],
    "abstract": "The success of foundation models in natural language processing and computer vision has motivated similar approaches in time series analysis. While foundational time series models have proven beneficial on a variety of tasks, their effectiveness in medical applications with limited data remains underexplored. In this work, we investigate this question in the context of electroencephalography (EEG) by evaluating general-purpose time series models on age prediction, seizure detection, and classification of clinically relevant EEG events. We compare their diagnostic performance against specialised EEG models and assess the quality of the extracted features. The results show that general-purpose models are competitive and capture features useful to localising demographic and disease-related biomarkers. These findings indicate that foundational time series models can reduce the reliance on large task-specific datasets and models, making them valuable in clinical practice.",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2502.21086.pdf",
    "abs_url": "https://arxiv.org/abs/2502.21086",
    "published": "2025-02-28T14:21:34Z",
    "updated": "2026-02-05T09:50:03Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.15798",
    "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
    "authors": [
      "Yuxuan Zhou",
      "Heng Li",
      "Zhi-Qi Cheng",
      "Xudong Yan",
      "Yifei Dong",
      "Mario Fritz",
      "Margret Keuper"
    ],
    "abstract": "Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.15798.pdf",
    "abs_url": "https://arxiv.org/abs/2502.15798",
    "published": "2025-02-18T20:10:34Z",
    "updated": "2026-02-05T09:10:42Z",
    "comment": "NeurIPS 2025 Oral (0.36% acceptance); code: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2503.04773",
    "title": "Invisible Walls in Cities: Designing LLM Agent to Predict Urban Segregation Experience with Social Media Content",
    "authors": [
      "Bingbing Fan",
      "Lin Chen",
      "Songwei Li",
      "Jian Yuan",
      "Fengli Xu",
      "Pan Hui",
      "Yong Li"
    ],
    "abstract": "Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose a novel Large Language Model (LLM) agent to automate online review mining for segregation prediction. Specifically, we propose a reflective LLM coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction.   Experiments on real-world data demonstrate that our agent substantially improves prediction accuracy, with a 22.79% elevation in R$^{2}$ and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving places of interest (POIs)' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with Web technology.",
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.SI"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2503.04773.pdf",
    "abs_url": "https://arxiv.org/abs/2503.04773",
    "published": "2025-02-17T09:52:17Z",
    "updated": "2026-02-05T06:07:03Z",
    "comment": "11 pages, 6 figures. This paper has been accepted at The ACM Web Conference 2026",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.10311",
    "title": "ExplainReduce: Generating global explanations from many local explanations",
    "authors": [
      "Lauri SeppÃ¤lÃ¤inen",
      "Mudong Guo",
      "Kai PuolamÃ¤ki"
    ],
    "abstract": "Most commonly used non-linear machine learning methods are closed-box models, uninterpretable to humans. The field of explainable artificial intelligence (XAI) aims to develop tools to examine the inner workings of these closed boxes. An often-used model-agnostic approach to XAI involves using simple models as local approximations to produce so-called local explanations; examples of this approach include LIME, SHAP, and SLISEMAP. This paper shows how a large set of local explanations can be reduced to a small \"proxy set\" of simple models, which can act as a generative global explanation. This reduction procedure, ExplainReduce, can be formulated as an optimisation problem and approximated efficiently using greedy heuristics. We show that, for many problems, as few as five explanations can faithfully emulate the closed-box model and that our reduction procedure is competitive with other model aggregation methods.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.10311.pdf",
    "abs_url": "https://arxiv.org/abs/2502.10311",
    "published": "2025-02-14T17:14:02Z",
    "updated": "2026-02-05T11:52:54Z",
    "comment": "21 pages with a 36 page appendix, 8 + 39 figures, 1+1 tables. The datasets and source code used in the paper are available at https://github.com/edahelsinki/explainreduce",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.08262",
    "title": "GenIAS: Generator for Instantiating Anomalies in time Series",
    "authors": [
      "Zahra Zamanzadeh Darban",
      "Qizhou Wang",
      "Geoffrey I. Webb",
      "Shirui Pan",
      "Charu C. Aggarwal",
      "Mahsa Salehi"
    ],
    "abstract": "Synthetic anomaly injection is a recent and promising approach for time series anomaly detection (TSAD), but existing methods rely on ad hoc, hand-crafted strategies applied to raw time series that fail to capture diverse and complex anomalous patterns, particularly in multivariate settings. We propose a synthetic anomaly generation method named Generator for Instantiating Anomalies in Time Series (GenIAS), which generates realistic and diverse anomalies via a novel learnable perturbation in the latent space of a variational autoencoder. This enables abnormal patterns to be injected across different temporal segments at varying scales based on variational reparameterization. To generate anomalies that align with normal patterns while remaining distinguishable, we introduce a learning strategy that jointly learns the perturbation scale and compact latent representations via a tunable prior, which improves the distinguishability of generated anomalies, as supported by our theoretical analysis. Extensive experiments show that GenIAS produces more diverse and realistic anomalies, and that detection models trained with these anomalies outperform 17 baseline methods on 9 popular TSAD benchmarks.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.08262.pdf",
    "abs_url": "https://arxiv.org/abs/2502.08262",
    "published": "2025-02-12T10:10:04Z",
    "updated": "2026-02-05T10:00:30Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.04700",
    "title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference",
    "authors": [
      "Prakhar Kaushik",
      "Ankit Vaidya",
      "Shravan Chaudhari",
      "Alan Yuille"
    ],
    "abstract": "The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.04700.pdf",
    "abs_url": "https://arxiv.org/abs/2502.04700",
    "published": "2025-02-07T07:07:04Z",
    "updated": "2026-02-05T18:59:59Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.01989",
    "title": "VFScale: Intrinsic Reasoning through Verifier-Free Test-time Scalable Diffusion Model",
    "authors": [
      "Tao Zhang",
      "Jia-Shu Pan",
      "Ruiqi Feng",
      "Tailin Wu"
    ],
    "abstract": "Inspired by human SYSTEM 2 thinking, LLMs excel at complex reasoning tasks via extended Chain-of-Thought. However, similar test-time scaling for diffusion models to tackle complex reasoning remains largely unexplored. From existing work, two primary challenges emerge in this setting: (i) the dependence on an external verifier indicating a notable gap from intrinsic reasoning of human intelligence without any external feedback, and (ii) the lack of an efficient search algorithm. In this paper, we introduce the Verifier-free Test-time Scalable Diffusion Model (VFScale) to achieve scalable intrinsic reasoning, which equips number-of-sample test-time scaling with the intrinsic energy function of diffusion models as the verifier. Concretely, VFScale comprises two key innovations to address the aforementioned challenges. On the training side, VFScale consists of a novel MRNCL loss and a KL regularization to improve the energy landscape, ensuring that the learned energy function itself serves as a reliable verifier. On the inference side, VFScale integrates the denoising process with a novel hybrid Monte Carlo Tree Search (hMCTS) to improve search efficiency. On challenging reasoning tasks of Maze and Sudoku, we demonstrate the effectiveness of VFScale's training objective and scalable inference method. In particular, trained with Maze sizes of up to $6\\times6$, our VFScale solves 88% of Maze problems with much larger sizes of $15\\times15$, while standard diffusion models completely fail. The code can be found at https://github.com/AI4Science-WestlakeU/VFScale.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2502.01989.pdf",
    "abs_url": "https://arxiv.org/abs/2502.01989",
    "published": "2025-02-04T04:07:48Z",
    "updated": "2026-02-05T03:12:04Z",
    "comment": "ICLR 2026. 30 pages, 13 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2502.01411",
    "title": "Human Body Restoration with One-Step Diffusion Model and A New Benchmark",
    "authors": [
      "Jue Gong",
      "Jingkai Wang",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yulun Zhang",
      "Xiaokang Yang"
    ],
    "abstract": "Human body restoration, as a specific application of image restoration, is widely applied in practice and plays a vital role across diverse fields. However, thorough research remains difficult, particularly due to the lack of benchmark datasets. In this study, we propose a high-quality dataset automated cropping and filtering (HQ-ACF) pipeline. This pipeline leverages existing object detection datasets and other unlabeled images to automatically crop and filter high-quality human images. Using this pipeline, we constructed a person-based restoration with sophisticated objects and natural activities (\\emph{PERSONA}) dataset, which includes training, validation, and test sets. The dataset significantly surpasses other human-related datasets in both quality and content richness. Finally, we propose \\emph{OSDHuman}, a novel one-step diffusion model for human body restoration. Specifically, we propose a high-fidelity image embedder (HFIE) as the prompt generator to better guide the model with low-quality human image information, effectively avoiding misleading prompts. Experimental results show that OSDHuman outperforms existing methods in both visual quality and quantitative metrics. The dataset and code will at https://github.com/gobunu/OSDHuman.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2502.01411.pdf",
    "abs_url": "https://arxiv.org/abs/2502.01411",
    "published": "2025-02-03T14:48:40Z",
    "updated": "2026-02-05T05:29:27Z",
    "comment": "8 pages, 9 figures. Accepted at ICML 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2412.19755",
    "title": "Can MLLMs generate human-like feedback in grading multimodal short answers?",
    "authors": [
      "Pritam Sil",
      "Pushpak Bhattacharyya",
      "Pawan Goyal",
      "Ganesh Ramakrishnan"
    ],
    "abstract": "In education, the traditional Automatic Short Answer Grading (ASAG) with feedback problem has focused primarily on evaluating text-only responses. However, real-world assessments often include multimodal responses containing both diagrams and text. To address this limitation, we introduce the Multimodal Short Answer Grading with Feedback (MMSAF) problem, which requires jointly evaluating textual and diagrammatic content while also providing explanatory feedback. Collecting data representative of such multimodal responses is challenging due to both scale and logistical constraints. To mitigate this, we develop an automated data generation framework that leverages LLM hallucinations to mimic common student errors, thereby constructing a dataset of 2,197 instances. We evaluate 4 Multimodal Large Language Models (MLLMs) across 3 STEM subjects, showing that MLLMs achieve accuracies of up to 62.5% in predicting answer correctness (correct/partially correct/incorrect) and up to 80.36% in assessing image relevance. This also includes a human evaluation with 9 annotators across 5 parameters, including a rubric-based approach. The rubrics also serve as a way to evaluate the feedback quality semantically rather than using overlap-based approaches. Our findings highlight which MLLMs are better suited for such tasks while also pointing out to drawbacks of the remaining MLLMs.",
    "categories": [
      "cs.AI"
    ],
    "primary_category": "cs.AI",
    "pdf_url": "https://arxiv.org/pdf/2412.19755.pdf",
    "abs_url": "https://arxiv.org/abs/2412.19755",
    "published": "2024-12-27T17:33:39Z",
    "updated": "2026-02-05T15:02:21Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2412.14865",
    "title": "Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning",
    "authors": [
      "Anthony Kobanda",
      "RÃ©my Portelas",
      "Odalric-Ambrym Maillard",
      "Ludovic Denoyer"
    ],
    "abstract": "We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2412.14865.pdf",
    "abs_url": "https://arxiv.org/abs/2412.14865",
    "published": "2024-12-19T14:00:03Z",
    "updated": "2026-02-05T13:36:27Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2412.09191",
    "title": "RAD: Region-Aware Diffusion Models for Image Inpainting",
    "authors": [
      "Sora Kim",
      "Sungho Suh",
      "Minsik Lee"
    ],
    "abstract": "Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains. Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, \\ie, conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2412.09191.pdf",
    "abs_url": "https://arxiv.org/abs/2412.09191",
    "published": "2024-12-12T11:38:46Z",
    "updated": "2026-02-05T04:17:52Z",
    "comment": "Code: https://github.com/srk1995/RAD",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2412.08419",
    "title": "Energy Guided smoothness to improve Robustness in Graph Classification",
    "authors": [
      "Farooq Ahmad Wani",
      "Maria Sofia Bucarelli",
      "Andrea Giuseppe Di Francesco",
      "Oleksandr Pryymak",
      "Fabrizio Silvestri"
    ],
    "abstract": "Graph Neural Networks (GNNs) are powerful at solving graph classification tasks, yet applied problems often contain noisy labels. In this work, we study GNN robustness to label noise, demonstrate GNN failure modes when models struggle to generalise on low-order graphs, low label coverage, or when a model is over-parameterized. We establish both empirical and theoretical links between GNN robustness and the reduction of the total Dirichlet Energy of learned node representations, which encapsulates the hypothesized GNN smoothness inductive bias. Finally, we introduce two training strategies to enhance GNN robustness: (1) by incorporating a novel inductive bias in the weight matrices through the removal of negative eigenvalues, connected to Dirichlet Energy minimization; (2) by extending to GNNs a loss penalty that promotes learned smoothness. Importantly, neither approach negatively impacts performance in noise-free settings, supporting our hypothesis that the source of GNNs robustness is their smoothness inductive bias.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2412.08419.pdf",
    "abs_url": "https://arxiv.org/abs/2412.08419",
    "published": "2024-12-11T14:35:37Z",
    "updated": "2026-02-05T17:56:08Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2411.12788",
    "title": "Efficient Scene Modeling via Structure-Aware and Region-Prioritized 3D Gaussians",
    "authors": [
      "Guangchi Fang",
      "Bing Wang"
    ],
    "abstract": "Reconstructing 3D scenes with high fidelity and efficiency remains a central pursuit in computer vision and graphics. Recent advances in 3D Gaussian Splatting (3DGS) enable photorealistic rendering with Gaussian primitives, yet the modeling process remains governed predominantly by photometric supervision. This reliance often leads to irregular spatial distribution and indiscriminate primitive adjustments that largely ignore underlying geometric context. In this work, we rethink Gaussian modeling from a geometric standpoint and introduce Mini-Splatting2, an efficient scene modeling framework that couples structure-aware distribution and region-prioritized optimization, driving 3DGS into a geometry-regulated paradigm. The structure-aware distribution enforces spatial regularity through structured reorganization and representation sparsity, ensuring balanced structural coverage for compact organization. The region-prioritized optimization improves training discrimination through geometric saliency and computational selectivity, fostering appropriate structural emergence for fast convergence. These mechanisms alleviate the long-standing tension among representation compactness, convergence acceleration, and rendering fidelity. Extensive experiments demonstrate that Mini-Splatting2 achieves up to 4$\\times$ fewer Gaussians and 3$\\times$ faster optimization while maintaining state-of-the-art visual quality, paving the way towards structured and efficient 3D Gaussian modeling.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2411.12788.pdf",
    "abs_url": "https://arxiv.org/abs/2411.12788",
    "published": "2024-11-19T11:47:40Z",
    "updated": "2026-02-05T13:49:34Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2411.00918",
    "title": "LIBMoE: A Library for comprehensive benchmarking Mixture of Experts in Large Language Models",
    "authors": [
      "Nam V. Nguyen",
      "Thong T. Doan",
      "Luong Tran",
      "Van Nguyen",
      "Quang Pham"
    ],
    "abstract": "Mixture of experts (MoE) architectures have become a cornerstone for scaling up and are a key component in most large language models such as GPT-OSS, DeepSeek-V3, Llama-4, and Gemini-2.5. However, systematic research on MoE remains severely constrained by the prohibitive computational costs of training and evaluation, restricting large-scale studies accessible to most researchers. We introduce LibMoE, a unified framework for reproducible, efficient, and extensible MoE research that supports both pretraining and sparse-upcycling regimes. Beyond unified implementations, the framework provides transparent analytical tools for probing routing and expert dynamics. Leveraging this foundation, we conduct a comprehensive analysis along three dimensions: (i) routing dynamics, covering expert selection patterns, routing stability and optimality, and how routing entropy reveals task specialization and expert diversity; (ii) the effect of lightweight initialization on load balancing, demonstrating how subtle changes in router initialization shape early expert utilization; and (iii) training regime differences, revealing how sparse upcycling and full pretraining exhibit distinct routing patterns and stability profiles. By lowering the barrier to entry and standardizing evaluation, along with our comprehensive analysis, LibMoE broadens access to MoE research and establishes a reliable benchmark to guide future innovations. GitHub: \\href{https://github.com/Fsoft-AIC/LibMoE}{https://github.com/Fsoft-AIC/LibMoE}.",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "primary_category": "cs.CL",
    "pdf_url": "https://arxiv.org/pdf/2411.00918.pdf",
    "abs_url": "https://arxiv.org/abs/2411.00918",
    "published": "2024-11-01T14:04:36Z",
    "updated": "2026-02-05T10:16:56Z",
    "comment": "15 pages, 9 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2410.13431",
    "title": "Solving Prior Distribution Mismatch in Diffusion Models via Optimal Transport",
    "authors": [
      "Zhanpeng Wang",
      "Shenghao Li",
      "Jiameng Che",
      "Chen Wang",
      "Shangling Jui",
      "Na Lei",
      "Zhongxuan Luo"
    ],
    "abstract": "Diffusion Models (DMs) have achieved remarkable progress in generative modeling. However, the mismatch between the forward terminal distribution and reverse initial distribution introduces prior error, leading to deviations of sampling trajectories from the true distribution and severely limiting model performance. This issue further triggers cascading problems, including non-zero Signal-to-Noise Ratio, accumulated denoising errors, degraded generation quality, and constrained sampling efficiency. To address this issue, this paper proposes a prior error elimination framework based on Optimal Transport (OT). Specifically, an OT map from the reverse initial distribution to the forward terminal distribution is constructed to achieve precise matching of the two distributions. Meanwhile, the upper bound of the prior error is quantified using the Wasserstein distance, proving that the prior error can be effectively eliminated via the OT map. Additionally, by deriving the asymptotic consistency between dynamic OT and probability flow, this method is revealed to be highly compatible with the intrinsic mechanism of the diffusion process. Experimental results demonstrate that the proposed method completely eliminates the prior error both theoretically and practically, providing a universal and rigorous solution for optimizing the performance of DMs.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.13431.pdf",
    "abs_url": "https://arxiv.org/abs/2410.13431",
    "published": "2024-10-17T10:54:55Z",
    "updated": "2026-02-05T15:52:57Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2410.04560",
    "title": "GAMformer: Bridging Tabular Foundation Models and Interpretable Machine Learning",
    "authors": [
      "Andreas Mueller",
      "Julien Siems",
      "Harsha Nori",
      "David Salinas",
      "Arber Zela",
      "Rich Caruana",
      "Frank Hutter"
    ],
    "abstract": "While interpretability is crucial for machine learning applications in safety-critical domains and for regulatory compliance, existing tabular foundation models like TabPFN lack transparency. Generalized Additive Models (GAMs) provide the needed interpretability through their additive structure, but traditional GAM methods rely on iterative learning algorithms (such as splines, boosted trees, or neural networks) that are fundamentally incompatible with the in-context learning paradigm of foundation models. In this paper, we introduce GAMformer, the first tabular foundation model for GAMs that bridges the gap between the power of foundation models and the interpretability requirements of critical real-world applications. GAMformer estimates GAM shape functions in a single forward pass using in-context learning, representing a significant departure from conventional iterative approaches. Building on previous research on tabular foundation models, we train GAMformer exclusively on synthetically generated tables to prevent data leakage. Our experiments demonstrate that GAMformer performs comparably to other leading GAMs across various classification benchmarks.",
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.04560.pdf",
    "abs_url": "https://arxiv.org/abs/2410.04560",
    "published": "2024-10-06T17:28:20Z",
    "updated": "2026-02-05T12:56:21Z",
    "comment": "22 pages, 15 figures",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2410.03159",
    "title": "WAVE: Weighted Autoregressive Varying Gate for Time Series Forecasting",
    "authors": [
      "Jiecheng Lu",
      "Xu Han",
      "Yan Sun",
      "Shihao Yang"
    ],
    "abstract": "We propose a Weighted Autoregressive Varying gatE (WAVE) attention mechanism equipped with both Autoregressive (AR) and Moving-average (MA) components. It can adapt to various attention mechanisms, enhancing and decoupling their ability to capture long-range and local temporal patterns in time series data. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that WAVE attention that incorporates the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2410.03159.pdf",
    "abs_url": "https://arxiv.org/abs/2410.03159",
    "published": "2024-10-04T05:45:50Z",
    "updated": "2026-02-05T05:34:29Z",
    "comment": "Camera-ready version. Accepted at ICML 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2410.02103",
    "title": "MVGS: Multi-view Regulated Gaussian Splatting for Novel View Synthesis",
    "authors": [
      "Xiaobiao Du",
      "Yida Wang",
      "Xin Yu"
    ],
    "abstract": "Recent works in volume rendering, \\textit{e.g.} NeRF and 3D Gaussian Splatting (3DGS), significantly advance the rendering quality and efficiency with the help of the learned implicit neural radiance field or 3D Gaussians. Rendering on top of an explicit representation, the vanilla 3DGS and its variants deliver real-time efficiency by optimizing the parametric model with single-view supervision per iteration during training which is adopted from NeRF. Consequently, certain views are overfitted, leading to unsatisfying appearance in novel-view synthesis and imprecise 3D geometries. To solve aforementioned problems, we propose a new 3DGS optimization method embodying four key novel contributions: 1) We transform the conventional single-view training paradigm into a multi-view training strategy. With our proposed multi-view regulation, 3D Gaussian attributes are further optimized without overfitting certain training views. As a general solution, we improve the overall accuracy in a variety of scenarios and different Gaussian variants. 2) Inspired by the benefit introduced by additional views, we further propose a cross-intrinsic guidance scheme, leading to a coarse-to-fine training procedure concerning different resolutions. 3) Built on top of our multi-view regulated training, we further propose a cross-ray densification strategy, densifying more Gaussian kernels in the ray-intersect regions from a selection of views. 4) By further investigating the densification strategy, we found that the effect of densification should be enhanced when certain views are distinct dramatically. As a solution, we propose a novel multi-view augmented densification strategy, where 3D Gaussians are encouraged to get densified to a sufficient number accordingly, resulting in improved reconstruction accuracy.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2410.02103.pdf",
    "abs_url": "https://arxiv.org/abs/2410.02103",
    "published": "2024-10-02T23:48:31Z",
    "updated": "2026-02-05T07:22:17Z",
    "comment": "Project Page:https://xiaobiaodu.github.io/mvgs-project/",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2409.15176",
    "title": "SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream",
    "authors": [
      "Jinze Yu",
      "Xin Peng",
      "Zhengda Lu",
      "Laurent Kneip",
      "Yiqun Wang"
    ],
    "abstract": "A spike camera is a specialized high-speed visual sensor that offers advantages such as high temporal resolution and high dynamic range compared to conventional frame cameras. These features provide the camera with significant advantages in many computer vision tasks. However, the tasks of novel view synthesis based on spike cameras remain underdeveloped. Although there are existing methods for learning neural radiance fields from spike stream, they either lack robustness in extremely noisy, low-quality lighting conditions or suffer from high computational complexity due to the deep fully connected neural networks and ray marching rendering strategies used in neural radiance fields, making it difficult to recover fine texture details. In contrast, the latest advancements in 3DGS have achieved high-quality real-time rendering by optimizing the point cloud representation into Gaussian ellipsoids. Building on this, we introduce SpikeGS, the method to learn 3D Gaussian fields solely from spike stream. We designed a differentiable spike stream rendering framework based on 3DGS, incorporating noise embedding and spiking neurons. By leveraging the multi-view consistency of 3DGS and the tile-based multi-threaded parallel rendering mechanism, we achieved high-quality real-time rendering results. Additionally, we introduced a spike rendering loss function that generalizes under varying illumination conditions. Our method can reconstruct view synthesis results with fine texture details from a continuous spike stream captured by a moving spike camera, while demonstrating high robustness in extremely noisy low-light scenarios. Experimental results on both real and synthetic datasets demonstrate that our method surpasses existing approaches in terms of rendering quality and speed.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2409.15176.pdf",
    "abs_url": "https://arxiv.org/abs/2409.15176",
    "published": "2024-09-23T16:28:41Z",
    "updated": "2026-02-05T03:55:30Z",
    "comment": "Accepted by ACCV 2024",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2409.12636",
    "title": "Image inpainting for corrupted images by using the semi-super resolution GAN",
    "authors": [
      "Mehrshad Momen-Tayefeh",
      "Mehrdad Momen-Tayefeh",
      "Amir Ali Ghafourian Ghahramani"
    ],
    "abstract": "Image inpainting is a valuable technique for enhancing images that have been corrupted. The primary challenge in this research revolves around the extent of corruption in the input image that the deep learning model must restore. To address this challenge, we introduce a Generative Adversarial Network (GAN) for learning and replicating the missing pixels. Additionally, we have developed a distinct variant of the Super-Resolution GAN (SRGAN), which we refer to as the Semi-SRGAN (SSRGAN). Furthermore, we leveraged three diverse datasets to assess the robustness and accuracy of our proposed model. Our training process involves varying levels of pixel corruption to attain optimal accuracy and generate high-quality images.",
    "categories": [
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2409.12636.pdf",
    "abs_url": "https://arxiv.org/abs/2409.12636",
    "published": "2024-09-19T10:21:16Z",
    "updated": "2026-02-05T07:25:37Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2409.07253",
    "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
    "authors": [
      "Buhua Liu",
      "Shitong Shao",
      "Bao Li",
      "Lichen Bai",
      "Zhiqiang Xu",
      "Haoyi Xiong",
      "James Kwok",
      "Sumi Helal",
      "Zeke Xie"
    ],
    "abstract": "Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions and generate results with undesired properties or even harmful content. Inspired by the success and popularity of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.",
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2409.07253.pdf",
    "abs_url": "https://arxiv.org/abs/2409.07253",
    "published": "2024-09-11T13:21:32Z",
    "updated": "2026-02-05T05:52:23Z",
    "comment": "Accepted at ACM Computing Surveys. 35 pages, 5 figures, 4 tables. Paper List: github.com/xie-lab-ml/awesome-alignment-of-diffusion-models",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2406.17835",
    "title": "The Use of AI-Robotic Systems for Scientific Discovery",
    "authors": [
      "Alexander H. Gower",
      "Konstantin Korovin",
      "Daniel BrunnsÃ¥ker",
      "Filip KronstrÃ¶m",
      "Gabriel K. Reder",
      "Ievgeniia A. Tiukova",
      "Ronald S. Reiserer",
      "John P. Wikswo",
      "Ross D. King"
    ],
    "abstract": "The process of developing theories and models and testing them with experiments is fundamental to the scientific method. Automating the entire scientific method then requires not only automation of the induction of theories from data, but also experimentation from design to implementation. This is the idea behind a robot scientist -- a coupled system of AI and laboratory robotics that has agency to test hypotheses with real-world experiments. In this chapter we explore some of the fundamentals of robot scientists in the philosophy of science. We also map the activities of a robot scientist to machine learning paradigms, and argue that the scientific method shares an analogy with active learning. We demonstrate these concepts using examples from previous robot scientists, and also from Genesis: a next generation robot scientist designed for research in systems biology, comprising a micro-fluidic system with 1000 computer-controlled micro-bioreactors and interpretable models based in controlled vocabularies and logic.",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2406.17835.pdf",
    "abs_url": "https://arxiv.org/abs/2406.17835",
    "published": "2024-06-25T15:33:01Z",
    "updated": "2026-02-05T12:29:48Z",
    "comment": "23 pages, book chapter",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2406.09643",
    "title": "A Policy Gradient-Based Sequence-to-Sequence Method for Time Series Prediction",
    "authors": [
      "Qi Sima",
      "Xinze Zhang",
      "Yukun Bao",
      "Siyue Yang",
      "Liang Shen"
    ],
    "abstract": "Sequence-to-sequence architectures built upon recurrent neural networks have become a standard choice for multi-step-ahead time series prediction. In these models, the decoder produces future values conditioned on contextual inputs, typically either actual historical observations (ground truth) or previously generated predictions. During training, feeding ground-truth values helps stabilize learning but creates a mismatch between training and inference conditions, known as exposure bias, since such true values are inaccessible during real-world deployment. On the other hand, using the model's own outputs as inputs at test time often causes errors to compound rapidly across prediction steps. To mitigate these limitations, we introduce a new training paradigm grounded in reinforcement learning: a policy gradient-based method to learn an adaptive input selection strategy for sequence-to-sequence prediction models. Auxiliary models first synthesize plausible input candidates for the decoder, and a trainable policy network optimized via policy gradients dynamically chooses the most beneficial inputs to maximize long-term prediction performance. Empirical evaluations on diverse time series datasets confirm that our approach enhances both accuracy and stability in multi-step forecasting compared to conventional methods.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2406.09643.pdf",
    "abs_url": "https://arxiv.org/abs/2406.09643",
    "published": "2024-06-14T00:24:29Z",
    "updated": "2026-02-05T15:42:52Z",
    "comment": null,
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2406.04897",
    "title": "From Link Prediction to Forecasting: Addressing Challenges in Batch-based Temporal Graph Learning",
    "authors": [
      "Moritz Lampert",
      "Christopher BlÃ¶cker",
      "Ingo Scholtes"
    ],
    "abstract": "Dynamic link prediction is an important problem considered in many recent works that propose approaches for learning temporal edge patterns. To assess their efficacy, models are evaluated on continuous-time and discrete-time temporal graph datasets, typically using a traditional batch-oriented evaluation setup. However, as we show in this work, a batch-oriented evaluation is often unsuitable and can cause several issues. Grouping edges into fixed-sized batches regardless of their occurrence time leads to information loss or leakage, depending on the temporal granularity of the data. Furthermore, fixed-size batches create time windows with different durations, resulting in an inconsistent dynamic link prediction task. In this work, we empirically show how traditional batch-based evaluation leads to skewed model performance and hinders the fair comparison of methods. We mitigate this problem by reformulating dynamic link prediction as a link forecasting task that better accounts for temporal information present in the data.",
    "categories": [
      "cs.LG"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2406.04897.pdf",
    "abs_url": "https://arxiv.org/abs/2406.04897",
    "published": "2024-06-07T12:45:12Z",
    "updated": "2026-02-05T10:21:15Z",
    "comment": "46 pages (12 pages main text), 19 figures. Published in Transactions on Machine Learning Research (2026)",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2405.14982",
    "title": "In-context Time Series Predictor",
    "authors": [
      "Jiecheng Lu",
      "Yan Sun",
      "Shihao Yang"
    ],
    "abstract": "Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate \"time series forecasting tasks\" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms, and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2405.14982.pdf",
    "abs_url": "https://arxiv.org/abs/2405.14982",
    "published": "2024-05-23T18:37:00Z",
    "updated": "2026-02-05T05:55:51Z",
    "comment": "Camera-ready version. Accepted at ICLR 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2404.15617",
    "title": "A Differential and Pointwise Control Approach to Reinforcement Learning",
    "authors": [
      "Minh Nguyen",
      "Chandrajit Bajaj"
    ],
    "abstract": "Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $\\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions.",
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "math.ST"
    ],
    "primary_category": "cs.LG",
    "pdf_url": "https://arxiv.org/pdf/2404.15617.pdf",
    "abs_url": "https://arxiv.org/abs/2404.15617",
    "published": "2024-04-24T03:11:12Z",
    "updated": "2026-02-05T05:28:08Z",
    "comment": "NeurIPS 2025",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2403.19254",
    "title": "Imperceptible Protection against Style Imitation from Diffusion Models",
    "authors": [
      "Namhyuk Ahn",
      "Wonhyuk Ahn",
      "KiYoon Yoo",
      "Daesik Kim",
      "Seung-Hun Nam"
    ],
    "abstract": "Recent progress in diffusion models has profoundly enhanced the fidelity of image generation, but it has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we introduce a visually improved protection method while preserving its protection capability. To this end, we devise a perceptual map to highlight areas sensitive to human eyes, guided by instance-aware refinement, which refines the protection intensity accordingly. We also introduce a difficulty-aware protection by predicting how difficult the artwork is to protect and dynamically adjusting the intensity based on this. Lastly, we integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.",
    "categories": [
      "cs.CV"
    ],
    "primary_category": "cs.CV",
    "pdf_url": "https://arxiv.org/pdf/2403.19254.pdf",
    "abs_url": "https://arxiv.org/abs/2403.19254",
    "published": "2024-03-28T09:21:00Z",
    "updated": "2026-02-05T05:55:23Z",
    "comment": "IEEE Transactions on Multimedia",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]