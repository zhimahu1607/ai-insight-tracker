[
  {
    "id": "659d39a5c30e3479",
    "title": "Build an intelligent photo search using Amazon Rekognition, Amazon Neptune, and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-photo-search-using-amazon-rekognition-amazon-neptune-and-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T18:22:26Z",
    "summary": "In this post, we show you how to build a comprehensive photo search system using the AWS Cloud Development Kit (AWS CDK) that integrates Amazon Rekognition for face and object detection, Amazon Neptune for relationship mapping, and Amazon Bedrock for AI-powered captioning.",
    "content": "Managing large photo collections presents significant challenges for organizations and individuals. Traditional approaches rely on manual tagging, basic metadata, and folder-based organization, which can become impractical when dealing with thousands of images containing multiple people and complex relationships. Intelligent photo search systems address these challenges by combining computer vision, graph databases, and natural language processing to transform how we discover and organize visual content. These systems capture not just who and what appears in photos, but the complex relationships and contexts that make them meaningful, enabling natural language queries and semantic discovery.\nIn this post, we show you how to build a comprehensive photo search system using the AWS Cloud Development Kit (AWS CDK) that integrates Amazon Rekognition for face and object detection, Amazon Neptune for relationship mapping, and Amazon Bedrock for AI-powered captioning. We demonstrate how these services work together to create a system that understands natural language queries like “Find all photos of grandparents with their grandchildren at birthday parties” or “Show me pictures of the family car during road trips.”\nThe key benefit is the ability to personalize and customize search focus on specific people, objects, or relationships while scaling to handle thousands of photos and complex family or organizational structures. Our approach demonstrates that integrating Amazon Neptune graph database capabilities with Amazon AI services enables natural language photo search that understands context and relationships, moving beyond simple metadata tagging to intelligent photo discovery. We showcase this through a complete serverless implementation that you can deploy and customize for your specific use case.\nSolution overview\nThis section outlines the technical architecture and workflow of our intelligent photo search system. As illustrated in the following diagram, the solution uses serverless AWS services to create a scalable, cost-effective system that automatically processes photos and enables natural language search.\n\nThe serverless architecture scales efficiently for multiple use cases:\n\nCorporate – Employee recognition and event documentation\nHealthcare – HIPAA-compliant photo management with relationship tracking\nEducation – Student and faculty photo organization across departments\nEvents – Professional photography with automated tagging and client delivery\n\nThe architecture combines several AWS services to create a contextually aware photo search system:\n\nAmazon API Gateway for REST API endpoints and web interface integration\nAmazon Bedrock with Anthropic’s Claude 3.5 Sonnet for AI-powered contextual image captioning\nAmazon DynamoDB for fast metadata storage and retrieval\nAWS Lambda for serverless compute orchestration across solution components\nAmazon Neptune for storing complex relationships as a graph database\nAmazon Rekognition for face detection, recognition, and object labeling\nAmazon Simple Storage Service (Amazon S3) for scalable photo storage and triggering processing workflows\n\nThe system follows a streamlined workflow:\n\nImages are uploaded to S3 buckets with automatic Lambda triggers.\nReference photos in the faces/ prefix are processed to build recognition models.\nNew photos trigger Amazon Rekognition for face detection and object labeling.\nNeptune stores connections between people, objects, and contexts.\nAmazon Bedrock creates contextual descriptions using detected faces and relationships.\nDynamoDB stores searchable metadata with fast retrieval capabilities.\nNatural language queries traverse the Neptune graph for intelligent results.\n\nThe complete source code is available on GitHub .\nPrerequisites\nBefore implementing this solution, ensure you have the following:\n\nAn AWS account with appropriate permissions for Amazon S3, Lambda, Amazon Rekognition, Neptune, Amazon Bedrock, and DynamoDB\nThe AWS Command Line Interface (AWS CLI) v2.15.0 or later configured with programmatic access\nThe AWS CDK v2.92.0 or later installed ( npm install -g aws-cdk )\nPython 3.11 or later with pip package manager\nNode.js 18.x or later for AWS CDK operations\nBasic knowledge of serverless architectures and graph databases\nAccess to Anthropic’s Claude 3.5 Sonnet on Amazon Bedrock in your AWS Region\n\nDeploy the solution\nDownload the complete source code from the GitHub repository . More detailed setup and deployment instructions are available in the README.\nThe project is organized into several key directories that separate concerns and enable modular development:\n\nsmart-photo-caption-and-search/\n├── lambda/\n│ ├── face_indexer.py # Indexes reference faces in Rekognition\n│ ├── faces_handler.py # Lists indexed faces via API\n│ ├── image_processor.py # Main processing pipeline\n│ ├── search_handler.py # Handles search queries\n│ ├── style_caption.py # Generates styled captions\n│ ├── relationships_handler_neptune.py # Manages Neptune relationships\n│ ├── label_relationships.py # Queries label hierarchies\n│ └── neptune_search.py # Neptune relationship parsing\n├── lambda_layer/ # Pillow image processing layer\n├── neptune_layer/ # Gremlin Python Neptune layer\n├── ui/\n│ └── demo.html # Web interface with Cognito authentication\n├── app.py # CDK application entry point\n├── image_name_cap_stack_neptune.py # Neptune-enabled CDK stack\n└── requirements_neptune.txt # Python dependencies\n\nThe solution uses the following key Lambda functions:\n\nimage_processor.py – Core processing with face recognition, label detection, and relationship-enriched caption generation\nsearch_handler.py – Natural language query processing with multi-step relationship traversal\nrelationships_handler_neptune.py – Configuration-driven relationship management and graph connections\nlabel_relationships.py – Hierarchical label queries, object-person associations, and semantic discovery\n\nTo deploy the solution, complete the following steps:\n\nRun the following command to install dependencies:\n\npip install -r requirements_neptune.txt\n\nFor a first-time setup, fun the following command to bootstrap the AWS CDK:\n\ncdk bootstrap\n\nRun the following command to provision AWS resources:\n\ncdk deploy\n\nSet up Amazon Cognito user pool credentials in the web UI.\nUpload reference photos to establish the recognition baseline.\nCreate sample family relationships using the API or web UI.\n\nThe system automatically handles face recognition, label detection, relationship resolution, and AI caption generation through the serverless pipeline, enabling natural language queries like “person’s mother with car” powered by Neptune graph traversals.\nKey features and use cases\nIn this section, we discuss the key features and use cases for this solution.\nAutomate face recognition and tagging\nWith Amazon Rekognition, you can automatically identify individuals from reference photos, without manual tagging. Upload a few clear images per person, and the system recognizes them across your entire collection, regardless of lighting or angles. This automation reduces tagging time from weeks to hours, supporting corporate directories, compliance archives, and event management workflows.\nEnable relationship-aware search\nBy using Neptune, the solution understands who appears in photos and how they are connected. You can run natural language queries such as “Sarah’s manager” or “Mom with her children,” and the system traverses multi-hop relationships to return relevant images. This semantic search replaces manual folder sorting with intuitive, context-aware discovery.\nUnderstand objects and context automatically\nAmazon Rekognition detects objects, scenes, and activities, and Neptune links them to people and relationships. This enables complex queries like “executives with company vehicles” or “teachers in classrooms.” The label hierarchy is generated dynamically and adapts to different domains—such as healthcare or education—without manual configuration.\nGenerate context-aware captions with Amazon Bedrock\nUsing Amazon Bedrock, the system creates meaningful, relationship-aware captions such as “Sarah and her manager discussing quarterly results” instead of generic ones. Captions can be tuned for tone (such as objective for compliance, narrative for marketing, or concise for executive summaries), enhancing both searchability and communication.\nDeliver an intuitive web experience\nWith the web UI, users can search photos using natural language, view AI-generated captions, and adjust tone dynamically. For example, queries like “mother with children” or “outdoor activities” return relevant, captioned results instantly. This unified experience supports both enterprise workflows and personal collections.\nThe following screenshot demonstrates using the web UI for intelligent photo search and caption styling.\n\nScale graph relationships with label hierarchies\nNeptune scales to model thousands of relationships and label hierarchies across organizations or datasets. Relationships are automatically generated during image processing, enabling fast semantic discovery while maintaining performance and flexibility as data grows.\nThe following diagram illustrates an example person relationship graph (configuration-driven).\n\nPerson relationships are configured through JSON data structures passed to the initialize_relationship_data() function. This configuration-driven approach supports unlimited use cases without code modifications—you can simply define your people and relationships in the configuration object.\nThe following diagram illustrates an example label hierarchy graph (automatically generated from Amazon Rekognition).\n\nLabel hierarchies and co-occurrence patterns are automatically generated during image processing. Amazon Rekognition provides category classifications that create the belongs_to relationships, and the appears_with and co_occurs_with relationships are built dynamically as images are processed.\nThe following screenshot illustrates a subset of the complete graph, demonstrating multi-layered relationship types.\n\nDatabase generation methods\nThe relationship graph uses a flexible configuration-driven approach through the initialize_relationship_data() function. This mitigates the need for hard-coding and supports unlimited use cases:\n\n# Generic configuration structure\nconfig = {\n\"people\": [\n{\"name\": \"alice\", \"gender\": \"woman\", \"role\": \"mother\"},\n{\"name\": \"jane\", \"gender\": \"girl\", \"role\": \"daughter\"}\n],\n\"relationships\": [\n{\"from\": \"alice\", \"to\": \"jane\", \"type\": \"parent_of\", \"subtype\": \"mother_of\"},\n{\"from\": \"jane\", \"to\": \"david\", \"type\": \"sibling_of\", \"bidirectional\": True}\n]\n}\n\n# Generic relationship creation\nfor rel in relationships_data:\ng.V().has('name', rel[\"from\"]).addE(rel[\"type\"]).to(\n__.V().has('name', rel[\"to\"])\n).property('type', rel[\"subtype\"]).next()\n# Business example - just change the configuration\nbusiness_config = {\n\"people\": [{\"name\": \"sarah\", \"role\": \"manager\"}],\n\"relationships\": [{\"from\": \"sarah\", \"to\": \"john\", \"type\": \"manages\", \"subtype\": \"manager_of\"}]\n}\n\nThe label relationship database is created automatically during image processing through the store_labels_in_neptune() function:\n\n# Rekognition provides labels with categories\nresponse = rekognition.detect_labels(\nImage={'Bytes': image_bytes},\nMaxLabels=20,\nMinConfidence=70\n)\n\n# Extract labels and categories\nfor label in response.get('Labels', []):\nlabel_data = {\n'name': label['Name'], # e.g., \"Car\"\n'categories': [cat['Name'] for cat in label.get('Categories', [])] # e.g., [\"Vehicle\", \"Transportation\"]\n}\n# Automatic hierarchy creation in Neptune\nfor category in categories:\n# Create belongs_to relationship (Car -> Vehicle -> Transportation)\ng.V().has('name', label_name).addE('belongs_to').to(\n__.V().has('name', category_name)\n).property('type', 'hierarchy').next()\n\n# Create appears_with relationship (Person -> Car)\ng.V().has('name', person_name).addE('appears_with').to(\n__.V().has('name', label_name)\n).property('confidence', confidence).next()\n\nWith these functions, you can manage large photo collections with complex relationship queries, discover photos by semantic context, and find themed collections through label co-occurrence patterns.\nPerformance and scalability considerations\nConsider the following performance and scalability factors:\n\nHandling bulk uploads – The system processes large photo collections efficiently, from small family albums to enterprise archives with thousands of images. Built-in intelligence manages API rate limits and facilitates reliable processing even during peak upload periods.\nCost optimization – The serverless architecture makes sure you only pay for actual usage, making it cost-effective for both small teams and large enterprises. For reference, processing 1,000 images typically costs approximately $15–25 (including Amazon Rekognition face detection, Amazon Bedrock caption generation, and Lambda function execution), with Neptune cluster costs of $100–150 monthly regardless of volume. Storage costs remain minimal at under $1 per 1,000 images in Amazon S3.\nScaling performance – The Neptune graph database approach scales efficiently from small family structures to enterprise-scale networks with thousands of people. The system maintains fast response times for relationship queries and supports bulk processing of large photo collections with automatic retry logic and progress tracking.\n\nSecurity and privacy\nThis solution implements comprehensive security measures to protect sensitive image and facial recognition data. The system encrypts data at rest using AES-256 encryption with AWS Key Management Service (AWS KMS) managed keys and secures data in transit with TLS 1.2 or later. Neptune and Lambda functions operate within virtual private cloud (VPC) subnets, isolated from direct internet access, and API Gateway provides the only public endpoint with CORS policies and rate limiting. Access control follows least-privilege principles with AWS Identity and Access Management (IAM) policies that grant only minimum required permissions: Lambda functions can only access specific S3 buckets and DynamoDB tables, and Neptune access is restricted to authorized database operations. Image and facial recognition data stays within your AWS account and is never shared outside AWS services. You can configure Amazon S3 lifecycle policies for automated data retention management, and AWS CloudTrail provides complete audit logs of data access and API calls for compliance monitoring, supporting GDPR and HIPAA requirements with additional Amazon GuardDuty monitoring for threat detection.\nClean up\nTo avoid incurring future charges, complete the following steps to delete the resources you created:\n\nDelete images from the S3 bucket:\n\naws s3 rm s3://YOUR_BUCKET_NAME –recursive\n\nDelete the Neptune cluster (this command also automatically deletes Lambda functions):\n\ncdk destroy\n\nRemove the Amazon Rekognition face collection:\n\naws rekognition delete-collection --collection-id face-collection\nConclusion\nThis solution demonstrates how Amazon Rekognition, Amazon Neptune, and Amazon Bedrock can work together to enable intelligent photo search that understands both visual content and context. Built on a fully serverless architecture, it combines computer vision, graph modeling, and natural language understanding to deliver scalable, human-like discovery experiences. By turning photo collections into a knowledge graph of people, objects, and moments, it redefines how users interact with visual data—making search more semantic, relational, and meaningful. Ultimately, it reflects the reliability and trustworthiness of AWS AI and graph technologies in enabling secure, context-aware photo understanding.\nTo learn more, refer to the following resources:\n\nAWS CDK Documentation\nAmazon Rekognition Developer Guide\nAmazon Neptune User Guide\nAmazon Bedrock User Guide\n\nAbout the authors\n\nKara Yang\nKara Yang is a Data Scientist and Machine Learning Engineer at AWS Professional Services, specializing in generative AI, large language models, and computer vision. Her projects span energy, automotive, aerospace, and manufacturing, where she designs AgentCore architectures and multi-agent systems with expertise in prompt engineering, guardrail design, and rigorous LLM evaluation to deliver scalable, production-grade AI solutions.\n\nBilly Dean\nBilly Dean is a ProServe Account Executive and AI Solutions Architect at Amazon Web Services with over 20 years of enterprise sales experience serving top Retail/CPG, Energy, Insurance, and Travel & Hospitality companies. He specializes in driving customer business outcomes through innovative cloud solutions and strategic partnerships.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "b97c48ca90ba3f00",
    "title": "Train CodeFu-7B with veRL and Ray on Amazon SageMaker Training jobs",
    "url": "https://aws.amazon.com/blogs/machine-learning/train-codefu-7b-with-verl-and-ray-on-amazon-sagemaker-training-jobs/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T15:46:50Z",
    "summary": "In this post, we demonstrate how to train CodeFu-7B, a specialized 7-billion parameter model for competitive programming, using Group Relative Policy Optimization (GRPO) with veRL, a flexible and efficient training library for large language models (LLMs) that enables straightforward extension of diverse RL algorithms and seamless integration with existing LLM infrastructure, within a distributed Ray cluster managed by SageMaker training jobs. We walk through the complete implementation, coverin...",
    "content": "The rapid advancement of artificial intelligence (AI) has created unprecedented demand for specialized models capable of complex reasoning tasks, particularly in competitive programming where models must generate functional code through algorithmic reasoning rather than pattern memorization. Reinforcement learning (RL) enables models to learn through trial and error by receiving rewards based on actual code execution, making it particularly well-suited for developing genuine problem-solving capabilities in algorithmic domains.\nHowever, implementing distributed RL training for code generation presents significant infrastructure challenges such as orchestrating multiple heterogeneous components, coordinating parallel code compilation across nodes, and maintaining fault tolerance for long-running processes. Ray is one of the frameworks for distributed workloads that address these challenges, due to its unified system that handles the entire AI pipeline, GPU-first architecture, and seamless integration with tools like Hugging Face Transformers and PyTorch .\nWorkloads can be run with Ray framework on SageMaker training jobs by using the Ray on Amazon SageMaker Training jobs solution, which combines Ray’s distributed computing framework with SageMaker’s fully managed infrastructure. This solution automatically handles Ray cluster initialization, multi-node coordination, and distributed resource management, enabling developers to focus on model development while benefiting from SageMaker’s enterprise-grade features.\nIn this post, we demonstrate how to train CodeFu-7B, a specialized 7-billion parameter model for competitive programming, using Group Relative Policy Optimization (GRPO) with veRL , a flexible and efficient training library for large language models (LLMs) that enables straightforward extension of diverse RL algorithms and seamless integration with existing LLM infrastructure, within a distributed Ray cluster managed by SageMaker training jobs. We walk through the complete implementation, covering data preparation, distributed training setup, and comprehensive observability, showcasing how this unified approach delivers both computational scale and developer experience for sophisticated RL training workloads.\nAbout CodeFu-7B\nCodeFu-7B-v0.1 is a 7B parameter language model specifically trained for solving Competitive Programming (CP) problems. Built upon the DeepSeek-R1-Distill-Qwen-7B  base model, CodeFu demonstrates how reinforcement learning can develop capabilities in algorithmic reasoning and efficient C++ code generation beyond traditional supervised fine-tuning approaches.\nThe model is trained using problem statements from the DeepMind CodeContest dataset without access to ground-truth solutions during training, forcing it to learn through trial and error based on code execution feedback. This approach enables the development of genuine problem-solving capabilities rather than pattern memorization\nCodeFu is publicly available on HuggingFace and released under the MIT license, making it accessible for researchers and practitioners interested in code generation and algorithmic reasoning. The model’s training methodology demonstrates the potential for applying reinforcement learning techniques to complex reasoning tasks beyond competitive programming.\nRay in SageMaker training jobs solution\nRay on Amazon SageMaker Training jobs is a solution that enables distributed data processing and model training using Ray within SageMaker’s managed training environment. The solution provides key capabilities including universal launcher architecture for automatic Ray cluster setup, multi-node cluster management with intelligent coordination, heterogeneous cluster support for mixed instance types, and integrated observability through Ray Dashboard, Prometheus, Grafana, and Amazon CloudWatch integration.\nThe solution seamlessly integrates with the SageMaker Python SDK using the modern ModelTrainer API. This publicly available solution on GitHub enables developers to use Ray’s distributed computing capabilities while benefiting from SageMaker’s managed infrastructure, making it ideal for complex workloads like reinforcement learning training that require sophisticated distributed coordination and resource management.\nSolution overview\nThe workflow for training CodeFu 7B with veRL and Ray on SageMaker training jobs, as illustrated in the accompanying diagram, consists of the following steps:\n\nData preparation : Upload the preprocessed DeepMind CodeContest dataset and training configuration.\nTraining job submission : Submit a SageMaker training job API request through the ModelTrainer class from the SageMaker Python SDK.\nMonitoring and observability : Monitor training progress in real-time through Ray Dashboard, and optionally with Prometheus metrics collection, Grafana visualization, and experiment tracking.\nAutomatic cleanup : Upon training completion, SageMaker automatically saves the trained model to S3, uploads training logs to CloudWatch, and decommissions the compute cluster.\n\nThis streamlined architecture delivers a fully managed reinforcement learning training experience, enabling developers to focus on model development while SageMaker and Ray handle the complex distributed infrastructure orchestration—within a pay-as-you-go pricing model that bills only for actual compute time.\nPrerequisites\nThe following prerequisites must be complete before the notebook can be run:\n\nMake the following quota increase requests for SageMaker AI. For this use case, request a minimum of 2 p4de.24xlarge instances (with 8 x NVIDIA A100 GPUs) and scale to more p4de.24xlarge instances (depending on time-to-train and cost-to-train trade-offs for your use case). P5 instances (with 8 x NVIDIA H100 GPUs) are also supported. On the Service Quotas console, request the following SageMaker AI quotas:\n\np4de instances ( p4de.24xlarge ) for training job usage: 2\n\nCreate an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess , AmazonS3FullAccess ,  AmazonSSMFullAccess to give required access to SageMaker AI to run the examples.\nAssign the following policy as the trust relationship to created IAM role:\n\n{\n   \"Version\":\"2012-10-17\",\n   \"Statement\":[\n      {\n         \"Sid\":\"\",\n         \"Effect\":\"Allow\",\n         \"Principal\":{\n            \"Service\":\n               \"sagemaker.amazonaws.com\"\n            ]\n         },\n         \"Action\":\"sts:AssumeRole\"\n      }\n   ]\n}\n\n(Optional) Create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI ) to access Jupyter notebooks for running the training code. Alternatively, JupyterLab can be used in a local setup or another Python development environment to execute the notebook and submit the SageMaker training job.\n\nNote: These permissions grant broad access and are not recommended for use in production environments. See the SageMaker Developer Guide for guidance on defining more fine-grained permissions\nThe code example can be found at  this GitHub repository .\nPrepare the dataset\nThe data preparation pipeline transforms the raw DeepMind CodeContest dataset into a format suitable for reinforcement learning training. We apply systematic filters to identify suitable problems, removing those with Codeforces ratings below 800 and implementing quality validation checks for missing test cases, malformed descriptions, and invalid constraints.\nWe categorize problems into three difficulty tiers: Easy (800-1000 points), Hard (1100-2200 points), and Expert (2300-3500 points). This post uses only the Easy dataset for training. Each problem is formatted with two components: a user prompt containing the problem statement, and a reward_model specification with test cases, time limits, and memory constraints. Crucially, the ground_truth field contains no solution code — only test cases, forcing the model to learn through reward signals rather than memorizing solutions.\n\n{\n  \"data_source\": \"code_contests\",\n  \"prompt\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Write a C++ solution for this problem: ...\"\n    }\n  ],\n  \"ability\": \"coding-cp\",\n  \"reward_model\": {\n    \"style\": \"rule\",\n    \"ground_truth\": {\n      \"name\": \"problem 1\",\n      \"public_tests\": {\n        \"input\": [\"test input 1\", \"test input 2\"],\n        \"output\": [\"expected output 1\", \"expected output 2\"]\n      },\n      \"private_tests\": {\n        \"input\": [\"private input 1\", \"private input 2\"],\n        \"output\": [\"private output 1\", \"private output 2\"]\n      },\n      \"time_limit\": 2.0,\n      \"memory_limit_bytes\": 268435456,\n      \"cf_rating\": 1200\n    }\n  }\n}\n\nFor this post, we provide a pre-processed subset of the Easy difficulty dataset in the code sample to streamline the training example, accessible from the GitHub repository .\nGRPO training using veRL\nThe training process uses Ray to orchestrate the distributed execution and synchronization of vLLM rollout, reward evaluation (code compilation and execution), FSDP model parallelism, and Ulysses sequence parallelism. We set the degree of sequence parallelism to 4 for long-form reasoning and code generations.\nThe veRL framework implements a sophisticated multi-component architecture through its main_ppo.py orchestrator, which coordinates three primary distributed worker types: ActorRolloutRefWorker for policy inference and rollouts, CriticWorker for value function estimation, and RewardModelWorker for scoring generated solutions.\nThe GRPO algorithm enhances traditional proximal policy optimization (PPO) by computing advantages using group-relative baselines, which helps stabilize training by reducing variance in policy gradient estimates.\nWe extended the TinyZero code repository by using Ray to manage and distribute reward function calculation. This enables parallel C++ code compilation and evaluation across the same cluster to address the compute-intensive and latency-bound nature of code execution. The entire pipeline is executed as a SageMaker training job running on ml.p4de.24xlarge instances. The training pipeline consists of the following steps as shown in the following architecture:\n\nRollout : Coding problem prompts are fed into the vLLM inference engine for rolling out potential solutions.\nResponse generation : vLLM generates multiple responses (reasoning + code) for each prompt.\nCode execution : Code solutions are extracted from responses and are compiled and executed by distributed workers (compilers and runtime) managed by Ray.\nReward calculation : Execution outcomes are used to calculate rewards (i.e. testcase pass ratios) and advantages are computed using group-relative baselines.\nPolicy update : The Actor uses advantages and token probabilities to compute the PPO loss, which is used to update CodeFu’s parameters through gradient descent.\nIteration : The process repeats with batches of prompt-response-reward cycles, with Ray managing the distributed sampling, execution, and training synchronization across the pipeline.\n\nThe training process orchestration involves several key components implemented across multiple modules. The core veRL training loop is implemented in main_ppo.py , which initializes Ray workers and manages the distributed training process:\n\n@ray.remote\ndef main_task(config):\n    # Initialize tokenizer and download model\n    local_path = copy_local_path_from_hdfs(config.actor_rollout_ref.model.path)\n    tokenizer = hf_tokenizer(local_path)\n    \n    # Define distributed worker roles\n    role_worker_mapping = {\n        Role.ActorRollout: ray.remote(ActorRolloutRefWorker),\n        Role.Critic: ray.remote(CriticWorker),\n        Role.RefPolicy: ray.remote(ActorRolloutRefWorker),\n    }\n    \n    # Initialize reward manager for code execution\n    reward_fn = RewardManager(tokenizer=tokenizer, num_examine=0)\n    \n    # Create and start trainer\n    trainer = RayPPOTrainer(\n        config=config,\n        tokenizer=tokenizer,\n        role_worker_mapping=role_worker_mapping,\n        resource_pool_manager=resource_pool_manager,\n        reward_fn=reward_fn,\n    )\n    trainer.init_workers()\n    trainer.fit()\n\nThe reward evaluation system implements parallel code execution through Ray remote functions, handling C++ compilation and test case execution:\n\n@ray.remote\ndef process_reward_item(idx, valid_response_length, sequences_str, data_source, reward_model_data):\n    # Extract and compile C++ code from model response\n    ground_truth = json.loads(reward_model_data)[\"ground_truth\"]\n    \n    # Select appropriate scoring function based on data source\n    if data_source == \"code_contests\":\n        compute_score = code_contests.compute_score\n    \n    # Execute code against test cases and calculate pass ratio\n    score = compute_score(solution_str=sequences_str, ground_truth=ground_truth)\n    return idx, score, valid_response_length, sequences_str, data_source\n\nThe parallel test case execution system optimizes evaluation efficiency by sampling test cases and using process pools:\n\ndef run_test_cases_parallel(\n    bin_file: str, test_inputs: List[str],\n    test_outputs: List[str],\n    prob_name: str, execution_timeout: float,\n    max_test_cases: int = 100,\n    max_workers: int = 100) -> Tuple[int, int]:\n    # Sample test cases if too many available\n    if len(test_inputs) > max_test_cases:\n        random_indices = np.random.choice(len(test_inputs), size=max_test_cases, replace=False)\n        test_inputs = test_inputs[random_indices]\n        test_outputs = test_outputs[random_indices]\n    \n    # Execute test cases in parallel using ProcessPoolExecutor\n    with ProcessPoolExecutor(max_workers=min(max_workers, len(test_inputs))) as executor:\n        results = list(executor.map(_process_test_case, args_list))\n        total_matches = sum(results)\n    \n    return total_matches, len(test_inputs)\n\nThis implementation enables efficient distributed training by separating concerns: the main_ppo.py orchestrator manages Ray worker coordination, while the reward system provides scalable code evaluation through parallel compilation and execution across the SageMaker cluster.\nBelow is the pseudocode for the reward calculation used in this post to train a competitive programming coding model. The reward function is the most important part of reinforcement learning as it defines what the model is encouraged to achieve and what it should avoid. This implementation uses a hierarchical penalty system that first checks for fundamental code execution issues, assigning severe penalties for non-executable code (-1) and moderate penalties for compilation failures (-0.5). Extracted code solutions are executed with strict time limit enforcement – code exceeding the problem’s specified time limit is given zero reward, facilitating realistic competitive programming conditions. For a successfully executed C++ solution, its reward is calculated as a linear function based on the fraction of private test cases passed, encouraging the model to solve as many private test cases as possible while avoiding overfitting to publicly visible tests. This design prioritizes code correctness and execution validity, with the private test performance serving as the sole signal for learning optimal coding solutions.\n\ndef compute_reward(code_output, ground_truth):\n    # Handle execution failures (same for both stages)\n    if not is_executable(code_output):\n        return -1\n    \n    if compilation_failed(code_output):\n        return -0.5\n    \n    if exceeds_time_limit(code_output):\n        return 0\n   \n    # Primary reward signal: correctness on hidden test cases\n    # Run code against private test cases\n    passed_private, total_private = run_private_tests(code_output, ground_truth, max_test_cases=1000)\n   \n   return passed_private / total_private\n\nRefer to scripts/verl/utils/reward_score/code_contests.py  for the complete Python code. Executing generated code in production environments requires appropriate sandboxing . In this controlled demonstration setting, we execute the code as a quick example to evaluate its correctness to assign rewards.\nRay workload with SageMaker training jobs\nTo train CodeFu-7B using veRL and Ray on SageMaker training jobs, we use the ModelTrainer class from the SageMaker Python SDK. Start by setting up the distributed training workload with the following steps:\n\nSelect the instance type and container image for the training job:\n\ninstance_type = \"ml.p4de.24xlarge\" \ninstance_count = 2\n\naccount_id = sts.get_caller_identity()[\"Account\"]\nregion = sagemaker_session.boto_session.region_name\nrepo_name = \"codefu-pytorch\"\ntag = \"latest\"\n\nimage_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repo_name}:{tag}\"\n\nThe training uses a custom Docker container that includes veRL, Ray, and the necessary dependencies for distributed RL training. Refer to the GitHub repository for the complete container definition and build instructions.\n\nCreate the ModelTrainer to encapsulate the Ray-based training setup:\n\nThe ModelTrainer class provides flexible execution options through its SourceCode configuration, allowing users to customize their training workflows with different frameworks and launchers. Specify either an entry_script for direct Python script execution or use the command parameter for custom execution commands, enabling integration with specialized frameworks such as Ray, Hugging Face Accelerate , or custom distributed training solutions.\n\n...\nargs = [\n\"--entrypoint\", \"train.py\",\n\"--config\", \"/opt/ml/input/data/config/args.yaml\",\n]\n\n# Define the script to be run with Ray launcher\nsource_code = SourceCode(\nsource_dir=\"./scripts\",\nrequirements=\"requirements.txt\",\ncommand=f\"python launcher.py {' '.join(args)}\",\n)\n\n# Define the compute configuration\ncompute_configs = Compute(\ninstance_type=instance_type,\ninstance_count=instance_count,\nkeep_alive_period_in_seconds=1800,\n)\n\njob_name = \"train-codefu-verl-ray\"\noutput_path = f\"s3://{bucket_name}/{job_name}\"\n\nmodel_trainer = ModelTrainer(\ntraining_image=image_uri,\nsource_code=source_code,\nbase_job_name=job_name,\ncompute=compute_configs,\nstopping_condition=StoppingCondition(max_runtime_in_seconds=3600 * 24 * 5),\noutput_data_config=OutputDataConfig(s3_output_path=output_path),\ncheckpoint_config=CheckpointConfig(\ns3_uri=output_path + \"/checkpoint\",\nlocal_path=\"/opt/ml/checkpoints\"\n),\nenvironment={\n\"RAY_PROMETHEUS_HOST\": \"<PROMETHEUS_HOST>\",\n\"RAY_GRAFANA_HOST\": \"<GRAFANA_HOST>\",\n\"RAY_PROMETHEUS_NAME\": \"prometheus\",\n\"BASE_MODEL\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\",\n\"RUN_NAME\": \"sagemaker-training-run\",\n...\n},\nrole=get_execution_role(),\n).with_remote_debug_config(RemoteDebugConfig(enable_remote_debug=True))\n\nThe launcher.py script serves as the universal entry point that detects the SageMaker environment (single-node or multi-node, homogeneous or heterogeneous cluster), initializes the Ray cluster with proper head/worker node coordination, and executes your custom training script. Key launcher.py functionalities are:\n\nRay cluster setup : Automatically detects the cluster environment and initializes Ray with proper head node selection.\nNode coordination : Manages communication between head and worker nodes across SageMaker instances.\nScript execution : Executes the specified --entrypoint script ( train.py ) within the Ray cluster context.\nPrometheus and grafana connectivity : Configures Ray to export metrics and establishes connection to external Prometheus and Grafana servers specified by RAY_PROMETHEUS_HOST and RAY_GRAFANA_HOST for comprehensive cluster monitoring. For additional information, refer to Ray on SageMaker training jobs – Observability with Prometheus and Grafana .\n\nFor the complete implementation of the Ray cluster setup with SageMaker training jobs, refer to launcher.py .\nThe train.py script serves as the actual training orchestrator that:\n\nLoads the veRL configuration from the provided YAML file\nSets up the distributed training environment with proper tokenizer and model initialization\nConstructs and executes the veRL training command with the necessary parameters\nHandles environment variable configuration for Ray workers and NVIDIA Collective Communications Library (NCCL) communication\nManages the complete training lifecycle from data loading to model checkpointing\n\nFor the complete implementation of the entry point script, refer to train.py .\n\nSet up the input channels for the ModelTrainer by creating InputData objects from the S3 bucket paths:\n\n...\n\ntrain_input = InputData(\n   channel_name=\"train\",\n   data_source=S3DataSource(\n       s3_data_type=\"S3Prefix\",\n       s3_uri=train_dataset_s3_path,\n       s3_data_distribution_type=\"FullyReplicated\",\n   ),\n)\n\nconfig_input = InputData(\n   channel_name=\"config\",\n   data_source=S3DataSource(\n       s3_data_type=\"S3Prefix\",\n       s3_uri=train_config_s3_path,\n       s3_data_distribution_type=\"FullyReplicated\",\n    ),\n)\n\nSubmit the training job using the train function call on the created ModelTrainer:\n\nmodel_trainer.train(\n   input_data_config=[train_input, val_input, config_input],\n   wait=False\n)\n\nThe job can be monitored directly from the notebook output or through the SageMaker console, which shows the job status and corresponding CloudWatch logs.\n\nSageMaker training jobs console\n\nSageMaker training jobs system metrics\nThe launcher.py script orchestrates the Ray cluster initialization through the following automated steps, which can be monitored in real-time through CloudWatch logs:\n\nSetup SageMaker training jobs and Ray environment variables : Configures necessary environment variables for both SageMaker integration and Ray cluster communication:\n\n__main__ - INFO - Entrypoint argument provided: train.py\n__main__ - INFO - Set source_dir=, entry_script=train.py\n...\n__main__ - INFO - Found SageMaker environment with hosts: ...\n__main__ - INFO - Current host: algo-1\n__main__ - INFO - Configured Prometheus host: <PROMETHEUS_HOST>\n__main__ - INFO - Configured Grafana host: <GRAFANA_HOST>\n__main__ - INFO - Ray runtime environment contains 137 total environment variables\n__main__ - INFO - Ray runtime environment: ...\n\nIdentify the SageMaker training job cluster type : Detects whether the deployment is single-node or multi-node, and determines a single or multi-node cluster, and if it’s a homogeneous or heterogeneous cluster configuration:\n\n__main__ - INFO - Homogeneous cluster configuration: 2 total hosts\n__main__ - INFO - All hosts: ['algo-1', 'algo-2']\n__main__ - INFO - Found multiple hosts, initializing Ray as a multi-node cluster\n\nSetup head and worker nodes : Identifies which instance serves as the Ray head node and configures the remaining instances as worker nodes :\n\n__main__ - INFO - Head node: algo-1, Current host: algo-1\n__main__ - INFO - CPUs for the head node: 192\n__main__ - INFO - GPUs for the head node: 8\n\nStart Ray node : Initializes the Ray head node and worker nodes with appropriate resource allocation and dashboard configuration, by verifying that the worker nodes successfully connect to the head node before proceeding:\n\n#011INFO worker.py:1723 -- Connecting to existing Ray cluster at address: ...\n#011INFO worker.py:1908 -- Connected to Ray cluster. View the dashboard at\n__main__ - INFO - All nodes connected to the Ray cluster!\n\nExecute the training script : Launches the specified entrypoint script (train.py) within the fully initialized Ray cluster context:\n\nScript path: /opt/ml/input/data/code/train.py\n...\n__main__ - INFO - Loading and executing Python script using importlib...\n\nAfter the job completes, the trained model weights and checkpoints will be available in the specified S3 output path, ready for deployment or further evaluation.\nExperiment tracking\nThe CodeFu training pipeline integrates seamlessly with Managed MLflow on Amazon SageMaker AI as well as third party solutions, for comprehensive experiment tracking and visualization of reinforcement learning metrics.\nThe following image shows the metrics that are particularly useful to monitor during CodeFu training.\n\nThe metrics plot shows a promising GRPO/PPO learning progression for the competitive programming model. The reward signals demonstrate clear improvement, with critic/reward/mean rising from -0.8 to 0.6 and critic/reward/min recovering from initial failures -1.0 to moderate performance -0.5 , while critic/reward/max maintains perfect scores 1.0 throughout training, indicating the model can achieve optimal solutions.\nThe Actor metrics reveal healthy training dynamics: actor/ppo_kl remains low ~0.0002 after an initial spike, confirming stable policy updates, while actor/pg_clipfrac stays in a reasonable range ~0.002-0.004 , suggesting appropriately sized learning steps.\nThe increasing actor/kl_loss trend indicates growing divergence from the reference model as expected during RL fine-tuning. Most importantly, val/test_score/code_contests shows consistent improvement from -0.6 to ~0.5 , and the train-validation comparison reveals good generalization with both curves tracking closely, indicating the model is learning to solve coding problems effectively without overfitting.\nThe table below explains key GRPO training metrics and why monitoring each one matters for diagnosing training health and performance:\n\nMetric\nDescription\nPurpose\n\ncritic/reward/min\nMinimum reward achieved on the training set\nDetect catastrophic failures : Extremely negative rewards indicate the model is producing poor outputs that need attention\n\ncritic/reward/mean\nAverage reward across the training set\nPrimary progress indicator : Shows overall model performance improvement; should generally trend upward during successful training\n\ncritic/reward/max\nMaximum reward achieved on the training set\nTrack best-case performance : Shows the model’s peak capability; helps identify if the model can achieve excellent results even if average is low\n\nactor/ppo_kl\nKL divergence between current and previous policy iteration\nTraining stability monitoring : High values indicate rapid policy changes that may destabilize training; should stay moderate\n\nactor/pg_clipfrac\nFraction of policy updates hitting the clipping boundary\nUpdate aggressiveness gauge : Moderate values indicate healthy learning; too high suggests overly aggressive updates that may destabilize training, too low (e.g. zero) suggests inefficient learning. This is valid only during off-policy PPO updates.\n\nactor/kl_loss\nKL divergence between current policy and fixed reference model\nReference drift prevention : Helps prevent the model from deviating too far from original behavior; important for maintaining coding capabilities\n\nval/test_score/code_contests\nReward/performance on held-out validation set\nGeneralization check : Most important metric for real performance; detects overfitting and measures true model improvement\n\n(Optional) Observability with Ray dashboard and Grafana\nTo access the Ray Dashboard and enable Grafana visualization during training, establish port forwarding using AWS Systems Manager (SSM) . To learn more about the setup of AWS SSM, please refer to AWS Systems Manager Quick Setup .\n\nFirst, identify the head node in your multi-node cluster by examining the CloudWatch logs:\n\n__main__ - INFO - Found multiple hosts, initializing Ray as a multi-node cluster\n__main__ - INFO - Head node: algo-1, Current host: algo-2\n\nAccess the Ray Dashboard by forwarding port 8265 from the head node:\n\naws ssm start-session —target sagemaker-training-job:train-codefu-verl-ray-20250821185206_algo-1 \\\n--region us-east-1\\\n--document-name AWS-StartPortForwardingSession \\\n--parameters '{\"portNumber\":[\"8265\"],\"localPortNumber\":[\"8265\"]}'\n\nEnable Grafana to collect Ray metrics by forwarding port 8080 (Ray metrics export port):\n\naws ssm start-session —target sagemaker-training-job:train-codefu-verl-ray-20250821185206_algo-1 \\\n--region us-east-1\\\n--document-name AWS-StartPortForwardingSession \\\n--parameters '{\"portNumber\":[\"8080\"],\"localPortNumber\":[\"<YOUR_LOCAL_PORT>\"]}'\n\nOnce port forwarding is established, the Ray Dashboard can be accessed at localhost:8265 in your browser, providing detailed insights into:\n\nWorker utilization across the distributed cluster\nTask execution  status and performance metrics\nResource consumption including GPU and memory usage\nActor and task scheduling across Ray workers\n\nThe integrated Grafana dashboards provide comprehensive visualization of the training metrics, system performance, and cluster health in real-time:\n\nThis observability setup is crucial for debugging distributed RL training issues, optimizing resource allocation, and making sure the training process progresses efficiently across the multi-node SageMaker cluster.\nClean up\nTo clean up your resources and avoid ongoing charges, follow these steps:\n\nDelete unused SageMaker Studio resources\n(Optional) Delete the SageMaker Studio domain\nOn the SageMaker console, choose Training in the navigation pane and verify that your training job isn’t running anymore.\n\nConclusions\nThis post demonstrates how to train specialized reasoning models for competitive programming using the Ray on Amazon SageMaker Training jobs solution combined with veRL’s reinforcement learning framework.\nThe Ray on SageMaker training jobs solution simplifies the complexity of orchestrating distributed RL workloads by automatically handling Ray cluster initialization, multi-node coordination, and resource management across heterogeneous compute environments. This integration enables organizations to use Ray’s advanced distributed computing capabilities—including support for complex multi-component architectures, dynamic resource allocation, and fault-tolerant execution—while benefiting from SageMaker’s fully managed infrastructure, enterprise-grade security, and pay-as-you-go pricing model.\nThe detailed metrics analysis demonstrated how to monitor training health through reward progression, policy stability indicators, and generalization performance, enabling practitioners to identify optimal training configurations and troubleshoot distributed training issues effectively.\nTo begin implementing distributed RL training with Ray on SageMaker, visit the Ray on Amazon SageMaker Training jobs GitHub repository for the foundational solution framework. The complete CodeFu-7B training implementation, including veRL integration and configuration examples, is available at this GitHub repository .\n\nAbout the authors\n\nBruno Pistone\nBruno Pistone  is a Senior Worldwide Generative AI/ML Specialist Solutions Architect at AWS based in Milan, Italy. He works with AWS product teams and large customers to help them fully understand their technical needs and design AI and machine learning solutions that take full advantage of the AWS cloud and Amazon ML stack. His expertise includes distributed training and inference workloads, model customization, generative AI, and end-to-end ML. He enjoys spending time with friends, exploring new places, and traveling to new destinations.\n\nGiuseppe Angelo Porcelli\nGiuseppe Angelo Porcelli  is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football.\n\nYin Song\nYin Song is a Senior Applied Scientist at the AWS Prototyping team in Sydney, Australia, with over five years of experience helping customers build tailored prototypes that demonstrate complex AWS service use cases. His work focuses on research in AI model fine-tuning and serving, enabling impactful end-to-end AI solutions. A passionate advocate for open source, Yin leads generative AI initiatives that have produced widely-adopted models.\n\nChen Wu\nChen Wu is a Principal Applied Scientist at the AWS Prototyping team, where he drives both applied research and high-impact customer engagements. He specializes in long-context language models, reasoning LLMs, agentic systems, and high-performance AI systems. Chen leads development of the Agent Training Kit, an open source framework for continual learning agents. He has delivered strategic engagements across genomic foundation models, LLM optimization, multi-scale image generation, and 3D/4D volumetric AI pipelines. His open LLMs on Hugging Face have achieved over 1 million downloads, and his long-context research has appeared in NeurIPS 2024 and ACL 2025. He is an ACM Gordon Bell Prize Finalist.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "22f77c672faf241b",
    "title": "Generate structured output from LLMs with Dottxt Outlines in AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/generate-structured-output-from-llms-with-dottxt-outlines-in-aws/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T15:42:34Z",
    "summary": "This post explores the implementation of Dottxt’s Outlines framework as a practical approach to implementing structured outputs using AWS Marketplace in Amazon SageMaker.",
    "content": "This post is cowritten with Remi Louf, CEO and technical founder of Dottxt.\nStructured output in AI applications refers to AI-generated responses conforming to formats that are predefined, validated, and often strictly entered. This can include the schema for the output, or ways specific fields in the output should be mapped. Structured outputs are essential for applications that require consistency, validation, and seamless integration with downstream systems. For example, banking loan approval systems must generate JSON outputs with strict field validation, healthcare systems need to validate patient data formats and enforce medication dosage constraints, and ecommerce systems require standardized invoice generation for their accounting systems.\nThis post explores the implementation of .txt ’s Outlines framework as a practical approach to implementing structured outputs using AWS Marketplace in Amazon SageMaker .\nStructured output: Use cases and business value\nStructured outputs elevate generative AI from ad hoc text generation to dependable business infrastructure, enabling precise data exchange, automated decisioning, and end-to-end workflows across high‑stakes, integration-heavy environments. By enforcing schemas and predictable formats, they unlock use-cases where accuracy, traceability, and interoperability are non-negotiable, from financial reporting and healthcare operations to ecommerce logistics and enterprise workflow automation. This section explores where structured outputs create the most value and how they translate directly into reduced errors, lower operational risk, and measurable ROI.\nWhat is structured output?\nThe category structured output combines multiple types of requirements for how models should produce outputs that follow specific constraints mechanisms. The following are examples of constraint mechanisms.\n\nSchema-based constraints: JSON Schema and XML Schema define object structures with type requirements, required fields, property constraints, and nested hierarchies. Models generate outputs matching these specifications exactly, helping to ensure that fields like  transaction_id  (string),  amount  (float), and  timestamp  (datetime) are present and correctly entered.\nEnumeration constraints: Enum expressions restrict outputs to predefined categorical values. Classification tasks use enum to force models to select from fixed options—such as categorizing instruments as Percussion , String , Woodwind , Brass , or Keyboard —removing arbitrary category generation.\nPattern-based constraints: Regular expressions validate specific formats such as email addresses, phone numbers, dates, or custom identifiers. Regex patterns make sure outputs match required structures without post-processing validation.\nGrammar-based constraints: Context-free grammars (CFGs) and EBNF notation define syntactic rules for generating code, SQL queries, configuration files, or domain-specific languages. Constrained decoding frameworks enforce these rules at token generation time.\nSemantic validation: Beyond syntactic constraints, large language models (LLMs) can validate outputs against natural language criteria—helping to ensure that content is professional , family-friendly , or constructive —addressing subjective requirements that rule-based validation can’t capture.\n\nCritical components that benefit from structured output\nIn modern applications, AI models are integrated with non-AI types of processing and business systems. These integrations and junction points require consistency, type safety, and machine readability, because parsing ambiguities or format deviations would break workflows. Here are some of the common architectural patterns where we see critical interoperability between LLMs and infrastructure components:\n\nAPI integration and data pipelines: Extract, transform, and load (ETL) processes and REST APIs require strict format compliance. Mistakes in the output of the model can create parsing errors and compromise direct database insertion or seamless transformation logic.\nTool calling and function execution: Agentic workflows depend on the ability of the LLM model to invoke functions with correctly typed parameters, enabling multi-step automation where each agent consumes validated inputs.\nDocument extraction and data capture: Parsing invoices, contracts, or medical records requires the model to semantically identify the desired entities and return them in a format that can truly automate data entry by extracting vendor names, amounts, and dates into predefined schemas, including specific categorization options among others.\nReal-time decision systems: Systems that require sub-50 millisecond decisions, such as fraud detection and transaction processing, can’t afford verbosity or retries on the structure of the output. Producing reliable and conformed risk scores, classification flags, and decision metadata mean that downstream systems can consume data instantly.\n\nBusiness applications: Where structured output provides the most value\nAcross high-stakes, integration-heavy domains, structured outputs transform generative models from flexible text engines into reliable business infrastructure that delivers predictability, auditability, and end‑to‑end automation.\n\nFinancial services and transaction processing: In financial institutions, structured outputs facilitate precision and consistency across reporting, auditing, and regulatory compliance. Transaction data, risk assessments, and portfolio analytics must adhere to predefined schemas to support real-time reconciliation, anti-money laundering (AML) reviews, and regulatory filings. Structured outputs enable seamless exchange among payment systems, risk engines, and audit tools—reducing manual oversight while maintaining full traceability and data integrity across high-stakes financial operations.\nHealthcare and clinical operations: Regulatory compliance demands strict validation—range checking for vital signs, medication dosages, and lab results helps prevent critical errors. Structured extraction from medical documents enables automated coding, billing accuracy, and audit trail creation for HIPAA compliance.\nEnterprise workflow automation: Legacy systems require machine-readable data without custom parsing logic. Structured outputs from customer support interactions generate case summaries with sentiment scores, action items, and routing metadata that integrate directly into customer relationship management (CRM) systems.\nEcommerce and logistics: Address validation, payment verification, and order attribute consistency reduce failed deliveries and fraudulent transactions. Structured outputs coordinate multi-party workflows where carriers, warehouses, and payment processors require standardized formats.\nRegulatory compliance and audit readiness: Industries facing strict oversight benefit from structured content management with immutable audit trails. Component-level repositories track every change with metadata (who, when, why, approver), so that auditors can verify compliance through direct system access rather than manual document review.\n\nThe common thread is operational complexity, integration requirements, and risk sensitivity. Structured outputs transform AI from text generation into reliable business infrastructure where predictability, auditability, and system interoperability drive measurable ROI through reduced errors, faster processing, and seamless automation.\nIntroducing .txt Outlines on AWS to produce structured outputs\nStructured output can be achieved in several ways. Most frameworks will, at the core, focus on validation to identify if the output adheres to the rules and requirements requested. If the output doesn’t conform, the framework will request a new output, and keep iterating as such until the model achieves the requested output structure.\nOutlines offers an advanced approach called generation-time validation , meaning that the validation happens as the model is producing tokens, which shifts validation to early in the generation process instead of validating after completion. While not integrated with Amazon Bedrock , understanding Outlines provides insight into cutting-edge structured output techniques that inform hybrid implementation strategies.\nOutlines , developed by the .txt team, is a Python library designed to bring deterministic structure and reliability to language model outputs—addressing a key challenge in deploying LLMs for production applications. Unlike traditional free-form generation, developers can use Outlines to enforce strict output formats and constraints during generation, not just after the fact. This approach makes it possible to use LLMs for tasks where accuracy, predictability, and integration with downstream systems are required.\nHow Outlines works\nOutlines enforces constraints through three main mechanisms:\n\nGrammar compilation: Converts schemas into token masks that guide the model’s choices\nPrefix trees: Prunes invalid paths during beam search to maintain valid structure\nSampling control: Uses finite automata for valid token selection during generation\n\nDuring generation, Outlines follows a precise workflow:\n\nThe language model processes the input sequence and produces token logits\nThe Outlines logits processor sets the probability of illegal tokens to 0%\nA token is sampled only from the set of legal tokens according to the defined structure\nThis process repeats until generation is complete, helping to ensure that the output conforms to the required format\n\nFor example, with a pattern like ^\\d*(\\.\\d+)?$ for decimal numbers, Outlines converts this into an automaton that only allows valid numeric sequences to be generated. If 748 has been generated, the system knows the only valid next tokens are another digit, a decimal point, or the end of sequence token.\nPerformance benefits\nEnforcing structured output during generation offers significant advantages for reliability and performance in production environments. It helps to increase the validity of the output’s structure and can significantly improve performance:\n\nZero inference overhead: The structured generation technique adds virtually no computational cost during inference\n5 times faster generation: According to .txt Engineering’s coalescence approach, structured generation can be dramatically faster than standard generation\nReduced computational resources: Constraints simplify model decision-making by removing invalid paths, reducing overall processing requirements\nImproved accuracy: By narrowing the output space, even base models can achieve higher precision on structured tasks\n\nBenchmark advantages\nHere are some of the proven benefits of the Outlines library:\n\n2 times faster than regex-based validation pipelines\n98% schema adherence compared to 76% for post-generation validation\nSupports complex constraints like recursive JSON schemas\n\nGetting started with Outlines\nOutlines can be seamlessly integrated into existing Python workflows:\n\nfrom pydantic import BaseModel\n\n# Define your data structure\nclass Patient(BaseModel):\nid: int\nname: str\ndiagnosis: str\nage: int\n\n# Load model and create structured generator\nmodel = models.transformers(\"microsoft/DialoGPT-medium\")\ngenerator = generate.json(model, Patient)\n\n# Generate structured output\nprompt = \"Create a patient record for John Smith, 45, with diabetes\"\nresult = generator(prompt) # Returns valid Patient instance\nprint(result.name) # \"John Smith\"\nprint(result.age) # 45\n\nFor more complex schemas:\n\nfrom enum import Enum\n\nclass Status(str, Enum):\nACTIVE = \"active\"\nINACTIVE = \"inactive\"\nPENDING = \"pending\"\n\nclass User(BaseModel):\nusername: str\nemail: str\nstatus: Status\ncreated_at: datetime\n\n# Generator enforces enum values and datetime format\nuser_generator = generate.json(model, User)\n\nUsing .txt’s dotjson in Amazon SageMaker\nYou can directly deploy .txt’s Amazon SageMaker real-time inference solution for generating structured output by deploying one of .txt’s models such as DeepSeek-R1-Distill-Qwen-32B through AWS Marketplace . The following code assumes that you have already deployed an endpoint in your AWS account.\nA Jupyter Notebook that walks through deploying the endpoint end-to-end is available in the product repository .\n\nimport json\nimport boto3\n# Set this based on your SageMaker endpoint\nendpoint_name = \"dotjson-with-DeepSeek-R1-Distill-Qwen-32B\"\nsession = boto3.Session()\nstructured_data = {\n\"patient_id\": 12345,\n\"first\": \"John\",\n\"last\": \"Adams\",\n\"appointment_date\": \"2025-01-27\",\n\"notes\": \"Patient presented with a headache and sore throat\",\n}\npayload = {\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a helpful, honest, and concise assistant.\",\n},\n{\n\"role\": \"user\",\n\"content\": f\"Create a medical record from the following visit data: {structured_data}\",\n},\n],\n\"response_format\": {\n\"type\": \"json_schema\",\n\"json_schema\": {\n\"name\": \"Medical Record\",\n\"schema\": {\n\"properties\": {\n\"patient_id\": {\"title\": \"Patient Id\", \"type\": \"integer\"},\n\"date\": {\"title\": \"Date\", \"type\": \"string\", \"format\": \"date-time\"},\n\"diagnosis\": {\"title\": \"Diagnosis\", \"type\": \"string\"},\n\"treatment\": {\"title\": \"Treatment\", \"type\": \"string\"},\n},\n\"required\": [\"patient_id\", \"diagnosis\", \"treatment\"],\n\"title\": \"MedicalRecord\",\n\"type\": \"object\",\n},\n},\n\"max_tokens\": 1000,\n},\n}\nruntime = session.client(\"sagemaker-runtime\")\nresponse = runtime.invoke_endpoint(\nEndpointName=endpoint_name,\nContentType=\"application/json\",\nAccept=\"application/json\",\nBody=json.dumps(payload).encode(),\n)\nbody = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n# View the structured output produced by the model\nmsg = body[\"choices\"][0][\"message\"]\ncontent = msg[\"content\"]\nmedical_record = json.loads(content)\nmedical_record\n\nThis hybrid approach removes the need for retries compared to validation after completion.\nAlternative structured output options on AWS\nWhile Outlines offers generation-time consistency, several other approaches provide structured outputs with different trade-offs:\nAlternative 1: LLM-based structured output strategies\nWhen using most modern LLMs, such as Amazon Nova, users can define output schemas directly in prompts, supporting type constraints, enumerations, and structured templates within the AWS environment. The following guide shows different prompting patterns for Amazon Nova.\n\n# Example Nova structured output\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\n\nresponse = bedrock.invoke_model(\nmodelId='amazon.nova-pro-v1:0',\nbody=json.dumps({\n\"messages\": [{\"role\": \"user\", \"content\": \"Extract customer info from this text...\"}],\n\"inferenceConfig\": {\"maxTokens\": 500},\n\"toolConfig\": {\n\"tools\": [{\n\"toolSpec\": {\n\"name\": \"extract_customer\",\n\"inputSchema\": {\n\"json\": {\n\"type\": \"object\",\n\"properties\": {\n\"name\": {\"type\": \"string\"},\n\"email\": {\"type\": \"string\"},\n\"phone\": {\"type\": \"string\"}\n},\n\"required\": [\"name\", \"email\"]\n}\n}\n}\n}]\n}\n})\n)\n\nAlternative 2: Post-generation validation OSS frameworks\nPost-generation validation open-source frameworks have emerged as a critical layer in modern generative AI systems, providing structured, repeatable mechanisms to evaluate and govern model outputs before they are consumed by users or downstream applications. By separating generation from validation, these frameworks enable teams to enforce safety, quality, and policy constraints without constantly retraining or fine-tuning underlying models.\nLMQL\nLanguage Models Query Language (LMQL) has a SQL-like interface and provides a query language for LLMs, so that developers can specify constraints, type requirements, and value ranges directly in prompts. Particularly effective for multiple-choice and type constraints.\nInstructor\nInstructor provides retry mechanisms by wrapping LLM outputs with schema validation and automatic retry mechanisms. Tight integration with Pydantic models makes it suitable for scenarios where post-generation validation and correction are acceptable.\n\nimport boto3\nimport instructor\nfrom pydantic import BaseModel\n# Create a Bedrock client for runtime interactions\nbedrock_client = boto3.client('bedrock-runtime')\n# Set up the instructor client with Bedrock runtime\nclient = instructor.from_bedrock(bedrock_client)\n# Define the structured response model\nclass User(BaseModel):\nname: str\nage: int\n# Invoke the Claude Haiku model with the correct message structure\nuser = client.chat.completions.create(\nmodelId=\"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\nmessages=[\n{\"role\": \"user\", \"content\": [{\"text\": \"Extract: Jason is 25 years old\"}]},\n],\nresponse_model=User,\n)\nprint(user)\n# Expected output:\n# User “name='Jason' age=25”\n\nGuidance\nGuidance offers fine-grained template-driven control over output structure and formatting, allowing token-level constraints. Useful for consistent response formatting and conversational flows.\nDecision factors and best practices\nSelecting the right structured output approach depends on several key factors that directly impact implementation complexity and system performance.\n\nLatency considerations: Response time requirements significantly influence structured output solutions. By adding retry mechanisms, post-generation validation can add latency. In comparison, approaches like Outlines are optimal for latency-sensitive applications. Enforcing schemas adds some processing time compared to the base model used but is still much faster than post-generation strategies.\nRetry capabilities: Automatic regeneration capabilities (like those in Instructor) are essential for structured outputs because they provide fallback mechanisms when initial generation attempts fail to meet schema requirements, improving overall reliability without developer intervention.\nStreaming support: Partial JSON validation during streaming enables progressive content delivery while maintaining structural integrity, vital for responsive user experiences in applications requiring real-time structured data.\nInput complexity: Context trimming techniques optimize handling of complex inputs, helping to ensure that lengthy or intricate prompts don’t compromise the structured nature of outputs or exceed token limitations.\nDeployment strategy: While the ability to access models through the Amazon Bedrock API ( Converse , InvokeModel ) offers a serverless solution, Outlines is currently only available through AWS Marketplace on Amazon SageMaker, requiring you to deploy and host the model.\nModel selection: The choice of model significantly impacts structured output quality and efficiency. Base models might require extensive prompt engineering for structure compliance, while specialized models with built-in structured output capabilities offer higher reliability and reduced post-processing needs.\nUser experience : Each option provides pros and cons.\n\nIn-process validation (Outlines) catches errors early during generation, increasing speed when mistakes are made by the model but also increasing latency when model output was already correct.\nPost-generation validation provides comprehensive quality control but requires error handling for non-adherent outputs.\n\nPerformance: While implementing structured outputs can increase the model accuracy by reducing hallucinations and improving output consistency, some of these gains can come with tradeoffs such as a reduction of reasoning capabilities in some scenarios or introduction of additional token overhead.\n\nConclusion\nOrganizations can use the structured output paradigm in AI to reliably enforce schemas, integrate with a wide range of models and APIs, and balance post-generation validation versus direct generation methods for greater control and consistency. By understanding the trade-offs in performance, integration complexity, and schema enforcement, builders can tailor solutions to their technical and business requirements, facilitating scalable and efficient automation across diverse applications.\nTo learn more about implementing structured outputs with LLMs on AWS:\n\nVisit the AWS Machine Learning Blog for detailed tutorials and implementation guides\nExplore complete code examples in our Amazon Bedrock GitHub repository\nReview the Amazon Bedrock documentation for best practices on structured outputs\nLearn more about.txt from their blog\n\nAbout the Authors\n\nClement Perrot\nClement Perrot  is a Senior GenAI Strategist in the GenAI Innovation Center, where he helps early-stage startups build and use AI on the AWS platform. Prior to AWS, Clement was an entrepreneur, whose last two AI and consumer hardware startups were acquired.\n\nRemi Louf\nRemi Louf is the CEO and technical founder of Dottxt. Before founding dottxt, Remi was a Senior Research Engineer at Normal Computing, a Research Engineer at Ampersand, an early Research Engineer at Hugging Face, a Research Fellow at Harvard and Chief Science Officer at Vimies. Remi has a Doctorate in Statistical Physics, Masters in the Philosophy of Physics, Undergraduate degree in fundamental Physics and a Eleve Normalien (French research degree) in Quantum Physics.\n\nMax Elfrink\nMax Elfrink is an Account Manager on the AWS Startup’s Team, where he helps early-stage startups build, scale and grow their AI + Infrastructure on AWS. Prior to AWS, Max worked in startups for 6 years Supporting early stage startups in CDN, HCLS Tech, and Unicorn Tech-Enabled Freight Forwarder, Flexport.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "a12b4fe695837633",
    "title": "Global cross-Region inference for latest Anthropic Claude Opus, Sonnet and Haiku models on Amazon Bedrock in Thailand, Malaysia, Singapore, Indonesia, and Taiwan",
    "url": "https://aws.amazon.com/blogs/machine-learning/global-cross-region-inference-for-latest-anthropic-claude-opus-sonnet-and-haiku-models-on-amazon-bedrock-in-thailand-malaysia-singapore-indonesia-and-taiwan/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T15:38:22Z",
    "summary": "In this post, we are exciting to announce availability of Global CRIS for customers in Thailand, Malaysia, Singapore, Indonesia, and Taiwan and give a walkthrough of technical implementation steps, and cover quota management best practices to maximize the value of your AI Inference deployments. We also provide guidance on best practices for production deployments.",
    "content": "Organizations across in Thailand, Malaysia, Singapore, Indonesia, and Taiwan can now access Anthropic Claude Opus 4.6, Sonnet 4.6, and Claude Haiku 4.5 through Global cross-Region inference (CRIS) on Amazon Bedrock —delivering foundation models through a globally distributed inference architecture designed for scale. Global CRIS offers three key advantages: higher quotas, cost efficiency, and intelligent request routing to inference capacity across AWS commercial Regions for enabling AI use-cases like chatbots, autonomous coding agents, and financial analysis systems for customers.\nIn this post, we are exciting to announce availability of Global CRIS for customers in Thailand, Malaysia, Singapore, Indonesia, and Taiwan and give a walkthrough of technical implementation steps, and cover quota management best practices to maximize the value of your AI Inference deployments. We also provide guidance on best practices for production deployments.\nGlobal cross Region inference\nCRIS  is a powerful Amazon Bedrock capability that organizations can use to seamlessly distribute inference processing across multiple AWS Regions. This capability helps you achieve higher throughput while building at scale, helping to make sure your generative AI applications remain responsive and reliable even under heavy load.\nYou access CRIS through inference profiles , which operate on two key concepts:\n\nSource Region – The Region from which you make the API request\nDestination Region – A Region to which Amazon Bedrock can route the request for inference\n\nCRIS operates through the secure AWS network with end-to-end encryption for both data in transit and at rest. When you submit an inference request from a source Region, CRIS intelligently routes the request to one of the destination Regions configured for the inference profile over the Amazon Bedrock managed network. The inference request travels over the AWS global network by Bedrock and responses are returned to your application in the source Region.\nThe key distinction is that while inference processing (the transient computation) might occur in another Region, the data at rest—including logs, knowledge bases, and stored configurations—remains exclusively within your source Region. Amazon Bedrock provides two types of cross-Region inference profiles: Geographic CRIS (which routes within a specific geography such as US, EU, APAC, Australia, Japan) and Global CRIS (which routes to supported commercial Regions worldwide). Customers in Thailand, Malaysia, Singapore, Taiwan, and Indonesia can now access Claude Opus 4.6, Sonnet 4.6, and Haiku 4.5 through Global CRIS, which routes requests across Regions for higher throughput and built-in resilience during traffic spikes.\nWhy Global CRIS for Thailand, Malaysia, Singapore, Taiwan, and Indonesia\nAs organizations shift from conversational AI assistants to autonomous agents that plan, execute, and coordinate complex workflows, production AI deployments require more resilient and scalable infrastructure. Global CRIS delivers Claude Opus 4.6, Sonnet 4.6 and Haiku 4.5 through a high availability architecture designed to meet the demands of this shift to production-scale autonomous systems. As autonomous agents increasingly handle merchant operations, coordinate logistics networks, and automate financial workflows across use-cases for customers in Thailand, Malaysia, Singapore, Taiwan, and Indonesia, infrastructure reliability directly impacts the continuity of these autonomous decision-making systems. Global CRIS routes inference requests across more inference capacity on AWS Regions worldwide, reducing the likelihood that your applications experience service throttling during traffic spikes. This routing capability delivers built-in resilience, allowing your agentic applications to maintain operational continuity even as demand patterns shift.\nSource Regions configuration in Thailand, Malaysia, Singapore, Taiwan, and Indonesia\nAt launch, customers in Thailand, Malaysia, Singapore, Taiwan, and Indonesia can call Global CRIS profiles from the following source Regions:\n\nSource Region\nAWS Commercial Regions\nAvailability\nGlobal CRIS routing\n\nAsia Pacific (Singapore)\nap-southeast-1\nAvailable now\nRoutes to more than 20 supported AWS commercial Regions globally\n\nAsia Pacific (Jakarta)\nap-southeast-3\nAvailable now\nRoutes to more than 20 supported AWS commercial Regions globally\n\nAsia Pacific (Taipei)\nap-east-2\nAvailable now\nRoutes to more than 20 supported AWS commercial Regions globally\n\nAsia Pacific (Thailand)\nap-southeast-7\nAvailable now\nRoutes to more than 20 supported AWS commercial Regions globally\n\nAsia Pacific (Malaysia)\nap-southeast-5\nAvailable now\nRoutes to more than 20 supported AWS commercial Regions globally\n\nOnce invoked behind the scenes, Global CRIS will manage routing of requests to any supported commercial AWS Regions.\nPrerequisites\nBefore using Global CRIS, you need to configure IAM permissions that enable cross-Region routing for your inference requests.\nConfigure IAM permissions\nBefore you can invoke Claude models through Global CRIS, you must configure IAM permissions that account for the cross-Region routing architecture. The following section walks through the policy structure and explains why three separate statements are required.\nComplete the following steps to configure IAM permissions for Global CRIS. The IAM policy grants permission to invoke Claude models through Global CRIS. The policy requires three statements because CRIS routes requests across Regions: you call the inference profile in your source Region (Singapore or Jakarta), which then invokes the foundation model in whichever destination Region CRIS selects. The third statement uses  \"aws:RequestedRegion\": \"unspecified\"  to grant the necessary permissions for Global CRIS to route your requests across Regions.\nReplace <ACCOUNT> with your AWS account ID and adjust the source Region if using Jakarta ( ap-southeast-3 ) instead of Singapore ( ap-southeast-1 ).\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"GrantGlobalCrisInferenceProfileRegionAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"bedrock:InvokeModel\",\n            \"Resource\": [\n                \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-opus-4-6-v1\",\n                \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:RequestedRegion\": \"ap-southeast-1\"\n                }\n            }\n        },\n        {\n            \"Sid\": \"GrantGlobalCrisInferenceProfileInRegionModelAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"bedrock:InvokeModel\",\n            \"Resource\": [\n                \"arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-opus-4-6-v1\",\n                \"arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                \"arn:aws:bedrock:ap-southeast-1::foundation-model/anthropic.claude-haiku-4-5-20251001-v1:0\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:RequestedRegion\": \"ap-southeast-1\",\n                    \"bedrock:InferenceProfileArn\": [\n\"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-opus-4-6-v1\",                        \n\"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                        \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n                    ]\n                }\n            }\n        },\n        {\n            \"Sid\": \"GrantGlobalCrisInferenceProfileGlobalModelAccess\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"bedrock:InvokeModel\",\n            \"Resource\": [\n                \"arn:aws:bedrock:::foundation-model/anthropic.claude-opus-4-6-v1\",\n                \"arn:aws:bedrock:::foundation-model/anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                \"arn:aws:bedrock:::foundation-model/anthropic.claude-haiku-4-5-20251001-v1:0\"\n            ],\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"aws:RequestedRegion\": \"unspecified\",\n                    \"bedrock:InferenceProfileArn\": [\n                        \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-opus-4-6-v1\",\n                        \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-sonnet-4-5-20250929-v1:0\",\n                        \"arn:aws:bedrock:ap-southeast-1:<ACCOUNT>:inference-profile/global.anthropic.claude-haiku-4-5-20251001-v1:0\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n\nIt’s important to note that if your organization’s service control policies (SCPs) deny access to unspecified Regions, Global CRIS will not function. We recommend validating your SCP configuration before deploying production workloads that depend on global routing.\nIf your organization restricts AWS API calls to specific Regions, make sure your SCP includes \"unspecified\" in the approved Regions list. The following example shows how to configure an SCP that permits Global CRIS routing. Add your source Region for Global CRIS (Singapore ap-southeast-1 or Jakarta ap-southeast-3 ) along with other Regions your organization uses:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"DenyAllOutsideApprovedRegions\",\n            \"Effect\": \"Deny\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"StringNotEquals\": {\n                    \"aws:RequestedRegion\": [\n                        \"ap-southeast-1\",\n                        \"unspecified\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n\nWith IAM permissions configured, you can start invoking Claude models through Global CRIS using inference profiles and the Converse API.\nUse cross-Region inference profiles\nGlobal inference profiles are identified by the global. prefix in their model identifier—a naming convention that you can use to distinguish global routing profiles from Regional or single-Region model IDs. Use these inference profile IDs when making API calls instead of the standard model IDs:\n\nModel\nBase model ID\nGlobal inference profile ID\n\nClaude Sonnet 4.6\nanthropic.claude-sonnet-4-6\nglobal .anthropic.claude-sonnet-4-6\n\nClaude Opus 4.6\nanthropic.claude-opus-4-6-v1\nglobal. anthropic.claude-opus-4-6-v1\n\nClaude Sonnet 4.5\nanthropic.claude-sonnet-4-5-20250929-v1:0\nglobal .anthropic.claude-sonnet-4-5-20250929-v1:0\n\nClaude Haiku 4.5\nanthropic.claude-haiku-4-5-20251001-v1:0\nglobal .anthropic.claude-haiku-4-5-20251001-v1:0\n\nBoth the InvokeModel and Converse APIs support cross-Region inference profiles. We recommend using the Converse API—this approach provides a simplified interface and consistent request/response format across different foundation models, so you can switch between models without rewriting integration code.\nMake your first API call\nGetting started with Global CRIS requires only a few changes to your existing application code. The following code snippet demonstrates how to invoke Claude Opus 4.6 using Global CRIS in Python with the boto3 SDK:\n\nimport logging\nimport os\n\nimport boto3\nfrom botocore.exceptions import ClientError\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Load configuration from environment variables with defaults\nREGION = os.getenv(\n\"AWS_REGION\", \"ap-southeast-1\"\n) # Singapore or Jakarta (ap-southeast-3)\nMODEL_ID = os.getenv(\"MODEL_ID\", \"global.anthropic.claude-opus-4-6-v1\")\nMAX_TOKENS = int(os.getenv(\"MAX_TOKENS\", \"8000\"))\nTEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"1\"))\nTHINKING_TYPE = os.getenv(\"THINKING_TYPE\", \"adaptive\")\nEFFORT_LEVEL = os.getenv(\"EFFORT_LEVEL\", \"medium\")\n\n# Initialize Bedrock Runtime client for your Region\nbedrock_runtime = boto3.client(service_name=\"bedrock-runtime\", region_name=REGION)\n\n# Example: Architecture trade-offs analysis\nuser_query = \"Analyze the trade-offs between microservices and monolithic architectures for a mid-size SaaS company.\"\n\n# Make inference request using Converse API with adaptive thinking\ntry:\nresponse = bedrock_runtime.converse(\nmodelId=MODEL_ID,\nmessages=[{\"role\": \"user\", \"content\": [{\"text\": user_query}]}],\ninferenceConfig={\"maxTokens\": MAX_TOKENS, \"temperature\": TEMPERATURE},\nadditionalModelRequestFields={\n\"thinking\": {\"type\": THINKING_TYPE},\n\"output_config\": {\"effort\": EFFORT_LEVEL},\n},\n)\nexcept ClientError as e:\nlogger.error(\"Failed to invoke model %s: %s\", MODEL_ID, e)\nraise\n\n# Extract response content\noutput_message = response[\"output\"][\"message\"]\nhas_thinking = any(block.get(\"type\") == \"thinking\" for block in output_message[\"content\"])\n\nlogger.info(\"Effort level: %s\", EFFORT_LEVEL)\nlogger.info(\"Claude decided to think: %s\", has_thinking)\n\nfor block in output_message[\"content\"]:\nif block.get(\"type\") == \"thinking\":\nthinking_tokens = len(block[\"thinking\"].split())\nlogger.info(\"[Thinking]: ~%d words\", thinking_tokens)\nelif block.get(\"text\"):\nlogger.info(\"[Response]: %s\", block[\"text\"])\n\nIf this is your first time working with a cross-Region capability, you might expect that routing requests to multiple Regions would complicate your monitoring setup. With Global CRIS, that’s not the case. Your Amazon CloudWatch metrics, CloudWatch logs, and AWS CloudTrail audit logs remain in your source Region, even when inference requests are processed elsewhere. Your existing dashboards, alarms, and audit trail continue to work exactly as they do today.\nFor more information on the Converse API and available parameters, see the Amazon Bedrock API Reference . Building on this foundation, let’s explore quota management strategies to make sure your deployment can scale with demand.\nQuota management\nAs your application scales from prototype to production, understanding and managing service quotas becomes critical for maintaining consistent performance. This section covers how quotas work, how to monitor your usage, and how to request increases when needed.\nThe following figure shows the Amazon Bedrock Service Quotas page in the AWS console, where you can view your applied account-level quota values for Global CRIS inference profiles.\n\nUnderstanding quotas and planning for scale\nUnderstanding quotas and planning for scale is the first step in making sure your Global CRIS deployment can handle production traffic without throttling. Amazon Bedrock enforces service quotas to facilitate fair resource allocation and system stability. This consideration becomes critical as your application scales from prototype to production. For Global CRIS, quotas are measured in two dimensions, each serving a distinct purpose in capacity management:\n\nTokens per minute (TPM) – The maximum number of tokens (input + output) that can be processed per minute\nRequests per minute (RPM) – The maximum number of inference requests that can be made per minute\n\nDefault quotas vary by model and are allocated per source Region. You can view your current quotas in the AWS Service Quotas console by navigating to Amazon Bedrock service quotas in your source Region (Singapore or Jakarta).\nBe advised that Amazon Bedrock uses a token burndown rate that weighs output tokens more heavily than input tokens when calculating quota consumption. The burndown rate is 5:1—output tokens consume five times more quota than input tokens because generating tokens requires more computation than processing input.\nQuota consumption = Input tokens + (Output tokens × 5)\nFor example , if your request uses 10,000 input tokens and generates 5,000 output tokens:\nTotal quota consumption = 10,000 + (5,000 × 5) = 35,000 tokens\nThe request consumes 35,000 tokens against your TPM quota for throttling purposes. When planning capacity requirements and requesting quota increases, you need to account for this burndown rate in your calculations. If your application processes requests with this same token pattern at 100 requests per minute, the total quota consumption would be 3,500,000 TPM (100 requests × 35,000 tokens per request). When working with your AWS Account Manager on quota increase requests, provide your expected request volume, average input tokens per request, and average output tokens per request so they can calculate the appropriate quota allocation using this burndown multiplier.\nManaging quotas effectively\nWe recommend setting up CloudWatch alarms at 70–80% quota utilization to request increases before hitting throttling limits. The CloudWatch metrics InputTokenCount and OutputTokenCount track your consumption in real-time, while the InvocationClientErrors metric indicates throttling when it spikes—providing early warning signals for capacity planning. For detailed guidance on available metrics and how to configure monitoring for your Bedrock workloads, refer to Monitoring the performance of Amazon Bedrock .\nFor non-time-sensitive workloads, Claude Haiku 4.5 supports batch inference at 50% cost savings. Batch requests process asynchronously within 24 hours and don’t count against your real-time TPM quota.\nRequesting quota increases\nConsider the following factors when determining whether you need quota increases: workload scale (requests per minute during peak traffic), output token ratio (high output generation consumes quota faster), and growth projections (account for 6–12 month scaling needs). If your workload requires quotas beyond the default limits, you can request increases through the AWS Service Quotas console .\nComplete the following steps to request quota increases through the AWS Service Quotas console:\n\nSign in to the AWS Management Console for AWS Service Quotas in your source Region.\nNavigate to AWS services and select Amazon Bedrock .\nSearch for Global cross-Region model inference tokens per minute for your specific model.\nSelect the quota and choose Request increase at account level .\nEnter your desired quota value with justification for the increase.\nSubmit the request for AWS review.\n\nPlan ahead when requesting quota increases to help ensure capacity is available before your launch or scaling events. For large-scale deployments or time-sensitive launches, we recommend working with your AWS account team to help ensure appropriate capacity planning and expedited review. With quota management strategies in place, let’s explore how to choose between Opus 4.6, Sonnet 4.6 and Haiku 4.5 for your specific use cases.\nMigrating from Claude 3.x to Claude 4.5 / 4.6\nThe migration from Claude 3.x to Claude 4.5 / 4.6 represents a substantial technological leap for organizations using either Opus, Sonnet or Haiku versions. Claude’s hybrid reasoning architecture introduces substantial improvements in tool integration, memory management, and context processing capabilities.\nFor more technical implementation guidance, see the AWS blog post, Migrate from Anthropic’s Claude Sonnet 3.x to Claude Sonnet 4.x on Amazon Bedrock , which provides essential best practices that are also valid for the migration to the new Claude Sonnet 4.6 model. Additionally, Anthropic’s migration documentation offers model-specific optimization strategies and considerations for transitioning to Claude 4.5 / 4.6 models.\nBest practices\nConsider the following optimization techniques to maximize performance and minimize costs for your workloads:\n1. Prompt caching for repeated context\nPrompt caching delivers up to 90% cost reduction on cached tokens and up to 85% latency improvement for workloads that repeatedly use the same context. Cache system prompts exceeding 500 tokens, documentation content, few-shot examples, and tool definitions. Structure prompts with static content first, followed by dynamic queries. See  Prompt caching for faster model inference User Guide for implementation details.\n2. Model selection strategy\nConsider task complexity, latency requirements, cost constraints, and accuracy needs when choosing between models. We recommend Claude Opus 4.6 for the most complex tasks requiring frontier intelligence, such as complex multi-step reasoning, sophisticated autonomous agents, and precision-critical analysis. Claude Sonnet 4.6 is well suited for complex problems requiring agent planning and execution. Claude Haiku 4.5 delivers near-frontier performance at lower cost, making it optimal for high-volume operations and latency-sensitive experiences. For multi-agent architectures, consider using Opus 4.6 or Sonnet 4.6 as orchestrator and Haiku 4.5 for parallel execution workers.\n3. Adaptive and extended thinking for complex tasks\nClaude Opus 4.6 supports adaptive thinking, an evolution of extended thinking that gives Claude the freedom to think if and when it determines reasoning is required. You can guide how much thinking Claude allocates using the effort parameter, optimizing both performance and speed. Sonnet 4.6 and Haiku 4.5 support extended thinking, where the model generates intermediate reasoning steps through problem decomposition, self-correction, and exploring multiple solution paths. These thinking capabilities deliver accuracy improvements on complex reasoning tasks, so enable them selectively where accuracy improvements justify the additional quota usage.\n4. Load testing for quota validation\nRun load tests before production launch to measure actual quota consumption under peak traffic. Configure your test client with adaptive retry mode (Config(retries={‘mode’: ‘adaptive’})) to handle throttling during the test, use tools like Locust or boto3 with threading to simulate concurrent requests, and monitor the CloudWatch metrics during your load test to observe TPM and RPM consumption patterns. A test with 20 concurrent threads making continuous requests will quickly reveal whether your quota allocation matches your expected load.\nSummary and next steps\nGlobal cross-Region inference on Amazon Bedrock delivers Claude Opus 4.6, Sonnet 4.6, and Haiku 4.5 models to organizations in Thailand, Malaysia, Singapore, Taiwan, and Indonesia with two key advantages: cost savings compared to Regional profiles, and intelligent routing across more than 20 AWS Regions for maximum availability and scale.\nThis infrastructure enables production AI applications across Southeast Asia, from real-time customer service to financial analysis and autonomous coding assistants. Claude Opus 4.6 provides intelligence for the most demanding enterprise workloads, Sonnet 4.6 delivers balanced performance for daily production use cases, and Haiku 4.5 enables cost-efficient high-volume operations. For multi-agent architectures, combine these models to optimize for both quality and economics.\nWe encourage you to get started today with Global cross-Region inference in your applications. Complete the following steps to begin:\n\nSign in to the Amazon Bedrock console in any of the source Regions listed above, e.g. Singapore ( ap-southeast-1 ) or Jakarta ( ap-southeast-3 ).\nConfigure IAM permissions using the policy template provided in this post.\nMake your first API call using the global inference profile ID.\nImplement prompt caching for cost savings on repeated context.\n\nFor more information:\n\nAmazon Bedrock Documentation\nCross-Region inference User Guide\nAmazon Bedrock pricing\nAnthropic Claude Documentation\nGet started with Amazon Bedrock\n\nAbout the Authors\n\nTraci Lim\nTraci Lim is a Senior AI/ML Specialist Technical Account Manager at AWS based in Singapore. A machine learning engineer by trade, he works with startups and enterprises to operationalize and scale AI/ML applications in production, with a focus on GenAIOps, Agentic Ops, operational excellence, and cost and performance optimization. Prior to AWS, Traci led engineering teams in the tech and financial industries, scaling distributed AI systems across AWS, Azure, GCP, and SAP. He is a builder at heart, always looking for ways to create meaningful impact through technology.\n\nVincent Wang\nVincent Wang serves as a GenAI Specialist Solutions Architect at AWS, based in Sydney, Australia. Drawing on more than 8 years of experience in cloud computing, he plays a key role in designing and consulting on modern cloud-native architectures that enable customers to harness the power of AI and machine learning for their businesses. His areas of expertise include AI, Agentic AI, and open source software.\n\nChanmi EUN\nChanmi EUN is a Senior Go-to-Market Specialist for Generative AI at Amazon Web Services in Singapore, where she drives adoption of cutting-edge AI technologies among startup customers. She crafts strategic initiatives, develops impactful sales plays, and orchestrates partnerships to accelerate generative AI adoption across the Asia-Pacific Region. Drawing on rich experience in the tech industry, Chanmi seamlessly combines deep expertise with her multilingual capabilities to deliver transformative results in the rapidly evolving AI landscape.\n\nMelanie Li\nMelanie Li , PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries.\n\nSaurabh Trikande\nSaurabh Trikande is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family.\n\nSharadha Kandasubramanian\nSharadha Kandasubramanian  is a Senior Technical Program Manager for Amazon Bedrock. She drives cross-functional GenAI programs for Amazon Bedrock, enabling customers to grow and scale their GenAI workloads. Outside of work, she’s an avid runner and biker who loves spending time outdoors in the sun.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "6790ef2bb0f1223a",
    "title": "Introducing Amazon Bedrock global cross-Region inference for Anthropic’s Claude models in the Middle East Regions (UAE and Bahrain)",
    "url": "https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-global-cross-region-inference-for-anthropics-claude-models-in-the-middle-east-regions/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T15:33:51Z",
    "summary": "We’re excited to announce the availability of Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 through Amazon Bedrock global cross-Region inference for customers operating in the Middle East. In this post, we guide you through the capabilities of each Anthropic Claude model variant, the key advantages of global cross-Region inference including improved resilience, real-world use cases you can implement, and a code example to help you start ...",
    "content": "We’re excited to announce the availability of Anthropic’s Claude Opus 4.6 , Claude Sonnet 4.6 , Claude Opus 4.5 , Claude Sonnet 4.5 , and Claude Haiku 4.5 through Amazon Bedrock global cross-Region inference for customers operating in the Middle East. This launch supports organizations in the Middle East to access Anthropic’s latest Claude models on Amazon Bedrock while benefiting from global, highly available inference routing across the AWS network. With global cross-Region inference, you can scale inference workloads seamlessly, improve resiliency, and reduce operational complexity.\nTo help you achieve the scale of your AI applications, Amazon Bedrock offers cross-Region inference profiles, a powerful feature organizations can use to seamlessly distribute inference processing across multiple AWS Regions. This capability helps you get higher throughput while you’re building at scale and helps keep your generative AI applications responsive and reliable even under heavy load. When you invoke a cross-Region inference profile in Amazon Bedrock, your request follows an intelligent routing path. The request originates from your source Region where you make the API call and is automatically routed to one of the destination Regions defined in the inference profile. Cross-Region inference operates through the secure AWS network with end-to-end encryption for data in transit.\nThe key distinction is that cross-Region inference doesn’t change where data is stored—customer data is not stored in a destination Region when using cross-Region inference; customer-managed logs (such as model invocation logging), knowledge bases, and stored configurations remain exclusively within the source Region. The inference request travels over the AWS Global Network managed by Amazon Bedrock, and responses are returned encrypted to your application in the source Region.\nIn this post, we discuss how to use global cross-Region inference in Amazon Bedrock for Anthropic Claude models in the Middle East. We guide you through the capabilities of each Anthropic Claude model variant, the key advantages of global cross-Region inference including improved resilience, real-world use cases you can implement, and a code example to help you start building generative AI applications immediately.\nAnthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 on Amazon Bedrock\nThe latest generation of Anthropic’s Claude models are now available on Amazon Bedrock in the Middle East (UAE) and Middle East (Bahrain) Regions. The new Claude Opus 4.6 brings advanced capabilities to Amazon Bedrock customers, including industry-leading performance for agentic tasks, complex coding projects, and enterprise-grade workflows that require deep reasoning and reliability. Claude Sonnet 4.6 balances intelligence with speed and cost-efficiency for production-ready applications and multi-step tasks. Claude Haiku 4.5 focuses on low-latency responses for real-time use cases like AI assistants and high-volume content generation. By combining these models with global cross-Region inference, you can dynamically scale your AI workloads across Regions while maintaining optimal performance. This helps organizations select the right model for their specific requirements—whether prioritizing intelligence, speed, or cost—while benefiting from seamless scaling and improved availability across global infrastructure.\nThe following table summarizes the available models and their source and destination Regions.\n\nModel\nSource Region\nDestination Region\n\nAnthropic Opus 4.6\nme-central-1 (UAE), me-south-1 (Bahrain)\nCommercial Regions\n\nAnthropic Sonnet 4.6\nme-central-1 (UAE), me-south-1 (Bahrain)\nCommercial Regions\n\nAnthropic Haiku 4.5\nme-central-1 (UAE), me-south-1 (Bahrain)\nCommercial Regions\n\nAnthropic Sonnet 4.5\nme-central-1 (UAE), me-south-1 (Bahrain)\nCommercial Regions\n\nAnthropic Opus 4.5\nme-central-1 (UAE), me-south-1 (Bahrain)\nCommercial Regions\n\nBenefits of global cross-Region inference\nAs generative AI adoption accelerates, customers increasingly require the ability to scale inference workloads reliably while maintaining consistent performance. Deploying large-scale generative AI applications often involves managing Regional capacity constraints, traffic spikes, and availability requirements. Amazon Bedrock global cross-Region inference addresses these challenges by allowing inference requests to be automatically routed to the optimal Region within a predefined global inference profile, helping deliver multiple advantages:\n\nEnhanced throughput during peak demand – For organizations in the Middle East, global cross-Region inference provides critical resilience during Regional peak periods, such as Ramadan, major shopping events, or high-traffic business hours. The system automatically routes requests to Regions with available capacity across the global infrastructure, making sure your applications maintain performance even during unexpected traffic surges. This dynamic routing happens seamlessly, and traffic routing is fully managed by Amazon Bedrock. For business-critical applications serving customers across the GCC and broader MENAT Region, this means avoiding costly downtime or degraded performance that could impact revenue and customer trust.\nSecure data transmission – The data transmitted during cross-Region operations is managed by Amazon Bedrock. Data is encrypted in transit between Regions, helping meet the stringent security and data protection requirements important to organizations in the Middle East.\nSimplified multi-Region strategy – Organizations no longer need to architect complex multi-Region deployments manually. Global cross-Region inference helps provide enterprise-grade resilience without the operational overhead of managing multiple Regional endpoints.\nSupport for rapid digital transformation – As Middle East organizations accelerate their digital transformation initiatives aligned with national visions (like Saudi Vision 2030 and UAE’s AI Strategy), global cross-Region inference provides the scalability needed to support ambitious AI projects without capacity constraints.\nStreamlined monitoring – Amazon CloudWatch and AWS CloudTrail continue to record the log entries in your Middle East source Region, providing a centralized view of your application’s performance. This simplified observability means your teams can monitor and manage generative AI applications using familiar AWS tools, regardless of where requests are processed globally, making compliance and operational management more straightforward.\nOn-demand quota flexibility – Global cross-Region inference helps remove the constraints of individual Regional capacity limits. Your workloads can dynamically access resources across the AWS global infrastructure, making it seamless to handle high-volume applications and sudden traffic spikes common in the rapidly growing digital economy of the Region.\n\nWith this capability now available for Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 in the Middle East, organizations across the Region can build and scale generative AI applications with greater confidence, knowing they can access enterprise-grade resilience and performance.\nGlobal inference use cases\nThe availability of Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 through global cross-Region inference unlocks a wide range of use cases for customers in the Middle East, including:\n\nEnterprise copilots and AI assistants that require high availability and consistent performance\nAgentic workflows that orchestrate complex reasoning and tool usage\nDeveloper productivity tools for code generation, review, and transformation\nCustomer engagement applications requiring elastic scale\nAdvanced data analysis and document processing\n\nQuota management\nTo see the default quotas for cross-Region throughput when using global inference profiles, refer to the global cross-Region model inference requests per minute and global cross-Region model inference tokens per minute values in Amazon Bedrock service quotas .\nYou can request, view, and manage quotas for the global cross-Region inference profile from the Service Quotas console or by using AWS Command Line Interface (AWS CLI) commands in your source Region.\nGetting started\nTo start using Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, or Claude Haiku 4.5 with global cross-Region inference (for example, the me-central-1 Region), complete the following steps:\n\nVerify your AWS Identity and Access Management (IAM) role or user has the necessary permissions to invoke Amazon Bedrock models using a cross-Region inference profile.\nInvoke the model using the Amazon Bedrock APIs or AWS SDKs:\n\nimport boto3\nimport json\nbedrock = boto3.client('bedrock-runtime', region_name='me-central-1')\nmodel_id = \"global.anthropic.claude-sonnet-4-6\"\nresponse = bedrock.converse(\nmessages=[{\"role\": \"user\", \"content\": [{\"text\": \"Explain cloud computing in 2 sentences.\"}]}],\nmodelId=model_id,\n)\n\nprint(\"Response:\", response['output']['message']['content'][0]['text'])\nprint(\"Token usage:\", response['usage'])\nprint(\"Total tokens:\", response['usage']['totalTokens'])\n\nYou can monitor usage, performance, and costs through CloudWatch and AWS Cost Explorer to scale your applications as demand grows.\nConclusion\nWith the launch of Anthropic’s Claude Opus 4.6, Claude Sonnet 4.6, Claude Opus 4.5, Claude Sonnet 4.5, and Claude Haiku 4.5 using Amazon Bedrock global cross-Region inference, customers in the Middle East can now build highly scalable, resilient generative AI applications without the operational overhead of managing Regional inference capacity. We are excited about this launch and look forward to seeing how you use these capabilities to accelerate innovation and deliver impactful AI-powered experiences across the Region. To learn more, see Getting started with cross-region inference in Amazon Bedrock .\n\nAbout the Authors\n\nHossam Basudan\nHossam Basudan is a Senior Specialist Solutions Architect based in Dubai, UAE. He works with AWS customers to efficiently train and deploy their foundation and AI/ML models at scale. He has a background in distributed systems and applied mathematics. Hossam is passionate about High Performance Computing (HPC) for large-scale AI workloads.\n\nSam Dabboussi\nSam Dabboussi is a Principal Go-To-Market Specialist based in Dubai, with over a decade of experience in technology sales and business development. He has held leadership roles at companies like Amazon Web Services, Qlik, and Sophos, driving revenue growth and strategic partnerships.\n\nSaurabh Trikande\nSaurabh Trikande is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family.\n\nMelanie Li\nMelanie Li , PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "11636271aa8ef8e2",
    "title": "Arvind KC appointed Chief People Officer",
    "url": "https://openai.com/index/arvind-kc-chief-people-officer",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-24T13:40:00Z",
    "summary": "OpenAI appoints Arvind KC as Chief People Officer to help scale the company, strengthen its culture, and lead how work evolves in the age of AI.",
    "content": "OpenAI appoints Arvind KC as Chief People Officer to help scale the company, strengthen its culture, and lead how work evolves in the age of AI.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]