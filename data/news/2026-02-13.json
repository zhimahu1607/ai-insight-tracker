[
  {
    "id": "63297c2e71c46399",
    "title": "AI meets HR: Transforming talent acquisition with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/ai-meets-hr-transforming-talent-acquisition-with-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-12T20:18:58Z",
    "summary": "In this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases, AWS Lambda, and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.",
    "content": "Organizations face significant challenges in making their recruitment processes more efficient while maintaining fair hiring practices. By using AI to transform their recruitment and talent acquisition processes, organizations can overcome these challenges. AWS offers a suite of AI services that can be used to significantly enhance the efficiency, effectiveness, and fairness of hiring practices. With AWS AI services, specifically Amazon Bedrock , you can build an efficient and scalable recruitment system that streamlines hiring processes, helping human reviewers focus on the interview and assessment of candidates.\nIn this post, we show how to create an AI-powered recruitment system using Amazon Bedrock, Amazon Bedrock Knowledge Bases , AWS Lambda , and other AWS services to enhance job description creation, candidate communication, and interview preparation while maintaining human oversight.\nThe AI-powered recruitment lifecycle\nThe recruitment process presents numerous opportunities for AI enhancement through specialized agents , each powered by Amazon Bedrock and connected to dedicated Amazon Bedrock knowledge bases. Let’s explore how these agents work together across key stages of the recruitment lifecycle.\nJob description creation and optimization\nCreating inclusive and attractive job descriptions is crucial for attracting diverse talent pools. The Job Description Creation and Optimization Agent uses advanced language models available in Amazon Bedrock and connects to an Amazon Bedrock knowledge base containing your organization’s historical job descriptions and inclusion guidelines.\nDeploy the Job Description Agent with a secure Amazon Virtual Private Cloud (Amazon VPC) configuration and AWS Identity and Access Management (IAM) roles . The agent references your knowledge base to optimize job postings while maintaining compliance with organizational standards and inclusive language requirements.\nCandidate communication management\nThe Candidate Communication Agent manages candidate interactions through the following components:\n\nLambda functions that trigger communications based on workflow stages\nAmazon Simple Notification Service (Amazon SNS) for secure email and text delivery\nIntegration with approval workflows for regulated communications\nAutomated status updates based on candidate progression\n\nConfigure the Communication Agent with proper VPC endpoints and encryption for all data in transit and at rest. Use Amazon CloudWatch monitoring to track communication effectiveness and response rates.\nInterview preparation and feedback\nThe Interview Prep Agent supports the interview process by:\n\nAccessing a knowledge base containing interview questions, SOPs, and best practices\nGenerating contextual interview materials based on role requirements\nAnalyzing interviewer feedback and notes using Amazon Bedrock to identify key sentiments and consistent themes across evaluations\nMaintaining compliance with interview standards stored in the knowledge base\n\nAlthough the agent provides interview structure and guidance, interviewers maintain full control over the conversation and evaluation process.\nSolution overview\nThe architecture brings together the recruitment agents and AWS services into a comprehensive recruitment system that enhances and streamlines the hiring process.The following diagram shows how three specialized AI agents work together to manage different aspects of the recruitment process, from job posting creation through summarizing interview feedback. Each agent uses Amazon Bedrock and connects to dedicated Amazon Bedrock knowledge bases while maintaining security and compliance requirements.\n\nThe solution consists of three main components working together to improve the recruitment process:\n\nJob Description Creation and Optimization Agent – The Job Description Creation and Optimization Agent uses the AI capabilities of Amazon Bedrock to create and refine job postings, connecting directly to an Amazon Bedrock knowledge base that contains example descriptions and best practices for inclusive language.\nCandidate Communication Agent – For candidate communications, the dedicated agent streamlines interactions through an automated system. It uses Lambda functions to manage communication workflows and Amazon SNS for reliable message delivery. The agent maintains direct connections with candidates while making sure communications follow approved templates and procedures.\nInterview Prep Agent – The Interview Prep Agent serves as a comprehensive resource for interviewers, providing guidance on interview formats and questions while helping structure, summarize, and analyze feedback. It maintains access to a detailed knowledge base of interview standards and uses the natural language processing capabilities of Amazon Bedrock to analyze interview feedback patterns and themes, helping maintain consistent evaluation practices across hiring teams.\n\nPrerequisites\nBefore implementing this AI-powered recruitment system, make sure you have the following:\n\nAWS account and access:\n\nAn AWS account with administrator access\nAccess to Amazon Bedrock foundation models (FMs)\nPermissions to create and manage IAM roles and policies\n\nAWS services required:\n\nAmazon API Gateway\nAmazon Bedrock with access to FMs\nAmazon Bedrock Knowledge Bases\nAmazon CloudWatch\nAWS Key Management Service (AWS KMS)\nAWS Lambda\nAmazon SNS\nAmazon Simple Storage Service (Amazon S3) for knowledge base storage\nAmazon VPC\n\nTechnical requirements:\n\nBasic knowledge of Python 3.9 or later (for Lambda functions)\nNetwork access to configure VPC endpoints\n\nSecurity and compliance:\n\nUnderstanding of AWS security best practices\nSSL/TLS certificates for secure communications\nCompliance approval from your organization’s security team\n\nIn the following sections, we examine the key components that make up our AI-powered recruitment system. Each piece plays a crucial role in creating a secure, scalable, and effective solution. We start with the infrastructure definition and work our way through the deployment, knowledge base integration, core AI agents, and testing tools.\nInfrastructure as code\nThe following AWS CloudFormation template defines the complete AWS infrastructure, including VPC configuration, security groups, Lambda functions, API Gateway, and knowledge bases. It facilities secure, scalable deployment with proper IAM roles and encryption.\n\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: 'AI-Powered Recruitment System with Security and Knowledge Bases'\n\nParameters:\n  Environment:\n    Type: String\n    Default: dev\n    AllowedValues: [dev, prod]\n\nResources:\n  # KMS Key for encryption\n  RecruitmentKMSKey:\n    Type: AWS::KMS::Key\n    Properties:\n      Description: \"Encryption key for recruitment system\"\n      KeyPolicy:\n        Statement:\n          - Effect: Allow\n            Principal:\n              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'\n            Action: 'kms:*'\n            Resource: '*'\n\n  RecruitmentKMSAlias:\n    Type: AWS::KMS::Alias\n    Properties:\n      AliasName: !Sub 'alias/recruitment-${Environment}'\n      TargetKeyId: !Ref RecruitmentKMSKey\n\n  # VPC Configuration\n  RecruitmentVPC:\n    Type: AWS::EC2::VPC\n    Properties:\n      CidrBlock: 10.0.0.0/16\n      EnableDnsHostnames: true\n      EnableDnsSupport: true\n      Tags:\n        - Key: Name\n          Value: !Sub 'recruitment-vpc-${Environment}'\n\n  PrivateSubnet:\n    Type: AWS::EC2::Subnet\n    Properties:\n      VpcId: !Ref RecruitmentVPC\n      CidrBlock: 10.0.1.0/24\n      AvailabilityZone: !Select [0, !GetAZs '']\n \n PrivateSubnetRouteTable:\n    Type: AWS::EC2::RouteTable\n    Properties:\n      VpcId: !Ref RecruitmentVPC\n      Tags:\n        - Key: Name\n          Value: !Sub 'recruitment-private-rt-\\${Environment}'\n \n PrivateSubnetRouteTableAssociation:\n    Type: AWS::EC2::SubnetRouteTableAssociation\n    Properties:\n      SubnetId: !Ref PrivateSubnet\n      RouteTableId: !Ref PrivateSubnetRouteTable\n \n# Example Interface Endpoints\nVPCEBedrockRuntime:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcId: !Ref RecruitmentVPC\n    ServiceName: !Sub 'com.amazonaws.${AWS::Region}.bedrock-runtime'\n    VpcEndpointType: Interface\n    SubnetIds: [ !Ref PrivateSubnet ]\n    SecurityGroupIds: [ !Ref LambdaSecurityGroup ]\n\nVPCEBedrockAgent:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcId: !Ref RecruitmentVPC\n    ServiceName: !Sub 'com.amazonaws.${AWS::Region}.bedrock-agent'\n    VpcEndpointType: Interface\n    SubnetIds: [ !Ref PrivateSubnet ]\n    SecurityGroupIds: [ !Ref LambdaSecurityGroup ]\n\nVPCESNS:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcId: !Ref RecruitmentVPC\n    ServiceName: !Sub 'com.amazonaws.${AWS::Region}.sns'\n    VpcEndpointType: Interface\n    SubnetIds: [ !Ref PrivateSubnet ]\n    SecurityGroupIds: [ !Ref LambdaSecurityGroup ]\n\n# Gateway endpoints for S3 (and DynamoDB if you add it later)\nVPCES3:\n  Type: AWS::EC2::VPCEndpoint\n  Properties:\n    VpcId: !Ref RecruitmentVPC\n    ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'\n    VpcEndpointType: Gateway\n    RouteTableIds:\n      - !Ref PrivateSubnetRouteTable   # create if not present\n  # Security Group\n  LambdaSecurityGroup:\n    Type: AWS::EC2::SecurityGroup\n    Properties:\n      GroupDescription: Security group for recruitment AWS Lambda functions\n      VpcId: !Ref RecruitmentVPC\n      SecurityGroupEgress:\n        - IpProtocol: tcp\n          FromPort: 443\n          ToPort: 443\n          CidrIp: 0.0.0.0/0\n\n  # KnowledgeBase IAM role\n  KnowledgeBaseRole:\n  Type: AWS::IAM::Role\n  Properties:\n    AssumeRolePolicyDocument:\n      Version: '2012-10-17'\n      Statement:\n        - Effect: Allow\n          Principal: { Service: bedrock.amazonaws.com }\n          Action: sts:AssumeRole\n    Policies:\n      - PolicyName: BedrockKBAccess\n        PolicyDocument:\n          Version: '2012-10-17'\n          Statement:\n            - Effect: Allow\n              Action:\n                - bedrock:Retrieve\n                - bedrock:RetrieveAndGenerate\n              Resource: \"*\"\n            - Effect: Allow\n              Action:\n                - s3:GetObject\n                - s3:ListBucket\n              Resource: \"*\"   # scope to your KB bucket(s) in real deployments\n\n    JobDescriptionKnowledgeBase:\n        Type: AWS::Bedrock::KnowledgeBase\n        Properties:\n            Name: !Sub 'job-descriptions-${Environment}'\n            RoleArn: !GetAtt KnowledgeBaseRole.Arn\n            KnowledgeBaseConfiguration:\n                Type: VECTOR\n                VectorKnowledgeBaseConfiguration:\n                    EmbeddingModelArn: !Sub 'arn:aws:bedrock:\\${AWS::Region}::foundation-model/amazon.titan-embed-text-v1'\n            StorageConfiguration:\n                Type: S3\n                S3Configuration:\n                    BucketArn: !Sub 'arn:aws:s3:::your-kb-bucket-${Environment}-${AWS::AccountId}-${AWS::Region}'\n                    BucketOwnerAccountId: !Ref AWS::AccountId\n\n    InterviewKnowledgeBase:\n        Type: AWS::Bedrock::KnowledgeBase\n        Properties:\n            Name: !Sub 'interview-standards-${Environment}'\n            RoleArn: !GetAtt KnowledgeBaseRole.Arn\n            KnowledgeBaseConfiguration:\n                Type: VECTOR\n                VectorKnowledgeBaseConfiguration:\n                   EmbeddingModelArn: arn:aws:bedrock:${AWS::Region}::foundation-model/amazon.titan-embed-text-v2:0\n            StorageConfiguration:\n                Type: S3\n                S3Configuration:\n                    BucketArn: !Sub 'arn:aws:s3:::your-kb-bucket-${Environment}-${AWS::AccountId}-${AWS::Region}'\n                    BucketOwnerAccountId: !Ref AWS::AccountId\n\n  # CloudTrail for audit logging\n  RecruitmentCloudTrail:\n    Type: AWS::CloudTrail::Trail\n    Properties:\n      TrailName: !Sub 'recruitment-audit-${Environment}'\n      S3BucketName: !Ref AuditLogsBucket\n      IncludeGlobalServiceEvents: true\n      IsMultiRegionTrail: true\n      EnableLogFileValidation: true\n      KMSKeyId: !Ref RecruitmentKMSKey\n\n  AuditLogsBucket:\n    Type: AWS::S3::Bucket\n    Properties:\n      BucketName: !Sub 'recruitment-audit-logs-${Environment}-${AWS::AccountId}-${AWS::Region}'\n      BucketEncryption:\n        ServerSideEncryptionConfiguration:\n          - ServerSideEncryptionByDefault:\n              SSEAlgorithm: aws:kms\n              KMSMasterKeyID: !Ref RecruitmentKMSKey\n  # IAM Role for AWS Lambda functions\n  LambdaExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n      Policies:\n        - PolicyName: BedrockAccess\n          PolicyDocument:\n            Version: '2012-10-17'\n            Statement:\n              - Effect: Allow\n                Action:\n                  - bedrock:InvokeModel\n                  - bedrock:Retrieve\n                Resource: '*'\n              - Effect: Allow\n                Action:\n                  - sns:Publish\n                Resource: !Ref CommunicationTopic\n              - Effect: Allow\n                Action:\n                  - kms:Decrypt\n                  - kms:GenerateDataKey\n                Resource: !GetAtt RecruitmentKMSKey.Arn\n              - Effect: Allow\n                Action:\n                  - aoss:APIAccessAll\n                Resource: '*'\n\n  # SNS Topic for notifications\n  CommunicationTopic:\n    Type: AWS::SNS::Topic\n    Properties:\n      TopicName: !Sub 'recruitment-notifications-${Environment}'\n\n  # AWS Lambda Functions\n  JobDescriptionFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: !Sub 'recruitment-job-description-${Environment}'\n      Runtime: python3.11\n      Handler: job_description_agent.lambda_handler\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Code:\n        ZipFile: |\n          # Code will be deployed separately\n          def lambda_handler(event, context):\n              return {'statusCode': 200, 'body': 'Placeholder'}\n      Timeout: 60\n\n  CommunicationFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: !Sub 'recruitment-communication-${Environment}'\n      Runtime: python3.11\n      Handler: communication_agent.lambda_handler\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Code:\n        ZipFile: |\n          def lambda_handler(event, context):\n              return {'statusCode': 200, 'body': 'Placeholder'}\n      Timeout: 60\n      Environment:\n        Variables:\n          SNS_TOPIC_ARN: !Ref CommunicationTopic\n          KMS_KEY_ID: !Ref RecruitmentKMSKey\n      VpcConfig:\n        SecurityGroupIds:\n          - !Ref LambdaSecurityGroup\n        SubnetIds:\n          - !Ref PrivateSubnet\n\n  InterviewFunction:\n    Type: AWS::Lambda::Function\n    Properties:\n      FunctionName: !Sub 'recruitment-interview-${Environment}'\n      Runtime: python3.11\n      Handler: interview_agent.lambda_handler\n      Role: !GetAtt LambdaExecutionRole.Arn\n      Code:\n        ZipFile: |\n          def lambda_handler(event, context):\n              return {'statusCode': 200, 'body': 'Placeholder'}\n      Timeout: 60\n\n  # API Gateway\n  RecruitmentAPI:\n    Type: AWS::ApiGateway::RestApi\n    Properties:\n      Name: !Sub 'recruitment-api-${Environment}'\n      Description: 'API for AI-Powered Recruitment System'\n\n  # API Gateway Resources and Methods\n  JobDescriptionResource:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ParentId: !GetAtt RecruitmentAPI.RootResourceId\n      PathPart: job-description\n\n  JobDescriptionMethod:\n    Type: AWS::ApiGateway::Method\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ResourceId: !Ref JobDescriptionResource\n      HttpMethod: POST\n      AuthorizationType: NONE\n      Integration:\n        Type: AWS_PROXY\n        IntegrationHttpMethod: POST\n        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${JobDescriptionFunction.Arn}/invocations'\n\n  CommunicationResource:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ParentId: !GetAtt RecruitmentAPI.RootResourceId\n      PathPart: communication\n\n  CommunicationMethod:\n    Type: AWS::ApiGateway::Method\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ResourceId: !Ref CommunicationResource\n      HttpMethod: POST\n      AuthorizationType: NONE\n      Integration:\n        Type: AWS_PROXY\n        IntegrationHttpMethod: POST\n        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${CommunicationFunction.Arn}/invocations'\n\n  InterviewResource:\n    Type: AWS::ApiGateway::Resource\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ParentId: !GetAtt RecruitmentAPI.RootResourceId\n      PathPart: interview\n\n  InterviewMethod:\n    Type: AWS::ApiGateway::Method\n    Properties:\n      RestApiId: !Ref RecruitmentAPI\n      ResourceId: !Ref InterviewResource\n      HttpMethod: POST\n      AuthorizationType: NONE\n      Integration:\n        Type: AWS_PROXY\n        IntegrationHttpMethod: POST\n        Uri: !Sub 'arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${InterviewFunction.Arn}/invocations'\n\n  # Lambda Permissions\n  JobDescriptionPermission:\n    Type: AWS::Lambda::Permission\n    Properties:\n      FunctionName: !Ref JobDescriptionFunction\n      Action: lambda:InvokeFunction\n      Principal: apigateway.amazonaws.com\n      SourceArn: !Sub '${RecruitmentAPI}/*/POST/job-description'\n\n  CommunicationPermission:\n    Type: AWS::Lambda::Permission\n    Properties:\n      FunctionName: !Ref CommunicationFunction\n      Action: lambda:InvokeFunction\n      Principal: apigateway.amazonaws.com\n      SourceArn: !Sub '${RecruitmentAPI}/*/POST/communication'\n      \n  InterviewPermission:\n    Type: AWS::Lambda::Permission\n    Properties:\n      FunctionName: !Ref InterviewFunction\n      Action: lambda:InvokeFunction\n      Principal: apigateway.amazonaws.com\n      SourceArn: !Sub '${RecruitmentAPI}/*/POST/interview'\n      \n  # API Deployment\n  APIDeployment:\n  Type: AWS::ApiGateway::Deployment\n  DependsOn:\n    - JobDescriptionMethod\n    - CommunicationMethod\n    - InterviewMethod\n    - JobDescriptionPermission\n    - CommunicationPermission\n    - InterviewPermission\n  Properties:\n    RestApiId: !Ref RecruitmentAPI\n    StageName: !Ref Environment\n \nOutputs:\n  APIEndpoint:\n    Description: 'API Gateway endpoint URL'\n    Value: !Sub 'https://${RecruitmentAPI}.execute-api.${AWS::Region}.amazonaws.com/${Environment}'\n  \n  SNSTopicArn:\n    Description: 'SNS Topic ARN for notifications'\n    Value: !Ref CommunicationTopic\n\nDeployment automation\nThe following automation script handles deployment of the recruitment system infrastructure and Lambda functions. It manages CloudFormation stack creation and updates and Lambda function code updates, making system deployment and updates streamlined and consistent.\n\n#!/usr/bin/env python3\n\"\"\"\nDeployment script for Basic Recruitment System\n\"\"\"\n\nimport boto3\nimport zipfile\nimport os\nimport json\nfrom pathlib import Path\n\nclass BasicRecruitmentDeployment:\n    def __init__(self, region='us-east-1'):\n        self.region = region\n        self.lambda_client = boto3.client('lambda', region_name=region)\n        self.cf_client = boto3.client('cloudformation', region_name=region)\n    \n    def create_lambda_zip(self, function_name):\n        \"\"\"Create deployment zip for Lambda function\"\"\"\n        zip_path = f\"/tmp/{function_name}.zip\"\n        \n        with zipfile.ZipFile(zip_path, 'w') as zip_file:\n            zip_file.write(f\"lambda_functions/{function_name}.py\", f\"{function_name}.py\")\n        \n        return zip_path\n    \n    def update_lambda_function(self, function_name, environment='dev'):\n        \"\"\"Update Lambda function code\"\"\"\n        zip_path = self.create_lambda_zip(function_name)\n        \n        try:\n            with open(zip_path, 'rb') as zip_file:\n                response = self.lambda_client.update_function_code(\n                    FunctionName=f'recruitment-{function_name.replace(\"_agent\", \"\")}-{environment}',\n                    ZipFile=zip_file.read()\n                )\n            print(f\"Updated {function_name}: {response['LastModified']}\")\n            return response\n        except Exception as e:\n            print(f\"Error updating {function_name}: {e}\")\n            return None\n        finally:\n            os.remove(zip_path)\n    \n    def deploy_infrastructure(self, environment='dev'):\n        \"\"\"Deploy CloudFormation stack\"\"\"\n        stack_name = f'recruitment-system-{environment}'\n        \n        with open('infrastructure/cloudformation.yaml', 'r') as template_file:\n            template_body = template_file.read()\n        \n        try:\n            response = self.cf_client.create_stack(\n                StackName=stack_name,\n                TemplateBody=template_body,\n                Parameters=[\n                    {'ParameterKey': 'Environment', 'ParameterValue': environment}\n                ],\n                Capabilities=['CAPABILITY_IAM']\n            )\n            print(f\"Created stack: {stack_name}\")\n            return response\n        except self.cf_client.exceptions.AlreadyExistsException:\n            response = self.cf_client.update_stack(\n                StackName=stack_name,\n                TemplateBody=template_body,\n                Parameters=[\n                    {'ParameterKey': 'Environment', 'ParameterValue': environment}\n                ],\n                Capabilities=['CAPABILITY_IAM']\n            )\n            print(f\"Updated stack: {stack_name}\")\n            return response\n        except Exception as e:\n            print(f\"Error with stack: {e}\")\n            return None\n    \n    def deploy_all(self, environment='dev'):\n        \"\"\"Deploy complete system\"\"\"\n        print(f\"Deploying recruitment system to {environment}\")\n        \n        # Deploy infrastructure\n        self.deploy_infrastructure(environment)\n        \n        # Wait for stack to be ready (simplified)\n        print(\"Waiting for infrastructure...\")\n        \n        # Update AWS Lambda functions\n        functions = [\n            'job_description_agent',\n            'communication_agent',\n            'interview_agent'\n        ]\n        \n        for func in functions:\n            self.update_lambda_function(func, environment)\n        \n        print(\"Deployment complete!\")\n\ndef main():\n    deployment = BasicRecruitmentDeployment()\n    \n    print(\"Basic Recruitment System Deployment\")\n    print(\"1. Deploys CloudFormation stack with AWS Lambda functions and API Gateway\")\n    print(\"2. Updates Lambda function code\")\n    print(\"3. Sets up SNS for notifications\")\n    \n    # Example deployment\n    # deployment.deploy_all('dev')\n\nif __name__ == \"__main__\":\n    main()\n\nKnowledge base integration\nThe central knowledge base manager interfaces with Amazon Bedrock knowledge base collections to provide best practices, templates, and standards to the recruitment agents. It enables AI agents to make informed decisions based on organizational knowledge.\n\nimport boto3\nimport json\n\nclass KnowledgeBaseManager:\n    def __init__(self):\n        self.bedrock_runtime = boto3.client('bedrock-runtime')\n        self.bedrock_agent_runtime = boto3.client('bedrock-agent-runtime')\n\n    def query_knowledge_base(self, kb_id: str, query: str):\n        try:\n            response = self.bedrock_agent_runtime.retrieve(\n                knowledgeBaseId=kb_id,\n                retrievalQuery={'text': query}\n                # optionally add retrievalConfiguration={...}\n            )\n            return [r['content']['text'] for r in response.get('retrievalResults', [])]\n        except Exception as e:\n            return [f\"Knowledge Base query failed: {str(e)}\"]\n\n# Knowledge base IDs (to be created via CloudFormation)\nKNOWLEDGE_BASES = {\n    'job_descriptions': 'JOB_DESC_KB_ID', \n    'interview_standards': 'INTERVIEW_KB_ID',\n    'communication_templates': 'COMM_KB_ID'\n}\n\nTo improve Retrieval Augmented Generation (RAG) quality, start by tuning your Amazon Bedrock knowledge bases. Adjust chunk sizes and overlap for your documents, experiment with different embedding models, and enable reranking to promote the most relevant passages. For each agent, you can also choose different foundation models. For example, use a fast model such as Anthropic’s Claude 3 Haiku for high-volume job description and communication tasks, and a more capable model such as Anthropic’s Claude 3 Sonnet or another reasoning-optimized model for the Interview Prep Agent, where deeper analysis is required. Capture these experiments as part of your continuous improvement process so you can standardize on the best-performing configurations.\nThe core AI agents\nThe integration between the three agents is handled through API Gateway and Lambda, with each agent exposed through its own endpoint. The system uses three specialized AI agents.\nJob Description Agent\nThis agent is the first step in the recruitment pipeline. It uses Amazon Bedrock to create inclusive and effective job descriptions by combining requirements with best practices from the knowledge base.\n\nimport json\nimport boto3\nfrom datetime import datetime\nimport sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom knowledge_bases import KnowledgeBaseManager, KNOWLEDGE_BASES\n\nbedrock = boto3.client('bedrock-runtime')\nkb_manager = KnowledgeBaseManager()\n\ndef lambda_handler(event, context):\n    \"\"\"Job Description Agent Lambda function\"\"\"\n    \n    body = json.loads(event.get('body', '{}'))\n    \n    role_title = body.get('role_title', '')\n    requirements = body.get('requirements', [])\n    company_info = body.get('company_info', {})\n    \n    # Query knowledge base for best practices\n    kb_context = kb_manager.query_knowledge_base(\n        KNOWLEDGE_BASES['job_descriptions'],\n        f\"inclusive job description examples for {role_title}\"\n    )\n    \n    prompt = f\"\"\"Create an inclusive job description for: {role_title}\n    \nRequirements: {', '.join(requirements)}\nCompany: {company_info.get('name', 'Our Company')}\nCulture: {company_info.get('culture', 'collaborative')}\nRemote: {company_info.get('remote', False)}\n\nBest practices from knowledge base:\n{' '.join(kb_context[:2])}\n\nInclude: role summary, key responsibilities, qualifications, benefits.\nEnsure inclusive language and avoid unnecessary barriers.\"\"\"\n    \n    try:\n        response = bedrock.invoke_model(\n            modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"max_tokens\": 2000,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n            })\n        )\n        \n        result = json.loads(response['body'].read())\n        \n        return {\n            'statusCode': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'job_description': result['content'][0]['text'],\n                'role_title': role_title,\n                'timestamp': datetime.utcnow().isoformat()\n            })\n        }\n        \n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n\nCommunication Agent\nThis agent manages candidate communications throughout the recruitment process. It integrates with Amazon SNS for notifications and provides professional, consistent messaging using approved templates.\n\nimport json\nimport boto3\nfrom datetime import datetime\n\nbedrock = boto3.client('bedrock-runtime')\nsns = boto3.client('sns')\n\ndef lambda_handler(event, context):\n    \"\"\"Communication Agent Lambda function\"\"\"\n    \n    body = json.loads(event.get('body', '{}'))\n    \n    message_type = body.get('message_type', '')\n    candidate_info = body.get('candidate_info', {})\n    stage = body.get('stage', '')\n    \n    prompt = f\"\"\"Generate {message_type} for candidate {candidate_info.get('name', 'Candidate')}\nat {stage} stage.\n\nMessage should be:\n- Professional and empathetic\n- Clear about next steps\n- Appropriate for the stage\n- Include timeline if relevant\n\nTypes: application_received, interview_invitation, rejection, offer\"\"\"\n    \n    try:\n        response = bedrock.invoke_model(\n            modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"max_tokens\": 1000,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n            })\n        )\n        \n        result = json.loads(response['body'].read())\n        communication = result['content'][0]['text']\n        \n        # Send notification via SNS if topic ARN provided\n        topic_arn = body.get('sns_topic_arn')\n        if topic_arn:\n            sns.publish(\n                TopicArn=topic_arn,\n                Message=communication,\n                Subject=f\"Recruitment Update - {message_type}\"\n            )\n        \n        return {\n            'statusCode': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'communication': communication,\n                'type': message_type,\n                'stage': stage,\n                'timestamp': datetime.utcnow().isoformat()\n            })\n        }\n        \n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n\nInterview Prep Agent\nThis agent prepares tailored interview materials and questions based on the role and candidate background. It helps maintain consistent interview standards while adapting to specific positions.\n\nimport json\nimport boto3\nfrom datetime import datetime\n\nbedrock = boto3.client('bedrock-runtime')\n\ndef lambda_handler(event, context):\n    \"\"\"Interview Prep Agent Lambda function\"\"\"\n    \n    body = json.loads(event.get('body', '{}'))\n    \n    role_info = body.get('role_info', {})\n    candidate_background = body.get('candidate_background', {})\n    \n    prompt = f\"\"\"Prepare interview for:\nRole: {role_info.get('title', 'Position')}\nLevel: {role_info.get('level', 'Mid-level')}\nKey Skills: {role_info.get('key_skills', [])}\n\nCandidate Background:\nExperience: {candidate_background.get('experience', 'Not specified')}\nSkills: {candidate_background.get('skills', [])}\n\nGenerate:\n1. 5-7 technical questions\n2. 3-4 behavioral questions  \n3. Evaluation criteria\n4. Red flags to watch for\"\"\"\n    \n    try:\n        response = bedrock.invoke_model(\n            modelId=\"anthropic.claude-3-haiku-20240307-v1:0\",\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"max_tokens\": 2000,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n            })\n        )\n        \n        result = json.loads(response['body'].read())\n        \n        return {\n            'statusCode': 200,\n            'headers': {'Content-Type': 'application/json'},\n            'body': json.dumps({\n                'interview_prep': result['content'][0]['text'],\n                'role': role_info.get('title'),\n                'timestamp': datetime.utcnow().isoformat()\n            })\n        }\n        \n    except Exception as e:\n        return {\n            'statusCode': 500,\n            'body': json.dumps({'error': str(e)})\n        }\n\nTesting and verification\nThe following test client demonstrates interaction with the recruitment system API. It provides example usage of major functions and helps verify system functionality.\n\n#!/usr/bin/env python3\n\"\"\"\nTest client for Basic Recruitment System API\n\"\"\"\n\nimport requests\nimport json\n\nclass RecruitmentClient:\n    def __init__(self, api_endpoint):\n        self.api_endpoint = api_endpoint.rstrip('/')\n    \n    def create_job_description(self, role_title, requirements, company_info):\n        \"\"\"Test job description creation\"\"\"\n        url = f\"{self.api_endpoint}/job-description\"\n        payload = {\n            \"role_title\": role_title,\n            \"requirements\": requirements,\n            \"company_info\": company_info\n        }\n        \n        response = requests.post(url, json=payload)\n        return response.json()\n   \n    def send_communication(self, message_type, candidate_info, stage):\n        \"\"\"Test communication sending\"\"\"\n        url = f\"{self.api_endpoint}/communication\"\n        payload = {\n            \"message_type\": message_type,\n            \"candidate_info\": candidate_info,\n            \"stage\": stage\n        }\n        \n        response = requests.post(url, json=payload)\n        return response.json()\n\n    def prepare_interview(self, role_info, candidate_background):\n        \"\"\"Test interview preparation\"\"\"\n        url = f\"{self.api_endpoint}/interview\"\n        payload = {\n            \"role_info\": role_info,\n            \"candidate_background\": candidate_background\n        }\n        \n        response = requests.post(url, json=payload)\n        return response.json()\n\ndef main():\n    # Replace with your actual API endpoint\n    api_endpoint = \"https://your-api-id.execute-api.us-east-1.amazonaws.com/dev\"\n    client = RecruitmentClient(api_endpoint)\n    \n    print(\"Testing Basic Recruitment System\")\n    \n    # Test job description\n    print(\"\\n1. Testing Job Description Creation:\")\n    job_result = client.create_job_description(\n        role_title=\"Senior Software Engineer\",\n        requirements=[\"5+ years Python\", \"AWS experience\", \"Team leadership\"],\n        company_info={\"name\": \"TechCorp\", \"culture\": \"collaborative\", \"remote\": True}\n    )\n    print(json.dumps(job_result, indent=2))\n    \n    # Test communication\n    print(\"\\n2. Testing Communication:\")\n    comm_result = client.send_communication(\n        message_type=\"interview_invitation\",\n        candidate_info={\"name\": \"Jane Smith\", \"email\": \"jane@example.com\"},\n        stage=\"initial_interview\"\n    )\n    print(json.dumps(comm_result, indent=2))\n    \n    # Test interview prep\n    print(\"\\n3. Testing Interview Preparation:\")\n    interview_result = client.prepare_interview(\n        role_info={\n            \"title\": \"Senior Software Engineer\",\n            \"level\": \"Senior\",\n            \"key_skills\": [\"Python\", \"AWS\", \"Leadership\"]\n        },\n        candidate_background={\n            \"experience\": \"8 years software development\",\n            \"skills\": [\"Python\", \"AWS\", \"Team Lead\"]\n        }\n    )\n    print(json.dumps(interview_result, indent=2))\n\nif __name__ == \"__main__\":\n    main()\n\nDuring testing, track both qualitative and quantitative results. For example, measure recruiter satisfaction with generated job descriptions, response rates to candidate communications, and interviewers’ feedback on the usefulness of prep materials. Use these metrics to refine prompts, knowledge base contents, and model choices over time.\nClean up\nTo avoid ongoing charges when you’re done testing or if you want to tear down this solution, follow these steps in order:\n\nDelete Lambda resources:\n\nDelete all functions created for the agents.\nRemove associated CloudWatch log groups.\n\nDelete API Gateway endpoints:\n\nDelete the API configurations.\nRemove any custom domain names.\nDelete all collections.\nRemove any custom policies.\nWait for collections to be fully deleted before continuing to the next steps.\n\nDelete SNS topics\n\nDelete all topics created for communications.\nRemove any subscriptions.\n\nDelete VPC resources:\n\nRemove VPC endpoints.\nDelete security groups.\nDelete the VPC if it was created specifically for this solution.\n\nClean up IAM resources:\n\nDelete IAM roles created for the solution.\nRemove any associated policies.\nDelete service-linked roles if no longer needed.\n\nDelete KMS keys:\n\nSchedule key deletion for unused KMS keys (keep keys if they’re used by other applications).\n\nDelete CloudWatch resources:\n\nDelete dashboards.\nDelete alarms.\nDelete any custom metrics.\n\nClean up S3 buckets:\n\nEmpty buckets used for knowledge bases.\nDelete the buckets.\n\nDelete the Amazon Bedrock knowledge base.\n\nAfter cleanup, take these steps to verify all charges are stopped:\n\nCheck your AWS bill for the next billing cycle\nVerify all services have been properly terminated\nContact AWS Support if you notice any unexpected charges\n\nDocument the resources you’ve created and use this list as a checklist during cleanup to make sure you don’t miss any components that could continue to generate charges.\nImplementing AI in recruitment: Best practices\nTo successfully implement AI in recruitment while maintaining ethical standards and human oversight, consider these essential practices.\nSecurity, compliance, and infrastructure\nThe security implementation should follow a comprehensive approach to protect all aspects of the recruitment system. The solution deploys within a properly configured VPC with carefully defined security groups. All data, whether at rest or in transit, should be protected through AWS KMS encryption, and IAM roles are implemented following strict least privilege principles. The system maintains complete visibility through CloudWatch monitoring and audit logging, with secure API Gateway endpoints managing external communications. To protect sensitive information, implement data tokenization for personally identifiable information (PII) and maintain strict data retention policies. Regular privacy impact assessments and documented incident response procedures support ongoing security compliance.Consider the implementation of Amazon Bedrock Guardrails to provide granular control over AI model outputs, helping you enforce consistent safety and compliance standards across your AI applications. By implementing rule-based filters and boundaries, teams can prevent inappropriate content, maintain professional communication standards, and make sure responses align with their organization’s policies. You can configure guardrails at multiple levels—from individual agents to organization-wide implementations—with customizable controls for content filtering, topic restrictions, and response parameters. This systematic approach helps organizations mitigate risks while using AI capabilities, particularly in regulated industries or customer-facing applications where maintaining appropriate, unbiased, and safe interactions is crucial.\nKnowledge base architecture and management\nThe knowledge base architecture should follow a hub-and-spoke model centered around a core repository of organizational knowledge. This central hub maintains essential information including company values, policies, and requirements, along with shared reference data used across the agents. Version control and backup procedures maintain data integrity and availability.Surrounding this central hub, specialized knowledge bases serve each agent’s unique needs. The Job Description Agent accesses writing guidelines and inclusion requirements. The Communication Agent draws from approved message templates and workflow definitions, and the Interview Prep Agent uses comprehensive question banks and evaluation criteria.\nSystem integration and workflows\nSuccessful system operation relies on robust integration practices and clearly defined workflows. Error handling and retry mechanisms facilitate reliable operation, and clear handoff points between agents maintain process integrity. The system should maintain detailed documentation of dependencies and data flows, with circuit breakers protecting against cascade failures. Regular testing through automated frameworks and end-to-end workflow validation supports consistent performance and reliability.\nHuman oversight and governance\nThe AI-powered recruitment system should prioritize human oversight and governance to promote ethical and fair practices. Establish mandatory review checkpoints throughout the process where human recruiters assess AI recommendations and make final decisions. To handle exceptional cases, create clear escalation paths that allow for human intervention when needed. Sensitive actions, such as final candidate selections or offer approvals, should be subject to multi-level human approval workflows.To maintain high standards, continuously monitor decision quality and accuracy, comparing AI recommendations with human decisions to identify areas for improvement. The team should undergo regular training programs to stay updated on the system’s capabilities and limitations, making sure they can effectively oversee and complement the AI’s work. Document clear override procedures, so recruiters can adjust or override AI decisions when necessary. Regular compliance training for team members reinforces the commitment to ethical AI use in recruitment.\nPerformance and cost management\nTo optimize system efficiency and manage costs effectively, implement a multi-faceted approach. Automatic scaling for Lambda functions makes sure the system can handle varying workloads without unnecessary resource allocation. For predictable workloads, use AWS Savings Plans to reduce costs without sacrificing performance. You can estimate the solution costs using the AWS Pricing Calculator , which helps plan for services like Amazon Bedrock, Lambda, and Amazon Bedrock Knowledge Bases.\nComprehensive CloudWatch dashboards provide real-time visibility into system performance, facilitating quick identification and addressing of issues. Establish performance baselines and regularly monitor against these to detect deviations or areas for improvement. Cost allocation tags help track expenses across different departments or projects, enabling more accurate budgeting and resource allocation.\nTo avoid unexpected costs, configure budget alerts that notify the team when spending approaches predefined thresholds. Regular capacity planning reviews make sure the infrastructure keeps pace with organizational growth and changing recruitment needs.\nContinuous improvement framework\nCommitment to excellence should be reflected in a continuous improvement framework. Conduct regular metric reviews and gather stakeholder feedback to identify areas for enhancement. A/B testing of new features or process changes allows for data-driven decisions about improvements. Maintain a comprehensive system of documentation, capturing lessons learned from each iteration or challenge encountered. This knowledge informs ongoing training data updates, making sure AI models remain current and effective. The improvement cycle should include regular system optimization, where algorithms are fine-tuned, knowledge bases updated, and workflows refined based on performance data and user feedback. Closely analyze performance trends over time, allowing proactive addressing of potential issues and capitalization on successful strategies. Stakeholder satisfaction should be a key metric in the improvement framework. Regularly gather feedback from recruiters, hiring managers, and candidates to verify if the AI-powered system meets the needs of all parties involved in the recruitment process.\nSolution evolution and agent orchestration\nAs AI implementations mature and organizations develop multiple specialized agents, the need for sophisticated orchestration becomes critical. Amazon Bedrock AgentCore provides the foundation for managing this evolution, facilitating seamless coordination and communication between agents while maintaining centralized control. This orchestration layer streamlines the management of complex workflows, optimizes resource allocation, and supports efficient task routing based on agent capabilities. By implementing Amazon Bedrock AgentCore as part of your solution architecture, organizations can scale their AI operations smoothly, maintain governance standards, and support increasingly complex use cases that require collaboration between multiple specialized agents. This systematic approach to agent orchestration helps future-proof your AI infrastructure while maximizing the value of your agent-based solutions.\nConclusion\nAWS AI services offer specific capabilities that can be used to transform recruitment and talent acquisition processes. By using these services and maintaining a strong focus on human oversight, organizations can create more efficient, fair, and effective hiring practices. The goal of AI in recruitment is not to replace human decision-making, but to augment and support it, helping HR professionals focus on the most valuable aspects of their roles: building relationships, assessing cultural fit, and making nuanced decisions that impact people’s careers and organizational success. As you embark on your AI-powered recruitment journey, start small, focus on tangible improvements, and keep the candidate and employee experience at the forefront of your efforts. With the right approach, AI can help you build a more diverse, skilled, and engaged workforce, driving your organization’s success in the long term.\nFor more information about AI-powered solutions on AWS, refer to the following resources:\n\nAmazon Bedrock blog posts\nResponsible AI\n\nAbout the Authors\nDola Adesanya is a Customer Solutions Manager at Amazon Web Services (AWS), where she leads high-impact programs across customer success, cloud transformation, and AI-driven system delivery. With a unique blend of business strategy and organizational psychology expertise, she specializes in turning complex challenges into actionable solutions. Dola brings extensive experience in scaling programs and delivering measurable business outcomes.\nRonHayman leads Customer Solutions for US Enterprise and Software Internet & Foundation Models at Amazon Web Services (AWS). His organization helps customers migrate infrastructure, modernize applications, and implement generative AI solutions. Over his 20-year career as a global technology executive, Ron has built and scaled cloud, security, and customer success teams. He combines deep technical expertise with a proven track record of developing leaders, organizing teams, and delivering customer outcomes.\nAchilles Figueiredo is a Senior Solutions Architect at Amazon Web Services (AWS), where he designs and implements enterprise-scale cloud architectures. As a trusted technical advisor, he helps organizations navigate complex digital transformations while implementing innovative cloud solutions. He actively contributes to AWS’s technical advancement through AI, Security, and Resilience initiatives and serves as a key resource for both strategic planning and hands-on implementation guidance.\nSai Jeedigunta is a Sr. Customer Solutions Manager at AWS. He is passionate about partnering with executives and cross-functional teams in driving cloud transformation initiatives and helping them realize the benefits of cloud. He has over 20 years of experience in leading IT infrastructure engagements for fortune enterprises.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "b41a02bb3a550c6c",
    "title": "Build long-running MCP servers on Amazon Bedrock AgentCore with Strands Agents integration",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-long-running-mcp-servers-on-amazon-bedrock-agentcore-with-strands-agents-integration/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-12T20:16:20Z",
    "summary": "In this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build prod...",
    "content": "AI agents are rapidly evolving from mere chat interfaces into sophisticated autonomous workers that handle complex, time-intensive tasks. As organizations deploy agents to train machine learning (ML) models, process large datasets, and run extended simulations, the Model Context Protocol (MCP) has emerged as a standard for agent-server integrations. But a critical challenge remains: these operations can take minutes or hours to complete, far exceeding typical session timeframes. By using Amazon Bedrock AgentCore and Strands Agents to implement persistent state management, you can enable seamless, cross-session task execution in production environments. Imagine your AI agent initiating a multi-hour data processing job, your user closing their laptop, and the system seamlessly retrieving completed results when the user returns days later—with full visibility into task progress, outcomes, and errors. This capability transforms AI agents from conversational assistants into reliable autonomous workers that can handle enterprise-scale operations. Without these architectural patterns, you’ll encounter timeout errors, inefficient resource utilization, and potential data loss when connections terminate unexpectedly.\nIn this post, we provide you with a comprehensive approach to achieve this. First, we introduce a context message strategy that maintains continuous communication between servers and clients during extended operations. Next, we develop an asynchronous task management framework that allows your AI agents to initiate long-running processes without blocking other operations. Finally, we demonstrate how to bring these strategies together with Amazon Bedrock AgentCore and Strands Agents to build production-ready AI agents that can handle complex, time-intensive operations reliably.\nCommon approaches to handle long-running tasks\nWhen designing MCP servers for long-running tasks, you might face a fundamental architectural decision: should the server maintain an active connection and provide real-time updates, or should it decouple task execution from the initial request? This choice leads to two distinct approaches: context messaging and async task management .\nUsing context messaging\nThe context messaging approach maintains continuous communication between the MCP server and client throughout task execution. This is achieved by using MCP’s built-in context object to send periodic notifications to the client. This approach is optimal for scenarios where tasks are typically completed within 10–15 minutes and network connectivity remains stable. The context messaging approach offers these advantages:\n\nStraightforward implementation\nNo additional polling logic required\nStraightforward client implementation\nMinimal overhead\n\nUsing async task management\nThe async task management approach separates task initiation from execution and result retrieval. After executing the MCP tool, the tool immediately returns a task initiation message while executing the task in the background. This approach excels in demanding enterprise scenarios where tasks might run for hours, users need flexibility to disconnect and reconnect, and system reliability is paramount. The async task management approach provides these benefits:\n\nTrue fire-and-forget operation\nSafe client disconnection while tasks continue processing\nData loss prevention through persistent storage\nSupport for long-running operations (hours)\nResilience against network interruptions\nAsynchronous workflows\n\nContext messaging\nLet’s begin by exploring the context messaging approach, which provides a straightforward solution for handling moderately long operations while maintaining active connections. This approach builds directly on existing capabilities of MCP and requires minimal additional infrastructure, making it an excellent starting point for extending your agent’s processing time limits. Imagine you’ve built an MCP server for an AI agent that helps data scientists train ML models. When a user asks the agent to train a complex model, the underlying process might take 10–15 minutes—far beyond the typical 30-second to 2-minute HTTP timeout limit in most environments. Without a proper strategy, the connection would drop, the operation would fail, and the user would be left frustrated. In a Streamable HTTP transport for MCP client implementation, these timeout constraints are particularly limiting. When task execution exceeds the timeout limit, the connection aborts and the agent’s workflow interrupts. This is where context messaging comes in. The following diagram illustrates the workflow when implementing the context messaging approach. Context messaging uses the built-in context object of MCP to send periodic signals from the server to the MCP client, effectively keeping the connection alive throughout longer operations. Think of it as sending “heartbeat” messages that help prevent the connection from timing out.\n\nFigure 1: Illustration of workflow in context messaging approach\n\nHere is a code example to implement the context messaging:\n\nfrom mcp.server.fastmcp import Context, FastMCP\nimport asyncio\n\nmcp = FastMCP(host=\"0.0.0.0\", stateless_http=True)\n\n@mcp.tool()\nasync def model_training(model_name: str, epochs: int, ctx: Context) -> str:\n    \"\"\"Execute a task with progress updates.\"\"\"\n\n    for i in range(epochs):\n        # Simulate long running time training work\n        progress = (i + 1) / epochs\n        await asyncio.sleep(5)\n        await ctx.report_progress(\n            progress=progress,\n            total=1.0,\n            message=f\"Step {i + 1}/{epochs}\",\n        )\n\n    return f\"{model_name} training completed. The model artifact is stored in s3://templocation/model.pickle . The model training score is 0.87, validation score is 0.82.\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n\nThe key element here is the Context parameter in the tool definition. When you include a parameter with the Context type annotation, FastMCP automatically injects this object, giving you access to methods such as ctx.info() and ctx.report_progress() . These methods send messages to the connected client without terminating tool execution.\nThe report_progress() calls within the training loop serve as those critical heartbeat messages, making sure the MCP connection remains active throughout the extended processing period.\nFor many real-world scenarios, exact progress can’t be easily quantified—such as when processing unpredictable datasets or making external API calls. In these cases, you can implement a time-based heartbeat system:\n\nfrom mcp.server.fastmcp import Context, FastMCP\nimport time\nimport asyncio\n\nmcp = FastMCP(host=\"0.0.0.0\", stateless_http=True)\n\n@mcp.tool()\nasync def model_training(model_name: str, epochs: int, ctx: Context) -> str:\n    \"\"\"Execute a task with progress updates.\"\"\"\n    done_event = asyncio.Event()\n    start_time = time.time()\n\n    async def timer():\n        while not done_event.is_set():\n            elapsed = time.time() - start_time\n            await ctx.info(f\"Processing ......: {elapsed:.1f} seconds elapsed\")\n            await asyncio.sleep(5)  # Check every 5 seconds\n        return\n\n    timer_task = asyncio.create_task(timer())\n\n    ## main task#####################################\n    for i in range(epochs):\n        # Simulate long running time training work\n        progress = (i + 1) / epochs\n        await asyncio.sleep(5)\n    #################################################\n\n    # Signal the timer to stop and clean up\n    done_event.set()\n    await timer_task\n\n    total_time = time.time() - start_time\n    print(f\"⏱ Total processing time: {total_time:.2f} seconds\")\n\n    return f\"{model_name} training completed. The model artifact is stored in s3://templocation/model.pickle . The model training score is 0.87, validation score is 0.82.\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n\nThis pattern creates an asynchronous timer that runs alongside your main task, sending regular status updates every few seconds. Using asyncio.Event() for coordination facilitates clean shutdown of the timer when the main work is completed.\nWhen to use context messaging\nContext messaging works best when:\n\nTasks take 1–15 minutes to complete*\nNetwork connections are generally stable\nThe client session can remain active throughout the operation\nYou need real-time progress updates during processing\nTasks have predictable, finite execution times with clear termination conditions\n\n*Note: “15 minutes” is based on the maximum time for synchronous requests Amazon Bedrock AgentCore offered. More details about Bedrock AgentCore service quotas can be found at Quotas for Amazon Bedrock AgentCore . If the infrastructure hosting the agent doesn’t implement hard time limits, be extremely cautious when using this approach for tasks that might potentially hang or run indefinitely. Without proper safeguards, a stuck task could maintain an open connection indefinitely, leading to resource depletion, unresponsive processes, and potentially system-wide stability issues.\nHere are some important limitations to consider:\n\nContinuous connection required – The client session must remain active throughout the entire operation. If the user closes their browser or the network drops, the work is lost.\nResource consumption – Keeping connections open consumes server and client resources, potentially increasing costs for long-running operations.\nNetwork dependency – Network instability can still interrupt the process, requiring a full restart.\nUltimate timeout limits – Most infrastructures have hard timeout limits that can’t be circumvented with heartbeat messages.\n\nTherefore, for truly long-running operations that might take hours or for scenarios where users need to disconnect and reconnect later, you’ll need the more robust asynchronous task management approach.\nAsync task management\nUnlike the context messaging approach where clients must maintain continuous connections, the async task management pattern follows a “fire and forget” model:\n\nTask initiation – Client makes a request to start a task and immediately receives a task ID\nBackground processing – Server executes the work asynchronously, with no client connection required\nStatus checking – Client can reconnect whenever to check progress using the task ID\nResult retrieval – When they’re completed, results remain available for retrieval whenever the client reconnects\n\nThe following figure illustrates the workflow in the asynchronous task management approach.\n\nFigure 2: Illustration of workflow in asynchronous task management approach\n\nThis pattern mirrors how you interact with batch processing systems in enterprise environments—submit a job, disconnect, and check back later when convenient. Here’s a practical implementation that demonstrates these principles:\n\nfrom mcp.server.fastmcp import Context, FastMCP\nimport asyncio\nimport uuid\nfrom typing import Dict, Any\n\nmcp = FastMCP(host=\"0.0.0.0\", stateless_http=True)\n\n# task storage\ntasks: Dict[str, Dict[str, Any]] = {}\n\nasync def _execute_model_training(\n        task_id: str,\n        model_name: str,\n        epochs: int\n    ):\n    \"\"\"Background task execution.\"\"\"\n    tasks[task_id][\"status\"] = \"running\"\n    \n    for i in range(epochs):\n        tasks[task_id][\"progress\"] = (i + 1) / epochs\n        await asyncio.sleep(2)\n\n    tasks[task_id][\"result\"] = f\"{model_name} training completed. The model artifact is stored in s3://templocation/model.pickle . The model training score is 0.87, validation score is 0.82.\"\n    \n    tasks[task_id][\"status\"] = \"completed\"\n\n@mcp.tool()\ndef model_training(\n    model_name: str,\n    epochs: int = 10\n    ) -> str:\n    \"\"\"Start model training task.\"\"\"\n    task_id = str(uuid.uuid4())\n    tasks[task_id] = {\n        \"status\": \"started\",\n        \"progress\": 0.0,\n        \"task_type\": \"model_training\"\n    }\n    asyncio.create_task(_execute_model_training(task_id, model_name, epochs))\n    return f\"Model Training task has been initiated with task ID: {task_id}. Please check back later to monitor completion status and retrieve results.\"\n\n@mcp.tool()\ndef check_task_status(task_id: str) -> Dict[str, Any]:\n    \"\"\"Check the status of a running task.\"\"\"\n    if task_id not in tasks:\n        return {\"error\": \"task not found\"}\n    \n    task = tasks[task_id]\n    return {\n        \"task_id\": task_id,\n        \"status\": task[\"status\"],\n        \"progress\": task[\"progress\"],\n        \"task_type\": task.get(\"task_type\", \"unknown\")\n    }\n\n@mcp.tool()\ndef get_task_results(task_id: str) -> Dict[str, Any]:\n    \"\"\"Get results from a completed task.\"\"\"\n    if task_id not in tasks:\n        return {\"error\": \"task not found\"}\n    \n    task = tasks[task_id]\n    if task[\"status\"] != \"completed\":\n        return {\"error\": f\"task not completed. Current status: {task['status']}\"}\n    \n    return {\n        \"task_id\": task_id,\n        \"status\": task[\"status\"],\n        \"result\": task[\"result\"]\n    }\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n\nThis implementation creates a task management system with three distinct MCP tools:\n\nmodel_training() – The entry point that initiates a new task. Rather than performing the work directly, it:\n\nGenerates a unique task identifier using Universally Unique Identifier (UUID)\nCreates an initial task record in the storage dictionary\nLaunches the actual processing as a background task using asyncio.create_task()\nReturns immediately with the task ID, allowing the client to disconnect\n\ncheck_task_status() – Allows clients to monitor progress at their convenience by:\n\nLooking up the task by ID in the storage dictionary\nReturning current status and progress information\nProviding appropriate error handling for missing tasks\n\nget_task_results() – Retrieves completed results when ready by:\n\nVerifying the task exists and is completed\nReturning the results stored during background processing\nProviding clear error messages when results aren’t ready\n\nThe actual work happens in the private _execute_model_training() function, which runs independently in the background after the initial client request is completed. It updates the task’s status and progress in the shared storage as it progresses, making this information available for subsequent status checks.\nLimitations to consider\nAlthough the async task management approach helps solve connectivity issues, it introduces its own set of limitations:\n\nUser experience friction – The approach requires users to manually check task status, remember task IDs across sessions, and explicitly request results, increasing interaction complexity.\nVolatile memory storage – Using in-memory storage (as in our example) means the tasks and results are lost if the server restarts, making the solution unsuitable for production without persistent storage.\nServerless environment constraints – In ephemeral serverless environments, instances are automatically terminated after periods of inactivity, causing the in-memory task state to be permanently lost. This creates a paradoxical situation where the solution designed to handle long-running operations becomes vulnerable to the exact duration it aims to support. Unless users maintain regular check-ins to help prevent session time limits, both tasks and results could vanish.\n\nMoving toward a robust solution\nTo address these critical limitations, you need to include external persistence that survives both server restarts and instance terminations. This is where integration with dedicated storage services becomes essential. By using external agent memory storage systems, you can fundamentally change where and how task information is maintained. Instead of relying on the MCP server’s volatile memory, this approach uses persistent external agent memory storage services that remain available regardless of server state.\nThe key innovation in this enhanced approach is that when the MCP server runs a long-running task, it writes the interim or final results directly into external memory storage, such as Amazon Bedrock AgentCore Memory that the agent can access, as illustrated in the following figure. This helps create resilience against two types of runtime failures:\n\nThe instance running the MCP server can be terminated due to inactivity after task completion\nThe instance hosting the agent itself can be recycled in ephemeral serverless environments\n\nFigure 3. MCP integration with external memory\n\nWith external memory storage, when users return to interact with the agent—whether minutes, hours, or days later—the agent can retrieve the completed task results from persistent storage. This approach minimizes runtime dependencies: even if both the MCP server and agent instances are terminated, the task results remain safely preserved and accessible when needed.\nThe next section will explore how to implement this robust solution using Amazon Bedrock AgentCore Runtime as a serverless hosting environment, AgentCore Memory for persistent agent memory storage, and the Strands Agents framework to orchestrate these components into a cohesive system that maintains task state across session boundaries.\nAmazon Bedrock AgentCore and Strands Agents implementation\nBefore diving into the implementation details, it’s important to understand the deployment options available for MCP servers on Amazon Bedrock AgentCore. There are two primary approaches: Amazon Bedrock AgentCore Gateway and AgentCore Runtime. AgentCore Gateway has a 5-minute timeout for invocations, making it unsuitable for hosting MCP servers that provide tools requiring extended response times or long-running operations. AgentCore Runtime offers significantly more flexibility with a 15-minute request timeout (for synchronous requests) and adjustable maximum session duration (for asynchronous processes; the default duration is 8 hours) and idle session timeout. Although you could host an MCP server in a traditional serverful environment for unlimited execution time, AgentCore Runtime provides an optimal balance for most production scenarios. You gain serverless benefits such as automatic scaling, pay-per-use pricing, and no infrastructure management, while the adjustable maximums session duration covers most real-world long running tasks—from data processing and model training to report generation and complex simulations. You can use this approach to build sophisticated AI agents without the operational overhead of managing servers while reserving serverful deployments only for the rare cases that genuinely require multiday executions. For more information about AgentCore Runtime and AgentCore Gateway service quotas, refer to Quotas for Amazon Bedrock AgentCore .\nNext, we walk through the implementation, which is illustrated in the following diagram. This implementation consists of two interconnected components: the MCP server that executes long-running tasks and writes results to AgentCore Memory, and the agent that manages the conversation flow and retrieves those results when needed. This architecture creates a seamless experience where users can disconnect during lengthy processes and return later to find their results waiting for them.\n\nMCP server implementation\nLet’s examine how our MCP server implementation uses AgentCore Memory to achieve persistence:\n\nfrom mcp.server.fastmcp import Context, FastMCP\nimport asyncio\nimport uuid\nfrom typing import Dict, Any\nimport json\nfrom bedrock_agentcore.memory import MemoryClient\n\nmcp = FastMCP(host=\"0.0.0.0\", stateless_http=True)\nagentcore_memory_client = MemoryClient()\n\nasync def _execute_model_training(\n        model_name: str,\n        epochs: int,\n        session_id: str,\n        actor_id: str,\n        memory_id: str\n    ):\n    \"\"\"Background task execution.\"\"\"\n    \n    for i in range(epochs):\n        await asyncio.sleep(2)\n\n    try:\n        response = agentcore_memory_client.create_event(\n            memory_id=memory_id,\n            actor_id=actor_id,\n            session_id=session_id,\n            messages=[\n                (\n                    json.dumps({\n                        \"message\": {\n                            \"role\": \"user\",\n                            \"content\": [\n                                {\n                                    \"text\": f\"{model_name} training completed. The model artifact is stored in s3://templocation/model.pickle . The model training score is 0.87, validation score is 0.82.\"\n                                }\n                            ]\n                        },\n                        \"message_id\": 0\n                    }),\n                    'USER'\n                )\n            ]\n        )\n        print(response)\n    except Exception as e:\n        print(f\"Memory save error: {e}\")\n\n    return\n\n@mcp.tool()\ndef model_training(\n        model_name: str,\n        epochs: int,\n        ctx: Context\n    ) -> str:\n    \"\"\"Start model training task.\"\"\"\n\n    print(ctx.request_context.request.headers)\n    mcp_session_id = ctx.request_context.request.headers.get(\"mcp-session-id\", \"\")\n    temp_id_list = mcp_session_id.split(\"@@@\")\n    session_id = temp_id_list[0]\n    memory_id= temp_id_list[1]\n    actor_id  = temp_id_list[2]\n\n    asyncio.create_task(_execute_model_training(\n            model_name,\n            epochs,\n            session_id,\n            actor_id,\n            memory_id\n        )\n    )\n    return f\"Model {model_name}Training task has been initiated. Total training epochs are {epochs}. The results will be updated once the training is completed.\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n\nThe implementation relies on two key components that enable persistence and session management.\n\nThe agentcore_memory_client.create_event() method serves as the bridge between tool execution and persistent memory storage. When a background task is completed, this method saves the results directly to the agent’s memory in AgentCore Memory using the specified memory ID, actor ID, and session ID. Unlike traditional approaches where results might be stored temporarily or require manual retrieval, this integration enables task outcomes to become permanent parts of the agent’s conversational memory. The agent can then reference these results in future interactions, creating a continuous knowledge-building experience across multiple sessions.\nThe second crucial component involves extracting session context through ctx.request_context.request.headers.get(\"mcp-session-id\", \"\") . The \"Mcp-Session-Id\" is part of standard MCP protocol . You can use this header to pass a composite identifier containing three essential pieces of information in a delimited format: session_id@@@memory_id@@@actor_id . This approach allows our implementation to retrieve the necessary context identifiers from a single header value. Headers are used instead of environment variables by necessity—these identifiers change dynamically with each conversation, whereas environment variables remain static from container startup. This design choice is particularly important in multi-tenant scenarios where a single MCP server simultaneously handles requests from multiple users, each with their own distinct session context.\n\nAnother important aspect in this example involves proper message formatting when storing events. Each message saved to AgentCore Memory requires two components: the content and a role identifier. These two components need to be formatted in a way that the agent framework can be recognized. Here is an example for Strands Agents framework:\n\nmessages=[\n    (\n        json.dumps({\n            \"message\": {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"text\": <message to the memory>\n                    }\n                ]\n            },\n            \"message_id\": 0\n        }),\n        'USER'\n    )\n]\n\nThe content is an inner JSON object (serialized with json.dumps() ) that contains the message details, including role, text content, and message ID. The outer role identifier (USER in this example) helps AgentCore Memory categorize the message source.\nStrands Agents implementation\nIntegrating Amazon Bedrock AgentCore Memory with Strands Agents is remarkably straightforward using the AgentCoreMemorySessionManager class from the Bedrock AgentCore SDK . As shown in the following code example, implementation requires minimal configuration—create an AgentCoreMemoryConfig with your session identifiers, initialize the session manager with this config, and pass it directly to your agent constructor. The session manager transparently handles the memory operations behind the scenes, maintaining conversation history and context across interactions while organizing memories using the combination of session_id , memory_id , and actor_id . For more information, refer to AgentCore Memory Session Manager .\n\nfrom bedrock_agentcore.memory.integrations.strands.config import AgentCoreMemoryConfig\nfrom bedrock_agentcore.memory.integrations.strands.session_manager import AgentCoreMemorySessionManager\n\n@app.entrypoint\nasync def strands_agent_main(payload, context):\n\n    session_id = context.session_id\n    if not session_id:\n        session_id = str(uuid.uuid4())\n    print(f\"Session ID: {session_id}\")\n\n    memory_id = payload.get(\"memory_id\")\n    if not memory_id:\n        memory_id = \"\"\n    print(f\"? Memory ID: {memory_id}\")\n\n    actor_id = payload.get(\"actor_id\")\n    if not actor_id:\n        actor_id = \"default\"\n        \n    agentcore_memory_config = AgentCoreMemoryConfig(\n        memory_id=memory_id,\n        session_id=session_id,\n        actor_id=actor_id\n    )\n\n    session_manager = AgentCoreMemorySessionManager(\n        agentcore_memory_config=agentcore_memory_config\n    )\n    \n    user_input = payload.get(\"prompt\")\n\n    headers = {\n        \"authorization\": f\"Bearer {bearer_token}\",\n        \"Content-Type\": \"application/json\",\n        \"Mcp-Session-Id\": session_id + \"@@@\" + memory_id + \"@@@\" + actor_id\n    }\n\n    # Connect to an MCP server using SSE transport\n    streamable_http_mcp_client = MCPClient(\n        lambda: streamablehttp_client(\n                mcp_url,\n                headers,\n                timeout=30\n            )\n        )\n\n    with streamable_http_mcp_client:\n        # Get the tools from the MCP server\n        tools = streamable_http_mcp_client.list_tools_sync()\n\n        # Create an agent with these tools        \n        agent = Agent(\n            tools = tools,\n            callback_handler=call_back_handler,\n            session_manager=session_manager\n        )\n\nThe session context management is particularly elegant here. The agent receives session identifiers through the payload and context parameters supplied by AgentCore Runtime. These identifiers form a crucial contextual bridge that connects user interactions across multiple sessions. The session_id can be extracted from the context object (generating a new one if needed), and the memory_id and actor_id can be retrieved from the payload. These identifiers are then packaged into a custom HTTP header ( Mcp-Session-Id ) that’s passed to the MCP server during connection establishment.\nTo maintain this persistent experience across multiple interactions, clients must consistently provide the same identifiers when invoking the agent:\n\n# invoke agentcore through boto3\nboto3_response = agentcore_client.invoke_agent_runtime(\n   agentRuntimeArn=agent_arn,\n   qualifier=\"DEFAULT\",\n   payload=json.dumps(\n           {\n               \"prompt\": user_input,\n               \"actor_id\": actor_id,\n               \"memory_id\": memory_id\n           }\n       ),\n   runtimeSessionId = session_id,\n)\n\nBy consistently providing the same memory_id , actor_id , and runtimeSessionId across invocations, users can create a continuous conversational experience where task results persist independently of session boundaries. When a user returns days later, the agent can automatically retrieve both conversation history and the task results that were completed during their absence.\nThis architecture represents a significant advancement in AI agent capabilities—transforming long-running operations from fragile, connection-dependent processes into robust, persistent tasks that continue working regardless of connection state. The result is a system that can deliver truly asynchronous AI assistance, where complex work continues in the background and results are seamlessly integrated whenever the user returns to the conversation.\nConclusion\nIn this post, we’ve explored practical ways to help AI agents handle tasks that take minutes or even hours to complete. Whether using the more straightforward approach of keeping connections alive or the more advanced method of injecting task results to agent’s memory, these techniques enable your AI agent to tackle valuable complex work without frustrating time limits or lost results.\nWe invite you to try these approaches in your own AI agent projects. Start with context messaging for moderate tasks, then move to async management as your needs grow. The solutions we’ve shared can be quickly adapted to your specific needs, helping you build AI that delivers results reliably—even when users disconnect and return days later. What long-running tasks could your AI assistants handle better with these techniques?\nTo learn more, see the  Amazon Bedrock AgentCore documentation  and explore our  sample notebook.\n\nAbout the Authors\nHaochen Xie is a Senior Data Scientist at AWS Generative AI Innovation Center. He is an ordinary person.\nFlora Wang is an Applied Scientist at AWS Generative AI Innovation Center, where she works with customers to architect and implement scalable Generative AI solutions that address their unique business challenges. She specializes in model customization techniques and agent-based AI systems, helping organizations harness the full potential of generative AI technology.\nYuan Tian is an Applied Scientist at the AWS Generative AI Innovation Center, where he works with customers across diverse industries—including healthcare, life sciences, finance, and energy—to architect and implement generative AI solutions such as agentic systems. He brings a unique interdisciplinary perspective, combining expertise in machine learning with computational biology.\nHari Prasanna Das is an Applied Scientist at the AWS Generative AI Innovation Center, where he works with AWS customers across different verticals to expedite their use of Generative AI. Hari holds a PhD in Electrical Engineering and Computer Sciences from the University of California, Berkeley. His research interests include Generative AI, Deep Learning, Computer Vision, and Data-Efficient Machine Learning.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "882674348fab9ca8",
    "title": "Introducing GPT-5.3-Codex-Spark",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex-spark",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-12T10:00:00Z",
    "summary": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
    "content": "Introducing GPT-5.3-Codex-Spark—our first real-time coding model. 15x faster generation, 128k context, now in research preview for ChatGPT Pro users.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]