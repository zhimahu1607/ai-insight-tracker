[
  {
    "id": "47e8cdb4cf80b105",
    "title": "Advanced fine-tuning techniques for multi-agent orchestration: Patterns from Amazon at scale",
    "url": "https://aws.amazon.com/blogs/machine-learning/advanced-fine-tuning-techniques-for-multi-agent-orchestration-patterns-from-amazon-at-scale/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T15:51:21Z",
    "summary": "In this post, we show you how fine-tuning enabled a 33% reduction in dangerous medication errors (Amazon Pharmacy), engineering 80% human effort reduction (Amazon Global Engineering Services), and content quality assessments improving 77% to 96% accuracy (Amazon A+). This post details the techniques behind these outcomes: from foundational methods like Supervised Fine-Tuning (SFT) (instruction tuning), and Proximal Policy Optimization (PPO), to Direct Preference Optimization (DPO) for human alig...",
    "content": "Our work with large enterprise customers and Amazon teams has revealed that high stakes use cases continue to benefit significantly from advanced large language model (LLM) fine-tuning and post-training techniques. In this post, we show you how fine-tuning enabled a 33% reduction in dangerous medication errors (Amazon Pharmacy), engineering 80% human effort reduction (Amazon Global Engineering Services), and content quality assessments improving 77% to 96% accuracy (Amazon A+). These aren’t hypothetical projections—they’re production results from Amazon teams. While many use cases can be effectively addressed through prompt engineering, Retrieval Augmented Generation (RAG) systems, and turn key agent deployment,, our work with Amazon and large enterprise accounts reveals a consistent pattern: One in four high-stakes applications—where patient safety, operational efficiency, or customer trust are on the line—demand advanced fine-tuning and post-training techniques to achieve production-grade performance.\nThis post details the techniques behind these outcomes: from foundational methods like Supervised Fine-Tuning (SFT) (instruction tuning ), and Proximal Policy Optimization (PPO), to Direct Preference Optimization (DPO) for human alignment, to cutting-edge reasoning optimizations such as Grouped-based Reinforcement Learning from Policy Optimization (GRPO), Direct Advantage Policy Optimization (DAPO), and Group Sequence Policy Optimization (GSPO) purpose-built for agentic systems. We walk through the technical evolution of each approach, examine real-world implementations at Amazon, present a reference architecture on Amazon Web Services (AWS) , and provide a decision framework for selecting the right technique based on your use case requirements.\nThe continued relevance of fine-tuning in the agentic AI\nDespite the growing capabilities of foundation models and agent frameworks, roughly one of four enterprise use cases still require advanced fine-tuning to achieve the necessary performance levels. These are typically scenarios where the stakes are high from revenue or customer trust perspectives, domain-specific knowledge is essential, enterprise integration at scale is required, governance and control are paramount, business process integration is complex, or multi-modal support is needed. Organizations pursuing these use cases have reported higher conversion to production, greater return on investment (ROI), and up to 3-fold year-over-year growth when advanced fine-tuning is appropriately applied.\nEvolution of LLM fine-tuning techniques for agentic AI\nThe evolution of generative AI has seen several key advancements in model customization and performance optimization techniques. Starting with SFT, which uses labeled data to teach models to follow specific instructions, the field established its foundation but faced limitations in optimizing complex reasoning. To address these limitations, reinforcement learning (RL) refines the SFT process with a reward-based system that provides better adaptability and alignment with human preference. Among multiple RL algorithms, a significant leap comes with PPO, which consists of a workflow with a value (critic) network and a policy network. The workflow contains a reinforcement learning policy to adjust the LLM weights based on the guidance of a reward model. PPO scales well in complex environments, though it has challenges with stability and configuration complexity.\nDPO emerged as a breakthrough in early 2024, addressing PPO’s stability issues by eliminating the explicit reward model and instead working directly with preference data that includes preferred and rejected responses for given prompts. DPO optimizes the LLM weights by comparing the preferred and rejected responses, allowing the LLM to learn and adjust its behavior accordingly. This simplified approach gained widespread adoption, with major language models incorporating DPO into their training pipelines to achieve better performance and more reliable outputs. Other alternatives including Odds Ratio Policy Optimization (ORPO), Relative Preference Optimization (RPO), Identity preference optimization (IPO), Kahneman-Tversky Optimization (KTO), they are all RL methods for human preference alignment. By incorporating comparative and identity-based preference structures, and grounding optimization in behavioral economics, these methods are computationally efficient, interpretable, and aligned with actual human decision-making processes.\nAs agent-based applications gained prominence in 2025, we observed increasing demands for customizing the reasoning model in agents, to encode domain-specific constraints, safety guidelines, and reasoning patterns that align with agents’ intended functions (task planning, tool use, or multi-step problem solving). The objective is to improve agents’ performance in maintaining coherent plans, avoiding logical contradictions, and making appropriate decisions for the domain specific use cases. To meet these needs, GRPO was introduced to enhance reasoning capabilities and became particularly notable for its implementation in DeepSeek-V1 .\nThe core innovation of GRPO lies in its group-based comparison approach: rather than comparing individual responses against a fixed reference, GRPO generates groups of responses and evaluates each against the average score of the group, rewarding those performing above average while penalizing those below. This relative comparison mechanism creates a competitive dynamic that encourages the model to produce higher-quality reasoning. GRPO is particularly effective for improving chain-of-thought (CoT) reasoning, which is the critical foundation for agent planning and complex task decomposition. By optimizing at the group level, GRPO captures the inherent variability in reasoning processes and trains the model to consistently outperform its own average performance.\nSome complex agent tasks might require more fine-grained and crisp corrections within long reasoning chains, DAPO addresses these use cases by building upon GRPO sequence-level rewards, employing a higher clip ratio (approximately 30% higher than GRPO) to encourage more diverse and exploratory thinking processes, implementing dynamic sampling to eliminate less meaningful samples and improve overall training efficiency, applying token-level policy gradient loss to provide more granular feedback on lengthy reasoning chains rather than treating entire sequences as monolithic units, and incorporating overlong reward shaping to discourage excessively verbose responses that waste computational resources. Additionally, when the agentic use cases require long text outputs in the Mixture-of-Experts (MoE) model training, GSPO supports these scenarios by shifting the optimization from GRPO’s token-level importance weights to the sequence level. With these improvements, the new methods (DAPO and GSPO) enable more efficient and sophisticated agent reasoning and planning strategy, while maintaining computational efficiency and appropriate feedback resolution of GRPO.\nReal-world applications at Amazon\nUsing the fine-tuning techniques described in the previous sections, the post-trained LLMs play two crucial roles in agentic AI systems. First is in the development of specialized tool-using components and sub-agents within the broader agent architecture. These fine-tuned models act as domain experts, each optimized for specific functions. By incorporating domain-specific knowledge and constraints during the fine-tuning process, these specialized components can achieve significantly higher accuracy and reliability in their designated tasks compared to general-purpose models. The second key application is to serve as the core reasoning engine, where the foundation models are specifically tuned to excel at planning, logical reasoning, and decision-making, for agents in a highly specific domain. The aim is to improve the model’s ability to maintain coherent plans and make logically sound decisions—essential capabilities for any agent system. This dual approach, combining a fine-tuned reasoning core with specialized sub-components, was emerging as a promising architecture in Amazon for evolving from LLM-driven applications to agentic systems, and building more capable and reliable generative AI applications. The following table depicts multi-agent AI orchestration with of advanced fine-tuning technique examples.\n\nAmazon Pharmacy\nAmazon Global Engineering Services\nAmazon A+ Content\n\nDomain\nHealthcare\nConstruction and facilities\nEcommerce\n\nHigh-stakes factor\nPatient safety\nOperational efficiency\nCustomer trust\n\nChallenge\n$3.5 B annual cost from medication errors\n3+ hour inspection reviews\nQuality assessment at 100 million+ scale\n\nTechniques\nSFT, PPO, RLHF, advanced RL\nSFT, PPO, RLHF, advanced RL\nFeature-based fine-tuning\n\nKey outcome\n33% reduction in medication errors\n80% reduction in human effort\n77%–96% accuracy\n\nAmazon Healthcare Services (AHS) began its journey with generative AI with a significant challenge two years ago, when the team tackled customer service efficiency through a RAG-based Q&A system . Initial attempts using traditional RAG with foundation models yielded disappointing results, with accuracy hovering between 60 and 70%. The breakthrough came when they fine-tuned the embedding model specifically for pharmaceutical domain knowledge, resulted in a significant improvement to 90% accuracy and an 11% reduction in customer support contacts. In medication safety, medication direction errors can pose serious safety risks and cost up to $3.5 billion annually to correct. By fine-tuning a model with thousands of expert-annotated examples, Amazon Pharmacy created an agent component that validates medication directions using pharmacy logic and safety guidelines. This reduced near-miss events by 33%, as indicated in their Nature Medicine publication . In 2025, AHS is expanding their AI capabilities and transform these separate LLM-driven applications into a holistic multi-agent system to enhance patient experience. These individual applications driven by fine-tuned models play a crucial role in the overall agentic architecture, serving as domain expert tools to address specific mission-critical functions in pharmaceutical services.\nThe Amazon Global Engineering Services (GES) team, responsible for overseeing hundreds of Amazon fulfillment centers worldwide, embarked on an ambitious journey to use generative AI in their operations. Their initial foray into this technology focused on creating a sophisticated Q&A system designed to assist engineers in efficiently accessing relevant design information from vast knowledge repositories. The team’s approach was fine-tuning a foundation model using SFT, which resulted in a significant improvement in accuracy (measured by semantic similarity score) from 0.64 to 0.81. To better align with the feedback from the subject matter experts (SMEs), the team further refined the model using PPO incorporating the human feedback data, which boosted the LLM-judge scores from 3.9 to 4.2 out of 5, a remarkable achievement that translated to a substantial 80% reduction in the effort required from the domain experts. Similar to the Amazon Pharmacy case, these fine-tuned specialized models will continue to function as domain expert tools within the broader agentic AI system.\nIn 2025, the GES team ventured into uncharted territory by applying agentic AI systems to optimize their business process. LLM fine-tuning methodologies constitute a critical mechanism for enhancing the reasoning capabilities in AI agents, enabling effective decomposition of complex objectives into executable action sequences that align with predefined behavioral constraints and goal-oriented outcomes. It also serves as critical architecture component in facilitating specialized task execution and optimizing for task-specific performance metrics.\nAmazon A+ Content powers rich product pages across hundreds of millions of annual submissions. The A+ team needed to evaluate content quality at scale—assessing cohesiveness, consistency, and relevancy, not just surface-level defects. Content quality directly impacts conversion and brand trust, making this a high-stakes application.\nFollowing the architectural pattern seen in Amazon Pharmacy and Global Engineering Services, the team built a specialized evaluation agent powered by a fine-tuned model. They applied feature-based fine-tuning to Nova Lite on Amazon SageMaker —training a lightweight classifier on vision language model (VLM)-extracted features rather than updating full model parameters. This approach, enhanced by expert-crafted rubric prompts, improved classification accuracy from 77% to 96%. The result: an AI agent that evaluates millions of content submissions and delivers actionable recommendations. This demonstrates a key principle from our maturity framework—technique complexity should match task requirements. The A+ use case, while high-stakes and operating at massive scale, is fundamentally a classification task well-suited to these methods. Not every agent component requires GRPO or DAPO; selecting the right technique for each problem is what delivers efficient, production-grade systems.\nReference architecture for advanced AI orchestration using fine-tuning\nAlthough fine-tuned models serve diverse purposes across different domains and use cases in an agentic AI system, the anatomy of an agent remains largely consistent and can be encompassed in component groupings, as shown in the following architecture diagram.\n\nThis modular approach adopts a number of AWS generative AI services, including Amazon Bedrock AgentCore , Amazon SageMaker , and Amazon Bedrock , that maintains structure of key groupings that make up an agent while providing various options within each group to improve an AI agent.\n\nLLM customization for AI agents\n\nBuilders can use various AWS services to fine-tune and post-train the LLMs for an AI agent using the techniques discussed in the previous section. If you use LLMs on Amazon Bedrock for your agents, you can use multiple model customization approaches to fine-tune your models. Distillation and SFT through parameter-efficient fine-tuning (PEFT) with low-rank adaptation (LoRA) can be used to address simple customization tasks. For advanced fine-tuning, Continued Pre-training (CPT) extends a foundation model’s knowledge by training on domain-specific corpora (medical literature, legal documents, or proprietary technical content), embedding specialized vocabulary and domain reasoning patterns directly into model weights. Reinforcement fine-tuning (RFT), launched at re:Invent 2025 , teaches models to understand what makes a quality response without large amounts of pre-labeled training data. There are two approaches supported for RFT: Reinforcement Learning with Verifiable Rewards (RLVR) uses rule-based graders for objective tasks like code generation or math reasoning, while Reinforcement Learning from AI Feedback (RLAIF) uses AI-based judges for subjective tasks like instruction following or content moderation.\nIf you require deeper control over model customization infrastructure for your AI agents, Amazon SageMaker AI provides a comprehensive platform for custom model development and fine-tuning. Amazon SageMaker JumpStart accelerates the customization journey by offering pre-built solutions with one-click deployment of popular foundation models (Llama, Mistral, Falcon, and others) and end-to-end fine-tuning notebooks that handle data preparation, training configuration, and deployment workflows. Amazon SageMaker Training jobs provide managed infrastructure for executing custom fine-tuning workflows, automatically provisioning GPU instances, managing training execution, and handling cleanup after completion. This approach suits most fine-tuning scenarios where standard instance configurations provide sufficient compute power and training completes reliably within the job duration limits. You can use SageMaker Training jobs with custom Docker containers and code dependencies housing any machine learning (ML) framework, training library, or optimization technique, enabling experimentation with emerging methods beyond managed offerings.\nAt re:Invent 2025, Amazon SageMaker HyperPod introduced two capabilities for large-scale model customization:  Checkpointless training reduces checkpoint-restart cycles, shortening recovery time from hours to minutes.  Elastic training automatically scales workloads to use idle capacity and yields resources when higher-priority workloads peak. These features build on the core strengths of HyperPod—resilient distributed training clusters with automatic fault recovery for multi-week jobs spanning thousands of GPUs. HyperPod supports NVIDIA NeMo and AWS Neuronx frameworks, and is ideal when training scale, duration, or reliability requirements exceed what job-based infrastructure can economically provide.\nIn SageMaker AI, for builders who want to customize models without managing infrastructure, Amazon SageMaker AI serverless customization , launched at re:Invent 2025, provides a fully managed, UI- and SDK-driven experience for model fine-tuning. This capability provides infrastructure management—SageMaker automatically selects and provisions appropriate compute resources (P5, P4de, P4d, and G5 instances) based on model size and training requirements. Through the SageMaker Studio UI, you can customize popular models (Amazon Nova, Llama, DeepSeek, GPT-OSS, and Qwen) using advanced techniques including SFT, DPO, RLVR, and RLAIF. You can also run the same serverless customization using SageMaker Python SDK in your Jupyter notebook. The serverless approach provides pay-per-token pricing, automatic resource cleanup, integrated MLflow experiment tracking, and seamless deployment to both Amazon Bedrock and SageMaker endpoints.\nIf you need to customize Amazon Nova models for your agentic workflow, you can do it through recipes  and train them on SageMaker AI . It provides end-to-end customization workflow including model training, evaluation, and deployment for inference. with greater flexibility and control to fine-tune the Nova models, optimize hyperparameters with precision, and implement techniques such as LoRA PEFT, full-rank SFT, DPO, RFT, CPT, PPO, and so on. For the Nova models on Amazon Bedrock, you can also train your Nova models by SFT and RFT with reasoning content to capture intermediate thinking steps or use reward-based optimization when exact correct answers are difficult to define. If you have more advanced agentic use cases that require deeper model customization, you can use Amazon Nova Forge —launched at re:Invent 2025—to build your own frontier models from early model checkpoints, blend your datasets with Amazon Nova-curated training data, and host your custom models securely on AWS.\n\nAI agent development environments and SDKs\n\nThe development environment is where developers author, test, and iterate on agent logic before deployment. Developers use integrated development environments (IDEs) such as SageMaker AI Studio (Jupyter Notebooks compared to code editors), Amazon Kiro, or IDEs on local machines like PyCharm. Agent logic is implemented using specialized SDKs and frameworks that abstract orchestration complexity—Strands provides a Python framework purpose-built for multi-agent systems, offering declarative agent definitions, built-in state management, and native AWS service integrations that handle the low-level details of LLM API calls, tool invocation protocols, error recovery, and conversation management. With these development tools handling the low-level details of LLM API calls, developers can focus on business logic rather than infrastructure design and maintenance.\n\nAI agent deployment and operation\n\nAfter your AI agent development is completed and ready to deploy in production, you can use Amazon Bedrock AgentCore to handle agent execution, memory, security, and tool integration without requiring infrastructure management. Bedrock AgentCore provides a set of integrated services, including:\n\nAgentCore Runtime offers purpose-built environments that abstract away infrastructure management, while container-based alternatives (SageMaker AI jobs, AWS Lambda , Amazon Elastic Kubernetes Service (Amazon EKS) , and Amazon Elastic Container Service (Amazon ECS) ) provide more control for custom requirements. Essentially, the runtime is where your carefully crafted agent code meets real users and delivers business value at scale.\nAgentCore Memory gives your AI agents the ability to remember past interactions, enabling them to provide more intelligent, context-aware, and personalized conversations. It provides a straightforward and powerful way to handle both short-term context and long-term knowledge retention without the need to build or manage complex infrastructure.\nWith AgentCore Gateway , developers can build, deploy, discover, and connect to tools at scale, providing observability into tool usage patterns, error handling for failed invocations, and integration with identity systems for accessing tools on behalf of users (using OAuth or API keys). Teams can update tool backends, add new capabilities, or modify authentication requirements without redeploying agents because the gateway architecture decouples tool implementation from agent logic—maintaining flexibility as business requirements evolve.\nAgentCore Observability helps you trace, debug, and monitor agent performance in production environments. It provides real-time visibility into agent operational performance through access to dashboards powered by Amazon CloudWatch and telemetry for key metrics such as session count, latency, duration, token usage, and error rates, using the OpenTelemetry (OTEL) protocol standard.\n\nLLM and AI agent evaluation\n\nWhen your fine-tuned LLM driven AI agents are running in production, it’s important to evaluate and monitor your models and agents continuously to ensure high quality and performance. Many enterprise use cases require custom evaluation criteria that encode domain expertise and business rules. For the Amazon Pharmacy medication direction validation process, evaluation criteria include: drug-drug interaction detection accuracy (percentage of known contraindications correctly identified), dosage calculation precision (correct dosing adjustments for age, weight, and renal function), near-miss prevention rate (reduction in medication errors that could cause patient harm), FDA labeling compliance (adherence to approved usage, warnings, and contraindications), and pharmacist override rate (percentage of agent recommendations accepted without modification by licensed pharmacists).\nFor your models on Amazon Bedrock, you can use Amazon Bedrock evaluations to generate predefined metrics and human review workflows. For advanced scenarios, you can use SageMaker Training jobs to fine-tune specialized judge models on domain-specific evaluation datasets. For holistic AI agent evaluation, AgentCore Evaluations , launched at re:Invent 2025, provides automated assessment tools to measure your agent or tools performance on completing specific tasks, handling edge cases, and maintaining consistency across different inputs and contexts.\nDecision guide and recommended phased approach\nNow that you understand the technical evolution of advanced fine-tuning techniques—from SFT to PPO, DPO, GRPO, DAPO and GSPO—the critical question becomes when and why you should use them. Our experience shows that organizations using a phased maturity approach achieve 70–85% production conversion rates (compared to the 30–40% industry average) and 3-fold year-over-year ROI growth. The 12–18 month journey from initial agent deployment to advanced reasoning capabilities delivers incremental business value at each phase. The key is letting your use case requirements, available data, and measured performance guide advancement—not technical sophistication for its own sake.\nThe maturity path progresses through four phases (shown in the following table). Strategic patience in this progression builds reusable infrastructure, collects quality training data, and validates ROI before major investments. As our examples demonstrate, aligning technical sophistication with human and business needs delivers transformative outcomes and sustainable competitive advantages in your most critical AI applications.\n\nPhase\nTimeline\nWhen to use\nKey outcomes\nData needed\nInvestment\n\nPhase 1: Prompt engineering\n6–8 weeks\n\nStarting agent journey\nValidating business value\nSimple workflows\n\n60–75% accuracy)\nFailure patterns identified\n\nMinimal prompts, examples\n$50K–$80K (2–3 full-time employees (FTE))\n\nPhase 2: Supervised Fine-Tuning (SFT)\n12 weeks\n\nDomain knowledge gaps\nIndustry terminology issues\nNeed 80-85% accuracy\n\n80–85% accuracy 60–80% SME effort reduction\n\n500–5,000 labeled examples\n$120K–$180K (3–4 FTE and compute)\n\nPhase 3: Direct Preference Optimization (DPO)\n16 weeks\n\nQuality/style alignment\nSafety/compliance critical\nBrand consistency needed\n\n85–92% accuracy\nCSAT over 20%\n\n1,000–10,000 preference pairs\n$180K–$280K (4–5 FTE and compute)\n\nPhase 4: GRPO and DAPO\n24 weeks\n\nComplex reasoning required\nHigh-stakes decisions\nMulti-step orchestration\nExplainability essential\n\n95–98% accuracy\nMission-critical deployment\n\n10,000+ reasoning trajectories\n$400K-$800K (6–8 FTE and HyperPod)\n\nConclusion\nWhile agents have transformed how we build AI systems, advanced fine-tuning remains a critical component for enterprises seeking competitive advantage in high-stakes domains. By understanding the evolution of techniques like PPO, DPO, GRPO, DAPO and GSPO, and applying them strategically within agent architectures, organizations can achieve significant improvements in accuracy, efficiency, and safety. The real-world examples from Amazon demonstrate –that the combination of agentic workflows with carefully fine-tuned models delivers dramatic business outcomes.\nAWS continues to accelerate these capabilities with several key launches at re:Invent 2025. Reinforcement fine-tuning (RFT) on Amazon Bedrock now enables models to learn quality responses through RLVR for objective tasks and RLAIF for subjective evaluations—without requiring large amounts of pre-labeled data. Amazon SageMaker AI Serverless Customization eliminates infrastructure management for fine-tuning, supporting SFT, DPO, and RLVR techniques with pay-per-token pricing. For large-scale training, Amazon SageMaker HyperPod introduced checkpointless training and elastic scaling to reduce recovery time and optimize resource utilization. Amazon Nova Forge empowers enterprises to build custom frontier models from early checkpoints, blending proprietary datasets with Amazon-curated training data. Finally, AgentCore Evaluation provides automated assessment tools to measure agent performance on task completion, edge cases, and consistency—closing the loop on production-grade agentic AI systems.\nAs you evaluate your generative AI strategy, use the decision guide and phased maturity approach outlined in this post to identify where advanced fine-tuning can tip the scales from good enough to transformative. Use the reference architecture as a baseline to structure your agentic AI systems, and use the capabilities introduced at re:Invent 2025 to accelerate your journey from initial agent deployment to production-grade outcomes.\n\nAbout the authors\nYunfei Bai is a Principal Solutions Architect at AWS. With a background in AI/ML, data science, and analytics, Yunfei helps customers adopt AWS services to deliver business results. He designs AI/ML and data analytics solutions that overcome complex technical challenges and drive strategic objectives. Yunfei has a PhD in Electronic and Electrical Engineering. Outside of work, Yunfei enjoys reading and music.\nKristine Pearce is a Principal Worldwide Generative AI GTM Specialist at AWS, focused on SageMaker AI model customization, optimization, and inference at scale. She combines her MBA, BS Industrial Engineering background, and human-centered design expertise to bring strategic depth and behavioral science to AI-enabled transformation. Outside work, she channels her creativity through art.\nHarsh Asnani is a Worldwide Generative AI Specialist Solutions Architect at AWS specializing in ML theory, MLOPs, and production generative AI frameworks. His background is in applied data science with a focus on operationalizing AI workloads in the cloud at scale.\nSung-Ching Lin  is a Principal Engineer at Amazon Pharmacy, where he leads the design and adoption of AI/ML systems to improve customer experience and operational efficiency. He focuses on building scalable, agent-based architectures, ML evaluation frameworks, and production-ready AI solutions in regulated healthcare domains.\nElad Dwek is a Senior AI Business Developer at Amazon, working within Global Engineering, Maintenance, and Sustainability. He partners with stakeholders from business and tech side to identify opportunities where AI can enhance business challenges or completely transform processes, driving innovation from prototyping to production. With a background in construction and physical engineering, he focuses on change management, technology adoption, and building scalable, transferable solutions that deliver continuous improvement across industries. Outside of work, he enjoys traveling around the world with his family.\nCarrie Song is a Senior Program Manager at Amazon, working on AI-powered content quality and customer experience initiatives. She partners with applied science, engineering, and UX teams to translate generative AI and machine learning insights into scalable, customer-facing solutions. Her work focuses on improving content quality and streamlining the shopping experience on product detail pages.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Amazon AWS ML发布文章，探讨高级LLM微调技术在多Agent编排中的规模化应用模式。基于Amazon的实际案例，文章展示SFT、PPO、DPO、GRPO、DAPO和GSPO等微调技术如何在高风险的Agent系统中显著提升性能，如Amazon Pharmacy减少33%用药错误、Amazon Global Engineering Services降低80%人力努力、Amazon A+内容准确性从77%提高到96%。这些技术对于实现生产级AI系统至关重要，尤其在医疗、工程和电商领域，AWS在re:Invent 2025推出Reinforcement Fine-Tuning、SageMaker AI Serverless Customization、HyperPod和AgentCore Evaluation等新功能以支持企业应用。文章提供决策框架和参考架构，强调微调技术能推动更高ROI和业务增长。",
      "category": "LLM",
      "sentiment": "positive",
      "keywords": [
        "AWS",
        "LLM fine-tuning",
        "Agentic AI",
        "GRPO",
        "SageMaker"
      ]
    },
    "analyzed_at": "2026-01-17T03:08:36.101789Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "48557470095e6869",
    "title": "How Palo Alto Networks enhanced device security infra log analysis with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-palo-alto-networks-enhanced-device-security-infra-log-analysis-with-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T15:46:36Z",
    "summary": "Palo Alto Networks’ Device Security team wanted to detect early warning signs of potential production issues to provide more time to SMEs to react to these emerging problems. They partnered with the AWS Generative AI Innovation Center (GenAIIC) to develop an automated log classification pipeline powered by Amazon Bedrock. In this post, we discuss how Amazon Bedrock, through Anthropic’ s Claude Haiku model, and Amazon Titan Text Embeddings work together to automatically classify and analyze log d...",
    "content": "This post is co-written by Fan Zhang, Sr Principal Engineer / Architect from Palo Alto Networks.\nPalo Alto Networks ’ Device Security team wanted to detect early warning signs of potential production issues to provide more time to SMEs to react to these emerging problems. The primary challenge they faced was that reactively processing over 200 million daily service and application log entries resulted in delayed response times to these critical issues, leaving them at risk for potential service degradation.\nTo address this challenge, they partnered with the AWS Generative AI Innovation Center (GenAIIC) to develop an automated log classification pipeline powered by Amazon Bedrock. The solution achieved 95% precision in detecting production issues while reducing incident response times by 83%.\nIn this post, we explore how to build a scalable and cost-effective log analysis system using Amazon Bedrock to transform reactive log monitoring into proactive issue detection. We discuss how Amazon Bedrock, through Anthropic’ s Claude Haiku model, and Amazon Titan Text Embeddings work together to automatically classify and analyze log data. We explore how this automated pipeline detects critical issues, examine the solution architecture, and share implementation insights that have delivered measurable operational improvements.\nPalo Alto Networks offers Cloud-Delivered Security Services (CDSS) to tackle device security risks. Their solution uses machine learning and automated discovery to provide visibility into connected devices, enforcing Zero Trust principles . Teams facing similar log analysis challenges can find practical insights in this implementation.\nSolution overview\nPalo Alto Networks’ automated log classification system helps their Device Security team detect and respond to potential service failures ahead of time. The solution processes over 200 million service and application logs daily, automatically identifying critical issues before they escalate into service outages that impact customers.\nThe system uses Amazon Bedrock with Anthropic’s Claude Haiku model to understand log patterns and classify severity levels, and Amazon Titan Text Embeddings enables intelligent similarity matching. Amazon Aurora provides a caching layer that makes processing massive log volumes feasible in real time. The solution integrates seamlessly with Palo Alto Networks’ existing infrastructure, helping the Device Security team focus on preventing outages instead of managing complex log analysis processes.\nPalo Alto Networks and the AWS GenAIIC collaborated to build a solution with the following capabilities:\n\nIntelligent deduplication and caching – The system scales by intelligently identifying duplicate log entries for the same code event. Rather than using a large language model (LLM) to classify every log individually, the system first identifies duplicates through exact matching, then uses overlap similarity, and finally employs semantic similarity only if no earlier match is found. This approach cost-effectively reduces the 200 million daily logs by over 99%, to logs only representing unique events. The caching layer enables real-time processing by reducing the need for redundant LLM invocations.\nContext retrieval for unique logs – For unique logs, Anthropic’s Claude Haiku model using Amazon Bedrock classifies each log’s severity. The model processes the incoming log along with relevant labeled historical examples. The examples are dynamically retrieved at inference time through vector similarity search. Over time, labeled examples are added to provide rich context to the LLM for classification. This context-aware approach improves accuracy for Palo Alto Networks’ internal logs and systems and evolving log patterns that traditional rule-based systems struggle to handle.\nClassification with Amazon Bedrock – The solution provides structured predictions, including severity classification (Priority 1 (P1), Priority 2 (P2), Priority 3 (P3)) and detailed reasoning for each decision. This comprehensive output helps Palo Alto Networks’ SMEs quickly prioritize responses and take preventive action before potential outages occur.\nIntegration with existing pipelines for action – Results integrate with their existing FluentD and Kafka pipeline, with data flowing to Amazon Simple Storage Service (Amazon S3) and Amazon Redshift for further analysis and reporting.\n\nThe following diagram (Figure 1) illustrates how the three-stage pipeline processes Palo Alto Networks’ 200 million daily log volume while balancing scale, accuracy, and cost-efficiency. The architecture consists of the following key components:\n\nData ingestion layer – FluentD and Kafka pipeline and incoming logs\nProcessing pipeline – Consisting of the following stages:\n\nStage 1: Smart caching and deduplication – Aurora for exact matching and Amazon Titan Text Embeddings for semantic matching\nStage 2: Context retrieval – Amazon Titan Text Embeddings to enable historical labeled examples, and vector similarity search\nStage 3: Classification – Anthropic’s Claude Haiku model for severity classification (P1/P2/P3)\n\nOutput layer – Aurora, Amazon S3, Amazon Redshift, and SME review interface\n\nFigure 1: Automated log classification system architecture\n\nThe processing workflow moves through the following stages:\n\nStage 1: Smart caching and deduplication – Incoming logs from Palo Alto Networks’ FluentD and Kafka pipeline are immediately processed through an Aurora based caching layer. The system first applies exact matching, then falls back to overlap similarity, and finally uses semantic similarity through Amazon Titan Text Embeddings if no earlier match is found. During testing, this approach identified that more than 99% of logs corresponded to duplicate events, although they contained different time stamps, log levels, and phrasing. The caching system reduced response times for cached results and reduced unnecessary LLM processing.\nStage 2: Context retrieval for unique logs – The remaining less than 1% of truly unique logs require classification. For these entries, the system uses Amazon Titan Text Embeddings to identify the most relevant historical examples from Palo Alto Networks’ labeled dataset. Rather than using static examples, this dynamic retrieval makes sure each log receives contextually appropriate guidance for classification.\nStage 3: Classification with Amazon Bedrock – Unique logs and their selected examples are processed by Amazon Bedrock using Anthropic’s Claude Haiku model. The model analyzes the log content alongside relevant historical examples to produce severity classifications (P1, P2, P3) and detailed explanations. Results are stored in Aurora and the cache and integrated into Palo Alto Networks’ existing data pipeline for SME review and action.\n\nThis architecture enables cost-effective processing of massive log volumes while maintaining 95% precision for critical P1 severity detection. The system uses carefully crafted prompts that combine domain expertise with dynamically selected examples:\nsystem_prompt = \"\"\"\n<Task>\nYou are an expert log analysis system responsible for classifying production system logs based on severity. Your analysis helps engineering teams prioritize their response to system issues and maintain service reliability.\n</Task>\n<Severity_Definitions>\nP1 (Critical): Requires immediate action - system-wide outages, repeated application crashes\nP2 (High): Warrants attention during business hours - performance issues, partial service disruption\nP3 (Low): Can be addressed when resources available - minor bugs, authorization failures, intermittent network issues\n</Severity_Definitions>\n\n<Examples>\n<log_snippet>\n2024-08-17 01:15:00.00 [warn] failed (104: Connection reset by peer) while reading response header from upstream\n</log_snippet>\nseverity: P3\ncategory: Category A\n\n<log_snippet>\n2024-08-18 17:40:00.00 <warn> Error: Request failed with status code 500 at settle\n</log_snippet>\nseverity: P2\ncategory: Category B\n\n</Examples>\n\n<Target_Log>\nLog: {incoming_log_snippet}\nLocation: {system_location}\n</Target_Log>\"\"\"\n\nProvide severity classification (P1/P2/P3) and detailed reasoning.\nImplementation insights\nThe core value of Palo Alto Networks’ solution lies in making an insurmountable challenge manageable: AI helps their team analyze 200 million of daily volumes efficiently, while the system’s dynamic adaptability makes it possible to extend the solution into the future by adding more labeled examples. Palo Alto Networks’ successful implementation of their automated log classification system yielded key insights that can help organizations building production-scale AI solutions:\n\nContinuous learning systems deliver compounding value – Palo Alto Networks designed their system to improve automatically as SMEs validate classifications and label new examples. Each validated classification becomes part of the dynamic few-shot retrieval dataset, improving accuracy for similar future logs while increasing cache hit rates. This approach creates a cycle where operational use enhances system performance and reduces costs.\nIntelligent caching enables AI at production scale – The multi-layered caching architecture processes more than 99% of logs through cache hits, transforming expensive per-log LLM operations into a cost-effective system capable of handling 200 million daily volumes. This foundation makes AI processing economically viable at enterprise scale while maintaining response times.\nAdaptive systems handle evolving requirements without code changes – The solution accommodates new log categories and patterns without requiring system modifications. When performance needs improvement for novel log types, SMEs can label additional examples, and the dynamic few-shot retrieval automatically incorporates this knowledge into future classifications. This adaptability allows the system to scale with business needs.\nExplainable classifications drive operational confidence – SMEs responding to critical alerts require confidence in AI recommendations, particularly for P1 severity classifications. By providing detailed reasoning alongside each classification, Palo Alto Networks enables SMEs to quickly validate decisions and take appropriate action. Clear explanations transform AI outputs from predictions into actionable intelligence.\n\nThese insights demonstrate how AI systems designed for continuous learning and explainability become increasingly valuable operational assets.\nConclusion\nPalo Alto Networks’ automated log classification system demonstrates how generative AI powered by AWS helps operational teams manage vast volumes in real time. In this post, we explored how an architecture combining Amazon Bedrock, Amazon Titan Text Embeddings, and Aurora processes 200 million of daily logs through intelligent caching and dynamic few-shot learning, enabling proactive detection of critical issues with 95% precision. Palo Alto Networks’ automated log classification system delivered concrete operational improvements:\n\n95% precision, 90% recall for P1 severity logs – Critical alerts are accurate and actionable, minimizing false alarms while catching 9 out of 10 urgent issues, leaving the remaining alerts to be captured by existing monitoring systems\n83% reduction in debugging time – SMEs spend less time on routine log analysis and more time on strategic improvements\nOver 99% cache hit rate – The intelligent caching layer processes 20 million daily volume cost-effectively through subsecond responses\nProactive issue detection – The system identifies potential problems before they impact customers, preventing the multi-week outages that previously disrupted service\nContinuous improvement – Each SME validation automatically improves future classifications and increases cache efficiency, resulting in reduced costs\n\nFor organizations evaluating AI initiatives for log analysis and operational monitoring, Palo Alto Networks’ implementation offers a blueprint for building production-scale systems that deliver measurable improvements in operational efficiency and cost reduction. To build your own generative AI solutions, explore Amazon Bedrock for managed access to foundation models. For additional guidance, check out the AWS Machine Learning resources and browse implementation examples in the AWS Artificial Intelligence Blog .\nThe collaboration between Palo Alto Networks and the AWS GenAIIC demonstrates how thoughtful AI implementation can transform reactive operations into proactive, scalable systems that deliver sustained business value.\nTo get started with Amazon Bedrock, see Build generative AI solutions with Amazon Bedrock .\n\nAbout the authors\n\nRizwan Mushtaq\nRizwan is a Principal Solutions Architect at AWS. He helps customers design innovative, resilient, and cost-effective solutions using AWS services. He holds an MS in Electrical Engineering from Wichita State University.\n\nHector Lopez\nHector Lopez, PhD is an Applied Scientist in AWS’s Generative AI Innovation Center, where he specializes in delivering production-ready generative AI solutions and proof-of-concepts across diverse industry applications. His expertise spans traditional machine learning and data science in life and physical sciences. Hector implements a first-principles approach to customer solutions, working backwards from core business needs to help organizations understand and leverage generative AI tools for meaningful business transformation.\n\nMeena Menon\nMeena Menon is a Sr. Customer Success Manager at AWS with over 20 years of experience delivering enterprise customer outcomes and digital transformation. At AWS, she partners with strategic ISVs including Palo Alto Networks, Proofpoint, New Relic, and Splunk to accelerate cloud modernization and migrations.\n\nFan Zhang\nFan is a Senior Principal Engineer/Architect at Palo Alto Networks, leading the IoT Security team’s infrastructure and data pipeline, as well as its generative AI infrastructure.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Palo Alto Networks 设备安全团队与 AWS Generative AI Innovation Center 合作，基于 Amazon Bedrock 开发自动化日志分类管道，处理每日超过 2 亿条服务日志。该系统采用智能缓存和去重技术，结合 Anthropic 的 Claude Haiku 模型和 Amazon Titan Text Embeddings，实现 95% 精度检测关键生产问题，减少 83% 响应时间。技术架构包括三层处理：缓存去重、上下文检索、模型分类，帮助团队从被动监控转向主动问题检测，防止服务中断，显著提升操作效率。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Palo Alto Networks",
        "Amazon Bedrock",
        "Claude Haiku",
        "log analysis",
        "Amazon Titan Text Embeddings"
      ]
    },
    "analyzed_at": "2026-01-17T03:09:10.507358Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "1bdf1bad206d7d4e",
    "title": "From beginner to champion: A student’s journey through the AWS AI League ASEAN finals",
    "url": "https://aws.amazon.com/blogs/machine-learning/from-beginner-to-champion-a-students-journey-through-the-aws-ai-league-asean-finals/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T15:41:55Z",
    "summary": "The AWS AI League, launched by Amazon Web Services (AWS), expanded its reach to the Association of Southeast Asian Nations (ASEAN) last year, welcoming student participants from Singapore, Indonesia, Malaysia, Thailand, Vietnam, and the Philippines. In this blog post, you’ll hear directly from the AWS AI League champion, Blix D. Foryasen, as he shares his reflection on the challenges, breakthroughs, and key lessons discovered throughout the competition.",
    "content": "The AWS AI League , launched by Amazon Web Services (AWS) , expanded its reach to the Association of Southeast Asian Nations (ASEAN) last year, welcoming student participants from Singapore, Indonesia, Malaysia, Thailand, Vietnam, and the Philippines. The goal was to introduce students of all backgrounds and experience levels to the exciting world of generative AI through a gamified, hands-on challenge focused on fine-tuning large language models (LLMs).\nIn this blog post, you’ll hear directly from the AWS AI League champion, Blix D. Foryasen, as he shares his reflection on the challenges, breakthroughs, and key lessons discovered throughout the competition.\nBehind the competition\nThe AWS AI League competition began with a tutorial session led by the AWS team and the Gen-C Generative AI Learning Community , featuring two powerful user-friendly services: Amazon SageMaker JumpStart and PartyRock .\n\nSageMaker JumpStart enabled participants to run the LLM fine-tuning process in a cloud-based environment, offering flexibility to adjust hyperparameters and optimize performance.\nPartyRock, powered by Amazon Bedrock , provided an intuitive playground and interface to curate the dataset used in fine-tuning a Llama 3.2 3B Instruct model. Amazon Bedrock offers a comprehensive selection of high-performing foundation models from leading AI companies, including Anthropic Claude, Meta Llama, Mistral, and more; all accessible through a single API.\n\nWith the goal of outperforming a larger LLM reference model in a quiz-based evaluation, participants engaged with three core domains of generative AI: Foundation models, responsible AI, and prompt engineering. The preliminary round featured an open leaderboard ranking the best-performing fine-tuned models from across the region. Each submitted model was tested against a larger baseline LLM using an automated, quiz-style evaluation of generative AI-related questions. The evaluation, conducted by an undisclosed LLM judge, prioritized both accuracy and comprehensiveness. A model’s win rate improved each time it outperformed the baseline LLM. The challenge required strategic planning beyond its technical nature. Participants had to maximize their limited training hours on SageMaker JumpStart while carefully managing a restricted number of leaderboard submissions. Initially capped at 5 hours, the limit was later expanded to 30 hours in response to community feedback. Submission count would also influence tiebreakers for finalist selection.\nThe top tuner from each country advanced to the Regional Grand Finale, held on May 29, 2025, in Singapore. There, finalists competed head-to-head, each presenting their fine-tuned model’s responses to a new set of questions. Final scores were determined by a weighted judging system:\n\n40% by an LLM-as-a-judge,\n40% by experts\n20% by a live audience.\n\nA pragmatic approach to fine-tuning\nBefore diving into the technical details, a quick disclaimer: the approaches shared in the following sections are largely experimental and born from trial and error. They’re not necessarily the most optimal methods for fine-tuning, nor do they represent a definitive guide. Other finalists had different approaches because of different technical backgrounds. What ultimately helped me succeed wasn’t just technical precision, but collaboration, resourcefulness, and a willingness to explore how the competition might unfold based on insights from previous iterations. I hope this account can serve as a baseline or inspiration for future participants who might be navigating similar constraints. Even if you’re starting from scratch, as I did, there’s real value in being strategic, curious, and community-driven. One of the biggest hurdles I faced was time, or the lack of it. Because of a late confirmation of my participation, I joined the competition 2 weeks after it had already begun. That left me with only 2 weeks to plan, train, and iterate. Given the tight timeline and limited compute hours on SageMaker JumpStart, I knew I had to make every training session count. Rather than attempting exhaustive experiments, I focused my efforts on curating a strong dataset and tweaking select hyperparameters. Along the way, I drew inspiration from academic papers and existing approaches in LLM fine-tuning, adjusting what I could within the constraints.\nCrafting synthetic brilliance\nAs mentioned earlier, one of the key learning sessions at the start of the competition introduced participants to SageMaker JumpStart and PartyRock, tools that make fine-tuning and synthetic data generation both accessible and intuitive. In particular, PartyRock allowed us to clone and customize apps to control how synthetic datasets were generated. We could tweak parameters such as the prompt structure, creativity level (temperature), and token sampling strategy (top-p). PartyRock also gave us access to a wide range of foundation models. From the start, I opted to generate my datasets using Claude 3.5 Sonnet, aiming for broad and balanced coverage across all three core sub-domains of the competition. To minimize bias and implement fair representation across topics, I curated multiple dataset versions, each ranging from 1,500 to 12,000 Q&A pairs, carefully maintaining balanced distributions across sub-domains. The following are a few example themes that I focused on:\n\nPrompt engineering : Zero-shot prompting, chain-of-thought (CoT) prompting, evaluating prompt effectiveness\nFoundation models : Transformer architectures, distinctions between pretraining and fine-tuning\nResponsible AI : Dataset bias, representation fairness, and data protection in AI systems\n\nTo maintain data quality, I fine-tuned the dataset generator to emphasize factual accuracy, uniqueness, and applied knowledge. Each generation batch consisted of 10 Q&A pairs, with prompts specifically designed to encourage depth and clarity\nQuestion prompt:\n\nYou are a quiz master in an AI competition preparing a set of challenging quiz bee questions about [Topic to generate] The purpose of these questions is to determine the better LLM between a fine-tuned LLaMA 3.2 3B Instruct and larger LLMs. Generate [Number of data rows to generate] questions on [Topic to generate], covering:\n* Basic Questions (1/3) → Direct Q&A without reasoning. Must require a clear explanation, example, or real-world application. Avoid one-word fact-based questions.\n* Hybrid Questions (1/3) → Requires a short analytical breakdown (e.g., comparisons, trade-offs, weaknesses, implications). Prioritize scenario-based or real-world dilemma questions.\n* Chain-of-thought (CoT) Questions (1/3) → Requires multi-step logical deductions. Focus on evaluating existing AI methods, identifying risks, and critiquing trade-offs. Avoid open-ended \"Design/Propose/Create\" questions. Instead, use \"Compare, Evaluate, Critique, Assess, Analyze, What are the trade-offs of…\"\n\nEnsure the questions on [Topic to generate]:\n* Are specific, non-trivial, and informative.\n* Avoid overly simple questions (e.g., mere definitions or fact-based queries).\n* Encourage applied reasoning (i.e., linking theoretical concepts to real-world AI challenges).\n\nAnswer prompt:\n\nYou are an AI expert specializing in generative AI, foundation models, agentic AI, prompt engineering, and responsible AI. Your task is to generate well-structured, logically reasoned responses to a list of [Questions], ensuring that all responses follow a chain-of-thought (CoT) approach, regardless of complexity, and formatted in valid JSONL. Here are the answering guidelines:\n* Every response must be comprehensive, factually accurate, and well-reasoned.\n* Every response must use a step-by-step logical breakdown, even for seemingly direct questions.\nFor all questions, use structured reasoning:\n* For basic Questions, use a concise yet structured explanation. Simple Q&As should still follow CoT reasoning, explaining why the answer is correct rather than just stating facts.\n* For hybrid and CoT questions, use Chain of Thought and analyze the problem logically before providing a concluding statement.\n* If applicable, use real-world examples or research references to enhance explanations.\n* If applicable, include trade-offs between different AI techniques.\n* Draw logical connections between subtopics to reinforce deep understanding.\n\nAnswering prompt examples:\n\n* Basic question (direct Q&A without reasoning) → Use concise yet comprehensive, structured responses that provide a clear, well-explained, and well-structured definition and explanation without unnecessary verbosity.\n* Applications. Highlight key points step-by-step in a few comprehensive sentences.\n* Complex CoT question (multi-step reasoning) → Use CoT naturally, solving each step explicitly, with in-depth reasoning\n\nFor question generation, I set the temperature to 0.7, favoring creative and novel phrasing without drifting too far from factual grounding. For answer generation, I used a lower temperature of 0.2, targeting precision and correctness. In both cases, I applied top-p = 0.9, allowing the model to sample from a focused yet diverse range of likely tokens, encouraging nuanced outputs. One important strategic assumption I made throughout the competition was that the evaluator LLM would prefer more structured, informative, and complete responses over overly creative or brief ones. To align with this, I included reasoning steps in my answers to make them longer and more comprehensive. Research has shown that LLM-based evaluators often score detailed, well-explained answers higher, and I leaned into that insight during dataset generation.\nRefining the submissions\nSageMaker JumpStart offers a wide array of hyperparameters to configure, which can feel overwhelming, especially when you’re racing against time and unsure of what to prioritize. Fortunately, the organizers emphasized focusing primarily on epochs and learning rate, so I honed in on those variables. Each training job with a single epoch took approximately 10–15 minutes, making time management critical. To avoid wasting valuable compute hours, I began with a baseline dataset of 1,500 rows to test combinations of epochs and learning rates. I explored:\n\nEpochs : 1 to 4\nLearning rates : 0.0001, 0.0002, 0.0003, and 0.0004\n\nAfter multiple iterations, the combination of two epochs and a learning rate of 0.0003 yielded the best result, achieving a 53% win rate on my 13th leaderboard submission. Encouraged by this, I continued using this combination for several subsequent experiments, even as I expanded my dataset. Initially, this strategy appeared to work. With a dataset of approximately 3,500 rows, my model reached a 57% win rate by my 16th submission. However, as I further increased the dataset to 5,500, 6,700, 8,500, and eventually 12,000 rows, my win rate steadily declined to 53%, 51%, 45%, and 42% respectively. At that point, it was clear that only increasing dataset size wasn’t enough, in fact, it might have been counterproductive without revisiting the hyperparameters. With only 5 training hours remaining and 54 submissions logged, I found myself stuck at 57%, while peers like the top tuner from the Philippines were already reaching a 71% win rate.\nLessons from the field\nWith limited time left, both for training and leaderboard submissions, I turned to cross-country collaboration for support. One of the most insightful conversations I had was with Michael Ismail Febrian, the top tuner from Indonesia and the highest scorer in the elimination round. He encouraged me to explore LoRA (low-rank adaptation) hyperparameters, specifically:\n\nlora_r\nlora_alpha\ntarget_modules\n\nMichael also suggested enriching my dataset by using API-generated responses from more capable teacher models, especially for answering PartyRock-generated questions. Looking back at my existing fine-tuning pipeline, I realized a critical weakness: the generated answers were often too concise or shallow. Here’s an example of a typical Q&A pair from my earlier dataset:\n\nInstruction: What factors influence model hallucination?\n\nResponse: Let's analyze this step by step:\n* Training factors:\n* Data quality and coverage\n* Training methodology\n* Model architecture\n* Deployment factors:\n* Prompt design\n* Temperature settings\n* Context window utilization\n* Mitigation strategies:\n*Retrieval augmentation\n* Fact-checking mechanisms\n* Confidence thresholds\n\nExample: GPT-3 showing lower hallucination rates (15% vs 21%) when using retrieval-augmented generation compared to base model.\n\nWhile this structure is clean and organized, it lacked deeper explanation for each point, something models like ChatGPT and Gemini typically do well. I suspect this limitation came from token constraints when generating multiple responses in bulk. In my case, I generated 10 responses at a time in JSONL format under a single prompt, which might have led PartyRock to truncate outputs. Not wanting to spend on paid APIs, I discovered OpenRouter.ai, which offers limited access to large models, albeit rate-limited. With a cap of roughly 200 Q&A pairs per day per account, I got creative—I created multiple accounts to support my expanded dataset. My teacher model of choice was DeepSeek R1, a popular option known for its effectiveness in training smaller, specialized models. It was a bit of a gamble, but one that paid off in terms of output quality.\nAs for LoRA tuning, here’s what I learned:\n\nlora_r and lora_alpha determine how much and how complex new information the model can absorb. A common rule of thumb is setting lora_alpha to 1x or 2x of lora_r .\ntarget_modules defines which parts of the model are updated, often the attention layers or the feed-forward network.\n\nI also consulted Kim, the top tuner from Vietnam, who flagged my 0.0003 learning rate as potentially too high. He, along with Michael, suggested a different strategy: increase the number of epochs and reduce the learning rate. This would allow the model to better capture complex relationships and subtle patterns, especially as dataset size grows. Our conversations underscored a hard-learned truth: data quality is more important than data quantity. There’s a point of diminishing returns when increasing dataset size without adjusting hyperparameters or validating quality—something I directly experienced. In hindsight, I realized I had underestimated how vital fine-grained hyperparameter tuning is, especially when scaling data. More data demands more precise tuning to match the growing complexity of what the model needs to learn.\nLast-minute gambits\nArmed with fresh insights from my collaborators and hard-won lessons from previous iterations, I knew it was time to pivot my entire fine-tuning pipeline. The most significant change was in how I generated my dataset. Instead of using PartyRock to produce both questions and answers, I opted to generate only the questions in PartyRock, then feed those prompts into the DeepSeek-R1 API to generate high-quality responses. Each answer was saved in JSONL format, and, crucially, included detailed reasoning. This shift significantly increased the depth and length of each answer, averaging around 900 tokens per response, compared to the much shorter outputs from PartyRock. Given that my earlier dataset of approximately 1,500 high-quality rows produced promising results, I stuck with that size for my final dataset. Rather than scale up in quantity, I doubled down on quality and complexity. For this final round, I made bold, blind tweaks to my hyperparameters:\n\nDropped the learning rate to 0.00008\nIncreased the LoRA parameters:\n\nlora_r = 256\nlora_alpha = 256\n\nExpanded LoRA target modules to cover both attention and feed-forward layers: q_proj , k_proj , v_proj , o_proj , gate_proj , up_proj , down_proj\n\nThese changes were made with one assumption: longer, more complex answers require more capacity to absorb and generalize nuanced patterns. I hoped that these settings would enable the model to fully use the high-quality, reasoning-rich data from DeepSeek-R1.With only 5 hours of training time remaining, I had just enough for two full training runs, each using different epoch settings (3 and 4). It was a make-or-break moment. If the first run underperformed, I had one last chance to redeem it. Thankfully, my first test run achieved a 65% win rate, a massive improvement, but still behind the current leader from the Philippines and trailing Michael’s impressive 89%. Everything now hinged on my final training job. It had to run smoothly, avoid errors, and outperform everything I had tried before. And it did. That final submission achieved a 77% win rate, pushing me to the top of the leaderboard and securing my slot for the Grand Finale. After weeks of experimentation, sleepless nights, setbacks, and late-game adjustments, the journey, from a two-week-late entrant to national champion, was complete.\nWhat I wish I had known sooner\nI won’t pretend that my success in the elimination round was purely technical—luck played a big part. Still, the journey revealed several insights that could save future participants valuable time, training hours, and submissions. Here are some key takeaways I wish I had known from the start:\n\nQuality is more important than quantity : More data doesn’t always mean better results. Whether you’re adding rows or increasing context length, you’re also increasing the complexity that the model must learn from. Focus on crafting high-quality, well-structured examples rather than blindly scaling up.\nFast learner compared to Slow learner : If you’re avoiding deep dives into LoRA or other advanced tweaks, understanding the trade-off between learning rate and epochs is essential. A higher learning rate with fewer epochs might converge faster, but could miss the subtle patterns captured by a lower learning rate over more epochs. Choose carefully based on your data’s complexity.\nDon’t neglect hyperparameters : One of my biggest missteps was treating hyperparameters as static, regardless of changes in dataset size or complexity. As your data evolves, your model settings should too. Hyperparameters should scale with your data.\nDo your homework : Avoid excessive guesswork by reading relevant research papers, documentation, or blog posts. Late in the competition, I stumbled upon helpful resources that I could have used to make better decisions earlier. A little reading can go a long way.\nTrack everything : When experimenting, it’s easy to forget what worked and what didn’t. Maintain a log of your datasets, hyperparameter combinations, and performance outcomes. This helps optimize your runs and aids in debugging.\nCollaboration is a superpower: While it’s a competition, it’s also a chance to learn. Connecting with other participants, whether they’re ahead or behind, gave me invaluable insights. You might not always walk away with a trophy, but you’ll leave with knowledge, relationships, and real growth.\n\nGrand Finale\nThe Grand Finale took place on the second day of the National AI Student Challenge, serving as the culmination of weeks of experimentation, strategy, and collaboration. Before the final showdown, all national champions had the opportunity to engage in the AI Student Developer Conference, where we shared insights, exchanged lessons, and built connections with fellow finalists from across the ASEAN region. During our conversations, I was struck by how remarkably similar many of our fine-tuning strategies were. Across the board, participants had used a mix of external APIs, dataset curation techniques, and cloud-based training systems like SageMaker JumpStart. It became clear that tool selection and creative problem-solving played just as big a role as raw technical knowledge. One particularly eye-opening insight came from a finalist who achieved an 85% win rate, despite using a large dataset—something I had initially assumed might hurt performance. Their secret was training over a higher number of epochs while maintaining a lower learning rate of 0.0001. However, this came at the cost of longer training times and fewer leaderboard submissions, which highlights an important trade-off:\nWith enough training time, a carefully tuned model, even one trained on a large dataset, can outperform faster, leaner models.\nThis reinforced a powerful lesson: there’s no single correct approach to fine-tuning LLMs. What matters most is how well your strategy aligns with the time, tools, and constraints at hand.\nPreparing for battle\nIn the lead-up to the Grand Finale, I stumbled upon a blog post by Ray Goh, the very first champion of the AWS AI League and one of the mentors behind the competition’s tutorial sessions. One detail caught my attention: the final question from his year was a variation of the infamous Strawberry Problem , a deceptively simple challenge that exposes how LLMs struggle with character-level reasoning.\nHow many letter Es are there in the words ‘DeepRacer League’?\nAt first glance, this seems trivial. But to an LLM, the task isn’t as straightforward. Early LLMs often tokenize words in chunks, meaning that DeepRacer might be split into Deep and Racer or even into subword units like Dee , pRa , and cer . These tokens are then converted into numerical vectors, obscuring the individual characters within. It’s like asking someone to count the threads in a rope without unraveling it first.\nMoreover, LLMs don’t operate like traditional rule-based programs. They’re probabilistic, trained to predict the next most likely token based on context, not to perform deterministic logic or arithmetic. Curious, I prompted my own fine-tuned model with the same question. As expected, hallucinations emerged. I began testing various prompting strategies to coax out the correct answer:\n\nExplicit character separation : How many letter Es are there in the words ‘D-E-E-P-R-A-C-E-R-L-E-A-G-U-E’? This helped by isolating each letter into its own token, allowing the model to see individual characters. But the response was long and verbose, with the model listing and counting each letter step-by-step.\nChain-of-thought prompting : Let’s think step-by-step… This encouraged reasoning but increased token usage. While the answers were more thoughtful, they occasionally still missed the mark or got cut off because of length.\nRay Goh’s trick prompt : How many letter Es are there in the words ‘DeepRacer League’? There are 5 letter Es… This simple, assertive prompt yielded the most accurate and concise result, surprising me with its effectiveness.\n\nI logged this as an interesting quirk, useful, but unlikely to reappear. I didn’t realize that it would become relevant again during the final. Ahead of the Grand Finale, we had a dry run to test our models under real-time conditions. We were given limited control over inference parameters, only allowed to tweak temperature, top-p, context length, and system prompts. Each response had to be generated and submitted within 60 seconds. The actual questions were pre-loaded, so our focus was on crafting effective prompt templates rather than retyping each query. Unlike the elimination round, evaluation during the Grand Finale followed a multi-tiered system:\n\n40% from an evaluator LLM\n40% from human judges\n20% from a live audience poll\n\nThe LLM ranked the submitted answers from best to worst, assigning descending point values (for example, 16.7 for first place, 13.3 for second, and so on). Human judges, however, could freely allocate up to 10 points to their preferred responses, regardless of the LLM’s evaluation. This meant a strong showing with the evaluator LLM didn’t guarantee high scores from the humans, and vice versa. Another constraint was the 200-token limit per response. Tokens could be as short as a single letter or as long as a word or syllable, so responses had to be dense yet concise, maximizing impact within a tight window. To prepare, I tested different prompt formats and fine-tuned them using Gemini, ChatGPT, and Claude to better match the evaluation criteria. I stored dry-run responses from the Hugging Face LLaMA 3.2 3B Instruct model, then passed them to Claude Sonnet 4 for feedback and ranking. I continued using the following two prompts because they provided the best response in terms of accuracy and comprehensiveness:\nPrimary prompt:\n\nYou are an elite AI researcher and educator specializing in Generative AI, Foundational Models, Agentic AI, Responsible AI, and Prompt Engineering. Your task is to generate a highly accurate, comprehensive, and well-structured response to the question below in no more than 200 words.\n\nEvaluation will be performed by Claude Sonnet 4, which prioritizes:\n* Factual Accuracy – All claims must be correct and verifiable. Avoid speculation.\n* Comprehensiveness – Cover all essential dimensions, including interrelated concepts or mechanisms.\n* Clarity & Structure – Use concise, well-organized sections (e.g., brief intro, bullet points, and/or transitions). Markdown formatting (headings/lists) is optional.\n* Efficiency – Every sentence must deliver unique insight. Avoid filler.\n* Tone – Maintain a professional, neutral, and objective tone.\n\nYour response should be dense with value while remaining readable and precise.\n\nBackup prompt:\n\nYou are a competitive AI practitioner with deep expertise in [Insert domain: e.g., Agentic AI or Prompt Engineering], answering a technical question evaluated by Claude Sonnet 4 for accuracy and comprehensiveness. You must respond in exactly 200 words.\n\nFormat your answer as follows:\n* Direct Answer (1–2 sentences) – Immediately state the core conclusion or definition.\n* Key Technical Points (3–4 bullet points) – Essential mechanisms, distinctions, or principles.\n* Practical Application (1–2 sentences) – Specific real-world use cases or design implications.\n* Critical Insight (1 sentence) – Mention a key challenge, trade-off, or future direction.\n\nAdditional requirements:\n\nUse precise technical language and terminology.\nInclude specific tools, frameworks, or metrics if relevant.\nEvery sentence must contribute uniquely—no redundancy.\nMaintain a formal tone and answer density without over-compression.\n\nIn terms of hyperparameters, I used:\n\nTop-p = 0.9\nMax tokens = 200\nTemperature = 0.2, to prioritize accuracy over creativity\n\nMy strategy was simple: appeal to the AI judge. I believed that if my answer ranked well with the evaluator LLM, it would also impress human judges. Oh, how I was humbled.\nJust aiming for third… until I wasn’t\nStanding on stage before a live audience was nerve-wracking. This was my first solo competition, and it was already on a massive regional scale. To calm my nerves, I kept my expectations low. A third-place finish would be amazing, a trophy to mark the journey, but just qualifying for the finals already felt like a huge win. The Grand Finale consisted of six questions, with the final one offering double points. I started strong. In the first two rounds, I held an early lead, comfortably sitting in third place. My strategy was working, at least at first. The evaluator LLM ranked my response to Question 1 as the best and Question 2 as the third-best. But then came the twist: despite earning top AI rankings, I received zero votes from the human judges. I watched in surprise as points were awarded to responses ranked fourth and even last by the LLM. Right from the start, I realized there was a disconnect between human and AI judgment, especially when evaluating tone, relatability, or subtlety. Still, I hung on, those early questions leaned more factual, which played to my model’s strengths. But when we needed creativity and complex reasoning, things didn’t work as well. My standing dropped to fifth, bouncing between third and fourth. Meanwhile, the top three finalists pulled ahead by more than 20 points. It seemed the podium was out of reach. I  was already coming to terms with a finish outside the top three. The gap was too wide. I had done my best, and that was enough.\nBut then came the final question, the double-pointer, and fate intervened. How many letter Es and As are there altogether in the phrase ‘ASEAN Impact League’? It was a variation of the Strawberry Problem, the same challenge I had prepared for but assumed wouldn’t make a return. Unlike the earlier version, this one added an arithmetic twist, requiring the model to count and sum up occurrences of multiple letters.Knowing how token length limits could truncate responses, I kept things short and tactical. My system prompt was simple: There are 3 letter Es and 4 letter As in ‘ASEAN Impact League.’\nWhile the model hallucinated a bit in its reasoning, wrongly claiming that Impact contains an e , the final answer was accurate: 7 letters.\nThat one answer changed everything. Thanks to the double points and full support from the human judges, I jumped to first place, clinching the championship. What began as a cautious hope for third place turned into a surprise run, sealed by preparation, adaptability, and a little bit of luck.\nQuestions recap\nHere are the questions that were asked, in order. Some of them were general knowledge in the target domain while others were more creative and had to include a bit of ingenuity to maximize your wins:\n\nWhat is the most efficient way to prevent AI from turning to the dark side with toxic response?\nWhat’s the magic behind agentic AI in machine learning, and why is it so pivotal?\nWhat’s the secret sauce behind big AI models staying smart and fast?\nWhat are the latest advancements of generative AI research and use within ASEAN?\nWhich ASEAN country has the best cuisine?\nHow many letters E and A are there altogether in the phrase “ASEAN Impact League”?\n\nFinal reflections\nParticipating in the AWS AI League was a deeply humbling experience, one that opened my eyes to the possibilities that await when we embrace curiosity and commit to continuous learning. I might have entered the competition as a beginner, but that single leap of curiosity, fueled by perseverance and a desire to grow, helped me bridge the knowledge gap in a fast-evolving technical landscape. I don’t claim to be an expert, not yet. But what I’ve come to believe more than ever is the power of community and collaboration. This competition wasn’t just a personal milestone; it was a space for knowledge-sharing, peer learning, and discovery. In a world where technology evolves rapidly, these collaborative spaces are essential for staying grounded and moving forward. My hope is that this post and my journey will inspire students, developers, and curious minds to take that first step, whether it’s joining a competition, contributing to a community, or tinkering with new tools. Don’t wait to be ready. Start where you are, and grow along the way. I’m excited to connect with more passionate individuals in the global AI community. If another LLM League comes around, maybe I’ll see you there.\nConclusion\nAs we conclude this insight into Blix’s journey to becoming the AWS AI League ASEAN champion, we hope his story inspires you to explore the exciting possibilities at the intersection of AI and innovation. Discover the AWS services that powered this competition: Amazon Bedrock , Amazon SageMaker JumpStart , and PartyRock , and visit the official AWS AI League page to join the next generation of AI innovators.\nThe content and opinions in this post are those of the third-party author and AWS is not responsible for the content or accuracy of this post.\n\nAbout the authors\nNoor Khan is a Solutions Architect at AWS supporting Singapore’s public sector education and research landscape. She works closely with academic and research institutions, leading technical engagements and designing secure, scalable architectures. As part of the core AWS AI League team, she architected and built the backend for the platform, enabling customers to explore real-world AI use cases through gamified learning. Her passions include AI/ML, generative AI, web development and empowering women in tech!\nVincent Oh is the Principal Solutions Architect in AWS for Data & AI. He works with public sector customers across ASEAN, owning technical engagements and helping them design scalable cloud solutions. He created the AI League in the midst of helping customers harness the power of AI in their use cases through gamified learning. He also serves as an Adjunct Professor in Singapore Management University (SMU), teaching computer science modules under School of Computer & Information Systems (SCIS). Prior to joining Amazon, he worked as Senior Principal Digital Architect at Accenture and Cloud Engineering Practice Lead at UST.\nBlix Foryasen  is a Computer Science student specializing in Machine Learning at National University – Manila. He is passionate about data science, AI for social good, and civic technology, with a strong focus on solving real-world problems through competitions, research, and community-driven innovation. Blix is also deeply engaged with emerging technological trends, particularly in AI and its evolving applications across industries, specifically in finance, healthcare, and education.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "在AWS AI League东盟决赛中，学生Blix D. Foryasen分享了从延迟两周参赛到最终夺冠的旅程。竞赛由Amazon Web Services组织，旨在通过游戏化挑战向东南亚学生介绍生成式AI，参与者使用Amazon SageMaker JumpStart和PartyRock微调Llama 3.2 3B Instruct模型。Blix面临时间限制，通过策略性生成高质量数据集、调整超参数（如学习率和LoRA），并与来自印尼和越南的参与者协作，最终在决赛中以77%胜率获胜。竞赛评估结合了AI评委、专家和观众投票，突出了数据质量优于数量、协作学习的重要性，促进了AI教育并展示了AWS工具在实践学习中的价值。",
      "category": "LLM",
      "sentiment": "positive",
      "keywords": [
        "AWS",
        "Amazon SageMaker JumpStart",
        "PartyRock",
        "LLM",
        "fine-tuning"
      ]
    },
    "analyzed_at": "2026-01-17T03:08:04.205768Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "832d207cf9d146cf",
    "title": "Deploy AI agents on Amazon Bedrock AgentCore using GitHub Actions",
    "url": "https://aws.amazon.com/blogs/machine-learning/deploy-ai-agents-on-amazon-bedrock-agentcore-using-github-actions/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T15:37:37Z",
    "summary": "In this post, we demonstrate how to use a GitHub Actions workflow to automate the deployment of AI agents on AgentCore Runtime. This approach delivers a scalable solution with enterprise-level security controls, providing complete continuous integration and delivery (CI/CD) automation.",
    "content": "Recently, AWS announced Amazon Bedrock AgentCore , a flexible service that helps developers seamlessly create and manage AI agents across different frameworks and models, whether hosted on Amazon Bedrock or other environments. Specifically, AgentCore Runtime provides a secure, serverless, and purpose-built hosting environment for deploying and running AI agents or tools. AgentCore Runtime is framework agnostic, working seamlessly with popular frameworks like LangGraph, Strands, and CrewAI for deploying your AI agents and tools with automatic scaling and built-in security.\nIn this post, we demonstrate how to use a GitHub Actions workflow to automate the deployment of AI agents on AgentCore Runtime. This approach delivers a scalable solution with enterprise-level security controls, providing complete continuous integration and delivery (CI/CD) automation. By implementing a comprehensive pipeline, we enable seamless agent deployment with AWS best practices, including OpenID Connect (OIDC) authentication, least-privilege access controls, and environment separation. Our solution facilitates efficient updates for existing agents and integrates continuous security scans and rigorous code quality checks. The result is a robust deployment strategy that helps minimize operational complexity, enhance security, and accelerate AI agent development across enterprise environments.\nBenefits of Amazon Bedrock AgentCore Runtime\nAgentCore Runtime is the ideal service for production agent deployments:\n\nProvides a framework agnostic environment to run your agents\nWorks with large language models (LLMs) such as models offered by Amazon Bedrock and Anthropic Claude\nProvides session isolation by running each user session in a dedicated microVM with isolated CPU, memory, and file system resources\nSupports both real-time interactions and long-running workloads up to 8 hours\nOffers built-in capabilities for authentication and observability\n\nSolution overview\nWe’ve developed a comprehensive CI/CD pipeline with GitHub Actions that streamlines the deployment of Agents in compliance with security standard. The pipeline is available as a ready-to-use solution that can integrate seamlessly with your existing development workflow.The solution consists of the following key components:\n\nGitHub Actions – A workflow orchestration tool to host the pipeline\nAmazon Bedrock AgentCore Runtime – An AWS service to host and run the deployed agents\nAmazon Elastic Container Registry (Amazon ECR) – An AWS service to store, manage, and deploy container images for agents\nAmazon Inspector – An AWS service to perform advanced and continuous vulnerability scanning on container images\nIAM OIDC identity provider – A federated authentication service to establish trust between GitHub and AWS to allow GitHub Actions to deploy on AWS without maintaining AWS secrets and credentials\n\nThe following diagram illustrates the architecture for the solution.\n\nThe data flow consists of the following steps:\n\nA developer commits code changes from their local repository to the GitHub repository. In this solution, the GitHub Action is triggered manually, but this can be automated.\nThe GitHub Action triggers the build stage.\nGitHub’s OIDC uses tokens to authenticate with AWS and access resources.\nGitHub Actions invokes the command to build and push the agent container image to Amazon ECR directly from the Dockerfile.\nAWS Inspector triggers an advanced security scan when the image is uploaded.\nAn AgentCore Runtime instance is created using the container image.\nThe agent can further query the Amazon Bedrock model and invoke tools according to its configuration.\n\nIn the following sections, we walk through the steps to deploy the solution:\n\nDownload the source code from the GitHub repo.\nCreate your agent code.\nSet up GitHub secrets.\nCreate an IAM role and policies.\nCreate the GitHub Actions workflow.\nTrigger and monitor the pipeline.\nVerify the deployment.\n\nPrerequisites\nBefore you can use our secure CI/CD pipeline for deploying agents to AgentCore Runtime, verify you have the following prerequisites in place:\n\nAn AWS account with sufficient permissions to create IAM Identity and Access Management (IAM) roles\nAccess to the required LLM model from the supported foundation models in Amazon Bedrock\nGitHub account with Actions enabled (GitHub.com or GitHub Enterprise)\nPython 3.10+ for local development and testing\n\nDownload source code\nClone the source code repository: bedrock-agentcore-runtime-cicd\ngit clone https://github.com/aws-samples/sample-bedrock-agentcore-runtime-cicd.git\nThe repository folder consists of the following structure:\n\nbedrock-agentcore-runtime-cicd/\n├── .github/\n│ └── workflows/\n│ └── deploy-agentcore.yml # file contains the set of action to build and deploy the agent on AgentCore Runtime\n│ └── test-agent.yml # after deployment this file is used to test agent via manual workflow dispatch\n├── agents/\n│ ├── strands_agent.py # uses BedrockAgentCoreApp app that creates an AI agent using the Strands framework with Claude as the underlying model\n│ ├── requirements.txt # contains dependencies\n├── scripts\n│ ├── create_iam_role.py # IAM role required for Bedrock AgentCore Runtime\n│ ├── deploy_agent.py # deploys a custom agent to AWS Bedrock's AgentCore Runtime platform, which allows you to run containerized AI agents on AWS infrastructure\n│ └── setup_oidc.py # OIDC setup for Github Authentication and Authorization to access AWS account to deploy required services\n│ └── cleanup_ecr.py # keeps 9 recent images in ECR registry, can be customized\n│ └── create_guardrail.py # setup minimum guardrail for content filtering, can be customized according to use case\n│ └── test_agent.py # contains test cases\n└── Dockerfile # contain instructions to build Docker image\n└── README.md\n\nCreate agent code\nCreate your agent with the framework of your choice using the AgentCore Runtime toolkit . The toolkit uses BedrockAgentCoreApp to create an application that provides a standardized way to package your AI agent code into a container that can run on AgentCore Runtime managed infrastructure. It also uses app.entrypoint , a Python decorator that marks a function as the main entry point. When the Amazon Bedrock agent receives the incoming API request, this function receives and processes the user’s request. In this sample agent code, when someone calls your Amazon Bedrock agent using an API, AgentCore Runtime will automatically call the strands_agent_bedrock(payload) function.\nIn this post, we use the agents/strands_agent.py file to create an agent using the Strands Agents framework:\n\n\"\"\"\nThis module defines a conversational AI agent that can perform calculations\nusing the Strands framework.\n\"\"\"\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom strands_tools import calculator\n# Initialize the Bedrock AgentCore application\napp = BedrockAgentCoreApp()\n# Configure the Claude model for the agent with guardrail\nmodel_id = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n# Load guardrail ID if available\nguardrail_config = None\ntry:\nwith open(\"guardrail_id.txt\", \"r\", encoding=\"utf-8\") as f:\nguardrail_id = f.read().strip()\nif guardrail_id:\nguardrail_config = {\n\"guardrailIdentifier\": guardrail_id,\n\"guardrailVersion\": \"1\",\n}\nprint(f\"Loaded guardrail: {guardrail_id}\")\nexcept FileNotFoundError:\nprint(\"No guardrail file found - running without guardrail\")\nmodel = BedrockModel(model_id=model_id, guardrail=guardrail_config)\n# Create the agent with tools and system prompt\nagent = Agent(\nmodel=model,\ntools=[calculator],\nsystem_prompt=\"You're a helpful assistant. You can do simple math calculation.\",\n)\n@app.entrypoint\ndef strands_agent_bedrock(payload):\n\"\"\"\nMain entrypoint for the Bedrock AgentCore Runtime.\nThis function is called by AWS Bedrock AgentCore when the agent receives\na request. It processes the user input and returns the agent's response.\nArgs:\npayload (dict): Request payload containing user input\nExpected format: {\"prompt\": \"user question\"}\nReturns:\nstr: The agent's text response to the user's prompt\n\"\"\"\n# Extract the user's prompt from the payload\nuser_input = payload.get(\"prompt\")\n# Process the input through the agent (handles tool selection and model inference)\nresponse = agent(user_input)\n# Extract and return the text content from the response\nreturn response.message[\"content\"][0][\"text\"]\nif __name__ == \"__main__\":\n# Run the application locally for testing\n# In production, this is handled by Bedrock AgentCore Runtime\napp.run()\n\nSet up GitHub secrets\nThe GitHub Actions workflow must access resources in your AWS account. In this post, we use an IAM OpenID Connect identity provider and IAM roles with IAM policies to access AWS resources. OIDC lets your GitHub Actions workflows access resources in AWS without needing to store the AWS credentials as long-lived GitHub secrets. These credentials are stored as GitHub secrets within your GitHub repository Settings under Secrets option. For more information, see Using secrets in GitHub Actions .\nCreate IAM roles and policies\nTo run agents or tools in AgentCore Runtime, you need an IAM execution role. For information about creating an IAM role, see IAM role creation .\nIn this post, we create the required trust policy and execution role for AgentCore Runtime. See IAM Permissions for AgentCore Runtime for more details.\nThe following code is for the AgentCore Runtime trust policy:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"AssumeRolePolicy\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": \"bedrock-agentcore.amazonaws.com\"\n},\n\"Action\": \"sts:AssumeRole\",\n\"Condition\": {\n\"StringEquals\": {\n\"aws:SourceAccount\": \"accountId\"\n},\n\"ArnLike\": {\n\"aws:SourceArn\": \"arn:aws:bedrock-agentcore:region:accountId:*\"\n}\n}\n}\n]\n}\n\nThe following code is for the AgentCore Runtime execution role:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"bedrock:InvokeModel\",\n\"bedrock:InvokeModelWithResponseStream\",\n\"bedrock:Converse\",\n\"bedrock:ConverseStream\"\n],\n\"Resource\": [\n\"arn:aws:bedrock:*::foundation-model/us.anthropic.claude-sonnet-4-*\",\n\"arn:aws:bedrock:*::foundation-model/anthropic.claude-*\",\n\"arn:aws:bedrock:*:*:inference-profile/us.anthropic.claude-sonnet-4-*\",\n\"arn:aws:bedrock:*:*:inference-profile/anthropic.claude-*\"\n]\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"ecr:GetAuthorizationToken\",\n\"ecr:BatchCheckLayerAvailability\",\n\"ecr:GetDownloadUrlForLayer\",\n\"ecr:BatchGetImage\"\n],\n\"Resource\": \"arn:aws:ecr:::repository/bedrock-agentcore-*\"\n},\n{\n\"Effect\": \"Allow\",\n\"Action\": [\n\"logs:CreateLogGroup\",\n\"logs:CreateLogStream\",\n\"logs:PutLogEvents\"\n],\n\"Resource\": \"arn:aws:logs:*:*:*\"\n}\n]\n}\n\nCreate the GitHub Actions workflow\nRefer the CI/CD workflow file at .github/workflows/deploy-agentcore.yml for details to create the workflow.The following steps will be performed by the workflow:\n\nIt uses the default Ubuntu Github Runner for the task provided in the pipeline.\nThe workflow installs the required dependencies mentioned in the requirement.txt file.\nIt builds the Docker image and deploys it on the ECR repository.\nThe image is scanned with Amazon Inspector to identify potential vulnerabilities.\nAgentCore Runtime deploys the agent as an endpoint.\nThe workflow tests the agent endpoint to verify functionality.\n\nTrigger and monitor pipeline\nThis pipeline can be triggered either by changing a code in the agents folder or manually using the workflow dispatch option. This might further change according to your organization’s branching strategy. Update the code in .github/workflows/deploy-agentcore.yml to change this trigger behavior.\n\nTest agent\nAfter the agent is deployed, we will verify its functionality by triggering the Test Agent workflow manually via workflow dispatch option.\n\nAgentCore Runtime versioning and endpoints\nAmazon Bedrock AgentCore implements automatic versioning for AgentCore Runtime and lets you manage different configurations using endpoints. Endpoints provide a way to reference specific versions of AgentCore Runtime. For more details and sample code, see AgentCore Runtime versioning and endpoints .\nClean up\nTo avoid incurring future charges, complete the following steps:\n\nDelete the ECR images from the Amazon ECR console created through the deployment using GitHub Actions.\nDelete the agent deployed in AgentCore Runtime.\n\nConclusion\nIn this post, we demonstrated a comprehensive approach to using GitHub Actions for a more secure and scalable deployment of AI agents on AgentCore Runtime. Our solution provides a robust, automated, and controlled environment for generative AI applications, addressing critical enterprise deployment challenges by automating dependency management, implementing continuous code quality checks, performing comprehensive vulnerability scanning, and facilitating consistent deployment processes. By abstracting infrastructure complexities, this pipeline helps developers focus on agent logic and functionality, while providing a framework-agnostic approach that supports seamless management of multiple AI agents at scale. As AI agents continue to transform enterprise capabilities, this solution represents a significant step towards streamlining AI agent development and operational management, offering a standardized, secure, and efficient deployment mechanism for modern generative AI applications.\nAs a next step, you can use Amazon Q to intelligently enhance and customize your AI agent deployment pipeline, transforming your CI/CD processes with advanced, context-aware automation.\n\nAbout the authors\nPrafful Gupta  is an Assoc. Delivery Consultant at AWS based in Gurugram, India. Having started his professional journey with Amazon a year ago, he specializes in DevOps and Generative AI solutions, helping customers navigate their cloud transformation journeys. Beyond work, he enjoys networking with fellow professionals and spending quality time with family. Connect on LinkedIn at:  linkedin.com/in/praffulgupta11/\nAnshu Bathla is a Lead Consultant – SRC at AWS, based in Gurugram, India. He works with customers across diverse verticals to help strengthen their security infrastructure and achieve their security goals. Outside of work, Anshu enjoys reading books and gardening at his home garden. Connect on LinkedIn at: linkedin.com/in/anshu-bathla/",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊 AWS 发布了 Amazon Bedrock AgentCore Runtime，这是一个灵活的服务器托管服务，帮助开发者跨不同框架和模型创建和管理 AI 代理。新闻中详细演示了如何使用 GitHub Actions 工作流自动化部署 AI 代理到 AgentCore Runtime，构建了一个可扩展的 CI/CD 流水线。该方案集成了企业级安全控制，包括 OpenID Connect 认证、最小权限访问和环境分离，并支持自动安全扫描和代码质量检查。AgentCore Runtime 是框架无关的，兼容 LangGraph、Strands 和 CrewAI 等流行框架，并能与 Amazon Bedrock 和 Anthropic Claude 等大语言模型集成。这个解决方案简化了 AI 代理的部署流程，降低了运营复杂性，提高了安全性，加速了企业环境中生成式 AI 应用的开发，为开发者提供了标准化、高效的部署机制。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon Bedrock AgentCore Runtime",
        "GitHub Actions",
        "CI/CD",
        "AI agents",
        "Amazon ECR"
      ]
    },
    "analyzed_at": "2026-01-17T03:09:10.725419Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "c85f32cab7bd923f",
    "title": "The truth left out from Elon Musk’s recent court filing",
    "url": "https://openai.com/index/the-truth-elon-left-out",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T12:00:00Z",
    "summary": "The truth left out from Elon Musk’s recent court filing.",
    "content": "The truth left out from Elon Musk’s recent court filing.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "该科技资讯的标题和正文均指出，Elon Musk 在最近的法庭文件中遗漏了真相，来源为 OpenAI。内容简短，没有提供具体事件细节或背景信息，因此无法确定遗漏的真相内容、涉及的具体技术或法律问题。从文本看，这可能与 Elon Musk 和 OpenAI 之间的关联有关，但没有说明原因或影响。核心信息点仅限于标题陈述，缺乏进一步的技术、行业或产品更新细节。",
      "category": "其他",
      "sentiment": "neutral",
      "keywords": [
        "Elon Musk",
        "OpenAI",
        "court filing"
      ]
    },
    "analyzed_at": "2026-01-17T03:08:50.211000Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "9de6d84c57af7165",
    "title": "Introducing ChatGPT Go, now available worldwide",
    "url": "https://openai.com/index/introducing-chatgpt-go",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T00:00:00Z",
    "summary": "ChatGPT Go is now available worldwide, offering expanded access to GPT-5.2 Instant, higher usage limits, and longer memory—making advanced AI more affordable globally.",
    "content": "ChatGPT Go is now available worldwide, offering expanded access to GPT-5.2 Instant, higher usage limits, and longer memory—making advanced AI more affordable globally.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI发布了ChatGPT Go，现已全球推出。该产品提供对GPT-5.2 Instant模型的扩展访问，具有更高的使用限制和更长的记忆功能，使高级AI技术更经济实惠，易于全球用户访问。此举降低了AI技术门槛，促进普及化，为用户提供更强工具，可能推动创新和应用扩展。通过优化资源和服务范围，ChatGPT Go旨在让更多用户负担得起AI能力，提升交互体验和效率。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "ChatGPT Go",
        "GPT-5.2 Instant",
        "使用限制",
        "记忆功能"
      ]
    },
    "analyzed_at": "2026-01-17T03:08:08.914010Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "d8bbfb0ca8738fde",
    "title": "Our approach to advertising and expanding access to ChatGPT",
    "url": "https://openai.com/index/our-approach-to-advertising-and-expanding-access",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-16T00:00:00Z",
    "summary": "OpenAI plans to test advertising in the U.S. for ChatGPT’s free and Go tiers to expand affordable access to AI worldwide, while protecting privacy, trust, and answer quality.",
    "content": "OpenAI plans to test advertising in the U.S. for ChatGPT’s free and Go tiers to expand affordable access to AI worldwide, while protecting privacy, trust, and answer quality.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI宣布计划在美国测试ChatGPT免费版和Go版本的广告，以扩展全球范围内负担得起的AI访问。此举旨在通过广告商业化来支持服务运营，降低用户使用成本，从而让更多人能够使用ChatGPT。同时，OpenAI承诺保护用户隐私、维持回答质量和建立信任，确保广告引入不影响核心体验。这一策略可能推动AI技术的普及，为OpenAI的商业模式提供新途径，并可能影响整个AI行业对普惠性和商业化的平衡考虑。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "ChatGPT",
        "广告",
        "AI访问",
        "隐私保护"
      ]
    },
    "analyzed_at": "2026-01-17T03:08:00.596954Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]