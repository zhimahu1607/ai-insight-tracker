[
  {
    "id": "07f6101b3bd38826",
    "title": "Structured outputs on Amazon Bedrock: Schema-compliant AI responses",
    "url": "https://aws.amazon.com/blogs/machine-learning/structured-outputs-on-amazon-bedrock-schema-compliant-ai-responses/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-06T20:12:14Z",
    "summary": "Today, we're announcing structured outputs on Amazon Bedrockâ€”a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance. In this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanismsâ€”JSON Schema output format and strict tool useâ€”along with implementation details, best practices, and practical code examples.",
    "content": "Today, weâ€™re announcing structured outputs on Amazon Bedrock â€”a capability that fundamentally transforms how you can obtain validated JSON responses from foundation models through constrained decoding for schema compliance.\nThis represents a paradigm shift in AI application development. Instead of validating JSON responses and writing fallback logic for when they fail, you can move straight to building with the data. With structured outputs, you can build zero-validation data pipelines that trust model outputs, reliable agentic systems that confidently call external functions, and simplified application architectures without retry logic.\nIn this post, we explore the challenges of traditional JSON generation and how structured outputs solves them. We cover the two core mechanismsâ€”JSON Schema output format and strict tool useâ€”along with implementation details, best practices, and practical code examples. Whether youâ€™re building data extraction pipelines, agentic workflows, or AI-powered APIs, youâ€™ll learn how to use structured outputs to create reliable, production-ready applications. Our companionÂ  Jupyter notebook Â provides hands-on examples for every feature covered here.\nThe problem with traditional JSON generation\nFor years, getting structured data from language models meant crafting detailed prompts, hoping for the best, and building elaborate error-handling systems. Even with careful prompting, developers routinely encounter:\n\nParsing failures : Invalid JSON syntax that breaksÂ  json.loads() Â calls\nMissing fields : Required data points absent from responses\nType mismatches : Strings where integers are expected, breaking downstream processing\nSchema violations : Responses that technically parse but donâ€™t match your data model\n\nIn production systems, these failures compound. A single malformed response can cascade through your pipeline, requiring retries that increase latency and costs. For agentic workflows where models call tools, invalid parameters can break function calls entirely.\nConsider a booking system requiringÂ  passengers: int . Without schema enforcement, the model might returnÂ  passengers: \"two\" Â orÂ  passengers: \"2\" â€”syntactically valid JSON, but semantically wrong for your function signature.\nWhat changes with structured outputs\nStructured outputs on Amazon Bedrock isnâ€™t incremental improvementâ€”itâ€™s a fundamental shift from probabilistic to deterministic output formatting. Through constrained decoding, Amazon Bedrock constrains model responses to conform to your specified JSON schema. Two complementary mechanisms are available:\n\nFeature\nPurpose\nUse case\n\nJSON Schema output format\nControl the modelâ€™s response format\nData extraction, report generation, API responses\n\nStrict tool use\nValidate tool parameters\nAgentic workflows, function calling, multi-step automation\n\nThese features can be used independently or together, giving you precise control over both what the model outputs and how it calls your functions.\nWhat structured outputs delivers:\n\nAlways valid : No moreÂ  JSON.parse() Â errors or parsing exceptions\nType safe : Field types are enforced and required fields are always present\nReliable : No retries needed for schema violations\nProduction ready : Deploy with confidence at enterprise scale\n\nHow structured outputs works\nStructured outputs uses constrained sampling with compiled grammar artifacts. Hereâ€™s what happens when you make a request:\n\nSchema validation : Amazon Bedrock validates your JSON schema against the supported JSON Schema Draft 2020-12 subset\nGrammar compilation : For new schemas, Amazon Bedrock compiles a grammar (first request might take longer)\nCaching : Compiled grammars are cached for 24 hours, making subsequent requests faster\nConstrained generation : The model generates tokens that produce valid JSON matching your schema\n\nPerformance considerations:\n\nFirst request latency : Initial compilation might add latency to new schemas\nCached performance : Subsequent requests with identical schemas have minimal overhead\nCache scope : Grammars are cached per account for 24 hours from first access\n\nChanging the JSON schema structure or a toolâ€™s input schema invalidates the cache, but changing onlyÂ  name Â orÂ  description Â fields does not.\nGetting started with structured outputs\nThe following example demonstrates structured outputs with the Converse API:\n\nimport boto3\nimport json\n# Initialize the Bedrock Runtime client\nbedrock_runtime = boto3.client(\nservice_name='bedrock-runtime',\nregion_name='us-east-1' # Choose your preferred region\n)\n# Define your JSON schema\nextraction_schema = {\n\"type\": \"object\",\n\"properties\": {\n\"name\": {\"type\": \"string\", \"description\": \"Customer name\"},\n\"email\": {\"type\": \"string\", \"description\": \"Customer email address\"},\n\"plan_interest\": {\"type\": \"string\", \"description\": \"Product plan of interest\"},\n\"demo_requested\": {\"type\": \"boolean\", \"description\": \"Whether a demo was requested\"}\n},\n\"required\": [\"name\", \"email\", \"plan_interest\", \"demo_requested\"],\n\"additionalProperties\": False\n}\n# Make the request with structured outputs\nresponse = bedrock_runtime.converse(\nmodelId=\"us.anthropic.claude-opus-4-5-20251101-v1:0\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"text\": \"Extract the key information from this email: John Smith (john@example.com) is interested in our Enterprise plan and wants to schedule a demo for next Tuesday at 2pm.\"\n}\n]\n}\n],\ninferenceConfig={\n\"maxTokens\": 1024\n},\noutputConfig={\n\"textFormat\": {\n\"type\": \"json_schema\",\n\"structure\": {\n\"jsonSchema\": {\n\"schema\": json.dumps(extraction_schema),\n\"name\": \"lead_extraction\",\n\"description\": \"Extract lead information from customer emails\"\n}\n}\n}\n}\n)\n# Parse the schema-compliant JSON response\nresult = json.loads(response[\"output\"][\"message\"][\"content\"][0][\"text\"])\nprint(json.dumps(result, indent=2))\n\nOutput:\n\n{\n\"name\": \"John Smith\",\n\"email\": \"john@example.com\",\n\"plan_interest\": \"Enterprise\",\n\"demo_requested\": true\n}\n\nThe response conforms to your schemaâ€”no additional validation required.\nRequirements and best practices\nTo use structured outputs effectively, follow these guidelines:\n\nSetÂ  additionalProperties: false Â on all objects. Â This is required for structured outputs to work. Without it, your schema wonâ€™t be accepted.\n\n{\n\"type\": \"object\",\n\"properties\": {\n\"name\": {\"type\": \"string\"}\n},\n\"required\": [\"name\"],\n\"additionalProperties\": false\n}\n\nUse descriptive field names and descriptions. Â Models use property names and descriptions to understand what data to extract. Clear names likeÂ  customer_email Â outperform generic names likeÂ  field1 .\nUseÂ  enum Â for constrained values. Â When a field has a limited set of valid values, useÂ  enum Â to constrain options. This improves accuracy and produces valid values.\nStart basic, then add complexity. Â Begin with the minimum required fields and add complexity incrementally. Basic schemas compile faster and are easier to maintain.\nReuse schemas to benefit from caching. Â Structure your application to reuse schemas across requests. The 24-hour grammar cache significantly improves performance for repeated queries.\nCheckÂ  stopReason Â in every response. Â Two scenarios can produce non-conforming responses: refusals (when the model declines for safety reasons) and token limits (whenÂ  max_tokens Â is reached before completing). Handle both cases in your code.\nTest with realistic data before deployment. Â Validate your schemas against production-representative inputs. Edge cases in real data often reveal schema design issues.\n\nSupported JSON Schema features:\n\nAll basic types:Â  object ,Â  array ,Â  string ,Â  integer ,Â  number ,Â  boolean ,Â  null\nenum Â (strings, numbers, bools, or nulls only)\nconst ,Â  anyOf ,Â  allOf Â (with limitations)\n$ref ,Â  $def , andÂ  definitions Â (internal references only)\nString formats:Â  date-time ,Â  time ,Â  date ,Â  duration ,Â  email ,Â  hostname ,Â  uri ,Â  ipv4 ,Â  ipv6 ,Â  uuid\nArrayÂ  minItems Â (only values 0 and 1)\n\nNot supported:\n\nRecursive schemas\nExternalÂ  $ref Â references\nNumerical constraints ( minimum ,Â  maximum ,Â  multipleOf )\nString constraints ( minLength ,Â  maxLength )\nadditionalProperties Â set to anything other thanÂ  false\n\nStrict tool use for agentic workflows\nWhen building applications where models call tools, setÂ  strict: true Â in your tool definition to constrain tool parameters to match your input schema exactly:\n\nimport boto3\nimport json\nbedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\nresponse = bedrock_runtime.converse(\nmodelId=\"us.anthropic.claude-opus-4-5-20251101-v1:0\",\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [{\"text\": \"What's the weather like in San Francisco?\"}]\n}\n],\ninferenceConfig={\"maxTokens\": 1024},\ntoolConfig={\n\"tools\": [\n{\n\"toolSpec\": {\n\"name\": \"get_weather\",\n\"description\": \"Get the current weather for a specified location\",\n\"strict\": True, # Enable strict mode\n\"inputSchema\": {\n\"json\": {\n\"type\": \"object\",\n\"properties\": {\n\"location\": {\n\"type\": \"string\",\n\"description\": \"The city and state, e.g., San Francisco, CA\"\n},\n\"unit\": {\n\"type\": \"string\",\n\"enum\": [\"celsius\", \"fahrenheit\"],\n\"description\": \"Temperature unit\"\n}\n},\n\"required\": [\"location\", \"unit\"],\n\"additionalProperties\": False\n}\n}\n}\n}\n]\n}\n)\n# Tool inputs conform to the schema\nfor content_block in response[\"output\"][\"message\"][\"content\"]:\nif \"toolUse\" in content_block:\ntool_input = content_block[\"toolUse\"][\"input\"]\nprint(f\"Tool: {content_block['toolUse']['name']}\")\nprint(f\"Input: {json.dumps(tool_input, indent=2)}\")\n\nWithÂ  strict: true , structured outputs constrains the output so that:\n\nTheÂ  location Â field is always a string\nTheÂ  unit Â field is always either celsius or fahrenheit\nNo unexpected fields appear in the input\n\nPractical applications across industries\nThe notebook demonstrates use cases that span industries:\n\nFinancial services : Extract structured data from earnings reports, loan applications, and compliance documents. With structured outputs, every required field is present and correctly typed for downstream processing.\nHealthcare : Parse clinical notes into structured, schema-compliant records. Extract patient information, diagnoses, and treatment plans into validated JSON for EHR integration.\nEcommerce : Build reliable product catalog enrichment pipelines. Extract specifications, categories, and attributes from product descriptions with consistent, reliable results.\nLegal : Analyze contracts and extract key terms, parties, dates, and obligations into structured formats suitable for contract management systems.\nCustomer service : Build intelligent ticket routing and response systems where extracted intents, sentiments, and entities match your applicationâ€™s data model.\n\nChoosing the right approach\nOur testing revealed clear patterns for when to use each feature:\nUse JSON Schema output format when:\n\nYou need the modelâ€™s response in a specific structure\nBuilding data extraction pipelines\nGenerating API-ready responses\nCreating structured reports or summaries\n\nUse strict tool use when:\n\nBuilding agentic systems that call external functions\nImplementing multi-step workflows with tool chains\nRequiring validated parameter types for function calls\nConnecting AI to databases, APIs, or external services\n\nUse both together when:\n\nBuilding complex agents that need validated tool calls and structured final responses\nCreating systems where intermediate tool results feed into structured outputs\nImplementing enterprise workflows requiring end-to-end schema compliance\n\nAPI comparison: Converse compared to InvokeModel\nBoth the Converse API and InvokeModel API support structured outputs, with slightly different parameter formats:\n\nAspect\nConverse API\nInvokeModel (Anthropic Claude)\nInvokeModel (open-weight models)\n\nSchema location\noutputConfig.textFormat\noutput_config.format\nresponse_format\n\nTool strict flag\ntoolSpec.strict\ntools[].strict\ntools[].function.strict\n\nSchema format\nJSON string inÂ  jsonSchema.schema\nJSON object inÂ  schema\nJSON object inÂ  json_schema.schema\n\nBest for\nConversational workflows\nSingle-turn inference (Claude)\nSingle-turn inference (open-weight)\n\nNote : The InvokeModel API uses different request field names depending on the model type. For Anthropic Claude models, useÂ  output_config.format Â for JSON schema outputs. For open-weight models, useÂ  response_format Â instead.\nChoose the Converse API for multi-turn conversations and the InvokeModel API when you need direct model access with provider-specific request formats.\nSupported models and availability\nStructured outputs is generally available in all commercial AWS Regions for select Amazon Bedrock model providers:\n\nAnthropic\nDeepSeek\nGoogle\nMiniMax\nMistral AI\nMoonshot AI\nNVIDIA\nOpenAI\nQwen\n\nThe feature works seamlessly with:\n\nCross-Region inference : Use structured outputs across AWS Regions without additional setup\nBatch inference : Process large volumes with schema-compliant outputs\nStreaming : Stream structured responses with ConverseStream or InvokeModelWithResponseStream\n\nConclusion\nIn this post, you discovered how structured outputs on Amazon Bedrock reduce the uncertainty of AI-generated JSON through validated, schema-compliant responses. By using JSON Schema output format and strict tool use, you can build reliable data extraction pipelines, robust agentic workflows, and production-ready AI applicationsâ€”without custom parsing or validation logic.Whether youâ€™re extracting data from documents, building intelligent automation, or creating AI-powered APIs, structured outputs deliver the reliability your applications demand.\nStructured outputs is now generally available on Amazon Bedrock. To use structured outputs with the Converse APIs, update to the latest AWS SDK. To learn more, see theÂ  Amazon Bedrock documentation Â and explore ourÂ  sample notebook .\nWhat workflows could validated, schema-compliant JSON unlock in your organization? The notebook provides everything you need to find out.\n\nAbout the authors\n\nJeffrey Zeng\nJeffrey ZengÂ is a Worldwide Specialist Solutions Architect for Generative AI at AWS, leading third-party models on Amazon Bedrock. He focuses on agentic coding and workflows, with hands-on experience helping customers build and deploy AI solutions from proof-of-concept to production.\n\nJonathan Evans\nJonathan Evans is a Worldwide Solutions Architect for Generative AI at AWS, where he helps customers leverage cutting-edge AI technologies with Anthropic Claude models on Amazon Bedrock, to solve complex business challenges. With a background in AI/ML engineering and hands-on experience supporting machine learning workflows in the cloud, Jonathan is passionate about making advanced AI accessible and impactful for organizations of all sizes.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "ac75664d9ded661d",
    "title": "Manage Amazon SageMaker HyperPod clusters using the HyperPod CLI and SDK",
    "url": "https://aws.amazon.com/blogs/machine-learning/manage-amazon-sagemaker-hyperpod-clusters-using-the-hyperpod-cli-and-sdk/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-06T19:27:45Z",
    "summary": "In this post, we demonstrate how to use the CLI and the SDK to create and manage SageMaker HyperPod clusters in your AWS account. We walk through a practical example and dive deeper into the user workflow and parameter choices.",
    "content": "Training and deploying large AI models requires advanced distributed computing capabilities, but managing these distributed systems shouldnâ€™t be complex for data scientists and machine learning (ML) practitioners. The command line interface (CLI) and software development kit (SDK) for Amazon SageMaker HyperPod Â with Amazon Elastic Kubernetes Service (Amazon EKS) orchestration simplify how you manage cluster infrastructure andÂ use the serviceâ€™s distributed training and inference capabilities.\nThe SageMaker HyperPod CLI provides data scientists with an intuitive command-line experience, abstracting away the underlying complexity of distributed systems. Built on top of the SageMaker HyperPod SDK, the CLI offers straightforward commands for managing HyperPod clusters and common workflows like launching training or fine-tuning jobs, deploying inference endpoints, and monitoring cluster performance. This makes it ideal for quick experimentation and iteration.\nA layered architecture for simplicity\nThe HyperPod CLI and SDK follow a multi-layered, shared architecture. The CLI and the Python module serve as user-facing entry points and are both built on top of common SDK components to provide consistent behavior across interfaces. For infrastructure automation, the SDK orchestrates cluster lifecycle management through a combination of AWS CloudFormation stack provisioning and direct AWS API interactions. Training and inference workloads and integrated development environments (IDEs) (Spaces) are expressed as Kubernetes Custom Resource Definitions (CRDs), which the SDK manages through the Kubernetes API.\nIn this post, we demonstrate how to use the CLI and the SDK to create and manage SageMaker HyperPod clusters in your AWS account. We walk through a practical example and dive deeper into the user workflow and parameter choices.\nThis post focuses on cluster creation and management. For a deep dive into using the HyperPod CLI and SDK to submit training jobs and deploy inference endpoints, see our companion post: Train and deploy models on Amazon SageMaker HyperPod using the new HyperPod CLI and SDK .\nPrerequisites\nTo follow the examples in this post, you must have the following prerequisites:\n\nAn AWS account with access to SageMaker HyperPod, Amazon Simple Storage Service (Amazon S3) and Amazon FSx for Lustre .\nSufficient service quota for creating the HyperPod cluster and instance groups.\nA local environment (either your local machine or a cloud-based compute environment) from which to run the SageMaker HyperPod CLI commands, configured as follows:\n\nOperating system based on Linux or MacOS.\nPython 3.8 or later installed.\nThe AWS Command Line Interface (AWS CLI) configured with the appropriate credentials to use the aforementioned services.\n\nInstall the SageMaker HyperPod CLI\nFirst, install the latest version of the SageMaker HyperPod CLI and SDK. The examples in this post are based on version 3.5.0. From your local environment, run the following command, you can alternatively install the CLI in a Python virtual environment:\n\n# Install the HyperPod CLI and SDK\npip install sagemaker-hyperpod\n\nThis command sets up the tools needed to interact with SageMaker HyperPod clusters. For an existing installation, make sure you have the latest version of the package installed (SageMaker HyperPod 3.5.0 or later) to be able to use the relevant set of features described in this post. To verify if the CLI is installed correctly, run the hyp command and check the outputs:\n\n# Check if the HyperPod CLI is correctly installed\nhyp\n\nThe output will be similar to the following, and includes instructions on how to use the CLI:\n\nUsage: hyp [OPTIONS] COMMAND [ARGS]...\n\nOptions:\nÂ Â --version Â Show version information\nÂ Â --help Â  Â  Show this message and exit.\n\nCommands:\nÂ Â configure Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Update any subset of fields in ./config.yaml by passing --<field> flags.\nÂ Â create Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Create endpoints, pytorch jobs, cluster stacks, space, space access or space admin config.\nÂ Â delete Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Delete endpoints, pytorch jobs, space, space access or space template.\nÂ Â describe Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Describe endpoints, pytorch jobs or cluster stacks, spaces or space template.\nÂ Â exec Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Execute commands in pods for endpoints or pytorch jobs.\nÂ Â get-cluster-context Â  Â  Â  Â  Â  Â  Get context related to the current set cluster.\nÂ Â get-logs Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Get pod logs for endpoints, pytorch jobs or spaces.\nÂ Â get-monitoring Â  Â  Â  Â  Â  Â  Â  Â  Â Get monitoring configurations for Hyperpod cluster.\nÂ Â get-operator-logs Â  Â  Â  Â  Â  Â  Â  Get operator logs for endpoints.\nÂ Â init Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Initialize a TEMPLATE scaffold in DIRECTORY.\nÂ Â invoke Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Invoke model endpoints.\nÂ Â list Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â List endpoints, pytorch jobs, cluster stacks, spaces, and space templates.\nÂ Â list-accelerator-partition-type\nÂ Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â List available accelerator partition types for an instance type.\nÂ Â list-cluster Â  Â  Â  Â  Â  Â  Â  Â  Â  Â List SageMaker Hyperpod Clusters with metadata.\nÂ Â list-pods Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  List pods for endpoints or pytorch jobs.\nÂ Â reset Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Reset the current directory's config.yaml to an \"empty\" scaffold: all schema keys set to default values (but keeping the...\nÂ Â set-cluster-context Â  Â  Â  Â  Â  Â  Connect to a HyperPod EKS cluster.\nÂ Â start Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Start space resources.\nÂ Â stop Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Stop space resources.\nÂ Â update Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Update an existing HyperPod cluster configuration, space, or space template.\nÂ Â validate Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Validate this directory's config.yaml against the appropriate schema.\n\nFor more information on CLI usage and the available commands and respective parameters, see the CLI reference documentation .\nThe HyperPod CLI provides commands to manage the full lifecycle of HyperPod clusters. The following sections explain how to create new clusters, monitor their creation, modify instance groups, and delete clusters.\nCreating a new HyperPod cluster\nHyperPod clusters can be created through the AWS Management Console or the HyperPod CLI, both of which provide streamlined experiences for cluster creation. The console offers the easiest and most guided approach, while the CLI is especially useful for customers who prefer a programmatic experienceâ€”for example, to enable reproducibility or to build automation around cluster creation. Both methods use the same underlying CloudFormation template, which is available in the SageMaker HyperPod cluster setup GitHub repository . For a walkthrough of the console-based experience, see the cluster creation experience blog post .\nCreating a new cluster through the HyperPod CLI follows a configuration-based workflow: the CLI first generates configuration files, which are then edited to match the intended cluster specifications. These files are subsequently submitted as a CloudFormation stack that creates the HyperPod cluster along with the required resources, such as a VPC and FSx for Lustre filesystem, among others.To initialize a new cluster configuration by running the following command: hyp init cluster-stack\nThis initializes a new cluster configuration in the current directory and generates a config.yaml file that you can use to specify the configuration of the cluster stack. Additionally it will create a README.md with information about the functionality and workflow in addition to a template for the CloudFormation stack parameters inÂ  cfn_params.jinja .\n\n(base)Â xxxxxxxx@3c06303f9abb hyperpod % hyp init cluster-stack\nInitializing new scaffold for 'cluster-stack'â€¦\nâœ” cluster-stack for schema version='1.0' is initialized in .\nğŸš€ Welcome!\nğŸ“˜ See ./README.md for usage.\n\nThe cluster stackâ€™s configuration variables are defined in config.yaml . The following is an excerpt from the file:\n\n...\n# Prefix to be used for all resources. A 4-digit UUID will be added to prefix during submission\nresource_name_prefix: hyp-eks-stack\n# Boolean to Create HyperPod Cluster Stack\ncreate_hyperpod_cluster_stack: True\n# Name of SageMaker HyperPod Cluster\nhyperpod_cluster_name: hyperpod-cluster\n# Boolean to Create EKS Cluster Stack\ncreate_eks_cluster_stack: True\n# The Kubernetes version\nkubernetes_version: 1.31\n...\n\nThe resource_name_prefix parameter serves as the primary identifier for the AWS resources created during deployment. Each deployment must use a unique resource name prefix to avoid conflicts. The value of the prefix parameter is automatically appended with a unique identifier during cluster creation to provide resource uniqueness.\nThe configuration can be edited either directly by opening config.yaml in an editor of your choice or by running the hyp configure command. The following example shows how to specify the Kubernetes version of the Amazon EKS cluster that will be created by the stack:\nhyp configure --kubernetes-version 1.33\nUpdating variables through the CLI commands provides added security by performing validation against the defined schema before setting the value in config.yaml .\nBesides the Kubernetes version and the resource name prefix, some examples of significant parameters are listed below:\n\n# List of string containing instance group configurations\ninstance_group_settings:\nÂ Â - {'InstanceCount': 1, 'InstanceGroupName': 'default', 'InstanceType': 'ml.t3.medium', 'TargetAvailabilityZoneId': 'use2-az2', 'ThreadsPerCore': 1, 'InstanceStorageConfigs': [{'EbsVolumeConfig': {'VolumeSizeInGB': 500}}]}\n\n# Boolean to Create EKS Cluster Stack\ncreate_eks_cluster_stack: True\n\n# The name of the S3 bucket used to store the cluster lifecycle scripts\ns3_bucket_name: amzn-s3-demo-bucket\n\n# Storage capacity for the FSx file system in GiB\nstorage_capacity: 1200\n\nThere are two important nuances when updating the configuration values through hyp configure Â commands:\n\nUnderscores ( _ ) in variable names within config.yaml Â become hyphens ( - ) in the CLI commands. Thus kubernetes_version Â in config.yaml Â is configured via hyp configure --kubernetes-version Â in the CLI.\nVariables that contain lists of entries within config.yaml Â are configured as JSON lists in the CLI command. For example, multiple instance groups are configured within config.yaml Â as the following:\n\ninstance_group_settings:\nÂ Â - {'InstanceCount': 1, 'InstanceGroupName': 'default', 'InstanceType': 'ml.t3.medium', 'TargetAvailabilityZoneId': 'use2-az2', 'ThreadsPerCore': 1, 'InstanceStorageConfigs': [{'EbsVolumeConfig': {'VolumeSizeInGB': 500}}]}\nÂ Â - {'InstanceCount': 2, 'InstanceGroupName': 'worker', 'InstanceType': 'ml.t3.large', 'TargetAvailabilityZoneId': 'use2-az2', 'ThreadsPerCore': 1, 'InstanceStorageConfigs': [{'EbsVolumeConfig': {'VolumeSizeInGB': 1000}}]}\n\nWhich translates to the following CLI command:\n\nhyp configure â€”instance-group-settings \"[{'InstanceCount': 1, 'InstanceGroupName': 'default', 'InstanceType': 'ml.t3.medium', 'TargetAvailabilityZoneId': 'use2-az2', 'ThreadsPerCore': 1, 'InstanceStorageConfigs': [{'EbsVolumeConfig': {'VolumeSizeInGB': 500}}]}, {'InstanceCount': 2, 'InstanceGroupName': 'worker', 'InstanceType': 'ml.t3.large', 'TargetAvailabilityZoneId': 'use2-az2', 'ThreadsPerCore': 1, 'InstanceStorageConfigs': [{'EbsVolumeConfig': {'VolumeSizeInGB': 1000}}]}]\"\n\nAfter youâ€™re done making the desired changes, validate your configuration file by running the following command: hyp validate\nThis will validate the parameters in config.yaml Â against the defined schema. If successful, the CLI will output the following:\n\n(base)Â xxxxxxxx@3c06303f9abb hyperpod % hyp validate\nâœ” Â config.yaml is valid!\n\nThe cluster creation stack can be submitted to CloudFormation by running the following command: hyp create --region <region>\nThe hyp create command performs validation and injects values from config.yaml into the cfn_params.jinja template. If no AWS Region is explicitly provided, the command uses the default Region from your AWS credentials configuration. The resolved configuration file and CloudFormation template values are saved to a timestamped subdirectory under the ./run/ directory, providing a lightweight local versioning mechanism to track which configuration was used to create a cluster at a given point in time. You can also choose to commit these artifacts to your version control system to improve reproducibility and auditability. If successful, the command outputs the CloudFormation stack ID:\n\n(base)Â xxxxxxxx@3c06303f9abb dev % hyp create\nâœ” config.yaml is valid!\nâœ” Submitted! Files written to run/20251118T101501\nSubmitting to default region: us-east-1.\nStack creation initiated. Stack ID: arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351/5b83ed40-c491-11f0-a31f-1234073395a1\n\nMonitoring the HyperPod cluster creation process\nYou can list the existing CloudFormation stacks by running the following command: hyp list cluster-stack --region <region>\nYou can optionally filter the output by stack status by adding the following flag:Â  --status \"['CREATE_COMPLETE', 'UPDATE_COMPLETE']\" .\nThe output of this command will look similar to the following:\n\n(base)Â xxxxxxxx@3c06303f9abb dev % hyp list cluster-stack\nğŸ“‹ HyperPod Cluster Stacks (94 found)\n\n[1] Stack Details:\nField | Value\n---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------\nStackId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8/e2898250-c491-11f0-bf25-0afff7e082cf\nStackName | HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8\nTemplateDescription | S3 Endpoint Stack\nÂ CreationTime | :18:50\nStackStatus | CREATE_COMPLETE\nParentId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351/5b83ed40-c491-11f0-a31f-1234073395a1\nRootId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351/5b83ed40-c491-11f0-a31f-1234073395a1\nDriftInformation | {'StackDriftStatus': 'NOT_CHECKED'}\n\nDepending on the configuration in config.yaml , several nested stacks are created that cover different aspects of the HyperPod cluster setup such as the EKSClusterStack , FsxStack Â and the VPCStack .\nYou can use the describe command to view details about any of the individual stacks: hyp describe cluster-stack <stack-name> --region <region>\nThe output for an exemplary substack,Â  S3EndpointStack , will look like the following:\n\n(base)Â xxxxxxxx@3c06303f9abb dev % hyp describe cluster-stack HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8\nğŸ“‹ Stack Details for: HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8\nStatus: CREATE_COMPLETE\nField | Value\n-----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------\nStackId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8/e2898250-c491-11f0-bf25-0afff7e082cf\nStackName | HyperpodClusterStack-d5351-S3EndpointStack-10JBD25F965A8\nDescription | S3 Endpoint Stack\nParameters | [\n| {\n| \"ParameterKey\": \"ResourceNamePrefix\",\n| \"ParameterValue\": \"hyp-eks-demo-stack\"\n| },\n| {\n| \"ParameterKey\": \"VpcId\",\n| \"ParameterValue\": \"vpc-XXXXXXXXXXXXXX\"\n| },\n| {\n| \"ParameterKey\": \"EksPrivateRouteTableIds\",\n| \"ParameterValue\": \"rtb-XXXXXXXXXXXXXX,rtb-XXXXXXXXXXXXXX\"\n| },\n| {\n| \"ParameterKey\": \"PrivateRouteTableIds\",\n| \"ParameterValue\": \"rtb-XXXXXXXXXXXXXX,rtb-XXXXXXXXXXXXXX\"\n| }\n| ]\nÂ CreationTime | :18:50.007000+00:00\nRollbackConfiguration | {}\nStackStatus | CREATE_COMPLETE\nDisableRollback | True\nNotificationARNs | []\nCapabilities | [\n| \"CAPABILITY_AUTO_EXPAND\",\n| \"CAPABILITY_IAM\",\n| \"CAPABILITY_NAMED_IAM\"\n| ]\nTags | []\nEnableTerminationProtection | False\nParentId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351/5b83ed40-c491-11f0-a31f-1234073395a1\nRootId | arn:aws:cloudformation:us-east-1:xxxxxxxxxxx:stack/HyperpodClusterStack-d5351/5b83ed40-c491-11f0-a31f-1234073395a1\nDriftInformation | {\n| \"StackDriftStatus\": \"NOT_CHECKED\"\n\nIf any of the stacks show CREATE_FAILED , ROLLBACK_* or DELETE_* , open the CloudFormation page in the console to investigate the root cause. Failed cluster creation stacks are often related to insufficient service quotas for the cluster itself, the instance groups, or the network components such as VPCs or NAT gateways. Check the correspondingÂ  SageMaker HyperPod Quotas to learn more about the required quotas for SageMaker HyperPod.\nConnecting to a cluster\nAfter the cluster stack has successfully created the required resources and the status has changed to CREATE_COMPLETE , you canÂ configure the CLI and your local Kubernetes environment to interact with the HyperPod cluster.\nhyp set-cluster-context --cluster-name <cluster-name> â€”region <region>\nThe --cluster-name Â option specifies the name of the HyperPod cluster to connect to and the --region option specifies the Region where the cluster has been created. Optionally, a specific namespace can be configured using the --namespace Â parameter. The command updates your local Kubernetes config in ./kube/config , so that you can use both the HyperPod CLI and Kubernetes utilities such as kubectl to manage the resources in your HyperPod cluster.\nSee our companion blog post for further information about how to use the CLI to submit training jobs and inference deployments to your newly created HyperPod cluster: Train and deploy models on Amazon SageMaker HyperPod using the new HyperPod CLI and SDK .\nModifying an existing HyperPod cluster\nThe HyperPod CLI provides a command to modify the instance groups and node recovery mode of an existing HyperPod cluster through theÂ  hyp update cluster Â command. This can be useful if you need to scale your cluster by adding or removing worker nodes, or if you want to change the instance types used by the node groups.\nTo update the instance groups, run the following command, adapted with your cluster name and desired instance group settings:\n\nhyp update cluster --cluster-name Â --region \\\nÂ --instance-groups '[{\nÂ  Â  Â Â Â  \"instance_count\": 2,\nÂ  Â  Â Â Â  \"instance_group_name\": \"worker-nodes\",\nÂ  Â  Â Â Â  \"instance_type\": \"ml.m5.large\",\nÂ  Â  Â Â Â  \"execution_role\": \"arn:aws:iam:::role/\",\nÂ  Â  Â Â Â  \"life_cycle_config\": {\nÂ  Â  Â  Â  Â Â Â  \"source_s3_uri\": \"s3:///amzn-s3-demo-source-bucket/\",\nÂ  Â  Â  Â  Â Â Â  \"on_create\": \"on_create.sh\"\nÂ  Â  Â Â Â  }\nÂ Â Â  }]'\n\nNote that all of the fields in the preceding command are required to run the update command, even if, for example, only the instance count is modified. You can list the current cluster and instance group configurations to obtain the required values by running the hyp describe cluster <cluster-name> --region <region> command.\nThe output of the update command will look like the following:\n\n[11/18/25 13:21:57] Update Params: {'instance_groups': [ClusterInstanceGroupSpecification(instance_count=2, instance_group_name='worker-nodes', instance_type='ml.m5.large', life_cycle_config=ClusterLifeCycleConfig(source_s3_uri='s3://amzn-s3-demo-source-bucket2', on_create='on_create.sh'), execution_role='arn:aws:iam::037065979077:role/hyp-eks-stack-4e5aExecRole', threads_per_core=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, instance_storage_configs=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, on_start_deep_health_checks=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, training_plan_arn=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, override_vpc_config=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, scheduled_update_config=<sagemaker_core.main.utils.Unassigned object at 0x106637810>, image_id=<sagemaker_core.main.utils.Unassigned object at 0x106637810>)], 'node_recovery': 'Automatic'}\n[11/18/25 13:21:58] Updating cluster resource. resources.py:3506\nINFO:sagemaker_core.main.resources:Updating cluster resource.\nCluster has been updated\nCluster hyperpod-cluster has been updated\n\nThe --node-recovery Â option lets you configure the node recovery behavior, which can be set to either Automatic Â or None . For information about the SageMaker HyperPod automatic node recovery feature, see Automatic node recovery .\nDeleting an existing HyperPod cluster\nTo delete an existing HyperPod cluster, run the following command. Note that this action is not reversible :\nhyp delete cluster-stack <stack-name> --region <region>\nThis command removes the specified CloudFormation stack and the associated AWS resources. You can use the optional --retain-resources Â flag to specify a comma-separated list of logical resource IDs to retain during the deletion process. Itâ€™s important to carefully consider which resources you need to retain, because the delete operation cannot be undone .\nThe output of this command will look like the following, asking you to confirm the resource deletion:\n\nâš  WARNING: This will delete the following 12 resources:\n\nOther (12):\n- EKSClusterStack\n- FsxStack\n- HelmChartStack\n- HyperPodClusterStack\n- HyperPodParamClusterStack\n- LifeCycleScriptStack\n- PrivateSubnetStack\n- S3BucketStack\n- S3EndpointStack\n- SageMakerIAMRoleStack\n- SecurityGroupStack\n- VPCStack\n\nContinue? [y/N]: y\nâœ“ Stack 'HyperpodClusterStack-d5351' deletion initiated successfully\n\nSageMaker HyperPod SDK\nSageMaker HyperPod also includes a Python SDK for programmatic access to the features described earlier. The Python SDK is used by the CLI commands and is installed when you install the sagemaker-hyperpod Â Python package as described in the beginning of this post. The HyperPod CLI is best suited for users who prefer a streamlined, interactive experience for common HyperPod management tasks like creating and monitoring clusters, training jobs, and inference endpoints. Itâ€™s particularly helpful for quick prototyping, experimentation, and automating repetitive HyperPod workflows through scripts or continuous integration and delivery (CI/CD) pipelines. In contrast, the HyperPod SDK provides more programmatic control and flexibility, making it the preferred choice when you need to embed HyperPod functionality directly into your application, integrate with other AWS or third-party services, or build complex, customized HyperPod management workflows. Consider the complexity of your use case, the need for automation and integration, and your teamâ€™s familiarity with programming languages when deciding whether to use the HyperPod CLI or SDK.\nThe SageMaker HyperPod CLI GitHub repository Â shows examples of how cluster creation and management can be implemented using the Python SDK.\nConclusion\nThe SageMaker HyperPod CLI and SDK simplify cluster creation and management. With the examples in this post, weâ€™ve demonstrated how these tools provide value through:\n\nSimplified lifecycle management â€“ From initial configuration to cluster updates and cleanup, the CLI aligns with how teams manage long-running training and inference environments and abstracts away unnecessary complexity.\nDeclarative control when needed â€“ The SDK exposes the underlying configuration model, so that teams can codify cluster specifications, instance groups, storage filesystems, and more.\nIntegrated observability â€“ Visibility into CloudFormation stacks is available without switching tools, supporting smooth iteration during development and operation.\n\nGetting started with these tools is as straightforward as installing the SageMaker HyperPod package. The SageMaker HyperPod CLI and SDK provide the right level of abstraction for both data scientists looking to quickly experiment with distributed training and ML engineers building production systems.\nIf youâ€™re interested in how to use the HyperPod CLI and SDK for submitting training jobs and deploying models to your new cluster, make sure to check our companion blog post: Train and deploy models on Amazon SageMaker HyperPod using the new HyperPod CLI and SDK .\n\nAbout the authors\n\nNicolas Jourdan\nNicolas JourdanÂ is a Specialist Solutions Architect at AWS, where he helps customers unlock the full potential of AI and ML in the cloud. He holds a PhD in Engineering from TU Darmstadt in Germany, where his research focused on the reliability and MLOps of industrial ML applications. Nicolas has extensive hands-on experience across industries, including autonomous driving, drones, and manufacturing, having worked in roles ranging from research scientist to engineering manager. He has contributed to award-winning research, holds patents in object detection and anomaly detection, and is passionate about applying cutting-edge AI to solve complex real-world problems.\n\nAndrew Brown\nAndrew Brown is a Sr. Solutions Architect who has been working at AWS in the Energy Industry for the last four years. He specializes in Deep Learning and High Performance Computing.\n\nGiuseppe Angelo Porcelli\nGiuseppe Angelo PorcelliÂ is a Principal Machine Learning Specialist Solutions Architect for Amazon Web Services. With several years of software engineering and an ML background, he works with customers of any size to understand their business and technical needs and design AI and ML solutions that make the best use of the AWS Cloud and the Amazon Machine Learning stack. He has worked on projects in different domains, including MLOps, computer vision, and NLP, involving a broad set of AWS services. In his free time, Giuseppe enjoys playing football.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "3e46996d56f2a93b",
    "title": "Evaluate generative AI models with an Amazon Nova rubric-based LLM judge on Amazon SageMaker AI (Part 2)",
    "url": "https://aws.amazon.com/blogs/machine-learning/evaluate-generative-ai-models-with-an-amazon-nova-rubric-based-llm-judge-on-amazon-sagemaker-ai-part-2/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-06T16:29:45Z",
    "summary": "In this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.",
    "content": "In the post Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI , we introduced the Amazon Nova LLM-as-a-judge capability, which is a specialized evaluation model available through Amazon SageMaker AI that you can use to systematically measure the relative performance of generative AI systems.\nSageMaker AI now offers a rubric-based large language model (LLM) judge powered by Amazon Nova. Instead of using the same general rules for every task, it automatically creates specific evaluation criteria for each individual prompt. This helps generative AI developers and machine learning (ML) engineers automatically generate precise, scenario-specific evaluation criterion for their LLMs and generative AI products, without manually crafting rule sets for every use case.\nIn this post, we explore the Amazon Nova rubric-based judge feature: what a rubric-based judge is, how the judge is trained, what metrics to consider, and how to calibrate the judge. We chare notebook code of the Amazon Nova rubric-based LLM-as-a-judge methodology to evaluate and compare the outputs of two different LLMs using SageMaker training jobs.\nOverview of Amazon Nova rubric-based judge\nA rubric-based LLM judge uses a highly capable LLM (specifically Amazon Nova) to act as a judge for responses generated by other AI models or human-generated responses. This capability provides pairwise comparisons between model iterations, so you can make data-driven decisions about model improvements with confidence. A rubric is a scoring guide used to evaluate performance. Traditionally, to evaluate an AI model, humans manually write a checklist of rules (a static rubric) that apply to every task. For example: â€œIs the answer polite?â€ or â€œIs the answer short?â€ The challenge with this approach is that these general rules donâ€™t scale to all users. A creative writing prompt needs different grading criteria than a coding prompt or a legal summary rather than one general checklist. The Amazon Nova powered judge creates a custom checklist for different interactions. Users send a specific prompt to the system (for example, â€œSummarize this medical document for a patientâ€). The judge analyzes that specific prompt and decides in the moment what an ideal answer should look like. It automatically writes criteria such as the following:\n\nDoes it use simple, non-medical jargon?\nDoes it capture the diagnosis accurately?\nIs the tone empathetic?\n\nThe judge then looks at the output generated by your LLM model and grades it against those specific criteria it just created. You get a quality score based on rules that were relevant to that specific task, not generic rules.\nExample input\nFor the example prompt or input â€œdo dinosaurs really exist?â€, we have two responses as shown in the following table.\n\nResponse A\nResponse B\n\nThe simple answer is: Yes, dinosaurs absolutely existed, but they do not exist today (except for their bird descendants). Hereâ€™s a breakdown of what that means: 1. Dinosaurs Were Very Real For over 150 million years, dinosaurs were the dominant land animals on Earth. We know they existed because we have an enormous amount of proof: Fossils: We have found millions of dinosaur fossilsâ€”bones, teeth, skulls, and clawsâ€”all over the world. These arenâ€™t myths; they are the actual preserved remains of these animals. Footprints: We have discovered countless fossilized trackways, which are footprints left in mud that later turned to stone. These tell us how dinosaurs walked, ran, and lived in groups. Eggs and Nests: Paleontologists have found fossilized dinosaur eggs and nests, showing us how they reproduced. Even Soft Tissues: In some extremely rare cases, scientists have found evidence of feathers, skin impressions, and even chemical traces of proteins from dinosaurs. 2. But They Are Extinct (Most of Them) About 66 million years ago, a massive asteroid struck the Earth (in what is now the Gulf of Mexico). This caused a catastrophic event that led to the extinction of all non-avian dinosaursâ€”that is, all dinosaurs except for the ones that could fly. 3. Their Descendants Are All Around Us: Birds! This is the most exciting part of the answer! Birds are actually a type of dinosaur. Think of it this way: The dinosaur group is huge. T. rex, Triceratops, and Brachiosaurus are all famous members. Another branch of the dinosaur family tree were small, feathered dinosaurs. One group of these feathered dinosaurs evolved the ability to fly. They are the direct ancestors of all modern birds. So, when you see a pigeon on the street or a chicken on your plate, you are looking at a living dinosaur. In summary:\n\nDid dinosaurs exist? Yes, overwhelmingly, we have proof they were real animals.\nDo they exist today? Not in the form of T. rex or Stegosaurus. Those specific types are extinct.\nBut wait! Their direct descendants, the birds, are everywhere. So, in a very real scientific sense, dinosaurs are still with us today.\n\nYes, dinosaurs did really exist â€” but not today. Dinosaurs were a diverse group of reptiles that lived on Earth millions of years ago, primarily during the Mesozoic Era, which spanned from about 252 million years ago to 66 million years ago. Fossil evidence â€” including bones, footprints, eggs, and even skin impressions â€” confirms that dinosaurs once roamed the planet. Scientists have identified hundreds of dinosaur species, ranging from the massive Tyrannosaurus rex and long-necked Brachiosaurus to smaller, agile predators like Velociraptor. Most non-avian dinosaurs went extinct around 66 million years ago, likely due to a massive asteroid impact combined with volcanic activity and climate change. However, one group of dinosaurs survived: the ancestors of modern birds. So in a scientific sense, birds are living dinosaurs, making them the closest living relatives to extinct dinosaurs like T. rex and Triceratops. So while you wonâ€™t see a Stegosaurus walking around today, the legacy of dinosaurs lives on â€” especially every time you see a sparrow or eagle fly by.\n\nExample output\nOut of the two responses, the rubric-based judge prefers Response A and also provides a justification of why it prefers response A over response B, as shown in the following screenshot.\n\nThe evaluation is tailored to the exact intent of the userâ€™s prompt (coding vs. writing vs. summarizing). Generative AI developers, data scientists, and ML engineers donâ€™t have to spend hundreds of hours manually writing evaluation rules for every possible scenario. You can evaluate thousands of different types of prompts instantly, achieving high quality across diverse use cases.\nEnterprise implementation examples\nThe Amazon Nova rubric-based LLM judge addresses critical evaluation challenges across different scenarios:\n\nModel development and checkpoint selection â€“ Development teams integrate the Amazon Nova rubric-based judge evaluation into training pipelines to automatically evaluate checkpoints. Per-criterion scores reveal which capabilities strengthened or regressed across iterations, enabling data-driven decisions about hyperparameter adjustments and data curation.\nTraining data quality control â€“ Teams use the Amazon Nova rubric-based judge evaluation to filter supervised fine-tuning datasets by generating point-wise scores on relevance criteria, identifying low-quality examples. For preference datasets, calculated margins between response pairs enable curriculum learning strategies that filter overwhelmingly one-sided examples providing limited learning signals.\nAutomated deep dive and root cause analysis â€“ Organizations deploying generative AI at scale can use the Amazon Nova rubric-based judge evaluation for systematic analysis across thousands of model outputs without manual review. When models exhibit quality issues, developers can examine which specific criteria drive preference judgments, identifying systematic weaknesses that inform targeted improvements instead of broad retraining efforts.\n\nHow dynamic rubric generation works\nThe Amazon Nova rubric-based LLM judge takes as input a triplet: <prompt, response_1, response_2> . The judge compares the quality of the two responses for the given prompt and outputs a preference label. In addition to the overall label, the judge generates a justification for its decision, guided by a rubric.\nA rubric is a set of weighted criteria used to evaluate the two responses. The rubric-based LLM judge is trained to generate criteria with weights that sum to 1. Each criterion in the rubric has a short_name , description , and weight . The judgeâ€™s decision includes a score for each response on each criterion in the rubric along with justifications for the scores.\nThe Amazon Nova rubric-based LLM judge employs an evaluation methodology where each judgment is supported by dynamically generated, prompt-specific criteria. When the judge receives an evaluation request containing a prompt and candidate responses, it analyzes the prompt to understand the prompt context, and generates criteria based on that context. This dynamic generation process makes sure evaluations are grounded in criteria directly applicable to the task at hand, providing transparent and interpretable assessments.\nFor each evaluation, the judge produces structured YAML output containing the generated criteria with their definitions, per-criterion scores on a 1â€“5 scale, and detailed justifications explaining each score. The final output includes one of four preference labels: [[A>B]] , [[B>A]] , [[A=B]] , or [[A=B (bothbad)] . Each criterion score is accompanied by a justification that grounds the assessment in observable characteristics of the responses, enabling deep-dive analysis and debugging of model behavior.\nComparing rubric-based Amazon Nova LLM-as-a-judge to previous versions\nThe rubric-based judge differs from previous versions in how it presents evaluation results and what information it provides.\nThe previous version of the Amazon Nova LLM-as-a-judge model returned simple preference labels ( [[A>B]] or [[B>A]] ). The rubric-based version generates a structured YAML output that consists of the following:\n\nA prompt-specific rubric for assessing the responses organized as a set of criteria with associated per-criterion importance weights (weights sum up to 1)\nBrief natural language descriptions of each criteria\nLikert score (on 1â€“5 scale) or binary (true/false) decision for each criterion for every candidate response in the input\nJustification for each criterion score for every candidate response\nOverall preference judgement: one of A>B, B>A, A=B, or A=B (both bad)\n\nThe new detailed output format facilitates a broad range of nuanced use cases. For example, specific criteria within rubrics allow for pointed comparisons of responses. A succinct response might be more suitable for certain use cases, whereas a comprehensive response might be needed in others. Justifications and explicit criteria scoring helps users discard certain criteria that are unsuitable for their needs and recompute the preference judgements without rerunning the query though the LLM judge.\nMetrics explanation\nIn our judge evaluation process, we use several important metrics to serve as comparison points for ranking judge quality. Forward agreement is a metric which computes agreement with human preference with the chosen response and rejected response in a specific order, which makes sure the correct label is always one of A>B or B>A for the entire dataset. Because positional consistency is an important desired property of a trustworthy LLM judge, we evaluate our checkpoints on reconciled agreementâ€”that is, we obtain two judgements with responses presented to the judge in both possible orders (for two response preference judgements). We only credit the judge with a correct answer if the judge agrees in both directions and the judgement matches human preference. This number, by definition, will always be lower than forward agreement. However, because real-world datasets arenâ€™t sorted, it provides a more accurate proxy for the real-world performance of an LLM judge model.\nWeighted scores ( weighted_score_A and weighted_score_B ) are new metrics added to the rubric judge evaluation output, which provide a view into the confidence of the judgment. A large difference between the weighted scores indicates a strong preference for one response over the over. These scores are calculated per sample based on the assigned scores for each criterion in the rubric. Each criterion score is normalized to a 0â€“1 range (where scale scores 1â€“5 map to 0.0â€“1.0, and binary True/False map to 1.0/0.0), then multiplied by the criterionâ€™s weight and summed to produce the weighted scores for each response.\nThe score_margin shows the difference between the weighted scores, with negative values indicating a preference towards response B and positive values indicating a preference towards response A. In the final evaluation output, these metrics are reported as averages across all samples. Per-sample criteria breakdowns, individual scores, and justifications can be found in the detailed Parquet output file.\nPer comparison sample, we can get the specific criteria that the new rubric judge model used during to compare the two results, which looks like the following example code:\n\n================================================================================\nRow 1:\nÂ Â Preference: ['B>A']\nÂ Â A wins: 0.0\nÂ Â B wins: 2.0\nÂ Â Weighted A: 0.225\nÂ Â Weighted B: 1.000\nÂ Â Margin: -0.775\n\nÂ Â Overall Justification:\nÂ Â  Â Response B provides a comprehensive and detailed explanation of photosynthesis, covering the process, location, chemical equation, and importance. Response A only provides a brief, surface-level description without explaining the mechanism or significance.\n\nÂ Â Criteria:\n\nÂ Â  Â completeness:\nÂ Â  Â  Â Score A: 2, Score B: 5\nÂ Â  Â  Â Weight: 0.5, Type: scale\nÂ Â  Â  Â Description: How thoroughly the response explains the photosynthesis process.\nÂ Â  Â  Â Justification A: Response A mentions the basic inputs and outputs but lacks detail on the mechanism, location in the cell, or the chemical equation.\nÂ Â  Â  Â Justification B: Response B provides a complete explanation including the process, chloroplasts, chemical equation, and the importance to life on Earth.\n\nÂ Â  Â clarity:\nÂ Â  Â  Â Score A: 3, Score B: 5\nÂ Â  Â  Â Weight: 0.3, Type: scale\nÂ Â  Â  Â Description: How clearly the response communicates the concept.\nÂ Â  Â  Â Justification A: Response A is clear but overly simplistic, lacking the detail needed for full understanding.\nÂ Â  Â  Â Justification B: Response B is well-structured and clearly explains each component of photosynthesis in an accessible way.\n\nÂ Â  Â accuracy:\nÂ Â  Â  Â Score A: 4, Score B: 5\nÂ Â  Â  Â Weight: 0.2, Type: scale\nÂ Â  Â  Â Description: How accurate the scientific information is.\nÂ Â  Â  Â Justification A: Response A is accurate in what it states but incomplete.\nÂ Â  Â  Â Justification B: Response B is fully accurate and includes the correct chemical equation and scientific terminology.\n================================================================================\n\nThese weighted metrics are informational and provide quantitative insight into the scoring breakdown, but the actual preference decision (A>B, B>A, or A=B) that determines the final win counts is based on the judge modelâ€™s overall preference output.\nTraining approach for the judge\nThe Amazon Nova rubric-based judge is trained with a multi-aspect reward package. In our training methodology, we optimize for several desirable characteristics for an LLM judge using an effective reward formulation. We mainly target the following criteria:\n\nPreference accuracy â€“ The judge is rewarded when it produces decisions that align with gold human preferences. When it chooses one response over another, the model is rewarded.\nPositional consistency â€“ The judgeâ€™s decisions are trained to be resilient towards positional inconsistency issues given a specific candidate response order.\nJustification quality â€“ The judgeâ€™s justifications for making the decision must align with the generated rubrics, scores, and final judgement.\nScore calibration â€“ The weighted scores for the responses must be calibrated with the decision accuracy (high confidence judgements must be correct more often than low confidence judgements).\n\nWe start with human annotated preference data and employ a custom data filtering and synthetic data generation setup to obtain rubric-aligned preference justifications. We sample from the generated synthetic rubrics and developed a custom pipeline to train the Amazon Nova rubric-based LLM judge to proficiently generate appropriate criteria with precise granularity for consistent and robust decision-making.\nBenchmark performance\nTesting on standard evaluation datasets shows improvements, particularly on tasks requiring nuanced judgment, as shown in the following table.\n\nBenchmark\nPrevious Amazon Nova Judge\nNew Amazon Nova Rubric-Based Judge\n\nPPE\n0.61\n0.64\n\nRMBench\n0.66\n0.88\n\nRewardBench\n0.88\n0.9\n\nJudgeBench\n0.51\n0.76\n\nCodeUltraFeedback\n0.69\n0.72\n\nMMEval\n0.8\n0.84\n\nThe larger improvements on JudgeBench and RMBench reflect better handling of complex evaluation scenarios.\nCalibration\nDuring our training process as well as during postprocessing, we evaluate the Amazon Nova rubric-based judgeâ€™s ability to make well-calibrated decisions. To achieve balanced calibration, we look at confidence buckets on a human annotated preference dataset. We look at the difference of weighted scores for response pairs. We aim for calibration of confidence to accuracy. Ideally, the LLM judge should be more accurate when making high confidence decisions and is allowed to be less accurate when making low confidence decisions. We find that this calibration methodology results in consistent decision-making in and out of distribution datasets. We also look at the distributions of scores generated for different criteria. We look for an approximately normal distribution over Likert scale scores (1â€“5) over the eval dataset. This two-pronged calibration checking process helps us identify better LLM judge checkpoints among several similarly well-performing checkpoints.\nUse cases of rubric-based judgement\nThe reliability of dynamically generated rubrics stems from three decisions:\n\nThe judge is trained on diverse, high-quality rubric-annotated preference data representing real-world use cases, teaching it patterns that distinguish effective evaluation criteria from superficial ones.\nOur filtering mechanism during training prioritizes rubrics exhibiting desirable propertiesâ€”comprehensiveness, mutual exclusivity, appropriate specificity, and task relevanceâ€”making sure the model learns from the best examples.\nOur reward formulation directly incentivizes rubric quality: criteria that lead to accurate, position-invariant preferences with well-calibrated confidence receiving positive rewards, whereas those producing inconsistent judgments are penalized.\n\nHow to use rubrics to improve practical applications\nMany modern applications operate in reference-free environments, where no gold-standard human answers exist. In these cases, the usefulness of the rubric is paramount. In this section, we spotlight instances where rubrics generated by our judge could be useful inputs for informed decision-making. We demonstrate how outputs of our rubric-based judgeâ€”specifically the weighted criteria, granular scores, and explicit justificationsâ€”serve as critical control mechanisms.\nEvaluating RAG systems\nIn Retrieval Augmented Generation (RAG), the primary failure mode is hallucinations. Traditional preference judges typically conflate â€œis the response good?â€ with â€œis this fluent?â€, â€œis this well-formatted?â€, â€œdoes the internal logic hold up?â€, and so on. A fluent but factually incorrect response is often perceived as more credible than a disjointed one containing accurate information. A factuality-focused evaluation can help you choose a summarization model because the retrieval results donâ€™t have hallucinations. Using a rubric-based judge for such judgements could help in understanding whether preference judgement is based on criteria like fluency and formatting, or if the judgement is based on relevant criteria such as faithfulness, context relevance, and so on. Users can disregard the scores of irrelevant criteria and re-valuate judgements based on a subset of criteria they care about for their application.\nThe creative critic\nIn this example, we look in the other direction, where creativity and originality are desirable over faithfulness to real-world facts or previous context. Consider a use case where you are using an LLM to generate short stories or scripts that are original, but the user provides a few examples of past scripts to demonstrate the requirements. Selecting good outputs from these generations require the generated stories to be sufficiently different from the examples, creative, original, and not borrow directly from existing training data. The end-user could index on criteria such as originality, coherence, and engagement to optimize for preference judgements suited to this use case, when using our rubric-based judge. You could further look at the explicit justifications for criteria scores for the specific type of originality and creativity that is desirable.\nSolution overview\nThis solution demonstrates how to evaluate generative AI models on SageMaker AI using a rubric-based judge capability. You can also evaluate human generated responses, but in this solution, we show how you can evaluate responses generated by other LLMs such as Qwen models using Amazon Nova as a rubric-based judge.\nFirst, we prepare a dataset by sampling questions from the Stanford Question Answering Dataset (SQuAD) and generating candidate responses from both Qwen2.5 1.5B Instruct and Qwen2.5 7B Instruct. Both models are accessed through SageMaker hosted Hugging Face endpoints. The responses from both models are saved in a JSONL file ( llm_judge.jsonl ) containing the prompt, response_A (from Qwen2.5 1.5B Instruct), and response_B (from Qwen2.5 7B Instruct).\nNext, the JSONL file is uploaded to an Amazon Simple Storage Service (Amazon S3) bucket. A PyTorch Estimator then launches an evaluation job using the Amazon Nova rubric-based LLM-as-a-judge recipe. The judge model dynamically generates evaluation rubrics and criteria tailored to each task, then compares the two candidate responses against these criteria. The job runs on GPU instances such as ml.g5.12xlarge and produces evaluation metrics, including per-criterion scores, justifications, comparative assessments, preference counts, and confidence measures. Results are saved to Amazon S3 for analysis.\nFinally, a visualization function renders charts and tables, summarizing the generated rubrics, score distributions across evaluation dimensions, comparative performance between the two Qwen2.5 models, and detailed examples with justifications. Through this end-to-end approach, you can assess which model performs better, identify specific strengths and weaknesses, track improvements, and make data-driven decisions about deploying generative modelsâ€”all without manual annotation.\nPrerequisites\nYou must complete the following prerequisites before you can run the notebook:\n\nMake the following quota increase requests for SageMaker AI. For this use case, you must request (on the Service Quotas console) a minimum of two g5.12xlarge instances for endpoint usage and at least one g5.12xlarge instance for training job usage.\n(Optional) You can create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI ) to access Jupyter notebooks with the preceding IAM role. (You can use JupyterLab in your local setup, too.)\n\nCreate an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess , AmazonS3FullAccess , and AmazonBedrockFullAccess to give required access to SageMaker AI and Amazon Bedrock to run the examples.\nBefore proceeding, make sure to grant the execution role direct s3:PutObject permissions for your S3 bucket prefix as an inline policy:\n\n{\n\"Effect\": \"Allow\",\nÂ Â \"Action\": [\n\"s3:PutObject\",\nÂ Â  Â \"s3:GetObject\",\nÂ Â  Â \"s3:ListBucket\"\n],\nÂ Â \"Resource\": [\n\"arn:aws:s3:::my-bucket-east\",\nÂ Â  Â \"arn:aws:s3:::my-bucket-east/*\"\n]\n}\n\nClone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets.\n\ngit clone https://github.com/aws-samples/amazon-nova-samples.git\ncd customization/Nova_2.0/04_eval/Amazon-Nova-Rubric-Based-LLM-As-A-Judge\n\nRun the notebook Amazon-Nova-Rubric-LLM-as-a-Judge-Sagemaker-AI.ipynb to start using the Amazon Nova LLM-as-a-judge implementation on SageMaker AI.\n\nConfigure models\nTo conduct a rubric-based Amazon Nova LLM-as-a-judge evaluation, you must generate outputs from both candidate models you want to compare. In this project, we deploy Qwen2.5 1.5B Instruct and Qwen2.5 7B Instruct on SageMaker to generate responses that will be compared by the Amazon Nova judge model.\nBoth models are open-weight multilingual language models deployed on dedicated SageMaker endpoints. This is achieved by using the HuggingFaceModel deployment interface. To deploy the Qwen2.5 1.5B Instruct and Qwen2.5 7B Instruct models, we provide a convenient script that accepts the model name as an argument:\n\npython3 deploy_model_arg.py Qwen/Qwen2.5-1.5B-Instruct\npython3 deploy_model_arg.py Qwen/Qwen2.5-7B-Instruct\n\nWe have also included the ability to test both of these deployed models. When you have deployed the models, you can move on to creating the evaluation data for the rubric-based Amazon Nova LLM-as-a-judge.\nPrepare dataset\nTo create a realistic evaluation dataset for comparing the Qwen models, we used SQuAD, a widely adopted benchmark in natural language understanding distributed under the CC BY-SA 4.0 license. SQuAD consists of thousands of crowd-sourced question-answer pairs covering a diverse range of Wikipedia articles. By sampling from this dataset, we made sure that our evaluation prompts reflected high-quality, factual question-answering tasks representative of real-world applications.\nWe began by loading a small subset of examples to keep the workflow fast and reproducible. Specifically, we used the Hugging Face datasets library to download and load the first 20 examples from the SQuAD training split:\n\nfromÂ datasets importÂ load_dataset\nsquad =Â load_dataset(\"squad\",Â split=\"train[:20]\")\n\nThis command retrieves a slice of the full dataset, containing 20 entries with structured fields including context, question, and answers. To verify the contents and inspect an example, we printed out a sample question and its ground truth answer:\n\nprint(squad[3][\"question\"])\nprint(squad[3][\"answers\"][\"text\"][0])\n\nFor the evaluation set, we selected the first six questions from this subset: questions = [squad[i][\"question\"] for i in range(6)]\nGenerate evaluation dataset\nAfter preparing a set of evaluation questions from SQuAD, we generated outputs from both Qwen2.5 models and assembled them into a structured dataset to be used by the Amazon Nova rubric-based LLM-as-a-judge workflow. This dataset serves as the core input for SageMaker AI evaluation recipes.To do this, we iterated over each question prompt and invoked the generation function for both SageMaker endpoints:\n\ngenerate_response(\"qwen25-15b-instruct-endpoint\", q) for completions from the Qwen2.5 1.5B Instruct model\ngenerate_response(\"qwen25-7b-instruct-endpoint\", q) for completions from the Qwen2.5 7B Instruct model\n\nFor each prompt, the workflow attempted to generate a response from each model.The following code calls two different versions of the Qwen 2.5 model. This allows the LLM judge to later determine if the larger model provides significantly better accuracy or if the smaller model is sufficient for the task.\n\n# Define the output file path for the LLM judge dataset\n\noutput_path = \"llm_judge.jsonl\"\n\nwith open(output_path, \"w\") as f:\nÂ Â  Â for q in questions:\nÂ Â  Â  Â  Â try:\n# Generate response from Model A (1.5B parameter model)\nÂ Â  Â  Â  Â  Â  Â response_a = generate_response(\"qwen25-15b-instruct-endpoint\", q)\nÂ Â  Â  Â  Â except Exception as e:\n# Fallback error message if the API call fails\nÂ Â  Â  Â  Â  Â  Â response_a = f\"[Qwen2.5 generation failed: {e}]\"\nÂ Â  Â  Â  Â try:\n# Generate response from Model B (7B parameter model)\nÂ Â  Â  Â  Â  Â  Â response_b = generate_response(\"qwen25-7b-instruct-endpoint\", q)\nÂ Â  Â  Â  Â except Exception as e:\n# Fallback error message if the API call fails\nÂ Â  Â  Â  Â  Â  Â response_b = f\"[ qwen25-7b generation failed: {e}]\"\n# Construct a dictionary containing the prompt and both model responses\nÂ Â  Â  Â  Â row = {\nÂ Â  Â  Â  Â  Â  Â \"prompt\": q,\nÂ Â  Â  Â  Â  Â  Â \"response_A\": response_a,\nÂ Â  Â  Â  Â  Â  Â \"response_B\": response_b\nÂ Â  Â  Â  Â }\nÂ Â  Â  Â  Â f.write(json.dumps(row) + \"\\n\")\n# Write the record to the JSONL file as a single line\n\nprint(f\"JSONL file created at: {output_path}\")\n\nThis workflow produced a JSON Lines file named llm_judge.jsonl . Each line contains a single evaluation record structured as follows:\n\n{\nÂ Â \"prompt\": \"What is the capital of France?\",\nÂ Â \"response_A\": \"The capital of France is Paris.\",\nÂ Â \"response_B\": \"Paris is the capital city of France.\"\n}\n\nThen, we uploaded the llm_judge.jsonl to an S3 bucket:\n\nupload_to_s3(\nÂ Â  Â \"llm_judge.jsonl\",\nÂ Â  Â \"s3://<YOUR_BUCKET_NAME>/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\"\n)\n\nLaunch Amazon Nova rubric-based LLM-as-a-judge evaluation job\nAfter preparing the dataset and creating the evaluation recipe, the final step is to launch the SageMaker training job that performs the Amazon Nova rubric-based LLM-as-a-judge evaluation. In this workflow, the training job acts as a fully managed, self-contained process that loads the judge model, processes the comparison dataset, applies dynamically generated rubrics, and generates comprehensive evaluation metrics in your designated Amazon S3 location. We use the PyTorch estimator class from the SageMaker Python SDK to encapsulate the configuration for the evaluation run. The estimator defines the compute resources, container image, evaluation recipe, and output paths for storing results:\n\nestimator = PyTorch(\nÂ Â  Â output_path=output_s3_uri,\nÂ Â  Â base_job_name=job_name,\nÂ Â  Â role=role,\nÂ Â  Â instance_type=instance_type,\nÂ Â  Â training_recipe=recipe_path,\nÂ Â  Â sagemaker_session=sagemaker_session,\nÂ Â  Â image_uri=image_uri,\nÂ Â  Â disable_profiler=True,\nÂ Â  Â debugger_hook_config=False,\n)\n\nAfter the estimator is configured, you initiate the evaluation job using the fit() method. This call submits the job to the SageMaker control plane, provisions the compute cluster (ml.g5.12xlarge instances), and begins processing your evaluation dataset:\nestimator.fit(inputs={\"train\": evalInput}) The job will execute the rubric-based comparison, with the Amazon Nova judge model dynamically generating evaluation criteria and scoring both Qwen2.5 model outputs. Results, including per-criterion scores, justifications, and comparative assessments, are automatically saved to your specified S3 output path for downstream analysis and visualization.\nResults from Amazon Nova rubric-based LLM-as-a-judge evaluation job\nThe following is an example result for a row of the evaluation. In this example, Assistant B is the clear winner because it prioritizes grounded, nuanced information over Assistant Aâ€™s suspiciously specific but unverified claim of 145 newspapers. The judge penalizes Assistant A for its lack of context, resulting in significantly lower scores for accuracy and completeness. By applying a custom weight thatÂ  allocates Â 50% of the total score to accuracy, the evaluation calculates a weighted margin that quantifies precisely why Assistant Bâ€™s detailed, verifiable response is superior.\n\n================================================================================\nRow 0:\nÂ Â Preference: ['B>A']\nÂ Â A wins: 0.0\nÂ Â B wins: 1.0\nÂ Â Weighted A: 0.175\nÂ Â Weighted B: 0.875\nÂ Â Margin: -0.700\n\nÂ Â Overall Justification:\nÂ Â  Â Assistant B's response is more accurate and complete as it provides specific examples of student publications and acknowledges the variability in the number of publications. Assistant A's response, while providing a specific number, lacks context and explanation, making it less useful for understanding the situation.\n\nÂ Â Criteria:\n\nÂ Â  Â accuracy:\nÂ Â  Â  Â Score A: 2, Score B: 4\nÂ Â  Â  Â Weight: 0.5, Type: scale\nÂ Â  Â  Â Description: How accurate the information provided is regarding the number of student newspapers at Notre Dame.\nÂ Â  Â  Â Justification A: Assistant A provides a specific number (145) but does not offer any context or explanation for this number, making it difficult to assess its accuracy.\nÂ Â  Â  Â Justification B: Assistant B provides a more nuanced answer, stating that there are at least three significant student publications but acknowledges that the number can vary. This response is more accurate given the dynamic nature of student publications.\n\nÂ Â  Â completeness:\nÂ Â  Â  Â Score A: 1, Score B: 5\nÂ Â  Â  Â Weight: 0.3, Type: scale\nÂ Â  Â  Â Description: How complete the response is in providing information about student newspapers at Notre Dame.\nÂ Â  Â  Â Justification A: Assistant A's response is incomplete as it does not provide any context or examples of student newspapers at Notre Dame.\nÂ Â  Â  Â Justification B: Assistant B's response is more complete as it provides examples of well-known student publications and acknowledges the variability in the number of publications.\n\nÂ Â  Â clarity:\nÂ Â  Â  Â Score A: 2, Score B: 5\nÂ Â  Â  Â Weight: 0.2, Type: scale\nÂ Â  Â  Â Description: How clear and understandable the response is.\nÂ Â  Â  Â Justification A: Assistant A's response is clear in providing a number but lacks clarity in explaining what this number represents.\nÂ Â  Â  Â Justification B: Assistant B's response is clear and understandable, providing examples and context to help the reader understand the number of student publications.\n\nAs in the post Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI , to help practitioners quickly interpret the outcome of an Amazon Nova rubric-based LLM-as-a-judge evaluation, we created a convenience function that produces a single, comprehensive visualization summarizing key metrics, as shown in the following screenshot.\n\nThis function, plot_nova_judge_results , uses Matplotlib and Seaborn to render an image with six panels, each highlighting a different perspective of the evaluation outcome.\nThis function takes the evaluation metrics dictionary produced when the evaluation job is complete and generates the following visual components:\n\nScore distribution bar chart â€“ Shows how many times Model A was preferred (three wins), how many times Model B was preferred (seven wins), how many ties occurred, and how often the judge failed to produce a decision (one inference error out of 11 evaluations). This provides an immediate sense of how decisive the evaluation was, clearly showing Model Bâ€™s dominance with a 70% preference rate.\nWin rate with 95% confidence interval â€“ Plots Model Bâ€™s overall win rate of 70% against Model A, including an error bar reflecting the confidence interval bounds of [0.400, 0.909]. A vertical reference line at 50% marks the point of no preference. Because the confidence interval doesnâ€™t cross this line, we can conclude the result is statistically significant, indicating meaningful superiority for the 7B model.\nPreference pie chart â€“ Visually displays the proportion of preferences among the 10 valid judgments: 70% for Model B and 30% for Model A. This can help users quickly understand the clear preference distribution favoring the larger model.\nA vs. B score comparison bar chart â€“ Compares the raw counts of preferences for each model side by side (three for Model A vs seven for Model B). A clear label annotates the margin of difference, emphasizing Model Bâ€™s four-win advantage. The chart also displays the weighted rubric-based scores: Model A averaged 0.495 whereas Model B averaged 0.630 across all evaluation criteria (accuracy, completeness, clarity), with an average margin of -0.135 favoring Model B.\nWin rate gauge â€“ Depicts the 70% win rate as a semicircular gauge with a needle pointing to Model Bâ€™s performance relative to the theoretical 0â€“100% range. This intuitive visualization helps nontechnical stakeholders immediately grasp that Model B outperformed Model A by a substantial margin based on dynamically generated rubric criteria tailored to each question-answer pair.\nSummary statistics table â€“ Compiles numerical metrics into a compact, clean table: 11 total evaluations, one error (9.1% error rate), 70% win rate, weighted rubric scores (0.630 for B vs 0.495 for A with -0.135 margin), and confidence intervals [0.400, 0.909]. This makes it straightforward to reference the exact numeric values behind the plots and understand both the statistical rigor and rubric-based assessment of the evaluation.\n\nBecause the function outputs a standard Matplotlib figure, you can quickly save the image, display it in Jupyter notebooks, or embed it in other documentation. The visualization clearly demonstrates that Model B shows statistically significant superiority overall with higher rubric-based scores across accuracy, completeness, and clarity dimensions.\nClean up\nTo stop and delete the SageMaker Studio spaces, follow these clean up steps in theÂ  SageMaker Studio documentation . You mustÂ  delete the S3 bucket Â and the hosted model endpoint to stop incurring costs. You can delete the real-time endpoints you created using the SageMaker console. For instructions, seeÂ  Delete Endpoints and Resources .\nConclusion\nEvaluating generative AI outputs at scale requires more than simple preference labels, it requires transparency into why one response outperforms another. The Amazon Nova rubric-based LLM judge addresses this need by dynamically generating task-specific evaluation criteria, providing per-criterion scores with explicit justifications, and delivering well-calibrated confidence signals. Compared to previous judge implementations, the rubric-based approach offers three key advantages: interpretability through structured YAML output with criterion-level breakdowns, flexibility enabling users to reweight or filter criteria for their specific use cases, and improved accuracy with significant gains across standard benchmarksâ€”including a 49% improvement on complex evaluation scenarios in JudgeBench. If you are selecting model checkpoints during development, filtering training data for quality, or debugging production model behavior at scale, the Amazon Nova rubric-based LLM-as-a-judge evaluation transforms opaque preference decisions into actionable insights. By exposing the reasoning behind each judgment, teams can identify systematic weaknesses, validate that evaluations align with their quality priorities, and build greater trust in automated evaluation pipelines.\nTo get started with the Amazon Nova rubric-based LLM judge on SageMaker AI, refer to Rubric Based Judge .\n\nAbout the authors\nSurya Kari is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the Llama family, and Qwen, focusing on their fine-tuning and optimization for specific scientific applications. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker, enabling the scaling of foundation models from development to production. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases.\nJoseph Moulton is a Software Engineer on the Amazon AGI Customization team supporting the implementation of evaluation and inference workflows for AWS Nova Forge. Current work focuses on developing and implementing new strategies for customers to evaluate their custom trained Nova models. He has been with the company as a software engineer for 4 years, joining the Alexa AI Machine Learning platform team in 2022 before transitioning to the Nova Forge team in 2025. In his free time he enjoys golfing and building computers.\nMorteza Ziyadi is an senior science lead and manager at Amazon AGI, where he leads several projects on post-training recipes and (Multimodal) large language models in the Amazon AGI Foundation modeling team. Before joining Amazon AGI, he spent four years at Microsoft Cloud and AI, where he led projects focused on developing natural language-to-code generation models for various products. He has also served as an adjunct faculty at Northeastern University. He earned his PhD from the University of Southern California (USC) in 2017 and has since been actively involved as a workshop organizer, and reviewer for numerous NLP, Computer Vision and machine learning conferences.\nRajkumar Pujari is an Applied Scientist II on the Nova Models post-training team at Amazon AGI. He obtained his Ph.D. in Computer Science from Purdue University, specializing in Machine Learning for Computational Social Science. Currently, his work focuses on post-training and reinforcement learning for Large Language Models. He develops large-scale, dynamic evaluation pipelines for frontier models and builds LLM-as-a-Judge frameworks.\nSwastik Roy is a Senior Applied Scientist on Amazonâ€™s AGI Foundation team, specializing in generalizability research and post-training of the Amazon Nova family of models. His expertise spans fine-tuning, reinforcement learning, and evaluation methodologies, where he drives efforts to advance the robustness of foundational AI systems.\nJoel Catapano is a Senior Applied Scientist on the Amazon AGI foundation modeling team. He primarily works on developing novel approaches for improving the LLM-as-a-Judge capability of the Nova family of models.\nMona Mona is a Sr World Wide Gen AI Specialist Solutions ArchitectÂ focusing on Gen AI Solutions in Amazon SageMaker AI team. She was a Lead Generative AI specialist in Google before joining Amazon. She is a published author of two books â€“ Natural Language Processing with AWS AI Services and Google Cloud Certified Professional Machine Learning Study Guide. She has authored 20+ blogs on AI/ML and cloud technology and a co-author on a research paper on CORD19 Neural Search which won an award for Best Research Paper at the prestigious AAAI (Association for the Advancement of Artificial Intelligence) conference.\nPradeep Natarajan is a Senior Principal Scientist in Amazon AGI Foundation modeling team working on post-training recipes and Multimodal large language models. He has 20+ years of experience in developing and launching multiple large-scale machine learning systems. He has a PhD in Computer Science from University of Southern California.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "16dd9c7f63e8fb58",
    "title": "Korea privacy policy",
    "url": "https://openai.com/policies/kr-privacy-policy",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-06T10:00:00Z",
    "summary": "Korea privacy policy",
    "content": "Korea privacy policy",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "1118bd5abc31ff39",
    "title": "Making AI work for everyone, everywhere: our approach to localization",
    "url": "https://openai.com/index/our-approach-to-localization",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-06T10:00:00Z",
    "summary": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
    "content": "OpenAI shares its approach to AI localization, showing how globally shared frontier models can be adapted to local languages, laws, and cultures without compromising safety.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]