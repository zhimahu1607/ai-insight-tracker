[
  {
    "id": "d1c989681d7d9025",
    "title": "Build a serverless AI Gateway architecture with AWS AppSync Events",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-a-serverless-ai-gateway-architecture-with-aws-appsync-events/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-26T17:20:27Z",
    "summary": "In this post, we discuss how to use AppSync Events as the foundation of a capable, serverless, AI gateway architecture. We explore how it integrates with AWS services for comprehensive coverage of the capabilities offered in AI gateway architectures. Finally, we get you started on your journey with sample code you can launch in your account and begin building.",
    "content": "AWS AppSync Events can help you create more secure, scalable Websocket APIs. In addition to broadcasting real-time events to millions of Websocket subscribers, it supports a crucial user experience requirement of your AI Gateway: low-latency propagation of events from your chosen generative AI models to individual users.\nIn this post, we discuss how to use AppSync Events as the foundation of a capable, serverless, AI gateway architecture. We explore how it integrates with AWS services for comprehensive coverage of the capabilities offered in AI gateway architectures. Finally, we get you started on your journey with sample code you can launch in your account and begin building.\nOverview of AI Gateway\nAI Gateway is an architectural middleware pattern that helps enhance the availability, security, and observability of large language models (LLMs). It supports the interests of several different personas. For example, users want low latency and delightful experiences. Developers want flexible and extensible architectures. Security staff need governance to protect information and availability. System engineers need monitoring and observability solutions that help them support the user experience. Product managers need information about how well their products perform with users. Budget managers need cost controls. The needs of these different people across your organization are important considerations for hosting generative AI applications.\nSolution overview\nThe solution we share in this post offers the following capabilities:\n\nIdentity – Authenticate and authorize users from the built-in user directory, from your enterprise directory, and from consumer identity providers like Amazon, Google, and Facebook\nAPIs – Provide users and applications low-latency access to your generative AI applications\nAuthorization – Determine what resources your users have access to in your application\nRate limiting and metering – Mitigate bot traffic, block access, and manage model consumption to manage cost\nDiverse model access – Offer access to leading foundation models (FMs), agents, and safeguards to keep users safe\nLogging – Observe, troubleshoot, and analyze application behavior\nAnalytics – Extract value from your logs to build, discover, and share meaningful insights\nMonitoring – Track key datapoints that help staff react quickly to events\nCaching – Reduce costs by detecting common queries to your models and returned predetermined responses\n\nIn the following sections, we dive into the core architecture and explore how you can build these capabilities into the solution.\nIdentity and APIs\nThe following diagram illustrates an architecture using the AppSync Events API to provide an interface between an AI assistant application and LLMs through Amazon Bedrock using AWS Lambda .\n\nThe workflow consists of the following steps:\n\nThe client application retrieves the user identity and authorization to access APIs using Amazon Cognito .\nThe client application subscribes to the AppSync Events channel, from which it will receive events like streaming responses from the LLMs in Amazon Bedrock.\nThe SubscribeHandler Lambda function attached to the Outbound Messages namespace verifies that this user is authorized to access the channel.\nThe client application publishes a message to the Inbound Message channel, such as a question posed to the LLM.\nThe ChatHandler Lambda function receives the message and verifies the user is authorized to publish messages on that channel.\nThe ChatHandler function calls the Amazon Bedrock ConverseStream API and waits for the response stream from the Converse API to emit response events.\nThe ChatHandler function relays the response messages from the Converse API to the Outbound Message channel for the current user, which passes the events to the WebSocket on which the client application is waiting for messages.\n\nAppSync Events namespaces and channels are the building blocks of your communications architecture in your AI Gateway. In the example, namespaces are used to attach different behaviors to our inbound and outbound messages. Each namespace can have different publish and subscribe integration to each namespace. Moreover, each namespace is divided into channels. Our channel structure design provides each user a private inbound and outbound channel, serving as one-to-one communications with the server side:\n\nInbound-Messages / ${sub}\nOutbound-Messages / ${sub}\n\nThe subject, or sub attribute, arrives in our Lambda functions as context from Amazon Cognito. It is an unchangeable, unique user identifier within each user pool. This makes it useful for segments of our channel names and is especially useful for authorization.\nAuthorization\nIdentity is established using Amazon Cognito, but we still need to implement authorization. One-to-one communication between a user and an AI assistant in our example should be private—we don’t want users with the knowledge of another user’s sub attribute to be able to subscribe to or publish to another user’s inbound or outbound channel.\nThis is why we use sub in our naming scheme for channels. This enables the Lambda functions attached to the namespaces as data sources to verify that a user is authorized to publish and subscribe.\nThe following code sample is our SubscribeHandler Lambda function:\n\ndef lambda_handler(event, context):\n    \"\"\"\n    Lambda function that checks if the first channel segment matches the user's sub.\n    Returns None if it matches or an error message otherwise.\n    \"\"\"\n\n    # Extract segments and sub from the event\n    segments = event.get(\"info\", {}).get(\"channel\", {}).get(\"segments\")\n    sub = event.get(\"identity\", {}).get(\"sub\", None)\n\n    # Check if segments exist and the first segment matches the user's sub\n    if not segments:\n        logger.error(\"No segments found in event\")\n        return \"No segments found in channel path\"\n\n    if sub != segments[1]:\n        logger.warning(\n            f\"Unauhotirzed: Sub '{sub}' did not match path segment '{segments[1]}'\"\n        )\n        return \"Unauthorized\"\n\n    logger.info(f\"Sub '{sub}' matched path segment '{segments[1]}'\")\n\n    return None\n\nThe function workflow consists of the following steps:\n\nThe name of the channel arrives in the event.\nThe user’s subject field, sub , is part of the context.\nIf the channel name and user identity don’t match, it doesn’t authorize the subscription and returns an error message.\nReturning None indicates no errors and that the subscription is authorized.\n\nThe ChatHandler Lambda function uses the same logic to make sure users are only authorized to publish to their own inbound channel. The channel arrives in the event and the context carries the user identity.\nAlthough our example is simple, it demonstrates how you can implement complex authorization rules using a Lambda function to authorize access to channels in AppSync Events.We have covered access control to an individual’s inbound and outbound channels. Many business models around access to LLMs involve controlling how many tokens an individual is allowed to use within some period of time. We discuss this capability in the following section.\nRate limiting and metering\nUnderstanding and controlling the number of tokens consumed by users of an AI Gateway is important to many customers. Input and output tokens are the primary pricing mechanism for text-based LLMs in Amazon Bedrock. In our example, we use the Amazon Bedrock Converse API to access LLMs. The Converse API provides a consistent interface that works with the models that support messages. You can write code one time and use it with different models.\nPart of the consistent interface is the stream metadata event . This event is emitted at the end of each stream and provides the number of tokens consumed by the stream. The following is an example JSON structure:\n\n{\n    \"metadata\": {\n        \"usage\": {\n            \"inputTokens\": 1062,\n            \"outputTokens\": 512,\n            \"totalTokens\": 1574\n        },\n        \"metrics\": {\n            \"latencyMs\": 4133\n        }\n    }\n}\n\nWe have input tokens, output tokens, total tokens, and a latency metric. To create a control with this data, we first consider the types of limits we want to implement. One approach is a monthly token limit that resets every month—a static window. Another is a daily limit based on a rolling window on 10-minute intervals. When a user exceeds their monthly limit, they must wait until the next month. After a user exceeds their daily rolling window limit, they must wait 10 minutes for more tokens to become available.\nWe need a way to keep atomic counters to track the token consumption, with fast real-time access to the counters with the user’s sub , and to delete old counters as they become irrelevant.\nAmazon DynamoDB is a serverless, fully managed, distributed NoSQL database with single-digit millisecond performance at many scales. With DynamoDB, we can keep atomic counters, provide access to the counters keyed by the sub , and roll off old data using its time to live feature. The following diagram shows a subset of our architecture from earlier in this post that now includes a DynamoDB table to track token usage.\n\nWe can use a single DynamoDB table with the following partition and sort keys:\n\nPartition key – user_id (String), the unique identifier for the user\nSort key – period_id (String), a composite key that identifies the time period\n\nThe user_id will receive the sub attribute from the JWT provided by Amazon Cognito. The period_id will have strings that sort lexicographically that indicate which time period the counter is for as well as the timeframe. The following are some example sort keys:\n\n10min:2025-08-05:16:40\n10min:2025-08-05:16:50\nmonthly:2025-08\n\n10min or monthly indicate the type of counter. The timestamp is set to the last 10-minute window (for example, (minute // 10) * 10 ).\nWith each record, we keep the following attributes:\n\ninput_tokens – Counter for input tokens used in this 10-minute window\noutput_tokens – Counter for output tokens used in this 10-minute window\ntimestamp – Unix timestamp when the record was created or last updated\nttl – Time to live value (Unix timestamp), set to 24 hours from creation\n\nThe two token columns are incremented with the DynamoDB atomic ADD operation with each metadata event from the Amazon Bedrock Converse API. The ttl and timestamp columns are updated to indicate when the record is automatically removed from the table.\nWhen a user sends a message, we check whether they have exceeded their daily or monthly limits.\nTo calculate daily usage, the meter.py module completes the following steps:\n\nCalculates the start and end keys for the 24-hour window.\nQueries records with the partition key user_id and sort key between the start and end keys.\nSums up the input_tokens and output_tokens values from the matching records.\nCompares the sums against the daily limits.\n\nSee the following example code:\n\nKeyConditionExpression: \"user_id = :uid AND period_id BETWEEN :start AND :end\"\nExpressionAttributeValues: {\n    \":uid\": {\"S\": \"user123\"},\n    \":start\": {\"S\": \"10min:2025-08-04:15:30\"},\n    \":end\": {\"S\": \"10min:2025-08-05:15:30\"}\n}\n\nThis range query takes advantage of the naturally sorted keys to efficiently retrieve only the records from the last 24 hours, without filtering in the application code.The monthly usage calculation on the static window is much simpler. To check monthly usage, the system completes the following steps:\n\nGets the specific record with the partition key user_id and sort key monthly:YYYY-MM for the current month.\nCompares the input_tokens and output_tokens values against the monthly limits.\n\nSee the following code:\n\nKey: {\n    \"user_id\": {\"S\": \"user123\"},\n    \"period_id\": {\"S\": \"monthly:2025-08\"}\n}\n\nWith an additional Python module and DynamoDB, we have a metering and rate limiting solution that works for both static and rolling windows.\nDiverse model access\nOur sample code uses the Amazon Bedrock Converse API. Not every model is included in the sample code, but many models are included for you to rapidly explore possibilities.The innovation in this area doesn’t stop at models on AWS. There are numerous ways to develop generative AI solutions at every level of abstraction. You can build on top of the layer that best suits your use case.\nSwami Sivasubramanian recently wrote on how AWS is enabling customers to deliver production-ready AI agents at scale . He discusses Strands Agents , an open source AI agents SDK, as well as Amazon Bedrock AgentCore , a comprehensive set of enterprise-grade services that help developers quickly and more securely deploy and operate AI agents at scale using a framework and model, hosted on Amazon Bedrock or elsewhere.\nTo learn more about architectures for AI agents, refer to Strands Agents SDK: A technical deep dive into agent architectures and observability . The post discusses the Strands Agents SDK and its core features, how it integrates with AWS environments for more secure, scalable deployments, and how it provides rich observability for production use. It also provides practical use cases and a step-by-step example.\nLogging\nMany of our AI Gateway stakeholders are interested in logs. Developers want to understand how their applications function. System engineers need to understand operational concerns like tracking availability and capacity planning. Business owners want analytics and trends so that they can make better decisions.\nWith Amazon CloudWatch Logs , you can centralize the logs from your different systems, applications, and AWS services that you use in a single, highly scalable service. You can then seamlessly view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. CloudWatch Logs makes it possible to see your logs, regardless of their source, as a single and consistent flow of events ordered by time.\nIn the sample AI Gateway architecture, CloudWatch Logs is integrated at multiple levels to provide comprehensive visibility. The following architecture diagram depicts the integration points between AppSync Events, Lambda, and CloudWatch Logs in the sample application.\n\nAppSync Events API logging\nOur AppSync Events API is configured with ERROR-level logging to capture API-level issues. This configuration helps identify issues with API requests, authentication failures, and other critical API-level problems.The logging configuration is applied during the infrastructure deployment:\n\nthis.api = new appsync.EventApi(this, \"Api\", {\n    // ... other configuration ...\n    logConfig: {\n        excludeVerboseContent: true,\n        fieldLogLevel: appsync.AppSyncFieldLogLevel.ERROR,\n        retention: logs.RetentionDays.ONE_WEEK,\n    },\n});\n\nThis provides visibility into API operations.\nLambda function structured logging\nThe Lambda functions use AWS Lambda Powertools for structured logging. The ChatHandler Lambda function implements a MessageTracker class that provides context for each conversation:\n\nlogger = Logger(service=\"eventhandlers\")\n\nclass MessageTracker:\n    \"\"\"\n    Tracks message state during processing to provide enhanced logging.\n    Handles event type detection and processing internally.\n    \"\"\"\n\n    def __init__(self, user_id, conversation_id, user_message, model_id):\n        self.user_id = user_id\n        self.conversation_id = conversation_id\n        self.user_message = user_message\n        self.assistant_response = \"\"\n        self.input_tokens = 0\n        self.output_tokens = 0\n        self.model_id = model_id\n        # ...\n\nKey information logged includes:\n\nUser identifiers\nConversation identifiers for request tracing\nModel identifiers to track which AI models are being used\nToken consumption metrics (input and output counts)\nMessage previews\nDetailed timestamps for time-series analysis\n\nEach Lambda function sets a correlation ID for request tracing, making it straightforward to follow a single request through the system:\n\n# Set correlation ID for request tracing\nlogger.set_correlation_id(context.aws_request_id)\n\nOperational insights\nCloudWatch Logs Insights enables SQL-like queries across log data, helping you perform the following actions:\n\nTrack token usage patterns by model or user\nMonitor response times and identify performance bottlenecks\nDetect error patterns and troubleshoot issues\nCreate custom metrics and alarms based on log data\n\nBy implementing comprehensive logging throughout the sample AI Gateway architecture, we provide the visibility needed for effective troubleshooting, performance optimization, and operational monitoring. This logging infrastructure serves as the foundation for both operational monitoring and the analytics capabilities we discuss in the following section.\nAnalytics\nCloudWatch Logs provides operational visibility, but for extracting business intelligence from logs, AWS offers many analytics services. With our sample AI Gateway architecture, you can use those services to transform data from your AI Gateway without requiring dedicated infrastructure or complex data pipelines.\nThe following architecture diagram shows the flow of data between the Lambda function, Amazon Data Firehose , Amazon Simple Storage Service (Amazon S3), the AWS Glue Data Catalog , and Amazon Athena .\n\nThe key components include:\n\nData Firehose – The ChatHandler Lambda function streams structured log data to a Firehose delivery stream at the end of each completed user response. Data Firehose provides a fully managed service that automatically scales with your data throughput, alleviating the need to provision or manage infrastructure. The following code illustrates how the API call that integrates the ChatHandler Lambda function with the delivery stream:\n\n# From messages.py\nfirehose_stream = os.environ.get(\"FIREHOSE_DELIVERY_STREAM\")\nif firehose_stream:\n    try:\n        firehose.put_record(\n            DeliveryStreamName=firehose_stream,\n            Record={\"Data\": json.dumps(log_data) + \"\\n\"},\n        )\n        logger.debug(f\"Successfully sent data to Firehose stream: {firehose_stream}\")\n    except Exception as e:\n        logger.error(f\"Failed to send data to Firehose: {str(e)}\")\n\nAmazon S3 with Parquet format – Firehose automatically converts the JSON log data to columnar Parquet format before storing it in Amazon S3. Parquet improves query performance and reduces storage costs compared to raw JSON logs. The data is partitioned by year, month, and day, enabling efficient querying of specific time ranges while minimizing the amount of data scanned during queries.\nAWS Glue Data Catalog – An AWS Glue database and table are created in the AWS Cloud Development Kit (AWS CDK) application to define the schema for our analytics data, including user_id , conversation_id , model_id , token counts, and timestamps. Table partitions are added as new S3 objects are stored by Data Firehose.\nAthena for SQL-based analysis – With the table in the Data Catalog, business analysts can use familiar SQL through Athena to extract insights. Athena is serverless and priced per query based on the amount of data scanned, making it a cost-effective solution for one-time analysis without requiring database infrastructure. The following is an example query:\n\n-- Example: Token usage by model\nSELECT\n    model_id,\n    SUM(input_tokens) as total_input_tokens,\n    SUM(output_tokens) as total_output_tokens,\n    COUNT(*) as conversation_count\nFROM firehose_database.firehose_table\nWHERE year='2025' AND month='08'\nGROUP BY model_id\nORDER BY total_output_tokens DESC;\n\nThis serverless analytics pipeline transforms the events flowing through AppSync Events into structured, queryable tables with minimal operational overhead. The pay-as-you-go pricing model of these services facilitates cost-efficiency, and their managed nature alleviates the need for infrastructure provisioning and maintenance. Furthermore, with your data cataloged in AWS Glue, you can use the full suite of analytics and machine learning services on AWS such as Amazon Quick Sight and Amazon SageMaker Unified Studio with your data.\nMonitoring\nAppSync Events and Lambda functions send metrics to CloudWatch so you can monitor performance, troubleshoot issues, and optimize your AWS AppSync API operations effectively. For an AI Gateway, you might need more information in your monitoring system to track important metrics such as token consumption from your models.\nThe sample application includes a call to CloudWatch metrics to record the token consumption and LLM latency at the end of each conversation turn so operators have visibility into this data in real time. This enables metrics to be included in dashboards and alerts. Moreover, the metric data includes the LLM model identifier as a dimension so you can track token consumption and latency by model. Metrics are just one component of what we can learn about our application at runtime with CloudWatch. Because our log messages are formatted as JSON, we can perform analytics on our log data for monitoring using CloudWatch Logs Insights. The following architecture diagram illustrates the logs and metrics made available by AppSync Events and Lambda through CloudWatch and CloudWatch Logs Insights.\n\nFor example, the following query against the sample application’s log groups shows us the users with the most conversations within a given time window:\n\nfields , \n| filter  like \"Message complete\"\n| stats count_distinct(conversation_id) as conversation_count by user_id\n| sort conversation_count desc\n| limit 10\n\n@timestamp and @message are standard fields for Lambda logs. On line 3, we compute the number of unique conversation identifiers for each user. Thanks to the JSON formatting of the messages, we don’t need to provide parsing instructions to read these fields. The Message complete log message is found in packages/eventhandlers/eventhandlers/messages.py in the sample application.\nThe following query example shows the number of unique users using the system for a given window:\n\nfields , \n| filter  like \"Message complete\"\n| stats count_distinct(user_id) by bin(5m) as unique_users\n\nAgain, we filter for Message complete , compute unique statistics on the user_id field from our JSON messages, and then emit the data as a time series with 5-minute intervals with the bin function.\nCaching (prepared responses)\nMany AI Gateways provide a cache mechanism for assistant messages. This would be appropriate in situations where large numbers of users ask exactly the same questions and need the same exact answers. This could be a considerable cost savings for a busy application in the right situation. A good candidate for caching might be about the weather. For example, with the question “Is it going to rain in NYC today?”, everyone should see the same response. A bad candidate for caching would be one where the user might ask the same thing but would receive private information in return, such as “How many vacation hours do I have right now?” Take care to use this idea safely in your area of work. A basic cache implementation is included in the sample to help you get started with this mechanism. Caches in conversational AI require a lot of care to be taken to make sure information doesn’t leak between users. Given the amount of context an LLM can use to tailor a response, caches should be used judiciously.\nThe following architecture diagram shows the use of DynamoDB as a storage mechanism for prepared responses in the sample application.\n\nThe sample application computes a hash on the user message to query a DynamoDB table with stored messages. If there is a message available for a hash key, the application returns the text to the user, the custom metrics record a cache hit in CloudWatch, and an event is passed back to AppSync Events to notify the application the response is complete. This encapsulates the cache behavior completely within the event structure the application understands.\nInstall the sample application\nRefer to the README file on GitHub for instructions to install the sample application. Both install and uninstall are driven by a single command to deploy or un-deploy the AWS CDK application.\nSample pricing\nThe following table estimates monthly costs of the sample application with light usage in a development environment. Actual cost will vary by how you use the services for your use case.\n\nService\nUnit\nPrice/Unit\nSample Usage\nLinks\n\nAWS Glue\nObjects Stored\nNo additional cost for first million objects stored, $1.00/100,000 above 1 million per month\nLess than 1 million\nhttps://aws.amazon.com/glue/pricing/\n\nAmazon DynamoDB\nRead Request Units\n$0.625 per million write request units\n$2.00\nhttps://aws.amazon.com/dynamodb/pricing/on-demand/\n\nWrite Request Units\n$0.125 per million write request units\n\nAmazon S3 (Standard)\nFirst 50 TB / Month\n$0.023 per GB\n$2.00\nhttps://aws.amazon.com/s3/pricing/\n\nAppSync Events\nEvent API Operations\n$1.00 per million API operations\n$1.00\nhttps://aws.amazon.com/appsync/pricing/\n\nConnection Minutes\n$0.08 per million connection minutes\n\nAmazon Data Firehose\nTB / month\n$0.029 First 500 TB / month\n$1.00\nhttps://aws.amazon.com/firehose/pricing/\n\nFormat Conversion per GB\n$0.018 / GB\n\nAWS Lambda\nRequests\n$0.20 / 1M requests\n$1.00\nhttps://aws.amazon.com/lambda/pricing/\n\nDuration\n$0.0000000067 / GB-seconds / month\n\nAmazon Bedrock\nInput Tokens\n$3.00 / 1M input tokens(Anthropic Claude 4 Sonnet)\n$20.00-$40.00\nhttps://aws.amazon.com/bedrock/pricing/\n\nOutput Tokens\n$15.00 / 1M Output tokens(Anthropic Claude 4 Sonnet)\n\nAWS WAF\nBase\n$5.00 / month\n$8.00\nhttps://aws.amazon.com/waf/pricing/\n\nRules\n$1.00 / rule / month\n\nRequests\n$0.60 / 1M requests\n\nAmazon Cognito\nMonthly Active Users\nFirst 10,000 users, no additional cost\n$0.00\nhttps://aws.amazon.com/cognito/pricing/\n\nAmazon CloudFront\nRequests, Data Transfer Out\nSee pricing\n$0.00\nhttps://aws.amazon.com/cloudfront/pricing/\n\nAmazon CloudWatch\nLogs, Metrics\nSee pricing\n$0.00\nhttps://aws.amazon.com/cloudwatch/pricing/\n\nThe monthly cost of the sample application, assuming light development use, is expected to be between $35–55 per month.\nSample UI\nThe following screenshots showcase the sample UI. It provides a conversation window on the right and a navigation bar on the left. The UI features the following key components:\n\nA Token Usage section is displayed and updated with each turn of the conversation\nThe New Chat option clears the messages from the chat interface so the user can start a new session\nThe model selector dropdown menu shows the available models\n\nThe following screenshot shows the chat interface of the sample application.\n\nThe following screenshot shows the model selection menu.\n\nConclusion\nAs the AI landscape evolves, you need an infrastructure that adapts as quickly as the models themselves. By centering your architecture around AppSync Events and the serverless patterns we’ve covered—including Amazon Cognito based identity authentication, DynamoDB powered metering, CloudWatch observability, and Athena analytics—you can build a foundation that grows with your needs. The sample application presented in this post gives you a starting point that demonstrates real-world patterns, helping developers explore AI integration, architects design enterprise solutions, and technical leaders evaluate approaches.\nThe complete source code and deployment instructions are available in the GitHub repo . To get started, deploy the sample application and explore the nine architectures in action. You can customize the authorization logic to match your organization’s requirements and extend the model selection to include your preferred models on Amazon Bedrock. Share your implementation insights with your organization, and leave your feedback and questions in the comments.\n\nAbout the authors\n\nArchie Cowan is a Senior Prototype Developer on the AWS Industries Prototyping and Cloud Engineering team. He joined AWS in 2022 and has developed software for companies in Automotive, Energy, Technology, and Life Sciences industries. Before AWS, he led the architecture team at ITHAKA, where he made contributions to the search engine on jstor.org and a production deployment velocity increase from 12 to 10,000 releases per year over the course of his tenure there. You can find more of his writing on topics such as coding with ai at fnjoin.com and x.com/archiecowan .",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "AWS在最新文章中详细介绍了如何使用AppSync Events构建无服务器AI Gateway架构。该架构作为中间件模式，旨在提升大型语言模型的可用性、安全性和可观察性，支持低延迟Websocket API事件传播到用户。它集成了多种AWS服务：Amazon Cognito进行身份验证和授权，DynamoDB实现速率限制和计量，CloudWatch提供日志记录和监控，Athena用于数据分析，以及Amazon Bedrock访问多样生成AI模型。关键功能还包括API访问、缓存机制和成本控制，帮助企业构建可扩展、安全的生成AI应用，降低运营成本并提高性能。文章提供了示例代码和定价估算，便于开发者快速部署和定制。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "AWS",
        "AppSync Events",
        "AI Gateway",
        "Amazon Bedrock",
        "DynamoDB"
      ]
    },
    "analyzed_at": "2026-01-27T03:46:02.559856Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "699406b1f79adac0",
    "title": "How Totogi automated change request processing with Totogi BSS Magic and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-totogi-automated-change-request-processing-with-totogi-bss-magic-and-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-26T16:16:25Z",
    "summary": "This blog post describes how Totogi automates change request processing by partnering with the AWS Generative AI Innovation Center and using the rapid innovation capabilities of Amazon Bedrock.",
    "content": "This post is cowritten by Nikhil Mathugar, Marc Breslow and Sudhanshu Sinha from Totogi.\nThis blog post describes how Totogi automates change request processing. Totogi is an AI company focused on helping helping telecom (telco) companies innovate, accelerate growth and adopt AI at scale. BSS Magic , Totogi’s flagship product, connects and models telco business operations, overlaying legacy systems with an AI layer. With BSS Magic, telcos can extend, customize, and modernize their systems without vendor dependencies or lengthy implementations. By partnering with the AWS Generative AI Innovation Center and using the rapid innovation capabilities of Amazon Bedrock , we accelerated the development of BSS Magic, helping Totogi’s customers innovate faster and gain more control over their tech stack.\nIn this post, we explore the challenges associated with the traditional business support system (BSS), and the innovative solutions provided by Totogi BSS Magic. We introduce intricacies of telco ontologies and the multi-agent framework that powers automated change request processing. Additionally, the post will outline the orchestration of AI agents and the benefits of this approach for telecom operators and beyond.\nChallenges with BSS\nBSS are notoriously difficult to manage. A typical BSS stack consists of hundreds of different applications from various vendors. But those BSS applications are difficult to integrate, either restricting telcos to the vendor’s ecosystem or requiring them to invest in costly customizations. Such customizations are slow and resource-intensive because of their reliance on specialized engineering talent.\nEach change request necessitates a thorough analysis of potential impacts across interconnected modules, consuming significant time and effort. Even small updates can involve multiple rounds of coding, testing, and reconfiguration to achieve stability. For telecom operators, where system reliability is critical, these safeguards are non-negotiable, but they come at a steep price. This process is further complicated by the scarcity of engineers with the necessary expertise, driving up costs and elongating timelines. As a result, development cycles for new features or services often take months to complete, leaving operators struggling to meet the demands of a fast-moving market.\nInitiatives like TM Forum’s Open Digital Architecture (ODA) aim to solve this, yet most vendors are slow to adopt such open standards. This dynamic amplifies technical debt and inflates operational expenses.\nBSS Magic solution overview\nTotogi BSS Magic reduces the complexity using AI-generated interoperability, which helps simplify integrations, customizations, and application development. BSS Magic has two key aspects:\n\nA telco ontology that understands the semantic meanings of data structures and the relationships between them, linking disparate data into a coherent network of knowledge.\nMulti-agent framework for fully automated change requests (CR), which reduces CR processing time from 7 days to a few hours.\n\nTelco ontology: The key to interoperability\nOntologies serve as semantic blueprints that detail concepts, relationships, and domain knowledge. In telecom, this means translating the BSS landscape into a clear, reusable, and interoperable ecosystem. Totogi’s telco ontology facilitates a deep understanding of data interaction and seamless integration across any vendor or system. By adopting FAIR principles (Findability, Accessibility, Interoperability, and Reusability), the ontology-driven architecture turns static, siloed data into dynamic, interconnected knowledge assets—unlocking trapped data and accelerating innovation. An overview diagram of the ontology is provided in the following figure.\n\nMulti-agent framework for automated change request processing\nAI agents are advanced software applications trained to perform specific tasks autonomously. Totogi’s BSS Magic AI agents have extensive domain knowledge and use this understanding to manage complex data interactions across multiple vendor systems. These agents automatically generate and test telco-grade code, replacing traditional integrations and customizations with intelligent, AI generated applications. At its core, BSS Magic uses a multi-agent AI approach with feedback loops to automate the entire software development pipeline. Each agent is designed to fulfill a specific role in the development pipeline:\n\nBusiness analysis agent translates unstructured requirements into formal business specifications.\nTechnical architect agent takes these business specs and defines technical architectures, APIs, and dependencies.\nDeveloper agent generates high-quality, deployable code, complete with modular designs and optimizations.\nQA agent validates the code for adherence to best practices, improving quality and security. It provides feedback which is used by the developer agent to update the code.\nTester agent generates robust unit test cases, streamlining validation and deployment. The result of the test cases is used by the developer agent to improve the code.\n\nAn overview of the system is provided in the following figure.\n\nThis integrated pipeline reduces the time to complete a change request from 7 days to a few hours, with minimal human intervention. The prerequisites for implementing the system include an AWS account with access to Amazon Bedrock, AWS Step Functions , AWS Lambda , and configured Amazon credentials. The AI agents are implemented using Anthropic Claude large language models (LLMs) through Amazon Bedrock. State management and workflow coordination are handled by Step Functions for reliable progression through each stage. The AWS infrastructure provides the enterprise-grade reliability, security, and scalability essential for telco-grade solutions.\nTo build the framework, Totogi collaborated with the AWS Generative AI Innovation Center (GenAIIC). GenAIIC offered access to AI expertise, industry-leading talent, and a rigorous iterative process to optimize the AI agents and code-generation workflows. It also provided guidance on prompt engineering, Retrieval Augmented Generation (RAG), model selection, automated code review, feedback loops, robust performance metrics for evaluating AI-generated outputs, and so on. The collaboration helped establish methods for maintaining reliability while scaling automation across the platform. The solution orchestrates multiple specialized AI agents to handle the complete software development lifecycle, from requirements analysis to test execution. The details of the AI agents are given in the following sections.\nMulti-agent orchestration layer\nThe orchestration layer coordinates specialized AI agents through a combination of Step Functions and Lambda functions. Each agent maintains context through RAG and few-shot prompting techniques to generate accurate domain-specific outputs. The system manages agent communication and state transitions while maintaining a comprehensive audit trail of decisions and actions.\nBusiness analysis generation\nThe Business Analyst agent uses Claude’s natural language understanding capabilities to process statement of work (SOW) documents and acceptance criteria. It extracts key requirements using custom prompt templates optimized for telecom BSS domain knowledge. The agent generates structured specifications for downstream processing while maintaining traceability between business requirements and technical implementations.\nTechnical architecture generation\nThe Technical Architect agent transforms business requirements into concrete AWS service configurations and architectural patterns. It generates comprehensive API specifications and data models and incorporates AWS Well-Architected principles. The agent validates architectural decisions against established patterns and best practices, producing infrastructure-as-code templates for automated deployment.\nCode generation pipeline\nThe Developer agent converts technical specifications into implementation code using Claude’s advanced code generation capabilities. It produces robust, production-ready code that includes proper error handling and logging mechanisms. The pipeline incorporates feedback from validation steps to iteratively improve code quality and maintain consistency with AWS best practices.\nAutomated quality assurance\nThe QA agent is built using Claude to perform comprehensive code analysis and validation. It evaluates code quality and identifies potential performance issues. The system maintains continuous feedback loops with the development stage, facilitating rapid iteration and improvement of generated code based on quality metrics and best practices adherence. The QA process consists of carefully crafted prompts.\nQA code analysis prompt:\n\n\"You are a senior QA backend engineer analyzing Python code for serverless applications.\nYour task is to:\nCompare requirements against implemented code\nIdentify missing features\nSuggest improvements in code quality and efficiency\n Provide actionable feedback\nFocus on overall implementation versus minor details\nConsider serverless best practices\"\n\nThis prompt helps the QA agent perform thorough code analysis, evaluate quality metrics, and maintain continuous feedback loops with development stages.\nTest automation framework\nThe Tester agent creates comprehensive test suites that verify both functional and non-functional requirements. It uses Claude to understand test contexts and generate appropriate test scenarios. The framework manages test refinement through evaluation cycles, achieving complete coverage of business requirements while maintaining test code quality and reliability. The testing framework uses a multi-stage prompt approach.\nInitial test structure prompt:\n\n\"As a senior QA engineer, create a pytest-based test structure including:\nDetailed test suite organization\nResource configurations\nTest approach and methodology\nRequired imports and dependencies\"\n\nTest implementation prompt:\n\n\"Generate complete pytest implementation including:\nUnit tests for each function\nIntegration tests for API endpoints\nAWS service mocking\nEdge case coverage\nError scenario handling\"\n\nTest results analysis prompt:\n\n\"Evaluate test outputs and coverage reports to:\nVerify test completion status\nTrack test results and outcomes\nMeasure coverage metrics\nProvide actionable feedback\"\n\nThis structured approach leads to comprehensive test coverage while maintaining high quality standards. The framework currently achieves 76% code coverage and successfully validates both functional and non-functional requirements.\nThe Tester agent provides a feedback loop to the Development agent to improve the code.\nConclusion\nThe integration of Totogi BSS Magic with Amazon Bedrock presents a comprehensive solution for modern telecom operators. Some takeaways for you to consider:\n\nEnd-to-end automation: BSS Magic automates the entire development lifecycle—from idea to deployment. AI agents handle everything from requirements, architecture, and code generation to testing and validation.\nResults: The agentic framework significantly boosted efficiency, reducing change request processing from seven days to a few hours. The automated testing framework achieved 76% code coverage, consistently delivering high-quality telecom-grade code.\nUnique value for telecom operators: By using Totogi BSS Magic, telecom operators can accelerate time-to-market and reduce operational costs. BSS Magic uses autonomous AI, independently managing complex tasks so telecom operators can concentrate on strategic innovation. The solution is supported by Amazon Bedrock, which offers scalable AI models and infrastructure, high-level security and reliability critical for telecom.\nImpact to other industries: While BSS Magic is geared towards the telecom industry, the multi-agent framework can be repurposed for general software development across other industries.\nFuture work: Future enhancements will focus on expanding the model’s domain knowledge in telecom and other domains. Another possible extension is to integrate an AI model to predict potential issues in change requests based on historical data, thereby preemptively addressing common pitfalls.\n\nAny feedback and questions are welcome in the comments below. Contact us to engage AWS Generative AI Innovation Center or to learn more.\n\nAbout the authors\nNikhil Mathugar is a Presales Full Stack Engineer at Totogi, where he designs and implements scalable AWS-based proofs-of-concept across Python and modern JavaScript frameworks. He has over a decade of experience in architecting and maintaining large-scale systems—including web applications, multi-region streaming infrastructures and high-throughput automation pipelines. Building on that foundation, he’s deeply invested in AI—specializing in generative AI, agentic workflows and integrating large-language models to evolve Totogi’s BSS Magic platform.\nMarc Breslow is Field CTO of Totogi, where he is utilizing AI to revolutionize the telecommunications industry. A veteran of Accenture, Lehman Brothers, and Citibank, Marc has a proven track record of building scalable, high-performance systems. At Totogi, he leads the development of AI-powered solutions that drive tangible results for telcos: reducing churn, increasing Average Revenue Per user (ARPU), and streamlining business processes. Marc is responsible for customer proof points demonstrating these capabilities. When not engaging with customers, Marc leads teams building Totogi’s BSS Magic technology, generating applications and improving efficiency using AI agents and workflows.\nSudhanshu Sinha is Chief Technology Officer and a founding team member at Totogi, where he works alongside Acting CEO Danielle Rios to drive the telecom industry’s shift to AI-native software. As the key strategist behind BSS Magic, he shaped its architecture, go-to-market, and early adoption—translating AI-native principles into measurable value for operators. He also helped define Totogi’s Telco Ontology, enabling interoperability and automation across complex BSS landscapes. With over two decades in telecommunications, Sudhanshu blends deep technical insight with commercial acumen to make AI-driven transformation practical and profitable for telcos worldwide.\nParth Patwa is a Data Scientist at the AWS Generative AI Innovation Center, where he works on customer projects using Generative AI and LLMs. He has an MS from University of California Los Angeles. He has published papers in top-tier ML and NLP venues, and has over 1000 citations.\nMofijul Islam is an Applied Scientist II and Tech Lead at the AWS Generative AI Innovation Center, where he helps customers tackle customer-centric research and business challenges using generative AI, large language models (LLM), multi-agent learning, code generation, and multimodal learning. He holds a PhD in machine learning from the University of Virginia, where his work focused on multimodal machine learning, multilingual NLP, and multitask learning. His research has been published in top-tier conferences like NeurIPS, ICLR, AISTATS, and AAAI, as well as IEEE and ACM Transactions.\nAndrew Ang is a Senior ML Engineer with the AWS Generative AI Innovation Center, where he helps customers ideate and implement generative AI proof of concept projects. Outside of work, he enjoys playing squash and watching competitive cooking shows.\nShinan Zhang is an Applied Science Manager at the AWS Generative AI Innovation Center. With over a decade of experience in ML and NLP, he has worked with large organizations from diverse industries to solve business problems with innovative AI solutions, and bridge the gap between research and industry applications.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Totogi公司（一家专注于电信行业的AI公司）与AWS Generative AI Innovation Center合作，利用Amazon Bedrock平台和Anthropic Claude大语言模型，开发了旗舰产品BSS Magic。该产品通过电信本体论理解数据结构语义关系，并采用多代理AI框架，实现电信业务支持系统（BSS）变更请求的自动化处理。传统BSS系统集成复杂、成本高昂，变更请求通常需7天；BSS Magic将其缩短至几小时，显著提升效率。AI代理包括业务分析、技术架构、代码生成、质量保证和测试，自动化整个软件开发生命周期。这帮助电信运营商加速创新、降低运营成本，并展示了多代理框架在软件自动化中的广泛应用潜力。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Totogi",
        "Totogi BSS Magic",
        "Amazon Bedrock",
        "multi-agent framework",
        "Claude LLM"
      ]
    },
    "analyzed_at": "2026-01-27T03:46:12.295214Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "b407300fc1f5f291",
    "title": "How Indeed uses AI to help evolve the job search",
    "url": "https://openai.com/index/indeed-maggie-hulce",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-26T00:00:00Z",
    "summary": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
    "content": "Indeed’s CRO Maggie Hulce shares how AI is transforming job search, recruiting, and talent acquisition for employers and job seekers.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "Indeed的首席营收官Maggie Hulce分享了人工智能如何变革求职、招聘和人才获取过程，针对雇主和求职者。这体现了AI在招聘领域的应用，正帮助进化招聘方式，提高效率和匹配精度。对于雇主，AI可以优化招募流程，通过机器学习算法和自然语言处理技术识别合适候选人；对于求职者，AI能够个性化推荐职位，提升求职体验。这种转型不仅推动了招聘行业向智能化、数据驱动方向迈进，还可能减少人为偏见，促进就业市场公平性，对整个行业产生深远影响。",
      "category": "行业",
      "sentiment": "positive",
      "keywords": [
        "Indeed",
        "AI",
        "job search",
        "recruiting",
        "talent acquisition"
      ]
    },
    "analyzed_at": "2026-01-27T03:45:37.524320Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]