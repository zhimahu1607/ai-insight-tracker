[
  {
    "id": "95c131a52b48b29e",
    "title": "How Thomson Reuters built an Agentic Platform Engineering Hub with Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-thomson-reuters-built-an-agentic-platform-engineering-hub-with-amazon-bedrock-agentcore/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T21:39:42Z",
    "summary": "This blog post explains how TR's Platform Engineering team, a geographically distributed unit overseeing TR's service availability, boosted its operational productivity by transitioning from manual to an automated agentic system using Amazon Bedrock AgentCore.",
    "content": "This post was co-written with Naveen Pollamreddi and Seth Krause from Thomson Reuters.\nThomson Reuters (TR) is a leading AI and technology company dedicated to delivering trusted content and workflow automation solutions. With over 150 years of expertise, TR provides essential solutions across legal, tax, accounting, risk, trade, and media sectors in a fast-evolving world. AI plays a critical role at TR. It’s embedded in how it helps create, enhance, connect, and deliver trusted information to customers. It powers the products used by professionals around the world. AI at TR empowers professionals with professional-grade AI that clarifies complex challenges.\nThis blog post explains how TR’s Platform Engineering team, a geographically distributed unit overseeing TR’s service availability, boosted its operational productivity by transitioning from manual to an automated agentic system using Amazon Bedrock AgentCore .\nBusiness challenge\nPlatform engineering teams face significant challenges in providing seamless, self-service experiences to its internal customers at scale for operational activities such as database management, information security and risk management (ISRM) operations, landing zone maintenance, infrastructure provisioning, secrets management, continuous integration and deployment (CI/CD) pipeline orchestration, and compliance automation. At TR, the Platform Engineering team supports multiple lines of business by providing essential cloud infrastructure and enablement services, including cloud account provisioning and database management. However, manual processes and the need for repeated coordination between teams for operational tasks created delays that slowed down innovation.\n“ Our engineers were spending considerable time answering the same questions and executing identical processes across different teams, ” says Naveen Polalmreddi, Distinguished Engineer at TR. “ We needed a way to automate these interactions while maintaining our security and compliance standards. ”\nCurrent state\nThe Platform Engineering team offers services to multiple product teams within TR including Product Engineering and Service Management. These teams consume their internal home-grown solutions as a service to build and run applications at scale on AWS services. Over a period, these services are offered not only as tools but also through TR’s internal processes, following Information Technology Infrastructure Library (ITIL) standards and using third party software as a service (SaaS) systems.\nSome of these services rely on humans to execute a predefined list of steps and are repeated many times, creating a significant dependency on engineers to execute the same tasks repeatedly for multiple applications. Current processes are semi-automated and are:-\n\nRepetitive and labor intensive – Because of the nature of the workflows and multi-team engagement model, these operational processes tend to be labor intensive and repetitive. The Platform Engineering team spent a lot of time doing work that is undifferentiated heavy lifting.\nLonger time to value – Because of process interdependencies, these operational workflows aren’t fully autonomous and take a long time to realize the value compared to fully automated processes.\nResource and cost intensive – Manual execution requires dedicated engineering resources whose time could be better spent on innovation rather than repetitive tasks. Each operational request consumes engineer hours across multiple teams for coordination, execution, and validation.\n\nThe Platform Engineering team is solving this problem by building autonomous agentic solutions that use specialized agents across multiple service domains and groups. The cloud account provisioning agent automates the creation and configuration of new cloud accounts according to internal standards, handling tasks such as setting up organizational units, applying security policies, and configuring baseline networking. The database patching agent manages the end-to-end database patching lifecycle, version upgrades. Network service agents handle network configuration requests such as VPC setup, subnet allocation, and connectivity establishment between environments. Architecture review agents assist in evaluating proposed architectures against best practices, security requirements, and compliance standards, providing automated feedback and recommendations. AgentCore serves as the foundational orchestration layer for these agents, providing the core agentic capabilities that enable intelligent decision-making, natural language understanding, tool calling and agent-to-agent (A2A) communication.\nSolution overview\nTR’s Platform Engineering team built this solution with scalability, extensibility, and security as core principles and designed it so that non-technical users can quickly create and deploy AI-powered automation. Designed for a broad enterprise audience, the architecture is designed so that business users can interact with specialized agents through basic natural language requests without needing to understand the underlying technical complexity. TR chose Amazon Bedrock AgentCore because it provides the complete foundational infrastructure needed to build, deploy, and operate enterprise-grade AI agents at scale without having to build that infrastructure from scratch. The Platform Engineering team gained the flexibility to innovate with their preferred frameworks while designing their autonomous agents operate with enterprise-level security, reliability, and scalability—critical requirements for managing production operational workflows at scale.\nThe following diagram illustrates the architecture of solution:\n\nTR built an AI-powered platform engineering hub using AgentCore. The solution consists of:\n\nA custom web portal for more secure agent interactions\nA central orchestrator agent that routes requests and manages interactions\nMultiple service-specific agents handling specialized tasks such as AWS account provisioning and database patching\nA human-in-the-loop validation service for sensitive operations\n\nTR decided to use AgentCore because it helped their developers to accelerate from prototype to production with fully managed services that minimize infrastructure complexity and build AI agents using different frameworks, models, and tools while maintaining complete control over how agents operate and integrate with their existing systems.\nSolution workflow\nThe team used the following workflow to develop and deploy the agentic AI system.\n\nDiscovery and architecture planning: Evaluated existing AWS resources and code base to design a comprehensive solution incorporating AgentCore, focusing on service objectives and integration requirements.\nCore development and migration: Developed a dual-track approach by migrating existing solutions to AgentCore while building TRACK (deployment engine), enabling rapid agent creation. Implemented a registry system as a modular bridge between the agent and the orchestrator.\nSystem enhancement and deployment: Refined orchestrator functionality, developed an intuitive UX , and executed a team onboarding process for the new agentic system deployment.\n\nBuilding the orchestrator agent\nTR’s Platform Engineering team designed their orchestrator service, named Aether, as a modular system using the LangGraph Framework. The orchestrator retrieves context from their agent registry to determine the appropriate agent for each situation. When an agent’s actions are required, the orchestrator makes a tool call that programmatically populates data from the registry, helping prevent potential prompt injection attacks and facilitating more secure communication between endpoints.\nTo maintain conversation context while keeping the system stateless, the orchestrator integrates with the AgentCore Memory service capabilities at both conversation and user levels. Short-term memory maintains context within individual conversations, while long-term memory tracks user preferences and interaction patterns over time. This dual-memory approach allows the system to learn from past interactions and avoid repeating previous mistakes.\nService Agent Development Framework\nThe Platform Engineering team developed their own framework, TR-AgentCore-Kit (TRACK), to simplify agent deployment across the organization. TRACK, which is a homegrown solution utilizes a customized version of the Bedrock AgentCore Starter Toolkit . The team customized this toolkit to meet TR’s specific compliance alignment requirements, which include asset identification standards and resource tagging standards. The framework handles connection to AgentCore Runtime , tool management, AgentCore Gateway connectivity, and baseline agent setup, so developers can focus on implementing business logic rather than dealing with infrastructure concerns. AgentCore Gateway provided a straightforward and more secure way for developers to build, deploy, discover, and connect to tools at scale. TRACK also handles the registration of service agents into the Aether environment by deploying agent cards into the custom-built A2A registry. TRACK maintains a seamless flow for developers by offering deployment capabilities to AWS and registration to the custom-built services in one package. By deploying the agent cards into the registry, the process to fully onboard an agent built by a service team can continue to make the agent available from the overarching orchestrator.\nAgent discovery and registration system\nTo enable seamless agent discovery and communication, TR implemented a custom A2A solution using Amazon DynamoDB and Amazon API Gateway . This system supports cross-account agent calls, which was essential for their modular architecture. The registration process occurs through the TRACK project, so that teams can register their agents directly with the orchestrator service. The A2A registry maintains a comprehensive history of agent versions for auditing purposes and requires human validation before allowing new agents into the production environment. This governance model facilitates conformance with TR’s ISRM standards while providing flexibility for future expansion.\nAether web portal integration\nThe team developed a web portal using React, hosted on Amazon Simple Storage Service (Amazon S3), to provide a more secure and intuitive interface for agent interactions. The portal authenticates users against TR’s enterprise single sign-on (SSO) and provides access to agent flows based on user permissions. This approach helps ensure that sensitive operations, such as AWS account provisioning or database patching, are only accessible to authorized personnel.\nHuman-in-the-loop validation service\nThe system includes Aether Greenlight, a validation service that makes sure critical operations receive appropriate human oversight. This service extends beyond basic requester approval, so that team members outside the initial conversation can participate in the validation process. The system maintains a complete audit trail of approvals and actions, supporting TR’s compliance requirements.\nOutcome\nBy building a self-service agentic system on AgentCore, TR implemented autonomous agents that use AI orchestration to handle complex operational workflows end-to-end.\nProductivity and efficiency\n\n15-fold productivity gain through intelligent automation of routine tasks\n70% automation rate achieved at first launch, dramatically reducing manual workload\nContinuous reliability with repeatable runbooks executed by agents around the clock\n\nSpeed and agility\n\nFaster time to value : Accelerated product delivery by automating environment setup, policy enforcement, and day-to-day operations\nSelf-service workflows : Empowered teams with clear standards and paved-road tooling\n\nSecurity and compliance\n\nStronger security posture : Applied guardrails and database patching by default\nHuman-in-the-loop approvals : Maintained oversight while automating verification of changes\n\nCost and resource optimization\n\nBetter cost efficiency: Automated infrastructure usage optimization\nStrategic talent allocation: Freed engineering teams to focus on highest-priority, high-value work\nReduced operational toil: Removed repetitive tasks and variance through standardization\n\nDeveloper experience\n\nImproved satisfaction: Streamlined workflows with intuitive self-service capabilities\nConsistent standards: Established repeatable patterns for other teams to adopt and scale\n\nConclusion\nThis agentic system described in this post establishes a replicable pattern that teams across the organization can use to adopt similar automation capabilities, creating a multiplier effect for operational excellence. The Aether project aims to help enhance the experience of engineers by removing the need for manual execution of tasks that could be automated to support further innovation and creative thinking. As Aether continues to improve, the team hopes that the pattern will be adopted more broadly to begin assisting teams beyond Platform Engineering to break-through productivity standards organization wide, solidifying TR as a front-runner in the age of artificial intelligence.\nUsing Amazon Bedrock AgentCore, TR transformed their platform engineering operations from manual processes to an AI-powered self-service hub. This approach not only improved efficiency but also strengthened security and compliance controls.\nReady to transform your platform engineering operations:\n\nExplore AgentCore\nExplore AgentCore documentation\nFor additional use cases, explore notebook-based tutorials\n\nAbout the Authors\nNaveen Pollamreddi is a Distinguished Engineer in Thomson Reuters as part of the Platform Engineering team and drives the Agentic AI strategy for Cloud Infrastructure services.\nSeth Krause is a Cloud Engineer on Thomson Reuters’ Platform Engineering Compute team. Since joining the company, he has contributed to architecting and implementing generative AI solutions that enhance productivity across the organization. Seth specializes in building cloud-based microservices with a current focus on integrating AI capabilities into enterprise workflows.\nPratip Bagchi is an Enterprise Solutions Architect at Amazon Web Services. He is passionate about helping customers to drive AI adoption and innovation to unlock business value and enterprise transformation.\nSandeep Singh is a Senior Generative AI Data Scientist at Amazon Web Services, helping businesses innovate with generative AI. He specializes in generative AI, machine learning, and system design. He has successfully delivered state-of-the-art AI/ML-powered solutions to solve complex business problems for diverse industries, optimizing efficiency and scalability.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Thomson Reuters（TR）的Platform Engineering团队使用Amazon Bedrock AgentCore构建了一个代理平台工程中心，将手动操作任务自动化。该系统通过名为Aether的编排器和TRACK框架，自动化了数据库管理、云账户配置、网络服务等复杂工作流，基于AWS服务如DynamoDB和API Gateway。此举解决了重复性任务导致的效率低下和资源浪费问题，实现了15倍的效率提升、70%的自动化率，并增强了安全性和合规性。影响包括优化工程资源分配、加速产品交付，并为组织内其他团队提供了可扩展的自动化模式，促进了AI在企业的应用。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "Thomson Reuters",
        "Amazon Bedrock AgentCore",
        "Aether",
        "TRACK",
        "AWS"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:05.578984Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "46e3ec6f68b3b2c8",
    "title": "Build agents to learn from experiences using Amazon Bedrock AgentCore episodic memory",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-agents-to-learn-from-experiences-using-amazon-bedrock-agentcore-episodic-memory/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T19:45:04Z",
    "summary": "In this post, we walk you through the complete architecture to structure and store episodes, discuss the reflection module, and share compelling benchmarks that demonstrate significant improvements in agent task success rates.",
    "content": "Today, most agents operate only on what’s visible in the current interaction: they can access facts and knowledge, but they can’t remember how they solved similar problems before or why certain approaches worked or failed. This creates a significant gap in their ability to learn and improve over time. Amazon Bedrock AgentCore episodic memory addresses this limitation by capturing and surfacing experience-level knowledge for AI agents. Although semantic memory helps an agent remember what it knows, episodic memory documents how it arrived there: the goal, reasoning steps, actions, outcomes, and reflections. By converting each interaction into a structured episode, you can enable agents to recall knowledge and interpret and apply prior reasoning. This helps agents adapt across sessions, avoid repeating mistakes, and evolve their planning over time.\nAmazon Bedrock AgentCore Memory is a fully managed service that helps developers create context-aware AI agents through both short-term memory and long-term intelligent memory capabilities. To learn more, see Amazon Bedrock AgentCore Memory: Building context-aware agents and Building smarter AI agents: AgentCore long-term memory deep dive .\nIn this post, we walk you through the complete architecture to structure and store episodes, discuss the reflection module, and share compelling benchmarks that demonstrate significant improvements in agent task success rates.\nKey challenges in designing agent episodic memory\nEpisodic memory enables agents to retain and reason over their own experiences. However, designing such a system requires solving several key challenges to make sure experiences remain coherent, evaluable, and reusable:\n\nMaintaining temporal and causal coherence – Episodes need to preserve the order and cause-effect flow of reasoning steps, actions, and outcomes so the agent can understand how its decisions evolved.\nDetecting and segmenting multiple goals – Sessions often involve overlapping or shifting goals. The episodic memory must identify and separate them to avoid mixing unrelated reasoning traces.\nLearning from experience – Each episode should be evaluated for success or failure. Reflection should then compare similar past episodes to identify generalizable patterns and principles, enabling the agent to adapt those insights to new goals rather than replaying prior trajectories.\n\nIn the next section, we describe how to build an AgentCore episodic memory strategy, covering its extraction, storage, retrieval, and reflection pipeline and how these components work together to help transform experience into adaptive intelligence.\nHow AgentCore episodic memory works\nWhen your agentic application sends conversational events to AgentCore Memory, raw interactions get transformed into rich episodic memory records through an intelligent extraction and reflection process. The following diagram illustrates how this episodic memory strategy works and how simple agent conversations become meaningful, reflective memories that shape future interactions.\n\nThe following diagram illustrates the detailed data flow of the same architecture with more elaborate details.\n\nThe preceding diagrams illustrate the different steps in the episodic memory strategy. The first two steps (marked pink and purple) are grouped together as a two-stage approach of the episode extraction module that serves distinct but complementary purposes. The third step (marked as blue) is the reflection module, which helps the agent learn from the past experience. In the following sections, we discuss the steps in detail.\nEpisode extraction module\nThe episode extraction module is the foundational step in the episodic strategy that transforms raw user-agent interaction data into structured, meaningful episodes. We follow a two-stage approach where the stages are designed to capture both granular step-wise mechanics of each interaction (called turn extraction) and broader episode-wise knowledge to create coherent narratives (called episode extraction). To make an analogy, think of it in terms of taking notes during a meeting (turn level) and writing the meeting summary at the end of the meeting (episode). Both stages are valuable but serve different purposes when learning from experience.\nIn the first stage of episode extraction, the system performs turn-level processing to understand what went right or wrong. Here, single exchange units between the user and the agent called conversational turns are identified, segmented, and transformed into structured summaries in the following dimensions:\n\nTurn situation – A brief description of the circumstances and context that the assistant is responding to in this turn. This includes the immediate context, the user’s overarching objectives that might span multiple turns, and the relevant history from previous interactions that informed the current exchange.\nTurn intent – The assistant’s specific purpose and primary goal for this turn, essentially answering the question “What was the assistant trying to accomplish in this moment?”\nTurn action – A detailed record of the concrete steps taken during the interaction, documenting which specific tools were used, what input arguments or parameters were provided to each tool, and how the assistant translated intent into executable actions.\nTurn thought – The reasoning behind the assistant’s decisions, explaining the “why” behind tool selection and approach.\nTurn assessment – An honest evaluation of whether the assistant successfully achieved its stated goal for this specific turn, providing immediate feedback on the effectiveness of the chosen approach and actions taken.\nGoal assessment – A broader perspective on whether the user’s overall objective across the entire conversation appears to be satisfied or progressing toward completion, looking beyond individual turns to evaluate holistic success.\n\nAfter processing and structuring individual turns, the system proceeds to the episode extraction stage, when a user completes their goal (detected by the large language model) or an interaction ends. This helps capture the complete user journey, because a user’s goal often spans multiple turns and individual turn data alone can’t convey whether the overall objective was achieved or what the holistic strategy looked like. In this stage, sequentially related turns are synthesized into coherent episodic memories that capture complete user journeys, from initial request to final resolution:\n\nEpisode situation – The broader circumstances that initiated the user’s need for assistance\nEpisode intent – A clear articulation of what the user ultimately wanted to accomplish\nSuccess evaluation – A definitive assessment of whether the conversation achieved its intended purpose for each episode\nEvaluation justification – Concrete reasoning for success or failure assessments, grounded in specific conversational moments that demonstrate progress toward or away from user goals\nEpisode insights – Insights capturing proven effective approaches and identifying pitfalls to avoid for the current episode\n\nReflection module\nThe reflection module highlights the ability of Amazon Bedrock AgentCore episodic memory to learn from past experiences and generate insights that help improve future performance. This is where individual episode learnings evolve into generalizable knowledge that can guide agents across diverse scenarios.\nThe reflection module operates through cross-episodic reflection, retrieving past similar successful episodes based on user intent and reflecting across multiple episodes to achieve more generalizable insights. When new episodes are processed, the system performs the following actions:\n\nUsing the user intent as a semantic key, the system identifies historically successful and relevant episodes from the vector store that share similar goals, contexts, or problem domains.\nThe system analyzes patterns across the main episode and relevant episodes, looking for transferable insights about what approaches work consistently across different contexts.\nExisting reflection knowledge is reviewed and either enhanced with new insights or expanded with entirely new patterns discovered through cross-episodic analysis.\n\nAt the end of the process, each reflection memory record contains the following information:\n\nUse case – When and where the insight applies, including relevant user goals and trigger conditions\nHints (insights) – Actionable guidance covering tool selection strategies, effective approaches, and pitfalls to avoid\nConfidence scoring – A score (0.1–1.0) indicating how well the insight generalizes across different scenarios\n\nEpisodes provide agents with concrete examples of how similar problems were solved before. These case studies show the specific tools used, reasoning applied, and outcomes achieved, including both successes and failures. This creates a learning framework where agents can follow proven strategies and avoid documented mistakes.\nReflection memories extract patterns from multiple episodes to deliver strategic insights. Instead of individual cases, they reveal which tools work best, what decision-making approaches succeed, and which factors drive outcomes. These distilled principles give agents higher-level guidance for navigating complex scenarios.\nCustom override configurations\nAlthough built-in memory strategies cover the common use cases, many domains require tailored approaches for memory processing. The system supports built-in strategy overrides through custom prompts that extend the built-in logic, helping teams adapt memory handling to their specific requirement. You can implement the following custom override configurations:\n\nCustom prompts – These prompts focus on criteria and logic rather than output formats and help developers define the following:\n\nExtraction criteria – What information gets extracted or filtered out.\nConsolidation rules – How related memories should be consolidated.\nConflict resolution – How to handle contradictory information.\nInsight generation – How cross-episode reflections are synthesized.\n\nCustom model: AgentCore Memory supports custom model selection for memory extraction, consolidation, and reflection operations. This flexibility helps developers balance accuracy and latency based on their specific requirements. You can define them using APIs when you create the _memory_resource_ as a strategy override or through the Amazon Bedrock AgentCore console (as shown in the following screenshot).\nNamespaces:  Namespaces provide a hierarchical organization for episodes and reflections, enabling access to your agent’s experiences at different levels of granularity and providing a seamless natural logical grouping. For instance, to design a namespace for a travel application, episodes could be stored under travel_booking/users/userABC/episodes and reflections could reside at travel_booking/users/userABC . Note that the namespace for reflections must be a sub-path of the namespace for episodes.\n\nPerformance evaluation\nWe evaluated Amazon Bedrock AgentCore episodic memory on real-world goal completion benchmarks from the retail and airline domain (sampled from τ2-bench) . These benchmarks contain tasks that mirror actual customer service scenarios where agents need to help users achieve specific goals.\nWe compared three different setups in our experiments:\n\nFor the baseline , we ran the agent (built with Anthropic’s Claude 3.7) without interacting with the memory component.\nFor memory-augmented agents, we explored two methods of using memories:\n\nIn-context learning examples – The first method uses extracted episodes as in-context learning examples. Specifically, we constructed a tool named retrieve_exemplars (tool definition in appendix) that agents can use by issuing a query (for example, “how to get refund?”) to get step-by-step instructions from the episodes repository. When agents face similar problems, the retrieved episodes will be added into the context to guide the agent to take the next action.\nReflection-as-guidance – The second method we explored is reflection-as-guidance. Specifically, we construct a tool named retrieve_reflections (tool definition in appendix) that agents can use to access broader insights from past experiences. Similar to retrieve_exemplars , the agent can generate a query to retrieve reflections as context, gaining insights to make informed decisions about strategy and approach rather than specific step-by-step actions.\n\nWe used the following evaluation methodology:\n\nThe baseline agent first processes a set of historical customer interactions, which become the source for memory extraction.\nThe agent then receives new user queries from τ2-bench.\nEach query is attempted four times in parallel.\nTo evaluate, pass rate metrics are measured across these four attempts. Pass^k measures the percentage of tasks where the agent succeeded in at least k out of four attempts:\n\nPass^1: Succeeded at least once (measures capability)\nPass^2: Succeeded at least twice (measures reliability)\nPass^3: Succeeded at least three times (measures consistency)\n\nThe results in the following table show clear improvements across both domains and multiple attempts.\n \n\nSystem\nMemory Type used by Agent\nRetail\n\nPass^1\nPass^2\nPass^3\nPass^1\nPass^2\nPass^3\n\nBaseline\nNo Memory\n65.80%\n49.70%\n42.10%\n47%\n33.30%\n24%\n\nMemory-Augmented Agent\nEpisodes as ICL Example\n69.30%\n53.80%\n43.40%\n55.00%\n46.70%\n43.00%\n\nCross Episodes Reflection Memory\n77.20%\n64.30%\n55.70%\n58%\n46%\n41%\n\nMemory-augmented agents consistently outperform the baseline across domains and consistency levels. Crucially, these results demonstrate that different memory retrieval strategies are better suited to different task characteristics. Cross-episode reflection improved Pass^1 by +11.4% and Pass^3 by +13.6% over the baseline, suggesting that generalized strategic insights are particularly valuable when handling open-ended customer service scenarios with diverse interaction patterns. In contrast, the airline domain – characterized by complex, rule-based policies and multi-step procedures—benefits more from episodes as examples, which achieved the highest Pass^3 (43.0% vs 41.0% for reflection). This indicates that concrete step-by-step examples help agents navigate structured workflows reliably. The relative improvement is most pronounced at higher consistency thresholds (Pass^3), where memory helps agents avoid the mistakes that cause intermittent failures.\nBest practices for using episodic memory\nThe key to effective episodic memory is knowing when to use it and which type fits your situation. In this section, we discuss what we’ve learned works best.\nWhen to use episodic memory\nEpisodic memory delivers the most value when you match the right memory type to your current need. It is ideal for complex, multi-step tasks where context matters and past experience matters significantly, such as debugging code, planning trips, and analyzing data. It’s also particularly valuable for repetitive workflows where learning from previous attempts can dramatically improve outcomes, and for domain-specific problems where accumulated expertise makes a real difference.\nHowever, episodic memory isn’t always the right choice. You can skip it for simple, one-time questions like weather checks or basic facts that don’t need reasoning or context. Simple customer service conversations, basic Q&A, or casual chats don’t need the advanced features that episodic memory adds. The true benefit of episodic memory is observed over time. For short tasks, a session summary provides sufficient information. However, for complex tasks and repetitive workflows, episodic memory helps agents build on past experiences and continuously improve their performance.\nChoosing episodes vs. reflection\nEpisodes work best when you’re facing similar specific problems and need clear guidance. If you’re debugging a React component that won’t render, episodes can show you exactly how similar problems were fixed before, including the specific tools used, thinking process, and results. They give you real examples when general advice isn’t enough, showing the complete path from finding the problem to solving it.\nReflection memories work best when you need strategic guidance across broader contexts rather than specific step-by-step solutions. Use reflections when you’re facing a new type of problem and need to understand general principles, like “What’s the most effective approach for data visualization tasks?” or “Which debugging strategies tend to work best for API integration issues?” Reflections are particularly valuable when you’re making high-level decisions about tool selection and which method to follow, or understanding why certain patterns consistently succeed or fail.\nBefore starting tasks, check reflections for strategy guidance, look at similar episodes for solution patterns, and find high-confidence mistakes documented in previous attempts. During tasks, look at episodes when you hit roadblocks, use reflection insights for tool choices, and think about how your current situation differs from past examples.\nConclusion\nEpisodic memory fills a critical gap in current agent capabilities. By storing complete reasoning paths and learning from outcomes, agents can avoid repeating mistakes and build on successful strategies.\nEpisodic memory completes the memory framework of Amazon Bedrock AgentCore alongside summarization, semantic, and preference memory. Each serves a specific purpose: summarization manages context length, semantic memory stores facts, preference memory handles personalization, and episodic memory captures experience. The combination helps give agents both structured knowledge and practical experience to handle complex tasks more effectively.\nTo learn more about episodic memory, refer to Episodic memory strategy , How to best retrieve episodes to improve agentic performance , and the AgentCore Memory GitHub samples .\n\nAppendix\nIn this section, we discuss two methods of using memories for memory-augmented agents.\nEpisode example\nThe following is an example using extracted episodes as in-context learning examples:\n\n** Context **\nA customer (Jane Doe) contacted customer service expressing frustration\nabout a recent flight delay that disrupted their travel plans and wanted\nto discuss compensation or resolution options for the inconvenience they\nexperienced.\n\n** Goal **\nThe user's primary goal was to obtain compensation or some form of resolution\nfor a flight delay they experienced, seeking acknowledgment of the disruption\nand appropriate remediation from the airline.\n\n---\n\n### Step 1:\n\n**Thought:**\nThe assistant chose to gather information systematically rather than making\nassumptions, as flight delay investigations require specific reservation and\nflight details. This approach facilitates accurate assistance and demonstrates\nprofessionalism by acknowledging the customer's frustration while taking concrete\nsteps to help resolve the issue.\n\n**Action:**\nThe assistant responded conversationally without using any tools, asking the\nuser to provide their user ID to access reservation details.\n\n--- End of Step 1 ---\n\n...\n\n** Episode Reflection **:\nThe conversation demonstrates an excellent systematic approach to flight\nmodifications: starting with reservation verification, then identifying\nconfirmation, followed by comprehensive flight searches, and finally processing\nchanges with proper authorization. The assistant effectively used appropriate\ntools in a logical sequence - get_reservation_details for verification, get_user_details\nfor identity/payment info, search_direct_flight for options, and update tools for\nprocessing changes. Key strengths included transparent pricing calculations,\nproactive mention of insurance benefits, clear presentation of options, and proper\nhandling of policy constraints (explaining why mixed cabin classes aren't allowed).\nThe assistant successfully leveraged user benefits (Gold status for free bags) and\nmaintained security protocols throughout. This methodical approach made sure user\nneeds were addressed while following proper procedures for reservation modifications.\n\nReflection example\nThe following is an example of Reflection memory, which can be used for agent guidance:\n\n**Title:** Proactive Alternative Search Despite Policy Restrictions\n\n**Use Cases:**\nThis applies when customers request flight modifications or changes that\nare blocked by airline policies (such as basic economy no-change rules,\nfare class restrictions, or booking timing limitations). Rather than simply\ndeclining the request, this pattern involves immediately searching for\nalternative solutions to help customers achieve their underlying goals.\nIt's particularly valuable for emergency situations, budget-conscious travelers,\nor when customers have specific timing needs that their current reservations\ndon't accommodate.\n\n**Hints:**\nWhen policy restrictions prevent the requested modification, immediately pivot\nto solution-finding rather than just explaining limitations. Use search_direct_flight\nto find alternative options that could meet the customer's needs, even if it requires\nseparate bookings or different approaches. Present both the policy constraint\nexplanation AND viable alternatives in the same response to maintain momentum toward\nresolution. Consider the customer's underlying goal (getting home earlier,\nchanging dates, etc.) and search for flights that accomplish this objective.\nWhen presenting alternatives, organize options clearly by date and price, highlight\nbudget-friendly choices, and explain the trade-offs between keeping existing reservations\nversus canceling and rebooking. This approach transforms policy limitations into problem-solving\nopportunities and maintains customer satisfaction even when the original request cannot be fulfilled.\n\nTool definitions\nThe following code is the tool definition for retrieve_exemplars :\ndef retrieve_exemplars(task: str) -> str:\n\"\"\"\nRetrieve example processes to help solve the given task.\nArgs:\ntask: The task to solve that requires example processes.\n\nReturns:\nstr: The example processes to help solve the given task.\n\"\"\"\n\nThe following is the tool definition for retrieve_reflections :\ndef retrieve_reflections(task: str, k: int = 5) -> str:\n\"\"\"\nRetrieve synthesized reflection knowledge from past agent experiences by matching\nagainst knowledge titles and use cases. Each knowledge entry contains: (1) a descriptive title,\n(2) specific use cases describing the types of goals where this knowledge applies and when to apply it,\nand (3) actionable hints including best practices from successful episodes and common pitfalls to avoid\nfrom failed episodes. Use this to get strategic guidance for similar tasks.\n\nArgs:\ntask: The current task or goal you are trying to accomplish. This will be matched\nagainst knowledge titles and use cases to find relevant reflection knowledge. Describe your task\nclearly to get the most relevant matches.\nk: Number of reflection knowledge entries to retrieve. Default is 5.\n\nReturns:\nstr: The synthesized reflection knowledge from past agent experiences.\n\"\"\"\n\nAbout the Authors\nJiarong Jiang is a Principal Applied Scientist at AWS, driving innovations in Retrieval Augmented Generation (RAG) and agent memory systems to improve the accuracy and intelligence of enterprise AI. She’s passionate about helping customers build context-aware, reasoning-driven applications that use their own data effectively.\nAkarsha Sehwag is a Generative AI Data Scientist for the Amazon Bedrock AgentCore Memory team. With over 6 years of expertise in AI/ML, she has built production-ready enterprise solutions across diverse customer segments in generative AI, deep learning, and computer vision domains. Outside of work, she likes to hike, bike, and play badminton.\nMani Khanuja is a Principal Generative AI Specialist SA and author of the book Applied Machine Learning and High-Performance Computing on AWS. She leads machine learning projects in various domains such as computer vision, natural language processing, and generative AI. She speaks at internal and external conferences such AWS re:Invent, Women in Manufacturing West, YouTube webinars, and GHC 23. In her free time, she likes to go for long runs along the beach.\nPeng Shi is a Senior Applied Scientist at AWS, where he leads advancements in agent memory systems to enhance the accuracy, adaptability, and reasoning capabilities of AI. His work focuses on creating more intelligent and context-aware applications that bridge cutting-edge research with real-world impact.\nAnil Gurrala is a Senior Solutions Architect at AWS based in Atlanta. With over 3 years at Amazon and nearly two decades of experience in digital innovation and transformation, he helps customers with modernization initiatives, architecture design, and optimization on AWS. Anil specializes in implementing agentic AI solutions while partnering with enterprises to architect scalable applications and optimize their deployment within the AWS cloud environment. Outside of work, Anil enjoys playing volleyball and badminton, and exploring new destinations around the world.\nRuo Cheng is a Senior UX Designer at AWS, designing enterprise AI and developer experiences across Amazon Bedrock and Amazon Bedrock AgentCore. With a decade of experience, she leads design for AgentCore Memory, shaping memory-related workflows and capabilities for agent-based applications. Ruo is passionate about translating complex AI and infrastructure concepts into intuitive, user-centered experiences.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊 AWS 发布了 Amazon Bedrock AgentCore episodic memory，这是一个全托管服务，旨在解决 AI agents 无法从过去经验中学习的核心局限。该系统通过捕获和利用 episodic memory，将每个交互转化为结构化情节，记录目标、推理步骤、行动、结果和反思，使 agents 能够回忆和应用先验知识。关键组件包括情节提取模块（负责将原始交互转化为结构化情节）和反思模块（从多情节中生成泛化洞察），这显著提升了 agents 在跨会话中的适应能力，帮助避免重复错误并改进规划。性能评估显示，在零售和航空领域的真实基准测试中，使用该记忆系统的 agents 任务成功率大幅提升，尤其在跨情节反思策略下，Pass^3 指标在零售领域比基线提高了 13.6%。这一创新填补了当前 agent 能力的关键空白，适用于复杂多步骤任务，并增强了 AI 应用的长期学习与改进能力。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon Bedrock AgentCore",
        "episodic memory",
        "AI agents",
        "Amazon AWS",
        "记忆系统"
      ]
    },
    "analyzed_at": "2026-01-22T03:41:53.420495Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "999d9c001173fd10",
    "title": "How bunq handles 97% of support with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-bunq-handles-97-of-support-with-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T17:50:35Z",
    "summary": "In this post, we show how bunq upgraded Finn, its in-house generative AI assistant, using Amazon Bedrock to transform user support and banking operations to be seamless, in multiple languages and time zones.",
    "content": "This post was co-authored with Benjamin Kleppe, Machine Learning Engineering Lead at bunq .\nThe integration of agentic AI is transforming the banking industry, marking a significant shift from traditional customer service systems. Agentic AI demonstrates autonomous decision-making capabilities in complex financial environments, enabling banks to provide round-the-clock multilingual support, process transactions, and deliver personalized financial insights at scale.\nbunq is Europe’s second-largest neobank, built to make life easy for people and businesses who live an international lifestyle. Founded in 2012 by serial entrepreneur Ali Niknam, bunq has always put users at the heart of everything they do. The company helps its 20 million users across Europe spend, save, budget, and invest confidently, all within a single, user-friendly application built on user feedback\nIn this post, we show how bunq upgraded Finn , its in-house generative AI assistant, using Amazon Bedrock to transform user support and banking operations to be seamless, in multiple languages and time zones.\nBusiness challenge\nBanks face a major challenge to deliver consistent, high-quality customer support across multiple channels, languages, and time zones. Traditional support systems struggle with the complexity of financial products, regulatory requirements, and the growing expectation for instant, accurate responses. Customers expect instant access to essential banking functions like transaction disputes, account management, and financial advice, and banks need to maintain strict security protocols and compliance standards. As a user-centric bank, bunq’s users expect round-the-clock support for their banking needs, such as requesting a refund or seeking guidance on features. Traditional support models couldn’t keep up with this demand, creating frustrating bottlenecks and straining internal resources. Beyond direct support, bunq’s team also needed efficient ways to analyze incoming feature requests and bug reports to continuously improve their system. It was clear that bunq needed a smarter solution that could provide instant, accurate assistance around the clock and help the team turn valuable user feedback into action.\nSolution overview\nLaunched in 2023, bunq’s generative AI assistant, Finn, is fully built in-house as part of bunq’s proprietary AI stack. Finn uses leading AI foundation models (FMs) and tooling, including Anthropic’s Claude models through Amazon Bedrock. Unlike generic chatbots, Finn processes natural language and provides real-time, intelligent answers. Finn can translate the bunq application into 38 languages and translate speech-to-speech calls to the support team in real time. It can also summarize complex banking information, provide financial insights and budgeting advice, and even recognize images, automating tedious tasks such as invoice processing. bunq’s approach uses AWS services to create a scalable AI agent infrastructure that can handle the demands of modern banking while maintaining security and compliance. The solution uses the following AWS services:\n\nAmazon Bedrock – A fully managed service that makes high-performing FMs from leading AI companies and Amazon available through a unified API. bunq uses Amazon Bedrock to access Anthropic’s Claude models with enhanced security features, scalability, and compliance—critical requirements for banking applications.\nAmazon Elastic Container Service (Amazon ECS) – A fully managed container orchestration service that makes it straightforward to deploy, manage, and scale containerized applications. Amazon ECS alleviates the need to install and operate container orchestration software or manage clusters of virtual machines, helping bunq focus on building Finn’s multi-agent architecture.\nAmazon DynamoDB – A fully managed, serverless, NoSQL database service designed to run high-performance applications at scale. DynamoDB delivers single-digit millisecond performance and stores agent memory, conversation history, and session data, enabling Finn to maintain context across customer interactions.\nAmazon OpenSearch Serverless – An on-demand, automatic scaling configuration for Amazon OpenSearch Service . OpenSearch Serverless automatically scales compute resources based on application needs and provides vector search capabilities for Finn’s Retrieval Augmented Generation (RAG) implementation, enabling semantic search across bunq’s knowledge base.\n\nBuilding a multi-agent implementation with Amazon Bedrock\nUsers can interact with Finn through bunq’s application and web interface, using natural language for their requests, such as account information, transaction history, financial advice, and support issues. The system processes requests in real time, accessing only pertinent data to the request, while maintaining strict security and privacy controls. User support scenarios demand more than what a single AI agent can deliver. A multi-agent architecture allows specialized agents to handle distinct tasks—one agent might excel at understanding the user, another focuses on extracting relevant documentation, and a third handles transaction analysis or account operations. For Finn, this means a user asking about a failed payment can trigger a coordinated response: one agent interprets the question, another checks transaction logs, and a third suggests solutions based on similar cases. They all work together seamlessly to deliver a comprehensive answer in seconds, instead of bouncing the user between departments. The initial multi-agent support system for banking services followed a seemingly straightforward pattern: a central router agent directed user queries to specialized sub-agents. Each agent handled specific domains—technical support, general inquiries, transaction status, account management, and so on. However, as the system grew, so did the size and complexity of the demands. As bunq added more specialized agents to handle the new ecosystem, three issues became apparent:\n\nRouting complexity – With multiple specialized agents, the router needed increasingly sophisticated logic to determine the correct destination.\nOverlapping capabilities – Multiple agents required access to the same data sources and capabilities, forcing the router to predict not just the primary intent but also which secondary agents might be needed downstream—an impossible task at scale.\nScalability bottleneck – Every new agent or capability meant updating the router’s logic. Adding a new specialized agent required comprehensive testing of all routing scenarios. The router became a single point of failure and a potential development bottleneck.\n\nRethinking the architecture\nbunq redesigned its system around an orchestrator agent that works fundamentally differently from the old router. Instead of trying to route to all possible agents, the orchestrator performs the following actions:\n\nRoutes queries to only three to five primary agents\nEmpowers these primary agents to invoke other agents as tools when needed\nDelegates decision-making to the agents themselves\n\nWith this agent-as-tool pattern, primary agents detect when they need specialized help. Tool agents are invoked dynamically by primary agents. Agents can call other agents through a well-defined interface—they become tools in each other’s toolkits.\nThe following diagram illustrates this workflow.\n\nbunq’s Finn service uses a comprehensive AWS infrastructure designed for security, scalability, and intelligent orchestration. The following architecture diagram shows how multiple AWS services work together to deliver a multi-agent AI system.\n\nOrchestration and agent architecture\nAt the core of the system is the orchestrator agent, running on Amazon Elastic Container Service (Amazon ECS). This orchestrator implements the agent-as-tool pattern, routing user queries to a limited set of primary agents rather than attempting to predict every possible scenario. The orchestrator maintains three to five primary agents (Primary Agent 1 through 5), each deployed as containerized services on Amazon ECS. This design provides horizontal scalability—as demand increases, additional agent instances can be spun up automatically. Each primary agent is empowered to invoke specialized agents as needed. These specialized agents (Specialized Agent 1, 2, 3, and so on) act as tools that primary agents can call upon for specific capabilities, such as analyzing transaction data, retrieving documentation, or processing complex queries. This hierarchical structure avoids the routing complexity bottleneck while maintaining flexibility.\nInfrastructure details\nThe architecture is built on a robust foundation of AWS services that enable Finn’s performance. Users access the service through bunq’s application, with traffic secured by AWS WAF and Amazon CloudFront , while authentication flows through bunq’s proprietary identity system. Amazon Bedrock provides access to Anthropic’s Claude models for natural language understanding, complemented by Amazon SageMaker hosted fine-tuned models for specialized banking scenarios. Agent memory and conversation history are stored in DynamoDB, and OpenSearch Service serves as a vector store for RAG capabilities, enabling semantic search across bunq’s knowledge base. Amazon Simple Storage Service (Amazon S3) handles document storage, and Amazon MemoryDB manages user sessions for real-time interactions. Comprehensive observability through AWS CloudTrail , Amazon GuardDuty , and Amazon CloudWatch helps the team monitor performance, detect threats, and maintain compliance—all within a secure virtual private cloud (VPC).\nReal-world impact\nThe transformation from bunq’s initial router-based architecture to the orchestrator pattern with Amazon Bedrock delivered measurable improvements across user support operations. The multi-agent deployment achieved significant operational efficiency gains:\n\nFinn now handles 97% of bunq’s user support activity, with over 82% fully automated. Average response times dropped to just 47 seconds, helping bunq deliver the real-time solutions users expect.\nThe rapid deployment timeline highlights bunq’s focus on innovation. The team moved from concept to production in 3 months, starting in January 2025. bunq brought together a team of 80 people—from AI engineers to support staff—who worked together to test, learn, and deploy updates three times a day.\nBefore implementing the orchestrator architecture, escalations were mainly manual processes. The new multi-agent system increased automation, transforming end-to-end support metrics. Beyond that, Finn expanded bunq’s reach by translating the application into 38 languages, making banking more accessible to millions of users across Europe.\nThe solution enabled bunq to become Europe’s first AI-powered bank, offering capabilities no traditional support system could deliver: real-time speech-to-speech translation (a first in global banking), image recognition for receipt processing and document verification, and intelligent financial insights—all while maintaining the round-the-clock availability users demand.\n\n“We went from concept to production in 3 months. Before the orchestrator architecture, escalations were mainly manual. Now Finn handles 97% of support with 70% fully automated and 47-second average response times.”\n– Benjamin Kleppe, Machine Learning Engineering Lead at bunq.\n\nConclusion\nbunq’s journey from manual support escalations to an intelligent multi-agent system shows how modern AI architecture can transform banking operations. By moving from a rigid router-based approach to a flexible orchestrator pattern with Amazon Bedrock, bunq avoided scalability bottlenecks while maintaining the agility needed to serve 20 million users across Europe. The orchestrator pattern with agent-as-tool capabilities proved essential to bunq’s success. Rather than predicting every possible user scenario upfront, the system empowers primary agents to dynamically invoke specialized agents as needed. This architectural shift reduced complexity, accelerated development cycles, and helped bunq deploy updates three times per day during the initial rollout. The results: 97% of support interactions handled by Finn, 70% fully automated, and average response times of just 47 seconds. Beyond efficiency gains, the solution expanded bunq’s reach to 38 languages and positioned the company as Europe’s first AI-powered bank. By freeing internal resources from manual processes, bunq can now focus on what it does best: building a bank that makes life easy for its users.\nTo learn more about building AI-powered applications with FMs, refer to Amazon Bedrock . Explore how Anthropic’s Claude on Amazon Bedrock can transform your customer experience with enhanced security features and scalability. Get started with the Amazon Bedrock documentation to build your own multi-agent solutions.\n\nAbout the Authors\nBenjamin Kleppe is Machine Learning Engineering Lead at bunq, where he leads the development and scaling of AI-powered solutions that make banking smarter and more personal for 20 million users across Europe. He focuses on building intelligent systems that enhance user experience, improve product discovery, and automate complex banking processes. Benjamin is passionate about pushing the boundaries of AI innovation in banking, having led bunq to become Europe’s first AI-powered bank with the launch of Finn, their proprietary generative AI platform.\nJagdeep Singh Soni is a Senior AI/ML Solutions Architect at AWS based in the Netherlands, specializing in generative AI and Amazon Bedrock. He helps customers and partners architect and implement intelligent agent solutions using Amazon Bedrock and other AWS AI/ML services. With 16 years of experience in innovation and cloud architecture, Jagdeep focuses on enabling organizations to build production-ready generative AI applications that use foundation models and agent frameworks for real-world business outcomes.\nGuy Kfir is a generative AI Lead at AWS with over 15 years of experience in cloud technology sales, business development, and AI/ML evangelism. He works with enterprise customers, startups, and partners across EMEA to accelerate adoption of generative AI solutions and execute go-to-market strategies.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "欧洲第二大数字银行bunq使用Amazon Bedrock升级其内部生成式AI助手Finn，以解决传统银行支持系统在多语言、全天候服务和复杂金融产品处理上的挑战。通过集成Anthropic的Claude模型，Finn采用多代理架构，协调器代理动态调用专业代理处理任务，如实时自然语言理解、交易分析和文档检索。关键改进包括从路由器模式转向协调器模式，避免了可扩展性瓶颈，并利用AWS服务如Amazon ECS、DynamoDB和OpenSearch Serverless构建安全合规的基础设施。结果，Finn现在处理bunq 97%的用户支持活动，82%完全自动化，平均响应时间降至47秒，支持38种语言翻译，提升了操作效率，使bunq成为欧洲首个AI驱动的银行。",
      "category": "LLM",
      "sentiment": "positive",
      "keywords": [
        "bunq",
        "Amazon Bedrock",
        "Finn",
        "Claude",
        "multi-agent"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:19.413585Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "8cc53038fab9662a",
    "title": "Using Strands Agents to create a multi-agent solution with Meta’s Llama 4 and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/using-strands-agents-to-create-a-multi-agent-solution-with-metas-llama-4-and-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T17:47:44Z",
    "summary": "In this post, we explore how to build a multi-agent video processing workflow using Strands Agents, Meta's Llama 4 models, and Amazon Bedrock to automatically analyze and understand video content through specialized AI agents working in coordination. To showcase the solution, we will use Amazon SageMaker AI to walk you through the code.",
    "content": "Multi-agent solutions, in which networks of agents collaborate, coordinate, and reason together, are changing how we approach real-world challenges. Enterprises manage environments with multiple data sources, changing goals, and various constraints. This is where multi-agent architectures shine. By empowering multiple agents that each have specialized tools, memory, or perspectives to interact and reason as a collective, organizations unlock powerful new capabilities:\n\nScalability – Multi-agent frameworks handle tasks of growing complexity, distributing workload intelligently and adapting to scale in real time.\nResilience – When agents work together, failure in one can be compensated or mitigated by others, creating robust, fault-tolerant systems.\nSpecialization – Individual agents excel in specific domains (such as finance, data transformation, and user support) yet can seamlessly cooperate to solve cross-disciplinary problems.\nDynamic problem solving – Multi-agent systems can rapidly reconfigure, pivot, and respond to change, which is essential in volatile business, security, and operations environments.\n\nRecent launches in agentic AI frameworks, such as Strands Agents , make it easier for developers to participate in the creation and deployment of model-driven, multi-agent solutions. You can define prompts and integrate toolsets, allowing robust language models to reason, plan, and invoke tools autonomously rather than relying on handcrafted, brittle workflows.\nIn production, services such as Amazon Bedrock AgentCore support secure, scalable deployment with features like persistent memory, identity integration, and enterprise-grade observability. This shift towards collaborative, multi-agent AI solutions is revolutionizing software architectures by making them more autonomous, resilient, and adaptable. From real-time troubleshooting in cloud infrastructure to cross-team automation in financial services and chat-based assistants coordinating complex multistep business processes, organizations adopting multi-agent solutions are positioning themselves for greater agility and innovation. Now, with open frameworks such as Strands, anyone can start building intelligent systems that think, interact, and evolve together.\nIn this post, we explore how to build a multi-agent video processing workflow using Strands Agents, Meta’s Llama 4 models, and Amazon Bedrock to automatically analyze and understand video content through specialized AI agents working in coordination. To showcase the solution, we will use Amazon SageMaker AI to walk you through the code.\nMeta’s Llama 4: Unlocking the value of 1M+ context windows\nLlama 4 is Meta’s latest family of large language models (LLMs) that stands out for its context window capabilities and multimodal intelligence. Both models use mixture-of-experts (MoE) architecture for efficiency, are designed for multimodal inputs, and are optimized to power agentic systems and complex workflows. The flagship variant, Meta’s Llama 4 Scout, supports a 10 million token context window—an industry-first—enabling the model to process and reason over large amounts of data in a single prompt.\nThis supports applications such as summarizing entire libraries of books, analyzing massive codebases, conducting holistic research across thousands of documents, and maintaining deep, persistent conversation context across long interactions. The Llama 4 Maverick variant also offers a 1 million token window, making it suitable for demanding language, vision, and cross-document tasks. These ultralong context windows open new possibilities for advanced summarization, memory retention, and complex, multistep workflows, positioning Meta’s Llama 4 as a versatile solution for both research and enterprise-grade AI applications\n\nModel name\nContext window\nKey capabilities and use cases\n\nMeta’s Llama 4 Scout\n10M tokens (up to 3.5M using Amazon Bedrock)\nUltralong document processing, entire book or codebase ingestion, large-scale summarization, extensive dialogue memory, advanced research\n\nMeta’s Llama 4 Maverick\n1M tokens\nLarge context multimodal tasks, advanced document and image understanding, code analysis, comprehensive Q&A, robust summarization\n\nSolution overview\nThis post demonstrates how to build a multi-agent video processing workflow by using the Strands Agents SDK , Meta’s Llama 4 with its multimodal capabilities and context window, and the scalable infrastructure of Amazon Bedrock. Although this post focuses primarily on building specialized agents to create this video analysis solution, the practices of creating a multi-agent workflow can be used to build your own adaptable, automated solution at the enterprise level.\nFor scaling, this approach extends naturally to handle larger and more diverse workloads, such as processing video streams from millions of connected devices in smart cities, industrial automation for predictive maintenance through continuous video and sensor data analysis, real-time surveillance systems across multiple locations, or media companies managing vast libraries for indexing and content retrieval. Using the Strands Agents built-in integration with Amazon Web Services (AWS) services and the managed AI infrastructure of Amazon Bedrock means that your multi-agent workflows can elastically scale, distribute tasks efficiently, and maintain high availability and fault tolerance. You can build complex, multistep workflows across heterogeneous data sources and use cases—from live video analytics to personalized media experiences—while maintaining the agility to adapt and expand as business needs evolve.\nIntroduction to agentic workflows using Strands Agents\nThis post demonstrates a video processing solution that implements an agent workflow using six specialized agents. Each agent performs a specific role, passing its output to the next agent to complete multistep tasks in the process. This is conducted through the same analysis as the deep research architecture, in which there is an orchestrator agent that coordinates the process of the other agents working together in tandem. This concept in Strands Agents is called Agents as Tools .\nThis architectural pattern in AI systems allows for specialized AI agents to be wrapped as callable functions (tools) that can be used by other agents. This agentic workflow has the following specialized agents:\n\nLlama4_coordinator_agent – Has access to the other agents and kicks off the process from frame extraction agent to summary generation\ns3_frame_extraction_agent – Uses OpenCV library to extract meaningful frames from videos, handling the complexity of video file operations\ns3_visual_analysis_agent – Has necessary tools that process the frames by analyzing each image and storing it as a JSON file to the provided Amazon Simple Storage Service (Amazon S3) bucket\nretrieve_json_agent – Retrieves the analysis on the frames in the form of a JSON file\nc_temporal_analysis_agent – AI agent that specializes in temporal sequences in video frames by analyzing images chronologically\nsummary_generation_agent – Specializes in creating a summary of the temporal analysis of the images\n\nModularizing the video analysis solution with Agents as Tools\nThe process begins with the orchestrator agent, implemented using Meta’s Llama 4, which coordinates communication and task delegation among specialized agents. This central agent initiates and monitors each step of the video processing pipeline. Using the Agents as Tools pattern in Strands Agents, each specialized agent is wrapped as a callable function (tool), enabling seamless inter-agent communication and modular orchestration. This hierarchical delegation pattern allows the coordinator agent to dynamically invoke domain-specific agents, reflecting how collaborative human teams function.\n\nCustomizability – Each agent’s system prompt can be independently tuned for optimal performance in its specialized task.\nSeparation of concerns – Agents focus on what they do best, making the system more straightforward to develop and maintain.\nWorkflow flexibility – The coordinator agent can orchestrate components in different sequences for various use cases.\nScalability – Components can be optimized individually based on their specific performance requirements.\nExtensibility – New capabilities can be added by introducing new specialized agents without disrupting existing ones.\n\nBy turning agents into tools, we create building blocks that can be combined to solve complex video understanding tasks, demonstrating how you can use Strands Agents to support multi-agent systems with specialized LLM-based reasoning. Let’s examine the coordinator_agent :\n\ndef new_llama4_coordinator_agent() -> Agent:\n    \"\"\"\n    Factory constructor: creates a NEW agent instance with a fresh conversation history.\n    Use this per video request for clean isolation.\n    \"\"\"\n    return Agent(\n        system_prompt=\"\"\"You are a video processing coordinator. Your job is to process videos step by step.\n##When asked to process a video:\n1. Extract frames from S3 video using run_frame_extraction\n2. Use the frame location from step 1 to run_visual_analysis\n3. WAIT for visual analysis to complete sending the json to s3\n4. Use the retrieve_json agent to extract the json from step 3\n5. Use the text result of retrieve_json_from_s3 by passing it to run_temporal_reasoning\n6. Pass the result from temporal reasoning to run_summary_generation\n7. Upload analysis generated in run_summary_generation and return s3 location\n##IMPORTANT:\n- Call ONE tool at a time and wait for the result\n- Use the EXACT result from the previous step as input\n- Do NOT call multiple tools simultaneously\n- Do NOT return raw JSON or function call syntax\n\"\"\",\n        model=bedrock_model,\n        tools=[\n            run_frame_extraction,\n            run_visual_analysis,\n            run_temporal_reasoning,\n            run_summary_generation,\n            upload_analysis_results,\n            retrieve_json_from_s3,\n        ],\n    )\n\nCalling the coordinator_agent triggers the agent workflow to call the s3_frame_extraction_agent . This specialized agent has the necessary tools to extract key frames from the input video using OpenCV, upload the frames to Amazon S3, and identify the folder path to pass off to the run_visual_analysis agent. The following diagram shows this flow.\n\nAfter the frames are stored in Amazon S3, the visual_analysis_agent will have access to tools that list the frames from the S3 folder, use Meta’s Llama in Amazon Bedrock to process the images, and upload the analysis as a JSON file to Amazon S3.\nThe code below will walk you through the different key parts of the different agents. The following example shows the visual_analysis_agent :\n\n@tool\ndef upload_local_json_to_s3(s3_video_path: str, local_filename: str = \"visual_analysis_results.json\") -> str:\n    \"\"\"Upload local JSON file to S3 bucket in video folder\"\"\"\n    try:\n        s3_parts = [part for part in s3_video_path.replace('s3://', '').split('/') if part bucket = s3_parts[0]\n        video_folder = s3_parts[-1]\n        \n        if '_' in video_folder:\n            base_video_name = video_folder.split('_')[0]\n        else:\n            base_video_name = video_folder\n        random_num = randint(1000, 9999)\n        \n        s3_key = f\"videos/{base_video_name}/{random_num}_{local_filename}\"\n        \n        s3_client = boto3.client('s3')\ns3_client.upload_file(local_filename, bucket, s3_key)\n        \n        return f\"s3://{bucket}/{s3_key}\"\n    except Exception as e:\n        return f\"Error uploading file: {str(e)}\"\n\ns_visual_analysis_agent = Agent(\n    system_prompt=\"\"\"You are an image analysis agent that processes frames from S3 buckets.\n\nYour workflow:\n1. Use the available tools to analyze images\n2. Use the video path folder to place the analysis results\n\nIMPORTANT:\n- Do NOT generate, write, or return any code\n- Focus on describing what you see in the images\n- Images are automatically resized if too large\n- Put numbered labels in front of each image description (e.g., \"1. \", \"2. \", etc.)\n- Always save analysis results locally first, then upload to S3\n\nReturn Format:\nThe uri from the upload_local_json_to_s3 tool\"\"\",\n    model=bedrock_model,\n    callback_handler=None,\n\n    tools=[list_s3_frames, analyze_image, analyze_all_frames, analyze_frames_batch, upload_local_json_to_s3],\n)\n\nAfter uploading the JSON to Amazon S3, there is a specialized agent that retrieves the JSON file from Amazon S3 and analyzes the text:\n\n@tool\ndef process_s3_analysis_json(s3_uri: str) -> str:\n    \"\"\"Retrieve JSON from S3 and extract only the analysis text\"\"\"\n    try:\n        # Parse S3 URI and download JSON\n        s3_parts = s3_uri.replace('s3://', '').split('/', 1)\n        bucket = s3_parts[0]\n        key = s3_parts[1]\n      \n        s3_client = boto3.client('s3')\n        response = s3_client.get_object(Bucket=bucket, Key=key)\n        json_content = response['Body'].read().decode('utf-8')\n        \n        # Parse and extract text\n        data = json.loads(json_content)\n        \n        # Handle both formats\n        if 'analyses' in data:\n            analyses = data['analyses']\n        elif 'sessions' in data:\n            analyses = [session['data'] for session in data['sessions'] if 'data' in session]\nelse:\n            return \"Error: No 'analyses' or 'sessions' field found\"\n        \n        # Extract text only\n        text_only = []\n        for analysis in analyses:\n            if 'analysis' in analysis:\n                text = analysis['analysis']\n                if not text.startswith(\"Failed:\"):\n                    text_only.append(text)\n        \n        # Clean up local file\n        local_file = \"visual_analysis_results.json\"\nif os.path.exists(local_file):\n            os.remove(local_file)\n        \n        return \"\\n\".join(text_only)\n    except Exception as e:\n        return f\"Error processing {s3_uri}: {str(e)}\"\n\nbedrock_model = BedrockModel(\n    model_id='us.meta.llama4-maverick-17b-instruct-v1:0',\n    region_name=region,\n    streaming=False,\n    temperature=0\n)  \n\nretrieve_json_agent = Agent(\nsystem_prompt=\"Call process_s3_analysis_json with the S3 URI. Your response must be the exact text output from the tool, nothing else.\",\n    model=bedrock_model,\n    callback_handler=None,\n\n    tools=[process_s3_analysis_json],\n)\n\nThis output will then be fed to\nthe temporal_analysis_agent to gain temporal awareness of the sequences in the video frames and provide a detailed description of the visual content.\nAfter the temporal analysis output has been generated, the summary_generation_agent will be kicked off to provide the final summary.\nPrerequisite and Setup Steps\nTo run the solution on either the notebook or the Gradio UI, you need the following:\n\nAn AWS account with access to Amazon Bedrock.\n\nTo copy over the project,\n\nClone the Meta-LLama-on-AWS github repository :\n\ngit clone https://github.com/aws-samples/Meta-Llama-on-AWS.git\ncd agents/strands/Bedrock/multi-agent-video-processing/\n\nIn your terminal, install the correct dependencies:\n\npip install -r requirements.txt\n\nDeploy video processing app on Gradio\nTo deploy the video processing app on Gradio, follow these application launch instructions:\n\nTo launch the Python terminal, open your Python3 command line interface\nTo install dependencies, execute pip install commands for the required libraries (refer to the preceding library installation section)\nTo execute the application, run the command python3 gradio_app.py\nTo access the interface, choose the generated hosted link displayed in the terminal\nTo initiate video processing, upload your video file through the interface and then choose Run\n\nThe Meta’s Llama video analysis assistant provides the following output for the video buglifeflik.mp4 provided in the GitHub repository:\n\nLlama Video Analysis Log\nFlik is shown determined in front of a tree.\nHe interacts with other insects.\nFlik gathers items and constructs a device.\nHe presents the invention to a group of insects.\nThe group reacts withskepticism.\nFlik is chased by a group of birds.\n\nKey visual elements:\nThe key visual elements include Flik’s determined expression, his interaction with other insects, the items he gathers, the complex device he constructs, the group’s skeptical reaction, and the chaotic scene of Flik being chased by birds.\nOverall Narrative:\nThe narrative follows Flik’s journey as he prepares and presents an invention, faces rejection, and experiences a dramatic consequence. The story is character-driven, showcasing Flik’s actions and their outcomes, and builds up to a climactic event.\n\nThe following screenshot shows the Gradio UI with this output.\n\nRunning in the Jupyter Notebook\nAfter the necessary libraries are imported, you need to manually upload your video to your S3 bucket:\n\ndef upload_to_sagemaker_bucket(local_video_path, base_folder=\"videos/\"):\n    sagemaker = boto3.client('sagemaker')\n    s3 = boto3.client('s3')\n\n    # Get default SageMaker bucket\n    account_id = boto3.client('sts').get_caller_identity()['Account']\n    region = boto3.Session().region_name\n    bucket_name = f\"sagemaker-{region}-{account_id}\"\n    # Get filename and create subfolder name\n    filename = os.path.basename(local_video_path)\n    filename_without_ext = os.path.splitext(filename)[0]\n    # Create the full S3 path: videos/filename_without_ext/filename\n    s3_key = os.path.join(base_folder, filename_without_ext, filename)\n    # Upload file\n    s3.upload_file(local_video_path, bucket_name, s3_key)  \n    s3_uri = f\"s3://{bucket_name}/{s3_key}\"\n    print(f\"Uploaded to {s3_uri}\")  \n\n    s3_folder_path = os.path.join(base_folder, filename_without_ext)\n    s3_folder_uri = f\"s3://{bucket_name}/{s3_folder_path}\"\n\n    return s3_folder_uri\n\n# Example usage: Pr ovide your local video path here\ns3_video_uri = upload_to_sagemaker_bucket(local_video_path)\n\nAfter the video is uploaded, you can start the agent workflow by instantiating a new agent with fresh conversation history:\n\n# Start the workflow\nagent = new_llama4_coordinator_agent()\nvideo_instruction = f\"Process a video from {s3_video_uri}. Use tools in this order: run_frame_extraction, run_visual_analysis, retrieve_json_from_s3, run temporal_reasoning, run_summary_generation_ upload_analysis_results\"\nresponse = agent(video_instruction)\nprint(response)\n\nTool #1: run_frame_extraction\n\nTool #2: run_visual_analysis\n\nTool #3: retrieve_json_from_s3\n\nTool #4: run_temporal_reasoning\n\nTool #5: run_summary_generation\n\nTool #6: run_summary_generation\n**What happens in the video:**\nThe video follows Flik as he navigates through a series of events, starting from being cautious in a natural setting, seeking help or communicating with other insects, participating in a crucial discussion or planning, and finally taking action with the group.\n\n**Chronological Sequence of Events:**\nThe sequence begins with Flik being cautious near a tree, followed by him approaching a group of insects, then being part of a significant gathering or discussion, and concludes with Flik and the insects taking action together.\n\n**Sequence of events:**\n1. Flik is initially seen being cautious in a natural environment.\n2. He then approaches a group of insects, likely to communicate or seek help.\n3. A gathering of insects is shown with Flik at the center, indicating a crucial discussion or planning.\n4. The final scene shows Flik and the insects in action, possibly executing a plan or facing a challenge.\n\n**Key visual elements:**\nThe key visual elements include Flik's cautious initial stance, his interaction with other insects, the gathering or discussion, and the final action scene, highlighting the progression from solitude to collective action.\n\n**Overall Narrative:**\nThe narrative follows Flik's journey from caution and seeking help to participating in a crucial discussion and finally to taking action with a group of insects, suggesting a story arc that involves progression, planning, and collective action.\nTool #7: upload_analysis_results\nThe video processing is co mplete. The final analysis results are saved to s3://sagemaker-us-west-2-333633606362/videos/buglifeflik/analysis_results_20250818_190012.json.The video processing is complete. The final analysis results are saved to s3://sagemaker-us-west-2-333633606362/videos/buglifeflik/analysis_results_20250818_190012.json.\n\nCleanup\nTo avoid incurring unnecessary future charges, clean up the resources you created as part of this solution:To delete the Amazon S3 files:\n\nOpen the AWS Management Console\nNavigate to Amazon S3\nFind and select your Amazon SageMaker bucket\nSelect the video files you uploaded\nChoose Delete and confirm\n\nTo stop and remove the SageMaker notebook:\n\nGo to Amazon SageMaker AI in the AWS Management Console\nChoose Notebook instances\nSelect your notebook\nChoose Stop if it’s running\nAfter it has stopped, choose Delete\n\nConclusion\nThis post highlights how combining the Strands Agents SDK with Meta’s Llama 4 models and Amazon Bedrock infrastructure enables building advanced, multi-agent video processing workflows. By using highly specialized agents that communicate and collaborate through the Agents as Tools pattern, developers can modularize complex tasks such as frame extraction, visual analysis, temporal reasoning, and summarization. This separation of concerns enhances maintainability, customization, and scalability while allowing seamless integration across AWS services.We encourage developers to explore and extend this architecture by adding new specialized agents and adapting workflows to diverse use cases—from smart cities and industrial automation to media content management. The combination of Strands Agents, Meta’s Llama 4, and Amazon Bedrock lays a robust foundation for creating autonomous, resilient AI solutions that tackle the complexity of modern business environments.\nTo get started, visit the official GitHub repository for the Meta-Llama-on-AWS agents project for code examples and deployment instructions. For further insights on building with Strands Agents, explore the Strands Agents documentation , which offers a code-first approach to integrating modular AI agents. For broader context on multi-agent AI architectures and orchestration, AWS blog posts on agent interoperability and autonomous agent frameworks provide valuable guidance shaping the future of intelligent systems.\n\nAbout the authors\nSebastian Bustillo is an Enterprise Solutions Architect at Amazon Web Services (AWS), working with airlines and is an active member of the AI/ML Technical Field Community. At AWS, he helps customers unlock business value through AI. Outside of work, he enjoys spending time with his family and exploring the outdoors. He’s also passionate about brewing specialty coffees.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Amazon AWS ML 发布了一篇技术博客，介绍如何使用 Strands Agents 框架结合 Meta 的 Llama 4 大语言模型和 Amazon Bedrock 服务构建多代理视频处理工作流。该方案通过六个专门代理（如帧提取、视觉分析、时间推理代理）协作，实现视频内容的自动分析和理解，展示了多代理架构在可扩展性、弹性和专业化方面的优势。关键技术包括 Strands Agents 的代理工具模式、Llama 4 的超长上下文窗口（如 Scout 支持 10M tokens），以及 Bedrock 的托管 AI 基础设施。这一进展简化了多代理 AI 系统的开发，适用于企业级应用，如视频分析、智能城市和工业自动化，推动软件架构更加自主和适应性强。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "Strands Agents",
        "Meta Llama 4",
        "Amazon Bedrock",
        "多代理系统",
        "视频处理"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:10.046346Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "92f800a2e87721c2",
    "title": "How Higgsfield turns simple ideas into cinematic social videos",
    "url": "https://openai.com/index/higgsfield",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T10:00:00Z",
    "summary": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
    "content": "Discover how Higgsfield gives creators cinematic, social-first video output from simple inputs using OpenAI GPT-4.1, GPT-5, and Sora 2.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "Higgsfield 是一个利用 OpenAI GPT-4.1、GPT-5 和 Sora 2 技术的工具，帮助创作者从简单想法生成电影化的社交媒体视频。它通过整合先进的人工智能模型，自动化视频制作过程，使非专业用户也能轻松创作高质量内容。这项技术的重要性在于降低了视频制作门槛，提升创意效率，适用于社交媒体平台。潜在影响包括推动 AI 在内容创作领域的应用，可能改变行业标准并促进创意表达。关键功能涉及使用 GPT 模型处理文本输入，Sora 2 生成视觉内容，实现从概念到成品的快速转换。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Higgsfield",
        "OpenAI",
        "GPT-4.1",
        "GPT-5",
        "Sora 2"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:07.423664Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "13755a0c64302336",
    "title": "Introducing Edu for Countries",
    "url": "https://openai.com/index/edu-for-countries",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T01:00:00Z",
    "summary": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
    "content": "Edu for Countries is a new OpenAI initiative helping governments use AI to modernize education systems and build future-ready workforces.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI公司最新推出了名为'Edu for Countries'的倡议，该倡议旨在帮助各国政府应用人工智能（AI）技术来现代化教育系统和构建未来就绪的劳动力队伍。这一举措的重要性在于，通过利用AI优化教育资源、个性化教学和提升学习效率，能够推动教育领域的数字化转型，培养适应未来经济需求的技能人才。影响范围可能包括促进全球教育创新、加强政府与科技公司的合作，以及对教育行业和劳动力市场产生积极变革，为社会发展提供技术支持。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Edu for Countries",
        "AI",
        "教育现代化",
        "劳动力发展"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:33.760766Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "6d323263652e3c0e",
    "title": "How countries can end the capability overhang",
    "url": "https://openai.com/index/how-countries-can-end-the-capability-overhang",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-21T01:00:00Z",
    "summary": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
    "content": "Our latest report reveals stark differences in advanced AI adoption across countries and outlines new initiatives to help nations capture productivity gains from AI.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI发布了题为《How countries can end the capability overhang》的最新报告，揭示了全球各国在先进人工智能（AI）采用方面的显著差异，并概述了新倡议以帮助国家捕获AI带来的生产力收益。报告核心在于分析AI能力过剩（capability overhang）问题，提出通过促进AI的广泛采用和优化资源分配，减少技术发展不均，从而提升生产力和推动经济增长。这一举措对全球AI政策制定、技术普及和国际合作具有重要影响，强调了AI作为经济驱动力潜力，并为政府和企业提供指导，以加速技术集成。OpenAI的倡议旨在帮助各国克服采用障碍，实现更均衡的AI发展，促进全球经济的可持续增长。",
      "category": "行业",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "AI adoption",
        "capability overhang",
        "productivity gains",
        "initiatives"
      ]
    },
    "analyzed_at": "2026-01-22T03:41:58.443856Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "13691a2ce123a89e",
    "title": "Horizon 1000: Advancing AI for primary healthcare",
    "url": "https://openai.com/index/horizon-1000",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-20T21:00:00Z",
    "summary": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
    "content": "OpenAI and the Gates Foundation launch Horizon 1000, a $50M pilot advancing AI capabilities for healthcare in Africa. The initiative aims to reach 1,000 clinics by 2028.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI与盖茨基金会联合推出了Horizon 1000项目，这是一个价值5000万美元的试点计划，旨在通过人工智能技术提升非洲的初级医疗保健能力。该计划的目标是在2028年前覆盖1000家诊所，专注于推进AI在医疗领域的应用，以优化资源分配和服务效率。这一合作结合了科技与慈善力量，重要性在于探索AI在资源有限环境中解决卫生问题的潜力，有望改善医疗可及性并推动全球AI医疗创新，对行业发展和公共卫生产生积极影响。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Gates Foundation",
        "Horizon 1000",
        "AI",
        "healthcare"
      ]
    },
    "analyzed_at": "2026-01-22T03:42:04.712705Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]