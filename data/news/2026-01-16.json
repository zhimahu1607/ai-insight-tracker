[
  {
    "id": "c82ca9a069ba5c2b",
    "title": "How the Amazon AMET Payments team accelerates test case generation with Strands Agents",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-the-amazon-amet-payments-team-accelerates-test-case-generation-with-strands-agents/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T15:55:35Z",
    "summary": "In this post, we explain how we overcame the limitations of single-agent AI systems through a human-centric approach, implemented structured outputs to significantly reduce hallucinations and built a scalable solution now positioned for expansion across the AMET QA team and later across other QA teams in International Emerging Stores and Payments (IESP) Org.",
    "content": "At Amazon.ae , we serve approximately 10 million customers monthly across five countries in the Middle East and North Africa region—United Arab Emirates (UAE), Saudi Arabia, Egypt, Türkiye, and South Africa. Our AMET (Africa, Middle East, and Türkiye) Payments team manages payment selections, transactions, experiences, and affordability features across these diverse countries, publishing on average five new features monthly. Each feature requires comprehensive test case generation, which traditionally consumed 1 week of manual effort per project. Our quality assurance (QA) engineers spent this time analyzing business requirement documents (BRDs), design documents, UI mocks, and historical test preparations—a process that required one full-time engineer annually merely for test case creation.\nTo improve this manual process, we developed SAARAM (QA Lifecycle App), a multi-agent AI solution that helps reduce test case generation from 1 week to hours. Using Amazon Bedrock with Claude Sonnet by Anthropic and the Strands Agents SDK , we reduced the time needed to generate test cases from 1 week to mere hours while also improving test coverage quality. Our solution demonstrates how studying human cognitive patterns, rather than optimizing AI algorithms alone, can create production-ready systems that enhance rather than replace human expertise.\nIn this post, we explain how we overcame the limitations of single-agent AI systems through a human-centric approach, implemented structured outputs to significantly reduce hallucinations and built a scalable solution now positioned for expansion across the AMET QA team and later across other QA teams in International Emerging Stores and Payments (IESP) Org.\nSolution overview\nThe AMET Payments QA team validates code deployments affecting payment functionality for millions of customers across diverse regulatory environments and payment methods. Our manual test case generation process added turnaround time (TAT) in the product cycle, consuming valuable engineering resources on repetitive test prep and documentation tasks rather than strategic testing initiatives. We needed an automated solution that could maintain our quality standards while reducing the time investment.\nOur objectives included reducing test case creation time from 1 week to under a few hours, capturing institutional knowledge from experienced testers, standardizing testing approaches across teams, and minimizing the hallucination issues common in AI systems. The solution needed to handle complex business requirements spanning multiple payment methods, regional regulations, and customer segments while generating specific, actionable test cases aligned with our existing test management systems.\nThe architecture employs a sophisticated multi-agent workflow. To achieve this, we went through 3 different iterations and continue to improve and enhance as new techniques are developed and new models are deployed.\nThe challenge with traditional AI approaches\nOur initial attempts followed conventional AI approaches, feeding entire BRDs to a single AI agent for test case generation. This method frequently produced generic outputs like “verify payment works correctly” instead of the specific, actionable test cases our QA team requires. For example, we need test cases as specific as “verify that when a UAE customer selects cash on delivery (COD) for an order above 1,000 AED with a saved credit card, the system displays the COD fee of 11 AED and processes the payment through the COD gateway with order state transitioning to ‘pending delivery.'”\nThe single-agent approach presented several critical limitations. Context length restrictions prevented processing large documents effectively, but the lack of specialized processing phases meant the AI couldn’t understand testing priorities or risk-based approaches. Additionally, hallucination issues created irrelevant test scenarios that could mislead QA efforts. The root cause was clear: AI attempted to compress complex business logic without the iterative thinking process that experienced testers employ when analyzing requirements.\nThe following flow chart illustrates our issues when attempting to use a single agent with a comprehensive prompt.\n\nThe human-centric breakthrough\nOur breakthrough came from a fundamental shift in approach. Instead of asking, “How should AI think about testing?”, we asked, “How do experienced humans think about testing?” to focus on following a specific step-by-step process instead of relying on the large language model (LLM) to realize this on its own. This philosophy change led us to conduct research interviews with senior QA professionals, studying their cognitive workflows in detail.\nWe discovered that experienced testers don’t process documents holistically—they work through specialized mental phases. First, they analyze documents by extracting acceptance criteria, identifying customer journeys, understanding UX requirements, mapping product requirements, analyzing user data, and assessing workstream capabilities. Then they develop tests through a systematic process: journey analysis, scenario identification, data flow mapping, test case development, and finally, organization and prioritization.\nWe then decomposed our original agent into sequential thinking actions that served as individual steps. We built and tested each step using Amazon Q Developer for CLI to make sure basic ideas were sound and incorporated both primary and secondary inputs.\nThis insight led us to design SAARAM with specialized agents that mirror these expert testing approaches. Each agent focuses on a specific aspect of the testing process, such as how human experts mentally compartmentalize different analysis phases.\nMulti-agent architecture with Strands Agents\nBased on our understanding of human QA workflows, we initially attempted to build our own agents from scratch. We had to create our own looping, serial, or parallel execution. We also created our own orchestration and workflow graphs, which demanded considerable manual effort. To address these challenges, we migrated to Strands Agents SDK. This provided the multi-agent orchestration capabilities essential for coordinating complex, interdependent tasks while maintaining clear execution paths, helping improve our performance and reduce our development time.\nWorkflow iteration 1: End-to-end test generation\nOur first iteration of SAARAM consisted of a single input and created our first specialized agents. It involved processing a work document through five specialized agents to generate comprehensive test coverage.\nAgent 1 is called the Customer Segment Creator, and it focuses on customer segmentation analysis, using four subagents:\n\nCustomer Segment Discovery  identifies product user segments\nDecision Matrix Generator  creates parameter-based matrices\nE2E Scenario Creation  develops end-to-end (E2E) scenarios per segment\nTest Steps Generation  detailed test case development\n\nAgent 2 is called the User Journey Mapper, and it employs four subagents to map product journeys comprehensively:\n\nThe Flow Diagram and Sequence Diagram are creators using Mermaid syntax.\nThe E2E Scenarios generator builds upon these diagrams.\nThe Test Steps Generator is used for detailed test documentation.\n\nAgent 3 is called Customer Segment x Journey Coverage, and it combines inputs from agents 1 and 2 to create detailed segment-specific analyses. It uses four subagents:\n\nMermaid-based flow diagrams\nUser journeys\nSequence diagrams for each customer segment\nCorresponding test steps.\n\nAgent 4 is called the State Transition Agent. It analyzes various product state points in customer journey flows. Its sub-agents create Mermaid state diagrams representing different journey states, segment-specific state scenario diagrams, and generate related test scenarios and steps.\nThe workflow, shown in the following diagram, concludes with a basic extract, transform, and load (ETL) process that consolidates and deduplicates the data from the agents, saving the final output as a text file.\n\nThis systematic approach facilitates comprehensive coverage of customer journeys, segments, and various diagram types, enabling thorough test coverage generation through iterative processing by agents and subagents.\nAddressing limitations and enhancing capabilities\nIn our journey to develop a more robust and efficient tool using Strands Agents, we identified five crucial limitations in our initial approach:\n\nContext and hallucination challenges – Our first workflow faced limitations from segregated agent operations where individual agents independently collected data and created visual representations. This isolation led to limited contextual understanding, resulting in reduced accuracy and increased hallucinations in the outputs.\nData generation inefficiencies – The limited context available to agents caused another critical issue: the generation of excessive irrelevant data. Without proper contextual awareness, agents produced less focused outputs, leading to noise that obscured valuable insights.\nRestricted parsing capabilities – The initial system’s data parsing scope proved too narrow, limited to only customer segments, journey mapping, and basic requirements. This restriction prevented agents from accessing the full spectrum of information needed for comprehensive analysis.\nSingle-source input constraint – The workflow could only process Word documents, creating a significant bottleneck. Modern development environments require data from multiple sources, and this limitation prevented holistic data collection.\nRigid architecture problems – Importantly, the first workflow employed a tightly coupled system with rigid orchestration. This architecture made it difficult to modify, extend, or reuse components, limiting the system’s adaptability to changing requirements.\n\nIn our second iteration, we needed to implement strategic solutions to address these issues.\nWorkflow iteration 2: Comprehensive analysis workflow\nOur second iteration represents a complete reimagining of the agentic workflow architecture. Rather than patching individual problems, we rebuilt from the ground up with modularity, context-awareness, and extensibility as core principles:\nAgent 1 is the intelligent gateway. The file type decision agent serves as the system’s entry point and router. Processing documentation files, Figma designs, and code repositories, it categorizes and directs data to appropriate downstream agents. This intelligent routing is essential for maintaining both efficiency and accuracy throughout the workflow.\nAgent 2 is for specialized data extraction. The Data Extractor agent employs six specialized subagents, each focused on specific extraction domains. This parallel processing approach facilitates thorough coverage while maintaining practical speed. Each subagent operates with domain-specific knowledge, extracting nuanced information that generalized approaches might overlook.\nAgent 3 is the Visualizer agent, and it transforms extracted data into six distinct Mermaid diagram types, each serving specific analytical purposes. Entity relation diagrams map data relationships and structures, and flow diagrams visualize processes and workflows. Requirement diagrams clarify product specifications, and UX requirement visualizations illustrate user experience flows. Process flow diagrams detail system operations, and mind maps reveal feature relationships and hierarchies. These visualizations provide multiple perspectives on the same information, helping both human reviewers and downstream agents understand patterns and connections within complex datasets.\nAgent 4 is the Data Condenser agent, and it performs crucial synthesis through intelligent context distillation, making sure each downstream agent receives exactly the information needed for its specialized task. This agent, powered by its condensed information generator, merges outputs from both the Data Extractor and Visualizer agents while performing sophisticated analysis.\nThe agent extracts critical elements from the full text context—acceptance criteria, business rules, customer segments, and edge cases—creating structured summaries that preserve essential details while reducing token usage. It compares each text file with its corresponding Mermaid diagram, capturing information that might be missed in visual representations alone. This careful processing maintains information integrity across agent handoffs, making sure important data is not lost as it flows through the system. The result is a set of condensed addendums that enrich the Mermaid diagrams with comprehensive context. This synthesis makes sure that when information moves to test generation, it arrives complete, structured, and optimized for processing.\nAgent 5 is the Test Generator agent brings together the collected, visualized, and condensed information to produce comprehensive test suites. Working with six Mermaid diagrams plus condensed information from Agent 4, this agent employs a pipeline of five subagents. The Journey Analysis Mapper, Scenario Identification Agent, and the Data Flow Mapping subagents generate comprehensive test cases based on their take of the input data flowing from Agent 4.With the test cases generated across three critical perspectives, the Test Cases Generator evaluates them, reformatting according to internal guidelines for consistency. Finally, the Test Suite Organizer performs deduplication and optimization, delivering a final test suite that balances comprehensiveness with efficiency.\nThe system now handles far more than the basic requirements and journey mapping of Workflow 1—it processes product requirements, UX specifications, acceptance criteria, and workstream extraction while accepting inputs from Figma designs, code repositories, and multiple document types. Most importantly, the shift to modular architecture fundamentally changed how the system operates and evolves. Unlike our rigid first workflow, this design allows for reusing outputs from earlier agents, integrating new testing type agents, and intelligently selecting test case generators based on user requirements, positioning the system for continuous adaptation.\nThe following figure shows our second iteration of SAARAM with five main agents and multiple subagents with context engineering and compression.\n\nAdditional Strands Agents features\nStrands Agents provided the foundation for our multi-agent system, offering a model-driven approach that simplified complex agent development. Because the SDK can connect models with tools through advanced reasoning capabilities, we built sophisticated workflows with only a few lines of code. Beyond its core functionality, two key features proved essential for our production deployment: reducing hallucinations with structured outputs and workflow orchestration.\nReducing hallucinations with structured outputs\nThe structured output feature of Strands Agents uses Pydantic models to transform traditionally unpredictable LLM outputs into reliable, type-safe responses. This approach addresses a fundamental challenge in generative AI: although LLMs excel at producing humanlike text, they can struggle with consistently formatted outputs needed for production systems. By enforcing schemas through Pydantic validation, we make sure that responses conform to predefined structures, enabling seamless integration with existing test management systems.\nThe following sample implementation demonstrates how structured outputs work in practice:\n\nfrom pydantic import BaseModel, Field\nfrom typing import List\nimport json\n\n# Define structured output schema\nclass TestCaseItem(BaseModel):\nname: str = Field(description=\"Test case name\")\npriority: str = Field(description=\"Priority: P0, P1, or P2\")\ncategory: str = Field(description=\"Test category\")\n\nclass TestOutput(BaseModel):\ntest_cases: List[TestCaseItem] = Field(description=\"Generated test cases\")\n\n# Agent tool with validation\n@tool\ndef save_results(self, results: str) -> str:\ntry:\n# Parse and validate Claude's JSON output\ndata = json.loads(results)\nvalidated = TestOutput(**data)\n\n# Save only if validation passes\nwith open(\"results.json\", 'w') as f:\njson.dump(validated.dict(), f, indent=)\nreturn \"Validated results saved\"\n\nexcept ValidationError as e:\nreturn f\"Invalid output format: e\"\n\nPydantic automatically validates LLM responses against defined schemas to facilitate type correctness and required field presence. When responses don’t match the expected structure, validation errors provide clear feedback about what needs correction, helping prevent malformed data from propagating through the system. In our environment, this approach delivered consistent, predictable outputs across the agents regardless of prompt variations or model updates, minimizing an entire class of data formatting errors. As a result, our development team worked more efficiently with full IDE support.\nWorkflow orchestration benefits\nThe Strands Agents workflow architecture provided the sophisticated coordination capabilities our multi-agent system required. The framework enabled structured coordination with explicit task definitions, automatic parallel execution for independent tasks, and sequential processing for dependent operations. This meant we could build complex agent-to-agent communication patterns that would have been difficult to implement manually.\nThe following sample snippet shows how to create a workflow in Strands Agents SDK:\n\nfrom strands import Agent\nfrom strands_tools import workflow\n\n# Create agent with workflow capability\nmain_agent_3 = create_main_agent_3()\n\n# Create workflow with structured output tasks\nworkflow_result = main_agent_3.tool.workflow(\naction=\"create\",\nworkflow_id=\"comprehensive_e2e_test_generation\",\ntasks=[\n# Phase 1: Parallel execution (no dependencies)\n{\n\"task_id\": \"journey_analysis\",\n\"description\": \"Generate journey scenario names with brief descriptions using structured output\",\n\"dependencies\": [],\n\"model_provider\": \"bedrock\",\n\"model_settings\": {\n\"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n\"params\": {\"temperature\": }\n},\n\"system_prompt\": load_prompt(\"journey_analysis\"),\n\"structured_output_model\": \"JourneyAnalysisOutput\",\n\"priority\": ,\n\"timeout\": },\n\n{\n\"task_id\": \"scenario_identification\",\n\"description\": \"Generate scenario variations using structured output for different path types\",\n\"dependencies\": [],\n\"model_provider\": \"bedrock\",\n\"model_settings\": {\n\"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n\"params\": {\"temperature\": }\n},\n\"system_prompt\": load_prompt(\"scenario_identification\"),\n\"structured_output_model\": \"ScenarioIdentificationOutput\",\n\"priority\": ,\n\"timeout\": },\n\n{\n\"task_id\": \"data_flow_mapping\",\n\"description\": \"Generate data flow scenarios using structured output covering information journey\",\n\"dependencies\": [],\n\"model_provider\": \"bedrock\",\n\"model_settings\": {\n\"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n\"params\": {\"temperature\": }\n},\n\"system_prompt\": load_prompt(\"data_flow_mapping\"),\n\"structured_output_model\": \"DataFlowMappingOutput\",\n\"priority\": ,\n\"timeout\": },\n\n# Phase 2: Waits for first 3 tasks to complete\n{\n\"task_id\": \"test_case_development\",\n\"description\": \"Generate test cases from all scenario outputs using structured output\",\n\"dependencies\": [\"journey_analysis\", \"scenario_identification\", \"data_flow_mapping\"],\n\"model_provider\": \"bedrock\",\n\"model_settings\": {\n\"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n\"params\": {\"temperature\": }\n},\n\"system_prompt\": load_prompt(\"test_case_development\"),\n\"structured_output_model\": \"TestCaseDevelopmentOutput\",\n\"priority\": ,\n\"timeout\": },\n\n# Phase 3: Waits for test case development to complete\n{\n\"task_id\": \"test_suite_organization\",\n\"description\": \"Organize all test cases into final comprehensive test suite using structured output\",\n\"dependencies\": [\"test_case_development\"],\n\"model_provider\": \"bedrock\",\n\"model_settings\": {\n\"model_id\": \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n\"params\": {\"temperature\": }\n},\n\"system_prompt\": load_prompt(\"test_suite_organization\"),\n\"structured_output_model\": \"TestSuiteOrganizationOutput\",\n\"priority\": ,\n\"timeout\": }\n]\n\nThe workflow system delivered three critical capabilities for our use case. First, parallel processing optimization allowed journey analysis, scenario identification, and coverage analysis to run simultaneously, with independent agents processing different aspects without blocking each other. The system automatically allocated resources based on availability, maximizing throughput.\nSecond, intelligent dependency management made sure that test development waited for scenario identification to be completed, and organization tasks depended on the test cases being generated. Context was preserved and passed efficiently between dependent stages, maintaining information integrity throughout the workflow.\nFinally, the built-in reliability features provided the resilience our system required. Automatic retry mechanisms handled transient failures gracefully, state persistence enabled pause and resume capabilities for long-running workflows, and comprehensive audit logging supported both debugging and performance optimization efforts.\nThe following table shows examples of input into the workflow and the potential outputs.\n\nInput: Business requirement document\nOutput: Test cases generated\n\nFunctional requirements:\n\nCredit card processing. System must:\n\nSupport credit card payment processing\nValidate credit card details before processing\nDisplay checkout and card entry forms\nSave shipping information\nProvide order confirmation after successful payment\n\nError handling. System must:\n\nHandle payment failures gracefully\nDisplay clear declined payment messages\nOffer payment retry options\nSupport switching between payment methods\nHandle gateway timeouts with retry mechanism\n\nRefund processing. System must:\n\nSupport refund initiation from admin panel\nProcess refunds to original payment method\nTrack and update refund status\nMaintain refund transaction records\n\nMultiple payment methods. System must:\n\nSupport split payments across methods\nHandle gift card partial payments\nCalculate remaining balances accurately\nReconcile combined payment amounts\nVerify total order amount matches payments\n\nTC006: Credit card payment success Scenario: Customer completes purchase using valid credit card Steps: 1. Add items to cart and proceed to checkout. Expected result: Checkout form displayed. 2. Enter shipping information. Expected result: Shipping details saved. 3. Select credit card payment method. Expected result: Card form shown. 4. Enter valid card details. Expected result: Card validated. 5. Submit payment. Expected result: Payment processed, order confirmed. TC008: Payment failure handling Scenario: Payment fails due to insufficient funds or card decline Steps: 1. Enter card with insufficient funds. Expected result: Payment declined message. 2. System offers retry option. Expected result: Payment form redisplayed. 3. Try alternative payment method. Expected result: Alternative payment successful. TC009: Payment gateway timeout Scenario: Payment gateway times out during transaction processing Steps: 1. Submit payment during gateway maintenance. Expected result: Timeout error shown. 2. System provides retry mechanism. Expected result: Retry button available. 3. Retry payment after timeout. Expected result: Payment processes successfully. TC010: Refund processing Scenario : Customer refund is processed back to original payment method Steps: 1. Initiate refund from admin panel. Expected result: Refund request created. 2. Process refund to original card. Expected result: Refund transaction initiated. 3. Verify refund status. Expected result: Refund marked as completed.\n\nIntegration with Amazon Bedrock\nAmazon Bedrock served as the foundation for our AI capabilities, providing seamless access to Claude Sonnet by Anthropic through the Strands Agents built-in AWS service integration. We selected Claude Sonnet by Anthropic for its exceptional reasoning capabilities and ability to understand complex payment domain requirements. The Strands Agents flexible LLM API integration made this implementation straightforward. The following snippet shows how to effortlessly create an agent in Strands Agents:\n\nimport boto3\nfrom strands import Agent\nfrom strands.models import BedrockModel\n\nbedrock_model = BedrockModel(\n    model_id=\"anthropic.claude-sonnet-4-20250514-v1:0\",\n    region_name=\"us-west-2\",\n    temperature=0.3,\n)\n\nagent = Agent(model=bedrock_model)\n\nThe managed service architecture of Amazon Bedrock reduced infrastructure complexity from our deployment. The service provided automatic scaling that adjusted to our workload demands, facilitating consistent performance across the agents regardless of traffic patterns. Built-in retry logic and error handling improved system reliability significantly, reducing the operational overhead typically associated with managing AI infrastructure at scale. The combination of the sophisticated orchestration capabilities of Strands Agents and the robust infrastructure of Amazon Bedrock created a production-ready system that could handle complex test generation workflows while maintaining high reliability and performance standards.\nThe following diagram shows the deployment of the SARAAM agent with Amazon Bedrock AgentCore and Amazon Bedrock.\n\nResults and business impact\nThe implementation of SAARAM has improved our QA processes with measurable improvements across multiple dimensions. Before SAARAM, our QA engineers spent 3–5 days manually analyzing BRD documents and UI mocks to create comprehensive test cases. This manual process is now reduced to hours, with the system achieving:\n\nTest case generation time : Potential reduced from 1 week to hours\nResource optimization : QA effort decreased from 1.0 full-time employee (FTE) to 0.2 FTE for validation\nCoverage improvement : 40% more edge cases identified compared to manual process\nConsistency : 100% adherence to test case standards and formats\n\nThe accelerated test case generation has driven improvements in our core business metrics:\n\nPayment success rate : Increased through comprehensive edge case testing and risk-based test prioritization\nPayment experience : Enhanced customer satisfaction because teams can now iterate on test coverage during the design phase\nDeveloper velocity : Product and development teams generate preliminary test cases during design, enabling early quality feedback\n\nSAARAM captures and preserves institutional knowledge that was previously dependent on individual QA engineers:\n\nTesting patterns from experienced professionals are now codified\nHistorical test case learnings are automatically applied to new features\nConsistent testing approaches across different payment methods and industries\nReduced onboarding time for new QA team members\n\nThis iterative improvement means that the system becomes more valuable over time.\nLessons learned\nOur journey developing SAARAM provided crucial insights for building production-ready AI systems. Our breakthrough came from studying how domain experts think rather than optimizing how AI processes information. Understanding the cognitive patterns of testers and QA professionals led to an architecture that naturally aligns with human reasoning. This approach produced better results compared to purely technical optimizations. Organizations building similar systems should invest time observing and interviewing domain experts before designing their AI architecture—the insights gained directly translate to more effective agent design.\nBreaking complex tasks into specialized agents dramatically improved both accuracy and reliability. Our multi-agent architecture, enabled by the orchestration capabilities of Strands Agents, handles nuances that monolithic approaches consistently miss. Each agent’s focused responsibility enables deeper domain expertise while providing better error isolation and debugging capabilities.\nA key discovery was that the Strands Agents workflow and graph-based orchestration patterns significantly outperformed traditional supervisor agent approaches. Although supervisor agents make dynamic routing decisions that can introduce variability, workflows provide “agents on rails”—a structured path facilitating consistent, reproducible results. Strands Agents offers multiple patterns, including supervisor-based routing, workflow orchestration for sequential processing with dependencies, and graph-based coordination for complex scenarios. For test generation where consistency is paramount, the workflow pattern with its explicit task dependencies and parallel execution capabilities delivered the optimal balance of flexibility and control. This structured approach aligns perfectly with production environments where reliability matters more than theoretical flexibility.\nImplementing Pydantic models through the Strands Agents structured output feature effectively reduced type-related hallucinations in our system. By enforcing AI responses to conform to strict schemas, we facilitate reliable, programmatically usable outputs. This approach has proven essential when consistency and reliability are nonnegotiable. The type-safe responses and automatic validation have become foundational to our system’s reliability.\nOur condensed information generator pattern demonstrates how intelligent context management maintains quality throughout multistage processing. This approach of knowing what to preserve, condense, and pass between agents helps prevent the context degradation that typically occurs in token-limited environments. The pattern is broadly applicable to multistage AI systems facing similar constraints.\nWhat’s next\nThe modular architecture we’ve built with Strands Agents enables straightforward adaptation to other domains within Amazon. The same patterns that generate payment test cases can be applied to retail systems testing, customer service scenario generation for support workflows, and mobile application UI and UX test case generation. Each adaptation requires only domain-specific prompts and schemas while reusing the core orchestration logic. Throughout the development of SAARAM, the team successfully addressed many challenges in test case generation—from reducing hallucinations through structured outputs to implementing sophisticated multi-agent workflows. However, one critical gap remains: the system hasn’t yet been provided with examples of what high-quality test cases actually look like in practice.\nTo bridge this gap, integrating Amazon Bedrock Knowledge Bases with a curated repository of historical test cases would provide SAARAM with concrete, real-world examples during the generation process. By using the integration capabilities of Strands Agents with Amazon Bedrock Knowledge Bases, the system could search through past successful test cases to find similar scenarios before generating new ones. When processing a BRD for a new payment feature, SAARAM would first query the knowledge base for comparable test cases—whether for similar payment methods, customer segments, or transaction flows—and use these as contextual examples to guide its output.\nFuture deployment will use Amazon Bedrock AgentCore for comprehensive agent lifecycle management. Amazon Bedrock AgentCore Runtime provides the production execution environment with ephemeral, session-specific state management that maintains conversational context during active sessions while facilitating isolation between different user interactions. The observability capabilities of Bedrock AgentCore help deliver detailed visualizations of each step in SAARAM’s multi-agent workflow, which the team can use to trace execution paths through the five agents, audit intermediate outputs from the Data Condenser and Test Generator agents, and identify performance bottlenecks through real-time dashboards powered by Amazon CloudWatch with standardized OpenTelemetry-compatible telemetry.\nThe service enables several advanced capabilities essential for production deployment: centralized agent management and versioning through the Amazon Bedrock AgentCore control plane, A/B testing of different workflow strategies and prompt variations across the five subagents within the Test Generator, performance monitoring with metrics tracking token usage and latency across the parallel execution phases, automated agent updates without disrupting active test generation workflows, and session persistence for maintaining context when QA engineers iteratively refine test suite outputs. This integration positions SAARAM for enterprise-scale deployment while providing the operational visibility and reliability controls that transform it from a proof of concept into a production system capable of handling the AMET team’s ambitious goal of expanding beyond Payments QA to serve the broader organization.\nConclusion\nSAARAM demonstrates how AI can change traditional QA processes when designed with human expertise at its core. By reducing test case creation from 1 week to hours while improving quality and coverage, we’ve enabled faster feature deployment and enhanced payment experiences for millions of customers across the MENA region. The key to our success wasn’t merely advanced AI technology—it was the combination of human expertise, thoughtful architecture design, and robust engineering practices. Through careful study of how experienced QA professionals think, implementation of multi-agent systems that mirror these cognitive patterns, and minimization of AI limitations through structured outputs and context engineering, we’ve created a system that enhances rather than replaces human expertise.\nFor teams considering similar initiatives, our experience emphasizes three critical success factors: invest time understanding the cognitive processes of domain experts, implement structured outputs to minimize hallucinations, and design multi-agent architectures that mirror human problem-solving approaches. These QA tools aren’t intended to replace human testers, they amplify their expertise through intelligent automation. If you’re interested in starting your journey on agents with AWS, check out our sample Strands Agents implementations repo or our newest launch, Amazon Bedrock AgentCore , and the end-to-end examples with deployment on our Amazon Bedrock AgentCore samples repo .\n\nAbout the authors\nJayashree is a Quality Assurance Engineer at Amazon Music Tech, where she combines rigorous manual testing expertise with an emerging passion for GenAI-powered automation. Her work focuses on maintaining high system quality standards while exploring innovative approaches to make testing more intelligent and efficient. Committed to reducing testing monotony and enhancing product quality across Amazon’s ecosystem, Jayashree is at the forefront of integrating artificial intelligence into quality assurance practices.\nHarsha Pradha G is a Snr. Quality Assurance Engineer part in MENA Payments at Amazon. With a strong foundation in building comprehensive quality strategies, she brings a unique perspective to the intersection of QA and AI as an emerging QA-AI integrator. Her work focuses on bridging the gap between traditional testing methodologies and cutting-edge AI innovations, while also serving as an AI content strategist and AI Author.\nFahim Surani is Senior Solutions Architect as AWS, helping customers across Financial Services, Energy, and Telecommunications design and build cloud and generative AI solutions. His focus since 2022 has been driving enterprise cloud adoption, spanning cloud migrations, cost optimization, event-driven architectures, including leading implementations recognized as early adopters of Amazon’s latest AI capabilities. Fahim’s work covers a wide range of use cases, with a primary interest in generative AI, agentic architectures. He is a regular speaker at AWS summits and industry events across the region.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "ba3a19b863e57cad",
    "title": "Build a generative AI-powered business reporting solution with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-a-generative-ai-powered-business-reporting-solution-with-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T15:53:15Z",
    "summary": "This post introduces generative AI guided business reporting—with a focus on writing achievements & challenges about your business—providing a smart, practical solution that helps simplify and accelerate internal communication and reporting.",
    "content": "Traditional business reporting processes are often time-consuming and inefficient. Associates typically spend about two hours per month preparing their reports, while managers dedicate up to 10 hours per month aggregating, reviewing, and formatting submissions. This manual approach often leads to inconsistencies in both format and quality, requiring multiple cycles of review. Additionally, reports are fragmented across various systems, making consolidation and analysis more challenging.\nGenerative artificial intelligence (AI) presents a compelling solution to these reporting challenges. According to a Gartner survey , generative AI has become the most widely adopted AI technology in organizations, with 29% already putting it into active use.\nThis post introduces generative AI guided business reporting—with a focus on writing achievements & challenges about your business—providing a smart, practical solution that helps simplify and accelerate internal communication and reporting. Built following Amazon Web Services (AWS) best practices, with this solution you will spend less time writing reports and more time focusing on driving business results. This solution tackles three real-world challenges:\n\nUncover valuable insights from vast amounts of data\nManage risks associated with AI implementation\nDrive growth through improved efficiency and decision-making\n\nThe full solution code is available in our GitHub repo , allowing you to deploy and test this solution in your own AWS environment.\nThe generative AI solution enhances the reporting process through automation. By utilizing large language model (LLM) processing, the reporting system can generate human-readable reports, answer follow-up questions, and make insights more accessible to non-technical stakeholders. This automation reduces costs and the need for extensive human resources while minimizing human error and bias. The result is a level of accuracy and objectivity that’s difficult to achieve with manual processes, ultimately leading to more efficient and effective business reporting.\nSolution overview\nThis generative AI-powered Enterprise Writing Assistant demonstrates a modern, serverless architecture that leverages AWS’s powerful suite of services to deliver an intelligent writing solution. Built with scalability and security in mind, this system combines AWS Lambda functions, Amazon Bedrock for AI capabilities, and various AWS services to create a robust, enterprise-grade writing assistant that can help organizations streamline content creation processes while maintaining high standards of quality and consistency.\n\nThis solution uses a serverless, scalable design built on AWS services. Let’s explore how the components work together:\nUser interaction layer\n\nUsers access the solution through a browser that connects to a frontend web application hosted on Amazon S3 and distributed globally via Amazon CloudFront for optimal performance\nAmazon Cognito user pools handle authentication and secure user management\n\nAPI layer\n\nTwo API types in Amazon API Gateway manage communication between frontend and backend:\n\nWebSocket API enables real-time, bidirectional communication for report writing and editing\nREST API handles transactional operations like submitting and retrieving reports\n\nAmazon CloudWatch monitors both APIs for operational visibility\nDedicated AWS Lambda authorizers secure both APIs by validating user credentials\n\nOrchestration layer\n\nSpecialized AWS Lambda functions orchestrate the core business logic:\n\nBusiness Report Writing Lambda handles report drafting and user assistance\nRephrase Lambda improves report clarity and professionalism\nSubmission Lambda processes final report submissions\nView Submission Lambda retrieves previously submitted reports\n\nAI and storage layer\n\nAmazon Bedrock provides the LLM capabilities for report writing and rephrasing\nTwo Amazon DynamoDB tables store different types of data:\n\nSession Management table maintains conversation context during active sessions\nBusiness Report Store table permanently archives completed reports\n\nThis architecture facilitates high availability, automatic scaling, and cost optimization by using serverless components that only incur charges when in use. Communications between components are secured following AWS best practices.\nYou can deploy this architecture in your own AWS account by following the step-by-step instructions in the GitHub repository .\nReal-world workflow: Report generation and rephrasing\nThe system’s workflow begins by analyzing and categorizing each user input through a classification process. This classification determines how the system processes and responds to the input. The system uses specific processing paths based on three distinct classifications:\n\nQuestion or command : When the system classifies the input as a question or command, it activates the LLM with appropriate prompting to generate a relevant response. The system stores these interactions in the conversation memory, allowing it to maintain context for future related queries. This contextual awareness provides coherent and consistent responses that build upon previous interactions.\nVerify submission : For inputs requiring verification, the system engages its evaluation protocols to provide detailed feedback on your submission. While the system stores these interactions in the conversation memory, it deliberately bypasses memory retrieval during the verification process. This design choice enables the verification process based solely on the current submission’s merits, without influence from previous conversations. This approach reduces system latency and facilitates more accurate and unbiased verification results.\nOutside of scope : When the input falls outside the system’s defined parameters, it responds with the standardized message: “Sorry, I can only answer writing-related questions.” This maintains clear boundaries for the system’s capabilities and helps prevent confusion or inappropriate responses.\n\nThese classifications support efficient processing while maintaining appropriate context only where necessary, optimizing both performance and accuracy in different interaction scenarios.\nUser experience walkthrough\nNow that we have explored the architecture, let’s dive into the user experience of our generative AI-powered Enterprise Writing Assistant. The following walkthrough demonstrates the solution in action, showcasing how AWS services come together to deliver a seamless, intelligent writing experience for enterprise users.\nHome page\nThe home page offers two views: Associate view and Manager view.\nAssociate view\nWithin the Associate view, you have three options: Write Achievement , Write Challenge , or View Your Submissions . For this post, we walk through the Achievement view. The Challenge view follows the same process but with different guidelines. In the Achievement view, the system prompts you to either ask questions or make a submission. Inputs go through the generative AI workflow. The following example demonstrates an incomplete submission, along with the system’s feedback. This feedback includes a visual summary that highlights the missing or completed components. The system evaluates the submission based on a predefined guideline . Users can adapt this approach in their solutions. At this stage, the focus should not be on grammar or formatting, but rather on the overall concept.\n\nIf the system is prompted with an irrelevant question, it declines to answer to avoid misuse.\n\nThroughout the conversation, you can ask questions related to writing a business report (achievement, or challenge about the business).\n\nOnce all criteria is met, the system can automatically rephrase the input text to fix grammatical and formatting issues. If you need to make changes to the input text, you can click the Previous button, which will take you back to the stage where you can modify your submission.\n\nAfter rephrasing, the system shows both the original version and the rephrased version with highlighted differences.\n\nThe system also automatically extracts customer name metadata.\n\nWhen complete, you can save or continue editing the output.\nManager view\nIn the Manager view, you have the ability to aggregate multiple submissions from direct reports into a consolidated roll-up report. The following shows how this interface appears.\nPrerequisites\nTo deploy this solution in your AWS account, the following is needed:\n\nAn AWS account with administrative access\nAWS CLI (2.22.8) installed and configured\nAccess to Amazon Bedrock models (Claude or Anthropic Claude)\nNode.js (20.12.7) the frontend components\nGit for cloning the repository\n\nDeploy the solution\nThe generative AI Enterprise Report Writing Assistant uses AWS CDK for infrastructure deployment, making it straightforward to set up in your AWS environment:\n\nClone the GitHub repository:\n\ngit clone https://github.com/aws-samples/sample-generative AI-enterprise-report-writing-assistant.git && cd sample-generative AI- enterprise-report-writing-assistant\n\nInstall dependencies:\n\nnpm install\n\nDeploy the application to AWS:\n\ncdk deploy\n\nAfter deployment completes, wait 1-2 minutes for the AWS CodeBuild process to finish.\nAccess the application using the VueAppUrl from the CDK/CloudFormation outputs.\n\nThe deployment creates the necessary resources including Lambda functions, API Gateways, DynamoDB tables, and the frontend application hosted on S3 and CloudFront.\nFor detailed configuration options and customizations, refer to the README in the GitHub repository.\nClean up resources\nTo avoid incurring future charges, delete the resources created by this solution when they are no longer needed:\n\ncdk destroy\n\nThis command removes the AWS resources provisioned by the CDK stack, including:\n\nLambda functions\nAPI Gateway endpoints\nDynamoDB tables\nS3 buckets\nCloudFront distributions\nCognito user pools\n\nBe aware that some resources, like S3 buckets containing deployment artifacts, might need to be emptied before they can be deleted.\nConclusion\nTraditional business reporting is time-consuming and manual, leading to inefficiencies across the board. The generative AI Enterprise Report Writing Assistant represents a significant leap forward in how organizations approach their internal reporting processes. By leveraging generative AI technology, this solution addresses the traditional pain points of business reporting while introducing capabilities that were previously unattainable. Through intelligent report writing assistance with real-time feedback, automated rephrasing for clarity and professionalism, streamlined submission and review processes, and robust verification systems, the solution delivers comprehensive support for modern business reporting needs. The architecture facilitates secure, efficient processing, striking the crucial balance between automation and human oversight. As organizations continue to navigate increasingly complex business problems, the ability to generate clear, accurate, and insightful reports quickly becomes not just an advantage but a necessity. The generative AI Enterprise Report Writing Assistant provides a framework that can scale with your organization’s needs while maintaining consistency and quality across the levels of reporting.\nWe encourage you to explore the GitHub repository to deploy and customize this solution for your specific needs. You can also contribute to the project by submitting pull requests or opening issues for enhancements and bug fixes.\nFor more information about generative AI on AWS, refer to the AWS Generative AI resource center .\nResources\n\nAWS CDK Documentation\nAmazon Bedrock Documentation\nVue.js Documentation\nCloudScape Design System\nLangChain Documentation\nAWS Amplify\n\nAbout the authors\nNick Biso is a Machine Learning Engineer at AWS Professional Services. He solves complex organizational and technical challenges using data science and engineering. In addition, he builds and deploys AI/ML models on the AWS Cloud. His passion extends to his proclivity for travel and diverse cultural experiences.\nMichael Massey is a Cloud Application Architect at Amazon Web Services, where he specializes in building frontend and backend cloud-native applications. He designs and implements scalable and highly-available solutions and architectures that help customers achieve their business goals.\nJeff Chen is a Principal Consultant at AWS Professional Services, specializing in guiding customers through application modernization and migration projects powered by generative AI. Beyond GenAI, he delivers business value across a range of domains including DevOps, data analytics, infrastructure provisioning, and security, helping organizations achieve their strategic cloud objectives.\nJundong Qiao is a Sr. Machine Learning Engineer at AWS Professional Service, where he specializes in implementing and enhancing AI/ML capabilities across various sectors. His expertise encompasses building next-generation AI solutions, including chatbots and predictive models that drive efficiency and innovation.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "556faa028ba7c356",
    "title": "Safeguard generative AI applications with Amazon Bedrock Guardrails",
    "url": "https://aws.amazon.com/blogs/machine-learning/safeguard-generative-ai-applications-with-amazon-bedrock-guardrails/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T15:50:54Z",
    "summary": "In this post, we demonstrate how you can address these challenges by adding centralized safeguards to a custom multi-provider generative AI gateway using Amazon Bedrock Guardrails.",
    "content": "Enterprises aiming to automate processes using AI agents or enhance employee productivity using AI chat-based assistants need to enforce comprehensive safeguards and audit controls for responsible use of AI and processing of sensitive data by large language models (LLMs). Many have developed a custom generative AI gateway or have adopted an off-the-shelf solution (such as LiteLLM or Kong AI Gateway ) to provide their AI practitioners and developers with access to LLMs from different providers. However, enforcing and maintaining consistent policies for prompt safety and sensitive data protection across a growing list of LLMs from various providers at scale is challenging.\nIn this post, we demonstrate how you can address these challenges by adding centralized safeguards to a custom multi-provider generative AI gateway using Amazon Bedrock Guardrails . Amazon Bedrock Guardrails provides a suite of safety features that help organizations build responsible generative AI applications at scale. You will learn how to use Amazon Bedrock ApplyGuardrail API to help enforce consistent policies for prompt safety and sensitive data protection for LLMs from both Amazon Bedrock and third-party providers such as Microsoft Azure OpenAI. The proposed solution provides additional benefits of central logging and monitoring, analytics, and a chargeback mechanism.\nSolution overview\nThere are several requirements you need to meet to safeguard generative AI applications with centralized guardrails. First, organizations need a robust and scalable infrastructure setup for the generative AI gateway and its guardrails components. The solution also needs a comprehensive logging and monitoring system to track AI interactions and analytics capabilities to assess usage patterns and compliance. For sensitive data protection, organizations need to establish clear data governance policies and implement appropriate safety controls. Additionally, they need to develop or integrate a chargeback mechanism to track and allocate AI usage costs across different departments or projects. Knowledge of regulatory requirements specific to their industry is crucial to make sure the guardrails are properly configured to meet compliance standards.\nThe following diagram depicts a conceptual illustration of our proposed solution. The workflow begins when authenticated users send HTTPS requests to the generative AI gateway, a centralized application running on Amazon Elastic Container Service (Amazon ECS) that serves as the primary interface for the LLM interactions. Within the generative AI gateway application logic, each incoming request is first forwarded to the Amazon Bedrock ApplyGuardrail API for content screening. The generative AI gateway then evaluates the content against predefined configurations, making critical decisions to either block the request entirely, mask sensitive information, or allow it to proceed unmodified.\nThis evaluation process, integral to the functionality of the generative AI gateway, facilitates adherence to established safety and compliance guidelines. For requests that pass this screening, the generative AI gateway logic determines the appropriate LLM provider (either Amazon Bedrock or a third-party service) based on the user’s specifications. The screened content is then forwarded to the selected LLM for processing. Finally, the generative AI gateway receives the LLM’s response and returns it to the user, completing the interaction cycle. The response flow follows two distinct paths: blocked requests result in users receiving a blocked content message, and approved requests deliver the model’s response with the necessary content masking applied to the user prompt. In our implementation, guardrails are only applied to the input or prompt and not to the LLM responses. This streamlined process provides a unified approach to LLM access, security, and compliance for both Amazon Bedrock and third-party providers.\n\nThe generative AI gateway application is hosted on AWS Fargate , and it’s built using FastAPI . The application interacts with other Amazon Web Services (AWS) services such as Amazon Simple Storage Service (Amazon S3), Amazon Bedrock, Amazon Kinesis and Amazon Data Firehose . The solution includes a robust data persistence layer that captures the interaction details and stores them on Amazon S3 through Amazon Kinesis Data Streams and Amazon Data Firehose. Data persisted includes sanitized requests and responses, transaction information, guardrail metadata, and blocked content with associated metadata. This comprehensive logging facilitates full auditability and enables continuous improvement of the guardrail mechanisms.\nSolution components\nScalability of the solution is achieved using the following tools and technologies:\n\nnginx to provide maximum performance and stability of the application by load balancing requests within each container.\nGunicorn , a Python Web Server Gateway Interface (WSGI) HTTP server commonly used to serve Python web applications in production environments. It’s a high-performance server that can handle multiple worker processes and concurrent requests efficiently. Gunicorn supports synchronous communications only but has robust process management functionality.\nUvicorn to provide lightweight and asynchronous request handling. Although Gunicorn is synchronous, it supports using asynchronous worker types such as Uvicorn, with which asynchronous communication can be established. This is needed for applications with longer wait times. In case of fetching responses from LLMs, you should anticipate higher wait times.\nFastAPI to serve the actual requests at the generative AI gateway application layer.\nAmazon ECS Fargate cluster to host the containerized application on AWS, and AWS Auto Scaling to scale up or down the tasks or containers automatically.\nAmazon Elastic Container Registry (Amazon ECR) for storing the Docker image of the generative AI gateway application.\nElastic Load Balancing (ELB) and Application Load Balancer for load balancing of requests across ECS containers.\nHashiCorp Terraform for resource provisioning.\n\nThe following figure illustrates the architecture design of the proposed solution. Consumer applications (such as on-premises business app, inference app, Streamlit app, and Amazon SageMaker Studio Lab ), dashboard, and Azure Cloud components aren’t included in the accompanying GitHub repository. They’re included in the architecture diagram to demonstrate integrations with downstream and upstream systems.\n\nCentralized guardrails\nThe generative AI gateway enforces comprehensive security controls through Amazon Bedrock Guardrails, using the ApplyGuardrail API to implement multiple layers of protection. These guardrails provide four core safety features: content filtering to screen inappropriate or harmful content, denied topics to help prevent specific subject matter discussions, word filters to block specific terms or phrases, and sensitive information detection to help protect personal and confidential data.\nOrganizations can implement these controls using three configurable strength level—low, medium, and high. This way, business units can align their AI security posture with their specific risk tolerance and compliance requirements. For example, a marketing team might operate with low-strength guardrails for creative content generation, whereas financial or healthcare divisions might require high-strength guardrails for handling sensitive customer data. Beyond these basic protections, Amazon Bedrock Guardrails also includes advanced features such as contextual grounding and automated reasoning checks, which help detect and prevent AI hallucinations (instances where models generate false or misleading information). Users can extend the functionalities of the generative AI gateway to support these advanced features based on their use case.\nMulti-provider integration\nThe generative AI gateway is both LLM provider and model-agnostic, which enables seamless integration with multiple providers and LLMs. Users can specify their preferred LLM model directly in the request payload, allowing the gateway to route requests to the appropriate model endpoint. AWS Secrets Manager is used for storing the generative AI gateway API access tokens and access tokens from third-party LLMs such as Azure OpenAI. The generative AI gateway API token is used for authenticating the caller. The LLM access token is used for establishing client connection for third-party providers.\nLogging, monitoring and alerting\nA key advantage of implementing a generative AI gateway is its centralized approach to logging and monitoring the LLM interactions. Every interaction, including user requests and prompts, LLM responses, and user context, is captured and stored in a standardized format and location. Organizations can use this collection strategy to perform analysis, troubleshoot issues, and derive insights. Logging, monitoring, and alerting is enabled using the following AWS services:\n\nAmazon CloudWatch captures the container and application logs. We can create custom metrics on specific log messages and create an alarm that can be used for proactive alerting (for example, when a 500 Internal Server Error occurs)\nAmazon Simple Notification Service (Amazon SNS) for notification to a distribution list (for example, when a 500 Internal Server Error happens)\nKinesis Data Streams and Data Firehose for streaming request and response data and metadata to Amazon S3 (for compliance and analytics or chargeback). Chargeback is a mechanism to attribute costs to a hierarchy of owners. For instance, an application running on AWS would incur some costs for every service, however the application could be serving an employee working for a project governed by a business unit. Chargeback is a process where costs can be attributed to the lowest level of an individual user with potential to roll up at multiple intermediate levels all the way to the business unit.\nAmazon S3 for persisting requests and responses at the transaction level (for compliance), in addition to transaction metadata and metrics (for example, token counts) for analytics and chargeback.\nAWS Glue Crawler API and Amazon Athena for exposing a SQL table of transaction metadata for analytics and chargeback.\n\nRepository structure\nThe GitHub repository contains the following directories and files:\n\ngenai-gateway/\n├── src/ -- Main application code\n│ └── clients/ -- API endpoints\n│ ├── controllers/ --FastAPI application entry point\n│ ├── generators/ --LLM integration\n│ ├── persistence/ -- Persistence logic\n│ └── utils/\n├── terraform/ -- IaC\n├── tests/ -- Testing scripts\n│ └── regressiontests/\n├── .gitignore\n├── .env.example\n├── Dockerfile\n├── ngnix.conf\n├── asgi.py\n├── docker-entrypoint.sh\n├── requirements.txt\n├── serve.py\n└── README.md\n\nPrerequisites\nYou need the following prerequisites before deploying this solution:\n\nAn AWS Account\nAn AWS Identity and Access Management (IAM) role with the following permissions:\n\nAmazon S3 access (CreateBucket, PutObject, GetObject, DeleteObject)\nAWS Secrets Manager access\nAmazon CloudWatch logs access\nAmazon Bedrock service\nAmazon Bedrock foundation model (FM) access\nAmazon Bedrock Guardrails IAM permissions\n\nIAM permissions for Amazon Bedrock Guardrails:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"VisualEditor0\",\n\"Effect\": \"Allow\",\n\"Action\": [\n\"bedrock:ApplyGuardrail\",\n\"bedrock:ListGuardrails\",\n\"bedrock:GetGuardrail\"\n],\n\"Resource\": \"arn:aws:bedrock:<AWS_REGION>:<AWS_ACCOUNT_ID>:guardrail/*\"\n}\n]\n}\n\nAccess to the serverless FMs on Amazon Bedrock are automatically enabled. You don’t need to manually request or enable model access, but you can use IAM policies and service control policies to restrict model access as needed.\nExternal LLM endpoints configured in the customer environment. For example, Azure OpenAI endpoints must be created in the customer Azure account with the following naming convention: {model_name}-{azure_tier}-{azure_region} . For example, {gpt-4o}-{dev}-{eastus} .\n\nDeploy the solution\nIn the deployment guide provided in this section, we assumed that deployment instructions include steps for dev environment. Similar steps can be used for higher environments.\nTo safeguard generative AI applications with centralized guardrails, follow these steps:\n\nClone the GitHub repository and make sure environment variables for AWS authentication are available in your environment.\nExecute ./deploy.sh , which automatically sets up a Terraform state bucket, creates an IAM policy for Terraform, and provisions the infrastructure with dependencies.\nInvoke ./verify.sh to verify the deployment and make sure the environment is ready for testing.\nFollow the instructions in the README, Auth Token Generation for Consumers , to generate consumer authorization tokens.\nFollow the instructions in the README, Testing the Gateway , to test your own generative AI gateway.\n\nFor development and testing, the entire setup can be done on the developer laptop with the generative AI gateway server and the client running on the user laptop by following the local setup instructions in the README.\nExamples\nIn this first example, the following code sample is a curl command to invoke anthropic.claude-3-sonnet-20240229-v1:0 model with a high strength guardrail to demonstrate how the generative AI gateway guardrails perform against denied topics. The first example illustrates the effectiveness of the safety mechanism in blocking denied topics by asking the model, I want to sell my house and invest the proceeds in a single stock. Which stock should I buy? :\n\n#!/bin/bash\n# Default Configuration (can be overridden by environment variables)\nURL=${URL:-\"https://<your-alb-dns-name>/process\"}\nAPPID=${APPID:-\"admin\"}\nAPITOKEN=${APITOKEN:-\"<your-api-token>\"}\nMODEL=${MODEL:-\"anthropic.claude-3-sonnet-20240229-v1:0\"}\nUSER_PROMPT=${USER_PROMPT:-\"I want to sell my house and invest the proceeds in a single stock. Which stock should I buy?\"}\nSYSTEM_PROMPT=${SYSTEM_PROMPT:-\" You are an expert financial advisor\"}\nGUARDRAIL_STRENGTH=${GUARDRAIL_STRENGTH:-\"high\"}\nENABLE_GUARDRAIL=${ENABLE_GUARDRAIL:-\"true\"}\nUSERID=${USERID:-\"skoppar\"}\nCOSTCENTER=${COSTCENTER:-\"ags\"}\nMAX_TOKENS=${MAX_TOKENS:-20}\nREQUEST_ID=$(uuidgen | tr '[:upper:]' '[:lower:]' | tr -d '-')\nREQUEST_DATETIME=$(date -u +\"%Y-%m-%dT%H:%M:%S%z\")\n# Bedrock request payload\necho \"Sending request to $URL...\"\ncurl -k -X POST \"$URL\" \\\n-H \"Content-Type: application/json\" \\\n-H \"appid: $APPID\" \\\n-H \"apitoken: $APITOKEN\" \\\n-w \"\\nHTTP Status: %{http_code}\\n\" \\\n-d @- << EOF\n{\n\"requestid\": \"$REQUEST_ID\",\n\"requestdatetime\": \"$REQUEST_DATETIME\",\n\"appid\": \"$APPID\",\n\"userid\": \"$USERID\",\n\"costcenter\": \"$COSTCENTER\",\n\"provider\": \"amazon-bedrock\",\n\"apicontext\": \"chatcompletions\",\n\"requestbody\": {\n\"model\": \"$MODEL\",\n\"body\": {\n\"anthropic_version\": \"bedrock-2023-05-31\",\n\"max_tokens\": $MAX_TOKENS,\n\"system\": \"$SYSTEM_PROMPT\",\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": \"$USER_PROMPT\"\n}\n]\n},\n\"accept\": \"application/json\",\n\"contentType\": \"application/json\"\n},\n\"guardrail_strength\": \"$GUARDRAIL_STRENGTH\",\n\"enable_guardrail\": $ENABLE_GUARDRAIL\n}\nEOF\n\nThe following sample code is the output from the preceding curl command. This result includes the model’s generated text and modifications or interventions applied by the high-strength guardrails. Analyzing this output helps verify the effectiveness of the guardrails and makes sure that the model’s response aligns with the specified safety and compliance parameters:\n\n{\n\"transactionid\":\"ff73cd3c-b924-40b3-85d7-bcd36cf26ab6\",\n\"dt\":\"20251027\",\n\"transactionstartdate\":\"2025-10-27 15:51:48+0000\",\n\"requestid\":\"6b274e0ad6ad447a90d33e882687767f\",\n\"requestdatetime\":\"2025-10-27T15:51:47+0000\",\n\"appid\":\"admin\",\n\"provider\":\"amazon-bedrock\",\n\"costcenter\":\"ags\",\n\"userid\":\"skoppar\",\n\"promptlength\":125,\n\"guardrail_id\":[\n\"arn:aws:bedrock:us-east-1: <account-id>:guardrail/o9mj8miraler\"\n],\n\"guardrail_action\":[\n\"topicPolicy\"\n],\n\"enable_guardrail\":true,\n\"responsebody\":\"{\\\"usage\\\": {\\\"topicPolicyUnits\\\": 1, \\\"contentPolicyUnits\\\": 1, \\\"wordPolicyUnits\\\": 1, \\\"sensitiveInformationPolicyUnits\\\": 1, \\\"sensitiveInformationPolicyFreeUnits\\\": 1, \\\"contextualGroundingPolicyUnits\\\": 0}, \\\"action\\\": \\\"GUARDRAIL_INTERVENED\\\", \\\"outputs\\\": [{\\\"text\\\": \\\"Sorry, the content doesn't comply with Responsible AI policies so it cannot be processed!\\\"}], \\\"assessments\\\": [{\\\"topicPolicy\\\": {\\\"topics\\\": [{\\\"name\\\": \\\"investment_topic\\\", \\\"type\\\": \\\"DENY\\\", \\\"action\\\": \\\"BLOCKED\\\"}]}}]}\"\n}\n\nThe second example tests the ability of the generative AI gateway to help protect sensitive personal information. It simulates a user query containing personally identifiable information (PII) such as a name, Social Security number, and email address.\n\nUSER_PROMPT=\"My name is John Smith, my SSN is 123-45-6789, and my email is john.smith@email.com. Can you help me with my account?\" ./bedrock_curl_test.sh\n\nIn this case, the guardrail successfully intervened and masked PII data before sending the user query to the LLM, as evidenced by the guardrail_action field, indicating the sensitiveInformationPolicy was applied:\n\n{\n\"transactionid\": \"47665380-bf9f-4ed2-836e-916199a45518\",\n\"dt\": \"20250626\",\n\"transactionstartdate\": \"2025-06-26 23:02:59+0000\",\n\"requestid\": \"ebaf1fbffcd344f3b3d96353e772205d\",\n\"requestdatetime\": \"2025-06-26T23:02:59+0000\",\n\"appid\": \"admin\",\n\"provider\": \"amazon-bedrock\",\n\"costcenter\": \"proserve\",\n\"userid\": \"bommi\",\n\"promptlength\": 149,\n\"guardrail_id\": [\n\"arn:aws:bedrock:us-east-1:<account-id>:guardrail/jvf0bhhvtyf7\",\n\"arn:aws:bedrock:us-east-1:<account-id>:guardrail/uekx7u8xra91\"\n],\n\"guardrail_action\": [\"sensitiveInformationPolicy\"],\n\"enable_guardrail\": true,\n\"responsebody\": {\n\"id\": \"msg_bdrk_012UbTrdpzy3iZ2s9wcKF6PU\",\n\"type\": \"message\",\n\"role\": \"assistant\",\n\"model\": \"claude-3-sonnet-20240229\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"I'm afraid I cannot provide any personal information or account details. For privacy reasons, I do not\"\n}\n],\n\"stop_reason\": \"max_tokens\",\n\"stop_sequence\": null,\n\"usage\": { \"input_tokens\": 57, \"output_tokens\": 20 }\n}\n}\n\nFor more comprehensive test scripts, please refer to the /test directory of the repository. These additional scripts offer a wider range of test cases and scenarios to thoroughly evaluate the functionality and performance of the generative AI gateway.\nClean up\nUpon concluding your exploration of this solution, you can clean up the resources by following these steps:\n\nEmploy the terraform destroy to delete the resources provisioned by Terraform.\n(Optional) From the AWS Management Console or AWS Command Line Interface (AWS CLI), delete resources that aren’t deleted by Terraform (such as the S3 bucket, ECR repository, and EC2 subnet).\n\nCost estimation\nThis section describes the underlying cost structure for running the solution. When implementing this solution, there are several cost categories to be considered:\n\nLLM provider costs – These represent the charges for using foundation models through various providers, including models hosted on Amazon Bedrock and third-party providers. Costs are typically calculated based on:\n\nNumber of input and output tokens processed\nModel complexity and capabilities\nUsage volume and patterns\nService level requirements\n\nAWS infrastructure costs – These encompass the infrastructure expenses associated with generative AI gateway:\n\nCompute resources (Amazon ECS Fargate)\nLoad balancing (Application Load Balancer)\nStorage (Amazon S3, Amazon ECR)\nMonitoring (Amazon CloudWatch)\nData processing (Amazon Kinesis)\nSecurity services (AWS Secrets Manager)\n\nAmazon Bedrock Guardrails costs – These are specific charges for implementing safety and compliance features:\n\nContent filtering and moderation\nPolicy enforcement\nSensitive data protection\n\nThe following tables provide a sample cost breakdown for deploying and using generative AI gateway. For actual pricing, refer to the AWS Pricing Calculator .\nInfrastructure costs:\n\nService\nEstimated usage\nEstimated monthly cost\n\nAmazon ECS Fargate\n2 tasks, 1 vCPU, 2 GB RAM, running constantly\n$70–$100\n\nApplication Load Balancer\n1 ALB, running constantly\n$20–$30\n\nAmazon ECR\nStorage for Docker images\n$1–$5\n\nAWS Secrets Manager\nStoring API keys and tokens\n$0.40 per secret per month\n\nAmazon CloudWatch\nLog storage and metrics\n$10–$20\n\nAmazon SNS\nNotifications\n$1–$2\n\nAmazon Kinesis Data Streams\n1 stream, low volume\n$15–$25\n\nAmazon Data Firehose\n1 delivery stream\n$0.029 per GB processed\n\nAmazon S3\nStorage for logs and data\n$2–$5\n\nAWS Glue\nCrawler runs (assuming weekly)\n$5–$10\n\nAmazon Athena\nQuery execution\n$1–$5\n\nLLM and guardrails costs:\n\nService\nEstimated usage\nEstimated monthly cost\n\nAmazon Bedrock Guardrails\n10,000 API calls per month\n$10–$20\n\nClaude 3 Sonnet (Input)\n1M tokens per month at $0.003 per 1K tokens\n$3\n\nClaude 3 Sonnet (Output)\n500K tokens per month at $0.015 per 1K tokens\n$7.50\n\nGPT-4 Turbo (Azure OpenAI)\n1M tokens per month at $0.01 per 1K tokens\n$10\n\nGPT-4 Turbo Output\n500K tokens per month at $0.03 per 1K tokens\n$15\n\nTotal estimated cost\n\n$170–$260 (Base)\n\nLLM costs can vary significantly based on the number of API calls, input/output token lengths, model selection, and volume discounts. We consider a moderate usage scenario to be about 50–200 queries per day, with an average input length of 500 tokens and average output length of 250 tokens. These costs could increase substantially with higher query volumes, longer conversations, use of more expensive models, and multiple model calls per request.\nConclusion\nThe centralized guardrails integrated with a custom multi-provider generative AI gateway solution offers a robust and scalable approach for enterprises to safely use LLMs while maintaining security and compliance standards. Through its implementation of Amazon Bedrock Guardrails ApplyGuardrail API , the solution provides consistent policy enforcement for prompt safety and sensitive data protection across both Amazon Bedrock and third-party LLM providers.\nKey advantages of this solution include:\n\nCentralized guardrails with configurable security levels\nMulti-provider LLM integration capabilities\nComprehensive logging and monitoring features\nProduction-grade scalability through containerization\nBuilt-in compliance and audit capabilities\n\nOrganizations, particularly those in highly regulated industries, can use this architecture to adopt and scale their generative AI implementations while maintaining control over data protection and AI safety regulations. The solution’s flexible design and robust infrastructure make it a valuable tool for enterprises that want to safely harness the power of generative AI while managing associated risks .\n\nAbout the authors\nHasan Shojaei Ph.D., is a Sr. Data Scientist with AWS Professional Services, where he helps customers across different industries such as sports, financial services, and manufacturing solve their business challenges using advanced AI/ML technologies. Outside of work, Hasan is passionate about books, photography, and skiing.\nSunita Koppar is a Senior Specialist Solutions Architect in Generative AI and Machine Learning at AWS, where she partners with customers across diverse industries to design solutions, build proof-of-concepts, and drive measurable business outcomes. Beyond her professional role, she is deeply passionate about learning and teaching Sanskrit, actively engaging with student communities to help them upskill and grow.\nAnuja Narwadkar is a Global Senior Engagement Manager in AWS Professional Services, specializing in enterprise-scale Machine Learning and GenAI transformations. She leads ProServe teams in strategizing, architecture, and building transformative AI/ML solutions on AWS for large enterprises across industries, including financial services. Beyond her professional role, she likes to drive AI up-skill initiatives especially for women, read and cook.\nKrishnan Gopalakrishnan is a Delivery Consultant at AWS Professional Services with 12+ years in Enterprise Data Architecture and AI/ML Engineering. He architects cutting-edge data solutions for Fortune 500 companies, building mission-critical pipelines and Generative AI implementations across retail, healthcare, fintech, and manufacturing. Krishnan specializes in scalable, cloud-native architectures that transform enterprise data into actionable AI-powered insights, enabling measurable business outcomes through data-driven decision making.\nBommi Shin is a Delivery Consultant with AWS Professional Services, where she helps enterprise customers implement secure, scalable artificial intelligence solutions using cloud technologies. She specializes in designing and building AI/ML and Generative AI platforms that address complex business challenges across a range of industries. Outside of work, she enjoys traveling, exploring nature, and delicious foods.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "6f1ff7d246014fbb",
    "title": "Scale creative asset discovery with Amazon Nova Multimodal Embeddings unified vector search",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-creative-asset-discovery-with-amazon-nova-multimodal-embeddings-unified-vector-search/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T15:45:02Z",
    "summary": "In this post, we describe how you can use Amazon Nova Multimodal Embeddings to retrieve specific video segments. We also review a real-world use case in which Nova Multimodal Embeddings achieved a recall success rate of 96.7% and a high-precision recall of 73.3% (returning the target content in the top two results) when tested against a library of 170 gaming creative assets. The model also demonstrates strong cross-language capabilities with minimal performance degradation across multiple langua...",
    "content": "Gaming companies face an unprecedented challenge in managing their advertising creative assets. Modern gaming companies produce thousands of video advertisements for A/B testing campaigns, with some organizations maintaining libraries with more than 100,000 video assets that grow by thousands of assets monthly. These assets are critical for user acquisition campaigns, where finding the right creative asset can make the difference between a successful launch and a costly failure.\nIn this post, we describe how you can use Amazon Nova Multimodal Embeddings to retrieve specific video segments. We also review a real-world use case in which Nova Multimodal Embeddings achieved a recall success rate of 96.7% and a high-precision recall of 73.3% (returning the target content in the top two results) when tested against a library of 170 gaming creative assets. The model also demonstrates strong cross-language capabilities with minimal performance degradation across multiple languages.\nTraditional methods for sorting, storing, and searching for creative assets can’t meet the dynamic needs of creative teams. Traditionally, creative assets have been manually tagged to enable keyword-based search and then organized in folder hierarchies, which are manually searched for the desired assets. Keyword-based search systems require manual tagging that is both labor-intensive and inconsistent. While large language model (LLM) solutions such as LLM-based automatic tagging offer powerful multimodal understanding capabilities, they can’t scale to meet the needs of creative teams to perform varied, real-time searches across massive asset libraries.\nThe core challenge lies in semantic search for creative asset discovery. The search needs to support unpredictable search requirements that can’t be pre-organized with fixed prompts or predefined tags. When creative professionals search for the character is pinched away by hand , or A finger taps a card in the game , the system must understand not just the keywords, but the semantic meaning across different media types.\nThis is where Nova Multimodal Embeddings transforms the landscape. Nova Multimodal Embeddings is a state-of-the-art multimodal embedding model for agentic  Retrieval-Augmented Generation (RAG)  and semantic search applications with a unified vector space architecture, available in  Amazon Bedrock . More importantly, the model generates embeddings directly from video assets without requiring intermediate conversion steps or manual tagging.\nNova Multimodal Embeddings video embedding generation enables true semantic understanding of video content. Nova Multimodal Embeddings can analyze the visual scenes, actions, objects, and context within videos to create rich semantic representations. When you search for the character is pinched away by hand , the model understands the specific action, visual elements, and context described—not just keyword matches. This semantic capability avoids the fundamental limitations of keyword-based search systems, so that creative teams can find relevant video content using natural language descriptions that would be impossible to tag or organize in advance with traditional approaches.\nSolution overview\nIn this section, you learn about Nova Multimodal Embeddings and its key capabilities, advantages, and integration with AWS services to create a comprehensive multimodal search architecture. The multimodal search architecture described in this post provides:\n\nInput flexibility : Accepts text queries, uploaded images, videos, and audio files as search inputs\nCross-modal retrieval : Users can find video, image, and audio content using text descriptions or use uploaded images to discover similar visual content across multiple media types\nOutput precision : Returns ranked results with similarity scores, precise timestamps for video segments, and detailed metadata\nSynchronous search and retrieval : Provides immediate search results through pre-computed embeddings and efficient vector similarity matching\nUnified asynchronous architecture : Search queries are processed asynchronously to handle varying processing times and provide a consistent user experience\n\nNova Multimodal Embeddings\nNova Multimodal Embeddings is the first unified embedding model that supports text, documents, images, video, and audio through a single model to enable cross-modal retrieval with industry-leading accuracy. It provides the following key capabilities and advantages:\n\nUnified vector space architecture : Unlike traditional tag-based systems or multimodal-to-text conversion pipelines that require complex mappings between different vector spaces, Nova Multimodal Embeddings generates embeddings that exist in the same semantic space regardless of input modality. This means a text description of racing car will be spatially close to images and videos containing racing cars, enabling intuitive cross-modal search.\nFlexible embedding dimensions : Nova Multimodal Embeddings offers four embedding dimension options (256, 384, 1024, and 3072), trained using Matryoshka Representation Learning (MRL), enabling low-latency retrieval with minimal accuracy loss across different dimensions. The 1024-dimension option provides an optimal balance for most enterprise applications, while 3072 dimensions offer maximum precision for critical use cases.\nSynchronous and asynchronous APIs : The model supports both real-time embedding generation for smaller content and asynchronous processing for large files with automatic segmentation. This flexibility allows systems to handle everything from quick text query retrieval to indexing hours of video content.\nAdvanced video understanding : For video content, Nova Multimodal Embeddings provides sophisticated segmentation capabilities, breaking long videos into meaningful segments (1–30 seconds) and generating embeddings for each segment. For advertising creative management, this segmented approach aligns perfectly with typical production workflows where creative teams need to manage and retrieve specific video segments rather than entire videos.\n\nIntegration with AWS services\nNova Multimodal Embeddings integrates seamlessly with other AWS services to create a production-ready multimodal search architecture:\n\nAmazon Bedrock : Provides foundation model access with enterprise-grade security and scalability\nAmazon OpenSearch Service : Serves as the vector database for storing and searching embeddings with millisecond-level query response times\nAWS Lambda : Handles serverless processing for embedding generation and search operations\nAmazon Simple Storage Service (Amazon S3) : Stores original media files and processing results with unlimited scalability\nAmazon API Gateway : Provides RESTful APIs for frontend integration\n\nTechnical implementation\nSystem architecture\nThe system operates through two primary workflows: content ingestion and search retrieval, shown in the following architecture diagram and described in the following sections.\n\nSystem execution flow\nThe content ingestion workflow transforms raw media files into searchable vector embeddings through a series of automated steps. This process begins when users upload content and culminates with the storage of embeddings in the vector database, making the content discoverable through semantic search.\n\nUser interaction : Users access the web interface through Amazon CloudFront , uploading media files (images, videos, and audio) using drag-and-drop or file selection.\nAPI processing : Files are converted to base64 format and sent through API Gateway to the main Lambda function for file type and size limit validation (the maximum file size is 10 MB).\nAmazon S3 storage : Lambda decodes base64 data and uploads raw files to Amazon S3 for persistent storage.\nAmazon S3 event trigger : Amazon S3 automatically triggers a dedicated embedding Lambda function when new files are uploaded, initiating the embedding generation process.\nAmazon Bedrock invocation : The embedding Lambda function asynchronously invokes the Amazon Bedrock Nova Multimodal Embeddings model to generate unified embedding vectors for multiple media types.\nVector storage : The embedding Lambda function stores generated embedding vectors along with metadata in OpenSearch Service, creating a searchable vector database.\n\nSearch and retrieval workflow\nThrough the search and retrieval workflow, users can find relevant content using multimodal queries. This process converts user queries into embeddings and performs similarity searches against the pre-built vector database, returning ranked results based on semantic similarity across different media types.\n\nSearch request : Users initiate searches through the web interface using uploaded files or text queries, with options to select different search modes (visual, semantic, or audio).\nAPI processing : Search requests are sent through API Gateway to the search API Lambda function for initial processing.\nTask creation : The search API Lambda function creates search task records in Amazon DynamoDB and sends messages to an Amazon Simple Queue Service (Amazon SQS) queue for asynchronous processing.\nQueue processing : The search API Lambda function sends messages to an Amazon SQS queue for asynchronous processing. This unified asynchronous architecture handles the API requirements of Nova Multimodal Embeddings (async invocation for video segmentation), prevents API Gateway timeouts, and helps ensure scalable processing for multiple query types.\nWorker activation : The search worker Lambda function is triggered by Amazon SQS messages, extracting search parameters and preparing for embedding generation.\nQuery embedding : The worker Lambda function invokes the Amazon Bedrock Nova Multimodal Embeddings model to generate embedding vectors for search queries (text or uploaded files).\nVector search : The worker Lambda function performs similarity search using cosine similarity in OpenSearch Service, then updates the results in DynamoDB for frontend polling.\n\nWorkflow integration\nThe two workflows described in the previous section share common infrastructure components but serve different purposes:\n\nUpload workflow (1–6) : Focuses on ingesting and processing media files to build a searchable vector database\nSearch workflow (A–G) : Processes user queries and retrieves relevant results from the pre-built vector database\nShared components : Both workflows use the same Amazon Bedrock model, OpenSearch Service indexes, and core AWS services\n\nKey technical features\n\nUnified vector space : All media types (images, videos, audio, and text) are embedded into the same dimensional space, enabling true cross-modal search.\nAsynchronous processing : The unified asynchronous architecture handles Amazon Nova Multimodal Embedding API requirements and helps ensure scalable processing through Amazon SQS queues and worker Lambda functions.\nMulti-modal search : Supports text-to-image, text-to-video, text-to-audio, and file-to-file similarity searches.\nScalable architecture : The serverless design automatically scales based on demand.\nStatus tracking : The polling mechanism provides updates on asynchronous processing status and search results.\n\nCore embedding generation using Nova Multimodal Embeddings\n\nrequest_body = {\n\"schemaVersion\": \"amazon.nova-embedding-v1:0\",\n\"taskType\": \"SEGMENTED_EMBEDDING\",\n\"segmentedEmbeddingParams\": {\n\"embeddingPurpose\": \"GENERIC_INDEX\",\n\"embeddingDimension\": self.dimension,\n\"video\": {\n\"format\": self._get_video_format(s3_uri),\n\"source\": {\n\"s3Location\": {\n\"uri\": s3_uri\n}\n},\n\"embeddingMode\": \"AUDIO_VIDEO_COMBINED\",\n\"segmentationConfig\": {\n\"durationSeconds\": 5 # Default 5 second segmentation\n}\n}\n}\n}\noutput_config = {\n\"s3OutputDataConfig\": {\n\"s3Uri\": output_s3_uri\n}\n}\n\nprint(f\"Nova async embedding request: {json.dumps(request_body, indent=2)}\")\n\n# Start an asynchronous call\nresponse = self.bedrock_client.start_async_invoke(\nmodelId=self.model_id,\nmodelInput=request_body,\noutputDataConfig=output_config\n)\n\ninvocation_arn = response['invocationArn']\nprint(f\"Started Nova async embedding job: {invocation_arn}\")\n\nCross-modal search implementation\nThe heart of the system lies in its intelligent cross-modal search capabilities using OpenSearch k-nearest neighbor (KNN) search, as shown in the following code:\n\ndef search_similar(self, query_vector: List[float], embedding_field: str,\ntop_k: int = 20, filters: Dict[str, Any] = None) -> List[Dict[str, Any]]:\n\"\"\"Search for similar vectors using OpenSearch KNN\"\"\"\nquery = {\n\"size\": top_k,\n\"query\": {\n\"knn\": {\nembedding_field: {\n\"vector\": query_vector,\n\"k\": top_k\n}\n}\n},\n\"_source\": [\n\"s3_uri\", \"file_type\", \"timestamp\", \"media_type\",\n\"segment_index\", \"start_time\", \"end_time\", \"duration\"\n]\n}\n\n# Add filters for media type or other criteria\nif filters:\nquery[\"query\"] = {\n\"bool\": {\n\"must\": [query[\"query\"]],\n\"filter\": [{\"terms\": {k: v}} for k, v in filters.items()]\n}\n}\n\nresponse = self.client.search(index=self.index, body=query)\n\n# Process and return results with metadata\nresults = []\nfor hit in response['hits']['hits']:\nsource = hit['_source']\nresults.append({\n'score': hit['_score'],\n's3_uri': source['s3_uri'],\n'file_type': source['file_type'],\n'media_type': source.get('media_type', 'unknown'),\n'segment_info': {\n'segment_index': source.get('segment_index'),\n'start_time': source.get('start_time'),\n'end_time': source.get('end_time')\n}\n})\n\nreturn results\n\nVector storage and retrieval\nThe system uses OpenSearch Service as its vector database, optimizing indexing for different embedding types, as shown in the following code:\n\ndef create_index_if_not_exists(self):\n\"\"\"Create OpenSearch index with optimized schema\"\"\"\nif not self.client.indices.exists(self.index):\nindex_body = {\n'settings': {\n'index': {\n'knn': True,\n\"mapping.total_fields.limit\": 5000\n}\n},\n'mappings': {\n'properties': {\n# Vector fields for different modalities with HNSW configuration\n'visual_embedding': {\n'type': 'knn_vector',\n'dimension': VECTOR_DIMENSION,\n'method': {\n'name': 'hnsw',\n'space_type': 'cosinesimil',\n'engine': 'faiss'\n}\n},\n'text_embedding': {\n'type': 'knn_vector',\n'dimension': VECTOR_DIMENSION,\n'method': {\n'name': 'hnsw',\n'space_type': 'cosinesimil',\n'engine': 'faiss'\n}\n},\n'audio_embedding': {\n'type': 'knn_vector',\n'dimension': VECTOR_DIMENSION,\n'method': {\n'name': 'hnsw',\n'space_type': 'cosinesimil',\n'engine': 'faiss'\n}\n},\n# Metadata fields\n's3_uri': {'type': 'keyword'},\n'media_type': {'type': 'keyword'},\n'file_type': {'type': 'keyword'},\n'timestamp': {'type': 'date'},\n'segment_index': {'type': 'integer'},\n'start_time': {'type': 'float'},\n'end_time': {'type': 'float'},\n'duration': {'type': 'float'},\n# Amazon Nova Multimodal Embeddings support audio_video_combined fields\n'audio_video_combined_embedding': {\n'type': 'knn_vector',\n'dimension': VECTOR_DIMENSION,\n'method': {\n'name': 'hnsw',\n'space_type': 'cosinesimil',\n'engine': 'faiss'\n}\n},\n# model fields\n'model_type': {'type': 'keyword'},\n'model_version': {'type': 'keyword'},\n'vector_dimension': {'type': 'integer'},\n# document fields\n'document_type': {'type': 'keyword'},\n'source_file': {'type': 'keyword'},\n'page_number': {'type': 'integer'},\n'total_pages': {'type': 'integer'}\n}\n}\n}\nself.client.indices.create(self.index, body=index_body)\nprint(f\"Created index: {self.index}\")\n\nThis schema supports multiple modalities (visual, text, and audio) with KNN indexing enabled, enabling flexible cross-modal search while preserving detailed metadata about video segments and model provenance.\nReal-world application and performance\nUsing a gaming industry use case, let’s examine a scenario of a creative professional who needs to find video segments showing characters celebrating victory with bright visual effects for a new campaign.\nTraditional approaches would require:\n\nManual tagging of thousands of videos, which is labor-intensive and might be inconsistent\nKeyword-based search that misses semantic nuances\nLLM-based analysis that’s too slow and expensive for real-time queries\n\nWith Nova Multimodal Embeddings, the same query becomes a straightforward text search that:\n\nGenerates a semantic embedding of the query\nSearches across all video segments in the unified vector space\nReturns ranked results based on semantic similarity\nProvides precise timestamps for relevant video segments\n\nPerformance metrics and validation\nBased on comprehensive testing with gaming industry partners using a library of 170 assets (130 videos and 40 images), Nova Multimodal Embeddings demonstrated exceptional performance across 30 test cases:\n\nRecall success rate : 96.7% of test cases successfully retrieved the target content\nHigh-precision recall : 73.3% of test cases returned the target content in the top two results\nCross-modal accuracy : Superior accuracy in text-to-video retrieval compared to traditional approaches\n\nKey findings\nHere’s what we learned from the results of our testing:\n\nSegmentation strategy : For advertising creative workflows, we recommend using SEGMENTED_EMBEDDING with 5-second video segments because it aligns with typical production requirements. Creative teams commonly need to segment original advertising materials for management and retrieve specific clips during production workflows, making the segmentation functionality of Nova Multimodal Embeddings particularly valuable for these use cases.\nEvaluation framework : To assess Nova Multimodal Embeddings effectiveness for your use case, focus on testing the following core capabilities:\nObject an entity detection : Test queries such as red sports car or character with sword to evaluate object recognition across modalities\nScene and context understanding: Assess contextual searches such as outdoor celebration scene or indoor meeting environment\nActivities and actions : Validate action-based queries such as running character or clicking interface elements\nVisual attributes : Test attribute-specific searches including colors, styles, and visual characteristics\nAbstract semantics : Evaluate conceptual understanding with queries such as victory celebration or tense atmosphere\nTesting methodology : Build a representative test dataset from your content library, create diverse query types matching real user needs, and measure both recall success (finding relevant content) and precision (ranking quality). Focus on queries that reflect your team’s actual search patterns rather than generic test cases.\nMulti-language performance : Nova Multimodal Embeddings demonstrates strong cross-language capabilities, particularly excelling in Chinese language queries with a score of 78.2 compared to English queries at 89.3 (3072-dimension). This represents a language gap of only 11.1, significantly better than another leading multimodal model that shows substantial performance degradation across different languages.\n\nScalability and cost benefits\nThe serverless architecture provides automatic scaling while optimizing costs. Keep the following dimension performance details and cost optimization strategies in mind when designing your multi-modal asset discovery system.\nDimension performance:\n\n3072-dimension : Highest accuracy (89.3 for English and 78.2 for Chinese) and higher storage costs\n1024-dimension : Balanced performance (85.7 for English and 68.3 for Chinese);recommended for most use cases\n384/256-dimension : Cost-optimized options for large-scale deployments\n\nCost optimization strategies:\n\nSelect the dimension based on accuracy requirements compared to storage costs\nUse asynchronous processing for large files to avoid timeout costs\nUse pre-computed embeddings reduce recurring LLM inference costs\nUse serverless architecture with pay-as-you-go on-demand pricing to reduce costs during low-usage periods\n\nGetting started\nThis section provides the essential requirements and steps to deploy and run the Nova Multimodal Embeddings multimodal search system.\n\nAn AWS account with Amazon Bedrock access and Nova Multimodal Embeddings model availability\nAWS Command Line Interface (AWS CLI) v2 configured with appropriate permissions for resource creation\nNode.js 18+ and AWS CDK v2 installed\nPython 3.11 for infrastructure deployment\nGit for cloning the demonstration repository\n\nQuick deployment\nThe complete system can be deployed using the following automation scripts:\n\n# Clone the demonstration repository\ngit clone https://github.com/aws-samples/sample-multimodal-embedding-models\ncd sample-multimodal-embedding-models\n# Configure service prefix (optional)\n# Edit config/settings.py to customize SERVICE_PREFIX\n# Deploy Amazon Nova Multimodal Embeddings system\n./deploy_model.sh nova-segmented\n\nThe deployment script automatically:\n\nInstalls required dependencies\nProvisions AWS resources (Lambda, OpenSearch, Amazon S3, and API Gateway)\nBuilds and deploys the frontend interface\nConfigures API endpoints and CloudFront distribution\n\nAccessing the system\nAfter successful deployment, the system provides web interfaces for testing:\n\nUpload interface : For adding media files to the system\nSearch interface : For performing multimodal queries\nManagement interface : For monitoring processing status\n\nMulti-modal input support (optional)\nThis optional subsection enables the system to accept image and video inputs in addition to text queries for comprehensive multimodal search capabilities.\n\ndef search_by_image(self, image_s3_uri: str) -> Dict:\n\"\"\"Find similar content using image as query\"\"\"\nquery_embedding = self.nova_service.get_image_embedding(image_s3_uri)\n\n# Search across all media types using visual similarity\nreturn self.opensearch_manager.search_similar(\nquery_embedding=query_embedding,\nembedding_field='visual_embedding',\nsize=10\n)\n\nClean up\nTo avoid ongoing charges, use the following command to remove the AWS resources created during deployment:\n\n# Remove all system resources\n./destroy_model.sh nova-segmented\n\nConclusion\nAmazon Nova Multimodal Embeddings represents a fundamental shift in how organizations can manage and discover multimodal content at scale. By providing a unified vector space that seamlessly integrates text, images, and video content, Nova Multimodal Embeddings removes the traditional barriers that have limited cross-modal search capabilities. The complete source code and deployment scripts are available in the demonstration repository .\n\nAbout the authors\nJia Li is an Industry Solutions Architect at Amazon Web Services, focused on driving technical innovation and business growth in the gaming industry. With 20 years of full-stack game development experience, previously worked at companies such as Lianzhong, Renren, and Hungry Studio, serving as a game producer and director of a large-scale R&D center. Possesses deep insight into industry dynamics and business models.\nXiaowei Zhu is an Industry Solutions Builder at Amazon Web Services (AWS). With over 10 years of experience in mobile application development, he also has in-depth expertise in embedding search, automated testing and Vibe Coding. Currently, he is responsible for building AWS Game industry Assets and leading the development of the open-source application SwiftChat.\nHanyi Zhang is a Solutions Architect at AWS, focused on cloud architecture design for the gaming industry. With extensive experience in big data analytics, generative AI, and cloud observability, Hanyi has successfully delivered multiple large-scale projects with cutting edge AWS services.\nZepei Yu is a Solutions Architect at AWS, responsible for consulting and design of cloud computing solutions, and has extensive experience in AI/ML, DevOps, Gaming Industry, etc.\nBao Cao is a AWS Solutions Architect, responsible for architectural design based on AWS cloud computing solutions, helping customers build more innovative applications using leading cloud service technologies. Prior to joining AWS, worked at companies such as ByteDance, with over 10 years of extensive experience in game development and architectural design.\nXi Wan is a Solutions Architect at Amazon Web Services, responsible for consulting on and designing cloud computing solutions based on AWS. A strong advocate of the AWS Builder culture. With over 12 years of game development experience, has participated in the management and development of multiple game projects and possesses deep understanding and insight into the gaming industry.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "b1db0c4242ddc5be",
    "title": "OptiMind: A small language model with optimization expertise",
    "url": "https://www.microsoft.com/en-us/research/blog/optimind-a-small-language-model-with-optimization-expertise/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T14:00:00Z",
    "summary": "OptiMind is a small language model that converts business operation challenges, described naturally, into mathematical formulations that optimization software can solve. It reduces formulation time & errors & enables fast, privacy-preserving local use.\nThe post OptiMind: A small language model with optimization expertise appeared first on Microsoft Research .",
    "content": "At a glance\n\nMany real-world business problems can benefit from optimization, but translating decisions, constraints, and goals from natural language into optimization algorithms is slow.\n\nOptiMind is a small language model designed to convert business problems described in natural language into the mathematical formulations needed by optimization software.\n\nOptiMind is trained on a carefully curated, expert-aligned dataset and applies domain-specific hints and self-checks at inference time, improving its accuracy.\n\nOptiMind matches or exceeds the performance of much larger systems, can run locally to protect sensitive data, produces more reliable formulations, and reduces the time and expertise needed to prepare optimization models.\n\nEnterprises across industries, from energy to finance, use optimization models to plan complex operations like supply chains and logistics. These models work by defining three elements: the choices that can be made (such as production quantities or delivery routes), the rules and limits those choices must follow, and the goal, whether that’s minimizing costs, meeting customer demand, or improving efficiency.\n\nOver the past few decades, many businesses have shifted from judgment-based decision-making to data-driven approaches, leading to major efficiency gains and cost savings. Advances in AI promise to accelerate this shift even further, potentially cutting decision times from days to minutes while delivering better results.\n\nIn practice, however, turning real-world business problems into a form that optimization software can understand is challenging. This translation process requires expressing decisions, constraints, and objectives in mathematical terms. The work demands specialized expertise, and it can take anywhere from one day to several weeks to solve complex problems. \n\nTo address this challenge, we’re introducing OptiMind , a small language model designed to convert problems described in plain language into the mathematical formulations that optimization software needs. Built on a 20-billion parameter model, OptiMind is compact by today’s standards yet matches the performance of larger, more complex systems. Its modest size means it can run locally on users’ devices, enabling fast iteration while keeping sensitive business data on users’ devices rather than transmitting it to external servers.\n\nSpotlight: AI-POWERED EXPERIENCE\n\nMicrosoft research copilot experience\n\nDiscover more about research at Microsoft through our AI-powered experience\n\nStart now\n\nOpens in a new tab\n\nHow it works\n\nOptiMind incorporates knowledge from optimization experts both during training and when it’s being used to improve formulation accuracy at scale. Three stages enable this: domain-specific hints improve training data quality, the model undergoes fine-tuning, and expert reasoning guides the model as it works.\n\nFigure 1. From problem description to solution \n\nOne of the central challenges in developing OptiMind was the poor quality of existing public datasets for optimization problems. Many examples were incomplete or contained incorrect solutions. To address this, we developed a systematic approach that combines automation with expert review. It organizes problems into well-known categories, such as scheduling or routing, and identifies common error patterns within each. Using these insights, we generated expert-verified “hints” to guide the process, enabling the system to regenerate higher-quality solutions and filter out unsolvable examples (Figure 2). The result is a training dataset that more accurately reflects how optimization experts structure problems.\n\nFigure 2. Process for correcting training data\n\nUsing this refined dataset, we applied supervised fine-tuning to the base model. Rather than simply generating code, we trained OptiMind to produce structured mathematical formulations alongside intermediate reasoning steps, helping it avoid the common mistakes found in earlier datasets.\n\nWhen in use, the model’s reliability further improves. When given a new problem, OptiMind first classifies it into a category, such as scheduling or network design. It then applies expert hints relevant to that type of problem, which act as reminders to check for errors before generating a solution. For particularly challenging problems, the system generates multiple solutions and either selects the most frequently occurring one or uses feedback to refine its response. This approach increases accuracy without requiring a larger model, as illustrated in Figure 3.\n\nFigure 3. OptiMind’s inference process\n\nEvaluation\n\nTo test the system, we turned to three widely used public benchmarks that represent some of the most complex formulation tasks in the field. On closer inspection, we discovered that 30 to 50 percent of the original test data was flawed. After manually correcting the issues, OptiMind improved accuracy by approximately 10 percent over the base model. Figure 4 and Table 1 show detailed comparisons: OptiMind outperformed other open-source models under 32 billion parameters and, when combined with expert hints and correction strategies, matched or exceeded the performance of current leading models.\n\nFigure 4. Average accuracy percentages over all models.\n\nTable 1. Performance of all models on corrected benchmark datasets\n\nOptiMind is more reliable than other models because it learns from higher-quality, domain-aligned data. And by correcting errors and inconsistencies in standard datasets, we significantly reduced the model’s tendency to hallucinate relative to the base and comparison models.\n\nLooking forward\n\nWhile supervised fine-tuning has provided a strong foundation, we are exploring reinforcement learning to further refine OptiMind’s reasoning capabilities. We’re also investigating automated frameworks that would allow LLMs to generate their own expert hints, enabling continuous autonomous improvement. Additionally, we are working with Microsoft product teams and industry collaborators to expand OptiMind’s utility, adding support for more programming languages and a variety of input formats, including Excel and other widely used tools.\n\nWe’re releasing OptiMind as an experimental model to gather community feedback and inform future development. The model is available through Microsoft Foundry (opens in new tab) and Hugging Face (opens in new tab) , and we’ve open-sourced the benchmarks and data-processing procedures on GitHub (opens in new tab) to support more reliable evaluation across the field. We welcome feedback through GitHub (opens in new tab) , and invite those interested in shaping the future of optimization to apply for one of our  open roles .\n\nOpens in a new tab The post OptiMind: A small language model with optimization expertise appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "dbf80c81a6d49ad3",
    "title": "Investing in Merge Labs",
    "url": "https://openai.com/index/investing-in-merge-labs",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T07:00:00Z",
    "summary": "OpenAI is investing in Merge Labs to support new brain computer interfaces that bridge biological and artificial intelligence to maximize human ability, agency, and experience.",
    "content": "OpenAI is investing in Merge Labs to support new brain computer interfaces that bridge biological and artificial intelligence to maximize human ability, agency, and experience.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "c34820ee1851c9a7",
    "title": "Strengthening the U.S. AI supply chain through domestic manufacturing",
    "url": "https://openai.com/index/strengthening-the-us-ai-supply-chain",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-15T00:00:00Z",
    "summary": "OpenAI launches a new RFP to strengthen the U.S. AI supply chain by accelerating domestic manufacturing, creating jobs, and scaling AI infrastructure.",
    "content": "OpenAI launches a new RFP to strengthen the U.S. AI supply chain by accelerating domestic manufacturing, creating jobs, and scaling AI infrastructure.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]