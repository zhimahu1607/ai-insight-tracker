[
  {
    "id": "6bbd02e4f71ee5bb",
    "title": "Automated Reasoning checks rewriting chatbot reference implementation",
    "url": "https://aws.amazon.com/blogs/machine-learning/automated-reasoning-checks-rewriting-chatbot-reference-implementation/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T19:34:05Z",
    "summary": "This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.",
    "content": "Today, we are publishing a new open source sample chatbot that shows how to use feedback from Automated Reasoning checks to iterate on the generated content, ask clarifying questions, and prove the correctness of an answer.\nThe chatbot implementation also produces an audit log that includes mathematically verifiable explanations for the answer validity and a user interface that shows developers the iterative, rewriting process happening behind the scenes. Automated Reasoning checks use logical deduction to automatically demonstrate that a statement is correct. Unlike large language models, Automated Reasoning tools are not guessing or predicting accuracy. Instead, they rely on mathematical proofs to verify compliance with policies. This blog post dives deeper into the implementation architecture for the Automated Reasoning checks rewriting chatbot.\nImprove accuracy and transparency with Automated Reasoning checks\nLLMs can sometimes generate responses that sound convincing but contain factual errors—a phenomenon known as hallucination. Automated Reasoning checks validate a user’s question and an LLM-generated answer, giving rewriting feedback that points out ambiguous statements, assertions that are too broad, and factually incorrect claims based on ground truth knowledge encoded in Automated Reasoning policies.\nA chatbot that uses Automated Reasoning checks to iterate on its answers before presenting them to users helps improve accuracy because it can make precise statements that explicitly answer users’ yes/no questions without leaving room for ambiguity; and helps improve transparency because it can provide mathematically verifiable proofs of why its statements are correct, making generative AI applications auditable and explainable even in regulated environments.\nNow that you understand the benefits, let’s explore how you can implement this in your own applications.\nChatbot reference implementation\nThe chatbot is a Flask application that exposes APIs to submit questions and check the status of an answer. To show the inner workings of the system, the APIs also let you retrieve information about the status of each iteration, the feedback from Automated Reasoning checks, and the rewriting prompt sent to the LLM.\nYou can use the frontend NodeJS application to configure an LLM from Amazon Bedrock to generate answers, select an Automated Reasoning policy for validation, and set the maximum number of iterations to correct an answer. Selecting a chat thread in the user interface opens a debug panel on the right that displays each iteration on the content and the validation output.\n\nFigure 1 – Chat interface with debug panel\n\nOnce Automated Reasoning checks say a response is valid, the verifiable explanation for the validity is displayed.\n\nFigure 2 – Automated Reasoning checks validity proof\n\nHow the iterative rewriting loop works\nThe open source reference implementation automatically helps improve chatbot answers by iterating on the feedback from Automated Reasoning checks and rewriting the response. When asked to validate a chatbot question and answer (Q&A), Automated Reasoning checks return a list of findings. Each finding represents an independent logical statement identified in the input Q&A. For example, for the Q&A “How much does S3 storage cost? In US East (N. Virginia), S3 costs $0.023/GB for the first 50Tb; in Asia Pacific (Sydney), S3 costs $0.025/GB for the first 50Tb” Automated Reasoning checks would produce two findings, one that validates the price for S3 in us-east-1 is $0.023, and one for ap-southeast-2.\nWhen parsing a finding for a Q&A, Automated Reasoning checks separate the input into a list of factual premises and claims made against those premises. A premise can be a factual statement in the user question, like “I’m an S3 user in Virginia,” or an assumption laid out in the answer, like “For requests sent to us-east-1…” A claim represents a statement being verified. In our S3 pricing example from the previous paragraph, the Region would be a premise, and the price point would be a claim.\nEach finding includes a validation result ( VALID , INVALID , SATISFIABLE , TRANSLATION_AMBIGUOUS , IMPOSSIBLE ) as well as the feedback necessary to rewrite the answer so that it is VALID . The feedback changes depending on the validation result. For example, ambiguous findings include two interpretations of the input text, satisfiable findings include two scenarios that show how the claims could be true in some cases and false in others. You can see the possible finding types in our API documentation .\nWith this context out of the way, we can dive deeper into how the reference implementation works:\nInitial response and validation\nWhen the user submits a question through the UI, the application first calls the configured Bedrock LLM to generate an answer, then calls the ApplyGuardrail API to validate the Q&A.\nUsing the output from Automated Reasoning checks in the ApplyGuardrail response, the application enters a loop where each iteration checks the Automated Reasoning checks feedback, performs an action like asking the LLM to rewrite an answer based on the feedback, and then calls ApplyGuardrail to validate the updated content again.\nThe rewriting loop (The heart of the system)\nAfter the initial validation, the system uses the output from the Automated Reasoning checks to decide the next step. First, it sorts the findings based on their priority – addressing the most important first: TRANSLATION_AMBIGUOUS , IMPOSSIBLE , INVALID , SATISFIABLE , VALID . Then, it selects the highest priority finding and addresses it with the logic below. Since VALID is last in the prioritized list, the system will only accept something as VALID after addressing the other findings.\n\nFor TRANSLATION_AMBIGUOUS findings, the Automated Reasoning checks return two interpretations of the input text. For SATISFIABLE findings, the Automated Reasoning checks return two scenarios that prove and disprove the claims. Using the feedback, the application asks the LLM to decide on whether it wants to try and rewrite the answer to clarify ambiguities or ask the user follow up questions to gather additional information. For example, the SATISFIABLE feedback may say that the price of $0.023 is valid only if the Region is US East (N. Virginia). The LLM can use this information to ask about the application Region. When the LLM decides to ask follow-up questions, the loop pauses and waits for the user to answer the questions, then the LLM regenerates the answer based on the clarifications and the loop restarts.\nFor IMPOSSIBLE findings, the Automated Reasoning checks return a list of the rules that contradict the premises – accepted facts in the input content. Using the feedback, the application asks the LLM to rewrite the answer to avoid logical inconsistencies.\nFor INVALID findings, the Automated Reasoning checks return the rules from the Automated Reasoning policy that make the claims invalid based on the premises and policy rules. Using the feedback, the application asks the LLM to rewrite its answer so that it is consistent with the rules.\nFor VALID findings, the application exits the loop and returns the answer to the user.\n\nAfter each answer rewrite, the system sends the Q&A to the ApplyGuardrail API for validation; the next iteration of the loop starts with the feedback from this call. Each iteration stores the findings and prompts with full context in the thread data structure, creating an audit trail of how the system arrived at the definitive answer.\nGetting Started with the Automated Reasoning checks rewriting chatbot\nTo try our reference implementation, the first step is to create an Automated Reasoning policy:\n\nNavigate to  Amazon Bedrock  in the AWS Management Console in one of the supported Regions in the United States or European Regions.\nFrom the left navigation, open the  Automated Reasoning  page in the Build category.\nUsing the dropdown menu of the Create policy button, choose Create sample policy .\nEnter a name for the policy and then choose Create policy at the bottom of the page.\n\nOnce you have created a policy, you can proceed to download and run the reference implementation:\n\nClone the Amazon Bedrock Samples repository.\nFollow the instructions in the README file to install dependencies, build the frontend, and start the application.\nUsing your preferred browser navigate to http://localhost8080 and start testing.\n\nBackend implementation details\nIf you’re planning to adapt this implementation for production use, this section goes over the key components in the backend architecture. You will find these components in the backend directory of the repository.\n\nThreadManager: Orchestrates a conversation lifecycle management. It handles the creation, retrieval, and status tracking of conversation threads, maintaining proper state throughout the rewriting process. The ThreadManager implements thread-safe operations using a lock to help prevent race conditions when multiple operations attempt to modify the same conversation simultaneously. It also tracks threads awaiting user input and can identify stale threads that have exceeded a configurable timeout.\nThreadProcessor: Handles the rewriting loop using a state machine pattern for clear, maintainable control flow. The processor manages state transitions between phases like GENERATE_INITIAL , VALIDATE , CHECK_QUESTIONS , HANDLE_RESULT , and REWRITING_LOOP , progressing the conversation correctly through each stage.\nValidationService: I ntegrates with Amazon Bedrock Guardrails. This service takes each LLM-generated response and submits it for validation using the ApplyGuardrail API. It handles the communication with AWS, manages retry logic with exponential backoff for transient failures, and parses the validation results into structured findings.\nLLMResponseParser: Interprets the LLM’s intentions during the rewriting loop. When the system asks the LLM to fix an invalid response, the model must decide whether to attempt a rewrite ( REWRITE ), ask clarifying questions ( ASK_QUESTIONS ), or declare the task impossible due to contradictory premises ( IMPOSSIBLE ). The parser examines the LLM’s response for specific markers like “ DECISION: “, “ ANSWER: “, and “ QUESTION: “, extracting structured information from natural language output. It handles markdown formatting gracefully and enforces limits on the number of questions (maximum 5).\nAuditLogger: Writes structured JSON logs to a dedicated audit log file, recording two key event types: VALID_RESPONSE when a response passes validation, and MAX_ITERATIONS_REACHED when the system exhausts the set number of retry attempts. Each audit entry captures the timestamp, thread ID, prompt, response, model ID, and validation findings. The logger also extracts and records Q&A exchanges from clarification iterations, including whether the user answered or skipped the questions.\n\nTogether, these components help create a robust foundation for building trustworthy AI applications that combine the flexibility of large language models with the rigor of mathematical verification.\nFor detailed guidance on implementing Automated Reasoning checks in production:\n\nWorkshop :  Generative AI Reliability with Automated Reasoning checks\nTechnical Blog :  Minimize generative AI hallucinations with Amazon Bedrock Automated Reasoning checks\nUse Case Blog :  Build verifiable explainability into financial services workflows with Automated Reasoning checks for Amazon Bedrock Guardrails\nDocumentation :  Amazon Bedrock Guardrails User Guide\n\nAbout the authors\n\nStefano Buliani\nStefano is a Product Manager in the Automated Reasoning team at AWS.  With over 10 years at AWS, he has worked on serverless technologies, including open source projects like Serverless Java Container and has helped customers deploy hundreds of applications to production.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "e21bb244689050a6",
    "title": "Scale LLM fine-tuning with Hugging Face and Amazon SageMaker AI",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-llm-fine-tuning-with-hugging-face-and-amazon-sagemaker-ai/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T16:48:46Z",
    "summary": "In this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications.",
    "content": "Enterprises are increasingly shifting from relying solely on large, general-purpose language models to developing specialized large language models (LLMs) fine-tuned on their own proprietary data. Although foundation models (FMs) offer impressive general capabilities, they often fall short when applied to the complexities of enterprise environments—where accuracy, security, compliance, and domain-specific knowledge are non-negotiable.\nTo meet these demands, organizations are adopting cost-efficient models tailored to their internal data and workflows. By fine-tuning on proprietary documents and domain-specific terminology, enterprises are building models that understand their unique context—resulting in more relevant outputs, tighter data governance, and simpler deployment across internal tools.\nThis shift is also a strategic move to reduce operational costs, improve inference latency, and maintain greater control over data privacy. As a result, enterprises are redefining their AI strategy as customized, right-sized models aligned to their business needs.\nScaling LLM fine-tuning for enterprise use cases presents real technical and operational hurdles, which are being overcome through the powerful partnership between Hugging Face and Amazon SageMaker AI .\nMany organizations face fragmented toolchains and rising complexity when adopting advanced fine-tuning techniques like Low-Rank Adaptation (LoRA) , QLoRA , and Reinforcement Learning with Human Feedback (RLHF). Additionally, the resource demands of large model training—including memory limitations and distributed infrastructure challenges—often slow down innovation and strains internal teams.\nTo overcome this, SageMaker AI and Hugging Face have joined forces to simplify and scale model customization. By integrating the Hugging Face Transformers libraries into SageMaker’s fully managed infrastructure, enterprises can now:\n\nRun distributed fine-tuning jobs out of the box, with built-in support for parameter-efficient tuning methods\nUse optimized compute and storage configurations that reduce training costs and improve GPU utilization\nAccelerate time to value by using familiar open source libraries in a production-grade environment\n\nThis collaboration helps businesses focus on building domain-specific, right-sized LLMs, unlocking AI value faster while maintaining full control over their data and models.\nIn this post, we show how this integrated approach transforms enterprise LLM fine-tuning from a complex, resource-intensive challenge into a streamlined, scalable solution for achieving better model performance in domain-specific applications. We use the meta-llama/Llama-3.1-8B model, and execute a Supervised Fine-Tuning (SFT) job to improve the model’s reasoning capabilities on the MedReason dataset by using distributed training and optimization techniques, such as Fully-Sharded Data Parallel (FSDP) and LoRA with the Hugging Face Transformers library, executed with Amazon SageMaker Training Jobs.\nUnderstanding the core concepts\nThe Hugging Face Transformers library is an open-source toolkit designed to fine-tune LLMs by enabling seamless experimentation and deployment with popular transformer models.\nThe Transformers library supports a variety of methods for aligning LLMs to specific objectives, including:\n\nThousands of pre-trained models – Access to a vast collection of models like BERT, Meta Llama, Qwen, T5, and more, which can be used for tasks such as text classification, translation, summarization, question answering, object detection, and speech recognition.\nPipelines API – Simplifies common tasks (such as sentiment analysis, summarization, and image segmentation) by handling tokenization, inference, and output formatting in a single call.\nTrainer API – Provides a high-level interface for training and fine-tuning models, supporting features like mixed precision, distributed training, and integration with popular hardware accelerators.\nTokenization tools – Efficient and flexible tokenizers for converting raw text into model-ready inputs, supporting multiple languages and formats.\n\nSageMaker Training Jobs is a fully managed, on-demand machine learning (ML) service that runs remotely on AWS infrastructure to train a model using your data, code, and chosen compute resources. This service abstracts away the complexities of provisioning and managing the underlying infrastructure, so you can focus on developing and fine-tuning your ML and foundation models. Key capabilities offered by SageMaker training jobs are:\n\nFully managed – SageMaker handles resource provisioning, scaling, and management for your training jobs, so you don’t need to manually set up servers or clusters.\nFlexible input – You can use built-in algorithms, pre-built containers, or bring your own custom training scripts and Docker containers, to execute training workloads with most popular frameworks such as the Hugging Face Transformers library.\nScalable – It supports single-node or distributed training across multiple instances, making it suitable for both small and large-scale ML workloads.\nIntegration with multiple data sources – Training data can be stored in Amazon Simple Storage Service (Amazon S3), Amazon FSx , and Amazon Elastic Block Store (Amazon EBS), and output model artifacts are saved back to Amazon S3 after training is complete.\nCustomizable – You can specify hyperparameters, resource types (such as GPU or CPU instances), and other settings for each training job.\nCost-efficient options – Features like managed Spot Instances , flexible training plans , and heterogeneous clusters help optimize training costs.\n\nSolution overview\nThe following diagram illustrates the solution workflow of using the Hugging Face Transformers library with a SageMaker Training job.\n\nThe workflow consists of the following steps:\n\nThe user prepares the dataset by formatting it with the specific prompt style used for the selected model.\nThe user prepares the training script by using the Hugging Face Transformers library to start the training workload, by specifying the configuration for the distribution option selected, such as Distributed Data Parallel (DDP) or Fully-Sharded Data Parallel (FSDP).\nThe user submits an API request to SageMaker AI, passing the location of the training script, the Hugging Face Training container URI, and the training configurations required, such as distribution algorithm, instance type, and instance count.\nSageMaker AI uses the training job launcher script to run the training workload on a managed compute cluster. Based on the selected configuration, SageMaker AI provisions the required infrastructure, orchestrates distributed training, and upon completion, automatically decommissions the cluster.\n\nThis streamlined architecture delivers a fully managed user experience, helping you quickly develop your training code, define training parameters, and select your preferred infrastructure. SageMaker AI handles the end-to-end infrastructure management with a pay-as-you-go pricing model that bills only for the net training time in seconds.\nPrerequisites\nYou must complete the following prerequisites before you can run the Meta Llama 3.1 8B fine-tuning notebook:\n\nMake the following quota increase requests for SageMaker AI. For this use case, you will need to request a minimum of 1 p4d.24xlarge instance (with 8 x NVIDIA A100 GPUs) and scale to more p4d.24xlarge instances (depending on time-to-train and cost-to-train trade-offs for your use case). To help determine the right cluster size for the fine-tuning workload, you can use tools like VRAM Calculator or “ Can it run LLM “. On the Service Quotas console, request the following SageMaker AI quotas:\n\nP4D instances ( p4.24xlarge ) for training job usage: 1\n\nCreate an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess and AmazonS3FullAccess to give required access to SageMaker AI to run the examples.\nAssign the following policy as a trust relationship to your IAM role:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n{\n\"Sid\": \"\",\n\"Effect\": \"Allow\",\n\"Principal\": {\n\"Service\": [\n\"sagemaker.amazonaws.com\"\n]\n},\n\"Action\": \"sts:AssumeRole\"\n}\n]\n}\n\n(Optional) Create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI ) to access Jupyter notebooks with the preceding role. You can also use JupyterLab in your local setup\n\nThese permissions grant broad access and are not recommended for use in production environments. See the SageMaker Developer Guide for guidance on defining more fine-grained permissions.\nPrepare the dataset\nTo prepare the dataset, you must load the UCSC-VLAA/MedReason dataset. MedReason is a large-scale, high-quality medical reasoning dataset designed to enable faithful and explainable medical problem-solving in LLMs. The following table shows an example of the data.\n\ndataset_name\nid_in_dataset\nquestion\nanswer\nreasoning\noptions\n\nmedmcqa\n7131\nUrogenital Diaphragm is made up of the following…\nColle’s fascia. Explanation: Colle’s fascia do…\nFinding reasoning paths:\\n1. Urogenital diaphr…\nAnswer Choices:\\nA. Deep transverse Perineus\\n…\n\nmedmcqa\n7133\nChild with Type I Diabetes. What is the advise…\nAfter 5 years. Explanation: Screening for diab…\n**Finding reasoning paths:**\\n\\n1. Type 1 Diab…\nAnswer Choices:\\nA. After 5 years\\nB. After 2 …\n\nmedmcqa\n7134\nMost sensitive test for H pylori is-\nBiopsy urease test. Explanation: Davidson&…\n**Finding reasoning paths:**\\n\\n1. Consider th…\nAnswer Choices:\\nA. Fecal antigen test\\nB. Bio…\n\nWe want to use the following columns for preparing our dataset:\n\nquestion – The question being posed\nanswer – The correct answer to the question\nreasoning – A detailed, step-by-step logical explanation of how to arrive at the correct answer\n\nWe can use the following steps to format the input in the proper style used for Meta Llama 3.1, and configure the data channels for SageMaker training jobs on Amazon S3:\n\nLoad the UCSC-VLAA/MedReason dataset, using the first 10,000 rows of the original dataset: from datasets import load_dataset\ndataset = load_dataset(\"UCSC-VLAA/MedReason\", split=\"train[:10000]\")\nApply the proper chat template to the dataset by using the apply_chat_template method of the Tokenizer:\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\ndef prepare_dataset(sample):\n\nsystem_text = (\n\"You are a deep-thinking AI assistant.\\n\\n\"\n\"For every user question, first write your thoughts and reasoning inside ... tags, then provide your answer.\"\n)\n\nmessages = []\n\nmessages.append({\"role\": \"system\", \"content\": system_text})\nmessages.append({\"role\": \"user\", \"content\": sample[\"question\"]})\nmessages.append(\n{\n\"role\": \"assistant\",\n\"content\": f\"\\n{sample['reasoning']}\\n\\n{sample['answer']}\",\n}\n)\n\n# Apply chat template\nsample[\"text\"] = tokenizer.apply_chat_template(\nmessages, tokenize=False\n)\n\nreturn sample\n\nThe function prepare_dataset will iterate over the elements of the dataset, and use the apply_chat_template function to have a prompt template in the following form:\n\nsystem\n{{SYSTEM_PROMPT}}\nuser\n{{QUESTION}}\nassistant\n\n{{REASONING}}\n\n{{FINAL_ANSWER}}\n\nThe following code is an example of the formatted prompt:\n\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nYou are a deep-thinking AI assistant.\nFor every user question, first write your thoughts and reasoning inside ... tags, then provide your answer.\n<|eot_id|><|start_header_id|>user<|end_header_id|>\nA 66-year-old man presents to the emergency room with blurred vision, lightheadedness, and chest pain that started 30 minutes ago. The patient is awake and alert.\nHis history is significant for uncontrolled hypertension, coronary artery disease, and he previously underwent percutaneous coronary intervention.\nHe is afebrile. The heart rate is 102/min, the blood pressure is 240/135 mm Hg, and the O2 saturation is 100% on room air.\nAn ECG is performed and shows no acute changes. A rapid intravenous infusion of a drug that increases peripheral venous capacitance is started.\nThis drug has an onset of action that is less than 1 minute with rapid serum clearance than necessitates a continuous infusion. What is the most severe side effect of this medication?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n### Finding Reasoning Paths:\n1. **Blurred vision, lightheadedness, and chest pain** → Malignant hypertension → Rapid IV antihypertensive therapy.\n2. **Uncontrolled hypertension and coronary artery disease** → Malignant hypertension → Rapid IV antihypertensive therapy.\n3. **Severe hypertension (BP 240/135 mm Hg)** → Risk of end-organ damage → Malignant hypertension → Rapid IV antihypertensive therapy.\n4. **Chest pain and history of coronary artery disease** → Risk of myocardial ischemia → Malignant hypertension → Rapid IV antihypertensive therapy. ---\n\n### Reasoning Process:\n1. **Clinical Presentation and Diagnosis**: - The patient presents with blurred vision...\n...\n\nCyanide poisoning\n<|eot_id|><|end_of_text|>\n\nSplit the dataset into train, validation, and test datasets:\n\nfrom datasets import Dataset, DatasetDict\nfrom random import randint\n\ntrain_dataset = Dataset.from_pandas(train)\nval_dataset = Dataset.from_pandas(val)\ntest_dataset = Dataset.from_pandas(test)\n\ndataset = DatasetDict({\"train\": train_dataset, \"val\": val_dataset})\ntrain_dataset = dataset[\"train\"].map(\nprepare_dataset, remove_columns=list(train_dataset.features)\n)\n\nval_dataset = dataset[\"val\"].map(\nprepare_dataset, remove_columns=list(val_dataset.features)\n)\n\nPrepare the training and validation datasets for the SageMaker training job by saving them as JSON files and constructing the S3 paths where these files will be uploaded:\n\n...\n\ntrain_dataset.to_json(\"./data/train/dataset.jsonl\")\nval_dataset.to_json(\"./data/val/dataset.jsonl\")\n\ns3_client.upload_file(\n\"./data/train/dataset.jsonl\", bucket_name, f\"{input_path}/train/dataset.jsonl\"\n)\ns3_client.upload_file(\n\"./data/val/dataset.jsonl\", bucket_name, f\"{input_path}/val/dataset.jsonl\"\n)\n\nPrepare the training script\nTo fine-tune meta-llama/Llama-3.1-8B with a SageMaker Training job, we prepared the train.py file, which serves as the entry point of the training job to execute the fine-tuning workload.\nThe training process can use Trainer or SFTTrainer classes to fine-tune our model. This simplifies the process of continued pre-training for LLMs. This approach makes fine-tuning efficient for adapting pre-trained models to specific tasks or domains.\nThe Trainer and SFTTrainer classes both facilitate model training with Hugging Face transformers. The Trainer class is the standard high-level API for training and evaluating transformer models on a wide range of tasks, including text classification, sequence labeling, and text generation. The SFTTrainer is a subclass built specifically for supervised fine-tuning of LLMs, particularly for instruction-following or conversational tasks.\nTo accelerate the model fine-tuning, we distribute the training workload by using the FSDP technique. It is an advanced parallelism technique designed to train large models that might not fit in the memory of a single GPU, with the following benefits:\n\nParameter sharding – Instead of replicating the entire model on each GPU, FSDP splits (shards) model parameters, optimizer states, and gradients across GPUs\nMemory efficiency – By sharding, FSDP drastically reduces the memory footprint on each device, enabling training of larger models or larger batch sizes\nSynchronization – During training, FSDP gathers only the necessary parameters for each computation step, then releases memory immediately after, further saving resources\nCPU offload – Optionally, FSDP can offload some data to CPUs to save even more GPU memory\n\nIn our example, we use the Trainer class and define the required TrainingArguments to execute the FSDP distributed workload:\n\nfrom transformers import (\nTrainer,\nTrainingArguments\n)\n\ntrainer = Trainer(\nmodel=model,\ntrain_dataset=train_ds,\neval_dataset=test_ds if test_ds is not None else None,\nargs=transformers.TrainingArguments(\n**training_args,\n),\ncallbacks=callbacks,\ndata_collator=transformers.DataCollatorForLanguageModeling(\ntokenizer, mlm=False\n)\n)\n\nTo further optimize the fine-tuning workload, we use the QLoRA technique, which quantizes a pre-trained language model to 4 bits and attaches small Low-Rank Adapters, which are fine-tuned:\n\nfrom transformers import (\nAutoModelForCausalLM,\nAutoTokenizer,\nBitsAndBytesConfig,\n)\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(script_args.model_id)\n\n# Define PAD token\ntokenizer.pad_token = tokenizer.eos_token\n\n# Configure quantization\nbnb_config = BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_use_double_quant=True,\nbnb_4bit_quant_type=\"nf4\",\nbnb_4bit_compute_dtype=torch.bfloat16,\nbnb_4bit_quant_storage=torch.bfloat16\n)\n\n# Load the model\nmodel = AutoModelForCausalLM.from_pretrained(\nscript_args.model_id,\ntrust_remote_code=True,\nquantization_config=bnb_config,\nuse_cache=not training_args.gradient_checkpointing,\ncache_dir=\"/tmp/.cache\",\n**model_configs,\n)\n\nThe script_args and training_args are provided as hyperparameters for the SageMaker Training job in a configuration recipe .yaml file and parsed in the train.py file by using the TrlParser class provided by Hugging Face TRL:\n\nmodel_id: \"meta-llama/Llama-3.1-8B-Instruct\" # Hugging Face model id\n# sagemaker specific parameters\noutput_dir: \"/opt/ml/model\" # path to where SageMaker will upload the model\ncheckpoint_dir: \"/opt/ml/checkpoints/\" # path to where SageMaker will upload the model checkpoints\ntrain_dataset_path: \"/opt/ml/input/data/train/\" # path to where S3 saves train dataset\nval_dataset_path: \"/opt/ml/input/data/val/\" # path to where S3 saves test dataset\nsave_steps: 100 # Save checkpoint every this many steps\ntoken: \"\"\n# training parameters\nlora_r: 32\nlora_alpha:64\nlora_dropout: 0.1\nlearning_rate: 2e-4 # learning rate scheduler\nnum_train_epochs: 2 # number of training epochs\nper_device_train_batch_size: 4 # batch size per device during training\nper_device_eval_batch_size: 2 # batch size for evaluation\ngradient_accumulation_steps: 4 # number of steps before performing a backward/update pass\ngradient_checkpointing: true # use gradient checkpointing\nbf16: true # use bfloat16 precision\ntf32: false # use tf32 precision\nfsdp: \"full_shard auto_wrap offload\" #FSDP configurations\nfsdp_config:\nbackward_prefetch: \"backward_pre\"\ncpu_ram_efficient_loading: true\noffload_params: true\nforward_prefetch: false\nuse_orig_params: true\nwarmup_steps: 100\nweight_decay: 0.01\nmerge_weights: true # merge weights in the base model\n\nFor the implemented use case, we decided to fine-tune the adapter with the following values:\n\nlora_r : 32 – Allows the adapter to capture more complex reasoning transformations.\nlora_alpha : 64 – Given the reasoning task we are trying to improve, this value allows the adapter to have a significant impact to the base.\nlora_dropout : 0.05 – We want to preserve reasoning connection by avoiding breaking important ones.\nwarmup_steps : 100 – Gradually increases the learning rate to the specified value. For this reasoning task, we want the model to learn a new structure without forgetting the previous knowledge.\nweight_decay : 0.01 – Maintains model generalization.\n\nPrepare the configuration file for the SageMaker Training job by saving them as JSON files and constructing the S3 paths where these files will be uploaded:\n\nimport os\n\nif default_prefix:\ninput_path = f\"{default_prefix}/datasets/llm-fine-tuning-modeltrainer-sft\"\nelse:\ninput_path = f\"datasets/llm-fine-tuning-modeltrainer-sft\"\n\ntrain_config_s3_path = f\"s3://{bucket_name}/{input_path}/config/args.yaml\"\n\n# upload the model yaml file to s3\nmodel_yaml = \"args.yaml\"\ns3_client.upload_file(model_yaml, bucket_name, f\"{input_path}/config/args.yaml\")\nos.remove(\"./args.yaml\")\n\nprint(f\"Training config uploaded to:\")\nprint(train_config_s3_path)\n\nSFT training using a SageMaker Training job\nTo run a fine-tuning workload using the SFT training script and SageMaker Training jobs, we use the ModelTrainer class.\nThe ModelTrainer class is a and more intuitive approach to model training that significantly enhances user experience and supports distributed training, Build Your Own Container (BYOC), and recipes. For additional information refer to the SageMaker Python SDK documentation .\nSet up the fine-tuning workload with the following steps:\n\nSpecify the instance type, the container image for the training job, and the checkpoint path where the model will be stored:\n\ninstance_type = \"ml.p4d.24xlarge\"\ninstance_count = 1\n\nimage_uri = image_uris.retrieve(\nframework=\"huggingface\",\nregion=sagemaker_session.boto_session.region_name,\nversion=\"4.56.2\",\nbase_framework_version=\"pytorch2.8.0\",\ninstance_type=instance_type,\nimage_scope=\"training\",\n)\n\nDefine the source code configuration by pointing to the created train.py :\n\nfrom sagemaker.train.configs import SourceCode\n\nsource_code = SourceCode(\nsource_dir=\"./scripts\",\nrequirements=\"requirements.txt\",\nentry_script=\"train.py\",\n)\n\nConfigure the training compute by optionally providing the parameter keep_alive_period_in_seconds to use managed warm pools , to retain and reuse the cluster during the experimentation phase:\n\nfrom sagemaker.train.configs Compute\n\ncompute_configs = Compute(\ninstance_type=instance_type,\ninstance_count=instance_count,\nkeep_alive_period_in_seconds=0,\n)\n\nCreate the ModelTrainer function by providing the required training setup, and define the argument distributed=Torchrun() to use torchrun as a launcher to execute the training job in a distributed manner across the available GPUs in the selected instance:\n\nfrom sagemaker.train.configs import (\nCheckpointConfig,\nOutputDataConfig,\nStoppingCondition,\n)\nfrom sagemaker.train.distributed import Torchrun\nfrom sagemaker.train.model_trainer import ModelTrainer\n\n# define Training Job Name\njob_name = f\"train-{model_id.split('/')[-1].replace('.', '-')}-sft\"\n\n# define OutputDataConfig path\noutput_path = f\"s3://{bucket_name}/{job_name}\"\n\n# Define the ModelTrainer\nmodel_trainer = ModelTrainer(\ntraining_image=image_uri,\nsource_code=source_code,\nbase_job_name=job_name,\ncompute=compute_configs,\ndistributed=Torchrun(),\nstopping_condition=StoppingCondition(max_runtime_in_seconds=18000),\nhyperparameters={\n\"config\": \"/opt/ml/input/data/config/args.yaml\" # path to TRL config which was uploaded to s3\n},\noutput_data_config=OutputDataConfig(s3_output_path=output_path),\ncheckpoint_config=CheckpointConfig(\ns3_uri=output_path + \"/checkpoint\", local_path=\"/opt/ml/checkpoints\"\n),\n)\n\nSet up the input channels for the ModelTrainer by creating InputData objects from the provided S3 bucket paths for the training and validation dataset, and for the configuration parameters:\n\nfrom sagemaker.train.configs import InputData\n# Pass the input data\ntrain_input = InputData(\nchannel_name=\"train\",\ndata_source=train_dataset_s3_path, # S3 path where training data is stored\n)\nval_input = InputData(\nchannel_name=\"val\",\ndata_source=val_dataset_s3_path, # S3 path where validation data is stored\n)\nconfig_input = InputData(\nchannel_name=\"config\",\ndata_source=train_config_s3_path, # S3 path where configurations are stored\n)\n# Check input channels configured\ndata = [train_input, val_input, config_input]\n\nSubmit the training job:\n\nmodel_trainer.train(input_data_config=data, wait=False)\n\nThe training job with Flash Attention 2 for one epoch with a dataset of 10,000 samples takes approximately 18 minutes to complete.\nDeploy and test fine-tuned Meta Llama 3.1 8B on SageMaker AI\nTo evaluate your fine-tuned model, you have several options. You can use an additional SageMaker Training job to evaluate the model with Hugging Face Lighteval on SageMaker AI, or you can deploy the model to a SageMaker real-time endpoint and interactively test the model by using techniques like LLM as judge to compare generated content with ground truth content. For a more comprehensive evaluation that demonstrates the impact of fine-tuning on model performance, you can use the MedReason evaluation script to compare the base meta-llama/Llama-3.1-8B model with your fine-tuned version.\nIn this example, we use the deployment approach, iterating over the test dataset and evaluating the model on those samples using a simple loop.\n\nSelect the instance type and the container image for the endpoint:\n\nimport boto3\n\nsm_client = boto3.client(\"sagemaker\", region_name=sess.boto_region_name)\n\nimage_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/vllm:0.13-gpu-py312\"\n\nCreate the SageMaker Model using the container URI for vLLM and the S3 path to your model. Set your vLLM configuration, including the number of GPUs and max input tokens. For a full list of configuration options, see vLLM engine arguments .\n\nenv = {\n\"SM_VLLM_MODEL\": \"/opt/ml/model\",\n\"SM_VLLM_DTYPE\": \"bfloat16\",\n\"SM_VLLM_GPU_MEMORY_UTILIZATION\": \"0.8\",\n\"SM_VLLM_MAX_MODEL_LEN\": json.dumps(1024 * 16),\n\"SM_VLLM_MAX_NUM_SEQS\": \"1\",\n\"SM_VLLM_ENABLE_CHUNKED_PREFILL\": \"true\",\n\"SM_VLLM_KV_CACHE_DTYPE\": \"auto\",\n\"SM_VLLM_TENSOR_PARALLEL_SIZE\": \"4\",\n}\n\nmodel_response = sm_client.create_model(\nModelName=f\"{model_id.split('/')[-1].replace('.', '-')}-model\",\nExecutionRoleArn=role,\nPrimaryContainer={\n\"Image\": image_uri,\n\"Environment\": env,\n\"ModelDataSource\": {\n\"S3DataSource\": {\n\"S3Uri\": f\"s3://{bucket_name}/{job_prefix}/{job_name}/output/model.tar.gz\",\n\"S3DataType\": \"S3Prefix\",\n\"CompressionType\": \"Gzip\",\n}\n},\n},\n)\n\nCreate the endpoint configuration by specifying the type and number of instances:\n\ninstance_count = 1\ninstance_type = \"ml.g5.12xlarge\"\nhealth_check_timeout = 700\n\nendpoint_config_response = sm_client.create_endpoint_config(\nEndpointConfigName=f\"{model_id.split('/')[-1].replace('.', '-')}-config\",\nProductionVariants=[\n{\n\"VariantName\": \"AllTraffic\",\n\"ModelName\": f\"{model_id.split('/')[-1].replace('.', '-')}-model\",\n\"InstanceType\": instance_type,\n\"InitialInstanceCount\": instance_count,\n\"ModelDataDownloadTimeoutInSeconds\": health_check_timeout,\n\"ContainerStartupHealthCheckTimeoutInSeconds\": health_check_timeout,\n\"InferenceAmiVersion\": \"al2-ami-sagemaker-inference-gpu-3-1\",\n}\n],\n)\n\nDeploy the model:\n\nendpoint_response = sm_client.create_endpoint(\nEndpointName=f\"{model_id.split('/')[-1].replace('.', '-')}-sft\",\nEndpointConfigName=f\"{model_id.split('/')[-1].replace('.', '-')}-config\",\n)\n\nSageMaker AI will now create the endpoint and deploy the model to it. This can take 5–10 minutes. Afterwards, you can test the model by sending some example inputs to the endpoint. You can use the invoke_endpoint method of the sagemaker-runtime client to send the input to the model and get the output:\n\nimport json\nimport pandas as pd\n\neval_dataset = []\n\nfor index, el in enumerate(test_dataset, 1):\nprint(\"Processing item \", index)\n\npayload = {\n\"messages\": [\n{\n\"role\": \"system\",\n\"content\": \"You are a deep-thinking AI assistant.\\n\\nFor every user question, first write your thoughts and reasoning inside <think>...</think> tags, then provide your answer.\",\n},\n{\"role\": \"user\", \"content\": el[\"question\"]},\n],\n\"max_tokens\": 4096,\n\"stop\": [\"<|eot_id|>\", \"<|end_of_text|>\"],\n\"temperature\": 0.4,\n\"top_p\": 0.9,\n\"repetition_penalty\": 1.15,\n\"no_repeat_ngram_size\": 3,\n\"do_sample\": True,\n}\n\nresponse = predictor.invoke_endpoint(\nEndpointName=endpoint_name,\nContentType=\"application/json\",\nBody=json.dumps(payload),\n)\n\nresult = json.loads(response[\"Body\"].read().decode())\neval_dataset.append([el[\"question\"], result[\"choices\"][0][\"message\"][\"content\"]])\n\nprint(\"**********************************************\")\n\neval_dataset_df = pd.DataFrame(\neval_dataset, columns=[\"question\", \"answer\"]\n)\n\neval_dataset_df.to_json(\n\"./eval_dataset_results.jsonl\", orient=\"records\", lines=True\n)\n\nThe following are some examples of generated answers:\n\nQuestion: \"Perl's stain or prussion blue test is for:\"\n\nAnswer Fine-tuned: \"\"\"\n<think>\nThe Perl's stain or Prussian blue test is used to detect the presence of iron in biological samples.\nIt involves adding potassium ferrocyanide (K4[Fe(CN)6]) to the sample,\nwhich reacts with the iron ions present in it to form a dark blue-colored compound known as ferric ferrocyanide.\nThis reaction can be observed visually, allowing researchers to determine if iron is present in the sample.\n</think>\n\nIn simpler terms, the Perl's stain or Prussian blue test is used to identify iron in biological samples.\n\"\"\"\n\nThe fine-tuned model shows strong reasoning capabilities by providing structured, detailed explanations with clear thought processes, breaking down the concepts step-by-step before arriving at the final answer. This example showcases the effectiveness of our fine-tuning approach using Hugging Face Transformers and a SageMaker Training job.\nClean up\nTo clean up your resources to avoid incurring additional charges, follow these steps:\n\nDelete any unused SageMaker Studio resources .\n(Optional) Delete the SageMaker Studio domain .\nVerify that your training job isn’t running anymore. To do so, on the SageMaker console, under Training in the navigation pane, choose Training jobs .\nDelete the SageMaker endpoint .\n\nConclusion\nIn this post, we demonstrated how enterprises can efficiently scale fine-tuning of both small and large language models by using the integration between the Hugging Face Transformers library and SageMaker Training jobs. This powerful combination transforms traditionally complex and resource-intensive processes into streamlined, scalable, and production-ready workflows.\nUsing a practical example with the meta-llama/Llama-3.1-8B model and the MedReason dataset, we demonstrated how to apply advanced techniques like FSDP and LoRA to reduce training time and cost—without compromising model quality.\nThis solution highlights how enterprises can effectively address common LLM fine-tuning challenges such as fragmented toolchains, high memory and compute requirements, and multi-node scaling inefficiencies and GPU underutilization.\nBy using the integrated Hugging Face and SageMaker architecture, businesses can now build and deploy customized, domain-specific models faster—with greater control, cost-efficiency, and scalability.\nTo get started with your own LLM fine-tuning project, explore the code samples provided in our GitHub repository .\n\nAbout the Authors\nFlorent Gbelidji is a Machine Learning Engineer for Customer Success at Hugging Face. Based in Paris, France, Florent joined Hugging Face 3.5 years ago as an ML Engineer in the Expert Acceleration Program, helping companies build solutions with open source AI. He is now the Cloud Partnership Tech Lead for the AWS account, driving integrations between the Hugging Face environment and AWS services.\nBruno Pistone is a Senior Worldwide Generative AI/ML Specialist Solutions Architect at AWS based in Milan, Italy. He works with AWS product teams and large customers to help them fully understand their technical needs and design AI and machine learning solutions that take full advantage of the AWS cloud and Amazon ML stack. His expertise includes distributed training and inference workloads, model customization, generative AI, and end-to-end ML. He enjoys spending time with friends, exploring new places, and traveling to new destinations.\nLouise Ping is a Senior Worldwide GenAI Specialist, where she helps partners build go-to-market strategies and leads cross-functional initiatives to expand opportunities and drive adoption. Drawing from her diverse AWS experience across Storage, APN Partner Marketing, and AWS Marketplace, she works closely with strategic partners like Hugging Face to drive technical collaborations. When not working at AWS, she attempts home improvement projects—ideally with limited mishaps.\nSafir Alvi is a Worldwide GenAI/ML Go-To-Market Specialist at AWS based in New York. He focuses on advising strategic global customers on scaling their model training and inference workloads on AWS, and driving adoption of Amazon SageMaker AI Training Jobs and Amazon SageMaker HyperPod. He specializes in optimizing and fine-tuning generative AI and machine learning models across diverse industries, including financial services, healthcare, automotive, and manufacturing.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "502f46b4cb58609a",
    "title": "New Relic transforms productivity with generative AI on AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/new-relic-transforms-productivity-with-generative-ai-on-aws/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T16:45:16Z",
    "summary": "Working with the Generative AI Innovation Center, New Relic NOVA (New Relic Omnipresence Virtual Assistant) evolved from a knowledge assistant into a comprehensive productivity engine. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.",
    "content": "New Relic Inc. is a San Francisco-based technology company that pioneered application performance monitoring (APM) and provides comprehensive observability solutions. Serving leading customers worldwide, including major brands like Ryanair, New Relic helps organizations monitor and optimize their digital systems to deliver better customer experiences.\nNew Relic faced a challenge common to many rapidly growing enterprises. Their engineers were spending valuable time searching through fragmented documentation across multiple systems, with time consuming internal system queries, in some cases, taking more than a day. As a leading observability platform supporting thousands of customers worldwide, New Relic knew a more efficient way to access and utilize organizational knowledge was needed.\nThis challenge led to the creation of New Relic NOVA (New Relic Omnipresence Virtual Assistant): an innovative artificial intelligence (AI) tool built on Amazon Web Services (AWS). New Relic NOVA has transformed how New Relic employees access and interact with company knowledge and systems.\nWorking with the Generative AI Innovation Center , New Relic NOVA evolved from a knowledge assistant into a comprehensive productivity engine. New Relic NOVA is built on AWS services including Amazon Bedrock , Amazon Kendra , Amazon Simple Storage Service (Amazon S3) , and Amazon DynamoDB . Through Strands Agents , New Relic NOVA provides intelligent code reviews, AI governance, and managed Model Context Protocol (MCP) services.\nAmazon Bedrock is a fully managed service that provides access to leading foundation models for building generative AI applications, eliminating the need to manage infrastructure while enabling teams to customize models for their specific use cases. Through a single API, developers can experiment with and evaluate different foundation models, integrate them with enterprise systems, and build secure AI applications at scale.\nThe solution has reduced information search time while automating complex operational workflows. Through collaboration with the Generative AI Innovation Center, New Relic NOVA was developed into a solution that now processes over 1,000 daily queries across their organization. New Relic NOVA integrates seamlessly with Confluence, GitHub, Salesforce, Slack, and various internal systems, maintaining 80% accuracy in its responses for both knowledge-based queries and transactional tasks.\nWe will show how New Relic NOVA is architected using AWS services to create a scalable, intelligent assistant that goes beyond document retrieval to handle complex tasks like automated team permission requests and rate limit management. We explore the technical architecture, development journey, and key lessons learned in building an enterprise-grade AI solution that delivers measurable productivity gains at scale.\nSolution overview\nIn designing New Relic NOVA, New Relic established several critical objectives beyond the initial goal of improving documentation search. These included maintaining data security during knowledge retrieval and achieving consistent response quality across different data sources. As shown in Figure 1, New Relic NOVA’s AWS architecture enables seamless interaction between users and various AWS services while maintaining security and scalability. The solution required a flexible framework that could evolve with the organization’s needs for both knowledge retrieval and transactional tasks. A key challenge was balancing these requirements while keeping response times under 20 seconds to maintain user engagement.\n\nFigure 1 – Solution architecture of New Relic NOVA framework\nThe development team identified several potential risks early in the project. These included the possibility of exposing sensitive information through AI responses, maintaining accuracy when retrieving from multiple data sources, and ensuring system reliability at enterprise scale. Figure 2 illustrates New Relic NOVA’s detailed agent workflow, demonstrating how queries are processed and routed through various specialized agents to address user intentions. Additionally, the team implemented comprehensive security controls which included personable identifiable information (PII) detection and masking, along with a robust evaluation framework to monitor and maintain response quality.\n\nFigure 2 – New Relic NOVA agent workflow architecture\nThe project also revealed opportunities for future optimization. These include expanding an agent hierarchy architecture to support additional automated workflows and developing more sophisticated analytics for tracking user interaction patterns. The team’s experience suggests that organizations undertaking similar projects should focus on establishing clear evaluation metrics early and building flexible architectures that can accommodate evolving business needs.\nSolution\nNew Relic NOVA was developed over an eight-week period, involving a collaborative effort between internal engineering, security, legal, and compliance teams and the AWS Generative AI Innovation Center. This partnership accelerated rapid development and iteration, leveraging AWS expertise in large-scale AI implementations.\nAgent architecture\nThe New Relic NOVA architecture consists of three key layers:\n\nMain agent layer – This acts as a controllable orchestration for executing different workflows by identifying the user intent and delegating efforts to the following downstream layers:\n\nRetrieval Augmented Generation (RAG) with customized ingested knowledge from Amazon Bedrock Knowledge Bases or Amazon Kendra .\nAgents for direct interaction with third-party platforms.\nCustomized agents for handling internal New Relic tasks.\nFallback handling if users’ responses cannot be determined.\n\nData source layers (vector DB, enrich, data sources) – These layers represent resources where internal knowledge (for example, New Relic standards documentation and code repository documentation) are ingested for retrieval or RAG purposes. The benefit of these custom resources is to enhance information and search performance for use information requests.\nAgents layer – Comprises two distinct agent types:\n\nStrands Agents with MCP: Handle multi-step processes for third-party services, leveraging MCP for standardized service interactions.\nCustom action agents: Execute New Relic-specific tasks such as permission requests and service limit modifications, providing precise control over internal systems.\n\nA central agent acts as an orchestrator, routing queries to specialized sub-agents in a delegation model where responses flow directly back to the user rather than requiring inter-agent reasoning or adjustments. Meanwhile, Strands Agents are used to efficiently manage third-party service integrations using MCP. This approach gives New Relic NOVA the best of both worlds: the orchestration model maintains flexibility for internal processes while standardizing external services through MCP, creating a scalable foundation for New Relic regarding future automation needs.\nData integration strategy\nThe power lies in the ability of New Relic NOVA to seamlessly integrate multiple data sources, providing a unified interface for knowledge retrieval. This approach includes:\n\nAmazon Bedrock Knowledge Bases for Confluence: Confirms direct synchronization with Confluence spaces and maintains up-to-date information.\nAmazon Kendra for GitHub Enterprise: Indexes and searches GitHub repositories, providing quick access to code documentation.\nStrands Agents for Salesforce and Jira: Custom agents execute SOQL and JQL queries, respectively, to fetch relevant data from their respective platforms (Salesforce and Jira).\nAmazon Q Index for Slack: Uses Amazon Q Index capabilities to implement a RAG solution for Slack channel history, chosen for its rapid development potential.\n\nA unique aspect of the data integration of New Relic NOVA is the custom document enrichment process. During ingestion, documents are enhanced with metadata, keywords, and summaries, significantly improving retrieval relevance and accuracy.\nUsing Amazon Nova models\nAmazon Nova is AWS’s new generation of foundation models designed to deliver frontier intelligence with industry-leading price performance for enterprise use cases. The Amazon Nova family of models can process diverse inputs including text, images, and video, excelling in tasks from interactive chat to document analysis, while supporting advanced capabilities like RAG systems and AI agent workflows.\nTo optimize performance and cost-efficiency, New Relic NOVA utilizes Amazon Nova Lite and Pro models through Amazon Bedrock. These models were carefully selected to balance response quality with latency, enabling New Relic NOVA to maintain sub-20 second response times while processing complex queries. Amazon Bedrock provides access to diverse foundation model families . Its standardized framework and prompt optimization supports seamless switching between models without code changes. This allows New Relic NOVA to optimize for speed with Amazon Nova Lite or, because of complexity, switch to Amazon Nova Pro while maintaining consistent performance and cost efficiency.\nAdvanced RAG implementation\nNew Relic NOVA employs a sophisticated RAG approach, utilizing Amazon Bedrock Knowledge Bases, Amazon Kendra, and Amazon Q Index. To maximize retrieval accuracy, New Relic NOVA implements several key optimization techniques:\n\nHierarchical chunking : Amazon Bedrock Knowledge Bases employs hierarchical chunking, a method proven most effective through extensive experimentation with various chunking methodologies.\nContext enrichment: A custom AWS Lambda function enhances chunks during knowledge base ingestion, incorporating relevant keywords and contextual information. This process is particularly valuable for code-related content, where structural and semantic cues significantly impact retrieval performance.\nMetadata integration: During knowledge base document ingestion, additional context, such as summaries, titles, authors, creation dates, and last modified dates, is appended as document metadata. This enriched metadata enhances the quality and relevance of retrieved information.\nCustom document processing: For specific data sources like GitHub repositories, tailored document processing techniques are applied to preserve code structure and improve search relevance.\n\nThese techniques work in concert to optimize the RAG system within New Relic NOVA, delivering highly accurate retrieval across varied document types while minimizing development effort through existing connectors. The combination of hierarchical chunking, context enrichment, metadata integration, and custom document processing enables New Relic NOVA to provide precise, context-aware responses regardless of the data source or document format.\nEvaluation framework\nNew Relic NOVA implements a comprehensive evaluation framework, leveraging Amazon Bedrock foundation models for its large language model (LLM)-as-a-judge approach, along with validation datasets that combine questions, ground truth answers, and source document URLs. This evaluation framework, which can be executed on-demand in development environments, encompasses three critical metrics for system validation:\n\nAnswer accuracy measurement utilizes a 1–5 discrete scale rating system, where the LLM evaluates the generated response’s factual alignment with the established ground truth data.\nContext relevance assessment on a scale of 1–5, analyzing the retrieved context’s relevance to the user query.\nResponse latency tracking measures workflow performance, from initial query input to final answer generation, ensuring optimal user experience through comprehensive timing analysis.\n\nThis triple-metric evaluation approach supports detailed performance optimization across the New Relic NOVA solution core functionalities.\nObservability and continuous improvements\nThe solution includes a comprehensive observability framework that collects metrics and analyzes user feedback. The metric and feedback collection is implemented through New Relic AI monitoring solutions. Feedback is implemented through the Slack reaction feature (emoji responses), users can quickly provide feedback on New Relic NOVA responses. These reactions are captured by a New Relic python agent and sent to a https://one.newrelic.com/ domain. The feedback collection system provides valuable insights for:\n\nMeasuring user satisfaction with responses.\nIdentifying areas where accuracy can be improved.\nUnderstanding usage patterns across different teams.\nTracking the effectiveness of different types of queries.\nMonitoring the performance of various data sources.\nTracing each LLM call and latency.\n\nThe collected feedback data can be analyzed using AWS analytics services such as AWS Glue for ETL processing, Amazon Athena for querying, and Amazon QuickSight for visualization. This data-driven approach enables continuous improvement of New Relic NOVA and helps prioritize future enhancements based on actual user interactions.\nInternal teams are already experiencing the advantages of New Relic NOVA. Figure 3 showcases some of the responses captured by the Slack feedback process.\n\nFigure 3 – Users Slack message exchanges about New Relic NOVA experience\nConsiderations and next steps\nThe success of New Relic NOVA highlights several key learnings for organizations looking to implement similar solutions:\n\nStart with a clear understanding of user pain points and measurable success criteria.\nImplement robust data integration strategies with custom document enrichment.\nUse the generative AI services and foundation models that best fit your use cases to achieve optimal results.\nBuild in feedback mechanisms from the start to enable continuous improvement.\nFocus on both speed and accuracy to ensure user adoption.\n\nIn terms of next steps, New Relic NOVA is evolving from a standalone solution into a comprehensive enterprise AI platform by integrating cutting-edge AWS technologies and open-source frameworks. In the future, New Relic anticipates leveraging Amazon S3 Vectors . It offers up to 90% cost reduction for vector storage and querying compared to conventional approaches, enabling the handling of massive-scale AI workloads more efficiently. New Relic is looking to explore Amazon Bedrock AgentCore for enterprise-grade security, memory management, and scalable AI agent deployment, supporting robust production capabilities.\nAdditionally, New Relic is exploring Strands Agent Workflows , an open-source SDK that streamlines building AI agents from simple conversational assistants to complex autonomous workflows. This technology stack positions New Relic NOVA to deliver enterprise-ready AI solutions that scale seamlessly while maintaining cost efficiency and developer productivity.\nConclusion\nThe journey of creating New Relic NOVA demonstrates how enterprises can use the generative AI services of AWS to transform organizational productivity. Through the integration of Amazon Bedrock, Amazon Kendra, and other AWS services, New Relic created an AI assistant that transforms their internal operations. Working with the Generative AI Innovation Center of AWS, New Relic achieved a 95% reduction in information search time across their organization while automating complex operational workflows.\nLearn more about transforming your business with generative AI by visiting the Generative AI Innovation Center or speak with an AWS Partner Specialist or AWS Representative to know how we can help accelerate your business.\nFurther reading\n\nBuilding generative AI applications on AWS – AWS Classroom Training\nGenerative AI Lens – AWS Well-Architected Framework – Gain a deep understanding of how to design, deploy, and operate generative AI applications on AWS effectively\nBuild an end-to-end RAG solution using Amazon Bedrock Knowledge Bases and AWS CloudFormation\nOpen Protocols for Agent Interoperability Part 1: Inter-Agent Communication on MCP\n\nAbout the authors\nYicheng Shen is a lead software engineer for New Relic NOVA, where he focuses on developing gen AI and agentic solutions that transform how businesses understand their application performance. When he’s not building intelligent systems, you’ll find him exploring the outdoors with his family and their dog.\nSarathy Varadarajan , Senior Director of Engineering at New Relic, drives AI-first transformation and developer productivity, aiming for tenfold gains via intelligent automation and enterprise AI. He scaled engineering teams from 15 to over 350 in Bangalore and Hyderabad. He enjoys family time and volleyball.\nJoe King is an AWS Senior Data Scientist at the Generative AI Innovation Center, where he helps organizations architect and implement cutting-edge generative AI solutions. With deep expertise in science, engineering, and AI/ML architecture, he specializes in transforming complex generative AI use cases into scalable solutions on AWS.\nPriyashree Roy is an AWS data scientist at the Generative AI Innovation Center, where she applies her deep expertise in machine learning and generative AI to build cutting-edge solutions for AWS strategic customers. With a PhD in experimental particle physics, she brings a rigorous scientific approach to solving complex real-world problems through advanced AI technologies.\nGene Su is an AWS Data Scientist at the Generative AI Innovation Center, specializing in generative AI solutions for finance, retail, and other industries. He uses his expertise in large language models (LLMs) to deliver generative AI applications on AWS.\nDipanshu Jain is a generative AI Strategist at AWS, helping unlock the potential of gen AI through strategic advisory and tailored solution development. Specialized in identifying high-impact generative AI use cases, shaping execution roadmaps, and guiding cross-functional teams through proofs of concept—from discovery to production.\nAmeer Hakme is an AWS Solutions Architect that collaborates with Independent Software Vendors (ISVs) in the Northeast region, assisting in designing and building scalable and modern platforms on the AWS Cloud. An expert in AI/ML and generative AI, Ameer helps customers unlock the potential of these cutting-edge technologies. In his leisure time, he enjoys riding his motorcycle and spending quality time with his family.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "e4bdb8560deb2bf6",
    "title": "Accelerate agentic application development with a full-stack starter template for Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/accelerate-agentic-application-development-with-a-full-stack-starter-template-for-amazon-bedrock-agentcore/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T16:40:58Z",
    "summary": "In this post, you will learn how to deploy Fullstack AgentCore Solution Template (FAST) to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.",
    "content": "Generative AI and agentic applications are reshaping how businesses operate—from customer support bots to research assistants—and teams need to move from prototype to production quickly. Last year, AWS released Amazon Bedrock AgentCore —a development platform for building, deploying, and scaling AI agents in production. AgentCore provides core building blocks like runtime hosting, memory, tool integration, and observability with enterprise-grade security and dynamic scaling.\nThe Fullstack AgentCore Solution Template (FAST) shows you how it works together from the start. It’s a ready-to-deploy starter project that connects AgentCore Runtime, Gateway, Memory, and Code Interpreter with a React frontend and Amazon Cognito authentication—all defined with AWS Cloud Development Kit (AWS CDK) . FAST provides a complete reference architecture that shows you how the pieces integrate, with a working chat application that you can learn from and build upon.\nIn this post, you will learn how to deploy FAST to your Amazon Web Services (AWS) account, understand its architecture, and see how to extend it for your requirements. You will learn how to build your own agent while FAST handles authentication, infrastructure as code (IaC), deployment pipelines, and service integration.\nSolution overview\nFAST provides a complete full-stack architecture for deploying agents on Amazon Bedrock AgentCore. The template handles authentication, frontend application hosting, agent runtime, memory, observability, and Model Context Protocol (MCP) tool integration by default.\n\nThe architecture is centered on Amazon Bedrock AgentCore Runtime , which hosts your agent. In FAST, users authenticate through Amazon Cognito, which secures four integration points:\n\nUser sign-in to the frontend web application on Amazon CloudFront\nToken-based authentication for the frontend to access AgentCore Runtime\nToken-based authentication for agents to access AgentCore Gateway\nToken-based authentication for API requests to Amazon API Gateway\n\nThe frontend is a React application that uses Tailwind CSS and shadcn components, hosted on AWS Amplify Hosting . It communicates with AgentCore Runtime using streamable HTTP for real-time response streaming.\nThe feedback mechanism is provided to demonstrate how to make synchronous and authenticated HTTP calls through API Gateway and store application data in Dynamo DB. AgentCore Runtime connects to several AgentCore capabilities:\n\nAgentCore Memory – Stores conversation history (short-term) and extracted insights like user preferences (long-term), so your agent remembers context across sessions without custom database work. (FAST includes short-term memory by default, and long-term memory can seamlessly be added with minor modifications.)\nAgentCore Gateway – Exposes APIs as Model Context Protocol (MCP) compatible tools to your agents\nAgentCore Code Interpreter – Executes Python code securely in isolated sandbox environments\nAgentCore Observability – Sends Open Telemetry (OTEL)-compatible metrics and logs to Amazon CloudWatch and traces to AWS X-Ray\n\nThe template includes patterns for Strands Agents and LangGraph . FAST and AgentCore are agent framework-agnostic so you can use the agent SDK of your choice. The infrastructure is defined in AWS CDK for repeatable deployments.\nThe architecture is modular by design. The frontend integrates with the backend powered by AgentCore, which you can use as an example for integrating with your own frontend application. That can be your own React application or a frontend using a completely different frontend framework.\nDesigned for AI-assisted development\nFAST includes extensive documentation by design. The repository includes:\n\nSteering documents – Rules and conventions that coding assistants follow automatically\nFeature guides – Detailed documentation on gateway, memory, streaming, and other integrations\nREADMEs throughout the codebase – Context for each component\n\nWhen you ask a coding assistant to make changes, it can read these documents and follow the documented patterns. This approach works with many AI coding assistants, including Kiro , Cline, Claude Code, Cursor, and others. The documentation and steering docs are system-agnostic.\nAI-assisted development is optional. The same documentation that guides coding assistants is equally useful for developers who prefer to write the code themselves.\nPrerequisites\nBefore deploying FAST, make sure you have the following installed:\n\nNode.js 20 or later\nPython 3.11 or later\nDocker\nAWS Command Line Interface (AWS CLI) configured with at least these minimum credentials\nAWS CDK\n\nThe AWS Identity and Access Management (IAM) user that you use must have permissions to make the necessary AWS service calls and manage AWS resources mentioned in this post. When providing permissions to the IAM user, follow the principle of least privilege.\nSolution deployment walkthrough\nStart by deploying the solution in your local environment.\nStep 1: Clone the repository\nStart by using the following commands to clone the repository.\n\ngit clone https://github.com/awslabs/fullstack-solution-template-for-agentcore.git\ncd fullstack-solution-template-for-agentcore\n\nStep 2: Configure your deployment\nEdit infra-cdk/config.yaml to customize your deployment:\n\nstack_name_base: your-project-name\nadmin_user_email: admin@example.com # Optional: auto-creates user and emails credentials\nbackend: pattern: strands-single-agent # Available: strands-single-agent, langgraph-single-agent\ndeployment_type: docker # Available: docker, zip\n\nStep 3: Deploy the backend with CDK\nUse the following commands to deploy the backend.\n\ncd infra-cdk\nnpm install\ncdk bootstrap # Only required once per account/region\ncdk deploy\n\nThis creates the Cognito User Pool, builds and pushes the agent container to Amazon Elastic Container Registry (Amazon ECR) , creates the AgentCore Runtime, and sets up the CloudFront distribution. Deployment takes approximately 5–10 minutes.\nStep 4: Deploy the frontend\nUse the following commands to deploy the frontend.\n\ncd ..\npython scripts/deploy-frontend.py\n\nThe script generates the authentication configuration from CDK stack outputs, installs dependencies, builds the React application, and deploys to AWS Amplify Hosting. The script outputs the application URL when complete:\n\n✓ Deployment completed successfully!\n\nConsole: https://console.aws.amazon.com/amplify/apps\nApp URL: https://main.d123490abcdef.amplifyapp.com\n\nStep 5: Create an Amazon Cognito user\nIf you provided admin_user_email in the configuration, you will receive an email with temporary credentials that you can use to sign in. Move to the next step.\nIf you didn’t provide an admin_user_email , create a user manually:\n\nOpen the Amazon Cognito console.\nFind your User Pool (named {stack_name_base}-user-pool ).\nNavigate to Users and choose Create user .\nEnter an email address and temporary password.\nSelect Mark email as verified .\nChoose Create user .\n\nStep 6: Access and test the application\nYou’re ready to access and test the application using the following steps:\n\nOpen the Amplify Hosting URL (printed in your terminal after deploying the frontend) in your browser.\nSign in with your Amazon Cognito user credentials.\nChange your temporary password when prompted.\n\nThe FAST example application is a straightforward multi-turn chat interface. The UI remains minimal by design; it’s built to be replaced with your own frontend or integrated into an existing application. The baseline agent includes two tools to demonstrate the architecture:\n\nText analysis tool – An AWS Lambda -based tool behind AgentCore Gateway that counts words and analyzes letter frequency. This demonstrates the Gateway integration pattern.\nCode Interpreter – Direct integration with AgentCore Code Interpreter for more secure Python execution in an isolated sandbox.\n\nTry these sample queries to verify the tools are working:\n\n“Analyze the text: The quick brown fox jumps over the lazy dog” – The agent should return word count and letter frequency analysis\n“Calculate the first 20 Fibonacci numbers” – Watch the agent write and execute Python code in real time\n\nAlong with the UI, the provided tools are also meant to be replaced. They exist to demonstrate two different architectures for adding tools (behind AgentCore Gateway as in the text analysis tool, and directly to the agent as in the code interpreter tool) and give you a working starting point. Additionally, the feedback collection mechanism exists to demonstrate how to make synchronous and authenticated HTTP calls through an API Gateway and can seamlessly be removed or repurposed.\nCustomize the application to your needs\nWhat if you need a document analysis agent instead of a chatbot? Or you want to integrate with your company’s existing identity provider? FAST handles exactly this; the baseline application is a starting point, not a constraint.\nThe following is a recording of a live stream on the AWS Events YouTube channel in which FAST is used to build an agentic personal assistant application in real time.\n\nChanging the agent pattern\nThe template includes two agent patterns in the patterns/ directory:\n\nstrands-single-agent – A basic conversational agent using the Strands framework with MCP tool integration\nlanggraph-single-agent – A basic conversational agent using the LangGraph with MCP tool integration\n\nTo switch patterns, update backend.pattern in infra-cdk/config.yaml and redeploy with cdk deploy .\nTo create your own pattern, add a new directory under patterns/ with your agent implementation, a requirements.txt file, and a Dockerfile . Update the configuration to point to your new pattern.\nAdding new tools using Gateway\nAgentCore Gateway routes tool calls to an AWS Lambda function. To add a new tool:\n\nCreate a Lambda function that implements your tool logic.\nDefine the tool schema (name, description, and input parameters) in the CDK stack.\nAdd the Lambda function as an AgentCore Gateway target.\nRedeploy with cdk deploy .\n\nThe agent automatically discovers tools from Gateway through MCP. See docs/GATEWAY.md for implementation details and examples.\nUsing Code Interpreter\nCode Interpreter is already integrated in the baseline agent. It provides more secure Python execution in isolated sandbox environments with session persistence. Users can ask the agent to run calculations, generate data, or execute arbitrary Python code.\nModifying the frontend\nThe frontend is a standard React application in the frontend/ directory. AI coding assistants like Kiro are skilled at modifying React frontends. Describe the changes you want and let the assistant implement them.\nAfter making changes, redeploy with python scripts/deploy-frontend.py\nBecause FAST is using AWS Amplify Hosting, you have the option of integrating with a supported version control system to take advantage of the built-in continuous integration and delivery (CI/CD) capabilities of Amplify Hosting, which can replace the provided deploy-frontend.py script.\nBeyond chat: Other use cases\nThe baseline application centers around a chat interface, but FAST supports many agentic use cases. For example, a document analysis agent might add a file upload component to the frontend, a Lambda tool that extracts text from PDFs, and agent logic that summarizes findings. Or a workflow automation agent might monitor Slack channels and automatically create Jira tickets from support requests. The same architecture applies, you’re only swapping the pieces. These customizations work within the default architecture of FAST. But what if you need to replace a core component entirely?\nFlexibility: swap out major components\nThe architecture is intentionally modular, so you can replace major components as your requirements evolve. Here are some examples:\n\nIdentity provider – Replace Amazon Cognito with Okta, Microsoft Entra ID, Auth0, or your existing OAuth 2.0-compatible identity system\nFrontend framework – Swap the React frontend for Vue, Angular, or integrate the agent backend into an existing application\nHosting – Move from AWS Amplify Hosting to Vercel, Netlify, a self-managed CloudFront distribution, or your preferred hosting solution\nAgent framework – Use Strands Agents, LangGraph, CrewAI, or an agent SDK even in other languages like TypeScript or Java\n\nThese are some of the flexibility points FAST offers. The modular CDK infrastructure and decoupled architecture make it straightforward to adapt the template to your specific needs.\nClean up\nUse the following commands to remove the resources created by FAST:\ncd infra-cdk\ncdk destroy --force\nThis deletes the AWS resources including Amazon Simple Storage Service (Amazon S3) buckets and Amazon Elastic Container Registry (Amazon ECR) images. If you leave resources running, you might incur charges for some running resources. Note that Amazon Bedrock AgentCore is pay per use.\nConclusion\nFAST helps reduce the time to build and deploy an agent application to under 30 minutes. You can get more secure authentication, a working frontend, and integrated AgentCore capabilities—Memory, Gateway, Code Interpreter, and Observability—without writing infrastructure code from scratch. The baseline chat application and sample tools are starting points, not constraints. Swap in your own agent logic, connect your tools, modify the frontend, or replace major components like the identity provider or hosting solution. The modular architecture adapts to your requirements.\nTo get started, star and clone the repository , deploy FAST to your AWS account, and have a working agent application running in under 30 minutes. From there, customize and ship something real.\nFor expert assistance, the AWS Generative AI Innovation Center , AWS Professional Services , and our AWS Partners are here to help.\n\nAbout the authors\n\nDavid Kaleko\nDavid Kaleko is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he leads applied research efforts into cutting-edge generative AI implementation strategies for AWS customers. He holds a PhD in particle physics from Columbia University.\n\nIsaac Privitera\nIsaac Privitera is a Principal Data Scientist with the AWS Generative AI Innovation Center, where he develops bespoke agentic AI-based solutions to address customers’ business problems. His primary focus lies in building responsible AI systems, using techniques such as RAG, multi-agent systems, and model fine-tuning. When not immersed in the world of AI, Isaac can be found on the golf course, enjoying a football game, or hiking trails with his loyal canine companion, Barry.\n\nRyan Razkenari\nRyan Razkenari is a Deep Learning Architect at the AWS Generative AI Innovation Center, where he designs and builds AI solutions for enterprise customers. He specializes in applying generative AI to solve complex business challenges, with a focus on translating cutting-edge research into production-ready systems.\n\nMonica Raj\nMonica Raj is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she partners with organizations across industries to architect and deploy production-ready AI solutions. She specializes in agentic AI systems, natural language processing, contact center automation, and intelligent document processing with a focus on building scalable, enterprise-grade infrastructure for customers.\n\nDavide Merlin\nDavide Merlin is a Machine Learning Engineer at the AWS Generative AI Innovation Center based in Jersey City. He specializes in backend development of AI cloud-native applications, with a focus on API architecture. In his free time, he enjoys playing video games, trying out new restaurants, and watching new shows.\n\nBrian Zambrano\nBrian Zambrano is a Senior Deep Learning Architect at AWS. He comes from an over 25-year career building software as an engineer and architect. He currently works in the AWS Generative AI Innovation Center where he helps bring customer’s generative AI solutions to life, using his background in cloud architecture, serverless, and event-driven systems. Brian is the author of Serverless Design Patterns and Best Practices published in 2018. In his more than 7 years at AWS, Brian has held multiple roles including software engineer, Enterprise Solutions Architect, and Specialist Solutions Architect.\n\nAnurag Bhagat\nAnurag Bhagat is Senior Strategist at the AWS Generative AI Innovation Center, where he leads applied AI efforts to help improve business operations for AWS enterprise customers. He comes with 15 years of experience applying AI across industries and functions and is a computer science graduate.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "9299ee70205b227e",
    "title": "Agent-to-agent collaboration: Using Amazon Nova 2 Lite and Amazon Nova Act for multi-agent systems",
    "url": "https://aws.amazon.com/blogs/machine-learning/agent-to-agent-collaboration-using-amazon-nova-2-lite-and-amazon-nova-act-for-multi-agent-systems/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T16:00:28Z",
    "summary": "This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.",
    "content": "My first attempt at building a travel planning agent looked exactly like most early prototypes: one big model, a few tools, and a long system prompt. It worked fine until the moment real-world complexity showed up. Flights came from a clean API, hotels lived behind a shifting web UI, and the model kept mixing instructions, forgetting context, or hallucinating steps. That’s when it became obvious: the single agent wasn’t the solution, it was the bottleneck.\nFinding a fix meant splitting the work instead of trying to fix the prompting. This post walks through how agent-to-agent collaboration on Amazon Bedrock works in practice, using Amazon Nova 2 Lite for planning and Amazon Nova Act for browser interaction, to turn a fragile single-agent setup into a predictable multi-agent system.\nSolution overview\nThe system is built as a small group of agents that work together, as shown in the following diagram. One agent plans the work and talks to the user. The other agents handle specific tasks, such as flight search or hotel search. They communicate through simple messages, so each agent stays focused and the task is straightforward to reason about.\n\nWhen I saw where the single agent design broke, the pattern was obvious: the agent wasn’t struggling because the task was hard, it was struggling because the tasks were fundamentally different. Flight search is structured and predictable. Hotel search is messy, visual, and full of dynamic elements. Forcing one model to juggle both was like asking the same engineer to write backend APIs and also manually click through a website in real time. And the more I tried to patch it with prompts, more tools, and more fallback logic, the worse it got. When a single agent is overloaded with too many responsibilities, it slows down, loses context, makes inconsistent choices, and eventually collapses under its own weight. The problem was with the design, and trying to address it through prompting wouldn’t work. The fix was to split the work across three agents and let each one focus on a single responsibility. The Travel Agent handled user intent and planning. The Flight Agent talked to the structured flight API. And the Hotel Agent used browser automation to navigate real hotel sites. Instead of one overloaded model, each agent did one thing well — and they coordinated through a simple message-passing layer, so the workflow still felt seamless from the outside. Each agent had a very clear job. The Travel Agent was the coordinator that interpreted the request, broke it into steps, and decided which agent should run each part. The Flight Agent focused entirely on structured API calls where the data was predictable. And the Hotel Agent took care of the messy parts: navigating web pages, handling dynamic layouts, and extracting hotel details from real sites. Separating the system this way kept the logic clean and prevented single agents from becoming overloaded again.\nTo make this setup work, the agents needed a simple way to talk to each other. The Travel Agent had to send a clear request to the Flight Agent, wait for the results, and then trigger the Hotel Agent with the next step. We didn’t need anything fancy, only a lightweight message format the agents could pass around so each one knew what to do next. When that communication loop was in place, the whole workflow finally felt coordinated instead of chaotic pages with dynamic layouts and no public APIs. Using the same agent to handle both often leads to tool overload, tangled instructions, and higher risk of hallucinations.\nImplementation overview\nNow that the architecture was clear, it was time to actually build the system. This is where the tools behind each agent finally come into play. Both the Travel Agent and the Flight Agent use Amazon Nova 2 Lite for reasoning and planning, and the Flight Agent also calls a structured flight API to retrieve real data. The Hotel Agent relies on Amazon Nova Act to automate browser interactions when no API exists. The three agents communicate through a straightforward agent-to-agent (A2A) message-passing pattern, where agents exchange small, structured messages to coordinate their work, which keeps the workflow predictable even as each agent runs in a different execution environment. In a multi-agent system, coordination is equally as important as specialization. Agents need a structured, predictable way to exchange goals, share state, and trigger behaviors especially when collaborating on a task like planning a trip.\nTravel agent implementation (Amazon Nova 2 Lite)\nThe Travel Agent is the orchestrator of the whole workflow. It receives the user request, interprets intent using Amazon Nova 2 Lite, and decides which agent to call next. Nova 2 Lite is doing the heavy reasoning here—it breaks the input into steps, identifies when to trigger the Flight Agent or the Hotel Agent, and keeps track of the overall plan. Because the Travel Agent doesn’t touch external systems directly, its only job is to think clearly and route messages using A2A.\nThe following code example is a simplified version of how the Travel Agent is initialized:\n\n# Initialize A2A client tools\nprovider = A2AClientToolProvider(known_agent_urls=[\n\"http://localhost:9000\", # Hotel Booking Expert (NovaAct)\n\"http://localhost:9001\" # Flight Booking Expert\n])\nbedrock_model = BedrockModel(\nmodel_id=\"global.amazon.nova-2-lite-v1:0\",\nregion_name=\"us-east-1\",\n)\n\n# Create client agent with A2A tools\nclient_agent = Agent(\nname=\"Travel Client\",\nmodel=bedrock_model,\ndescription=\"Client agent that coordinates travel planning using specialized A2A agents\",\ntools=provider.tools,\nsystem_prompt=\"\"\"You are an autonomous travel planning agent. You MUST take action immediately without asking for confirmation.\n\nIn practice, the Travel Agent receives a single natural language request, such as “Find me flights from NYC to Tokyo on July 10 and a hotel until July 15.” From there, Amazon Nova 2 Lite does the heavy reasoning: it recognizes that two separate tasks are required, generates a clear plan, sends a message to the Flight Agent, waits for the results, and then triggers the Hotel Agent with the next instruction. Finally, it assembles both outputs into one coherent response. This keeps the orchestration logic clean and flowing. Nova 2 Lite effectively acts as the brain of the workflow, and the specialized agents handle the actual execution.\nFlight agent implementation (Amazon Nova 2 Lite and API)\nThe Flight Agent has a much narrower job: turn a structured request into real flight options. It uses Amazon Nova 2 Lite for the light reasoning it needs for validating inputs, formatting the search, and deciding whether to call the live flight API or fall back to mock data when credentials aren’t available. After the API call is made, the agent returns a clean, predictable JSON response back to the Travel Agent through A2A.\nThe following code example shows a simplified version of the flight search tool. The full implementation, including OAuth, fallback logic, and airport code handling, is available in the Agent to Agent with Amazon Nova GitHub repository:\n\n@tool\ndef search_flights(origin: str, destination: str, departure_date: str, return_date: Optional[str] = None) -> str:\n# Nova 2 Lite handles the reasoning around which path to take\nif amadeus_configured():\nreturn _search_amadeus_flights(\norigin=origin,\ndestination=destination,\ndeparture_date=departure_date,\nreturn_date=return_date\n)\nelse:\n# Local development fallback\nreturn _search_flights_web(origin, destination, departure_date, return_date)\n{\n\"flights\": [\n{ \"flight_number\": \"DL456\", \"price\": 520, \"duration\": \"14h 30m\" },\n{ \"flight_number\": \"JL701\", \"price\": 545, \"duration\": \"13h 50m\" }\n],\n\"source\": \"Amadeus API\"\n}\n\nBecause this agent deals with clean, structured data, the reasoning load is light and the job of Amazon Nova 2 Lite is mostly about choosing the right execution path and normalizing the output. This keeps the entire pipeline predictable and avoids embedding API specific logic inside the Travel Agent.\nHotel agent implementation (Amazon Nova Act)\nHotels are nothing like flights. There isn’t one clean API you can call, and most booking sites load content in ways that change from one visit to the next. This is where Amazon Nova Act comes in. The Hotel Agent uses Nova Act to control a real browser and follow natural language instructions. Instead of writing fragile scraping code, the agent tells Nova Act what it needs, and Nova Act takes care of the browsing and returns structured data.\nHere’s a shortened version of the tool:\n\n@tool\ndef search_hotels(location: str, checkin_date: str, nights: int = 2) -> str:\nwith NovaAct() as nova:\nresult = nova.act(\nf\"Search for hotels in {location} from {checkin_date} for {nights} nights. \"\nf\"Return the top 3 listings with name, price, and rating.\",\nschema=HotelSearchResults.model_json_schema()\n)\nreturn json.dumps(result)\n\nAnd here’s a simplified example of the response. The full code, including scrolling, cookie banners, and other details, is in the Agent to Agent with Amazon Nova GitHub repo:\n\n{\n\"hotels\": [\n{ \"name\": \"Shinjuku Grand\", \"price\": \"$180\", \"rating\": 4.3 },\n{ \"name\": \"Park Tower Tokyo\", \"price\": \"$210\", \"rating\": 4.6 },\n{ \"name\": \"Hotel Blossom\", \"price\": \"$155\", \"rating\": 4.0 }\n],\n\"source\": \"Anycompany.com via Nova Act\"\n}\n\nUsing Amazon Nova Act keeps the Hotel Agent from breaking every time the site changes its layout. And by using it, you can avoid writing your own scraping or DOM parsing logic.\nA2A message flow (how the agents talk)\nNow that each agent knows what it’s responsible for, they need a way to talk to each other. Before the Travel Agent starts sending real work, it first checks that the other agents are up by calling their A2A endpoints. It also loads the list of tools each agent exposes so Nova 2 Lite knows what capabilities are available. After that’s done, the flow is straightforward. The Travel Agent sends a message to the Flight Agent with the fields it needs. When the Flight Agent finishes, it sends a message back. Then the Travel Agent passes the next message to the Hotel Agent. Each message is a small JSON object with things like the action, the input data, and where to send the response.\nEnd-to-end example run\nHere’s how one full run looks. The user sends a single request to the Travel Agent:\n\nPlease arrange travel for one person from NYC to Paris on December 6, 2025, including a two night stay in Paris.\n\nTravel Agent -> Flight Agent:\n\nThe Travel Agent pulls out the flight part of the request and sends it to the Flight Agent.\nThe Flight Agent returns three direct, cheap flights from JFK to Paris, including the airline, times, price, and duration.\n\nTravel Agent -> Hotel Agent:\n\nThe Travel Agent sends the hotel part of the request to the Hotel Agent.\nThe Hotel Agent, using Nova Act, checks Paris hotels and returns the top three options with names, prices, and short notes.\n\nFinal result to the user\n\nThe Travel Agent combines both answers and sends back a clear summary that includes:\n\nThe recommended flight\nThe recommended hotel\nThe check-in and check-out dates\nThe prices\nA question asking whether to book\n\nConclusion\nBuilding this travel planner with three small agents turned out to be much easier to manage than one big one. Each agent focuses on one job, and Amazon Nova 2 Lite handles the thinking needed to move the work from step to step. Amazon Nova Act covers the parts that don’t have APIs, such as hotel searches, without writing scraping code. The A2A message flow keeps everything connected but still straightforward.\nThis setup isn’t tied to travel. Tasks that mix different skillsets can use the same idea: let one agent plan the work, let the others do the parts they’re good at, and pass small messages between them. It makes the system seamless to change and explain.\nIf you want to try this yourself, the full code and examples are in the Agent to Agent with Amazon Nova GitHub repo.\n\nAbout the authors\nYoav Fishman is an AWS Solutions Architect with 12 years of cloud and engineering experience, specializing in GenAI, Agentic AI, and cybersecurity. He guides startups—from early stage to growth—in building secure, scalable architectures and implementing Agentic AI flows that drive business impact.\nElior Farajpur is a Solutions Architect at AWS with 7 years of experience in the cloud world and a passion for AI and cloud technologies. He helps organizations design innovative cloud-based solutions that drive real business value.\nDan Kolodny is an AWS Solutions Architect specializing in big data, analytics, and GenAI. He is passionate about helping customers adopt best practices, discover insights from their data, and embrace new GenAI technologies.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "afa742ed3af9487b",
    "title": "Bringing ChatGPT to GenAI.mil",
    "url": "https://openai.com/index/bringing-chatgpt-to-genaimil",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T11:00:00Z",
    "summary": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
    "content": "OpenAI for Government announces the deployment of a custom ChatGPT on GenAI.mil, bringing secure, safety-forward AI to U.S. defense teams.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "16251d7b149eacdc",
    "title": "Testing ads in ChatGPT",
    "url": "https://openai.com/index/testing-ads-in-chatgpt",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-09T11:00:00Z",
    "summary": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
    "content": "OpenAI begins testing ads in ChatGPT to support free access, with clear labeling, answer independence, strong privacy protections, and user control.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]