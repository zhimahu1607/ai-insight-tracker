[
  {
    "id": "f5b759d06303f886",
    "title": "Efficiently serve dozens of fine-tuned models with vLLM on Amazon SageMaker AI and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/efficiently-serve-dozens-of-fine-tuned-models-with-vllm-on-amazon-sagemaker-ai-and-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-25T20:56:13Z",
    "summary": "In this post, we explain how we implemented multi-LoRA inference for Mixture of Experts (MoE) models in vLLM, describe the kernel-level optimizations we performed, and show you how you can benefit from this work. We use GPT-OSS 20B as our primary example throughout this post.",
    "content": "Organizations and individuals running multiple custom AI models, especially recent Mixture of Experts (MoE) model families, can face the challenge of paying for idle GPU capacity when the individual models don’t receive enough traffic to saturate a dedicated compute endpoint. To solve this problem, we have partnered with the vLLM community and developed an efficient solution for Multi-Low-Rank Adaptation (Multi-LoRA) serving of popular open-source MoE models like GPT-OSS or Qwen. Multi-LoRA is a popular approach to fine-tune models. Instead of retraining entire model weights, multi-LoRA keeps the original weights frozen and injects small, trainable adapters into the model’s layers. With multi-LoRA, at inference time, multiple custom models share the same GPU, with only the adapters swapped in and out per request. For example, five customers each utilizing only 10% of a dedicated GPU can be served from a single GPU with multi-LoRA, turning five underutilized GPUs into one efficiently shared GPU.\nIn this post, we explain how we implemented multi-LoRA inference for Mixture of Experts (MoE) models in vLLM, describe the kernel-level optimizations we performed, and show you how you can benefit from this work. We use GPT-OSS 20B as our primary example throughout this post.\nYou can use these improvements today in your local vLLM deployments with version 0.15.0 or later. Multi-LoRA serving now works for MoE model families including GPT-OSS, Qwen3-MoE, DeepSeek, and Llama MoE. Our optimizations also help improve multi-LoRA hosting for dense models, e.g., Llama3.3 70B or Qwen3 32B. Amazon-specific optimizations deliver additional latency improvements over vLLM 0.15.0, e.g., 19% higher Output Tokens Per Second (OTPS) (i.e., how fast the model generates output) and 8% lower Time To First Token (TTFT) (i.e., how long you have to wait before the model starts to generate output) for GPT-OSS 20B. To benefit from these optimizations, host your LoRA customized models on Amazon SageMaker AI or Amazon Bedrock .\nImplementing multi-LoRA inference for MoE models in vLLM\nBefore we dive into our initial implementation of multi-LoRA inference for MoE models in vLLM, we want to provide some background information on MoE models and LoRA fine-tuning that is important for understanding the rationale behind our optimizations. MoE models contain multiple specialized neural networks called experts. A router directs each input token to the most relevant experts, whose outputs are then aggregated. This sparse architecture processes larger models with fewer computational resources because only a fraction of the model’s total parameters are activated per token, see Figure 1 below for a visualization.\nEach expert is a small feed-forward network that processes a token’s hidden state in two stages. First, the gate_up projection expands the compact hidden state (e.g., 4096 dims) into a larger intermediate space (e.g., 11008 dims). This expansion is necessary because features in the compact space are tightly entangled – the larger space gives the network room to pull them apart, transform them, and selectively gate which ones matter. Second, the down projection compresses the result back to the original dimension. This helps keep the output compatible with the rest of the model and acts as a bottleneck, forcing the network to retain only the most useful features. Together, this “expand-then-compress” pattern lets each expert apply rich transformations while maintaining a consistent output size. vLLM uses a fused_moe kernel to execute these projections as Group General Matrix Multiply (Group GEMM) operations — one GEMM per expert assigned to a given token. Multi-LoRA fine-tuning keeps the base model weights W, e.g., W_gate_up for the gate_up projection, frozen and trains two small matrices A and B that together form an adapter. For a projection with base weights W of shape h_in × h_out , LoRA trains A of shape h_in × r and B of shape r × h_out , where r is the LoRA rank (typically 16-64). The fine-tuned output becomes y = xW + xAB . Each LoRA adapter adds two operations to a projection. The shrink operation computes z=xA , reducing the input from h_in dimensions down to r dimensions. The expand operation takes that r-dimensional result and projects it back to h_out dimensions by multiplying z with B . This is illustrated on the right of Figure 1.\n\nFigure 1: Illustration of how MoE-LoRA models work with an example hidden state dimension 4096, intermediate representation dimension 11008 and LoRA rank r = 32.\nEach expert has two weight projections: gate_up and down . When a LoRA adapter is applied, it adds two low-rank operations, i.e., shrink and expand, to each projection. This means every expert requires four LoRA kernel operations in total: shrink and expand for gate_up , and shrink and expand for down . In a multi-LoRA serving setup, where multiple LoRA adapters are served simultaneously for different users or tasks, the system must efficiently manage these four operations per expert, per adapter, per request. This makes it a key performance bottleneck for MoE models. The four operations involve matrices, where one dimension (the LoRA rank r ) is 100-300× smaller than the other (e.g., hidden state and intermediate representation dimension). Standard GEMM kernels are designed for roughly square matrices and perform poorly on skinny matrices, which is why the kernel optimizations described later in this post are necessary. Besides having to optimize for skinny matrices, adding multi-LoRA support for MoE models presented two technical challenges. First, vLLM lacked a kernel to perform LoRA on MoE layers because existing dense multi-LoRA kernels do not handle expert routing. Second, MoE LoRA combines two sources of sparsity: expert routing (tokens assigned to different experts) and adapter selection (requests using different LoRA adapters). This compound sparsity requires a specialized kernel design. To address these challenges, we created a fused_moe_lora kernel that integrates LoRA operations into the fused_moe kernel. This new kernel performs LoRA shrink and expand GEMMs for the gate_up and down projections. The fused_moe_lora kernel follows the same logic as the fused_moe kernel and adds an additional dimension to the grid for the corresponding activated LoRA adapters.\nWith this implementation merged into vLLM, we could run a multi-LoRA serving with GPT-OSS 20B on an H200 GPU, reaching 26 OTPS and 1053 ms TTFT on the Sonnet dataset (a poetry-based benchmark) with input length of 1600, output length of 600 and concurrency of 16. To reproduce these results, check out our PR in the release 0.11.1.rc3 from the vLLM GitHub repository. In the rest of this blog, we will show how we optimized the performance from these baseline enablement numbers.\nImproving multi-LoRA inference performance in vLLM\nAfter finalizing our initial implementation, we used NVIDIA Nsight Systems (Nsys) to identify bottlenecks and found the fused_moe_lora kernel to be the highest-latency component. We then used NVIDIA Nsight Compute (NCU) to profile compute and memory throughput for the four kernel operations: gate_up_shrink , gate_up_expand , down_shrink , and down_expand . These findings led us to develop execution optimizations, kernel-level optimizations, and tuned configurations for these four kernels.\nExecution optimizations\nWith our initial implementation, the multi-LoRA TTFT was 10x higher (worse) than the base model TTFT (i.e., the public release version of GPT-OSS 20B). Our profiling revealed that the Triton compiler treated input-length-dependent variables as compile-time constants, causing the fused_moe_lora kernel to be recompiled from scratch for every new context length instead of being reused. This is visible in Figure 2: the cuModuleLoadData calls before each fused_moe_lora kernel execution indicate that the GPU is loading a newly compiled kernel binary rather than reusing a cached one, and the large gaps between kernel start times show the GPU sitting idle during recompilation. This overhead drove the 10× TTFT regression over the base model. We resolved this by adding a do_not_specialize compiler hint for these variables, instructing Triton to compile the kernel once and reuse it across all context lengths.\n\nFigure 2: Profiling results for  fused_moe_lora  kernel before our execution optimizations.\nProfiling also revealed that our fused_moe_lora kernel launched with the same high overhead regardless of whether the request used the base model only, attention-only adapters (LoRA adapters with weights only on the attention layers), or full LoRA adapters (adapters with weights on both attention and MoE layers). To help resolve this, we added early exit logic to skip the fused_moe_lora kernel on layers without LoRA adapters, helping prevent unnecessary kernel execution.\nThe shrink and expand kernels run serially, which created bubbles between executions of two kernels in our early implementation. To overlap the kernel execution, we implemented Programmatic Dependent Launch (PDL). With PDL, a dependent kernel can begin to launch before the primary kernel finishes, which lets the expand kernel pre-fetch weights into shared memory and L2 cache while the shrink kernel runs. When the shrink kernel completes, the expand kernel has already loaded its weights and can immediately begin computation.\nWe also added support for speculative decoding with CudaGraph for LoRA, fixing an issue in vLLM which would capture different CudaGraphs for the base model and adapter. CudaGraphs are important for efficiency since they are used to capture sequences of GPU operations to help reduce GPU kernel overhead, e.g., kernels as a single unit. As a result, CudaGraphs can reduce CPU overheads and these kernel launch latencies. With our execution optimizations, OTPS improved to 50/100 without/with speculative decoding and TTFT improved to 150 ms for GPT-OSS 20B using the default configuration. For the remainder of the blog, we report the numbers with speculative decoding on.\nKernel optimizations\nSplit-K is a work decomposition strategy that helps improve load balancing for skinny matrices. LoRA shrink computes xA where x has dimension 1×h_in and A has dimension h_in×r . Each of the r output elements requires summing h_in multiplications. Standard GEMM kernels assign different thread groups — batches of GPU threads that share fast on-chip memory — to different output elements, but each thread group computes its h_in summation sequentially. With r in the tens and h_in in the thousands, there are few output elements to parallelize across while each requires a long sequential summation. Split-K addresses this by splitting the summation over the inner dimension K of a GEMM (in this example K=h_in ) across multiple thread groups, which compute partial sums in parallel and then combine their results. These partial results require an atomic add to produce the final sum. Since we perform pure atomic addition with no extra logic, we use the Triton compiler freedom for optimizations by setting the parameter sem=\"relaxed\" for the atomic add operation.\nThe GPU scheduler assigns multiple thread groups to the same output element and runs thread groups for different output elements at the same time. For lora_shrink , each output element requires reading one column of A , which spans the h_in rows. With h_in in the thousands, each column touches cache lines spread across a large memory region. Nearby columns share the same rows and overlap in cache, so thread groups working on neighboring columns can benefit from reusing each other’s loaded data. Cooperative Thread Array (CTA) swizzling reorders the schedule so that thread groups working on nearby columns run at the same time, increasing L2 cache reuse. We applied CTA swizzling to the lora_shrink operation.\nWe also removed unnecessary masking and dot product operations from the shrink and expand LoRA kernels. Triton kernels load data in fixed-size blocks, but matrix dimensions may not divide evenly into these block sizes. For example, if BLOCK_SIZE_K is 64 but the matrix dimension K is 100, the second block would attempt to read 28 invalid memory locations. Masking helps prevent these illegal memory accesses by checking whether each index is within bounds before loading. However, these conditional checks execute on every load operation, which adds overhead even when the elements are valid. We introduced an EVEN_K parameter that checks whether K divides evenly by BLOCK_SIZE_K . When true, the loads are valid and masking can be skipped entirely, helping reduce both masking overhead and unnecessary dot product computations.\nLastly, we fused the addition of the LoRA weights with the base model weights into the LoRA expand kernel. This optimization helps reduce the kernel launch overhead. These kernel optimizations helped us reach 144 OTPS and 135 ms TTFT for GPT-OSS 20B.\nTuning kernel configurations for Amazon SageMaker AI and Amazon Bedrock\nTriton kernels require tuning of parameters such as block sizes ( BLOCK_SIZE_M , BLOCK_SIZE_N , BLOCK_SIZE_K ), which control how the matrix computation is divided across thread groups. Advanced parameters include GROUP_SIZE_M , which controls thread group ordering for cache locality, and SPLIT_K , which parallelizes summations across the inner matrix dimension.\nWe found that the MoE LoRA kernels using default configurations optimized for standard fused MoE performed poorly for multi-LoRA serving. These defaults did not account for the additional grid dimension corresponding to the LoRA index and the compound sparsity from multiple adapters. To address this bottleneck, we added support for users to load custom tuned configurations by providing a folder path. For more information, see the vLLM LoRA Tuning documentation. We tuned the four fused_moe_lora operations ( gate_up shrink, gate_up expand, down shrink, down expand) simultaneously since they share the same BLOCK_SIZE_M parameter. Amazon SageMaker AI and Bedrock customers now have access to these tuned configurations, which are loaded automatically and achieve 171 OTPS and 124 ms TTFT for GPT-OSS 20B.\nResults & Conclusion\nThrough our collaboration with the vLLM community, we implemented and open-sourced multi-LoRA serving for MoE models including GPT-OSS, Qwen3 MoE, DeepSeek, and Llama MoE. We then applied optimizations, e.g, yielding 454% OTPS improvements and 87% lower TTFT for GPT-OSS 20B in vLLM 0.15.0 vs vLLM 0.11.1rc3. Some optimizations, particularly kernel tuning and CTA swizzling, also improved performance for dense models, e.g., Qwen3 32B OTPS improved by 99%. To leverage this work in your local deployments, use vLLM 0.15.0 or later. Amazon-specific optimizations, available in Amazon Bedrock and Amazon SageMaker AI, help deliver additional latency improvements across models, e.g., 19% faster OTPS and 8% better TTFT vs vLLM 0.15.0 for GPT-OSS 20B. To get started with custom model hosting on Amazon, see the Amazon SageMaker AI hosting and Amazon Bedrock documentation.\n\nFigure 3: Output tokens per second (OTPS) and time to first token (TTFT) for GPT-OSS 20B multi-LoRA inference: 1/ Initial implementation in vLLM 0.11.1rc3; 2/ with vLLM 0.15.0; 3/ with vLLM 0.15.0 and AWS custom kernel tuning. Experiments used 1600 input tokens and 600 output tokens with LoRA rank 32 and 8 adapters loaded in parallel.\nAcknowledgments\nWe would like to acknowledge the contributors and collaborators from the vLLM community: Jee Li, Chen Wu, Varun Sundar Rabindranath, Simon Mo and Robert Shaw, and our team members: Xin Yang, Sadaf Fardeen,  Ashish Khetan, and George Karypis.\n\nAbout the authors\n\nDanielle Maddix Robinson\nDanielle Maddix Robinson is a Senior Applied Scientist at AWS AI/ML research. She earned her PhD in Computational and Mathematical Engineering from Stanford’s ICME. Her expertise spans numerical analysis, linear algebra, and optimization for numerical PDEs and deep learning. Since joining AWS in 2018, she’s developed time series forecasting and tabular foundation models including Chronos and Mitra. She is leading the Multi-LoRA inference optimization effort, and research on physics-constrained machine learning for scientific computing.\n\nXiang Song\nXiang Song is a Principal Applied Scientist at AWS AI/ML research, where he works on ML Systems challenges in distributed training & inference. Xiang has also been a lead developer for AWS’s graph ML efforts, including the deep learning frameworks GraphStorm. He received his PhD in computer systems and architecture at the Fudan University, Shanghai, in 2014.\n\nHaipeng Li\nHaipeng Li is an Applied Scientist at AWS, where he specializes in large language model (LLM) inference optimization. His work focuses on improving model efficiency, developing high-performance inference kernels, and reducing latency and computational costs for deploying LLMs at scale. Haipeng is passionate about bridging the gap between cutting-edge AI research and practical, production-ready solutions that enhance the performance of cloud-based machine learning services.\n\nYu Gong\nYu Gong works on advancing AI systems for real-world deployment. As an Applied Scientist at Amazon, he designs scalable architectures and performance-optimized inference frameworks, with a focus on GPU kernel optimization and hardware-aware system design. His work bridges AI research and production systems to deliver efficient and reliable intelligent services.\n\nGeorge Novack\nGeorge Novack is a software engineer at AWS, where he is focused on writing efficient GPU kernels and building fast, reliable inference systems.\n\nMani Kumar Adari\nMani Kumar Adari is a Principal Software Engineer at AWS. His current focus is on building scalable infrastructure for large language model inference and fine-tuning on AWS. Since joining Amazon in 2015, Mani has worked across natural language understanding and conversational AI systems—including Amazon Lex and Amazon Connect—before moving to generative AI infrastructure in AWS Bedrock.\n\nFlorian Saupe\nFlorian Saupe is a Principal Technical Product Manager at AWS AI/ML research working on model inference optimization, large scale distributed training, and fault resilience. Before joining AWS, Florian lead technical product management for automated driving at Bosch, was a strategy consultant at McKinsey & Company, and worked as a control systems and robotics scientist—a field in which he holds a PhD.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "f61358796e513e3f",
    "title": "Building intelligent event agents using Amazon Bedrock AgentCore and Amazon Bedrock Knowledge Bases",
    "url": "https://aws.amazon.com/blogs/machine-learning/building-intelligent-event-agents-using-amazon-bedrock-agentcore-and-amazon-bedrock-knowledge-bases/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-25T19:51:08Z",
    "summary": "This post demonstrates how to quickly deploy a production-ready event assistant using the components of Amazon Bedrock AgentCore. We'll build an intelligent companion that remembers attendee preferences and builds personalized experiences over time, while Amazon Bedrock AgentCore handles the heavy lifting of production deployment: Amazon Bedrock AgentCore Memory for maintaining both conversation context and long-term preferences without custom storage solutions, Amazon Bedrock AgentCore Identity...",
    "content": "Large conferences and events generate overwhelming amounts of information—from hundreds of sessions and workshops to speaker profiles, venue maps, and constantly updating schedules. While basic AI assistants can answer simple questions about event logistics, most fail to deliver the personalized guidance and contextual awareness that attendees need to navigate complex, multi-day conferences effectively. More importantly, moving these prototypes from demo to production—with enterprise security, scalability for thousands of concurrent users, and reliable performance—typically requires months of infrastructure development.\nThis post demonstrates how to quickly deploy a production-ready event assistant using the components of Amazon Bedrock AgentCore . We’ll build an intelligent companion that remembers attendee preferences and builds personalized experiences over time, while Amazon Bedrock AgentCore handles the heavy lifting of production deployment: Amazon Bedrock AgentCore Memory for maintaining both conversation context and long-term preferences without custom storage solutions, Amazon Bedrock AgentCore Identity for secure multi-IDP authentication, and Amazon Bedrock AgentCore Runtime for serverless scaling and session isolation. We will also use Amazon Bedrock Knowledge Bases for managed RAG and event data retrieval.\nBy the end, you’ll learn how to deploy an assistant that grows more helpful with every interaction – a proactive guide that makes sure attendees can discover their most valuable sessions – ready to serve thousands of concurrent conference attendees with enterprise-grade security and reliability, all without managing infrastructure.\nIf you are new to Amazon Bedrock AgentCore, feel free to review the following blog posts to understand the foundational concepts before diving into this implementation:\n\nAmazon Bedrock AgentCore Runtime\nAmazon Bedrock AgentCore Memory\nAmazon Bedrock AgentCore Identity\n\nSolution architecture\nLet’s walk through the architecture and workflow of our intelligent event agent. The complete implementation is available in this GitHub repository which provides a guided notebook you can follow to deploy this solution in your own AWS account.\n\nHow the solution works\nLet’s explore the different sections:\n1. User login and identity retrieval\nThe user logs in to the application using Amazon Cognito (AgentCore Identity also supports other identity providers such as Okta, Auth0, and OIDC-compliant IDPs), which generates a bearer token upon successful authentication. This token contains information about the user and will be used throughout the workflow for authentication and retrieving user-specific data.\n2. Agent invocation and initialization\nWhen the user interacts with the application and submits a query, the application calls Amazon Bedrock AgentCore Runtime with three key parameters: the user’s query, a session ID (for example, SessionA ), and the bearer token from Amazon Cognito. This links the interactions to both the user and their current session. AgentCore Identity authenticates and authorizes the user before allowing access to the agent.\nOn the first interaction, the Strands Agent initializes within Amazon Bedrock AgentCore Runtime and retrieves any available user preferences from the long-term storage of Amazon Bedrock AgentCore Memory, priming itself with personalized context. For subsequent interactions within the same session, the agent continues with the conversation using the context it has already established.\n3. Message processing\nThe agent stores both user and assistant messages in the short-term storage of Amazon Bedrock AgentCore Memory, containing both actor_id and session_id . The actor_id is the sub (subject identifier) claim extracted from the Amazon Cognito bearer token. The conversation context remains private and available only to the specific user within their current session. Behind the scenes, Amazon Bedrock AgentCore Memory employs a transformation pipeline that automatically processes these conversation events through configured memory strategies.\nEach memory strategy uses pattern recognition and natural language understanding to analyze the raw conversation data, identifying specific types of valuable information—such as user preferences—and extracting meaningful insights that warrant long-term retention. The system then structures this extracted information into standardized memory records, tags them with relevant metadata, and stores them in dedicated namespaces within long-term memory, for the agent to use to build an increasingly refined understanding of users across multiple sessions.\n4. Knowledge and memory retrieval\nTo fulfill the user’s request, the agent may trigger specialized tools. It can call an Amazon Bedrock knowledge base to fetch up-to-date event details such as session descriptions, schedules or speaker biographies.\n5. Response generation\nThe agent processes the query enriched with three layers of context: insights from long-term memory (personalized attendee history), context from short-term memory (recent messages in the session), and current event data from the knowledge base. It then generates a contextualized and personalized response.This architecture transforms a simple query like “What sessions should I attend tomorrow?” into a personalized experience—the agent recalls topics the user liked yesterday, considers the current conversation and responds with specific recommendations tailored to the user’s interests and history.\nSolution components\nLet’s now understand the role of each component of the solution.\nThe agent (AgentCore Runtime and identity integration)\nAt the core of our event assistant solution is the Amazon Bedrock AgentCore Runtime, a component which provides a secure, serverless environment for hosting our agent. Runtime manages the complete lifecycle of user interactions through isolated sessions—each running in dedicated microVMs with separate CPU, memory, and filesystem resources. This session isolation makes sure that thousands of conference attendees can simultaneously interact with personalized instances of the agent without cross-session data contamination.\n\nSecurity and authentication are handled through Amazon Bedrock AgentCore Identity, which integrates seamlessly with Amazon Cognito (and other IDPs). When an attendee sends a query to the event assistant, Amazon Bedrock AgentCore Identity validates the bearer token from Amazon Cognito before allowing the interaction to proceed. The bearer token is propagated through headers, and the agent retrieves this token to call the Amazon Cognito discovery server, extracting the user’s sub and other related user information. For this use case, we use the user sub as the actor_id for session information, enabling the agent to maintain user-specific context and deliver personalized recommendations. This identity-awareness helps make sure that each attendee’s preferences, conversation history, and session data can remain private and secure.\nBy combining the scalable hosting infrastructure of Runtime with the authentication framework of Identity, our event assistant can serve massive conferences more securely while delivering consistently personalized guidance to each individual attendee.\nAgent memory\nAmazon Bedrock AgentCore Memory provides the critical context awareness that transforms our event assistant from a simple Q&A tool into a truly personalized guide. The service is composed of short-term and long-term memory, which work together to enable continuous, evolving relationships between attendees and the event assistant.\nShort-term memory: Capturing the conversation\nShort-term memory is where interactions begin. As conversations unfold, the agent synchronously stores each message exchange as an immutable event in short-term memory. These events are organized hierarchically by actor_id and session_id as shown in the diagram. Remember that the actor_id is extracted from the Cognito bearer token (the user’s sub), while the session_id comes from the AgentCore Runtime session identifier.\nThis organizational structure serves two critical purposes. First, it maintains the chronological narrative of each conversation, preserving the natural flow of the dialogue. Second, it provides precise isolation—Actor A’s conversation in Session A1 remains separate from Session B or from another actor’s sessions. This facilitates privacy and enables the agent to retrieve exactly the right conversation context without loading unrelated data. The agent can quickly retrieve recent messages from short-term memory to maintain conversation continuity. When an attendee asks a follow-up question like “What time is that session?” the agent knows which session was just discussed because it has immediate access to the conversation history.\n\nLong-term memory: Building persistent intelligence\nWhile short-term memory captures what was said, long-term memory extracts what matters. As conversations occur, the AgentCore Memory service automatically processes these interactions to identify and store meaningful insights that persist across sessions. Our event agent uses a User Preferences strategy to capture explicit preferences about session formats, topics, or presentation styles. For example, “Prefers hands-on workshops over lectures” or “Interested in serverless and machine learning topics.”\nThese preferences are stored in a dedicated namespace within long-term memory (for example, /event-agent/actor-A/preferences ), for clear organization and targeted retrieval.\nAgent and memory orchestration\nThe seamless integration between the Strands Agent and AgentCore Memory is supported through agent hooks—event-driven touchpoints that automatically trigger memory operations throughout the agent’s lifecycle. As shown in the following diagram:\n\nThe Agent Initialized Event hook retrieves user preferences from long-term memory when a session begins, loading the attendee’s interests and preferred session types to enable personalized recommendations from the first query.\nThe Message Added Event hooks capture each user and assistant message, synchronously storing them in short-term memory to maintain conversation history.\n\nNote : While AgentCore provides an  automated memory manager  that implements memory tools automatically, this solution uses hooks for precise control over when and how memory operations are invoked—for fine-tuned optimization for the event assistant’s specific workflow.\nBeyond these automatic operations, if there isn’t enough information, the agent then uses the Retrieve sessions data tool to query the Amazon Bedrock knowledge base for current event details.\nThis dual approach—preloading essential context at startup and selectively retrieving details on-demand—delivers both speed and precision without bloating the session context with unnecessary information.\n\nAmazon Bedrock Knowledge Bases\nWhile AgentCore Memory maintains personalized context and conversation history, large conferences generate vast amounts of structured information—hundreds of session details, speaker profiles, venue maps, and schedule updates—that require efficient organization and retrieval.\nAmazon Bedrock Knowledge Bases is a fully managed service that supports Retrieval-Augmented Generation (RAG) by connecting foundation models to your data sources. It automatically handles the ingestion, processing, and indexing of documents, converting them into vector embeddings stored in a vector database. This allows agents to perform semantic searches—retrieving information based on meaning and context rather than exact keyword matches.\nThe architecture implements the knowledge base as a specialized tool within the Strands framework. When attendees ask specific questions about sessions, speakers, or venue logistics, the agent invokes this tool to retrieve precise, up-to-date information. The integration between memory and knowledge base creates a powerful combination. When an attendee asks, “Which AI sessions should I attend?”, the agent retrieves session details from the knowledge base while using memory to filter and prioritize recommendations based on the attendee’s previously expressed interests. This facilitates responses that are both factually complete and personally relevant, helping transform the overwhelming complexity of large conferences into manageable, tailored guidance.\nConclusion\nIn this post, we’ve explored how you can use Amazon Bedrock AgentCore components to rapidly productionize an event assistant—taking it from prototype to enterprise-ready deployment at scale. While building intelligent conversational agents is achievable with various tools, the real challenge lies in production deployment. The value of Amazon Bedrock Knowledge Bases and Amazon Bedrock AgentCore is in providing managed services that handle authentication, scaling, memory management, and RAG capabilities out of the box—helping remove months of infrastructure work.\nThe result is an event assistant that remembers an attendee’s interest in serverless technologies from yesterday’s conversations, understands they prefer hands-on workshops, and uses this context to deliver relevant recommendations from tomorrow’s schedule—all while maintaining the conversation flow of an ongoing planning session.\nThis is the difference between a prototype that demonstrates intelligent behavior and a production-ready system that can deliver personalized experiences to thousands of concurrent conference attendees with enterprise security and reliability. Whether you’re planning a small corporate gathering or a massive multi-day conference, Amazon Bedrock AgentCore provides the managed infrastructure to deploy intelligent assistance in days rather than months.\nNext Steps\nReady to enhance your event assistant further? Here are some ways to extend this solution:\n\nExpand capabilities with AgentCore Gateway : Amazon Bedrock AgentCore Gateway helps to connect your event assistant to additional tools and services at scale. Convert existing APIs, Lambda functions, or third-party services into tools your agent can use—whether that’s integrating with event registration systems, connecting to Slack for attendee notifications, or linking to Salesforce for lead tracking.\nExplore the GitHub Repository : Visit our complete implementation with step-by-step guidance, example code, and deployment instructions to build this solution in your own AWS account.\n\nAbout the authors\n\nDani Mitchell\nDani Mitchell is a Senior Generative AI Specialist Solutions Architect at Amazon Web Services (AWS). He is focused on helping accelerate enterprises across the world on their generative AI journeys with Bedrock AgentCore and Amazon Bedrock.\n\nSergio Garcés Vitale\nSergio Garcés Vitale is a Senior Solutions Architect at AWS, passionate about generative AI. With over 10 years of experience in the telecommunications industry, where he helped build data and observability systems. Over the past 5 years, Sergio has been focused on guiding customers in their cloud adoption, as well as implementing artificial intelligence use cases.\n\nAkarsha Sehwag\nAkarsha Sehwag is a Generative AI Data Scientist for the Amazon Bedrock AgentCore GTM team. With over six years of expertise in AI/ML, she has built production-ready enterprise solutions across diverse customer segments in Generative AI, Deep Learning and Computer Vision domains. Outside of work, she likes to hike, bike or play badminton.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "c9c0a5df837b38c2",
    "title": "Disrupting malicious uses of AI | February 2026",
    "url": "https://openai.com/index/disrupting-malicious-ai-uses",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-25T00:00:00Z",
    "summary": "Our latest threat report examines how malicious actors combine AI models with websites and social platforms—and what it means for detection and defense.",
    "content": "Our latest threat report examines how malicious actors combine AI models with websites and social platforms—and what it means for detection and defense.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]