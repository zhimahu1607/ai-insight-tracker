[
  {
    "id": "edaa5acbd0ed2582",
    "title": "Build unified intelligence with Amazon Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-unified-intelligence-with-amazon-bedrock-agentcore/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-18T23:54:29Z",
    "summary": "In this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of the Customer Agent and Knowledge Engine (CAKE).",
    "content": "Building cohesive and unified customer intelligence across your organization starts with reducing the friction your sales representatives face when toggling between Salesforce, support tickets, and Amazon Redshift. A sales representative preparing for a customer meeting might spend hours clicking through several different dashboards—product recommendations, engagement metrics, revenue analytics, etc. – before developing a complete picture of the customer’s situation. At AWS, our sales organization experienced this firsthand as we scaled globally. We needed a way to unify siloed customer data across metrics databases, document repositories, and external industry sources – without building complex custom orchestration infrastructure.\nWe built the Customer Agent & Knowledge Engine (CAKE), a customer centric chat agent using Amazon Bedrock AgentCore to solve this challenge. CAKE coordinates specialized retriever tools – querying knowledge graphs in Amazon Neptune , metrics in Amazon DynamoDB , documents in Amazon OpenSearch Service, and external market data using a web search API, along with security enforcement using Row Level Security tool (RLS), delivering customer insights through natural language queries in under 10 seconds (as observed in agent load tests).\nIn this post, we demonstrate how to build unified intelligence systems using Amazon Bedrock AgentCore through our real-world implementation of CAKE. You can build custom agents that unlock the following features and benefits:\n\nCoordination of specialized tools through dynamic intent analysis and parallel execution\nIntegration of purpose-built data stores (Neptune, DynamoDB, OpenSearch Service) with parallel orchestration\nImplementation of row-level security and governance within workflows\nProduction engineering practices for reliability, including template-based reporting to adhere to business semantic and style\nPerformance optimization through model flexibility\n\nThese architectural patterns can help you accelerate development for different use cases, including customer intelligence systems, enterprise AI assistants, or multi-agent systems that coordinate across different data sources.\nWhy customer intelligence systems need unification\nAs sales organizations scale globally, they often face three critical challenges: fragmented data across specialized tools (product recommendations, engagement dashboards, revenue analytics, etc.) requiring hours to gather comprehensive customer views, loss of business semantics in traditional databases that can’t capture semantic relationships explaining why metrics matter, and manual consolidation processes that can’t scale with growing data volumes. You need a unified system that can aggregate customer data, understand semantic relationships, and reason through customer needs in business context, making CAKE the essential linchpin for enterprises everywhere.\nSolution overview\nCAKE is a customer-centric chat agent that transforms fragmented data into unified, actionable intelligence. By consolidating internal and external data sources/tables into a single conversational endpoint, CAKE delivers personalized customer insights powered by context-rich knowledge graphs—all in under 10 seconds. Unlike traditional tools that simply report numbers, the semantic foundation of CAKE captures the meaning and relationships between business metrics, customer behaviors, industry dynamics, and strategic contexts. This enables CAKE to explain not just what is happening with a customer, but why it’s happening and how to act.\nAmazon Bedrock AgentCore provides the runtime infrastructure that multi-agent AI systems require as a managed service, including inter-agent communication, parallel execution, conversation state tracking, and tool routing. This helps teams focus on defining agent behaviors and business logic rather than implementing distributed systems infrastructure.\nFor CAKE, we built a custom agent on Amazon Bedrock AgentCore that coordinates five specialized tools, each optimized for different data access patterns:\n\nNeptune retriever tool for graph relationship queries\nDynamoDB agent for instant metric lookups\nOpenSearch retriever tool for semantic document search\nWeb search tool for external industry intelligence\nRow level security (RLS) tool for security enforcement\n\nThe following diagram shows how Amazon Bedrock AgentCore supports the orchestration of these components.\n\nThe solution flows through several key phases in response to a question (for example, “What are the top expansion opportunities for this customer?”):\n\nAnalyzes intent and routes the query – The supervisor agent, running on Amazon Bedrock AgentCore, analyzes the natural language query to determine its intent. The question requires customer understanding, relationship data, usage metrics, and strategic insights. The agent’s tool-calling logic, using Amazon Bedrock AgentCore Runtime, identifies which specialized tools to activate.\nDispatches tools in parallel – Rather than executing tool calls sequentially, the orchestration layer dispatches multiple retriever tools in parallel, using the scalable execution environment of Amazon Bedrock AgentCore Runtime. The agent manages the execution lifecycle, handling timeouts, retries, and error conditions automatically.\nSynthesizes multiple results – As specialized tools return results, Amazon Bedrock AgentCore streams these partial responses to the supervisor agent, which synthesizes them into a coherent answer. The agent reasons about how different data sources relate to each other, identifies patterns, and generates insights that span multiple knowledge domains.\nEnforces security boundaries – Before data retrieval begins, the agent invokes the RLS tool to deterministically enforce user permissions. The custom agent then verifies that subsequent tool calls respect these security boundaries, automatically filtering results and helping prevent unauthorized data access. This security layer operates at the infrastructure level, reducing the risk of implementation errors.\n\nThis architecture operates on two parallel tracks: Amazon Bedrock AgentCore provides the runtime for the real-time serving layer that responds to user queries with minimal latency, and an offline data pipeline periodically refreshes the underlying data stores from the analytical data warehouse. In the following sections, we discuss the agent framework design and core solution components, including the knowledge graph, data stores, and data pipeline.\nAgent framework design\nOur multi-agent system leverages the AWS Strands Agents framework to deliver structured reasoning capabilities while maintaining the enterprise controls required for regulatory compliance and predictable performance. The multi-agent system is built on the AWS Strands Agents framework, which provides a model-driven foundation for building agents from many different models. The supervisor agent analyzes incoming questions to intelligently select which specialized agents and tools to invoke and how to decompose user queries. The framework exposes agent states and outputs to implement decentralized evaluation at both agent and supervisor levels. Building on model-driven approach, we implement agentic reasoning through GraphRAG reasoning chains that construct deterministic inference paths by traversing knowledge relationships. Our agents perform autonomous reasoning within their specialized domains, grounded around pre-defined ontologies while maintaining predictable, auditable behavior patterns required for enterprise applications.\nThe supervisor agent employs a multi-phase selection protocol:\n\nQuestion analysis  – Parse and understand user intent\nSource selection  – Intelligent routing determines which combination of tools are needed\nQuery decomposition  – Original questions are broken down into specialized sub-questions optimized for each selected tool\nParallel execution  – Selected tools execute concurrently through serverless AWS Lambda action groups\n\nTools are exposed through a hierarchical composition pattern (accounting for data modality—structured vs. unstructured) where high-level agents and tools coordinate multiple specialized sub-tools:\n\nGraph reasoning tool  – Manages entity traversal, relationship analysis, and knowledge extraction\nCustomer insights agent  – Coordinates multiple fine-tuned models in parallel for generating customer summaries from tables\nSemantic search tool  – Orchestrates unstructured text analysis (such as field notes)\nWeb research tool  – Coordinates web/news retrieval\n\nWe extend the core AWS Strands Agents framework with enterprise-grade capabilities including customer access validation, token optimization, multi-hop LLM selection for model throttling resilience, and structured GraphRAG reasoning chains. These extensions deliver the autonomous decision-making capabilities of modern agentic systems while facilitating predictable performance and regulatory compliance alignment.\nBuilding the knowledge graph foundation\nCAKE’s knowledge graph in Neptune represents customer relationships, product usage patterns, and industry dynamics in a structured format that empowers AI agents to perform efficient reasoning. Unlike traditional databases that store information in isolation, CAKE’s knowledge graph captures the semantic meaning of business entities and their relationships.\nGraph construction and entity modeling\nWe designed the knowledge graph around AWS sales ontology—the core entities and relationships that sales teams discuss daily:\n\nCustomer entities – With properties extracted from data sources including industry classifications, revenue metrics, cloud adoption phase, and engagement scores\nProduct entities – Representing AWS services, with connections to use cases, industry applications, and customer adoption patterns\nSolution entities – Linking products to business outcomes and strategic initiatives\nOpportunity entities – Tracking sales pipeline, deal stages, and associated stakeholders\nContact entities – Mapping relationship networks within customer organizations\n\nAmazon Neptune excels at answering questions that require understanding connections—finding how two entities are related, identifying paths between accounts, or discovering indirect relationships that span multiple hops. The offline data construction process runs scheduled queries against Redshift clusters to prepare data to be loaded in the graph.\nCapturing relationship context\nCAKE’s knowledge graph captures how relationships connect entities. When the graph connects a customer to a product through an increased usage relationship, it also stores contextual attributes: the rate of increase, the business driver (from account plans), and related product adoption patterns. This contextual richness helps the LLM understand business context and provide explanations grounded in actual relationships rather than statistical correlation alone.\nPurpose-built data stores\nRather than storing data in a single database, CAKE uses specialized data stores, each designed for how it gets queried. Our custom agent, running on Amazon Bedrock AgentCore, manages the coordination across these stores—sending queries to the right database, running them at the same time, and combining results—so both users and developers work with what feels like a single data source:\n\nNeptune for graph relationships – Neptune stores the web of connections between customers, accounts, stakeholders, and organizational entities. Neptune excels at multi-hop traversal queries that require expensive joins in relational databases—finding relationship paths between disconnected accounts, or discovering customers in an industry who’ve adopted specific AWS services. When Amazon Bedrock AgentCore identifies a query requiring relationship reasoning, it automatically routes to the Neptune retriever tool.\nDynamoDB for instant metrics – DynamoDB operates as a key-value store for precomputed aggregations. Rather than computing customer health scores or engagement metrics on-demand, the offline pipeline pre-computes these values and stores them indexed by customer ID. DynamoDB then delivers sub-10ms lookups, enabling instant report generation. Tool chaining in Amazon Bedrock AgentCore allows it to retrieve metrics from DynamoDB, pass them to the magnifAI agent (our custom table-to-text agent) for formatting, and return polished reports—all without custom integration code.\nOpenSearch Service for semantic document search – OpenSearch Service stores unstructured content like account plans and field notes. Using embedding models, OpenSearch Service converts text into vector representations that support semantic matching. When Amazon Bedrock AgentCore receives a query about “digital transformation,” for example, it recognizes the need for semantic search and automatically routes to the OpenSearch Service retriever tool, which finds relevant passages even when documents use different terminology.\nS3 for document storage – Amazon Simple Storage Service (Amazon S3) provides the foundation for OpenSearch Service. Account plans are stored as Parquet files in Amazon S3 before being indexed because the source warehouse (Amazon Redshift) has truncation limits that would cut off large documents. This multi-step process—Amazon S3 storage, embedding generation, OpenSearch Service indexing—preserves complete content while maintaining the low latency required for real-time queries.\n\nBuilding on Amazon Bedrock AgentCore makes these multi-database queries feel like a single, unified data source. When a query requires customer relationships from Neptune, metrics from DynamoDB, and document context from OpenSearch Service, our agent automatically dispatches requests to all three in parallel, manages their execution, and synthesizes their results into a single coherent response.\nData pipeline and continuous refresh\nThe CAKE offline data pipeline operates as a batch process that runs on a scheduled cadence to keep the serving layer synchronized with the latest business data. The pipeline architecture separates data construction from data serving, so the real-time query layer can maintain low latency while the batch pipeline handles computationally intensive aggregations and graph construction.\nThe Data Processing Orchestration layer coordinates transformations across multiple target databases. For each database, the pipeline performs the following steps:\n\nExtracts relevant data from Amazon Redshift using optimized queries\nApplies business logic transformations specific to each data store’s requirements\nLoads processed data into the target database with appropriate indexes and partitioning\n\nFor Neptune, this involves extracting entity data, constructing graph nodes and edges with property attributes, and loading the graph structure with semantic relationship types. For DynamoDB, the pipeline computes aggregations and metrics, structures data as key-value pairs optimized for customer ID lookups, and applies atomic updates to maintain consistency. For OpenSearch Service, the pipeline follows a specialized path: large documents are first exported from Amazon Redshift to Amazon S3 as Parquet files, then processed through embedding models to generate vector representations, which are finally loaded into the OpenSearch Service index with appropriate metadata for filtering and retrieval.\nEngineering for production: Reliability and accuracy\nWhen transitioning CAKE from prototype to production, we implemented several critical engineering practices to facilitate reliability, accuracy, and trust in AI-generated insights.\nModel flexibility\nThe Amazon Bedrock AgentCore architecture decouples the orchestration layer from the underlying LLM, allowing flexible model selection. We implemented model hopping to provide automatic fallback to alternative models when throttling occurs. This resilience happens transparently within AgentCore’s Runtime—detecting throttling conditions, routing requests to available models, and maintaining response quality without user-visible degradation.\nRow-Level Security (RLS) and Data Governance\nBefore data retrieval occurs, the RLS tool enforces row-level security based on user identity and organizational hierarchy. This security layer operates transparently to users while maintaining strict data governance:\n\nSales representatives access only customers assigned to their territories\nRegional managers view aggregated data across their regions\nExecutives have broader visibility aligned with their responsibilities\n\nThe RLS tool routes queries to appropriate data partitions and applies filters at the database query level, so security can be enforced in the data layer rather than relying on application-level filtering.\nResults and impact\nCAKE has transformed how AWS sales teams access and act on customer intelligence. By providing instant access to unified insights through natural language queries, CAKE reduces the time spent searching for information from hours to seconds as per surveys/feedback from users, helping sales representatives focus on strategic customer engagement rather than data gathering.\nThe multi-agent architecture delivers query responses in seconds for most queries, with the parallel execution model supporting simultaneous data retrieval from multiple sources. The knowledge graph enables sophisticated reasoning that goes beyond simple data aggregation—CAKE explains why trends occur, identifies patterns across seemingly unrelated data points, and generates recommendations grounded in business relationships. Perhaps most importantly, CAKE democratizes access to customer intelligence across the organization. Sales representatives, account managers, solutions architects, and executives interact with the same unified system, providing consistent customer insights while maintaining appropriate security and access controls.\nConclusion\nIn this post, we showed how Amazon Bedrock AgentCore supports CAKE’s multi-agent architecture. Building multi-agent AI systems traditionally requires significant infrastructure investment, including implementing custom agent coordination protocols, managing parallel execution frameworks, tracking conversation state, handling failure modes, and building security enforcement layers. Amazon Bedrock AgentCore reduces this undifferentiated heavy lifting by providing these capabilities as managed services within Amazon Bedrock.\nAmazon Bedrock AgentCore provides the runtime infrastructure for orchestration, and specialized data stores excel at their specific access patterns. Neptune handles relationship traversal, DynamoDB provides instant metric lookups, and OpenSearch Service supports semantic document search, but our custom agent, built on Amazon Bedrock AgentCore, coordinates these components, automatically routing queries to the right tools, executing them in parallel, synthesizing their results, and maintaining security boundaries throughout the workflow. The CAKE experience demonstrates how Amazon Bedrock AgentCore can help teams build multi-agent AI systems, speeding up the process from months of infrastructure development to weeks of business logic implementation. By providing orchestration infrastructure as a managed service, Amazon Bedrock AgentCore helps teams focus on domain expertise and customer value rather than building distributed systems infrastructure from scratch.\nTo learn more about Amazon Bedrock AgentCore and building multi-agent AI systems, refer to the Amazon Bedrock User Guide , Amazon Bedrock Workshop , and Amazon Bedrock Agents . For the latest news on AWS, see What’s New with AWS .\nAcknowledgments\nWe extend our sincere gratitude to our executive sponsors and mentors whose vision and guidance made this initiative possible: Aizaz Manzar , Director of AWS Global Sales; Ali Imam , Head of Startup Segment; and Akhand Singh , Head of Data Engineering.\nWe also thank the dedicated team members whose technical expertise and contributions were instrumental in bringing this product to life: Aswin Palliyali Venugopalan , Software Dev Manager; Alok Singh , Senior Software Development Engineer; Muruga Manoj Gnanakrishnan , Principal Data Engineer; Sai Meka , Machine Learning Engineer; Bill Tran , Data Engineer; and Rui Li , Applied Scientist.\n\nAbout the authors\nMonica Jain  is a Senior Technical Product Manager at AWS Global Sales and an analytics professional driving AI-powered sales intelligence at scale. She leads the development of generative AI and ML-powered data products—including knowledge graphs, AI-augmented analytics, natural language query systems, and recommendation engines, that improve seller productivity and decision-making. Her work enables AWS executives and sellers worldwide to access real-time insights and accelerate data-driven customer engagement and revenue growth.\nM. Umar Javed is a Senior Applied Scientist at AWS, with over 8 years of experience across academia and industry and a PhD in ML theory. At AWS, he builds production-grade generative AI and machine learning solutions, with work spanning multi-agent LLM architectures, research on small language models, knowledge graphs, recommendation systems, reinforcement learning, and multi-modal deep learning. Prior to AWS, Umar contributed to ML research at NREL, CISCO, Oxford, and UCSD. He is a recipient of the ECEE Excellence Award (2021) and contributed to two Donald P. Eckman Awards (2021, 2023).\nDamien Forthomme is a Senior Applied Scientist at AWS, leading a Data Science team in AWS Sales, Marketing, and Global Services (SMGS). With more than 10 years of experience and a PhD in Physics, he focuses on using and building advanced machine learning and generative AI tools to surface the right data to the right people at the right time. His work encompasses initiatives such as forecasting, recommendation systems, core foundational datasets creation, and building generative AI products that enhance sales productivity for the organization.\nMihir Gadgil is a Senior Data Engineer in AWS Sales, Marketing, and Global Services (SMGS), specializing in enterprise-scale data solutions and generative AI applications. With over 9 years of experience and a Master’s in Information Technology & Management, he focuses on building robust data pipelines, complex data modeling, and ETL/ELT processes. His expertise drives business transformation through innovative data engineering solutions and advanced analytics capabilities.\nSujit Narapareddy , Head of Data & Analytics at AWS Global Sales, is a technology leader driving global enterprise transformation. He leads data product and platform teams that power the AWS’s Go-to-Market through AI-augmented analytics and intelligent automation. With a proven track record in enterprise solutions, he has transformed sales productivity, data governance, and operational excellence. Previously at JPMorgan Chase Business Banking, he shaped next-generation FinTech capabilities through data innovation.\nNorman Braddock , Senior Manager of AI Product Management at AWS, is a product leader driving the transformation of business intelligence through agentic AI. He leads the Analytics & Insights Product Management team within Sales, Marketing, and Global Services (SMGS), delivering products that bridge AI model performance with measurable business impact. With a background spanning procurement, manufacturing, and sales operations, he combines deep operational expertise with product innovation to shape the future of autonomous business management.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "2c652b1871a98af2",
    "title": "Introducing OpenAI for India",
    "url": "https://openai.com/index/openai-for-india",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-18T21:00:00Z",
    "summary": "OpenAI for India expands AI access across the country—building local infrastructure, powering enterprises, and advancing workforce skills.",
    "content": "OpenAI for India expands AI access across the country—building local infrastructure, powering enterprises, and advancing workforce skills.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "4c69cb9f8648a0ef",
    "title": "Evaluating AI agents: Real-world lessons from building agentic systems at Amazon",
    "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-ai-agents-real-world-lessons-from-building-agentic-systems-at-amazon/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-18T19:21:28Z",
    "summary": "In this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations, along with Amazon use case-specific evaluation approaches and metrics.",
    "content": "The generative AI industry has undergone a significant transformation from using large language model (LLM)-driven applications to agentic AI systems , marking a fundamental shift in how AI capabilities are architected and deployed. While early generative AI applications primarily relied on LLMs to directly generate text and respond to prompts, the industry has evolved from those static, prompt-response paradigms toward autonomous agent frameworks to build dynamic, goal-oriented systems capable of tool orchestration, iterative problem-solving, and adaptive task execution in production environments.\nWe have witnessed this evolution in Amazon; since 2025, there have been thousands of agents built across Amazon organizations. While single-model benchmarks serve as a crucial foundation for assessing individual LLM performance in LLM-driven applications, agentic AI systems require a fundamental shift in evaluation methodologies. The new paradigm assesses not only the underlying model performance but also the emergent behaviors of the complete system, including the accuracy of tool selection decisions, the coherence of multi-step reasoning processes, the efficiency of memory retrieval operations, and the overall success rates of task completion across production environments.\nIn this post, we present a comprehensive evaluation framework for Amazon agentic AI systems that addresses the complexity of agentic AI applications at Amazon through two core components: a generic evaluation workflow that standardizes assessment procedures across diverse agent implementations, and an agent evaluation library that provides systematic measurements and metrics in Amazon Bedrock AgentCore Evaluations , along with Amazon use case-specific evaluation approaches and metrics. We also share best practices and experiences captured during engagements with multiple Amazon teams, providing actionable insights for AWS developer communities facing similar challenges in evaluating and deploying agentic AI systems within their own business contexts.\nAI agent evaluation framework in Amazon\nWhen builders design, develop, and evaluate AI agents, they face significant challenges. Unlike traditional LLM-driven applications that only generate responses to isolated prompts, AI agents autonomously pursue goals through multi-step reasoning, tool use, and adaptive decision-making across multi-turn interactions. Traditional LLM evaluation methods treat agent systems as black boxes and evaluate only the final outcome, failing to provide sufficient insights to determine why AI agents fail or pinpoint the root causes. Although multiple specific evaluation tools are available in the industry, builders must navigate among them and consolidate results with significant manual efforts. Additionally, while agent development frameworks, such as Strands Agents , LangChain , and LangGraph , have built-in evaluation modules, builders want a framework-agnostic evaluation approach rather than being locked into methods within a single framework.\nAdditionally, robust self-reflection and error handling in AI agents requires systematic assessment of how agents detect, classify, and recover from failures across the execution lifecycle in reasoning, tool-use, memory handling, and action taking. For example, the evaluation frameworks must measure the agent’s ability to recognize diverse failure scenarios such as inappropriate planning from the reasoning model, invalid tool invocations, malformed parameters, unexpected tool response formats, authentication failures, and memory retrieval errors. A production-grade agent must demonstrate consistent error recovery patterns and resilience in maintaining the coherence of user interactions after encountering exceptions.\nTo meet these needs, AI agents deployed in production environments at scale require continuous monitoring and systematic evaluation to promptly detect and mitigate agent decay and performance degradation. This demands that the agent evaluation framework streamline the end-to-end process and provide near real-time issue detection, notification, and problem resolution. Finally, incorporating human-in-the-loop (HITL) processes is essential to audit evaluation results, helping to ensure the reliability of system outputs.\nTo address these challenges, we propose a holistic agentic AI evaluation framework, as shown in the following figure. The framework contains two key components: an automated AI agent evaluation workflow and an AI agent evaluation library.\n\nThe automated AI agent evaluation workflow drives the holistic evaluation approach with four steps.\nStep 1 : Users define inputs for evaluation, typically trace files from agent execution. These can be offline traces collected after the agent completes the task and uploaded to the framework using a unified API access point or online traces where users can define evaluation dimensions and metrics.\nStep 2 : The AI agent evaluation library is used to automatically generate default and user-defined evaluation metrics. The methods in the library are described in the next list.\nStep 3 : The evaluation results are shared through an Amazon Simple Storage Service (Amazon S3) bucket or a dashboard that visualizes the agent trace observability and evaluation results.\nStep 4 : Results are analyzed through agent performance auditing and monitoring. Builders can define their own rules to send notifications upon agent performance degradation and can take action to resolve problems. Builders can also HITL mechanisms to schedule periodic human audits of agent trace subsets and evaluation results, improving consistent agent quality and performance.\nThe AI agent evaluation library operates across three layers: calculating and generating evaluation metrics for the agent’s final output, assessing individual agent components, and measuring the performance of the underlying LLMs that power the agent.\n\nBottom layer : Benchmarks multiple foundation models to select the appropriate models powering the AI agent and determine how different models impact the agent overall quality and latency.\nMiddle layer : Evaluates the performance of the components of the agent, including intent detection, multi-turn conversation, memory, LLM reasoning and planning, tool-use, and others. For example, the middle layer determines whether the agent understands user intents correctly, how the LLM drives agentic workflow planning through chain-of-thought (CoT) reasoning, whether the tool selection and execution are aligned with the agentic plan, and if the plan is completed successfully.\nUpper layer : Assesses the agent’s final response, the task completion, and whether the agent meets the goal defined in the use case. It also covers overall responsibility and safety, the costs, and the customer experience impacts.\n\nAmazon Bedrock AgentCore Evaluations provides automated assessment tools to measure how well your agent or tools perform specific tasks, handle edge cases, and maintain consistency across different inputs and contexts. In the agent evaluation library, we provide a set of pre-defined evaluation metrics for the agent’s final response and its components, based on the built-in configurations, evaluators, and metrics of AgentCore Evaluations. We further extended the evaluation library with specialized metrics designed for the heterogeneous scenario complexity and application-specific requirements of Amazon. The primary metrics in the library include\n\nFinal response quality:\n\nCorrectness : The factual accuracy and correctness of an AI assistant’s response to a given task.\nFaithfulness : Whether an AI assistant’s response remains consistent with the conversation history.\nHelpfulness : How effectively an AI assistant’s response helps users appropriately address query and progress toward their goals.\nResponse relevance : How well an AI assistant’s response addresses the specific question or request.\nConciseness : How efficiently an AI assistant communicates information, for instance, whether the response is appropriately brief without missing key information.\n\nTask completion: \n\nGoal success : Did the AI assistant successfully complete all user goals within a conversation session.\nGoal accuracy : Compares the output to the ground truth.\n\nTool use:\n\nTool selection accuracy : Did the AI assistant choose the appropriate tool for a given situation.\nTool parameter accuracy : Did the AI assistant correctly use contextual information when making tool calls.\nTool call error rate : The frequency of failures when an AI assistant makes tool calls.\nMulti-turn function calling accuracy : Are multiple tools being called and how often the tools are called in the correct sequence.\n\nMemory:\n\nContext retrieval : Assesses the accuracy of findings and surfaces the most relevant contexts for a given query from memory, prioritizing relevant information based on similarity or ranking, and balancing precision and recall.\n\nMulti-turn: \n\nTopic adherence classification : If a multi-turn conversation includes multiple topics, assesses whether the conversation stays on predefined domains and topics during the interaction.\nTopic adherence refusal : Determines if the AI agent refuse to answer questions about a topic.\n\nReasoning:\n\nGrounding accuracy : Does the model understand the task, appropriately select tools, and is the CoT aligned with the provided context and data returned by external tools.\nFaithfulness score : Measures logical consistency across the reasoning process.\nContext score : Is each step taken by the agent contextually grounded.\n\nResponsibility and safety:\n\nHallucination : Do the outputs align with established knowledge, verifiable data, logical inference, or include any elements that are implausible, misleading, or entirely fictional.\nToxicity : Do the outputs contain language, suggestions, or attitudes that are harmful, offensive, disrespectful, or promote negativity. This include content that might be aggressive, demeaning, bigoted, or excessively critical without constructive purpose.\nHarmfulness : Is there potentially harmful content in an AI assistant’s response, including insults, hate speech, violence, inappropriate sexual content, and stereotyping.\n\nSee AgentCore evaluation templates  for other agent output quality metrics, or how to create custom evaluators that are tailored to your specific use cases and evaluation requirements.\nEvaluating real-world agent systems used by Amazon\nIn the past few years, Amazon has been working to advance its approach in building agentic AI applications to address complex business challenges, streamlining business processes, improving operational efficiency, and optimizing business outcomes—moving from early experimentation to production-scale deployments across multiple business units. These agentic AI applications operate at enterprise scale and are deployed across AWS infrastructure, transforming how work gets done across global operations within Amazon. In this section, we introduce a few real-world agentic AI use cases from Amazon, to demonstrate how Amazon teams improve AI agent performance through holistic evaluation using the framework discussed in the previous section.\nEvaluating tool-use in the Amazon shopping assistant AI agent\nTo deliver a smooth shopping experience to Amazon consumers, the Amazon shopping assistant can seamlessly interact with numerous APIs and web services from underlying Amazon systems, as shown in the following figure. The AI agent needs to onboard hundreds, sometimes thousands, of tools from underlying Amazon systems to engage in long-running multi-turn conversations with the consumer. The agent uses these tools to deliver a personalized experience that includes customer profiling, product and inventory discovery, and order placement. However, manually onboarding so many enterprise APIs and web services to an AI agent is a cumbersome process that typically takes months to complete.\n\nTransforming legacy APIs and web services into agent-compatible tools requires the systematic definition of structured schemas and semantic descriptions for the endpoints of the API and web services, enabling the agent’s reasoning and planning mechanisms to accurately identify and select contextually appropriate tools during task execution. Poorly defined tool schemas and imprecise semantic descriptions result in erroneous tool selection during agent runtime, leading to the invocation of irrelevant APIs that unnecessarily expand the context window, increase inference latency, and escalate computational costs through redundant LLM calls. To address these challenges, Amazon defined cross-organizational standards for tool schema and description formalization, creating a governance framework that specifies mandatory compliance requirements for all builder teams involved in tool development and agent integration. This standardization initiative establishes uniform specifications for tool interfaces, parameter definitions, capability descriptions, and usage constraints, helping to ensure that tools developed across diverse organizational units maintain consistent structural patterns and semantic clarity to produce reliable agent-tool interactions. All builder teams engaged in tool development and agent integration must conform to these architectural specifications, which prescribe standardized formats for tool signatures, input validation schemas, output contracts, and human-readable documentation. This helps ensure consistency in tool representation across the enterprise agentic systems. Furthermore, manually defining tool schemas and descriptions for hundreds or thousands of tools represents a significant engineering burden, and the complexity escalates substantially when multiple APIs require coordinated orchestration to accomplish composite tasks. Amazon builders implemented an API self-onboarding system that uses LLMs to automate the generation of standardized tool schemas and descriptions. This significantly improved the efficiency in onboarding large numbers of APIs and services into agent-compatible tools, accelerating integration timelines and reducing manual engineering overhead. To evaluate the tool-selection and tool-use after integration of the APIs is completed, Amazon teams created golden datasets for regression testing. The datasets are generated synthetically using LLMs from historical API invocation logs upon user queries. Using pre-defined tool-selection and tool-use metrics such as tool selection accuracy, tool parameter accuracy, and multi-turn function call accuracy, the Amazon builders can systematically evaluate the shopping assistant AI agent’s capability to correctly identify appropriate tools, populate their parameters with accurate values, and maintain coherent tool invocation sequences across conversational turns. As the agent continues to evolve, the ability to rapidly and reliably integrate new APIs as tools in the agent and evaluate the tool-use performance becomes increasingly crucial. The objective assessment of agent’s functional reliability in production environments effectively reduces development overhead while maintaining robust performance in the agentic AI applications.\nEvaluating user intent detection in the Amazon customer service AI agent\nIn the Amazon customer-service landscape, AI agents are instrumental in handling customer inquiries and resolving issues. At the heart of these systems lies a crucial capability: an orchestration AI agent using it’s reasoning model to accurately detect customer intent, which determines whether a customer’s query is correctly understood and routed to the appropriate specialized resolver implemented by agent tools or subagents, as shown in the following figure. The stakes are high when it comes to intent detection accuracy. When the customer service agent misinterprets a customer’s intent, it can trigger a cascade of problems: queries get routed to the wrong specialized resolvers, customers receive irrelevant responses, and frustration builds. This impacts customer experience and leads to increased operational costs as more customers seek intervention from human agents.\n\nTo evaluate the agent’s reasoning capability for intent detection, the Amazon team developed an LLM simulator that uses LLM driven virtual customer personas to simulate diverse user scenarios and interactions. The evaluation is mainly focused on correctness of the intent generated by the orchestration agent and routing to the correct subagent. The simulation dataset contains a set of user query and ground truth intent pairs collected from anonymized historical customer interactions. Using the simulator, the orchestration agent generates the intents upon the user queries in the simulation dataset. By comparing the agent response intent to the ground truth intent, we can validate if the agent-generated intents comply with the ground truth.\nIn addition to the intent correctness, the evaluation covers the task completion—the agent’s final response and intent resolution—as the final goal of the customer service tasks. For the multi-turn conversation, we also include the metrics of topic adherence classification and topic adherence refusal to help ensure conversational coherence and user experience quality. As AI customer service systems continue to evolve, the importance of robust agent reasoning evaluation for user intent detection only grows, the impact extends beyond immediate customer satisfaction. It also optimizes customer service operation efficiency and service delivery costs, and so maximizes the return on AI investments.\nEvaluating multi-agent systems at Amazon\nAs enterprises increasingly confront multifaceted challenges in complex business environments, ranging from cross-functional workflow orchestration to real-time decision-making under uncertainty, Amazon teams are progressively adopting multi-agent system architectures that decompose monolithic AI solutions into specialized, collaborative agents capable of distributed reasoning, dynamic task allocation, and adaptive problem-solving at scale. One example is the Amazon seller assistant AI agent that encompasses collaborations among multiple AI agents, depicted in the following flow chart.\n\nThe agentic workflow, beginning with an LLM planner and task orchestrator, receives user requests, decomposes complex tasks into specialized subtasks, and intelligently assigns each subtask to the most appropriate underlying agent based on their capabilities and current workload. The underlying agents then operate autonomously, executing their assigned tasks by using their specialized tools, reasoning capabilities, and domain expertise to complete objectives without requiring continuous oversight from the orchestrator. Upon task completion the specialized agents communicate back to the orchestration agent, reporting task status updates, completion confirmations, intermediate results, or escalation requests when they encounter scenarios beyond their operational boundaries. The orchestration agent aggregates these responses, monitors overall progress, handles dependencies between subtasks, and synthesizes the collective outputs into a coherent final result that addresses the original user request. To evaluate this multi-agent collaboration process, the evaluation workflow accounts for both individual agent performance and the overall collective system dynamics. In addition to evaluating the overall task execution quality and performance of specialized agents in task completion, reasoning, tool-use and memory retrieval, we also need to measure the interagent communication patterns, coordination efficiency, and task handoff accuracy. For this, Amazon teams use the metrics such as the planning score (successful subtask assignment to subagents), communication score (interagent communication messages for subtask completion), and collaboration success rate (percentage of successful sub-task completion). In multi-agent systems evaluation, HITL becomes critical because of the increased complexity and potential for unexpected emergent behaviors that automated metrics might fail to capture. Human intervention in the evaluation workflow provides essential oversight for assessing inter-agent communication to identify coordination failure in specific edge cases, evaluating the appropriateness of agent specialization and whether task decomposition aligns with agent capabilities, and validating potential conflict resolution strategies when agents produce contradictory recommendations. It also helps ensure logical consistency when multiple agents contribute to a single decision, and that the collective agent behavior serves the intended business objective. These are the dimensions that are difficult to quantify through automated metrics alone but are critical for production deployment success.\nLessons learned and best practices\nThrough extensive engagements with Amazon product and engineering teams deploying agentic AI systems in production environments, we have identified critical lessons learned and established best practices that address the unique challenges of evaluating autonomous agent architectures at scale.\n\nHolistic evaluation across multiple dimensions : Agentic application evaluation must extend beyond traditional accuracy metrics to encompass a comprehensive assessment framework that covers agent quality, performance, responsibility, and cost. Quality evaluation includes measuring reasoning coherence, tool selection accuracy, and task completion success rates across diverse scenarios. Performance assessment captures latency, throughput, and resource utilization under production workloads. Responsibility evaluation addresses safety, toxicity, bias mitigation, hallucination detection, and guardrails to align with organizational policies and regulatory requirements. Cost analysis quantifies both direct expenses including model inference, tool invocation, data processing, and indirect costs such as human efforts and error remediation. This multi-dimensional approach helps ensure holistic optimization across balanced trade-offs.\nUse case and application-specific evaluation: Besides the standardized metrics discussed in the previous sections, application-specific evaluation metrics also contribute to the overall application assessment. For instance, customer service applications require metrics such as customer satisfaction scores, first-contact resolution rates, and sentiment analysis scores to measure final business outcomes. This approach requires close collaboration with domain experts to define meaningful success criteria, define appropriate metrics, and create evaluation datasets that reflect real-world operational complexity to complete the assessment process.\nHuman-in-the-loop (HITL) as a critical evaluation component: As discussed in the multi-agent system evaluation case, HITL is indispensable, particularly for high-stakes decision scenarios. It provides essential evaluation of agent reasoning chains, the coherence of multi-step workflows, and the alignment of agent behavior with business requirements. HITL also helps provide ground truth labels for building golden testing datasets, and calibration of LLM-as-a-judge in the automatic evaluator to align with human preferences.\nContinuous evaluation in production environments: It’s essential to maintain quality because the pre-deployment evaluation might not fully capture the performance characteristics. Also, production evaluation monitors real-world performance across diverse user behaviors, usage patterns, and edge cases not represented before production deployment to identify performance degradation over time. You can track key metrics through operational dashboards, implement alert thresholds, automate anomaly detection process, and establish feedback loops. When the issues are detected, you can start model retraining, refine context engineering, and align with your ultimate business objectives.\n\nConclusion\nAs AI systems become increasingly complex, the importance of a thorough AI agent evaluation approach cannot be overstated. Through holistic evaluation across quality, performance, responsibility, and cost dimensions, in addition to continuous production monitoring and human-in-the-loop validation, the full lifecycle of agentic AI deployment from development to production can be addressed. You can learn from the presented examples, best practices, and lessons learned in this post—many of which are available in Amazon Bedrock AgentCore Evaluations—to accelerate your own agentic AI initiatives while avoiding common pitfalls in evaluation design and implementation.\n\nAbout the authors\n\nYunfei Bai\nYunfei Bai is a Principal Applied AI Architect at AWS. With a background in AI/ML, data science, and analytics, Yunfei helps customers adopt AWS services to deliver business results. He designs AI/ML and data analytics solutions that overcome complex technical challenges and drive strategic objectives. Yunfei has a PhD in Electronic and Electrical Engineering. Outside of work, Yunfei enjoys reading and music.\n\nWinnie Xiong\nWinnie Xiong is a Senior Technical Product Manager on the Amazon’s Benchmarking team. She partners with engineers and scientists to build AI and data solutions that solve complex business challenges for Amazon teams. Her expertise span across model evaluation, agent evaluation, and data management.\n\nAllie Colin\nAllie Colin is a Head of Product and Science at Amazon’s Benchmarking team. She leads a team of scientists and product managers building tools that help Amazonians test their products for quality through the lens of real customer experiences. Previously, she worked at MicroStrategy as Chief of Staff to the CTO, as well as at Deutsche Bank and Northwestern Mutual. Outside of work, Allie is a mom of four who loves the nightly comedy show they put on and enjoys anything that gets her outdoors—hiking, swimming, and traveling.\n\nKashif Imran\nKashif Imran is a seasoned engineering and product leader with deep expertise in AI/ML, cloud architecture, and large-scale distributed systems. With a decade of experience at AWS, Kashif has driven innovation across cloud and AI technologies. Currently a Senior Manager at Amazon Prime Video, he leads AI-native engineering teams building scalable agentic AI solutions to drive business transformation.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "1bfca55921cf45a8",
    "title": "Project Silica’s advances in glass storage technology",
    "url": "https://www.microsoft.com/en-us/research/blog/project-silicas-advances-in-glass-storage-technology/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-18T16:11:45Z",
    "summary": "Project Silica introduces new techniques for encoding data in borosilicate glass, as described in the journal Nature. These advances lower media cost and simplify writing and reading systems while supporting 10,000-year data preservation.\nThe post Project Silica’s advances in glass storage technology appeared first on Microsoft Research .",
    "content": "At a glance\n\nMicrosoft Research publishes breakthrough in  Nature  on glass-based data storage that could preserve information for 10,000 years. \n\nNew technique extends technology from expensive fused silica to ordinary borosilicate glass found in kitchen cookware. \n\nInnovations enable faster parallel writing, simplified readers (one camera instead of three), and easier manufacturing. \n\nPhase voxel method requires only a single laser pulse, significantly reducing complexity and cost.\n\nLong-term preservation of digital information has long challenged archivists and datacenters, as magnetic tapes and hard drives degrade within decades. Existing archival storage solutions have limited media lifespans that make them less than ideal for preserving information for future generations.\n\nNow, we are excited to report significant progress on Project Silica (opens in new tab) , our effort to encode data in glass using femtosecond lasers, a technology that could preserve information for 10,000 years. Glass is a permanent data storage material that is resistant to water, heat, and dust.\n\nIn findings published in Nature (opens in new tab) , we describe a breakthrough that extends the technology beyond expensive fused silica to ordinary borosilicate glass. A readily available and lower-cost medium, this is the same material found in kitchen cookware and oven doors. This advance addresses key barriers to commercialization: cost and availability of storage media. We have unlocked the science for parallel high-speed writing and developed a technique to permit accelerated aging tests on the written glass, suggesting that the data should remain intact for at least 10,000 years.\n\nStoring data inside glass with femtosecond (opens in new tab) laser pulses is one of the few technologies on the horizon with the potential for durable, immutable, and long-lived storage. Although we have been leading innovation in this type of storage for years , prior to this research the technique only worked with pure fused silica glass, a type of glass that is relatively difficult to manufacture and available from only a few sources.\n\nIn the paper, we show how data can be stored in borosilicate glass. The new technique stores hundreds of layers of data in glass only 2mm thin, as with previous methods, but with important improvements. The reader for the glass now needs only one camera, not three or four, reducing cost and size. In addition, the writing devices require fewer parts, making them easier to manufacture and calibrate, and enabling them to encode data more quickly.\n\nvideo series\n\nOn Second Thought\n\nA video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.\n\nExplore the series\n\nOpens in a new tab\n\nKey scientific discoveries\n\nThe Nature paper details several key new scientific discoveries:\n\nAdvances in birefringent voxel (opens in new tab) writing : For the previous type of data storage in fused silica glass using birefringent (i.e., polarization) voxels, we developed a technique to reduce the number of pulses used to form the voxel from many to only two, critically showing that the polarization of the first pulse is not important to the polarization of the voxel formed. We further developed this to enable pseudo-single-pulse writing, in which a single pulse can be split after its polarization is set to simultaneously form the first pulse for one voxel (where the polarization doesn’t matter) and the second pulse of another (where the set polarization is essential). We demonstrated how to use this pseudo-single-pulse writing to enable fast writing with beam scanning across the media.\n\nPhase voxels, a new storage method : We invented a new type of data storage in glass called phase voxels, in which the phase change of the glass is modified instead of its polarization, showing that only a single pulse is necessary to make a phase voxel. We demonstrated that these phase voxels can also be formed in borosilicate glass and devised a technique to read the phase information from phase voxels encoded in this material. We showed that the much higher levels of three-dimensional inter-symbol interference in phase voxels can be mitigated with a machine learning classification model.\n\nParallel writing capabilities : By combining a mathematical model of pre-heating and post-heating within the glass with the invention of a multi-beam delivery system, we showed that many data voxels can be written in proximity in the glass at the same time, significantly increasing writing speed. We explained a method for using light emissions (a side effect of voxel formation) for both static calibration and dynamic control to fully support automatic writing operations.\n\nOptimization and longevity testing : We developed a new way to optimize symbol encodings using machine learning and a better way to understand the tradeoff between error rates, error protection, and error recovery when evaluating new digital storage systems. We also created a new nondestructive optical method (opens in new tab) to identify the aging of data storage voxels within the glass, using this and standard accelerated aging techniques to support data lasting 10,000 years. We extended the industry standard Gray codes to apply to nonpower-of-two numbers of symbols.\n\nSkip slideshow for:\n\nPrevious slide\n\nPrevious slide\n\nA piece of Project Silica media written with data.\n\nA research-grade Writer used to set the record for high speed data writing into glass.\n\nA research-grade Reader for retrieving data from glass.\n\nClose up of Writer showing high-speed multi-beam data encoding on laser pulses.\n\nEnd of slideshow for:\n\nDemonstrating the technology\n\nAs a research initiative, Project Silica has demonstrated these advances through several proofs of concept, including storing Warner Bros.’ “Superman” movie on quartz glass (opens in new tab) , partnering with Global Music Vault (opens in new tab) to preserve music under ice for 10,000 years (opens in new tab) , and working with students on a “Golden Record 2.0” project (opens in new tab) , a digitally curated archive of images, sounds, music, and spoken language, crowdsourced to represent and preserve humanity’s diversity for millennia.\n\nLooking ahead\n\nThe research phase is now complete, and we are continuing to consider learnings from Project Silica as we explore the ongoing need for sustainable, long-term preservation of digital information. We have added this paper to our published works so that others can build on them.\n\nRelated work\n\nProject Silica has made scientific advances across multiple areas beyond laser direct writing (LDW) in glass, including archival storage systems design, archival workload analysis, datacenter robotics, erasure coding, free-space optical components, and machine learning-based methods for symbol decoding in storage systems. Many of these innovations were described in our ACM Transactions on Storage publication (opens in new tab) in 2025.\nOpens in a new tab The post Project Silica’s advances in glass storage technology appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]