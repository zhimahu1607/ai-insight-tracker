[
  {
    "id": "14d0def2a071ab0d",
    "title": "Evaluating generative AI models with Amazon Nova LLM-as-a-Judge on Amazon SageMaker AI",
    "url": "https://aws.amazon.com/blogs/machine-learning/evaluating-generative-ai-models-with-amazon-nova-llm-as-a-judge-on-amazon-sagemaker-ai/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-30T21:07:34Z",
    "summary": "Evaluating the performance of large language models (LLMs) goes beyond statistical metrics like perplexity or bilingual evaluation understudy (BLEU) scores. For most real-world generative AI scenarios, it’s crucial to understand whether a model is producing better outputs than a baseline or an earlier iteration. This is especially important for applications such as summarization, content generation, […]",
    "content": "Evaluating the performance of large language models (LLMs) goes beyond statistical metrics like perplexity or bilingual evaluation understudy (BLEU) scores. For most real-world generative AI scenarios, it’s crucial to understand whether a model is producing better outputs than a baseline or an earlier iteration. This is especially important for applications such as summarization, content generation, or intelligent agents where subjective judgments and nuanced correctness play a central role.\nAs organizations deepen their deployment of these models in production, we’re experiencing an increasing demand from customers who want to systematically assess model quality beyond traditional evaluation methods. Current approaches like accuracy measurements and rule-based evaluations, although helpful, can’t fully address these nuanced assessment needs, particularly when tasks require subjective judgments, contextual understanding, or alignment with specific business requirements. To bridge this gap, LLM-as-a-judge has emerged as a promising approach, using the reasoning capabilities of LLMs to evaluate other models more flexibly and at scale.\nToday, we’re excited to introduce a comprehensive approach to model evaluation through the Amazon Nova LLM-as-a-Judge capability on Amazon SageMaker AI , a fully managed Amazon Web Services (AWS) service to build, train, and deploy machine learning (ML) models at scale. Amazon Nova LLM-as-a-Judge is designed to deliver robust, unbiased assessments of generative AI outputs across model families. Nova LLM-as-a-Judge is available as optimized workflows on SageMaker AI, and with it, you can start evaluating model performance against your specific use cases in minutes. Unlike many evaluators that exhibit architectural bias, Nova LLM-as-a-Judge has been rigorously validated to remain impartial and has achieved leading performance on key judge benchmarks while closely reflecting human preferences. With its exceptional accuracy and minimal bias, it sets a new standard for credible, production-grade LLM evaluation.\nNova LLM-as-a-Judge capability provides pairwise comparisons between model iterations, so you can make data-driven decisions about model improvements with confidence.\nHow Nova LLM-as-a-Judge was trained\nNova LLM-as-a-Judge was built through a multistep training process comprising supervised training and reinforcement learning stages that used public datasets annotated with human preferences. For the proprietary component, multiple annotators independently evaluated thousands of examples by comparing pairs of different LLM responses to the same prompt. To verify consistency and fairness, all annotations underwent rigorous quality checks, with final judgments calibrated to reflect broad human consensus rather than an individual viewpoint.\nThe training data was designed to be both diverse and representative. Prompts spanned a wide range of categories, including real-world knowledge, creativity, coding, mathematics, specialized domains, and toxicity, so the model could evaluate outputs across many real-world scenarios. Training data included data from over 90 languages and is primarily composed of English, Russian, Chinese, German, Japanese, and Italian.Importantly, an internal bias study evaluating over 10,000 human-preference judgments against 75 third-party models confirmed that Amazon Nova LLM-as-a-Judge shows only a 3% aggregate bias relative to human annotations. Although this is a significant achievement in reducing systematic bias, we still recommend occasional spot checks to validate critical comparisons.\nIn the following figure, you can see how the Nova LLM-as-a-Judge bias compares to human preferences when evaluating Amazon Nova outputs compared to outputs from other models. Here, bias is measured as the difference between the judge’s preference and human preference across thousands of examples. A positive value indicates the judge slightly favors Amazon Nova models, and a negative value indicates the opposite. To quantify the reliability of these estimates, 95% confidence intervals were computed using the standard error for the difference of proportions, assuming independent binomial distributions.\n\nAmazon Nova LLM-as-a-Judge achieves advanced performance among evaluation models, demonstrating strong alignment with human judgments across a range of tasks. For example, it scores 45% accuracy on JudgeBench (compared to 42% for Meta J1 8B) and 68% on PPE (versus 60% for Meta J1 8B). The data from Meta’s J1 8B was pulled from Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning .\nThese results highlight the strength of Amazon Nova LLM-as-a-Judge in chatbot-related evaluations, as shown in the PPE benchmark. Our benchmarking follows current best practices, reporting reconciled results for positionally swapped responses on JudgeBench, CodeUltraFeedback, Eval Bias, and LLMBar, while using single-pass results for PPE.\n\nModel\nEval Bias\nJudge Bench\nLLM Bar\nPPE\nCodeUltraFeedback\n\nNova LLM-as-a-Judge\n0.76\n0.45\n0.67\n0.68\n0.64\n\nMeta J1 8B\n–\n0.42\n–\n0.60\n–\n\nNova Micro\n0.56\n0.37\n0.55\n0.6\n–\n\nIn this post, we present a streamlined approach to implementing Amazon Nova LLM-as-a-Judge evaluations using SageMaker AI, interpreting the resulting metrics, and applying this process to improve your generative AI applications.\nOverview of the evaluation workflow\nThe evaluation process starts by preparing a dataset in which each example includes a prompt and two alternative model outputs. The JSONL format looks like this:\n\n{\n\"prompt\":\"Explain photosynthesis.\",\n\"response_A\":\"Answer A...\",\n\"response_B\":\"Answer B...\"\n}\n{\n\"prompt\":\"Summarize the article.\",\n\"response_A\":\"Answer A...\",\n\"response_B\":\"Answer B...\"\n}\n\nAfter preparing this dataset, you use the given SageMaker evaluation recipe , which configures the evaluation strategy, specifies which model to use as the judge, and defines the inference settings such as temperature and top_p .\nThe evaluation runs inside a SageMaker training job using pre-built Amazon Nova containers. SageMaker AI provisions compute resources, orchestrates the evaluation, and writes the output metrics and visualizations to Amazon Simple Storage Service (Amazon S3).\nWhen it’s complete, you can download and analyze the results, which include preference distributions, win rates, and confidence intervals.\nUnderstanding how Amazon Nova LLM-as-a-Judge works\nThe Amazon Nova LLM-as-a-Judge uses an evaluation method called binary overall preference judge . The binary overall preference judge is a method where a language model compares two outputs side by side and picks the better one or declares a tie. For each example, it produces a clear preference. When you aggregate these judgments over many samples, you get metrics like win rate and confidence intervals. This approach uses the model’s own reasoning to assess qualities like relevance and clarity in a straightforward, consistent way.\n\nThis judge model is meant to provide low-latency general overall preferences in situations where granular feedback isn’t necessary\nThe output of this model is one of [[A>B]] or [[B>A]]\nUse cases for this model are primarily those where automated, low-latency, general pairwise preferences are required, such as automated scoring for checkpoint selection in training pipelines\n\nUnderstanding Amazon Nova LLM-as-a-Judge evaluation metrics\nWhen using the Amazon Nova LLM-as-a-Judge framework to compare outputs from two language models, SageMaker AI produces a comprehensive set of quantitative metrics. You can use these metrics to assess which model performs better and how reliable the evaluation is. The results fall into three main categories: core preference metrics, statistical confidence metrics, and standard error metrics.\nThe core preference metrics report how often each model’s outputs were preferred by the judge model. The a_scores metric counts the number of examples where Model A was favored, and b_scores counts cases where Model B was chosen as better. The ties metric captures instances in which the judge model rated both responses equally or couldn’t identify a clear preference. The inference_error metric counts cases where the judge couldn’t generate a valid judgment due to malformed data or internal errors.\nThe statistical confidence metrics quantify how likely it is that the observed preferences reflect true differences in model quality rather than random variation. The winrate reports the proportion of all valid comparisons in which Model B was preferred. The lower_rate and upper_rate define the lower and upper bounds of the 95% confidence interval for this win rate. For example, a winrate of 0.75 with a confidence interval between 0.60 and 0.85 suggests that, even accounting for uncertainty, Model B is consistently favored over Model A. The score field often matches the count of Model B wins but can also be customized for more complex evaluation strategies.\nThe standard error metrics provide an estimate of the statistical uncertainty in each count. These include a_scores_stderr , b_scores_stderr , ties_stderr , inference_error_stderr , and score_stderr . Smaller standard error values indicate more reliable results. Larger values can point to a need for additional evaluation data or more consistent prompt engineering.\nInterpreting these metrics requires attention to both the observed preferences and the confidence intervals:\n\nIf the winrate is substantially above 0.5 and the confidence interval doesn’t include 0.5, Model B is statistically favored over Model A.\nConversely, if the winrate is below 0.5 and the confidence interval is fully below 0.5, Model A is preferred.\nWhen the confidence interval overlaps 0.5, the results are inconclusive and further evaluation is recommended.\nHigh values in inference_error or large standard errors suggest there might have been issues in the evaluation process, such as inconsistencies in prompt formatting or insufficient sample size.\n\nThe following is an example metrics output from an evaluation run:\n\n{\n\"a_scores\": 16.0,\n\"a_scores_stderr\": 0.03,\n\"b_scores\": 10.0,\n\"b_scores_stderr\": 0.09,\n\"ties\": 0.0,\n\"ties_stderr\": 0.0,\n\"inference_error\": 0.0,\n\"inference_error_stderr\": 0.0,\n\"score\": 10.0,\n\"score_stderr\": 0.09,\n\"winrate\": 0.38,\n\"lower_rate\": 0.23,\n\"upper_rate\": 0.56\n}\n\nIn this example, Model A was preferred 16 times, Model B was preferred 10 times, and there were no ties or inference errors. The winrate of 0.38 indicates that Model B was preferred in 38% of cases, with a 95% confidence interval ranging from 23% to 56%. Because the interval includes 0.5, this outcome suggests the evaluation was inconclusive, and additional data might be needed to clarify which model performs better overall.\nThese metrics, automatically generated as part of the evaluation process, provide a rigorous statistical foundation for comparing models and making data-driven decisions about which one to deploy.\nSolution overview\nThis solution demonstrates how to evaluate generative AI models on Amazon SageMaker AI using the Nova LLM-as-a-Judge capability. The provided Python code guides you through the entire workflow.\nFirst, it prepares a dataset by sampling questions from SQuAD and generating candidate responses from Qwen2.5 and Anthropic’s Claude 3.7 . These outputs are saved in a JSONL file containing the prompt and both responses.\nWe accessed Anthropic’s Claude 3.7 Sonnet in Amazon Bedrock using the bedrock-runtime client. We accessed Qwen2.5 1.5B using a SageMaker hosted Hugging Face endpoint .\nNext, a PyTorch Estimator launches an evaluation job using an Amazon Nova LLM-as-a-Judge recipe. The job runs on GPU instances such as ml.g5.12xlarge and produces evaluation metrics, including win rates, confidence intervals, and preference counts. Results are saved to Amazon S3 for analysis.\nFinally, a visualization function renders charts and tables, summarizing which model was preferred, how strong the preference was, and how reliable the estimates are. Through this end-to-end approach, you can assess improvements, track regressions, and make data-driven decisions about deploying generative models—all without manual annotation.\nPrerequisites\nYou need to complete the following prerequisites before you can run the notebook:\n\nMake the following quota increase requests for SageMaker AI. For this use case, you need to request a minimum of 1 g5.12xlarge instance. On the Service Quotas console, request the following SageMaker AI quotas, 1 G5 instances (g5.12xlarge) for training job usage\n(Optional) You can create an Amazon SageMaker Studio domain (refer to Use quick setup for Amazon SageMaker AI ) to access Jupyter notebooks with the preceding role. (You can use JupyterLab in your local setup, too.)\n\nCreate an AWS Identity and Access Management (IAM) role with managed policies AmazonSageMakerFullAccess , AmazonS3FullAccess ,   and AmazonBedrockFullAccess to give required access to SageMaker AI and Amazon Bedrock to run the examples.\nAssign as trust relationship to your IAM role the following policy:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Service\": [\n                    \"bedrock.amazonaws.com\",\n                    \"sagemaker.amazonaws.com\"\n                ]\n            },\n            \"Action\": \"sts:AssumeRole\"\n        }\n    ]\n}\n\nClone the GitHub repository with the assets for this deployment. This repository consists of a notebook that references training assets:\n\ngit clone https://github.com/aws-samples/amazon-nova-samples.git\ncd customization/SageMakerTrainingJobs/Amazon-Nova-LLM-As-A-Judge/\n\nNext, run the notebook Nova Amazon-Nova-LLM-as-a-Judge-Sagemaker-AI.ipynb to start using the Amazon Nova LLM-as-a-Judge implementation on Amazon SageMaker AI.\nModel setup\nTo conduct an Amazon Nova LLM-as-a-Judge evaluation, you need to generate outputs from the candidate models you want to compare. In this project, we used two different approaches: deploying a Qwen2.5 1.5B model on Amazon SageMaker and invoking Anthropic’s Claude 3.7 Sonnet model in Amazon Bedrock. First, we deployed Qwen2.5 1.5B, an open-weight multilingual language model, on a dedicated SageMaker endpoint. This was achieved by using the HuggingFaceModel deployment interface. To deploy the Qwen2.5 1.5B model, we provided a convenient script for you to invoke: python3 deploy_sm_model.py\nWhen it’s deployed, inference can be performed using a helper function wrapping the SageMaker predictor API:\n\n# Initialize the predictor once\npredictor = HuggingFacePredictor(endpoint_name=\"qwen25-<endpoint_name_here>\")\ndef generate_with_qwen25(prompt: str, max_tokens: int = 500, temperature: float = 0.9) -> str:\n\"\"\"\nSends a prompt to the deployed Qwen2.5 model on SageMaker and returns the generated response.\nArgs:\nprompt (str): The input prompt/question to send to the model.\nmax_tokens (int): Maximum number of tokens to generate.\ntemperature (float): Sampling temperature for generation.\nReturns:\nstr: The model-generated text.\n\"\"\"\nresponse = predictor.predict({\n\"inputs\": prompt,\n\"parameters\": {\n\"max_new_tokens\": max_tokens,\n\"temperature\": temperature\n}\n})\nreturn response[0][\"generated_text\"]\nanswer = generate_with_qwen25(\"What is the Grotto at Notre Dame?\")\nprint(answer)\n\nIn parallel, we integrated Anthropic’s Claude 3.7 Sonnet model in Amazon Bedrock. Amazon Bedrock provides a managed API layer for accessing proprietary foundation models (FMs) without managing infrastructure. The Claude generation function used the bedrock-runtime AWS SDK for Python (Boto3) client, which accepted a user prompt and returned the model’s text completion:\n\n# Initialize Bedrock client once\nbedrock = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n# (Claude 3.7 Sonnet) model ID via Bedrock\nMODEL_ID = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\ndef generate_with_claude4(prompt: str, max_tokens: int = 512, temperature: float = 0.7, top_p: float = 0.9) -> str:\n\"\"\"\nSends a prompt to the Claude 4-tier model via Amazon Bedrock and returns the generated response.\nArgs:\nprompt (str): The user message or input prompt.\nmax_tokens (int): Maximum number of tokens to generate.\ntemperature (float): Sampling temperature for generation.\ntop_p (float): Top-p nucleus sampling.\nReturns:\nstr: The text content generated by Claude.\n\"\"\"\npayload = {\n\"anthropic_version\": \"bedrock-2023-05-31\",\n\"messages\": [{\"role\": \"user\", \"content\": prompt}],\n\"max_tokens\": max_tokens,\n\"temperature\": temperature,\n\"top_p\": top_p\n}\nresponse = bedrock.invoke_model(\nmodelId=MODEL_ID,\nbody=json.dumps(payload),\ncontentType=\"application/json\",\naccept=\"application/json\"\n)\nresponse_body = json.loads(response['body'].read())\nreturn response_body[\"content\"][0][\"text\"]\nanswer = generate_with_claude4(\"What is the Grotto at Notre Dame?\")\nprint(answer)\n\nWhen you have both functions generated and tested, you can move on to creating the evaluation data for the Nova LLM-as-a-Judge.\nPrepare the dataset\nTo create a realistic evaluation dataset for comparing the Qwen and Claude models, we used the Stanford Question Answering Dataset ( SQuAD ), a widely adopted benchmark in natural language understanding distributed under the CC BY-SA 4.0 license. SQuAD consists of thousands of crowd-sourced question-answer pairs covering a diverse range of Wikipedia articles. By sampling from this dataset, we made sure that our evaluation prompts reflected high-quality, factual question-answering tasks representative of real-world applications.\nWe began by loading a small subset of examples to keep the workflow fast and reproducible. Specifically, we used the Hugging Face datasets library to download and load the first 20 examples from the SQuAD training split:\n\nfrom datasets import load_dataset\nsquad = load_dataset(\"squad\", split=\"train[:20]\")\n\nThis command retrieves a slice of the full dataset, containing 20 entries with structured fields including context, question, and answers. To verify the contents and inspect an example, we printed out a sample question and its ground truth answer:\n\nprint(squad[3][\"question\"])\nprint(squad[3][\"answers\"][\"text\"][0])\n\nFor the evaluation set, we selected the first six questions from this subset:\nquestions = [squad[i][\"question\"] for i in range(6)]\nGenerate the Amazon Nova LLM-as-a-Judge evaluation dataset\nAfter preparing a set of evaluation questions from SQuAD, we generated outputs from both models and assembled them into a structured dataset to be used by the Amazon Nova LLM-as-a-Judge workflow. This dataset serves as the core input for SageMaker AI evaluation recipes. To do this, we iterated over each question prompt and invoked the two generation functions defined earlier:\n\ngenerate_with_qwen25() for completions from the Qwen2.5 model deployed on SageMaker\ngenerate_with_claude() for completions from Anthropic’s Claude 3.7 Sonnet in Amazon Bedrock\n\nFor each prompt, the workflow attempted to generate a response from each model. If a generation call failed due to an API error, timeout, or other issue, the system captured the exception and stored a clear error message indicating the failure. This made sure that the evaluation process could proceed gracefully even in the presence of transient errors:\n\nimport json\noutput_path = \"llm_judge.jsonl\"\nwith open(output_path, \"w\") as f:\nfor q in questions:\ntry:\nresponse_a = generate_with_qwen25(q)\nexcept Exception as e:\nresponse_a = f\"[Qwen2.5 generation failed: {e}]\"\n\ntry:\nresponse_b = generate_with_claude4(q)\nexcept Exception as e:\nresponse_b = f\"[Claude 3.7 generation failed: {e}]\"\nrow = {\n\"prompt\": q,\n\"response_A\": response_a,\n\"response_B\": response_b\n}\nf.write(json.dumps(row) + \"\\n\")\nprint(f\"JSONL file created at: {output_path}\")\n\nThis workflow produced a JSON Lines file named llm_judge.jsonl . Each line contains a single evaluation record structured as follows:\n\n{\n\"prompt\": \"What is the capital of France?\",\n\"response_A\": \"The capital of France is Paris.\",\n\"response_B\": \"Paris is the capital city of France.\"\n}\n\nThen, upload this llm_judge.jsonl to an S3 bucket that you’ve predefined:\n\nupload_to_s3(\n\"llm_judge.jsonl\",\n\"s3://<YOUR_BUCKET_NAME>/datasets/byo-datasets-dev/custom-llm-judge/llm_judge.jsonl\"\n)\n\nLaunching the Nova LLM-as-a-Judge evaluation job\nAfter preparing the dataset and creating the evaluation recipe, the final step is to launch the SageMaker training job that performs the Amazon Nova LLM-as-a-Judge evaluation. In this workflow, the training job acts as a fully managed, self-contained process that loads the model, processes the dataset, and generates evaluation metrics in your designated Amazon S3 location.\nWe use the PyTorch estimator class from the SageMaker Python SDK to encapsulate the configuration for the evaluation run. The estimator defines the compute resources, the container image, the evaluation recipe, and the output paths for storing results:\n\nestimator = PyTorch(\noutput_path=output_s3_uri,\nbase_job_name=job_name,\nrole=role,\ninstance_type=instance_type,\ntraining_recipe=recipe_path,\nsagemaker_session=sagemaker_session,\nimage_uri=image_uri,\ndisable_profiler=True,\ndebugger_hook_config=False,\n)\n\nWhen the estimator is configured, you initiate the evaluation job using the fit() method. This call submits the job to the SageMaker control plane, provisions the compute cluster, and begins processing the evaluation dataset:\nestimator.fit(inputs={\"train\": evalInput})\nResults from the Amazon Nova LLM-as-a-Judge evaluation job\nThe following graphic illustrates the results of the Amazon Nova LLM-as-a-Judge evaluation job.\n\nTo help practitioners quickly interpret the outcome of a Nova LLM-as-a-Judge evaluation, we created a convenience function that produces a single, comprehensive visualization summarizing key metrics. This function, plot_nova_judge_results , uses Matplotlib and Seaborn to render an image with six panels, each highlighting a different perspective of the evaluation outcome.\nThis function takes the evaluation metrics dictionary—produced when the evaluation job is complete—and generates the following visual components:\n\nScore distribution bar chart – Shows how many times Model A was preferred, how many times Model B was preferred, how many ties occurred, and how often the judge failed to produce a decision (inference errors). This provides an immediate sense of how decisive the evaluation was and whether either model is dominating.\nWin rate with 95% confidence interval – Plots Model B’s overall win rate against Model A, including an error bar reflecting the lower and upper bounds of the 95% confidence interval. A vertical reference line at 50% marks the point of no preference. If the confidence interval doesn’t cross this line, you can conclude the result is statistically significant.\nPreference pie chart – Visually displays the proportion of times Model A, Model B, or neither was preferred. This helps quickly understand preference distribution among the valid judgments.\nA vs. B score comparison bar chart – Compares the raw counts of preferences for each model side by side. A clear label annotates the margin of difference to emphasize which model had more wins.\nWin rate gauge – Depicts the win rate as a semicircular gauge with a needle pointing to Model B’s performance relative to the theoretical 0–100% range. This intuitive visualization helps nontechnical stakeholders understand the win rate at a glance.\nSummary statistics table – Compiles numerical metrics—including total evaluations, error counts, win rate, and confidence intervals—into a compact, clean table. This makes it straightforward to reference the exact numeric values behind the plots.\n\nBecause the function outputs a standard Matplotlib figure, you can quickly save the image, display it in Jupyter notebooks, or embed it in other documentation.\nClean up\nComplete the following steps to clean up your resources:\n\nDelete your Qwen 2.5 1.5B Endpoint\n\nimport boto3\n\n# Create a low-level SageMaker service client.\n\nsagemaker_client = boto3.client('sagemaker', region_name=<region>)\n\n# Delete endpoint\n\nsagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n\nIf you’re using a SageMaker Studio JupyterLab notebook, shut down the JupyterLab notebook instance.\n\nHow you can use this evaluation framework\nThe Amazon Nova LLM-as-a-Judge workflow offers a reliable, repeatable way to compare two language models on your own data. You can integrate this into model selection pipelines to decide which version performs best, or you can schedule it as part of continuous evaluation to catch regressions over time.\nFor teams building agentic or domain-specific systems, this approach provides richer insight than automated metrics alone. Because the entire process runs on SageMaker training jobs, it scales quickly and produces clear visual reports that can be shared with stakeholders.\nConclusion\nThis post demonstrates how Nova LLM-as-a-Judge —a specialized evaluation model available through Amazon SageMaker AI —can be used to systematically measure the relative performance of generative AI systems. The walkthrough shows how to prepare evaluation datasets, launch SageMaker AI training jobs with Nova LLM-as-a-Judge recipes, and interpret the resulting metrics, including win rates and preference distributions. The fully managed SageMaker AI solution simplifies this process, so you can run scalable, repeatable model evaluations that align with human preferences.\nWe recommend starting your LLM evaluation journey by exploring the official Amazon Nova documentation and examples. The AWS AI/ML community offers extensive resources, including workshops and technical guidance, to support your implementation journey.\nTo learn more, visit:\n\nAmazon Nova Documentation\nAmazon Bedrock Nova Overview\nFine-tuning Amazon Nova models\nAmazon Nova customization guide\n\nAbout the authors\nSurya Kari is a Senior Generative AI Data Scientist at AWS, specializing in developing solutions leveraging state-of-the-art foundation models. He has extensive experience working with advanced language models including DeepSeek-R1, the Llama family, and Qwen, focusing on their fine-tuning and optimization. His expertise extends to implementing efficient training pipelines and deployment strategies using AWS SageMaker. He collaborates with customers to design and implement generative AI solutions, helping them navigate model selection, fine-tuning approaches, and deployment strategies to achieve optimal performance for their specific use cases.\nJoel Carlson is a Senior Applied Scientist on the Amazon AGI foundation modeling team. He primarily works on developing novel approaches for improving the LLM-as-a-Judge capability of the Nova family of models.\nSaurabh Sahu is an applied scientist in the Amazon AGI Foundation modeling team. He obtained his PhD in Electrical Engineering from University of Maryland College Park in 2019. He has a background in multi-modal machine learning working on speech recognition, sentiment analysis and audio/video understanding. Currently, his work focuses on developing recipes to improve the performance of LLM-as-a-judge models for various tasks.\nMorteza Ziyadi is an Applied Science Manager at Amazon AGI, where he leads several projects on post-training recipes and (Multimodal) large language models in the Amazon AGI Foundation modeling team. Before joining Amazon AGI, he spent four years at Microsoft Cloud and AI, where he led projects focused on developing natural language-to-code generation models for various products. He has also served as an adjunct faculty at Northeastern University. He earned his PhD from the University of Southern California (USC) in 2017 and has since been actively involved as a workshop organizer, and reviewer for numerous NLP, Computer Vision and machine learning conferences.\nPradeep Natarajan is a Senior Principal Scientist in Amazon AGI Foundation modeling team working on post-training recipes and Multimodal large language models. He has 20+ years of experience in developing and launching multiple large-scale machine learning systems. He has a PhD in Computer Science from University of Southern California.\nMichael Cai is a Software Engineer on the Amazon AGI Customization Team supporting the development of evaluation solutions. He obtained his MS in Computer Science from New York University in 2024. In his spare time he enjoys 3d printing and exploring innovative tech.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊 AWS 在 Amazon SageMaker AI 上推出了 Amazon Nova LLM-as-a-Judge 功能，旨在为生成式 AI 模型提供全面、可靠的评估框架。该功能使用大型语言模型（LLM）作为评判者，通过多步骤训练（包括监督训练和强化学习）和人类偏好标注的数据集，实现了低偏差（总体偏差仅3%）和高准确率（如在 JudgeBench 上达到45%准确率）。它支持成对比较模型输出，生成核心偏好指标、统计置信度指标和标准误差指标，帮助用户快速评估模型性能并做出数据驱动决策。这一创新解决了传统评估方法（如困惑度或 BLEU 分数）无法处理主观判断和业务对齐的局限性，为组织在部署生成式 AI 应用时提供了可扩展、无偏的评估标准，提升了模型质量和选择效率。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon Nova LLM-as-a-Judge",
        "Amazon SageMaker",
        "LLM-as-a-Judge",
        "模型评估",
        "AWS"
      ]
    },
    "analyzed_at": "2026-01-31T03:37:17.447407Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "f3d2a2c12b456462",
    "title": "Simplify ModelOps with Amazon SageMaker AI Projects using Amazon S3-based templates",
    "url": "https://aws.amazon.com/blogs/machine-learning/simplify-modelops-with-amazon-sagemaker-ai-projects-using-amazon-s3-based-templates/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-30T17:18:57Z",
    "summary": "This post explores how you can use Amazon S3-based templates to simplify ModelOps workflows, walk through the key benefits compared to using Service Catalog approaches, and demonstrates how to create a custom ModelOps solution that integrates with GitHub and GitHub Actions—giving your team one-click provisioning of a fully functional ML environment.",
    "content": "Managing ModelOps workflows can be complex and time-consuming. If you’ve struggled with setting up project templates for your data science team, you know that the previous approach using AWS Service Catalog required configuring portfolios, products, and managing complex permissions—adding significant administrative overhead before your team could start building machine learning (ML) pipelines.\nAmazon SageMaker AI Projects now offers an easier path: Amazon S3 based templates . With this new capability, you can store AWS CloudFormation templates directly in Amazon Simple Storage Service (Amazon S3) and manage their entire lifecycle using familiar S3 features such as versioning , lifecycle policies , and S3 Cross-Region replication . This means you can provide your data science team with secure, version-controlled, automated project templates with significantly less overhead.\nThis post explores how you can use Amazon S3-based templates to simplify ModelOps workflows, walk through the key benefits compared to using Service Catalog approaches, and demonstrates how to create a custom ModelOps solution that integrates with GitHub and GitHub Actions—giving your team one-click provisioning of a fully functional ML environment.\nWhat is Amazon SageMaker AI Projects?\nTeams can use Amazon SageMaker AI Projects to create, share, and manage fully configured ModelOps projects. Within this structured environment, you can organize code, data, and experiments—facilitating collaboration and reproducibility.\nEach project can include continuous integration and delivery (CI/CD) pipelines, model registries, deployment configurations, and other ModelOps components, all managed within SageMaker AI. Reusable templates help standardize ModelOps practices by encoding best practices for data processing, model development, training, deployment, and monitoring. The following are popular use-cases you can orchestrate using SageMaker AI Projects:\n\nAutomate ML workflows : Set up CI/CD workflows that automatically build, test, and deploy ML models.\nEnforce governance and compliance : Help your projects follow organizational standards for security, networking, and resource tagging. Consistent tagging practices facilitate accurate cost allocation across teams and projects while streamlining security audits.\nAccelerate time-to-value : Provide pre-configured environments so data scientists focus on ML problems, not infrastructure.\nImprove collaboration : Establish consistent project structures for easier code sharing and reuse.\n\nThe following diagram shows how SageMaker AI Projects offers separate workflows for administrators and ML engineers and data scientists. Where the admins create and manage the ML use-case templates and the ML engineers and data scientists consume the approved templates in self-service fashion.\n\nWhat’s new: Amazon SageMaker AI S3-based project templates\nThe latest update to SageMaker AI Projects introduces the ability for administrators to store and manage ML project templates directly in Amazon S3. S3-based templates are a less complicated and more flexible alternative to the previously required Service Catalog. With this enhancement, AWS CloudFormation templates can be versioned, secured, and efficiently shared across teams using the rich access controls, lifecycle management, and replication features provided by S3. Now, data science teams can launch new ModelOps projects from these S3-backed templates directly within Amazon SageMaker Studio . This helps organizations maintain consistency and compliance at scale with their internal standards.\nWhen you store templates in Amazon S3, they become available in all AWS Regions where SageMaker AI Projects is supported. To share templates across AWS accounts, you can use S3 bucket policies and cross-account access controls. The ability to turn on versioning in S3 provides a complete history of template changes, facilitating audits and rollbacks, while also supplying an immutable record of project template evolution over time. If your teams currently use Service Catalog-based templates, the S3-based approach provides a straightforward migration path. When migrating from Service Catalog to S3, the primary considerations involve provisioning new SageMaker roles to replace Service Catalog-specific roles, updating template references accordingly, uploading templates to S3 with proper tagging, and configuring domain-level tags to point to the template bucket location. For organizations using centralized template repositories, cross-account S3 bucket policies must be established to permit template discovery from consumer accounts, with each consumer account’s SageMaker domain tagged to reference the central bucket. Both S3-based and Service Catalog templates are displayed in separate tabs within the SageMaker AI Projects creation interface, so organizations can introduce S3 templates gradually without disrupting existing workflows during the migration.\nThe S3-based ModelOps projects support custom CloudFormation templates that you create for your organization ML use case. AWS-provided templates (such as the built-in ModelOps project templates) continue to be available exclusively through Service Catalog. Your custom templates must be valid CloudFormation files in YAML format. To start using S3-based templates with SageMaker AI Projects, your SageMaker domain (the collaborative workspace for your ML teams) must include the tag sagemaker:projectS3TemplatesLocation with value s3://<bucket-name>/<prefix>/ . Each template file uploaded to S3 must be tagged with sagemaker:studio-visibility=true to appear in the SageMaker AI Studio Projects console. You will need to grant read access to SageMaker execution roles on the S3 bucket policy and enable CORS onfiguration on the S3 bucket to allow SageMaker AI Projects access to the S3 templates.\nThe following diagram illustrates how S3-based templates integrate with SageMaker AI Projects to enable scalable ModelOps workflows. The setup operates in two separate workflows – one-time configuration by administrators and project launch by ML Engineers / Data Scientists. When ML Engineers / Data Scientists launch a new ModelOps project in SageMaker AI, SageMaker AI launches an AWS CloudFormation stack to provision the resources defined in the template and once the process is complete, you can access all specified resources and the configured CI/CD pipelines in your project.\n\nManaging the lifecycle of launched projects can be accomplished through the SageMaker Studio console where users can navigate to S3 Templates, select a project, and use the Actions dropdown menu to update or delete projects. Project updates can be used to modify existing template parameters or the template URL itself, triggering CloudFormation stack updates that are validated before execution, while project deletion removes all associated CloudFormation resources and configurations. These lifecycle operations can also be performed programmatically using the SageMaker APIs.\nTo demonstrate the power of S3-based templates, let’s look at a real-world scenario where an admin team needs to provide data scientists with a standardized ModelOps workflow that integrates with their existing GitHub repositories.\nUse case: GitHub-integrated MLOps template for enterprise teams\nMany organizations use GitHub as their primary source control system and want to use GitHub Actions for CI/CD while using SageMaker for ML workloads. However, setting up this integration requires configuring multiple AWS services, establishing secure connections, and implementing proper approval workflows—a complex task that can be time-consuming if done manually. Our S3-based template solves this challenge by provisioning a complete ModelOps pipeline that includes, CI/CD orchestration, SageMaker Pipelines components and event-drive automation. The following diagram illustrates the end-to-end workflow provisioned by this ModelOps template.\n\nThis sample ModelOps project with S3-based templates enables fully automated and governed ModelOps workflows. Each ModelOps project includes a GitHub repository pre-configured with Actions workflows and secure AWS CodeConnections for seamless integration. Upon code commits, a SageMaker pipeline is triggered to orchestrate a standardized process involving data preprocessing, model training, evaluation, and registration. For deployment, the system supports automated staging on model approval, with robust validation checks, a manual approval gate for promoting models to production, and a secure, event-driven architecture using AWS Lambda and Amazon EventBridge . Throughout the workflow, governance is supported by SageMaker Model Registry for tracking model versions and lineage, well-defined approval steps, secure credential management using AWS Secrets Manager , and consistent tagging and naming standards for all resources.\nWhen data scientists select this template from SageMaker Studio, they provision a fully functional ModelOps environment through a streamlined process. They push their ML code to GitHub using built-in Git functionality within the Studio integrated development environment (IDE), and the pipeline automatically handles model training, evaluation, and progressive deployment through staging to production—all while maintaining enterprise security and compliance requirements. The complete setup instructions along with the code for this ModelOps template is available in our GitHub repository .\nAfter you follow the instructions in the repository you can find the mlops-github-actions template in the SageMaker AI Projects section in the SageMaker AI Studio console by choosing Projects from the navigation pane and selecting the Organization templates tab and choosing Next , as shown in the following image.\n\nTo launch the ModelOps project, you must enter project-specific details including the Role ARN field. This field should contain the AmazonSageMakerProjectsLaunchRole ARN created during setup, as shown in the following image.\nAs a security best practice, use the AmazonSageMakerProjectsLaunchRole Amazon Resource Name (ARN), not your SageMaker execution role.\nThe AmazonSageMakerProjectsLaunchRole is a provisioning role that acts as an intermediary during the ModelOps project creation. This role contains all the permissions needed to create your project’s infrastructure, including AWS Identity and Access Management (IAM) roles, S3 buckets, AWS CodePipeline , and other AWS resources. By using this dedicated launch role, ML engineers and data scientists can create ModelOps projects without requiring broader permissions in their own accounts. Their personal SageMaker execution role remains limited in scope—they only need permission to assume the launch role itself.\nThis separation of responsibilities is important for maintaining security. Without launch roles, every ML practitioner would need extensive IAM permissions to create code pipelines, AWS CodeBuild projects, S3 buckets, and other AWS resources directly. With launch roles, they only need permission to assume a pre-configured role that handles the provisioning on their behalf, keeping their personal permissions minimal and secure.\n\nEnter your desired project configuration details and choose Next . The template will then create two automated ModelOps workflows—one for model building and one for model deployment—that work together to provide CI/CD for your ML models. The complete ModelOps example can be found in the mlops-github-actions repository.\n\nClean up\nAfter deployment, you will incur costs for the deployed resources. If you don’t intend to continue using the setup, delete the ModelOps project resources to avoid unnecessary charges.\nTo destroy the project, open SageMaker Studio and choose More in the navigation pane and select Projects . Choose the project you want to delete, choose the vertical ellipsis above the upper-right corner of the projects list and choose Delete . Review the information in the Delete project dialog box and select Yes, delete the project to confirm. After deletion, verify that your project no longer appears in the projects list.\nIn addition to deleting a project, which will remove and deprovision the SageMaker AI Project, you also need to manually delete the following components if they’re no longer needed: Git repositories, pipelines, model groups, and endpoints.\nConclusion\nThe Amazon S3-based template provisioning for Amazon SageMaker AI Projects transforms how organizations standardize ML operations. As demonstrated in this post, a single AWS CloudFormation template can provision a complete CI/CD workflow integrating your Git repository (GitHub, Bitbucket, or GitLab), SageMaker Pipelines, and SageMaker Model Registry—providing data science teams with automated workflows while maintaining enterprise governance and security controls. For more information about SageMaker AI Projects and S3-based templates, see ModelOps Automation With SageMaker Projects .\nBy usging S3-based templates in SageMaker AI Projects, administrators can define and govern the ML infrastructure, while ML engineers and data scientists gain access to pre-configured ML environments through self-service provisioning. Explore the GitHub samples repository for popular ModelOps templates and get started today by following the provided instructions. You can also create custom templates tailored to your organization’s specific requirements, security policies, and preferred ML frameworks.\n\nAbout the authors\nChristian Kamwangala is an AI/ML and Generative AI Specialist Solutions Architect at AWS, based in Paris, France. He partners with enterprise customers to architect, optimize, and deploy production-grade AI solutions leveraging the comprehensive AWS machine learning stack . Christian specializes in inference optimization techniques that balance performance, cost, and latency requirements for large-scale deployments. In his spare time, Christian enjoys exploring nature and spending time with family and friends\nSandeep Raveesh is a Generative AI Specialist Solutions Architect at AWS. He works with customer through their AIOps journey across model training, generative AI applications like agents, and scaling generative AI use-cases. He also focuses on go-to-market strategies helping AWS build and align products to solve industry challenges in the generative AI space. You can connect with Sandeep on LinkedIn  to learn about generative AI solutions.\nPaolo Di Francesco is a Senior Solutions Architect at Amazon Web Services (AWS). He holds a PhD in Telecommunications Engineering and has experience in software engineering. He is passionate about machine learning and is currently focusing on using his experience to help customers reach their goals on AWS, in discussions around MLOps. Outside of work, he enjoys playing football and reading.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Amazon AWS 通过 Amazon SageMaker AI Projects 推出了基于 Amazon S3 的模板功能，旨在简化 ModelOps 工作流的管理。这一新功能允许管理员将 AWS CloudFormation 模板直接存储在 Amazon S3 中，利用 S3 的版本控制、生命周期策略和跨区域复制等特性，替代了之前使用 AWS Service Catalog 所需的复杂配置和权限管理，显著减少了管理开销。关键改进包括提供更灵活的模板管理方式，支持自定义模板以匹配组织需求，并集成 GitHub Actions 实现自动化 CI/CD 管道，使数据科学团队能够快速启动标准化项目，提高协作效率、一致性和合规性，从而加速机器学习模型的开发和部署周期。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon SageMaker",
        "Amazon S3",
        "ModelOps",
        "AWS CloudFormation",
        "GitHub Actions"
      ]
    },
    "analyzed_at": "2026-01-31T03:37:18.799473Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "fab491bc48af9872",
    "title": "Scale AI in South Africa using Amazon Bedrock global cross-Region inference with Anthropic Claude 4.5 models",
    "url": "https://aws.amazon.com/blogs/machine-learning/scale-ai-in-south-africa-using-amazon-bedrock-global-cross-region-inference-with-anthropic-claude-4-5-models/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-30T17:12:02Z",
    "summary": "In this post, we walk through how global cross-Region inference routes requests and where your data resides, then show you how to configure the required AWS Identity and Access Management (IAM) permissions and invoke Claude 4.5 models using the global inference profile Amazon Resource Name (ARN). We also cover how to request quota increases for your workload. By the end, you'll have a working implementation of global cross-Region inference in af-south-1.",
    "content": "Building AI applications with Amazon Bedrock presents throughput challenges impacting the scalability of your applications. Global  cross-Region inference  in the af-south-1 AWS Region changes that. You can now invoke models from the Cape Town Region while Amazon Bedrock automatically routes requests to Regions with available capacity. Your applications get consistent response times, your users get reliable experiences, and your  Amazon CloudWatch and AWS CloudTrail logs stay centralized in af-south-1 .\nGlobal cross-Region inference with Anthropic Claude Sonnet 4.5, Haiku 4.5 and Opus 4.5 on Amazon Bedrock in the Cape Town Region ( af-south-1 ) gives you access to the Claude 4.5 model family. South African customers can now use global inference profiles to access these models with enhanced throughput and resilience. Global cross-Region inference routes requests to supported commercial Regions worldwide, optimizing resources and enabling higher throughput—particularly valuable during peak usage times. The feature supports Amazon Bedrock prompt caching , batch inference , Amazon Bedrock Guardrails , Amazon Bedrock Knowledge Bases , and more.\nIn this post, we walk through how global cross-Region inference routes requests and where your data resides, then show you how to configure the required AWS Identity and Access Management (IAM)  permissions and invoke Claude 4.5 models using the global inference profile Amazon Resource Name (ARN). We also cover how to request quota increases for your workload. By the end, you’ll have a working implementation of global cross-Region inference in af-south-1 .\nUnderstanding cross-Region inference\nCross-Region inference is a powerful feature that organizations can use to seamlessly distribute inference processing across multiple Regions. This capability helps you get higher throughput while building at scale, allowing your generative AI applications to remain responsive and reliable even under heavy load.\nAn inference profile in Amazon Bedrock defines a foundation model (FM) and one or more Regions to which it can route model invocation requests. Inference profiles operate on two key concepts:\n\nSource Region – The Region from which the API request is made\nDestination Region – A Region to which Amazon Bedrock can route the request for inference\n\nCross-Region inference operates through the secure AWS network with end-to-end encryption for both data in transit and at rest. When a customer submits an inference request from a source Region, cross-Region inference intelligently routes the request to one of the destination Regions configured for the inference profile over the Amazon Bedrock managed network.\nThe key distinction is that while inference processing (the transient computation) can occur in another Region, data at rest—including logs, knowledge bases, and stored configurations—is designed to remain within your source Region. Requests travel over the  AWS Global Network managed by Bedrock. Data transmitted during cross-Region inference is encrypted and remains within the secure AWS network. Sensitive information is designed to stay protected throughout the inference process, regardless of which Region handles the request, and encrypted responses are returned to your application in your source Region.\nAmazon Bedrock provides two types of cross-Region inference profiles:\n\nGeographic cross-Region inference : Amazon Bedrock automatically selects the optimal commercial Region within a defined geography (US, EU, Australia, and Japan) to process your inference request. (Recommended for use-cases with data residency needs.)\nGlobal cross-Region inference : Global cross-Region inference further enhances cross-Region inference by enabling the routing of inference requests to supported commercial Regions worldwide, optimizing available resources and enabling higher model throughput. (Recommended for use-cases that don’t have data residency needs).\n\nMonitoring and logging\nWith global cross-Region inference from af-south-1 , your requests can be processed anywhere across the AWS global infrastructure. However, Amazon CloudWatch and AWS CloudTrail logs are recorded in af-south-1 , simplifying monitoring by keeping your records in one place.\nData security and compliance\nSecurity and compliance is a shared responsibility between AWS and each customer. Global cross-Region inference is designed to maintain data security. Data transmitted during cross-Region inference is encrypted by Amazon Bedrock and is designed to remain within the secure AWS network. Sensitive information remains protected throughout the inference process, regardless of which Region processes the request. Customers are responsible for configuring their applications and IAM policies appropriately and for evaluating whether global cross-Region inference meets their specific security and compliance requirements. Because global cross-Region inference routes requests to supported commercial Regions worldwide, you should evaluate whether this approach aligns with your regulatory obligations, including the Protection of Personal Information Act (POPIA) and other sector-specific requirements. We recommend consulting with your legal and compliance teams to determine the appropriate approach for your specific use cases.\nImplement global cross-Region inference\nTo use global cross-Region inference with Claude 4.5 models, developers must complete the following key steps:\n\nUse the global inference profile ID – When making API calls to Amazon Bedrock, specify the global Claude 4.5 model’s inference profile ID (for example, global.anthropic.claude-opus-4-5-20251101-v1:0 ). This works with both InvokeModel and Converse APIs.\nConfigure IAM permissions – Grant IAM permissions to access the inference profile and FMs in potential destination Regions. In the next section, we provide more details. You can also read more about prerequisites for inference profiles .\n\nImplementing global cross-Region inference with Claude 4.5 models is straightforward, requiring only a few changes to your existing application code. The following is an example of how to update your code in Python:\n\nimport boto3\nimport json\n\n# Connect to Bedrock from your deployed region\nbedrock = boto3.client('bedrock-runtime', region_name='af-south-1')\n\n# Use global cross-Region inference inference profile for Opus 4.5\nmodel_id = \"global.anthropic.claude-opus-4-5-20251101-v1:0\"\n\n# Make request - Global CRIS automatically routes to optimal AWS Region globally\nresponse = bedrock.converse(\nmessages=[\n{\n\"role\": \"user\",\n\"content\": [{\"text\": \"Explain cloud computing in 2 sentences.\"}]\n}\n],\nmodelId=model_id,\n)\n\nprint(\"Response:\", response['output']['message']['content'][0]['text'])\nprint(\"Token usage:\", response['usage'])\nprint(\"Total tokens:\", response['usage']['totalTokens'])\n\nIf you’re using the Amazon Bedrock InvokeModel API , you can quickly switch to a different model by changing the model ID, as shown in Invoke model code examples .\nIAM policy requirements for global cross-Region inference\nGlobal cross-Region inference requires three specific permissions because the routing mechanism spans multiple scopes: your Regional inference profile, the FM definition in your source Region, and the FM definition at the global level. Without these three, the service cannot resolve the model, validate your access, and route requests across Regions. Access to Anthropic models requires a use case submission before invoking a model. This submission can be completed at either the individual account level or centrally through the organization’s management account. To submit your use case, use the PutUseCaseForModelAccess API or select an Anthropic model from the model catalog in the AWS Management Console for Amazon Bedrock. AWS Marketplace permissions are required to enable models and can be scoped to specific product IDs where supported.\nThe following example IAM policy provides granular control:\n\n{\n\"Version\": \"2012-10-17\",\n\"Statement\": [{\n\"Sid\": \"GrantGlobalCrisInferenceProfileRegionAccess\",\n\"Effect\": \"Allow\",\n\"Action\": \"bedrock:InvokeModel\",\n\"Resource\": [\n\"arn:aws:bedrock:af-south-1:<ACCOUNT>:inference-profile/global.<MODEL NAME>\"\n],\n\"Condition\": {\n\"StringEquals\": {\n\"aws:RequestedRegion\": \"af-south-1\"\n}\n}\n},\n{\n\"Sid\": \"GrantGlobalCrisInferenceProfileInRegionModelAccess\",\n\"Effect\": \"Allow\",\n\"Action\": \"bedrock:InvokeModel\",\n\"Resource\": [\n\"arn:aws:bedrock:af-south-1::foundation-model/<MODEL NAME>\"\n],\n\"Condition\": {\n\"StringEquals\": {\n\"aws:RequestedRegion\": \"af-south-1\",\n\"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:af-south-1:<ACCOUNT>:inference-profile/global.<MODEL NAME>\"\n}\n}\n},\n{\n\"Sid\": \"GrantGlobalCrisInferenceProfileGlobalModelAccess\",\n\"Effect\": \"Allow\",\n\"Action\": \"bedrock:InvokeModel\",\n\"Resource\": [\n\"arn:aws:bedrock:::foundation-model/<MODEL NAME> \"\n],\n\"Condition\": {\n\"StringEquals\": {\n\"aws:RequestedRegion\": \"unspecified\",\n\"bedrock:InferenceProfileArn\": \"arn:aws:bedrock:af-south-1:<ACCOUNT>:inference-profile/global.<MODEL NAME>\"\n}\n}\n}\n]\n}\n\nThe policy comprises three parts. The first statement grants access to the Regional inference profile in af-south-1 , so that users can invoke the specified global cross-Region inference inference profile from South Africa. The second statement provides access to the Regional FM resource, which the service needs to understand which model is being requested within the Regional context. The third statement grants access to the global FM resource, which allows cross-Region routing to function.\nWhen implementing these policies, verify that the three ARNs are included:\n\nThe Regional inference profile ARN follows the pattern arn:aws:bedrock:af-south-1:<ACCOUNT>:inference-profile/global. <MODEL NAME> . This grants access to the global inference profile in your source Region.\nThe Regional FM uses arn:aws:bedrock:af-south-1::foundation-model/ <MODEL NAME> . This grants access to the model definition in af-south-1 .\nThe global FM requires arn:aws:bedrock:::foundation-model/ <MODEL NAME> . This grants access to the model across Regions—note that this ARN intentionally omits the Region and account segments to allow cross-Region routing.\n\nThe global FM ARN has no Region or account specified, which is intentional and required for the cross-Region functionality.\nImportant note on Service Control Policies (SCPs):  If your organization uses Region-specific SCPs, verify that \"aws:RequestedRegion\": \"unspecified\" isn’t included in the deny Regions list, because global cross-Region inference requests use this Region value. Organizations using restrictive SCPs that deny multiple Regions except specifically approved ones will need to explicitly allow this value to enable global cross-Region inference functionality.\nIf your organization determines that global cross-Region inference isn’t appropriate for certain workloads because of data residency or compliance requirements, you can disable it using one of two approaches:\n\nRemove IAM permissions – Remove one or more of the three required IAM policy statements. Because global cross-Region inference requires the three statements to function, removing one of these statements causes requests to the global inference profile to return an access denied error.\nImplement an explicit deny policy – Create a deny policy that specifically targets global cross-Region inference profiles using the condition \"aws:RequestedRegion\": \"unspecified\" . This approach clearly documents your security intent, and the explicit deny takes precedence even if allow policies are accidentally added later.\n\nRequest limit increases for global cross-Region inference\nWhen using global cross-Region inference profiles from af-south-1 , you can request quota increases through the AWS Service Quotas console . Because this is a global limit, requests must be made in your source Region ( af-south-1 ).\nBefore requesting an increase, calculate your required quota using the burndown rate for your model. For Sonnet 4.5 and Haiku 4.5, output tokens have a five-fold burndown rate—each output token consumes 5 tokens from your quota—while input tokens maintain a 1:1 ratio. Your total token consumption per request is:\n\nInput token count + Cache write input tokens + (Output token count x Burndown rate)\n\nTo request a limit increase:\n\nSign in to the AWS Service Quotas console in af-south-1 .\nIn the navigation pane, choose AWS services .\nFind and choose Amazon Bedrock .\nSearch for the specific global cross-Region inference quotas (for example, Global cross-Region model inference tokens per minute for Claude Sonnet 4.5 V1 ).\nSelect the quota and choose Request increase at account level .\nEnter your desired quota value and submit the request.\n\nConclusion\nGlobal cross-Region inference also brings the Claude 4.5 model family to the Cape Town Region, giving you access to the same capabilities available in other Regions. You can build with Sonnet 4.5, Haiku 4.5, and Opus 4.5 from your local Region while the routing infrastructure handles distribution transparently. To get started, update your applications to use the global inference profile ID, configure appropriate IAM permissions, and monitor performance as your applications use the worldwide AWS infrastructure. Visit the Amazon Bedrock console and explore how global cross-Region inference can enhance your AI applications. For more information, see the following resources:\n\nIncrease throughput with cross-Region inference\nSupported Regions and models for inference profiles\nUse an inference profile in model invocation\n\nAbout the authors\nChristian Kamwangala is an AI/ML and Generative AI Specialist Solutions Architect at AWS, where he partners with enterprise customers to architect, optimize, and deploy production-grade AI solutions. His expertise lies in inference optimization—balancing performance, cost, and latency for large-scale deployments. Outside of work, he enjoys exploring nature and spending time with family and friends.\nJarryd Konar is a Senior Cloud Support Engineer at AWS, based in Cape Town, South Africa. He specializes in helping customers architect, optimize, and operate AI/ML and generative AI workloads in the cloud. Jarryd works closely with customers to implement best practices across the AWS AI/ML service portfolio, turning complex technical requirements into practical, scalable solutions. He is passionate about building sustainable and secure AI systems that empower both customers and teams.\nMelanie Li  PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions using state-of-the-art AI/ML tools. She has been actively involved in multiple generative AI initiatives across APJ, harnessing the power of LLMs. Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries.\nSaurabh Trikande  is a Senior Product Manager for Amazon Bedrock and Amazon SageMaker Inference. He is passionate about working with customers and partners, motivated by the goal of democratizing AI. He focuses on core challenges related to deploying complex AI applications, inference with multi-tenant models, cost optimizations, and making the deployment of generative AI models more accessible. In his spare time, Saurabh enjoys hiking, learning about innovative technologies, following TechCrunch, and spending time with his family.\nJared Dean is a Principal AI/ML Solutions Architect at AWS. Jared works with customers across industries to develop machine learning applications that improve efficiency. He is interested in all things AI, technology, and BBQ.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "AWS 在 Amazon Bedrock 平台上为南非开普敦区域（af-south-1）推出全球跨区域推理功能，支持 Anthropic Claude 4.5 模型系列（包括 Sonnet、Haiku、Opus）。该功能通过自动将推理请求路由到全球有容量的 AWS 区域，解决应用程序吞吐量挑战，提供一致的响应时间和可靠用户体验，同时日志和数据存储保留在源区域以简化监控。技术细节包括使用全局推理配置文件 ID、配置 IAM 权限、数据加密传输，并支持提示缓存、批量推理等功能。南非客户可借此访问高性能 AI 模型，增强可扩展性，但需评估数据驻留合规要求如 POPIA。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon Bedrock",
        "Anthropic Claude 4.5",
        "global cross-Region inference",
        "AWS",
        "South Africa"
      ]
    },
    "analyzed_at": "2026-01-31T03:37:30.899736Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]