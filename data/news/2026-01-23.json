[
  {
    "id": "5cb1047b539ca677",
    "title": "How PDI built an enterprise-grade RAG system for AI applications with AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-pdi-built-an-enterprise-grade-rag-system-for-ai-applications-with-aws/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-22T17:11:47Z",
    "summary": "PDI Technologies is a global leader in the convenience retail and petroleum wholesale industries. In this post, we walk through the PDI Intelligence Query (PDIQ) process flow and architecture, focusing on the implementation details and the business outcomes it has helped PDI achieve.",
    "content": "PDI Technologies is a global leader in the convenience retail and petroleum wholesale industries. They help businesses around the globe increase efficiency and profitability by securely connecting their data and operations. With 40 years of experience, PDI Technologies assists customers in all aspects of their business, from understanding consumer behavior to simplifying technology ecosystems across the supply chain.\nEnterprises face a significant challenge of making their knowledge bases accessible, searchable, and usable by AI systems. Internal teams at PDI Technologies were struggling with information scattered across disparate systems including websites, Confluence pages, SharePoint sites, and various other data sources. To address this, PDI Technologies built PDI Intelligence Query (PDIQ), an AI assistant that gives employees access to company knowledge through an easy-to-use chat interface. This solution is powered by a custom Retrieval Augmented Generation (RAG) system, built on Amazon Web Services (AWS) using serverless technologies. Building PDIQ required addressing the following key challenges:\n\nAutomatically extracting content from diverse sources with different authentication requirements\nNeeding the flexibility to select, apply, and interchange the most suitable large language model (LLM) for diverse processing requirements\nProcessing and indexing content for semantic search and contextual retrieval\nCreating a knowledge foundation that enables accurate, relevant AI responses\nContinuously refreshing information through scheduled crawling\nSupporting enterprise-specific context in AI interactions\n\nIn this post, we walk through the PDIQ process flow and architecture, focusing on the implementation details and the business outcomes it has helped PDI achieve.\nSolution architecture\nIn this section, we explore PDIQ’s comprehensive end-to-end design. We examine the data ingestion pipeline from initial processing through storage to user search capabilities, as well as the zero-trust security framework that protects key user personas throughout their platform interactions. The architecture consists of these elements:\n\nScheduler – Amazon EventBridge maintains and executes the crawler scheduler.\nCrawlers – AWS Lambda invokes crawlers that are executed as tasks by Amazon Elastic Container Service (Amazon ECS).\nAmazon DynamoDB – Persists crawler configurations and other metadata such as Amazon Simple Storage Service (Amazon S3) image location and captions.\nAmazon S3 – All source documents are stored in Amazon S3. Amazon S3 events trigger the downstream flow for every object that is created or deleted.\nAmazon Simple Notification Service (Amazon SNS) – Receives notification from Amazon S3 events.\nAmazon Simple Queue Service (Amazon SQS) – Subscribed to Amazon SNS to hold the incoming requests in a queue.\nAWS Lambda – Handles the business logic for chunking, summarizing, and generating vector embeddings.\nAmazon Bedrock – Provides API access to foundation models (FMs) used by PDIQ:\n\nAmazon Nova Lite to generate image caption\nAmazon Nova Micro to generate document summary\nAmazon Titan Text Embeddings V2 to generate vector embeddings\nAmazon Nova Pro to generate responses to user inquiries\n\nAmazon Aurora PostgreSQL-Compatible Edition – Stores vector embeddings.\n\nThe following diagram is the solution architecture.\n\nNext, we review how PDIQ implements a zero-trust security model with role-based access control for two key personas:\n\nAdministrators configure knowledge bases and crawlers through Amazon Cognito user groups integrated with enterprise single sign-on. Crawler credentials are encrypted at rest using AWS Key Management Service (AWS KMS) and only accessible within isolated execution environments.\nEnd users access knowledge bases based on group permissions validated at the application layer. Users can belong to multiple groups (such as human resources or compliance) and switch contexts to query role-appropriate datasets.\n\nProcess flow\nIn this section, we review the end-to-end process flow. We break it down by sections to dive deeper into each step and explain the functionality.\n\nCrawlers\nCrawlers are configured by Administrator to collect data from a variety of sources that PDI relies on. Crawlers hydrate the data into the knowledge base so that this information can be retrieved by end users. PDIQ currently supports the following crawler configurations:\n\nWeb crawler – By using Puppeteer for headless browser automation, the crawler converts HTML web pages to markdown format using turndown . By following the embedded links on the website, the crawler can capture full context and relationships between pages. Additionally, the crawler downloads assets such as PDFs and images while preserving the original reference and offers users configuration options such as rate limiting.\nConfluence crawler – This crawler uses Confluence REST API with authenticated access to extract page content, attachments, and embedded images. It preserves page hierarchy and relationships, handles special Confluence elements such as info boxes, notes, and many more.\nAzure DevOps crawler – PDI uses Azure DevOps to manage its code base, track commits, and maintain project documentation in a centralized repository. PDIQ uses Azure DevOps REST API with OAuth or personal access token (PAT) authentication to extract this information. Azure DevOps crawler preserves project hierarchy, sprint relationships, and backlog structure also maps work item relationships (such as parent/child or linked items), thereby providing a complete view of the dataset.\nSharePoint crawler – It uses Microsoft Graph API with OAuth authentication to extract document libraries, lists, pages, and file content. The crawler processes MS Office documents (Word, Excel, PowerPoint) into searchable text and maintains document version history and permission metadata.\n\nBy building separate crawler configurations, PDIQ offers easy extensibility into the platform to configure additional crawlers on demand. It also offers the flexibility to administrator users to configure the settings for their respective crawlers (such as frequency, depth, or rate limits).\nThe following figure shows the PDIQ UI to configure the knowledge base.\n\nThe following figure shows the PDI UI to configure your crawler (such as Confluence).\n\nThe following figure shows the PDIQ UI to schedule crawlers.\n\nHandling images\nData crawled is stored in Amazon S3 with proper metadata tags. If the source is in HTML format, the task converts the content into markdown ( .md ) files. For these markdown files, there is an additional optimization step performed to replace the images in the document with the Amazon S3 reference location. Key benefits of this approach include:\n\nPDI can use S3 object keys to uniquely reference each image, thereby optimizing the synchronization process to detect changes in source data\nYou can optimize storage by replacing images with captions and avoiding the need to store duplicate images\nIt provides the ability to make the content of the images searchable and relatable to the text content in the document\nSeamlessly inject original images when rendering a response to user inquiry\n\nThe following is a sample markdown file where images are replaced with the S3 file location:\n\n![image-20230113-074652](https:// amzn-s3-demo-bucket.s3.amazonaws.com/kb/123/file/attachments/12133171243_image-20230113-074652.png)\n\nDocument processing\nThis is the most critical step of the process. The key objective of this step is to generate vector embeddings so that they can be used for similarity matching and effective retrieval based on user inquiry. The process follows several steps, starting with image captioning, then document chunking, summary generation, and embedding generation. To caption the images, PDIQ scans the markdown files to locate image tags <image>. For each of these images, PDIQ scans and generates an image caption that explains the content of the image. This caption gets injected back into the markdown file, next to the <image> tag, thereby enriching the document content. This approach offers improved contextual searchability. PDIQ enhances content discovery by embedding insights extracted from images directly into the original markdown files. This approach ensures that image content becomes part of the searchable text, enabling richer and more accurate context retrieval during search and analysis. The approach also saves costs. To avoid unnecessary LLM inference calls for exact same images, PDIQ stores image metadata (file location and generated captions) in Amazon DynamoDB. This step enables efficient reuse of previously generated captions, eliminating the need for repeated caption generation calls to LLM.\nThe following is an example of an image caption prompt:\n\nYou are a professional image captioning assistant. Your task is to provide clear, factual, and objective descriptions of images. Focus on describing visible elements, objects, and scenes in a neutral and appropriate manner.\n\nThe following is a snippet of markdown file that contains the image tag, LLM-generated caption, and the corresponding S3 file location:\n\n![image-20230818-114454: The image displays a security tip notification on a computer screen. The notification is titled \"Security tip\" and advises the user to use generated passwords to keep their accounts safe. The suggested password, \"2m5oFX#g&tLRMhN3,\" is shown in a green box. Below the suggested password, there is a section labeled \"Very Strong,\" indicating the strength of the password. The password length is set to 16 characters, and it includes lowercase letters, uppercase letters, numbers, and symbols. There is also a \"Dismiss\" button to close the notification. Below the password section, there is a link to \"See password history.\" The bottom of the image shows navigation icons for \"Vault,\" \"Generator,\" \"Alerts,\" and \"Account.\" The \"Generator\" icon is highlighted in red.]\n(https:// amzn-s3-demo-bucket.s3.amazonaws.com/kb/ABC/file/attachments/12133171243_image-20230818-114454.png)\n\nNow that markdown files are injected with image captions, the next step is to break the original document into chunks that fit into the context window of the embeddings model. PDIQ uses Amazon Titan Text Embeddings V2 model to generate vectors and stores them in Aurora PostgreSQL-Compatible Serverless. Based on internal accuracy testing and chunking best practices from AWS , PDIQ performs chunking as follows:\n\n70% of the tokens for content\n10% overlap between chunks\n20% for summary tokens\n\nUsing the document chunking logic from the previous step, the document is converted into vector embeddings. The process includes:\n\nCalculate chunk parameters – Determine the size and total number of chunks required for the document based on the 70% calculation.\nGenerate document summary – Use Amazon Nova Lite to create a summary of the entire document, constrained by the 20% token allocation. This summary is reused across all chunks to provide consistent context.\nChunk and prepend summary – Split the document into overlapping chunks (10%), with the summary prepended at the top.\nGenerate embeddings – Use Amazon Titan Text Embeddings V2 to generate vector embeddings for each chunk (summary plus content), which is then stored in the vector store.\n\nBy designing a customized approach to generate a summary section atop of all chunks, PDIQ ensures that when a particular chunk is matched based on similarity search, the LLM has access to the entire summary of the document and not only the chunk that matched. This approach enriches end user experience resulting in an increase of approval rate for accuracy from 60% to 79%.\nThe following is an example of a summarization prompt:\n\nYou are a specialized document summarization assistant with expertise in business and technical content.\n\nYour task is to create concise, information-rich summaries that:\nPreserve all quantifiable data (numbers, percentages, metrics, dates, financial figures)\nHighlight key business terminology and domain-specific concepts\nExtract important entities (people, organizations, products, locations)\nIdentify critical relationships between concepts\nMaintain factual accuracy without adding interpretations\nFocus on extracting information that would be most valuable for:\nAnswering specific business questions\nSupporting data-driven decision making\nEnabling precise information retrieval in a RAG system\nThe summary should be comprehensive yet concise, prioritizing specific facts over general descriptions.\nInclude any tables, lists, or structured data in a format that preserves their relationships.\nEnsure all technical terms, acronyms, and specialized vocabulary are preserved exactly as written.\n\nThe following is an example of summary text, available on each chunk:\n\n### Summary: PLC User Creation Process and Password Reset\n**Document Overview:**\nThis document provides instructions for creating new users and resetting passwords\n**Key Instructions:**\n\n{Shortened for Blog illustration}\n\nThis summary captures the essential steps, requirements, and entities involved in the PLC user creation and password reset process using Jenkins.\n---\n\nChunk 1 has a summary at the top followed by details from the source:\n\n{Summary Text from above}\nThis summary captures the essential steps, requirements, and entities involved in the PLC user creation and password reset process using Jenkins.\n\ntitle: 2. PLC User Creation Process and Password Reset\n\n![image-20230818-114454: The image displays a security tip notification on a computer screen. The notification is titled \"Security tip\" and advises the user to use generated passwords to keep their accounts safe. The suggested password, \"2m5oFX#g&tLRMhN3,\" is shown in a green box. Below the suggested password, there is a section labeled \"Very Strong,\" indicating the strength of the password. The password length is set to 16 characters, and it includes lowercase letters, uppercase letters, numbers, and symbols. There is also a \"Dismiss\" button to close the notification. Below the password section, there is a link to \"See password history.\" The bottom of the image shows navigation icons for \"Vault,\" \"Generator,\" \"Alerts,\" and \"Account.\" The \"Generator\" icon is highlighted in red.](https:// amzn-s3-demo-bucket.s3.amazonaws.com/kb/123/file/attachments/12133171243_image-20230818-114454.png)\n\nChunk 2 has a summary at the top, followed by continuation of details from the source:\n\n{Summary Text from above}\nThis summary captures the essential steps, requirements, and entities involved in the PLC user creation and password reset process using Jenkins.\n---\nMaintains a menu with options such as\n\n![image-20230904-061307: - The generated text has been blocked by our content filters.](https:// amzn-s3-demo-bucket.s3.amazonaws.com/kb/123/file/attachments/12133171243_image-20230904-061307.png)\n\nPDIQ scans each document chunk and generates vector embeddings. This data is stored in Aurora PostgreSQL database with key attributes, including a unique knowledge base ID, corresponding embeddings attribute, original text (summary plus chunk plus image caption), and a JSON binary object that includes metadata fields for extensibility. To keep the knowledge base in sync, PDI implements the following steps:\n\nAdd – These are net new source objects that should be ingested. PDIQ implements the document processing flow described previously.\nUpdate – If PDIQ determines the same object is present, it compares the hash key value from the source with the hash value from the JSON object.\nDelete – If PDIQ determines that a specific source document no longer exists, it triggers a delete operation on the S3 bucket ( s3:ObjectRemoved:* ), which results in a cleanup job, deleting the records corresponding to the key value in the Aurora table.\n\nPDI uses Amazon Nova Pro to retrieve the most relevant document and generates a response by following these key steps:\n\nUsing similarity search, retrieves the most relevant document chunks, which include summary, chunk data, image caption, and image link.\nFor the matching chunk, retrieve the entire document.\nLLM then replaces the image link with the actual image from Amazon S3.\nLLM generates a response based on the data retrieved and the preconfigured system prompt.\n\nThe following is a snippet of system prompt:\n\nSupport assistant specializing in PDI's Logistics(PLC) platform, helping staff research and resolve support cases in Salesforce. You will assist with finding solutions, summarizing case information, and recommending appropriate next steps for resolution.\n\nProfessional, clear, technical when needed while maintaining accessible language.\n\nResolution Process:\nResponse Format template:\nHandle Confidential Information:\n\nOutcomes and next steps\nBy building this customized RAG solution on AWS, PDI realized the following benefits:\n\nFlexible configuration options allow data ingestion at consumer-preferred frequencies.\nScalable design enables future ingestion from additional source systems through easily configurable crawlers.\nSupports crawler configuration using multiple authentication methods, including username and password, secret key-value pairs, and API keys.\nCustomizable metadata fields enable advanced filtering and improve query performance.\nDynamic token management helps PDI intelligently balance tokens between content and summaries, enhancing user responses.\nConsolidates diverse source data formats into a unified layout for streamlined storage and retrieval.\n\nPDIQ provides key business outcomes that include:\n\nImproved efficiency and resolution rates – The tool empowers PDI support teams to resolve customer queries significantly faster, often automating routine issues and providing immediate, precise responses. This has led to less customer waiting on case resolution and more productive agents.\nHigh customer satisfaction and loyalty – By delivering accurate, relevant, and personalized answers grounded in live documentation and company knowledge, PDIQ increased customer satisfaction scores (CSAT), net promoter scores (NPS), and overall loyalty. Customers feel heard and supported, strengthening PDI brand relationships.\nCost reduction – PDIQ handles the bulk of repetitive queries, allowing limited support staff to focus on expert-level cases, which improves productivity and morale. Additionally, PDIQ is built on serverless architecture, which automatically scales while minimizing operational overhead and cost.\nBusiness flexibility – A single platform can serve different business units, who can curate the content by configuring their respective data sources.\nIncremental value – Each new content source adds measurable value without system redesign.\n\nPDI continues to enhance the application with several planned improvements in the pipeline, including:\n\nBuild additional crawler configuration for new data sources (for example, GitHub).\nBuild agentic implementation for PDIQ to be integrated into larger complex business processes.\nEnhanced document understanding with table extraction and structure preservation.\nMultilingual support for global operations.\nImproved relevance ranking with hybrid retrieval techniques.\nAbility to invoke PDIQ based on events (for example, source commits).\n\nConclusion\nPDIQ service has transformed how users access and use enterprise knowledge at PDI Technologies. By using Amazon serverless services, PDIQ can automatically scale with demand, reduce operational overhead, and optimize costs. The solution’s unique approach to document processing, including the dynamic token management and the custom image captioning system, represents significant technical innovation in enterprise RAG systems. The architecture successfully balances performance, cost, and scalability while maintaining security and authentication requirements. As PDI Technologies continue to expand PDIQ’s capabilities, they’re excited to see how this architecture can adapt to new sources, formats, and use cases.\n\nAbout the authors\nSamit Kumbhani is an Amazon Web Services (AWS) Senior Solutions Architect in the New York City area with over 18 years of experience. He currently partners with independent software vendors (ISVs) to build highly scalable, innovative, and secure cloud solutions. Outside of work, Samit enjoys playing cricket, traveling, and biking.\nJhorlin De Armas is an Architect II at PDI Technologies, where he leads the design of AI-driven platforms on Amazon Web Services (AWS). Since joining PDI in 2024, he has architected a compositional AI service that enables configurable assistants, agents, knowledge bases, and guardrails using Amazon Bedrock, Aurora Serverless, AWS Lambda, and DynamoDB. With over 18 years of experience building enterprise software, Jhorlin specializes in cloud-centered architectures, serverless platforms, and AI/ML solutions.\nDavid Mbonu is a Sr. Solutions Architect at Amazon Web Services (AWS), helping horizontal business application ISV customers build and deploy transformational solutions on AWS. David has over 27 years of experience in enterprise solutions architecture and system engineering across software, FinTech, and public cloud companies. His recent interests include AI/ML, data strategy, observability, resiliency, and security. David and his family reside in Sugar Hill, GA.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "PDI Technologies 使用 Amazon Web Services (AWS) 构建了企业级检索增强生成（RAG）系统 PDIQ，以解决内部知识分散于多种源（如网站、Confluence、SharePoint）的挑战。该系统通过自定义爬虫自动提取内容，利用 AWS 无服务器技术和 Amazon Bedrock 的 LLM 模型（如 Nova Pro 和 Titan Text Embeddings）进行文档处理、图像字幕生成和向量嵌入，存储在 Aurora PostgreSQL 中。PDIQ 实现了基于角色的零信任安全模型，支持语义搜索和上下文检索，显著提升了知识访问效率。业务影响包括支持团队解决查询速度加快、客户满意度提高、成本降低，并支持未来扩展多语言和代理集成，展示了 RAG 系统在企业 AI 应用中的创新和可扩展性。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "PDI Technologies",
        "AWS",
        "RAG",
        "Amazon Bedrock",
        "PDIQ"
      ]
    },
    "analyzed_at": "2026-01-23T03:35:44.986261Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "e920e21056dec777",
    "title": "How CLICKFORCE accelerates data-driven advertising with Amazon Bedrock Agents",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-clickforce-accelerates-data-driven-advertising-with-amazon-bedrock-agents/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-22T17:04:04Z",
    "summary": "In this post, we demonstrate how CLICKFORCE used AWS services to build Lumos and transform advertising industry analysis from weeks-long manual work into an automated, one-hour process.",
    "content": "CLICKFORCE is one of leaders in digital advertising services in Taiwan, specializing in data-driven advertising and conversion (D4A – Data for Advertising & Action). With a mission to deliver industry-leading, trend-aligned, and innovative marketing solutions, CLICKFORCE helps brands, agencies, and media partners make smarter advertising decisions.\nHowever, as the advertising industry rapidly evolves, traditional analysis methods and generic AI outputs are no longer sufficient to provide actionable insights. To remain competitive, CLICKFORCE turned to AWS to build Lumos, a next-generation AI-driven marketing analysis solution powered by Amazon Bedrock , Amazon SageMaker AI , Amazon OpenSearch , and AWS Glue .\nIn this post, we demonstrate how CLICKFORCE used AWS services to build Lumos and transform advertising industry analysis from weeks-long manual work into an automated, one-hour process.\nDigital advertising challenges\nBefore adopting Amazon Bedrock, CLICKFORCE faced several roadblocks in building actionable intelligence for digital advertising. Large language models (LLMs) tend to produce generic recommendations rather than actionable industry-specific intelligence. Without an understanding of the advertising environment, these models didn’t have the industry context needed to align their suggestions with actual industry realities.\nAnother significant challenge was the absence of integrated internal datasets, which weakened the reliability of outputs and increased the risk of hallucinated or inaccurate insights. At the same time, marketing teams relied on disconnected tools and technique such as vibe coding, without standardized architectures or workflows, making the processes difficult to maintain and scale.\nPreparing a comprehensive industry analysis report was also a time-consuming process, typically requiring between two and six weeks. The timeline stemmed from multiple labor-intensive stages: one to three days to define objectives and set the research plan, one to four weeks to gather and validate data from different sources, one to two weeks to conduct statistical analysis and build charts, one to two to extract strategic insights, and finally three to seven days to draft and finalize the report. Each stage often required back-and-forth coordination across teams, which further extended the timeline. As a result, marketing strategies were frequently delayed and based more on intuition than timely, data-backed insights.\nSolutions overview\nTo address these challenges, CLICKFORCE built Lumos , an integrated AI-powered industry analysis service, using AWS services.\nThe solution is designed around Amazon Bedrock Agents for contextualized reasoning and Amazon SageMaker AI for fine-tuning Text-to-SQL accuracy. CLICKFORCE chose Amazon Bedrock because it provides managed access to foundation models without the need to build or maintain infrastructure, while also offering agents that can orchestrate multi-step tasks and integrate with enterprise data sources through Knowledge Bases. This allowed the team to ground insights in real, verifiable data, minimize hallucinations, and quickly experiment with different models, while also reducing operational overhead and accelerating time-to-market.\n\nThe first step was to build a unified AI agent using Amazon Bedrock. End-users interact with a chatbot interface that runs on Amazon ECS , developed with Streamlit and fronted by an Application Load Balancer. When a user submits a query, it is routed to an AWS Lambda function that invokes an Amazon Bedrock Agent . The agent retrieves relevant information from a Amazon Bedrock Knowledge Bases , which is built from source documents—such as campaign reports, product descriptions, and industry analysis files—hosted in Amazon S3. These documents are automatically converted into vector embeddings and indexed in Amazon OpenSearch Service. By grounding model responses in this curated document set, CLICKFORCE made sure that outputs were contextualized, reduced hallucinations, and aligned with real-world advertising data.\nNext, CLICKFORCE made the workflows more action-oriented by using Text-to-SQL requests. When queries required data retrieval, the Bedrock Agent generated JSON schemas via the Agent Actions API Schema. These were passed to Lambda Executor functions that translated requests into Text-to-SQL queries. With AWS Glue crawlers continuously updating SQL databases from CSV files in Amazon S3, analysts were able to run precise queries on campaign performance, audience behaviors, and competitive benchmarks.\nFinally, the company improved accuracy by incorporating Amazon SageMaker and MLflow into the development workflow. Initially, CLICKFORCE relied on foundation models for Text-to-SQL translation but found them to be inflexible and often inaccurate. By using SageMaker, the team processed data, evaluated different approaches, and tuned the overall Text-to-SQL pipeline. Once validated, the optimized pipeline was deployed through AWS Lambda functions and integrated back into the agent, making sure that improvements flowed directly into the Lumos application. With MLflow providing experiment tracking and evaluation, the cycle of data processing, pipeline tuning, and deployment became streamlined, allowing Lumos to achieve higher precision in query generation and deliver automated, data-driven marketing reports.\nResults\nThe impact of adopting Amazon Bedrock Agents and SageMaker AI has been transformative for CLICKFORCE. Industry analysis that previously required two to six weeks can now be completed in under one hour, dramatically accelerating decision-making. The company also reduced its reliance on third-party industry research reports, which resulted in a 47 percent reduction in operational costs.\nIn addition to time and cost savings, the Lumos system has extended scalability across roles within the marketing environment. Brand owners, agencies, analysts, marketers, and media partners can now independently generate insights without waiting for centralized analyst teams. This autonomy has led to greater agility across campaigns. Moreover, by grounding outputs in both internal datasets and industry-specific context, Lumos significantly reduced the risk of hallucinations and made sure that insights aligned more closely with industry realities.\n\nUsers can generate industry analysis reports through natural language conversations and iteratively refine the content by continuing the dialogue.\n\nThese visual reports, generated through the Lumos system powered by Amazon Bedrock Agents and SageMaker AI, showcase the platform’s ability to produce comprehensive market intelligence within minutes. The charts illustrate brand sales distribution, retail and e-commerce performance, and demonstrating how AI-driven analytics automate data aggregation, visualization, and insight generation with high precision and efficiency.\nConclusion\nCLICKFORCE’s Lumos system represents a breakthrough in how digital marketing decisions are made. By combining Amazon Bedrock Agents, Amazon SageMaker AI, Amazon OpenSearch Service, and AWS Glue, CLICKFORCE transformed its industry analysis workflow from a slow, manual process into a fast, automated, and reliable system. In this post, we demonstrated how CLICKFORCE used these AWS services to build Lumos and transform advertising industry analysis from weeks-long manual work into an automated, one-hour process.\n\nAbout the Authors\nRay Wang  is a Senior Solutions Architect at AWS. With 12+ years of experience in the backend and consultant, Ray is dedicated to building modern solutions in the cloud, especially in especially in NoSQL, big data, machine learning, and Generative AI. As a hungry go-getter, he passed all 12 AWS certificates to increase the breadth and depth of his technical knowledge. He loves to read and watch sci-fi movies in his spare time.\nShanna Chang  is a Solutions Architect at AWS. She focuses on observability in modern architectures and cloud-native monitoring solutions. Before joining AWS, she was a software engineer.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "CLICKFORCE是台湾数字广告服务领导者，利用AWS服务构建了Lumos AI驱动营销分析系统。该系统基于Amazon Bedrock Agents进行上下文化推理和Amazon SageMaker AI微调Text-to-SQL准确性，通过整合内部数据集和行业文档，实现自动化分析。技术细节涉及使用Amazon OpenSearch索引向量嵌入、AWS Glue更新数据库，以及Streamlit和AWS Lambda构建聊天机器人界面。关键改进包括：行业分析报告时间从2-6周缩短到1小时内，运营成本降低47%，减少幻觉风险，并提高营销决策的可扩展性和效率，使团队能通过自然语言对话快速生成数据驱动洞察。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "CLICKFORCE",
        "Amazon Bedrock",
        "Lumos",
        "Amazon SageMaker",
        "AWS Glue"
      ]
    },
    "analyzed_at": "2026-01-23T03:35:44.076616Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "93ae90489e60d596",
    "title": "Scaling PostgreSQL to power 800 million ChatGPT users",
    "url": "https://openai.com/index/scaling-postgresql",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-22T12:00:00Z",
    "summary": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
    "content": "An inside look at how OpenAI scaled PostgreSQL to millions of queries per second using replicas, caching, rate limiting, and workload isolation.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI 最近分享了如何扩展 PostgreSQL 数据库以支持 ChatGPT 的 8 亿用户。通过采用副本提高读取性能、缓存减少数据库负载、速率限制控制请求流量，以及工作负载隔离确保操作互不干扰，成功将数据库处理能力提升到每秒数百万查询。这一技术方案对于大规模 AI 服务至关重要，确保了高可扩展性和稳定性，为其他类似应用提供了重要参考。",
      "category": "行业",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "PostgreSQL",
        "ChatGPT",
        "副本",
        "缓存"
      ]
    },
    "analyzed_at": "2026-01-23T03:36:13.560823Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "b746527079a49b96",
    "title": "Inside Praktika's conversational approach to language learning",
    "url": "https://openai.com/index/praktika",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-22T05:00:00Z",
    "summary": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
    "content": "How Praktika uses GPT-4.1 and GPT-5.2 to build adaptive AI tutors that personalize lessons, track progress, and help learners achieve real-world language fluency",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "Praktika 利用 OpenAI 的 GPT-4.1 和 GPT-5.2 模型构建自适应 AI 导师系统，专注于语言学习。该系统通过对话方式个性化课程内容，实时跟踪学习者进度，并帮助用户实现真实世界的语言流利度。这一应用的重要性在于整合了先进的大语言模型技术，提供定制化学习体验，可能显著提升语言学习效率。技术细节上，它利用 GPT 模型的自然语言处理能力，实现自适应教学和进度监控。影响方面，展示了 AI 在教育领域的创新应用，推动个性化学习发展，并为未来 AI 辅助工具提供参考。",
      "category": "LLM",
      "sentiment": "positive",
      "keywords": [
        "Praktika",
        "GPT-4.1",
        "GPT-5.2",
        "AI tutors",
        "language learning"
      ]
    },
    "analyzed_at": "2026-01-23T03:35:44.660503Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]