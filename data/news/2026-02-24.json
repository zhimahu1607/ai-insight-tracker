[
  {
    "id": "8661519f5c738e02",
    "title": "Scaling data annotation using vision-language models to power physical AI systems",
    "url": "https://aws.amazon.com/blogs/machine-learning/scaling-data-annotation-using-vision-language-models-to-power-physical-ai-systems/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T23:20:37Z",
    "summary": "In this post, we examine how Bedrock Robotics tackles this challenge. By joining the AWS Physical AI Fellowship, the startup partnered with the AWS Generative AI Innovation Center to apply vision-language models that analyze construction video footage, extract operational details, and generate labeled training datasets at scale, to improve data preparation for autonomous construction equipment.",
    "content": "Critical labor shortages are constraining growth across manufacturing, logistics, construction, and agriculture. The problem is particularly acute in construction: nearly 500,000 positions remain unfilled in the United States, with 40% of the current workforce approaching retirement within the decade. These workforce limitations result in delayed projects, escalating costs, and deferred development plans. To address these constraints, organizations are developing autonomous systems that can perform tasks that fill capacity gaps, extend operational capabilities, and offer the added benefit of around-the-clock productivity.\nBuilding autonomous systems requires large, annotated datasets to train AI models. Effective training determines whether these systems deliver business value. The bottleneck: the high cost of data preparation. Critically, the act of labeling video data—identifying information about equipment, tasks, and the environment—is required to make sure that the data is useful for model training. This step can impede model deployment, which slows down the delivery of AI-powered products and services to customers. For construction companies managing millions of hours of video, manual data preparation and annotation become impractical. Vision-language models (VLMs) help to address this by interpreting images and video, responding to natural language queries, and generating descriptions at a speed and scale that manual processes cannot match, providing a cost-effective alternative.\nIn this post, we examine how Bedrock Robotics tackles this challenge. By joining the AWS Physical AI Fellowship, the startup partnered with the AWS Generative AI Innovation Center to apply vision-language models that analyze construction video footage, extract operational details, and generate labeled training datasets at scale, to improve data preparation for autonomous construction equipment.\nBedrock Robotics: a case study in accelerating autonomous construction\nSince 2024, Bedrock Robotics has been developing autonomous systems for construction equipment. The company’s product, Bedrock Operator, is a retrofit solution that combines hardware with AI models to enable excavators and other machinery to operate with minimal human intervention. These systems can perform tasks like digging, grading, and material handling with centimeter-level precision. Training these models requires massive volumes of video footage capturing equipment, tasks, and the surrounding environment – a highly resource-intensive process that limits scalability.\nVLMs offer a solution by analyzing this image and video data and generating text descriptions. This makes them well-suited for annotation tasks, which is critical for teaching models how to associate visual patterns with human language. Bedrock Robotics used this technology to streamline data preparation for training AI models, enabling autonomous operations for equipment. Additionally, through proper model selection and prompt engineering, the company improved tool identification from 34% to 70%. This transformed a manual, time-intensive process into an automated, scalable data pipeline solution. The breakthrough accelerated deployment of autonomous equipment.\nThis approach provides a replicable framework for organizations facing similar data challenges and demonstrates how strategic investment in foundation models (FMs) can deliver measurable operational outcomes and a competitive advantage. Foundation models are models trained on massive amounts of data using self-supervised learning techniques that learn general representations that can be adapted to many downstream tasks. VLMs leverage these large-scale pretraining techniques to bridge visual and textual modalities, enabling them to understand, analyze, and generate content across both image and language.\nIn the following sections, we look at the process that Bedrock Robotics used to annotate millions of hours of video footage and accelerate innovation using a VLM-based solution.\nFrom unstructured video data to a strategic asset using VLMs\nEnabling autonomous construction equipment requires extracting useful information from millions of hours of unstructured operational footage. Specifically, Bedrock Robotics needed to identify tool attachments, tasks, and worksite conditions across diverse scenarios. The following images are example video frames from this dataset.\n\nConstruction equipment operates with multiple tool attachments, each requiring accurate classification to train reliable AI models. Working with the Innovation Center, Bedrock Robotics focused their innovation efforts by addressing a few critical tool categories: lifting hooks for material handling, hammers for concrete demolition, grading beams for surface leveling, and trenching buckets for narrow excavation.\nThese labels allow Bedrock Robotics to select relevant video segments and assemble training datasets that represent a variety of equipment configurations and operating conditions.\nAccelerating AI deployment through strategic model optimization\nOff-the-shelf VLMs (VLMs without prompt optimization) struggle with construction video data because they’re trained on web images, not operator footage from excavator cabins. They can’t handle unusual angles, equipment-specific visuals, or poor visibility from dust and weather. They also lack the domain knowledge to distinguish visually similar tools like digging buckets from trenching buckets.\nBedrock Robotics and the Innovation Center addressed this through targeted model selection and prompt optimization. The teams evaluated multiple VLMs—including open source options and FMs available in Amazon Bedrock —then refined prompts with detailed visual descriptions of each tool, guidance for commonly confused tool pairs, and step-by-step instructions for analyzing video frames.\nThese modifications enhanced the classification accuracy from 34% to 70% on a test set comprising 130 videos, at $10 per hour of video processing. These results demonstrate how prompt engineering adapts VLMs to specialized tasks. For Bedrock Robotics, this customization delivered faster training cycles, reduced time-to-deployment, and a cost-effective scalable annotation pipeline that evolves with operational needs.\nThe path forward: addressing labor shortages through automation\nThe Competitive Advantage. For Bedrock Robotics, vision-language systems enabled rapid identification and extraction of critical datasets, providing necessary insights from massive construction video footage. With an overall accuracy of 70%, this cost-effective approach provides a practical foundation for scaling data preparation for model training. It demonstrates how strategic AI innovation can transform workforce constraints and accelerate industry transformations. Organizations that streamline data preparation can accelerate autonomous system deployment, reduce operational costs, and explore new areas for growth in industries impacted by labor shortages. With this repeatable framework, manufacturing and industrial automation leaders facing similar challenges can apply these principles to drive competitive differentiation within their own domains.\nTo learn more, visit Bedrock Robotics or explore the physical AI resources on AWS.\n\nAWS Physical AI Fellowship\nTransforming the Physical World with AI\nPhysical AI in Practice\n\nAbout the authors\n\nLaura Kulowski\nLaura Kulowski is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where she works to develop physical AI solutions. Before joining Amazon, Laura completed her PhD at Harvard’s Department of Earth and Planetary Sciences and investigated Jupiter’s deep zonal flows and magnetic field using Juno data.\n\nAlla Simoneau\nAlla Simoneau is a technology and commercial leader with over 15 years of experience, currently serving as the Emerging Technology Physical AI Lead at Amazon Web Services (AWS), where she drives global innovation at the intersection of AI and real-world applications. With over a decade at Amazon, Alla is a recognized leader in strategy, team building, and operational excellence, specializing in turning cutting-edge technologies into real-world transformations for startups and enterprise customers.\n\nParmida Atighehchian\nParmida Atighehchian is a Senior Data Scientist at AWS Generative AI Innovation Center. With over 10 years of experience in Deep Learning and Generative AI, Parmida brings deep expertise in AI and customer focused solutions. Parmida has led and co-authored highly impactful scientific papers focused on domains such as computer vision, explainability, video and image generation. With a strong focus on scientific practices, Parmida helps customers with practical design of systems using generative AI in robust and scalable pipelines.\n\nDan Volk\nDan Volk is a Senior Data Scientist at the AWS Generative AI Innovation Center. He has 10 years of experience in machine learning, deep learning, and time series analysis, and holds a Master’s in Data Science from UC Berkeley. He is passionate about transforming complex business challenges into opportunities by leveraging cutting-edge AI technologies.\n\nPaul Amadeo\nPaul Amadeo is a seasoned technology leader with over 30 years of experience spanning artificial intelligence, machine learning, IoT systems, RF design, optics, semiconductor physics, and advanced engineering. As Technical Lead for Physical AI in the AWS Generative AI Innovation Center, Paul specializes in translating AI capabilities into tangible physical systems, guiding enterprise customers through complex implementations from concept to production. His diverse background includes architecting computer vision systems for edge environments, designing robotic smart card manufacturing technologies that have produced billions of devices globally, and leading cross-functional teams in both commercial and defense sectors. Paul holds an MS in Applied Physics from the University of California, San Diego, a BS in Applied Physics from Caltech, and holds six patents spanning optical systems, communication devices, and manufacturing technologies.\n\nSri Elaprolu\nSri Elaprolu is Director of the AWS Generative AI Innovation Center, where he leads a global team implementing cutting-edge AI solutions for enterprise and government organizations. During his 13-year tenure at AWS, he has led ML science teams partnering with global enterprises and public sector organizations. Prior to AWS, he spent 14 years at Northrop Grumman in product development and software engineering leadership roles. Sri holds a Master’s in Engineering Science and an MBA.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "54ce10e5897537fb",
    "title": "How Sonrai uses Amazon SageMaker AI to accelerate precision medicine trials",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-sonrai-uses-amazon-sagemaker-ai-to-accelerate-precision-medicine-trials/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T17:31:45Z",
    "summary": "In this post, we explore how Sonrai, a life sciences AI company, partnered with AWS to build a robust MLOps framework using Amazon SageMaker AI that addresses these challenges while maintaining the traceability and reproducibility required in regulated environments.",
    "content": "In precision medicine, researchers developing diagnostic tests for early disease detection face a critical challenge: datasets containing thousands of potential biomarkers but only hundreds of patient samples. This curse of dimensionality can determine the success or failure of breakthrough discoveries.\nModern bioinformatics use multiple omic modalities—genomics, lipidomics, proteomics, and metabolomics—to develop early disease detection tests. Researchers in this industry are also often challenged with datasets where features outnumber samples by orders of magnitude. As new modalities are considered, the permutations increase exponentially, making experiment tracking a significant challenge. Additionally, source control and code quality are a mission-critical aspect of the overall machine learning architecture. Without efficient machine learning operations (MLOps) processes in place, this can be overlooked, especially in the early discovery stage of the cycle.\nIn this post, we explore how Sonrai , a life sciences AI company, partnered with AWS to build a robust MLOps framework using Amazon SageMaker AI that addresses these challenges while maintaining the traceability and reproducibility required in regulated environments.\nOverview of MLOps\nMLOps combines ML, DevOps, and data engineering practices to deploy and maintain ML systems in production reliably and efficiently.\nImplementing MLOps best practices from the start enables faster experiment iterations for and confident, traceable model deployment, all of which are essential in healthcare technology companies where governance and validation are paramount.\nSonrai’s data challenge\nSonrai partnered with a large biotechnology company developing biomarker tests for an underserved cancer type. The project involved a rich dataset spanning multiple omic modalities: proteomics, metabolomics, and lipidomics, with the objective to identify the optimal combination of features for an early detection biomarker with high sensitivity and specificity.The customer faced several critical challenges. Their dataset contained over 8,000 potential biomarkers across three modalities, but only a few hundred patient samples. This extreme feature-to-sample ratio required sophisticated feature selection to avoid overfitting. The team needed to evaluate hundreds of combinations of modalities and modeling approaches, making manual experiment tracking infeasible. As a diagnostic test destined for clinical use, complete traceability from raw data through every modeling decision to the final deployed model was essential for regulatory submissions.\nSolution overview\nTo address these MLOps challenges, Sonrai architected a comprehensive solution using SageMaker AI, a fully managed service for data scientists and developers to build, train, and deploy ML models at scale. This solution helps provide more secure data management, flexible development environments, robust experiment tracking, and streamlined model deployment with full traceability.The following diagram illustrates the architecture and process flow.\n\nThe end-to-end MLOps workflow follows a clear path:\n\nCustomers provide sample data to the secure data repository in Amazon Simple Storage Service (Amazon S3).\nML engineers use Amazon SageMaker Studio Lab and Code Editor , connected to source control.\nPipelines read from the data repository, process data, and write results to Amazon S3.\nThe experiments are logged in MLflow within Amazon SageMaker Studio .\nGenerated reports are stored in Amazon S3 and shared with stakeholders.\nValidated models are promoted to the Amazon SageMaker Model Registry .\nFinal models are deployed for inference or further validation.\n\nThis architecture facilitates complete traceability: each registered model can be traced back through hyperparameter selection and dataset splits to the source data and code version that produced it.\nSecure data management with Amazon S3\nThe foundation of Sonrai’s solution is secure data management with the help of Amazon S3. Sonrai configured S3 buckets with tiered access controls for sensitive patient data. Sample and clinical data were stored in a dedicated data repository bucket with restricted access, facilitating governance with data protection requirements. A separate results repository bucket stores processed data, model outputs, and generated reports. This separation makes sure raw patient data can remain secure while enabling flexible sharing of analysis results. Seamless integration with Git repositories enables collaboration, source control, and quality assurance processes while keeping sensitive patient data secure within the AWS environment—critical for maintaining governance in regulated industries.\nSageMaker AI MLOps\nFrom project inception, Sonrai used both JupyterLab and Code Editor interfaces within their SageMaker AI environment. This environment was integrated with the customer’s Git repository for source control, establishing version control and code review workflows from day one.SageMaker AI offers a wide range of ML-optimized compute instances that can be provisioned in minutes and stopped when not in use, optimizing cost-efficiency. For this project, Sonrai used compute instances with sufficient memory to handle large omic datasets, spinning them up for intensive modeling runs and shutting them down during analysis phases.Code Editor served as the primary development environment for building production-quality pipelines, with its integrated debugging and Git workflow features. JupyterLab was used for data exploration and customer collaboration meetings, where its interactive notebook format facilitated real-time discussion of results.\nThird-party tools such as Quarto , an open source technical publishing system, were installed within the SageMaker compute environments to enable report generation within the modeling pipeline itself. A single quarto render command executes the complete pipeline and creates stakeholder-ready reports with interactive visualizations, statistical tables, and detailed markdown annotations. Reports are automatically written to the results S3 bucket, where customers can download them within minutes of pipeline completion.\nManaged MLflow\nThe managed MLflow capability within SageMaker AI enabled seamless experiment tracking. Experiments executed within the SageMaker AI environment are automatically tracked and recorded in MLflow, capturing a comprehensive view of the experimentation process. For this project, MLflow became the single source of truth for the modeling experiments, logging performance metrics, hyperparameters, feature importance rankings, and custom artifacts such as ROC curves and confusion matrices. The MLflow UI provided an intuitive interface for comparing experiments side-by-side, enabling the team to quickly identify promising approaches and share results during customer review sessions.\nMLOps pipelines\nSonrai’s modeling pipelines are structured as reproducible, version-controlled workflows that process raw data through multiple stages to produce final models:\n\nRaw omic data from Amazon S3 is loaded, normalized, and quality-controlled.\nDomain-specific transformations are applied to create modeling-ready features.\nRecursive Feature Elimination (RFE) reduces thousands of features to the most significant for disease detection.\nMultiple models are trained across individual and combined modalities.\nModel performance is assessed and comprehensive reports are generated.\n\nEach pipeline execution is tracked in MLflow, capturing input data versions, code commits, hyperparameters, and performance metrics. This creates an auditable trail from raw data to final model, essential for regulatory submissions. The pipelines are executed on SageMaker training jobs, which provide scalable compute resources and automatic capture of training metadata.The most critical pipeline stage was RFE, which iteratively removes less important features while monitoring model performance. MLflow tracked each iteration, logging which features were removed, the model’s performance at each step, and the final selected feature set. This detailed tracking enabled validation of feature selection decisions and provided documentation for regulatory review.\nModel deployment\nSonrai uses both MLflow and the SageMaker Model Registry in a complementary fashion to manage model artifacts and metadata throughout the development lifecycle. During active experimentation, MLflow serves as the primary tracking system, enabling rapid iteration with lightweight experiment tracking. When a model meets predetermined performance thresholds and is ready for broader validation or deployment, it is promoted to the SageMaker Model Registry.This promotion represents a formal transition from research to development. Candidate models are evaluated against success criteria, packaged with their inference code and containers, and registered in the SageMaker Model Registry with a unique version identifier. The SageMaker Model Registry supports a formal deployment approval workflow aligned with Sonrai’s quality management system:\n\nPending – Newly registered models awaiting review\nApproved – Models that have passed validation criteria and are ready for deployment\nRejected – Models that did not meet acceptance criteria, with documented reasons\n\nFor the cancer biomarker project, models were evaluated against stringent clinical criteria: sensitivity of at least 90%, specificity of at least 85%, and AUC-ROC of at least 0.90. For approved models, deployment options include SageMaker endpoints for real-time inference, batch transform jobs for processing large datasets, or retrieval of model artifacts for deployment in customer-specific environments.\nResults and model performance\nUsing ML-optimized compute instances on SageMaker AI, the entire pipeline—from raw data to final models and reports—executed in under 10 minutes. This rapid iteration cycle enabled daily model updates, real-time collaboration during customer meetings, and immediate validation of hypotheses. What previously would have taken days could now be accomplished in a single customer call.The modeling pipeline generated 15 individual models across single-modality and multi-modality combinations. The top-performing model combined proteomic and metabolomic features, achieving 94% sensitivity and 89% specificity with an AUC-ROC of 0.93. This multi-modal approach outperformed single modalities alone, demonstrating the value of integrating different omic data types.The winning model was promoted to the SageMaker Model Registry with complete metadata, including model artifact location, training dataset, MLflow experiment IDs, evaluation metrics, and custom metadata. This registered model underwent additional validation by the customer’s clinical team before approval for clinical validation studies. “Using SageMaker AI for the full model development process enabled the team to collaborate and rapidly iterate with full traceability and confidence in the final result. The rich set of services available in Amazon SageMaker AI make it a complete solution for robust model development, deployment, and monitoring,” says Matthew Lee, Director of AI & Medical Imaging at Sonrai.\nConclusion\nSonrai partnered with AWS to develop an MLOps solution that accelerates precision medicine trials using SageMaker AI. The solution addresses key challenges in biomarker discovery: managing datasets with thousands of features from multiple omic modalities while working with limited patient samples, tracking hundreds of complex experimental permutations, and maintaining version control and traceability for regulatory readiness.The result is a scalable MLOps framework that reduces development iteration time from days to minutes while facilitating reproducibility and regulatory readiness. The combination of the SageMaker AI development environment, MLflow experiment tracking, and SageMaker Model Registry provides end-to-end traceability from raw data to deployed models—essential for both scientific validity and governance. Sonrai saw the following key results:\n\n8,916 biomarkers modeled and tracked\nHundreds of experiments performed with full lineage\n50% reduction in time spent curating data for biomarker reports\n\nBuilding on this foundation, Sonrai is expanding its SageMaker AI MLOps capabilities. The team is developing automated retraining pipelines that trigger model updates when new patient data becomes available, using Amazon EventBridge to orchestrate SageMaker AI pipelines that monitor data drift and model performance degradation.\nSonrai is also extending the architecture to support federated learning across multiple clinical sites, enabling collaborative model development while keeping sensitive patient data at each institution. Selected models are being deployed to SageMaker endpoints for real-time predictions, supporting clinical decision support applications.\nGet started today with Amazon SageMaker for MLOps to build your own ML Ops piplines. Please find our introductory Amazon SageMaker ML Ops workshop to get started.\n\nAbout the Authors\n\nMatthew Lee\nMatthew Lee is Director of AI & Medical Imaging at Sonrai, bringing extensive experience as a data scientist specializing in computer vision and medical imaging. With a background as a medical physicist, he focuses on developing impactful AI solutions—from initial experimentation through proof of concept to scalable production code that addresses real business needs. Matthew has successfully built and deployed AI models in cloud environments for clients, and regularly shares his work through customer presentations, conference talks, and industry meetups.\n\nJonah Craig\nJonah Craig is a Startup Solutions Architect based in Dublin, Ireland. He works with startup customers across the UK and Ireland and focuses on developing AI/ML and generative AI solutions. Jonah has a master’s degree in computer science and regularly speaks on stage at AWS conferences, such as the annual AWS London Summit and the AWS Dublin Cloud Day. In his spare time, he enjoys creating music and releasing it on Spotify.\n\nSiamak Nariman\nSiamak Nariman  is a Senior Product Manager at AWS. He is focused on AI/ML technology, ML model management, and ML governance to improve overall organizational efficiency and productivity. He has extensive experience automating processes and deploying various technologies.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "7c846a86ffbdf275",
    "title": "Accelerating AI model production at Hexagon with Amazon SageMaker HyperPod",
    "url": "https://aws.amazon.com/blogs/machine-learning/accelerating-ai-model-production-at-hexagon-with-amazon-sagemaker-hyperpod/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T17:29:11Z",
    "summary": "In this blog post, we demonstrate how Hexagon collaborated with Amazon Web Services to scale their AI model production by pretraining state-of-the-art segmentation models, using the model training infrastructure of Amazon SageMaker HyperPod.",
    "content": "This blog post was co-authored with Johannes Maunz, Tobias Bösch Borgards, Aleksander Cisłak, and Bartłomiej Gralewicz from Hexagon.\nHexagon is the global leader in measurement technologies and provides the confidence that vital industries rely on to build, navigate, and innovate. From microns to Mars , Hexagon’s solutions drive productivity, quality, safety, and sustainability across aerospace, agriculture, automotive, construction, manufacturing, and mining.\nApplications in these industries often rely on capturing the reality by recording vast amounts of highly accurate point cloud data with Hexagon measurement technology. A point cloud is a collection of data points in 3D space, typically representing the external surface of an object or a scene. Point clouds are commonly used in applications like 3D modeling, computer vision, robotics, autonomous vehicles, and geospatial analysis.\nHexagon provides specialized AI models to its customers to help them ensure productivity, quality, safety, or sustainability in their applications. These AI models are purpose built for a given domain and usually focus on understanding the built environment.\nIn this blog post, we demonstrate how Hexagon collaborated with Amazon Web Services to scale their AI model production by pretraining state-of-the-art segmentation models, using the model training infrastructure of Amazon SageMaker HyperPod .\nAI impact and opportunity\nAI models provided by Hexagon to its customers help them solve complex challenges. These challenges are solved by specialized AI models that are often more effective than large, general-purpose ones. Before using scanned point clouds in geospatial applications, it’s essential to perform preprocessing and point cloud cleaning operations. Instead of relying on a single AI model to classify an entire dataset, targeted AI models have been developed that tackle distinct operations: one efficiently removes stray points from dust or sensor noise, another helps separate land types even in complex environments, and another detects and eliminates moving objects like cars and pedestrians while keeping fixed objects in the scene. This AI approach not only improves precision and efficiency, but also reduces processing demands and leads to faster creation of and more accurate 3D models.\nThe following figures illustrate the practical application of specialized AI models, such as the point cloud classification models that Hexagon is developing.\nThe first figure shows how mobile mapping road models enable the creation of digital twins of entire cities.\n\nThe second figure is a heavy construction model that enables on-site decision making.\n\nThere’s a significant opportunity to accelerate Hexagon’s AI innovation and time-to-market by implementing a robust, scalable, and high-performance infrastructure that enables efficient and fast model training and development of new, specialized AI use cases in days rather than months.\nHexagon and Amazon SageMaker HyperPod: A success story\nTo address Hexagon’s need for scalable compute resources, access to the latest GPUs, and streamlined training pipelines, the Hexagon team evaluated the key features of Amazon SageMaker HyperPod for their model training requirements:\n\nResilient architecture: SageMaker HyperPod streamlined operations through proactive node health checks and automated cluster monitoring. With built-in self-healing capabilities and automated job resumption, it enables training runs to run for weeks or months without interruptions. In the event of a node failure, it will automatically detect the failure, replace the faulty node, and resume the training from the most recent checkpoint.\nScalable infrastructure : Using single-spine node topology and pre-configured Elastic Fabric Adapter (EFA) , SageMaker HyperPod delivers optimal inter-node communication. Its flexible compute capacity allocation enables seamless scaling without compromising performance, making it ideal for growing workloads spanning multiple nodes.\nVersatile deployment : Compatible with a wide range of generative AI software stacks, SageMaker HyperPod simplifies deployment through lifecycle scripts and Helm customization. It supports leading Amazon Elastic Compute Cloud (Amazon EC2) instances like the P6-B200 and P6e-GB200, which are accelerated by NVIDIA Blackwell GPUs, offering versatility in implementation.\nEfficient operations : Through intelligent task governance and integrated SageMaker tools, SageMaker HyperPod automatically optimizes cluster utilization. Pre-configured Deep Learning Amazon Machine Images (DLAMI) with compatible drivers and libraries, combined with quick start training recipes, help to ensure maximum operational efficiency.\n\nSolution overview\nHexagon implemented a robust training environment using Amazon SageMaker HyperPod managed infrastructure, shown in the following figure. It includes an integrated data pipeline, compute cluster management, and MLOps monitoring stack.\n\nSolution Architecture Diagram\nData pipeline and storage\nTraining data is stored in Amazon Simple Storage Service (Amazon S3) within Hexagon’s AWS account, with Amazon FSx for Lustre providing high-performance parallel file system capabilities. The Amazon FSx for Lustre file system is configured with a data repository association (DRA) that automatically synchronizes with the S3 bucket, enabling lazy loading of training data and automatic export of model checkpoints back to Amazon S3.\nThis configuration enables streaming of terabytes of training data directly to GPU accelerated compute nodes at multi-GBs per second throughput rates, eliminating data transfer bottlenecks during model training. The DRA helps ensure that data scientists can work with familiar Amazon S3 interfaces while benefiting from the performance advantages of a parallel file system during training.\nCompute cluster management\nSageMaker HyperPod cluster provisioned with built-in health checks and automated instance management. Through Amazon SageMaker Training Plans, Hexagon can flexibly reserve GPU capacity from 1 day to 6 months, helping to ensure resource availability for both short experimental runs and extended training campaigns. These training plans provide predictable pricing and dedicated capacity, eliminating the uncertainty of on-demand resource availability for critical model development. The cluster automatically handles node failures and job resumption, maintaining training continuity without manual intervention.\nMLOps and monitoring stack\nThe environment integrates with a one-click observability solution from Amazon SageMaker HyperPod, which automatically publishes comprehensive metrics to Amazon Managed Service for Prometheus and visualizes them through pre-built Amazon Managed Grafana dashboards optimized for foundation model development.\nThis unified observability consolidates health and performance data from NVIDIA Data Center GPU Manager, Kubernetes node exporters, EFA, integrated file systems, and SageMaker HyperPod task operators, enabling per-GPU level monitoring of resource utilization, GPU memory, and FLOPs.\nFor experiment tracking, MLflow on Amazon SageMaker AI provides a fully managed solution that requires minimal code modifications to Hexagon’s training containers. This integration enables automatic tracking of training parameters, metrics, model artifacts, and lineage across all experiment runs, with the ability to compare model performance and reproduce results reliably.\nKey outcomes from using SageMaker HyperPod at Hexagon\nHexagon’s implementation of SageMaker HyperPod delivered measurable improvements across deployment speed, training efficiency, and model performance.\n\nQuick integration and deployment: Hexagon successfully integrated SageMaker HyperPod for training and achieved their first training deployment within hours, reflecting the ease of set-up and enhanced end-user experience for machine learning (ML) developers. Having all the services required for training models under a single ecosystem helped meet security and governance needs.\nTraining time reduction: Hexagon reduced their training time from 80 days on-premises for a given network and configuration to approximately 4 days on AWS using 6x ml.p5.48xlarge instances each containing eight NVIDIA H100 GPUs, with EFA network interface that boosts distributed training efficiency through low-latency, high throughput networking for multi-node GPU training.\nPerformance enhancement: SageMaker HyperPod enabled larger batch sizes during training, which led to better training performance, resulting in higher accuracy scores for the trained AI models.\n\nAWS Enterprise Support played a crucial role in Hexagon’s successful implementation of Amazon SageMaker HyperPod. Through proactive guidance, deep technical expertise, and dedicated partnership, the AWS Enterprise Support team helped Hexagon navigate their cloud journey from initial AWS adoption to advanced generative AI implementations. The comprehensive support included best practices guidance, cost optimization strategies, and continuous architectural advice, so that Hexagon’s team could focus on innovation while maintaining operational excellence. This strategic partnership demonstrates how AWS Enterprise Support goes beyond traditional support services, becoming a trusted advisor that helps customers accelerate their business transformation and achieve their desired outcomes in the cloud.\nConclusion\nHexagon’s collaboration with Amazon Web Services delivered a remarkable 95% reduction in training time through Amazon SageMaker HyperPod. With flexible training plans, Hexagon teams can now provision the exact amount of accelerated compute capacity needed for each model training project with complete flexibility and freedom. This combination of flexibility, scalability, and performance unlocks a transformative approach to model development at Hexagon, accelerating innovation and powering the next generation of AI-enabled products that help customers build, navigate, and innovate across critical industries.\n\nAbout the Authors\n\nJohannes Maunz\nJohannes Maunz joined Hexagon Geosystems’ research and development department as an electronics/software engineer in 2007. Since 2017, he has been working for the Innovation Hub, Hexagon’s central technology organization. In his role he leads Hexagon’s central AI group, is responsible for applied research, development, deployment, and strategy of AI across sensors and solutions for all industries served by Hexagon. Furthermore he’s responsible for the AI enabled company program, a program for Hexagons workforce to use AI in daily operations across departments and functions.\n\nTobias Bösch Borgards\nTobias Bösch Borgards is an electrical engineer by training and leads the AI engineering team at Hexagon. Together with his team, Tobias bring ML to life in Hexagon products. In his free time, he enjoys hiking and skiing.\n\nBartlomiej Gralewicz\nBartlomiej Gralewicz is an Expert Software Engineer for AI at Hexagon AI Hub. He focuses on productizing AI solutions that allow understanding and automating 3D geometry analysis. In his free time, he enjoys bouldering, great coffee, and watching F1.\n\nMohan Gowda\nMohan Gowda is a Principal Solutions Architect for AI/ML at Amazon Web Services, helping customers across Switzerland, Austria, and Central & Eastern Europe drive innovation and digital transformation using AWS generative AI and machine learning services. In his free time, he enjoys playing tennis and skiing in the Swiss Alps.\n\nRoy Allela\nRoy Allela is a Senior AI/ML Specialist Solutions Architect at AWS. He helps AWS customers, from small startups to large enterprises to train and deploy foundation models efficiently on AWS. He has a background in Microprocessor Engineering passionate about computational optimization problems and improving the performance of AI workloads.\n\nAnkit Anand\nAnkit Anand is a Principal Foundation Models Go-To-Market (GTM) Specialist at AWS. He partners with top generative AI model builders, strategic customers, and AWS service teams to enable the next generation of AI/ML workloads on AWS. Ankit’s experience includes product management expertise within the financial services industry for high-frequency and low-latency trading and business development for Amazon Alexa.\n\nJann Wild\nJann Wild is a Senior Solutions Architect at Amazon Web Services (AWS), where he has spent nearly 8 years helping organizations harness the power of cloud computing and artificial intelligence. With deep expertise in software architecture and AI/ML solutions, Jann specializes in guiding enterprises through complex digital transformation initiatives while ensuring robust cloud security practices.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "d539a03cb0dbdcc9",
    "title": "Agentic AI with multi-model framework using Hugging Face smolagents on AWS",
    "url": "https://aws.amazon.com/blogs/machine-learning/agentic-ai-with-multi-model-framework-using-hugging-face-smolagents-on-aws/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T15:47:06Z",
    "summary": "Hugging Face smolagents is an open source Python library designed to make it straightforward to build and run agents using a few lines of code. We will show you how to build an agentic AI solution by integrating Hugging Face smolagents with Amazon Web Services (AWS) managed services. You'll learn how to deploy a healthcare AI agent that demonstrates multi-model deployment options, vector-enhanced knowledge retrieval, and clinical decision support capabilities.",
    "content": "This post is cowritten by Jeff Boudier, Simon Pagezy, and Florent Gbelidji from Hugging Face.\nAgentic AI systems represent an evolution from conversational AI to autonomous agents capable of complex reasoning, tool usage, and code execution. Enterprise applications benefit from strategic deployment approaches tailored to specific needs. These needs include managed endpoints, which deliver auto-scaling capabilities, foundation model APIs to support complex reasoning, and containerized deployment options that support custom integration requirements.\nHugging Face smolagents is an open source Python library designed to make it straightforward to build and run agents using a few lines of code. We will show you how to build an agentic AI solution by integrating Hugging Face smolagents with Amazon Web Services (AWS) managed services. You’ll learn how to deploy a healthcare AI agent that demonstrates multi-model deployment options, vector-enhanced knowledge retrieval, and clinical decision support capabilities.\nWhile we use healthcare as an example, this architecture applies to multiple industries where domain-specific intelligence and reliability are critical. The solution uses the model-agnostic, modality-agnostic, and tool-agnostic design of smolagents to orchestrate across Amazon SageMaker AI endpoints, Amazon Bedrock APIs, and containerized model servers.\nSolution overview\nMany AI systems face limitations with single-model approaches that can’t adapt to diverse enterprise needs. These systems often have rigid deployment options, inconsistent APIs across different AI services, and lack multi-model deployment options for optimal model selection.\nThis solution demonstrates how organizations can build AI systems that address these limitations. The solution allows deployment selection based on operational needs and provides consistent request and response formats across different AI backends and deployment methods. It generates contextual responses through medical knowledge integration and vector search, supporting deployment from development to production environments through containerized architecture.\nThis healthcare use case illustrates how the AI agent can process complex medical queries for six medications with clinical decision support and AWS security and compliance capabilities.\nArchitecture\nThe solution consists of the following services and features to deliver the agentic AI capabilities:\n\nSageMaker AI with BioM-ELECTRA-Large-SQuAD2 model for specialized medical queries and managed auto-scaling.\nAmazon Bedrock with Claude 3.5 Sonnet V2 by Anthropic for complex reasoning and foundation model access.\nAmazon OpenSearch Service for vector similarity matching and contextual knowledge retrieval with medical knowledge indexing.\nAmazon Elastic Container Service (Amazon ECS) , with AWS Fargate , for serverless container orchestration and scalable deployment of the Python application that uses the smolagents library.\nAWS Identity and Access Management (IAM) for security and access control.\nContainerized model server with BioM-ELECTRA-Large-SQuAD2 for self-hosted model deployment.\n\nThe following diagram illustrates the solution architecture.\n\nThe architecture is a complete integration of the Hugging Face smolagents framework with AWS services. A client web interface connects to a healthcare agent container that orchestrates across three model backends: SageMaker AI with BioM-ELECTRA, Amazon Bedrock with Claude 3.5 Sonnet V2, and a containerized model server with BioM-ELECTRA. The solution includes a vector store powered by OpenSearch Service and a security layer with data encryption at rest and in transit. The security layer also handles IAM access control and authentication, and any medical disclaimers for regulatory compliance.\nThis solution supports deployment options through smolagents with each backend optimized for different scenarios:\n\nSageMaker AI for managed endpoints with auto-scaling and production workloads using Hugging Face Hub models.\nAmazon Bedrock for serverless access to foundation models and complex reasoning through AWS APIs.\nA containerized model server for self-hosted model deployment and tool integration from Hugging Face Hub.\n\nThe three backends implement Hugging Face Messages API compatibility , confirming consistent request and response formats regardless of the selected model service. Users select the appropriate backend based on their requirements—the solution provides deployment options rather than automatic routing.\nThe complete implementation is available in the sample-healthcare-agent-with-smolagents-on-aws GitHub repository .\nKey benefits\nThe integration of Hugging Face smolagents with AWS managed services offers significant advantages for enterprise agentic AI deployments.\nDeployment choice\nOrganizations can choose the optimal deployment for each use case: Amazon Bedrock for serverless access to foundation models and self-hosted containerized deployment for custom tool integration or SageMaker AI for specialized domain models. These options help to match specific workload requirements, rather than a one-size-fits-all approach.\nMulti-model deployment options\nOrganizations can optimize their infrastructure choices without changing their agent logic. You can switch between containerized model server, SageMaker AI, and Amazon Bedrock without modifying your application code. This provides deployment options, while maintaining consistent agent behavior.\nCode generation capabilities\nThe CodeAgent approach of smolagents streamlines multi-step operations through direct Python code generation and processing. The following comparison illustrates the multi-step operations of smolagents:\nMulti-step JSON-based approach:\n\n{\n\"action\": \"search\",\n\"parameters\": {\"query\": \"drug interactions\"},\n\"next_action\": {\n\"action\": \"filter\",\n\"parameters\": {\"criteria\": \"severity > moderate\"}\n}\n}\n\nsmolagents CodeAgent:\n\n# Search and filter in single code block\nresults = search_tool(\"drug interactions\")\nfiltered_results = [r for r in results if r.severity > \"moderate\"]\nfinal_answer(f\"Found {len(filtered_results)} severe interactions: {filtered_results}\")\n\nThe smolagents CodeAgent supports single code blocks to handle multi-step operations, reducing large language model (LLM) calls while streamlining agent development. It provides full control of agent logic across AWS service deployments.\nScalable architecture\nBy deploying the application on AWS, you gain access to security features and auto-scaling capabilities that help you meet organizational security requirements and maintain regulatory compliance. Running containerized workloads with Amazon ECS and Fargate helps you achieve reliable operations and optimize costs through automated resource scaling.\nLet’s walk through implementing this solution.\nPrerequisites\nBefore you deploy the solution, you need the following:\n\nAn AWS account with appropriate permissions to create IAM roles , Amazon ECS clusters , and Amazon OpenSearch Service domains .\nAWS Command Line Interface (AWS CLI) version 2.0 or later installed and configured.\nPython 3.10 or later for running deployment scripts.\nDocker installed and running (required for production environments to provide secure code execution sandbox).\nAccess to Amazon Bedrock, SageMaker AI, and OpenSearch Service in your AWS Region with appropriate IAM permissions to create and manage resources.\nFor this implementation, we’re using Python 3.10+ , smolagents framework , transformers 4.28.1+ , PyTorch 2.0.0+ , and boto3 .\n\nRun the following command to install the required Python packages:\n\npip install -r healthcare_ai_agent/phase_00_installation/requirements.txt\n\nConfigure environment variables\nSet the required environment variables for your AWS Region and resource names before deploying the infrastructure.\n\nSet the following environment variables in your terminal:\n\nexport AWS_REGION=us-west-2\nexport SAGEMAKER_ENDPOINT_NAME=healthcare-qa-endpoint-1\nexport OPENSEARCH_DOMAIN=healthcare-vector-store\nexport OPENSEARCH_INDEX=medical-knowledge\nexport BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0\nexport SAGEMAKER_MODEL_ID=sultan/BioM-ELECTRA-Large-SQuAD2\nexport CONTAINERIZED_MODEL_ID=sultan/BioM-ELECTRA-Large-SQuAD2\n\nVerify the variables are set:\n\necho $AWS_REGION\necho $SAGEMAKER_ENDPOINT_NAME\n\nThese environment variables are used throughout the deployment and testing processes. Verify that they’re set before proceeding to the next step.\nSet up AWS infrastructure\nStart by creating the foundational AWS infrastructure components using the SampleAWSInfrastructureManager class from the Smolagents_SageMaker_Bedrock_Opensearch.py implementation.\nDeploy complete infrastructure (automated approach)\nFor automated deployment of the AWS infrastructure components, you can use the enhanced main function.\n\nStart the enhanced main function:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\nenhanced_main()\n# Select option 1 for complete AWS infrastructure deployment\n\nThe deployment automatically creates an Amazon ECS cluster, IAM roles, and an OpenSearch Service domain.\nWait for deployment to complete (approximately 15–20 minutes).\n\nCreate individual AWS components (alternative approach)\nIf you prefer to create components individually, you can set up an OpenSearch Service domain for vector-enhanced knowledge retrieval and an Amazon ECS cluster for containerized deployment.\nBoth the OpenSearch Service domain and Amazon ECS cluster are automatically created as part of the complete AWS infrastructure deployment (Option 1 in enhanced_main ). If you’ve already deployed the complete infrastructure, both components are ready and you can skip to the Deploy the Amazon SageMaker AI endpoint section.\nDeploy the Amazon SageMaker AI endpoint\nDeploy the BioM-ELECTRA-Large-SQuAD2 model to SageMaker AI for specialized medical query processing. An automated method (for deployment through the enhanced main) and a manual method (for deployment through the SageMaker AI endpoint) are provided.\nTo deploy using enhanced main (automated method)\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n# Select option 2 for SageMaker endpoint deployment\n\nTo deploy the SageMaker AI endpoint (manual method)\n\nVerify your SageMaker environment variables are set (from the Configure environment variables section):\n\necho $SAGEMAKER_MODEL_ID\necho $SAGEMAKER_ENDPOINT_NAME\n\nRun the Amazon SageMaker deployment function:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import deploy_sagemaker_endpoint_safe\nendpoint_name = deploy_sagemaker_endpoint_safe()\n\nThe deployment uses the HuggingFaceModel with transformers 4.28.1, PyTorch 2.0.0, and ml.m5.xlarge instance type.\nWait for the endpoint deployment to complete (approximately 5–10 minutes).\nVerify the endpoint deployment:\n\n# Verify endpoint status\nprint(f\"✅ Endpoint deployed: {endpoint_name}\")\n\nThe endpoint is configured for question-answering tasks with MAX_LENGTH=512 and TEMPERATURE=0.1 .\n\nConfigure the multi-model backends\nConfigure the two additional backend options using the SampleTripleHealthcareAgent class for model selection based on operational needs.\nOption 1 – Set up Amazon Bedrock access\nConfigure access to Amazon Bedrock for foundation model integration with Claude 3.5 Sonnet V2.\n\nVerify your Amazon Bedrock model configuration is set (from the Configure environment variables section):\n\necho $BEDROCK_MODEL_ID\n\nAccess to Claude 3.5 Sonnet V2 is automatically available for your AWS account.\nYou can verify model availability in the Amazon Bedrock console under Model catalog .\n\nInitialize the medical knowledge base\nSet up the medical knowledge database with six medications and vector embeddings in OpenSearch Service. You can use the enhanced_main() function, which provides an interactive menu for deployment tasks, or initialize manually using the SampleOpenSearchManager class.\nTo initialize using enhanced main (Automated method)\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n# Select option 4 for OpenSearch initialization and medical knowledge indexing\n\nTo initialize the medical knowledge base (Manual method)\n\nInitialize the SampleOpenSearchManager :\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n# Select option 4 for OpenSearch initialization and medical knowledge indexing\n\nThe system creates the medical knowledge index with mappings for drug_name , content , and content_type fields.\nMedical knowledge for Metformin, Lisinopril, Atorvastatin, Amlodipine, Omeprazole, and Simvastatin is automatically indexed.\nEach medication includes side effects, monitoring requirements, and drug class information.\nThe vector store supports similarity search with content type filtering for targeted queries.\nVerify the indexing completed successfully:\n\n# Verify the indexing completed successfully\nprint(\"Medical knowledge base initialized with 6 medications\")\n\nOption 2 – Deploy the containerized model server\nAfter setting up the core infrastructure, deploy a containerized model server that provides self-hosted model deployment capabilities.\nDeploy the containerized model server using the LocalContainerizedModelServer class with BioM-ELECTRA-Large-SQuAD2 for model deployment.\nTo deploy containers to Amazon ECS (automated method)\nFor automated deployment, use the containerized Amazon ECS deployment:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n# Select option 3 for ECS container deployment\n\nTo deploy the containerized model server (manual method)\n\nVerify your containerized model configuration is set (from the Configure environment variables step):\n\necho $CONTAINERIZED_MODEL_ID\n\nInitialize the containerized model server:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import LocalContainerizedModelServer\ncontainerized_server = LocalContainerizedModelServer()\ncontainerized_server.start_server()\n\nThe containerized model server uses Docker sandboxing for secure code execution.\n\nVerify that the containerized model server is running:\n\nprint(f\"✅ Containerized Model Server status: {containerized_server.get_status()}\")\n\nThe containerized model server includes fallback mechanisms with medical knowledge database integration.\nThe containerized model server supports deployment with Amazon ECS service discovery.\n\nImplement the healthcare agent\nDeploy the core healthcare agent using the SampleTripleHealthcareAgent class that demonstrates smolagents integration with deployment capabilities across the three backends.\nInitialize the triple healthcare agent\nSet up the main healthcare agent that orchestrates across SageMaker AI, Amazon Bedrock, and containerized model server with smolagents framework integration.\n\nCreate the vector store instance:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import SampleOpenSearchVectorStore\nvector_store = SampleOpenSearchVectorStore()\nvector_store_available = vector_store.initialize_client()\n\nInitialize the triple healthcare agent:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import SampleTripleHealthcareAgent\nimport os\nagent = SampleTripleHealthcareAgent(\nendpoint_name=os.getenv('SAGEMAKER_ENDPOINT_NAME'),\nvector_store=vector_store if vector_store_available else None\n)\n\nThe agent initializes three model backends:\n\nSampleSageMakerEndpointModel with BioM-ELECTRA integration\nSampleBedrockClaudeModel with Claude 3.5 Sonnet V2 API\nSampleContainerizedModel with fallback mechanisms\n\nEach backend is wrapped with SampleHealthcareCodeAgent for smolagents integration.\nThe agents are configured with max_steps=3 and DuckDuckGoSearchTool integration.\n\nConfigure deployment options\nThe system demonstrates three deployment options based on operational needs.\nTo understand the deployment options:\n\nSageMaker AI deployment : Specialized medical queries about drug information, side effects, and monitoring requirements.\nAmazon Bedrock deployment: Complex reasoning tasks requiring detailed medical analysis and multi-step reasoning.\nContainerized deployment: Custom model integration with specialized tools and self-hosted model access.\n\nEach backend includes database fallback with medical knowledge for six medications and Amazon OpenSearch Service provides contextual information across the model backends with similarity matching.\nTest the interactive system\nUse the sample_interactive_healthcare_assistant function to test the multi-model deployment.\n\nStart the interactive healthcare assistant:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import sample_interactive_healthcare_assistant\nsample_interactive_healthcare_assistant()\n\nThe system provides commands for model switching:\n\n1 for SageMaker Endpoint\n2 for containerized model server\n3 for Amazon Bedrock Claude API\ncompare <question> to view the three responses\n\nTest queries are limited to 100 characters for optimal performance.\nResponses include vector context when available and database fallback information.\n\nRun and test the solution\nYou can now run and test the multi-model healthcare agent to observe the multi-backend deployment capabilities of smolagents using the actual medical knowledge database with six medications.\nRun the application\nThe solution provides multiple ways to interact with the healthcare AI agent based on your preferred development environment.\nStreamlit web interface\nFor an interactive web-based experience:\n\ncd healthcare_ai_agent/streamlit_demo\nstreamlit run healthcare_app.py\n\nJupyter Notebook\nFor interactive development and experimentation:\n\njupyter lab Smolagents_SageMaker_Bedrock_Opensearch.ipynb\n\nDirect Python\nFor command-line execution:\n\npython Smolagents_SageMaker_Bedrock_Opensearch.py\n\nThese methods provide access to the same multi-model healthcare agent functionality with different user interfaces.\nTest the deployment options\nUse the enhanced_main function to access testing utilities and validate the multi-model deployment.\nInteractive healthcare demo\nExperience the healthcare assistant’s conversational interface with near real-time model switching capabilities.\n\nStart the enhanced main function:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n\nChoose option 5 for Interactive 3-model demo to launch the conversational interface.\nThe system provides commands for model switching:\n\n1 for SageMaker Endpoint\n2 for containerized model server\n3 for Amazon Bedrock Claude API\ncompare <question> to see the three responses\n\nTest queries are limited to 100 characters for optimal performance.\nResponses include vector context when available and database fallback information.\n\nCompare model responses\nUse the enhanced_main function to compare the three deployment options across different\nquery types.\nTo run interactive tests\n\nStart the enhanced main function:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\nenhanced_main()\n\nChoose option 7 for Compare the 3 models to test deployment options.\n\nTest a medical query using SageMaker AI:\n\nQuery: \"What are the side effects of metformin?\"\nExpected deployment: SageMaker AI (specialized medical knowledge)\nResponse: \"Metformin side effects: Nausea, Diarrhea. Serious: Lactic acidosis.\"\n\nTest a complex reasoning query using Amazon Bedrock:\n\nQuery: \"Compare cardiovascular risks of atorvastatin and simvastatin\"\nExpected deployment: Amazon Bedrock (complex reasoning with Claude 3.5 Sonnet V2)\nResponse: Detailed analysis with mechanism of action and monitoring requirements\n\nTest a query that uses the containerized model server:\n\nQuery: \"Tell me about lisinopril monitoring requirements\"\nExpected deployment: Containerized Model Server (specialized tools and fallback)\nResponse: \"Monitor Blood pressure, Kidney function for Lisinopril\"\n\nActual responses include vector context from OpenSearch Service when available, showing similarity matching results alongside the model responses.\nVerify deployment status\nUse the built-in testing utilities to verify healthcare agent deployment and component status.\n\nRun the component test (option 6 in enhanced_main ):\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\nenhanced_main()\n# Select option 6 for component test\n\nExpected output:\n\nOpenSearch: Connected\nSageMaker endpoint: Active\nBedrock: Available\nContainerized model server: Started\nECS cluster: Active\n\n \n\nMonitor the agent status using get_container_status() :\n\nstatus = agent.get_container_status()\nprint(status)\n# Shows: Healthcare Agent Container, smolagents framework Active,\n# Messages API Compatible, 6 medications loaded\n\nExpected output:\n\nHealthcare agent container: Active\nsmolagents framework: Active\nMessages API: Compatible\nDrug database: 6 medications loaded\nVector store: OpenSearch integrated\n\n \n\nTest containerized model server functionality (option 10 ) to validate the containerized model server independently:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main\n# Start the enhanced main function\nenhanced_main()\n# Select option 10 for containerized model server test\n\nExpected output:\n\nContainerized model server: Started successfully\nBioM-ELECTRA model: Loaded\nMedical knowledge: Available\nFallback mechanisms: Active\n\n \n\nTest OpenSearch Service vector similarity (option 9 ) to observe how the system matches queries (such as metformin side effects and lisinopril monitoring) with vector similarity scores and content matching.\n\nClean up resources\nTo avoid incurring future charges, delete the resources you created using the cleanup utilities provided in the implementation.\n\nDelete the Amazon OpenSearch Service domain:\n\naws opensearch delete-domain --domain-name healthcare-vector-store\n\nDelete the Amazon ECS cluster and healthcare agent service:\n\naws ecs update-service --cluster healthcare-agent-cluster --service healthcare-agent-service --desired-count 0\naws ecs delete-service --cluster healthcare-agent-cluster --service healthcare-agent-service\naws ecs delete-cluster --cluster healthcare-agent-cluster\n\nClean up the Amazon SageMaker AI endpoint and containerized model server. Choose either of the following methods:\n\nMethod 1: Use the enhanced_main menu\n\nRun the enhanced main function:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import enhanced_main enhanced_main()\n\nSelect option 8 for SageMaker cleanup when prompted.\n\nMethod 2: Use direct cleanup functions\n\nUse the direct cleanup functions:\n\nfrom Smolagents_SageMaker_Bedrock_Opensearch import cleanup_sagemaker_endpoint cleanup_sagemaker_endpoint(\"healthcare-qa-endpoint-1\")\n\nIf the containerized server is running, stop it:\n\ncontainerized_server.stop_server()\n\nRemove the IAM role:\n\naws iam detach-role-policy --role-name ecsTaskExecutionRole --policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\naws iam delete-role --role-name ecsTaskExecutionRole\n\nAdditional considerations\nFor production deployments, implementing observability is essential for monitoring agent performance, tracking execution traces, and verifying reliability. Amazon Bedrock AgentCore Runtime provides observability with automatic instrumentation. It captures session metrics, performance data, error tracking, and complete execution traces (including tool invocations). Read more about implementing observability in Build trustworthy AI agents with Amazon Bedrock AgentCore observability .\nUse cases\nWhile we demonstrated a healthcare implementation, this solution applies to multiple industries requiring domain-specific intelligence and reliability. With multi-model deployment, organizations can choose the optimal backend for each use case. This includes managed endpoints for production workloads, foundation models for complex reasoning, or self-hosted deployment for custom integration.\nFinancial services\nFinancial institutions can deploy agents for regulatory compliance, risk assessment, and fraud detection while meeting strict security and audit requirements. This deployment approach supports specialized fraud detection models, complex regulatory analysis, and custom financial tools integration.\nManufacturing and industrial operations\nManufacturing organizations can implement intelligent agents for predictive maintenance, quality control, and supply chain optimization. Multi-model deployment allows equipment monitoring, with domain-specific models and complex supply chain reasoning with foundation models.\nEnergy and utilities\nEnergy companies can deploy agents for grid operations, regulatory compliance, and infrastructure management. This approach supports near real-time demand forecasting with specialized models and complex environmental impact analysis with foundation models.\nConclusion\nWe showed how to build an agentic AI solution by integrating Hugging Face smolagents with AWS managed services. The solution demonstrates multi-model deployment options, vector-enhanced knowledge retrieval, and deployment capabilities so organizations can deploy domain-specific AI agents with AWS security features and compliance controls.\nThe healthcare use case illustrates how the model-agnostic design of smolagents supports deployment orchestration across Amazon SageMaker AI, Amazon Bedrock, and a containerized model server. Key technical innovations include the messages API compatibility across the backends, smolagents framework integration, and containerized deployment with AWS Fargate.\nThis solution architecture is extensible to financial services, manufacturing, energy, and other industries where domain-specific intelligence and reliability are critical.\nDiscuss a project and requirements to find the right help for your business needs with Hugging Face . Or if there are questions about getting started with AWS, speak with an AWS generative AI Specialist to learn how we can help accelerate your business today.\nFurther reading\n\nAgentic AI on AWS – Build, deploy, and scale AI agents with AWS\nGetting started with Amazon SageMaker AI\nVector search in Amazon OpenSearch Service\nDeploying Hugging Face models on Amazon SageMaker\n\nAbout the authors\nSanhita Sarkar , PhD, Global Partner Solutions, AI/ML, and generative AI at AWS. She drives AI/ML and generative AI partner solutions at AWS, with extensive leadership experience across edge, cloud, and data center environments. She holds several patents, has published research papers, and serves as chair for technical conferences.\nJeff Boudier , Head of Products at Hugging Face. Jeff was also a co-founder of Stupeflix, acquired by GoPro, where he served as director of product management, product marketing, business development, and corporate development.\nSimon Pagezy , Partner Success Manager at Hugging Face. He leads partnerships at Hugging Face with major cloud and hardware companies. He works to make the Hugging Face offerings broadly accessible across diverse deployment environments.\nFlorent Gbelidji , Cloud Partnership Tech Lead for the AWS account, driving integrations between the Hugging Face offerings and AWS services. He has also been an ML Engineer in the Expert Acceleration Program where he helped companies build solutions with open-source AI.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "3817c9dbb9b711f2",
    "title": "Why we no longer evaluate SWE-bench Verified",
    "url": "https://openai.com/index/why-we-no-longer-evaluate-swe-bench-verified",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T11:00:00Z",
    "summary": "SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.",
    "content": "SWE-bench Verified is increasingly contaminated and mismeasures frontier coding progress. Our analysis shows flawed tests and training leakage. We recommend SWE-bench Pro.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "264772bed11e3a29",
    "title": "OpenAI announces Frontier Alliance Partners",
    "url": "https://openai.com/index/frontier-alliance-partners",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-23T05:30:00Z",
    "summary": "OpenAI announces Frontier Alliance Partners to help enterprises move from AI pilots to production with secure, scalable agent deployments.",
    "content": "OpenAI announces Frontier Alliance Partners to help enterprises move from AI pilots to production with secure, scalable agent deployments.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]