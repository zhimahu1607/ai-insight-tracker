[
  {
    "id": "b9432c800297c765",
    "title": "Build reliable Agentic AI solution with Amazon Bedrock: Learn from Pushpay’s journey on GenAI evaluation",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-reliable-agentic-ai-solution-with-amazon-bedrock-learn-from-pushpays-journey-on-genai-evaluation/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-27T17:39:57Z",
    "summary": "In this post, we walk you through Pushpay's journey in building this solution and explore how Pushpay used Amazon Bedrock to create a custom generative AI evaluation framework for continuous quality assurance and establishing rapid iteration feedback loops on AWS.",
    "content": "This post was co-written wit h Saurabh Gupta and Todd Colby   from Pushpay.\nPushpay  is a market-leading digital giving and engagement platform designed to help churches and faith-based organizations drive community engagement, manage donations, and strengthen generosity fundraising processes efficiently. Pushpay’s church management system provides church administrators and ministry leaders with insight-driven reporting, donor development dashboards, and automation of financial workflows.\nUsing the power of generative AI, Pushpay developed an innovative agentic AI search feature built for the unique needs of ministries. The approach uses natural language processing so ministry staff can ask questions in plain English and generate real-time, actionable insights from their community data. The AI search feature addresses a critical challenge faced by ministry leaders: the need for quick access to community insights without requiring technical expertise. For example, ministry leaders can enter “show me people who are members in a group, but haven’t given this year” or “show me people who are not engaged in my church,” and use the results to take meaningful action to better support individuals in their community. Most community leaders are time-constrained and lack technical backgrounds; they can use this solution to obtain meaningful data about their congregations in seconds using natural language queries.\nBy empowering ministry staff with faster access to community insights, the AI search feature supports Pushpay’s mission to encourage generosity and connection between churches and their community members. Early adoption users report that this solution has shortened their time to insights from minutes to seconds. To achieve this result, the Pushpay team built the feature using agentic AI capabilities on Amazon Web Services (AWS) while implementing robust quality assurance measures and establishing a rapid iterative feedback loop for continuous improvements.\nIn this post, we walk you through Pushpay’s journey in building this solution and explore how Pushpay used  Amazon Bedrock to create a custom generative AI evaluation framework for continuous quality assurance and establishing rapid iteration feedback loops on AWS.\nSolution overview: AI powered search architecture\nThe solution consists of several key components that work together to deliver an enhanced search experience. The following figure shows the solution architecture diagram and the overall workflow.\n\nFigure 1: AI Search Solution Architecture\n\nUser interface layer: The solution begins with Pushpay users submitting natural language queries through the existing Pushpay application interface. By using natural language queries, church ministry staff can obtain data insights using AI capabilities without learning new tools or interfaces.\nAI search agent: At the heart of the system lies the AI search agent, which consists of two key components:\n\nSystem prompt : Contains the large language model (LLM) role definitions, instructions, and application descriptions that guide the agent’s behavior.\nDynamic prompt constructor (DPC) : automatically constructs additional customized system prompts based on the user specific information, such as church context, sample queries, and application filter inventory. They also use semantic search to select only relevant filters among hundreds of available application filters. The DPC improves response accuracy and user experience.\n\nAmazon Bedrock advanced feature:  The solution uses the following Amazon Bedrock managed services:\n\nPrompt caching : Reduces latency and costs by caching frequently used system prompt.\nLLM processing : Uses Claude Sonnet 4.5 to process prompts and generate JSON output required by the application to display the desired query results as insights to users.\n\nEvaluation system: The evaluation system implements a closed-loop improvement solution where user interactions are instrumented, captured and evaluated offline. The evaluation results feed into a dashboard for product and engineering teams to analyze and drive iterative improvements to the AI search agent. During this process, the data science team collects a golden dataset and continuously curates this dataset based on the actual user queries coupled with validated responses.\n\nThe challenges of initial solution without evaluation\nTo create the AI search feature, Pushpay developed the first iteration of the AI search agent. The solution implements a single agent configured with a carefully tuned system prompt that includes the system role, instructions, and how the user interface works with detailed explanation of each filter tool and their sub-settings. The system prompt is cached using Amazon Bedrock prompt caching to reduce token cost and latency. The agent uses the system prompt to invoke an Amazon Bedrock LLM which generates the JSON document that Pushpay’s application uses to apply filters and present query results to users.\nHowever, this first iteration quickly revealed some limitations. While it demonstrated a 60-70% success rate with basic business queries, the team reached an accuracy plateau. The evaluation of the agent was a manual and tedious process Tuning the system prompt beyond this accuracy threshold proved challenging given the diverse spectrum of user queries and the application’s coverage of over 100 distinct configurable filters. These presented critical blockers for the team’s path to production.\nFigure 2: AI Search First Solution\nImproving the solution by adding a custom generative AI evaluation framework\nTo address the challenges of measuring and improving agent accuracy, the team implemented a generative AI evaluation framework integrated into the existing architecture, shown in the following figure. This framework consists of four key components that work together to provide comprehensive performance insights and enable data-driven improvements.\n\nFigure 3: Introducing the GenAI Evaluation Framework\n\nThe golden dataset:  A curated golden dataset containing over 300 representative queries, each paired with its corresponding expected output, forms the foundation of automated evaluation. The product and data science teams carefully developed and validated this dataset to achieve comprehensive coverage of real-world use cases and edge cases. Additionally, there is a continuous curation process of adding representative actual user queries with validated results.\nThe evaluator:  The evaluator component processes user input queries and compares the agent-generated output against the golden dataset using the LLM as a judge pattern This approach generates core accuracy metrics while capturing detailed logs and performance data, such as latency, for further analysis and debugging.\nDomain category : Domain categories are developed using a combination of generative AI domain summarization and human-defined regular expressions to effectively categorize user queries. The evaluator determines the domain category for each query, enabling nuanced, category-based evaluation as an additional dimension of evaluation metrics.\nGenerative AI evaluation dashboard:  The dashboard serves as the mission control for Pushpay’s product and engineering teams, displaying domain category-level metrics to assess performance and latency and guide decisions. It shifts the team from single aggregate scores to nuanced, domain-based performance insights.\n\nThe accuracy dashboard: Pinpointing weaknesses by domain\nBecause user queries are categorized into domain categories, the dashboard incorporates statistical confidence visualization using a 95% Wilson score interval to display accuracy metrics and query volumes at each domain level. By using categories, the team can pinpoint the AI agent’s weaknesses by domain. In the following example , the “activity” domain shows significantly lower accuracy than other categories.\n\nFigure 4: Pinpointing Agent Weaknesses by Domain\nAdditionally, a performance dashboard, shown in the following figure, visualizes latency indicators at the domain category level, including latency distributions from p50 to p90 percentiles. In the following example, the activity domain exhibits notably higher latency than others.\n\nFigure 5: Identifying Latency Bottlenecks by Domain\nStrategic rollout through domain-Level insights\nDomain-based metrics revealed varying performance levels across semantic domains, providing crucial insights into agent effectiveness. Pushpay used this granular visibility to make strategic feature rollout decisions. By temporarily suppressing underperforming categories—such as activity queries—while undergoing optimization, the system achieved 95% overall accuracy. By using this approach, users experienced only the highest-performing features while the team refined others to production standards.\n\nFigure 6: Achieving 95% Accuracy with Domain-Level Feature Rollout\nStrategic prioritization: Focusing on high-impact domains\nTo prioritize improvements systematically, Pushpay employed a 2×2 matrix framework plotting topics against two dimensions (shown in the following figure): Business priority (vertical axis) and current performance or feasibility (horizontal axis). This visualization placed topics with both high business value and strong existing performance in the top-right quadrant. The team then focused on these areas because they required less heavy lifting to achieve further accuracy improvement from already-good levels to an exceptional 95% accuracy for the business focused topics.\nThe implementation followed an iterative cycle: after each round of enhancements, they re-analyze the results to identify the next set of high-potential topics. This systematic, cyclical approach enabled continuous optimization while maintaining focus on business-critical areas.\n\nFigure 7: Strategic Prioritization Framework for Domain Category Optimization\nDynamic prompt construction\nThe insights gained from the evaluation framework led to an architectural enhancement: the introduction of a dynamic prompt constructor. This component enabled rapid iterative improvements by allowing fine-grained control over which domain categories the agent could address. The structured field inventory – previously embedded in the system prompt – was transformed into a dynamic element, using semantic search to construct contextually relevant prompts for each user query. This approach tailors the prompt filter inventory based on three key contextual dimensions: query content, user persona, and tenant-specific requirements. The result is a more precise and efficient system that generates highly relevant responses while maintaining the flexibility needed for continuous optimization.\nBusiness impact\nThe generative AI evaluation framework became the cornerstone of Pushpay’s AI feature development, delivering measurable value across three dimensions:\n\nUser experience : The AI search feature reduced time-to-insight from approximately 120 seconds (experienced users manually navigating complex UX) to under 4 seconds – a 15-fold acceleration that directly helps enhance ministry leaders’ productivity and decision-making speed. This feature democratized data insights, so that users of different technical levels can access meaningful intelligence without requiring specialized expertise.\nDevelopment velocity : The scientific evaluation approach transformed optimization cycles. Rather than debating prompt modifications, the team now validates changes and measures domain-specific impacts within minutes, replacing prolonged deliberations with data-driven iteration.\nProduction readiness : Improvements from 60–70% accuracy to more than 95% accuracy using high-performance domains provided the quantitative confidence required for customer-facing deployment, while the framework’s architecture enables continuous refinement across other domain categories.\n\nKey takeaways for your AI agent journey\nThe following are key takeaways from Pushpay’s experience that you can use in your own AI agent journey.\n1/ Build with production in mind from day one\nBuilding agentic AI systems is straightforward, but scaling them to production is challenging. Developers should adopt a scaling mindset during the proof-of-concept phase, not after. Implementing robust tracing and evaluation frameworks early, provides a clear pathway from experimentation to production. By using this method, teams can identify and address accuracy issues systematically before they become blockers.\n2/ Take advantage of the advanced features of Amazon Bedrock\nAmazon Bedrock prompt caching significantly reduces token costs and latency by caching frequently used system prompts. For agents with large, stable system prompts, this feature is essential for production-grade performance.\n3/ Think beyond aggregate metrics\nAggregate accuracy scores can sometimes mask critical performance variations. By evaluating agent performance at the domain category level, Pushpay uncovered weaknesses beyond what a single accuracy metric can capture. This granular approach enables targeted optimization and informed rollout decisions, making sure users only experience high-performing features while others are refined.\n4/ Data security and responsible AI\nWhen developing agentic AI systems, consider information protection and LLM security considerations from the outset, following the AWS Shared Responsibility Model , because security requirements fundamentally impact the architectural design. Pushpay’s customers are churches and faith-based organizations who are stewards of sensitive information—including pastoral care conversations, financial giving patterns, family struggles, prayer requests and more. In this implementation example, Pushpay set a clear approach to incorporating AI ethically within its product ecosystem, maintaining strict security standards to ensure church data and personally identifiable information (PII) remains within its secure partnership ecosystem. Data is shared only with secure and appropriate data protections applied and is never used to train external models. To learn more about Pushpay’s standards for incorporating AI within their products, visit the Pushpay Knowledge Center for a more in-depth review of company standards.\nConclusion: Your Path to Production-Ready AI Agents\nPushpay’s journey from a 60–70% accuracy prototype to a 95% accurate production-ready AI agent demonstrates that building reliable agentic AI systems requires more than just sophisticated prompts—it demands a scientific, data-driven approach to evaluation and optimization. The key breakthrough wasn’t in the AI technology itself, but in implementing a comprehensive evaluation framework built on strong observability foundation that provided granular visibility into agent performance across different domains. This systematic approach enabled rapid iteration, strategic rollout decisions, and continuous improvement.\nReady to build your own production-ready AI agent?\n\nExplore Amazon Bedrock : Begin building your agent with Amazon Bedrock\nImplement LLM-as-a-judge : Create your own evaluation system using the patterns described in this LLM-as-a-judge on Amazon Bedrock Model Evaluation\nBuild your golden dataset : Start curating representative queries and expected outputs for your specific use case\n\nAbout the authors\nRoger Wang is a Senior Solution Architect at AWS. He is a seasoned architect with over 20 years of experience in the software industry. He helps New Zealand and global software and SaaS companies use cutting-edge technology at AWS to solve complex business challenges. Roger is passionate about bridging the gap between business drivers and technological capabilities and thrives on facilitating conversations that drive impactful results.\nMelanie Li, PhD, is a Senior Generative AI Specialist Solutions Architect at AWS based in Sydney, Australia, where her focus is on working with customers to build solutions leveraging state-of-the-art AI and machine learning tools. She has been actively involved in multiple Generative AI initiatives across APJ, harnessing the power of Large Language Models (LLMs). Prior to joining AWS, Dr. Li held data science roles in the financial and retail industries.\nFrank Huang , PhD, is a Senior Analytics Specialist Solutions Architect at AWS based in Auckland, New Zealand. He focuses on helping customers deliver advanced analytics and AI/ML solutions. Throughout his career, Frank has worked across a variety of industries such as financial services, Web3, hospitality, media and entertainment, and telecommunications. Frank is eager to use his deep expertise in cloud architecture, AIOps, and end-to-end solution delivery to help customers achieve tangible business outcomes with the power of data and AI.\nSaurabh Gupta is a data science and AI professional at Pushpay based in Auckland, New Zealand, where he focuses on implementing practical AI solutions and statistical modeling. He has extensive experience in machine learning, data science, and Python for data science applications, with specialized experience training in database agents and AI implementation. Prior to his current role, he gained experience in telecom, retail and financial services, developing expertise in marketing analytics and customer retention programs. He has a Master’s in Statistics from University of Auckland and a Master’s in Business Administration from the Indian Institute of Management, Calcutta.\nTodd Colby is a Senior Software Engineer at Pushpay based in Seattle. His expertise is focused on evolving complex legacy applications with AI, and translating user needs into structured, high-accuracy solutions. He leverages AI to increase delivery velocity and produce cutting edge metrics and business decision tools.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "Pushpay 使用 Amazon Bedrock 构建了可靠的代理式 AI 搜索解决方案，专为教堂和宗教组织设计。该功能利用自然语言处理，使非技术用户能用英语查询社区数据，快速生成可操作的洞察，解决了教堂领袖获取实时社区信息的技术障碍。初始版本准确率为 60-70%，面临瓶颈后，Pushpay 实施了自定义的生成式 AI 评估框架，包括黄金数据集、LLM 作为裁判的模式、域分类和仪表板，将准确率提升至 95%。关键技术改进涉及动态提示构造和提示缓存，使用 Claude Sonnet 4.5 LLM。影响包括用户洞察时间从 120 秒缩短到 4 秒，开发迭代加速，并实现生产就绪部署，提升了教堂管理的效率和决策速度。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "Pushpay",
        "Amazon Bedrock",
        "Claude Sonnet 4.5",
        "Agentic AI",
        "Generative AI evaluation framework"
      ]
    },
    "analyzed_at": "2026-01-28T03:44:28.779169Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "23fef0003888d956",
    "title": "UniRG: Scaling medical imaging report generation with multimodal reinforcement learning",
    "url": "https://www.microsoft.com/en-us/research/blog/unirg-scaling-medical-imaging-report-generation-with-multimodal-reinforcement-learning/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-27T17:00:00Z",
    "summary": "AI can help generate medical image reports, but today’s models struggle with varying reporting schemes. Learn how UniRG uses reinforcement learning to boost performance of medical vision-language models.\nThe post UniRG: Scaling medical imaging report generation with multimodal reinforcement learning appeared first on Microsoft Research .",
    "content": "At a glance\n\nAI-driven medical image report generation can help medical providers become more efficient and productive.\n\nCurrent models are difficult to train because reporting practices vary widely among providers.\n\nUniversal Report Generation (UniRG) uses reinforcement learning to align model training with real-world radiology practice rather than proxy text-generation objectives.\n\nUniRG has achieved state-of-the-art performance across datasets, metrics, diagnostic tasks, longitudinal settings, and demographic subgroups.\n\nTest results show that reinforcement learning, guided by clinically meaningful reward signals, can substantially improve the reliability and generality of medical vision–language models.\n\nAI can be used to produce clinically meaningful radiology reports using medical images like chest x-rays. Medical image report generation can reduce reporting burden while improving workflow efficiency for healthcare professionals. Beyond the real-world benefits, report generation has also become a critical benchmark for evaluating multimodal reasoning in healthcare AI.\n\nDespite recent advances driven by large vision–language models, current systems still face major limitations in real-world clinical settings. One challenge stems from the wide variation in radiology reporting practices across institutions, departments, and patient populations. A model trained with supervised fine-tuning on one set of data may learn its specific phrasing and conventions instead of more general patterns—a problem known as overfitting . As a result, the model performs well on that data but delivers poor results when evaluated on unseen institutions or external datasets. Moreover, since model training is often aimed at producing text that looks similar to existing reports, some well written but clinically inaccurate reports can slip through.\n\nIn this blog, we introduce Universal Report Generation (UniRG) (opens in new tab) , a reinforcement learning–based framework for medical imaging report generation. This work is a research prototype intended to advance medical AI research and is not validated for clinical use. UniRG uses reinforcement learning as a unifying mechanism to directly optimize clinically grounded evaluation signals, aligning model training with real-world radiology practice rather than proxy text-generation objectives. Using this framework, we train UniRG-CXR (opens in new tab) , a state-of-the-art chest x-ray report generation model at scale, spanning over 560,000 studies, 780,000 images, and 226,000 patients from more than 80 medical institutions.\n\nTo our knowledge, this is the first report generation model to achieve consistent state-of-the-art performance across report-level metrics, disease-level diagnostic accuracy, cross-institution generalization, longitudinal report generation, and demographic subgroups. These results demonstrate that reinforcement learning, when guided by clinically meaningful reward signals, can substantially improve both the reliability and generality of medical vision–language models.\n\nAzure AI Foundry Labs\n\nGet a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.\n\nAzure AI Foundry\n\nOpens in a new tab\n\nA unified framework for scaling medical image report generation\n\nUniRG builds state-of-the-art report generation models by combining supervised fine-tuning with reinforcement learning, which optimizes a composite reward that integrates rule-based metrics, model-based semantic metrics, and LLM-based clinical error signals. This approach allows the resulting model UniRG-CXR to learn from diverse data sources, move beyond dataset-specific reporting patterns, and learn representations that generalize across institutions, metrics, and clinical contexts. Notably, UniRG-CXR sets a new state of the art on the authoritative  ReXrank leaderboard (opens in new tab) , a public leaderboard for chest X-ray image interpretation, as of 01/22/2026, surpassing previous best models by substantial margins (Figure 1).\n\nFigure 1. Overview of UniRG-CXR. (a) Training Data: UniRG-CXR is trained on the training splits of MIMIC-CXR, CheXpert Plus, and ReXGradient-160k, covering diverse institutions and patient demographics. (b) Training and Rewards: Taking input from the current image, clinical context (e.g., indication), and optionally prior studies, UniRG-CXR uses GRPO reinforcement learning to optimize composite rewards that combine rule-based, model-based, and LLM-based metrics. (c) Evaluation: We assess UniRG-CXR on held-out test sets (MIMIC-CXR, CheXpert Plus, ReXGradient), and unseen datasets (IU Xray and proprietary data). Report quality measured using ReXrank metrics and an LLM-based clinical-error metric, while diagnostic ability is evaluated via F1-based disease classification from generated reports. (d) ReXrank Results: UniRG-CXR achieves SOTA performance across four datasets and two generation settings (findings only and findings + impression), showing substantial gains over prior state-of-the-art systems.\n\nUniversal improvements across metrics and clinical errors\n\nRather than excelling on one metric at the expense of others, UniRG-CXR delivers balanced improvements across many different measures of report quality. More importantly, it produces reports with substantially fewer clinically significant errors. This indicates that the model is not just learning how to sound like a radiology report, but is better capturing the underlying clinical facts. Explicitly optimizing for clinical correctness helps the model avoid common failure modes where fluent language masks incorrect or missing findings (Figure 2).\n\nFigure 2. UniRG-CXR achieves state-of-the-art performance, delivering consistent and comprehensive performance gains across metrics. (a) On the ReXrank leaderboard, UniRG-CXR (green) shows robust, universal improvement across all evaluation metrics.  (b). Starting from the same SFT checkpoint, RL with our combined reward achieves more balanced gains across metrics and the highest RadCliQ-v1 score compared to RL on single metrics. This ablation study is trained and tested on MIMIC (c). Ablation study on the training dynamics shows RL full (UniRG-CXR) achieves significantly better RadCliQ-v1 score than RL only on BLEU. (d). During training, RL full (UniRG-CXR) shows a steady decrease in clinical errors per report as compared with a fluctuating trajectory without consistent improvement from an ablation run without error awareness (i.e. removing CheXprompt metric optimization). Both (c) and (d) show results on 1024 MIMIC validation set from ablations that are trained on MIMIC. (e). Case studies illustrate that UniRG-CXR can produce error-free reports, unlike MedVersa and MedGemma. (f). UniRG-CXR yields a substantially higher proportion of reports with $\\leq 1$ error and fewer with $\\geq 4$ errors than prior models.\n\nStrong performance in longitudinal report generation\n\nIn clinical practice, radiologists often compare current images with prior exams to determine whether a condition is improving, worsening, or unchanged. UniRG-CXR is able to incorporate this historical information effectively, generating reports that reflect meaningful changes over time. This allows the model to describe new findings, progression, or resolution of disease more accurately, moving closer to how radiologists reason across patient histories rather than treating each exam in isolation (Figure 3).\n\nFigure 3. UniRG-CXR enhances longitudinal report generation. (a). Comparing UniRG-CXR and its non-longitudinal ablation with prior models on longitudinal report generation, we show UniRG-CXR exhibits the best performance and the longitudinal information is beneficial to the performance. (b). UniRG-CXR achieves the best performance across different longitudinal encounter points ranging from the first encounter to the more complex 5th+ encounters, showcasing its improvements are across the board. In comparison, prior models such as GPT-5, GPT-4o and MedGemma are barely surpassing the copy prior report baseline (grey lines).  (c). Compared with prior models which barely improve over the copy prior baseline (dashed line), UniRG-CXR significantly and consistently improves performance across different temporal disease change categories including new development, no change, progression and regression (categorized by GPT-5 on ground truth report). Qualitative examples are shown for each category where UniRG-CXR correctly predicts the temporal change based on the input. All results in this figure are on MIMIC test set with prior information where available.\n\nRobust generalization across institutions and populations\n\nUniRG-CXR maintains strong performance even when applied to data from institutions it has never seen before. This suggests that the model is learning general clinical patterns rather than memorizing institution-specific reporting styles. In addition, its performance remains stable across different patient subgroups, including age, gender, and race. This robustness is critical for real-world deployment, where models must perform reliably across diverse populations and healthcare environments (Figure 4).\n\nFigure 4. Generalization and robustness of UniRG-CXR. (a). We evaluate UniRG-CXR in a zero-shot setting on two datasets from previously unseen institutions: IU-Xray and PD (proprietary data). UniRG-CXR consistently outperforms prior models, maintaining substantial performance gains in this challenging setup. (b) and (c) present condition-level F1 scores on MIMIC-CXR and PD and highlight that UniRG-CXR remains the overall top-performing model in condition-level diagnostic accuracy. (d). UniRG-CXR demonstrates stable and robust performance across gender, age, and race subgroups, all of which exceed the performance of the second-best model (the dashed lines).\n\nUniRG is a promising step toward scaling medical imaging report generation\n\nUniRG introduces a reinforcement learning–based framework that rethinks how medical imaging report generation models are trained and evaluated. By directly optimizing clinically grounded reward signals, UniRG-CXR achieves state-of-the-art performance across datasets, metrics, diagnostic tasks, longitudinal settings, and demographic subgroups, addressing longstanding limitations of supervised-only approaches.\n\nLooking ahead, this framework can be extended to additional imaging modalities and clinical tasks, and combined with richer multimodal patient data such as prior imaging, laboratory results, and clinical notes. More broadly, UniRG highlights the promise of reinforcement learning as a core component of next-generation medical foundation models that are robust, generalizable, and clinically aligned.\n\nUniRG reflects Microsoft’s larger commitment to advancing multimodal generative AI for precision health (opens in new tab) , with other exciting progress such as GigaPath , BiomedCLIP , LLaVA-Rad (opens in new tab) , BiomedJourney , BiomedParse , TrialScope , Curiosity .\n\nPaper co-authors: Qianchu Liu , Sheng Zhang , Guanghui Qin , Yu Gu , Ying Jin, Sam Preston, Yanbo Xu, Sid Kiblawi, Wen-wai Yim , Tim Ossowski, Tristan Naumann , Mu Wei, Hoifung Poon\nOpens in a new tab The post UniRG: Scaling medical imaging report generation with multimodal reinforcement learning appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": {
      "summary": "Microsoft Research 的研究人员开发了 UniRG（Universal Report Generation），一个基于强化学习的医学影像报告生成框架。该框架通过结合监督微调和强化学习，优化了包括规则、模型和 LLM 指标的复合奖励，训练出 UniRG-CXR 模型，用于胸部 X 光报告生成。UniRG-CXR 在多个数据集、指标和临床任务中取得了最先进性能，解决了现有模型过度拟合和临床准确性不足的问题，能泛化到未见过的机构、人口统计组，并支持纵向报告生成。这项工作提高了医疗 AI 的可靠性和泛化性，有望帮助医疗提供者提高效率、减少报告负担。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "Microsoft",
        "UniRG",
        "UniRG-CXR",
        "强化学习",
        "医学影像报告生成"
      ]
    },
    "analyzed_at": "2026-01-28T03:44:27.414661Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "6e9a27a6f0669d51",
    "title": "Build an intelligent contract management solution with Amazon Quick Suite and Bedrock AgentCore",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-an-intelligent-contract-management-solution-with-amazon-quick-suite-and-bedrock-agentcore/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-27T16:28:16Z",
    "summary": "This blog post demonstrates how to build an intelligent contract management solution using Amazon Quick Suite as your primary contract management solution, augmented with Amazon Bedrock AgentCore for advanced multi-agent capabilities.",
    "content": "Organizations managing hundreds of contracts annually face significant inefficiencies, with fragmented systems and complex workflows that require teams to spend hours on contract review cycles. This solution addresses these challenges through multi-agent collaboration—specialized AI agents that can work simultaneously on different aspects of contract analysis, reducing cycle times while maintaining accuracy and oversight.\nThis guide demonstrates how to build an intelligent contract management solution using Amazon Quick Suite as your primary contract management solution, augmented with Amazon Bedrock AgentCore for advanced multi-agent capabilities.\nWhy Quick Suite augmented with Amazon Bedrock AgentCore\nQuick Suite serves as your agentic workspace, providing a unified interface for chat, research, business intelligence, and automation. Quick Suite helps you seamlessly transition from getting answers to taking action, while also automating tasks from routine daily activities to complex business processes such as contract processing and analysis.\nBy using Amazon Bedrock AgentCore with Quick Suite, you can encapsulate business logic in highly capable AI agents more securely at scale. AgentCore services work with many frameworks including Strands Agents , in addition to foundation models in or outside of Amazon Bedrock.\nSolution overview\nThis solution demonstrates an intelligent contract management system using Quick Suite as the user interface and knowledge base, with Amazon Bedrock AgentCore providing multi-agent collaboration functionality. The system uses specialized agents to analyze contracts, assess risks, evaluate compliance, and provide structured insights through a streamlined architecture, shown in the following figure.\n\nArchitecture components\nThe components of the solution architecture include:\n\nQuick Suite components:\n\nSpaces for contract management workflows\nChat agents for conversational contract interactions\nKnowledge bases for integrating legal documents stored in Amazon S3\nTopics for integrating structured contract data\nActions for connecting to custom agents developed with Amazon Bedrock AgentCore\nFlows for recurring semi-manual document review processes\nAutomate for daily and monthly contract automation tasks\n\nMulti-agent system powered by AgentCore:\n\nContract collaboration agent : Central orchestrator coordinating workflow\nLegal agent : Analyzes legal terms and extracts key obligations\nRisk agent : Assesses financial and operational risks\nCompliance agent : Evaluates regulatory compliance\n\nSupporting infrastructure:\n\nAmazon API Gateway and AWS Lambda for managing API requests\nAmazon Simple Storage Service (Amazon S3) for document storage\nAmazon Redshift for structured data\n\nContract management workflow\nThe solution implements a streamlined contract management workflow that significantly reduces processing time while improving accuracy. The system processes contracts through coordinated AI agents, typically completing analysis within minutes compared to days of manual review.\n\nAgent type\nPrimary function\nKey outputs\n\nContract collaboration agent\nCentral orchestrator and workflow manager\nDocument routing decisions, and consolidated results\n\nLegal agent\nLegal term analysis and obligation extraction\nParty details, key terms, obligations, and risk flags\n\nRisk agent\nFinancial and operational risk assessment\nRisk scores, exposure metrics, and negotiation recommendations\n\nCompliance agent\nRegulatory compliance evaluation\nCompliance status, regulatory flags, and remediation suggestions\n\nLet’s explore an example of processing a sample service agreement contract. The workflow consists of the following steps:\n\nThe contract collaboration agent identifies the document as requiring legal, risk, and compliance analysis.\nThe legal agent extracts parties, payment terms, and obligations.\nThe risk agent identifies financial exposure and negotiation leverage points.\nThe compliance agent evaluates regulatory requirements and flags potential issues.\nThe contract collaboration agent consolidates findings into a comprehensive report.\n\nPrerequisites\nBefore setting up Quick Suite, make sure you have:\n\nAn AWS account with administrative permissions\nAccess to supported AWS Regions where Quick Suite is available\nAppropriate AWS Identity and Access Management (IAM) roles and policies for Quick Suite service access\n\nSetup part 1: Set up Quick Suite\nIn the following steps we set up the Quick Suite components.\nEnable Quick Suite\nYour AWS administrator can enable Quick Suite by:\n\nSigning in to the AWS Management Console\nNavigating to Quick Suite from the console\nSubscribing to Quick Suite service for your organization\nConfiguring identity and access management as needed\n\nAfter Quick Suite is enabled, navigate to the Amazon Quick Suite web interface and sign in with your credentials.\nCreate the contract management space\nIn Quick Suite, create a new space called Contract Management to organize your contract-related workflows and resources. You can then use the assistant on the right to ask queries about the resources in the space. The following figure shows the initial space.\n\nSet up a knowledge base for unstructured data (Amazon S3)\nFollow these steps:\n\nNavigate to Knowledge bases : In the Integrations section, select Knowledge bases .\nAdd Amazon S3 integration:\n\nSelect Amazon S3 as your data source.\nConfigure the S3 bucket that will store your contract documents.\nAfter the knowledge base is created, add it to the Contract Management space.\n\nSet up a knowledge base for structured data (Amazon Redshift)\nFollow these steps:\n\nAdd dataset : In the Datasets section, configure your contract data warehouse (Amazon Redshift) for structured contract data. Follow these instructions in Creating a dataset from a database and wait until your dataset is configured.\nAdd data topics : In the Topics section, integrate structured contract data sources such as:\n\nContract databases\nVendor information systems\nCompliance tracking systems\n\nFor adding topics in Quick Suite, see Adding datasets to a topic in Amazon Quick Sight .\n\nAdd topics to your space: Add the relevant topics to your Contract Management space.\n\nSetup part 2: Deploy Amazon Bedrock AgentCore\nAmazon Bedrock AgentCore provides enterprise-grade infrastructure for deploying AI agents with session isolation, where each session runs with isolated CPU, memory, and filesystem resources. This creates separation between user sessions, helping to safeguard stateful agent reasoning processes.\n\nYou can find the required code in this GitHub repository . Go to the subfolder legal-contract-solution/deployment .\nThe solution includes a comprehensive deploy_agents.py script that handles the complete deployment of the AI agents to AWS using cloud-centered builds. These instructions require Python>=3.10 .\n\npip3 install -r requirements.txt\npython3 deploy_agents.py\n\nWhat the deployment script does\nThe deployment process is fully automated and handles:\n\nDependency management :\n\nAutomatically installs bedrock-agentcore-starter-toolkit if needed\nVerifies the required Python packages are available\n\nAWS infrastructure setup :\n\nCreates IAM roles with the necessary permissions for agent execution\nSets up Amazon Elastic Container Registry (Amazon ECR) repository for container images\nConfigures Amazon CloudWatch logging for monitoring\n\nAgent deployment :\n\nDeploys four specialized agents\nUses AWS CodeBuild for cloud-centered ARM64 container builds\nNo local Docker required—the builds happen in AWS infrastructure\n\nConfiguration management :\n\nAutomatically configures agent communication protocols\nSets up security boundaries between agents\nEstablishes monitoring and observability\n\nAfter the agents are deployed, you can see them in the Amazon Bedrock AgentCore console, as shown in the following figure.\n\nSetup part 3: Integrate Amazon Bedrock AgentCore with Quick Suite\nQuick Suite can connect to enterprise solutions and agents through actions integrations, making tools available to chat agents and automation workflows.\nDeploy API Gateway and Lambda \nGo to the subfolder legal-contract-solution/deployment and run the following command: python3 deploy_quicksuite_integration.py\nThis will provision Amazon Cognito with a user pool to permission access to the API Gateway endpoint. The Quick Suite configuration references the OAuth details for this user pool. After successful deployment, two files will be generated for your Quick Suite integration:\n\nquicksuite_integration_config.json – Complete configuration\nquicksuite_openapi_schema.json – OpenAPI schema for Quick Suite import\n\nSet up actions integration in Quick Suite\nIn the Actions section, prepare the integration points that will connect to your agents deployed by AgentCore:\n\nGet the OpenAPI specification file quicksuite_openapi_schema.json from the working folder.\nIn the Integrations/Actions section, go to OpenAPI Specification . Create a new OpenAPI integration by uploading the api_gateway_openapi_schema.json  file, and enter the following Name and Description for the provided agents. Enter the endpoint with the URL by using the information from the quicksuite_integration_config.json  file.\n\nName : Legal Contract Analyzer\nDescription : Analyze a legal contract using AI agents for clause extraction, risk assessment, and compliance checking\n\nSet up chat agent definition details\nIn the Chat agents section, set up the following agent and enter the following details:\n\nName : Legal Contract AI Analyzer\nDescription :\n\nAn AI-powered system that analyzes legal contracts and performs comprehensive risk\nassessments using advanced machine learning capabilities to identify potential issues,\ncompliance gaps, and contractual risks.\n\nAgent identity:\n\nYou are an expert legal contract analysis AI system powered by advanced GenAI\ncapabilities. Your purpose is to provide comprehensive contract review and risk\nassessment services.\n\nPersona instructions:\n\nUse the legal contract analyzer when possible. Always categorize risks by\nseverity (High, Medium, Low). Highlight non-standard clauses, missing provisions,\nand potential compliance issues. Provide specific recommendations for contract improvements.\nWhen analyzing liability clauses, pay special attention to indemnification, limitation of\nliability, and force majeure provisions. Flag any unusual termination conditions or intellectual\nproperty concerns.\n\nCommunication style:   Professional, precise, and analytical with clear legal terminology.\nResponse format: \n\nProvide structured analysis with clear risk categorization, severity levels, and actionable\nrecommendations. Use bullet points for key findings and numbered lists for prioritized recommendations.\n\nLength: \n\nComprehensive analysis covering all critical aspects while maintaining clarity and focus on actionable insights.\n\nWelcome message: \n\nWelcome to the Legal Contract AI Analyzer. Upload contracts for intelligent analysis and risk assessment.\n\nSuggested prompts: \n\nAnalyze this contract for potential legal risks and compliance issues\nReview the liability clauses in this agreement for red flags\nAssess the termination conditions and notice requirements in this contract\n\nTest your contract management solution\nNow that you’ve deployed the infrastructure and configured Quick Suite, you can test the contract management solution by selecting the Contract Management space. You can use the agent interface to ask questions about the knowledge base and instruct agents to review the documents. Your space will look like the following figure:\nClean up\nThere are associated infrastructure costs with the deployed solution. Once you no longer need it in your AWS account, you can go to the subfolder legal-contract-solution/deployment and run the following command for clean up: python3 cleanup.py\nConclusion\nThe combination of Amazon Quick Suite and Amazon Bedrock AgentCore offers procurement and legal teams immediate operational benefits while positioning them for future AI advancements. You can use Amazon Bedrock multi-agent collaboration to build and manage multiple specialized agents that work together to address increasingly complex business workflows. By implementing this intelligent contract management solution, you can transform your organization’s procurement processes, reduce contract cycle times, and enable your teams to focus on strategic decision-making rather than administrative tasks. Because of the solution’s extensible architecture, you can start with core contract management functions and gradually expand to address more complex use cases as your organization’s needs evolve. Whether you’re looking to streamline routine contract reviews or implement comprehensive procurement transformation, the intelligent contract management solution provides a powerful foundation for achieving your business objectives. To learn more about Amazon Quick Suite and Amazon Bedrock AgentCore, see:\n\nAmazon Quick Suite\nAmazon Bedrock AgentCore\n\nAbout the authors\nOliver Steffmann is a Principal Solutions Architect at AWS based in New York and is passionate about GenAI and public blockchain use cases. He has over 20 years of experience working with financial institutions and helps his customers get their cloud transformation off the ground. Outside of work he enjoys spending time with his family and training for the next Ironman.\nDavid Dai is an Enterprise Solutions Architect at AWS based in New York. He works with customers across various industries, helping them design and implement cloud solutions that drive business value. David is passionate about cloud architecture and enjoys guiding organizations through their digital transformation journeys. Outside of work, he values spending quality time with family and exploring the latest technologies.\nKrishna Pramod is a Senior Solutions Architect at AWS. He works as a trusted advisor for customers, guiding them through innovation with modern technologies and development of well-architected applications in the AWS cloud. Outside of work, Krishna enjoys reading, music and exploring new destinations.\nMalhar Mane is an Enterprise Solutions Architect at AWS based in Seattle, where he serves as a trusted advisor to enterprise customers across diverse industries. With a deep passion for Generative AI and storage solutions, Malhar specializes in guiding organizations through their cloud transformation journeys and helping them harness the power of generative AI to optimize business operations and drive innovation. Malhar holds a Bachelor’s degree in Computer Science from the University of California, Irvine. In his free time, Malhar enjoys hiking and exploring national parks.\nPraveen Panati is a Senior Solutions Architect at Amazon Web Services. He is passionate about cloud computing and works with AWS enterprise customers to architect, build, and scale cloud-based applications to achieve their business goals. Praveen’s area of expertise includes cloud computing, big data, streaming analytics, and software engineering.\nSesan Komaiya is a Solutions Architect at Amazon Web Services. He works with a variety of customers, helping them with cloud adoption, cost optimization and emerging technologies. Sesan has over 15 year’s experience in Enterprise IT and has been at AWS for 5 years. In his free time, Sesan enjoys watching various sporting activities like Soccer, Tennis and Moto sport. He has 2 kids that also keeps him busy at home.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "AWS发布指南，展示如何利用Amazon Quick Suite和Amazon Bedrock AgentCore构建智能合同管理解决方案。该方案针对企业合同管理效率低下问题，通过多代理协作技术部署四个专业代理：合同协作代理作为协调中心，法律代理分析条款和提取义务，风险代理评估财务和运营风险，合规代理检查法规遵循。结合Quick Suite作为用户界面和知识库，以及AgentCore提供多代理功能，该系统可将合同审核时间从数天缩短至几分钟，提高准确性并减少人工成本。这不仅有助于企业优化采购流程、降低周期时间，还使团队能更专注于战略决策，是AI在企业应用中的典型范例。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon Quick Suite",
        "Amazon Bedrock AgentCore",
        "multi-agent collaboration",
        "contract management",
        "AI agents"
      ]
    },
    "analyzed_at": "2026-01-28T03:44:21.147007Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "241c9fd7457d2cb7",
    "title": "PVH reimagines the future of fashion with OpenAI",
    "url": "https://openai.com/index/pvh-future-of-fashion",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-27T06:00:00Z",
    "summary": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
    "content": "PVH Corp., parent company of Calvin Klein and Tommy Hilfiger, is adopting ChatGPT Enterprise to bring AI into fashion design, supply chain, and consumer engagement.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "PVH 公司（PVH Corp.）作为 Calvin Klein 和 Tommy Hilfiger 的母公司，宣布采用 OpenAI 的 ChatGPT Enterprise 人工智能解决方案。这一举措旨在将先进的大语言模型技术融入时尚行业，具体应用于时尚设计创新、供应链管理优化和消费者互动增强。通过集成 ChatGPT Enterprise，PVH 能够利用 AI 的自然语言处理能力来提升设计效率、实现供应链智能化，并提供个性化消费者服务。这不仅展示了 AI 在传统行业的应用潜力，还标志着时尚行业向数字化和智能化转型的重要一步，可能引领行业未来发展方向，推动创新和竞争力提升。",
      "category": "LLM",
      "sentiment": "positive",
      "keywords": [
        "PVH 公司",
        "ChatGPT Enterprise",
        "OpenAI",
        "人工智能",
        "时尚设计"
      ]
    },
    "analyzed_at": "2026-01-28T03:44:50.800479Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "2d79546ea8ed8501",
    "title": "Introducing Prism",
    "url": "https://openai.com/index/introducing-prism",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-27T00:00:00Z",
    "summary": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
    "content": "Prism is a free LaTeX-native workspace with GPT-5.2 built in, helping researchers write, collaborate, and reason in one place.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI 推出了 Prism，这是一个免费的 LaTeX 原生工作空间，内置了 GPT-5.2。该产品旨在帮助研究人员在一个地方进行写作、协作和推理。通过集成 LaTeX 编辑功能和强大的语言模型，Prism 有望简化研究过程并促进合作。作为免费工具，它可能对学术社区产生积极影响，提高研究效率。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Prism",
        "LaTeX",
        "GPT-5.2"
      ]
    },
    "analyzed_at": "2026-01-28T03:44:26.130924Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]