[
  {
    "id": "9424761a3771f113",
    "title": "How AutoScout24 built a Bot Factory to standardize AI agent development with Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-autoscout24-built-a-bot-factory-to-standardize-ai-agent-development-with-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-14T21:24:09Z",
    "summary": "In this post, we explore the architecture that AutoScout24 used to build their standardized AI development framework, enabling rapid deployment of secure and scalable AI agents.",
    "content": "AutoScout24 is Europe’s leading automotive marketplace platform that connects buyers and sellers of new and used cars, motorcycles, and commercial vehicles across several European countries. Their long-term vision is to build a Bot Factory, a centralized framework for creating and deploying artificial intelligence (AI) agents that can perform tasks and make decisions within workflows, to significantly improve operational efficiency across their organization.\nFrom disparate experiments to a standardized framework\nAs generative AI agents (systems that can reason, plan, and act) become more powerful, the opportunity to improve internal productivity for AutoScout24 was clear. This led to various engineering teams experimenting with the technology. As AI innovation accelerated across AutoScout24, they recognized an opportunity to pioneer a standardized approach for AI development. While AutoScout24 had successfully experimented with various tools and frameworks on Amazon Web Services (AWS), they envisioned creating a unified, enterprise-grade framework that could enable faster innovation. Their goal was to establish a paved path that could make it easier for teams across the organization to build secure, scalable, and maintainable AI agents. The AutoScout24 AI Platform Engineering team partnered with the AWS Prototype and Cloud Engineering (PACE) team in a three-week AI bootcamp. The goal was to move from fragmented experiments to a coherent strategy by creating a reusable blueprint, a Bot Factory, to standardize how future AI agents are built and operated within their company.\nThe challenge: identifying a high-impact use case\nTo ground the Bot Factory blueprint in a tangible business case, the team targeted a significant operational cost: internal developer support. The problem was well-defined. AutoScout24 AI Platform engineers were spending up to 30% of their time on repetitive tasks like answering questions, granting access to tools, and locating documentation. This support tax reduced overall productivity. It diverted skilled engineers from high-priority feature development and forced other developers to wait for routine requests to be completed. An automated support bot was an ideal first use case because it needed to perform two core agent functions:\n\nKnowledge retrieval: Answering “how-to” questions by searching internal documentation, a capability known as Retrieval Augmented Generation (RAG).\nAction execution: Performing tasks in other systems, such as assigning a GitHub Copilot license, which requires secure API integration, or “tool use.”\n\nBy building a bot that could do both, the team could validate the blueprint while delivering immediate business value.\nArchitectural overview\nIn this post, we explore the architecture that AutoScout24 used to build their standardized AI development framework, enabling rapid deployment of secure and scalable AI agents.\n\nThe architecture is designed with a simple, decoupled flow to make sure the system is both resilient and straightforward to maintain. The diagram provides a simplified view focused on the core generative-AI workflow. In a production environment, additional AWS services such as AWS Identity and Access Management (IAM) , Amazon CloudWatch , AWS X-Ray , AWS CloudTrail , AWS Web Application Firewall (WAF) , and AWS Key Management Service (KMS) could be integrated to enhance security, observability, and operational governance.\nHere is how a request flows through the system:\n\nUser interaction via Slack: A developer posts a message in a support channel, for example, “@SupportBot, can I get a GitHub Copilot license?“\nSecure ingress via Amazon API Gateway & AWS Lambda : Slack sends the event to an Amazon API Gateway endpoint, which triggers an AWS Lambda function. This function performs an essential security check, verifying the request’s cryptographic signature to confirm it’s authentically from Slack.\nDecoupling via Amazon Simple Queue Service (SQS ) : The verified request is placed onto an Amazon SQS First-In, First-Out (FIFO) queue. This decouples the front-end from the agent, improving resilience. Using a FIFO queue with the message’s thread timestamp as the MessageGroupId makes sure that replies within a single conversation are processed in order, which is important for maintaining coherent conversations.\nAgent execution via Amazon Bedrock AgentCore : The SQS queue triggers a Lambda function when messages arrive, which activates the agent running in the AgentCore Runtime. AgentCore manages the operational tasks, including orchestrating calls to the foundation model and the agent’s tools. The Orchestrator Agent’s logic, built with Strands Agents , analyzes the user’s prompt and determines the correct specialized agent to invoke—either the Knowledge Base Agent for a question or the GitHub Agent for an action request.\n\nA crucial implementation detail is how the system leverages AgentCore’s complete session isolation. To maintain conversational context, the system generates a unique, deterministic sessionId for each Slack thread by combining the channel ID and the thread’s timestamp. This sessionId is passed with every agent invocation within that thread. Interactions in a thread share this same sessionId, so the agent treats them as one continuous conversation. Meanwhile, interactions in other threads get different sessionIds, keeping their contexts separate. In effect, each conversation runs in an isolated session: AgentCore spins up separate resources per sessionId, so context and state do not leak between threads. In practice, this means that if a developer sends multiple messages in one Slack thread, the agent remembers the earlier parts of that conversation. Each thread’s history is preserved automatically by AgentCore.\nThis session management strategy is also vital for observability. Based on a unique sessionId, the interaction can be traced using AWS X-Ray , which offers insight into the flow – from the Slack message arriving at API Gateway to the message being enqueued in SQS. It follows the orchestrator’s processing, the call to the foundation model, subsequent tool invocations (such as a knowledge-base lookup or a GitHub API call), and finally the response back to Slack.\nMetadata and timing help indicate the flow of each step to understand where time is spent. If a step fails or is slow (for example, a timeout on an external API call), X-Ray pinpoints which step caused the issue. This is invaluable for diagnosing problems quickly and building confidence in the system’s behavior.\nThe solution: A reusable blueprint powered by AWS\nThe Bot Factory architecture designed by the AutoScout24 and AWS teams is event-driven, serverless, and built on a foundation of managed AWS services. This approach provides a resilient and scalable pattern that can be adapted for new use cases.\nThe solution builds on Amazon Bedrock and its integrated capabilities:\n\nAmazon Bedrock provides access to high-performing foundation models (FMs), which act as the reasoning engine for the agent.\nAmazon Bedrock Knowledge Bases enables the RAG capability, allowing the agent to connect to AutoScout24’s internal documentation and retrieve information to answer questions accurately.\nAmazon Bedrock AgentCore is a key component of the operational side of the blueprint. It provides the fully managed, serverless runtime environment to deploy, operate, and scale the agents.\n\nThis solution provides a significant advantage for AutoScout24. Instead of building foundational infrastructure for session management, security, and observability, they use AgentCore’s purpose-built services. This allows the team to focus on the agent’s business logic rather than the underlying infrastructure. AgentCore also provides built-in security and isolation features. Each agent invocation runs in its own isolated container, helping to prevent data leakage between sessions. Agents are assigned specific IAM roles to restrict their AWS permissions (following the principle of least privilege). Credentials or tokens needed by agent tools (such as a GitHub API key) are stored securely in AWS Secrets Manager and accessed at runtime. These features give the team a secure environment for running agents with minimal custom infrastructure.\nThe agent itself was built using the Strands Agents SDK, an open-source framework that simplifies defining an agent’s logic, tools, and behavior in Python. This combination proves effective: Strands to build the agent, and AgentCore to securely run it at scale. The team adopted a sophisticated “agents-as-tools” design pattern, where a central orchestrator Agent acts as the main controller. This orchestrator does not contain the logic for every possible task. Instead, it intelligently delegates requests to specialized, single-purpose agents. For the support bot, this included a Knowledge Base agent for handling informational queries and a GitHub agent for executing actions like assigning licenses. This modular design makes it straightforward to extend the system with new capabilities, such as adding a PR review agent without re-architecting the entire pipeline. Running these agents on Amazon Bedrock further enhances flexibility, since the team can choose from a broad range of foundation models. More powerful models can be applied to complex reasoning tasks, while lighter, cost-efficient models are well-suited for routine worker agents such as GitHub license requests or operational workflows. This ability to mix and match models allows Autoscout24 to balance cost, performance, and accuracy across their agent architecture.\nOrchestrator agent: built with Strands SDK\nUsing the Strands Agents SDK helped the team to define the orchestrator agent with concise, declarative code. The framework uses a model-driven approach, where the developer focuses on defining the agent’s instructions and tools, and the foundation model handles the reasoning and planning. The orchestrator agent can be expressed in just a few dozen lines of Python. The example snippet below (simplified for clarity, not intended for direct use) shows how the agent is configured with a model, a system prompt, and a list of tools (which in this architecture represent the specialized agents):\n\n# A simplified, representative example of the orchestrator agent logic\n# built with the Strands Agents SDK and deployed on Amazon Bedrock AgentCore.\nfrom bedrock_agentcore.runtime import BedrockAgentCoreApp\nfrom strands import Agent\nfrom strands.models import BedrockModel\nfrom tools import knowledge_base_query_tool, github_copilot_seat_agent\n# Initialize the AgentCore application, which acts as the serverless container\napp = BedrockAgentCoreApp()\nclass OrchestratorAgent:\ndef __init__(self):\n# 1. Define the Model: Point to a foundation model in Amazon Bedrock.\nself.model = BedrockModel(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n\n# 2. Define the Prompt: Give the agent its core instructions.\nself.system_prompt = \"\"\"\nYou are a helpful and friendly support bot for the AutoScout24 Platform Engineering team.\nYour goal is to answer developer questions and automate common requests.\nUse your tools to answer questions or perform actions.\nIf you cannot handle a request, politely say so.\n\"\"\"\n\n# 3. Define the Tools: Provide the agent with its capabilities.\n# These tools are entry points to other specialized Strands agents.\nself.tools = [\nknowledge_base_query_tool,\ngithub_copilot_seat_agent\n]\n\n# Create the agent instance\nself.agent = Agent(\nmodel=self.model,\nsystem_prompt=self.system_prompt,\ntools=self.tools\n)\ndef __call__(self, user_input: str):\n# Run the agent to get a response for the user's input\nreturn self.agent(user_input)\n# Define the entry point that AgentCore will invoke when a new event arrives from SQS\n@app.entrypoint\ndef main(event):\n# Extract the user's query from the incoming event\nuser_query = event.get(\"prompt\")\n\n# Instantiate and run the orchestrator agent\nreturn OrchestratorAgent()(user_query)\n\nAnother example is the GitHub Copilot license agent. It is implemented as a Strands tool function. The following snippet shows how the team defined it using the @tool decorator. This function creates a GitHubCopilotSeatAgent, passes the user’s request (a GitHub username) to it, and returns the result:\n\nfrom strands import Agent, tool\nclass GitHubCopilotSeatAgent:\ndef __call__(self, query: str):\nagent = Agent(model=self.model, system_prompt=self.system_prompt, tools=self.tools)\nreturn agent(query)\n\n@tool\ndef github_copilot_seat_agent(github_username: str) -> str:\nagent = GitHubCopilotSeatAgent() response = agent(f\"Request GitHub Copilot license for user: {github_username}\")\nreturn str(response)\n\nKey benefits of this approach include clear separation of concerns. The developer writes declarative code focused on the agent’s purpose. The complex infrastructure logic, including scaling, session management, and secure execution, is handled by Amazon Bedrock AgentCore. This abstraction enables rapid development and allowed AutoScout24 to move from prototype to production more quickly. The tools list effectively makes other agents callable functions, allowing the orchestrator to delegate tasks without needing to know their internal implementation.\nThe impact: A validated blueprint for enterprise AI\nThe Bot Factory project delivers results that extended beyond the initial prototype. It creates immediate business value and establishes a strategic foundation for future AI innovation at AutoScout24.The key outcomes were:\n\nA production-ready support bot: The team deployed a functional Slack bot that is actively reducing the manual support load on the AutoScout24 AI Platform Engineering Team, addressing the 30% of time previously spent on repetitive tasks.\nA reusable Bot Factory blueprint: The project produces a validated, reusable architectural pattern. Now, teams at AutoScout24 can build a new agent by starting with this proven template (Slack -> API Gateway -> SQS -> AgentCore). This significantly accelerates innovation by allowing teams to focus on their unique business logic, not on reinventing the infrastructure. This modular design also prepares them for more advanced multi-agent collaboration, potentially using standards like the Agent-to-Agent (A2A) protocol as their needs evolve.\nEnabling broader AI development: By abstracting away the infrastructure complexity, the Bot Factory empowers more people to build AI solutions. A domain expert in security or data analytics can now create a new tool or specialized agent and “plug it in” to the factory without needing to be an expert in distributed systems.\n\nConclusion: A new model for enterprise agents\nAutoScout24’s partnership with AWS turned fragmented generative AI experiments into a scalable, standardized framework. By adopting Amazon Bedrock AgentCore, the team moved their support bot from prototype to production, while focusing on their Bot Factory vision. AgentCore manages session state and scaling, so engineers can focus on high-value business logic instead of infrastructure. The outcome is more than a support bot: it’s a reusable foundation for building enterprise agents. With AgentCore, AutoScout24 can move from prototype to production efficiently, setting a model for how organizations can standardize generative AI development on AWS. To start building enterprise agents with Amazon Bedrock, explore the following resources:\n\nAmazon Bedrock AgentCore documentation\nAmazon Bedrock Knowledge Bases documentation\nSecurely launch and scale your agents with Amazon Bedrock AgentCore\nBuild trustworthy AI agents with Amazon Bedrock AgentCore Observability\n\nAbout the authors\nAndrew Shved  is a Senior AWS Prototyping Architect who leads teams and customers in building and shipping Generative AI–driven solutions, from early prototypes to production on AWS.\nMuhammad Uzair Aslam is a tenured Technical Program Manager on the AWS Prototyping team, where he works closely with customers to accelerate their cloud and AI journeys. He thrives on diving deep into technical details and turning complexity into impactful, value-driven solutions.\nArslan Mehboob is a Platform Engineer and AWS-certified solutions architect with deep expertise in cloud infrastructure, scalable systems, and software engineering. He currently builds resilient cloud platforms and is passionate about AI and emerging technologies.\nVadim Shiianov is a Data Scientist specializing in machine learning and AI-driven systems for real-world business applications. He works on designing and deploying ML and Generative AI solutions that translate complex data into measurable impact. He is passionate about emerging technologies and building practical, scalable systems around them.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "AutoScout24，欧洲领先的汽车市场平台，与亚马逊 AWS 合作开发了 Bot Factory，这是一个基于 Amazon Bedrock 的标准化 AI 代理开发框架。该框架采用事件驱动和无服务器架构，结合 Amazon Bedrock AgentCore 运行时和 Strands Agents SDK，以自动化内部任务。团队首先应用于开发者支持用例，构建 Slack 机器人实现知识检索（通过 RAG）和行动执行（如分配 GitHub Copilot 许可证），减少了工程师 30% 的重复任务时间。框架提供了会话隔离、安全性和可观察性，支持模块化扩展，不仅提升了操作效率，还为企业创建了可复用的蓝图，加速了未来的 AI 创新。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "AutoScout24",
        "Amazon Bedrock",
        "AWS",
        "Bot Factory",
        "AI agent"
      ]
    },
    "analyzed_at": "2026-01-15T03:39:45.719888Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "cab4cdb03fe45998",
    "title": "Transform AI development with new Amazon SageMaker AI model customization and large-scale training capabilities",
    "url": "https://aws.amazon.com/blogs/machine-learning/transform-ai-development-with-new-amazon-sagemaker-ai-model-customization-and-large-scale-training-capabilities/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-14T21:13:42Z",
    "summary": "This post explores how new serverless model customization capabilities, elastic training, checkpointless training, and serverless MLflow work together to accelerate your AI development from months to days.",
    "content": "With the advancement in tools and services that make generative AI models accessible, businesses can now access the same foundation models (FMs) as their competitors. True differentiation comes from building AI that is highly customized for your business—something your competitors can’t effortlessly replicate. Although today’s FMs are genuinely intelligent with vast knowledge and reasoning capabilities, intelligence without context is merely potential. A model knows how to think, but it doesn’t know how you think, your vocabulary, your data patterns, or your industry constraints.\nBuilding models that deeply understand your business depends on how you make the model learn from your data and preferences. Models learn through a step-by-step process that mirrors human learning: they first acquire general world knowledge through pre-training, then gain specialized knowledge through supervised fine-tuning, and finally learn to align with specific preferences through techniques like direct preference optimization (DPO). At the inference stage, models can apply everything they’ve learned to real-world tasks, and they can continue adapting through parameter-efficient methods such as Low-Rank Adaptation (LoRA) without retraining the entire base model.\nThis learning journey spans from pre-training massive FMs to customizing them for specific use cases, and Amazon SageMaker AI now provides capabilities across this entire spectrum.\nAt AWS re:Invent 2025 , Amazon SageMaker AI announced significant advances that change how organizations can approach model customization and large-scale training. The new capabilities address two persistent challenges: the complexity and time required to customize FMs for specific use cases, and the costly infrastructure failures that derail weeks of training progress.\nSince launching Amazon SageMaker AI in 2017, we’ve been committed to making AI development accessible for builders of different skill levels. With over 450 capabilities introduced since launch, SageMaker AI continues to remove barriers that slow innovation. This post explores how new serverless model customization capabilities, elastic training, checkpointless training, and serverless MLflow work together to accelerate your AI development from months to days.\nServerless AI model customization with advanced reinforcement learning\nThe new serverless model customization capability in Amazon SageMaker AI transforms what has traditionally been a months-long process into a matter of days. For AI developers who want the highest level of abstraction, we’re introducing an AI agent-guided workflow (in preview) that makes advanced model customization accessible through natural language.\nInstead of requiring deep expertise in reinforcement learning techniques, you can now describe your business objectives in plain language. The AI agent engages in a multiturn conversation to understand your use case, then generates a comprehensive specification that includes dataset guidelines, evaluation criteria, associated metrics, and a recommended model that your team can implement without needing specialized knowledge.\n\nThe AI agentic workflow supports supervised fine-tuning (SFT), direct preference optimization (DPO), reinforcement learning from AI feedback (RLAIF), and Reinforcement Learning from Verifiable Rewards (RLVR). Models can use these reinforcement learning capabilities to learn from human preferences and verifiable outcomes, creating AI that aligns more closely with your business objectives. You can also generate synthetic data when real-world data is limited, analyze data quality, and handle training and evaluation for accuracy and responsible AI controls. This approach is entirely serverless to remove infrastructure complexity.\nFor AI developers who want more control over the customization process, SageMaker AI offers a straightforward interface with built-in best practices. Through SageMaker Studio , you can select from popular models including Amazon Nova , Meta’s Llama, Qwen, DeepSeek, and GPT-OSS, then choose your preferred customization technique.\nThe self-guided workflow provides flexibility at every step. You can upload your own datasets or select from existing ones, configure hyperparameters such as batch size and learning rate with recommended defaults, and choose between parameter-efficient fine-tuning with LoRA or full fine-tuning. The interface integrates with the newly introduced MLflow capability for automatic experiment tracking, giving you visibility into training progress and model performance through a single interface.\nLike the AI agentic approach, self-guided customization is completely serverless. SageMaker AI automatically handles compute provisioning, scaling, and optimization, so you can focus on model development instead of infrastructure management. With pay-per-token pricing, you can avoid the overhead of selecting instance types or managing clusters.\nCollinear AI cut their experimentation cycles from weeks to days using the serverless model customization capability of SageMaker AI. Soumyadeep Bakshi, Co-founder, Collinear AI, said:\n\n“At Collinear, we build curated datasets and simulation environments for frontier AI labs and Fortune 500 enterprises to improve their models. Fine-tuning AI models is critical to creating high-fidelity simulations, and it used to require stitching together different systems for training, evaluation, and deployment. Now with the new Amazon SageMaker AI serverless model customization capability, we have a unified way that empowers us to cut our experimentation cycles from weeks to days. This end-to-end serverless tooling helps us focus on what matters: building better training data and simulations for our customers, not maintaining infrastructure or juggling disparate platforms.”\n\nBridging model customization and pre-training\nWhile serverless model customization accelerates development for specific use cases through fine-tuning and reinforcement learning, organizations are also rapidly expanding their use of generative AI across many parts of the business. Applications requiring deep domain expertise or specific business context need models that truly understand their proprietary knowledge, workflows, and unique requirements. Techniques such as prompt engineering and Retrieval Augmented Generation (RAG) work well for many use cases, but they have fundamental limitations when it comes to embedding specialized knowledge into a model’s core understanding. When organizations attempt deeper customization through continued pre-training (CPT) using only their proprietary data, they often encounter catastrophic forgetting, where models lose their foundational capabilities as they learn new content.\nAmazon SageMaker AI supports the complete spectrum of model development, from serverless customization with advanced reinforcement learning, to building frontier models from early checkpoints. For organizations with proprietary data that need models with deep domain expertise beyond what customization alone can provide, we recently introduced a new capability that addresses the limitations of traditional approaches while preserving foundational model capabilities.\nLast week, we introduced Amazon Nova Forge . Accessible on Amazon SageMaker AI, this new service gives AI developers the opportunity to build their own frontier models using Amazon Nova. You can use Nova Forge to start model development from early checkpoints across pre-training, mid-training, and post-training phases—which means you can intervene at the optimal stage rather than waiting until training is complete. You can blend your proprietary data with Amazon Nova curated data throughout the training phases using demonstrated recipes on the fully managed infrastructure of SageMaker AI. This data mixing approach significantly reduces catastrophic forgetting compared to training with raw data alone. This helps preserve foundational skills, including core intelligence, general instruction following capabilities, and safety benefits while incorporating your specialized knowledge. Nova Forge is the simplest and most cost-effective way to build your own frontier model.\nThe following video introduces Amazon Nova Forge.\n\nNova Forge is designed for organizations with access to proprietary or industry-specific data who want to build AI that truly understands their domain, including:\n\nManufacturing and automation – Building models that understand specialized processes and equipment data\nResearch and development – Creating models trained on proprietary research data\nContent and media – Developing models that understand brand voice and content standards\nSpecialized industries – Training models on industry-specific terminology, regulations, and best practices\n\nCompanies like Nomura Research Institute are using Amazon Nova Forge to build industry-specific large language models (LLMs) by combining Amazon Nova curated data with their proprietary datasets.\nTakahiko Inaba, Head of AI and Managing Director, Nomura Research Institute, Ltd., said:\n\n“Nova Forge enables us to build industry-specific LLMs as a compelling alternative to open-weight models. Running on SageMaker AI with managed training infrastructure, we can efficiently develop specialized models like our Japanese financial services LLM by combining Amazon Nova curated data with our proprietary datasets.”\n\nElastic training for intelligent resource management at scale\nThe demand for AI accelerators constantly fluctuates as inference workloads scale with traffic patterns, completed experiments release resources, and new training jobs shift priorities. Traditional training workloads remain locked into their initial compute allocation, unable to take advantage of idle capacity without manual intervention—a process that consumes hours of your engineering time each week.\nElastic training on Amazon SageMaker HyperPod transforms this dynamic. Training jobs now automatically scale based on compute resource availability, expanding to absorb idle AI accelerators and maximizing infrastructure utilization. When higher-priority workloads such as inference or evaluation need resources, the training scales down gracefully to continue with fewer resources instead of halting entirely.\n\nThe technical architecture maintains training quality throughout scaling transitions by preserving global batch size and learning rate across different data-parallel configurations. This supports consistent convergence properties regardless of current scale. The SageMaker HyperPod training operator orchestrates scaling decisions through integration with the Kubernetes control plane, continuously monitoring cluster state through pod lifecycle events, node availability changes, and resource scheduler priority signals.\nGetting started is straightforward. New elastic SageMaker HyperPod recipes for publicly available FMs including Meta’s Llama and GPT-OSS require no code changes—only YAML configuration updates to specify the elastic policy.\nSalesforce is using elastic training to automatically scale workloads and absorb idle GPUs as they become available, explaining that elastic training “will enable our workloads to automatically scale to absorb idle GPUs as they become available and seamlessly yield resources, all without disrupting development cycles. Most importantly, it will save us hours spent manually reconfiguring jobs to match available compute, time that we can reinvest in innovation.”\nMinimizing recovery downtime with checkpointless training\nInfrastructure failures have long been the enemy of progress in large-scale training. Training runs that take weeks can be derailed by a single node failure, forcing you to restart from your last checkpoint and losing hours or days of expensive GPU time. Traditional checkpoint-based recovery involves sequential stages—job termination and restart, process discovery and network setup, checkpoint retrieval, GPU context reinitialization, and training loop resumption. When failures occur, the entire cluster must wait for every stage to be completed before training can resume.\nCheckpointless training on Amazon SageMaker HyperPod removes this bottleneck. The system maintains continuous model state preservation across distributed clusters, automatically swapping faulty components and recovering training through peer-to-peer transfer of model states from healthy AI accelerators. When infrastructure faults occur, recovery happens in seconds with zero manual intervention. The following video introduces checkpointless training.\n\nThis translates to upwards of 95% training goodput on cluster sizes with thousands of AI accelerators, meaning compute infrastructure is actively utilized for training jobs up to 95% of the time. You can now focus on innovation rather than infrastructure management, accelerating time-to-market by weeks.\nIntercom is already integrating checkpointless training into their pipelines to remove manual checkpoint recovery, stating:\n\n“At Intercom, we’re constantly training new models to improve Fin, and we’re very excited to integrate checkpointless training into our pipelines. This will completely eliminate the need for manual checkpoint recovery. Combined with elastic training, it will allow us to deliver improvements to Fin faster and with lower infrastructure costs.”\n\nServerless MLflow: Observability for every AI developer\nWhether customizing models or training at scale, you need capabilities to track experiments, observe behavior, and evaluate performance. However, managing MLflow infrastructure traditionally requires administrators to continuously maintain and scale tracking servers, make complex capacity planning decisions, and deploy separate instances for data isolation. This infrastructure burden diverts resources away from core AI development.\nAmazon SageMaker AI now offers a serverless MLflow capability that removes this complexity. You can begin tracking, comparing, and evaluating experiments without waiting for infrastructure setup. MLflow scales dynamically to deliver fast performance for demanding and unpredictable model development tasks, then scales down during idle time. The following screenshot shows the MLFlow application in the SageMaker AI UI.\n\nThe capability works natively with Amazon SageMaker AI serverless model customization so you can visualize in-progress training jobs and evaluations through a single interface. Advanced tracing capabilities help quickly identify bugs or unexpected behaviors in agentic workflows and multistep applications. Teams can use the MLflow Prompt Registry to version, track, and reuse prompts across organizations, maintaining consistency and improving collaboration.\nIntegration with SageMaker Model Registry provides seamless model governance, automatically synchronizing models registered in MLflow with the production lifecycle. After models achieve the desired accuracy and performance goals, you can deploy them to SageMaker AI inference endpoints in only a few clicks.\nAdministrators can help enhance productivity by setting up cross-account access using AWS Resource Access Manager (AWS RAM) to simplify collaboration across organizational boundaries. The serverless MLflow capability is offered at no additional charge and automatically upgrades to the latest version of MLflow, giving you access to the newest features without maintenance windows or migration effort.\nWildlife Conservation Society is using the new serverless capability to enhance productivity and accelerate time-to-insights. Kim Fisher, MERMAID Lead Software Engineer, WCS, said:\n\n“WCS is advancing global coral reef conservation through MERMAID, an open source platform that uses ML models to analyze coral reef photos from scientists around the world. Amazon SageMaker with MLflow has enhanced our productivity by eliminating the need to configure MLflow tracking servers or manage capacity as our infrastructure needs change. By enabling our team to focus entirely on model innovation, we’re accelerating our time-to-deployment to deliver critical cloud-driven insights to marine scientists and managers.”\n\nAccelerating AI innovation at every level\nThese announcements represent more than individual feature improvements—they establish a comprehensive system for AI model development that meets builders wherever they are on their journey. From natural language–guided customization to self-directed workflows, from intelligent resource management to fault-tolerant training, from experiment tracking to production deployment, Amazon SageMaker AI provides the complete toolkit for transforming AI ideas into production reality.\nGetting started\nThe new SageMaker AI model customization and SageMaker HyperPod capabilities are available today in AWS Regions worldwide. Existing SageMaker AI customers can access these features through the SageMaker AI console , and new customers can get started with the AWS Free Tier .\nFor more information about the latest capabilities of Amazon SageMaker AI, visit aws.amazon.com/sagemaker/ai .\n\nAbout the authors\nAnkur Mehrotra  joined Amazon back in 2008 and is currently the General Manager of Amazon SageMaker AI. Before Amazon SageMaker AI, he worked on building Amazon.com’s advertising systems and automated pricing technology.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊在AWS re:Invent 2025上宣布了Amazon SageMaker AI的新功能，包括服务器端AI模型自定义、弹性训练、无检查点训练和服务器端MLflow。这些功能利用AI代理工作流支持监督微调（SFT）和直接偏好优化（DPO）等技术，使开发者通过自然语言描述业务目标即可快速定制模型，将定制过程从数月缩短到数天。弹性训练在SageMaker HyperPod上自动缩放资源以提高利用率，无检查点训练在基础设施故障时快速恢复，减少停机时间，而服务器端MLflow提供实验跟踪。这些改进解决了模型定制复杂和训练管理困难的问题，降低了技术门槛和成本，加速AI开发，适用于金融、制造等行业，已有企业如Collinear AI和Salesforce受益。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "Amazon SageMaker AI",
        "Amazon Nova Forge",
        "Serverless model customization",
        "Elastic training",
        "SageMaker HyperPod"
      ]
    },
    "analyzed_at": "2026-01-15T03:39:57.887419Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "33cb87dbd94ec9b2",
    "title": "OpenAI partners with Cerebras",
    "url": "https://openai.com/index/cerebras-partnership",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-01-14T14:00:00Z",
    "summary": "OpenAI partners with Cerebras to add 750MW of high-speed AI compute, reducing inference latency and making ChatGPT faster for real-time AI workloads.",
    "content": "OpenAI partners with Cerebras to add 750MW of high-speed AI compute, reducing inference latency and making ChatGPT faster for real-time AI workloads.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI 宣布与 Cerebras 合作，增加 750MW 的高速 AI 计算能力。这一合作旨在减少推理延迟，使 ChatGPT 在实时 AI 工作负载中运行更快。通过提升计算基础设施，OpenAI 能够优化其 AI 模型的性能，提高响应速度，从而改善用户体验，应对日益增长的实时 AI 需求。合作突出了计算资源对于支持大规模 AI 服务的重要性，并可能促进 ChatGPT 在高并发场景下的应用扩展。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Cerebras",
        "ChatGPT",
        "AI compute",
        "inference latency"
      ]
    },
    "analyzed_at": "2026-01-15T03:39:42.463327Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]