[
  {
    "id": "c943585d35a3cfe0",
    "title": "Learnings from COBOL modernization in the real world",
    "url": "https://aws.amazon.com/blogs/machine-learning/learnings-from-cobol-modernization-in-the-real-world/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T18:16:43Z",
    "summary": "Delivering successful COBOL modernization requires a solution that can reverse engineer deterministically, produce validated and traceable specs, and help those specs flow into any AI-powered coding assistant for the forward engineering. A successful modernization requires both reverse engineering and forward engineering. Learn more about COBOL in this post.",
    "content": "There’s a lot of excitement right now about AI enabling mainframe application modernization. Boards are paying attention. CIOs are getting asked for a plan. AI is a genuine accelerator for COBOL modernization but to get results, AI needs additional context that source code alone can’t provide.Here’s what we’ve learned working with 400+ enterprise customers: mainframe modernization has two very different halves. The first half is reverse engineering, understanding what your existing systems actually do. The second half is forward engineering, building the new applications.\nThe first half is where mainframe projects live or die. However, coding assistants are genuinely good at only the second half. Give them a clear, validated spec and they’ll build modern applications fast.\nWe have learned that delivering successful COBOL modernization requires a solution that can reverse engineer deterministically, produce validated and traceable specs, and help those specs flow into any AI-powered coding assistant for the forward engineering. A successful modernization requires both reverse engineering and forward engineering.\nWhat a successful mainframe modernization requires\nBounded, complete context\nMainframe applications are big. Really big. A single program can run tens of thousands of lines, pulling in shared data definitions from across the system, calling other programs, orchestrated through JCL that spans the entire landscape. Today, AI can only process a limited amount of code at a time. Feed it one program and it can’t see the copybooks, the called subroutines, the shared files, or the JCL that ties everything together. It will produce output that looks reasonable for the code it can see but miss dependencies it was never shown. In working with customers, we solve this by extracting all implicit dependencies deterministically first, then feeding AI bounded, complete units with everything it needs already resolved. That way AI focuses on what it’s great at (understanding business logic, generating specifications) instead of guessing at connections it can’t see.\nPlatform-aware context\nHere’s something that surprises people: the same COBOL source code behaves differently depending on the compiler and runtime. How numbers get rounded, how data sits in memory, how programs talk to middleware. These aren’t in the source code. They’re determined by the specific compiler and runtime environment the code was built for. Decades of hardware-software integration can’t be replicated by simply moving code. We found that AI does its best work when platform-specific behavior has already been resolved. Feed AI clean, platform-aware input, and it delivers. Feed it raw source code, and it’ll generate output that looks right but behaves differently than the original. In financial systems, a rounding difference isn’t a cosmetic issue. It’s a material error.\nA traceable foundation\nIf you’re in banking, insurance, or government, your regulators will ask one question: can you prove you didn’t miss anything? AI on its own isn’t enough to extract business logic and generate documentation that regulators will accept. Regulatory compliance requires every output to have a formal, auditable connection back to the original system. We learned early that traceability doesn’t come from AI reading source code. It comes from structuring the code into precise, bounded units so we know exactly what goes into the AI and can trace every output back to its source. For customers in regulated industries, this is often the difference between a project that moves forward and one that stalls.\nHow we set AI up for success in AWS Transform\nWe built AWS Transform to modernize mainframe applications at scale. The idea is straightforward: give AI the right foundation, and customers get traceable, correct, and complete results they can take to production. AWS Transform starts by building a complete, deterministic model of the application. Specialized agents extract code structure, runtime behavior, and data relationships across the entire system — not one program at a time, but the whole landscape. This produces a dependency graph aligned with the actual compiler semantics, capturing cross-program dependencies, middleware interactions, and platform-specific behavior before AI gets involved. From there, large programs get decomposed into bounded, processable, units. Platform-specific behavior is resolved deterministically. The units are sized for AI to process effectively. Then AI extracts business logic in natural language, and every output gets validated against the deterministic evidence we’ve already extracted. Specs map back to the original code. When a regulator asks “did you miss anything?”, there’s a verifiable answer. What sets this apart is that AI never operates in the dark. Every unit it processes has known inputs and expected outputs, so we can validate what comes back. No other approach on the market closes that loop. What comes out is a set of validated, traceable technical specifications that plug into any modern development environment. The hard part of modernization is understanding what exists today. Once you’ve captured that in precise specs, AI-powered IDEs can build the new application with confidence.\nAn end-to-end platform for enterprise transformation\nNobody modernizes one app. Our customers are staring at portfolios of hundreds or thousands of interconnected applications, and they need way more than analysis help. AWS Transform automates across the full lifecycle: analysis, test planning, refactoring, reimagination. The whole thing. And within that, different apps need different paths. Some get re-imagined from scratch. Some just need a clean, deterministic conversion to Java. Some need to get out of the data center first and modernize later. Some will remain on the mainframe. We learned the hard way that treating them all the same is how projects blow up. The portfolio decision (which app, which path, what order) matters as much as the tech. In our experience, this is the only way enterprise modernization actually finishes. One-size-fits-all approaches are why these projects fail. One more thing that gets overlooked constantly: test data. You can’t prove the modernized app works without real production data and real scenarios. We’ve watched teams get all the way through code conversion and then stall because nobody planned for data capture. So, we built test planning and on-prem data capture into the platform from day one. Not a cleanup exercise at the end. That’s what this actually looks like when it works. End-to-end automation, the right path for each app, validation baked in.\nHow to get this right\nThe question isn’t “should we use AI for COBOL modernization?” Of course you should. The question is how you set AI up to deliver: traceability for regulators, platform-specific behavior handled correctly, consistency across your application portfolio, and the ability to scale to hundreds of interconnected programs. That’s what we figured out building AWS Transform. Deterministic analysis as the foundation. AI as the accelerator. An AWS service that covers the full range of modernization patterns.\nAnd it’s working.\nBMW Group reduced testing time by 75% and increased test coverage by 60%, significantly lowering risk while accelerating modernization timelines.\nFiserv completed a mainframe modernization project that would have taken 29+ months in just 17 months.\nItau cut mainframe application discovery time and testing time by more than 90%, enabling teams to modernize applications 75% faster than with previous manual efforts.\n\nAbout the authors\n\nDr. Asa Kalavade\nAsa leads AWS Transform, helping customers migrate and modernize their infrastructure, applications, and code. Previously, she led the AWS go-to-market tools transformation, incorporating generative AI capabilities. She also managed hybrid storage and data transfer services. Before joining AWS in 2016, Asa founded two venture-backed startups and remains active in mentoring Boston startups. She holds a PhD in electrical engineering and computer science from UC Berkeley and more than 40 patents.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊AWS推出了AWS Transform平台，专门利用人工智能加速COBOL大型机应用的现代化进程。该平台通过确定性分析作为基础，结合AI技术，处理大型应用的有界完整上下文、平台特定行为，并确保可追溯性，以支持监管合规。关键创新包括逆向工程和正向工程两阶段方法，自动化整个生命周期，为不同应用提供定制路径，如重构为Java或重新构想。成功案例如BMW、Fiserv和Itau显示，测试时间减少高达75%，项目时间显著缩短，降低了风险并加速了企业转型。这解决了AI在现代化旧系统时面临的依赖处理和准确性挑战，使AI能更有效地应用于金融、政府等严格监管行业。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "AWS",
        "AWS Transform",
        "COBOL",
        "mainframe modernization",
        "AI"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:54.521261Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "63e7f7d59dbf6306",
    "title": "Reinforcement fine-tuning for Amazon Nova: Teaching AI through feedback",
    "url": "https://aws.amazon.com/blogs/machine-learning/reinforcement-fine-tuning-for-amazon-nova-teaching-ai-through-feedback/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T17:48:37Z",
    "summary": "In this post, we explore reinforcement fine-tuning (RFT) for Amazon Nova models, which can be a powerful customization technique that learns through evaluation rather than imitation. We'll cover how RFT works, when to use it versus supervised fine-tuning, real-world applications from code generation to customer service, and implementation options ranging from fully managed Amazon Bedrock to multi-turn agentic workflows with Nova Forge. You'll also learn practical guidance on data preparation, re...",
    "content": "Foundation models deliver impressive out-of-the-box performance for general tasks, but many organizations need models to consume their business knowledge. Model customization helps you bridge the gap between general-purpose AI and your specific business needs when building applications that require domain-specific expertise, enforcing communication styles, optimizing for specialized tasks like code generation, financial reasoning, or ensuring compliance with industry regulations. The challenge lies in how to customize effectively. Traditional supervised fine-tuning delivers results, but only if you have thousands of carefully labeled examples showing not just the correct final answer, but also the complete reasoning path to reach it. For many real-world applications, especially those tasks where multiple valid solution paths exist, creating these detailed step-by-step demonstrations can sometimes be expensive, time-consuming.\nIn this post, we explore reinforcement fine-tuning (RFT) for Amazon Nova models, which can be a powerful customization technique that learns through evaluation rather than imitation. We’ll cover how RFT works, when to use it versus supervised fine-tuning, real-world applications from code generation to customer service, and implementation options ranging from fully managed Amazon Bedrock to multi-turn agentic workflows with Nova Forge . You’ll also learn practical guidance on data preparation, reward function design, and best practices for achieving optimal results.\nA new paradigm: Learning by evaluation rather than imitation\nWhat if you could teach a car to not only learn all the paths on a map, but to also learn how to navigate if a wrong turn is taken? That’s the core idea behind reinforcement fine-tuning (RFT), a model customization technique we’re excited to bring to Amazon Nova models. RFT shifts the paradigm from learning by imitation to learning by evaluation. Instead of providing thousands of labeled examples, you provide prompts and define what makes a final answer correct through test cases, verifiable outcomes, or quality criteria. The model then learns to optimize those criteria through iterative feedback, discovering its own path to correct solutions.\nRFT supports model customization for code generation and math reasoning by verifying outputs automatically, eliminating the need for providing detailed step by step reasoning. We made RFT available across our AI services to meet you wherever you are in your AI journey: start simple with the fully-managed experience available in Amazon Bedrock , gain more control with SageMaker Training Jobs , scale to advanced infrastructure with SageMaker HyperPod , or unlock frontier capabilities with Nova Forge for multi-turn conversations and custom reinforcement learning environments.\nIn December 2025, Amazon launched the Nova 2 family —Amazon’s first models with built-in reasoning capabilities. Unlike traditional models that generate responses directly, reasoning models like Nova 2 Lite engage in step-by-step problem decomposition, performing intermediate thinking steps before producing final answers. This extended thinking process mirrors how humans approach complex analytical tasks. When combined with RFT, this reasoning capability becomes particularly powerful, RFT can optimize not just what answer the model produces, but how it reasons through problems, teaching it to discover more efficient reasoning paths while reducing token usage. As of today, RFT is only supported with text-only use cases.\nReal-World Use Cases\nRFT excels in scenarios where you can define and verify correct outcomes, but creating detailed step-by-step solution demonstrations at scale is impractical. Below are some of the use cases, where RFT can be a good option:\n\nCode generation: You want code that’s not just correct, but also efficient, readable, and handles edge cases gracefully, such as qualities you can verify programmatically through test execution and performance metrics.\nCustomer service : You need to evaluate whether replies are helpful, maintain your brand’s voice, and strike the right tone for each situation. These are judgment calls that can’t be reduced to simple rules but can be assessed by an AI judge trained on your communication standards.\nOther applications : Content moderation, where context and nuance matter; multi-step reasoning tasks like financial analysis or legal document review; and tool usage, where you need to teach models when and how to call APIs or query databases. In each case, you can define and verify correct outcomes programmatically, even when you can’t easily demonstrate the step-by-step reasoning process at scale.\nExploration-heavy problems : Use cases like game playing and strategy, resource allocation, and scheduling benefit from cases where the model uses different approaches and learns from feedback.\nLimited labeled data scenarios: Use cases where limited labeled datasets are available like domain-specific applications with few expert-annotated examples, new problem domains without established solution patterns, expensive-to-label tasks (medical diagnosis, legal analysis). In these use cases, RFT helps to optimize the rewards computed from the reward functions.\n\nHow RFT Works\nRFT operates through a three-stage automated process (shown in Figure 1):\nStage 1: Response generation – The actor model (the model you’re customizing) receives prompts from your training dataset and generates multiple responses per prompt—typically 4 to 8 variations. This diversity gives the system a range of responses to evaluate and learn from.\nStage 2: Reward computation – Instead of comparing responses to labeled examples, the system evaluates quality using reward functions. You have two options:\n\nReinforcement learning via verifiable rewards (RLVR) : Rule-based graders implemented as AWS Lambda functions, perfect for objective tasks like code execution or math problem verification where you can programmatically check correctness.\nReinforcement learning from AI feedback (RLAIF) : AI-based judges that evaluate responses based on criteria you configure, ideal for subjective tasks like assessing helpfulness, creativity, or adherence to brand voice.\n\nStage 3: Actor model training – The system uses the scored prompt-response pairs to train your model through a reinforcement learning algorithm, like Group Relative Policy Optimization (GRPO) , optimized for language models. The model learns to maximize the probability of generating high-reward responses while minimizing low-reward responses. This iterative process continues until the model achieves your desired performance.\n\nFigure 1: Illustration of how single pass of RFT works\n\nKey Benefits of RFT\nThe following are the key benefits of RFT:\n\nNo massive, labeled datasets required – RFT only needs prompts and a way to evaluate quality. If using Bedrock RFT, you can even leverage existing Bedrock API invocation logs as RFT data, eliminating the need for specially created datasets.\nOptimized for verifiable outcomes – Unlike supervised fine-tuning that requires explicit demonstrations of how to reach correct answers, RFT is optimized for tasks where you can define and verify correct outcomes, but multiple valid reasoning paths may exist.\nReduced token usage – By optimizing the model’s reasoning process, RFT can reduce the number of tokens required to accomplish a task, lowering both cost and latency in production.\nSecure and monitored – Your proprietary data never leaves AWS’s secure environment during the customization process, and you get real-time monitoring of training metrics to track progress and ensure quality.\n\nImplementation tiers: From simple to complex\nAmazon offers multiple implementation paths for reinforcement fine-tuning with Nova models, ranging from fully managed experiences to customizable infrastructure. By following this tiered approach you can match your RFT implementation to your specific needs, technical expertise, and desired level of control.\n\nAmazon Bedrock\nAmazon Bedrock provides an entry point to RFT with a fully managed experience that requires minimal ML expertise. Through the Amazon Bedrock console or API, you can upload your training prompts, configure your reward function as an AWS Lambda, and launch your reinforcement fine-tuning job with just a few clicks. Bedrock handles all infrastructure provisioning, training orchestration, and model deployment automatically. This approach works well for straightforward use cases where you need to optimize specific criteria without managing infrastructure. The simplified workflow makes RFT accessible to teams without dedicated ML engineers while still delivering powerful customization capabilities. Bedrock RFT supports both RLVR (rule-based rewards) and RLAIF (AI-based feedback) approaches, with built-in monitoring and evaluation tools to track your model’s improvement. To get started, see the Amazon Nova RFT GitHub repository.\nAmazon SageMaker Serverless Model Customization\nAmazon SageMaker AI’s serverless model customization is purpose-built for ML practitioners who are ready to move beyond prompt engineering and RAG, and into fine-tuning LLMs for high-impact, specialized use cases. Whether the goal is improving complex reasoning, domain-specific code generation, or optimizing LLMs for agentic workflows including planning, tool calling, and reflection, SageMaker’s offering removes the traditional infrastructure and expertise barriers that slow experimentation. At its core, the service brings advanced reinforcement learning techniques like GRPO with RLVR/RLAIF to developers without requiring complex RL setup, alongside a comprehensive evaluation suite that goes well beyond basic accuracy metrics. Complementing this, AI-assisted synthetic data generation, integrated experiment tracking, and full lineage and audit trail support round out a production-grade customization pipeline. Deployment flexibility allows teams to ship fine-tuned models to SageMaker endpoints, Amazon Bedrock, or custom infrastructure, making it a compelling end-to-end serverless solution for teams looking to accelerate their model customization cycles and unlock the full potential of models like Amazon Nova in real-world applications.\nSageMaker Training Jobs\nFor teams that need more control over the training process, Amazon SageMaker Training Jobs offer a flexible middle ground with managed compute and ability to tweak multiple hyperparameters. You can also save intermediate checkpoints and use them to create iterative training workflows like chaining supervised fine-tuning (SFT) and RFT jobs to progressively refine your model. You have the flexibility to choose between LoRA and full-rank training approaches, with full control over hyperparameters. For deployment, you can choose between Amazon Bedrock for fully managed inference or Amazon SageMaker endpoints where you control instance types, batching, and performance tuning. This tier is ideal for ML engineers and data scientists who need customization beyond Amazon Bedrock but don’t require dedicated infrastructure. SageMaker Training Jobs also integrate seamlessly with the broader Amazon SageMaker AI ecosystem for experiment tracking, model registry, and deployment pipelines. Amazon Nova RFT on SageMaker Training Job uses YAML recipe files to configure training jobs. You can obtain base recipes from the SageMaker HyperPod recipes repository.\nBest practices:\n\nData format : Use JSONL format with one JSON object per line.\nReference answers : Include ground truth values that your reward function will compare against model predictions.\nStart small : Begin with 100 examples to validate your approach before scaling.\nCustom fields : Add any metadata your reward function needs for evaluation.\nReward Function : Design for speed and scalability using AWS Lambda.\n\nTo get started with Amazon Nova RFT job on Amazon SageMaker Training Jobs, see the SFT and RFT notebooks.\n\nSageMaker HyperPod\nSageMaker HyperPod delivers enterprise-grade infrastructure for large-scale RFT workloads with persistent Kubernetes-based clusters optimized for distributed training. This tier builds on all the features available in SageMaker Training Jobs—including checkpoint management, iterative training workflows, LoRA and full-rank training options, and flexible deployment— on a much larger scale with dedicated compute resources and specialized networking configurations. The RFT implementation in HyperPod is optimized for higher throughput and faster convergence through state-of-the-art asynchronous reinforcement learning algorithms, where inference servers and training servers work independently at full speed. These algorithms account for this asynchrony and implement cutting-edge techniques used to train foundation models. HyperPod also provides advanced data filters that give you granular control over the training process and reduce the chances of crashes. You gain granular control over hyperparameters to maximize throughput and performance. HyperPod is designed for ML platform teams and research organizations that need to push the boundaries of RFT at scale. Amazon Nova RFT uses YAML recipe files to configure training jobs. You can obtain base recipes from the SageMaker HyperPod recipes repository.\n\nFor more information, see the RFT based evaluation to get started with Amazon Nova RFT job on Amazon SageMaker HyperPod.\n\nNova Forge\nNova Forge provides advanced reinforcement feedback training capabilities designed for AI research teams and practitioners in building sophisticated agentic applications. By breaking free from single-turn interaction and Lambda timeout constraints, Nova Forge enables complex, multi-turn workflows with custom-scaled environments running in your own VPC. This architecture gives you complete control over trajectory generation, reward functions, and direct interaction with training and inference servers capabilities essential for frontier AI applications that standard RFT tiers cannot support. Nova Forge uses Amazon SageMaker HyperPod as the training platform along with providing other features such as data mixing with the Amazon Nova curated datasets along with intermediate checkpoints.\nKey Features:\n\nMulti-turn conversation support\nReward functions with >15-minute execution time\nAdditional algorithms and tuning options\nCustom training recipe modifications\nState-of-the-art AI techniques\n\nEach tier in this progression builds on the previous one, offering a natural growth path as your RFT needs to evolve. Start with Amazon Bedrock for initial experiments, move to SageMaker Training Jobs as you refine your approach, and graduate to HyperPod or Nova Forge using HyperPod for specialized use cases. This flexible architecture ensures you can implement RFT at the level of complexity that matches your current needs while providing a clear path forward as those needs grow.\nSystematic approach to reinforcement fine-tuning (RFT)\nReinforcement fine-tuning (RFT) progressively improves pre-trained models through structured, reward-based learning iterations. The following is a systematic approach to implementing RFT.\n\nStep 0: Evaluate baseline performance\nBefore starting RFT, evaluate whether your model performs at a minimally acceptable level. RFT requires that the model can produce at least one correct solution among several attempts during training.\nKey requirement: Group relative policies require outcome diversity across multiple rollouts (typically 4-8 generations per prompt) to learn effectively. The model needs at least one success or at least one failure among the attempts so it can distinguish between positive and negative examples for reinforcement. If all rollouts consistently fail, the model has no positive signal to learn from, making RFT ineffective. In such cases, you should first use supervised fine-tuning (SFT) to establish basic task capabilities before attempting RFT. In cases where the failure modes are primarily due to lack of knowledge, in those cases as well SFT might be more effective starting point, whereas if the failure modes are due to poor reasoning, then RFT might be a better option to optimize on reasoning quality.  \nStep 1: Identify the right dataset and reward function\nSelect or create a dataset of prompts that represent the scenarios your model will encounter in production. More importantly, design a reward function that:\n\nCrisply follows what your evaluation metrics track : Your reward function should directly measure the same qualities you care about in production.\nCaptures what you need from the model : Whether that’s correctness, efficiency, style adherence, or a combination of objectives.\n\nStep 2: Debug and iterate\nMonitor training metrics and model rollouts throughout the training process\nTraining metrics to watch:\n\nReward trends over time (should generally increase)\nPolicy divergence (KL) from the base model\nGeneration length over time\n\nModel rollout analysis:\n\nSample and review generated outputs at regular intervals\nTrack how the model’s behavior evolves across training steps\n\nCommon issues and solutions\nIssues solvable directly in the reward function:\n\nFormat correctness : Add reward penalties for malformed outputs\nLanguage mixing : Penalize unwanted language switches\nGeneration length : Reward appropriate response lengths for your use case\n\nIssues requiring dataset/prompt improvements:\n\nLimited coverage : Create a more comprehensive prompt set covering various difficulty\nLack of exploration diversity : Ensure prompts allow the model to explore diverse scenarios and edge cases\n\nRFT is an iterative process. Use insights from each training run to refine your reward function, expand your prompt set, or adjust hyperparameters before the next iteration.\nKey RFT features and when to choose what\nThis section outlines the key features of RFT through a systematic breakdown of its core components and capabilities for effective model optimization.\nFull Rank compared to LoRA\nRFT supports two training approaches with different resource tradeoffs. Full Rank training updates all model parameters during training, providing maximum model adaptation potential but requiring more computational resources and memory. Low-Rank Adaptation (LoRA) offers parameter-efficient fine-tuning that updates only a small subset of parameters through lightweight adapter layers while keeping most of the model frozen.\nLoRA requires significantly less computational resources and results in smaller model artifacts. Importantly, LoRA models deployed in Amazon Bedrock support on-demand inference —you don’t need dedicated instances and only pay for the tokens you use. This makes LoRA an excellent default starting point: you can quickly iterate and validate your customized model without upfront infrastructure costs. As your traffic demand grows or high-performance requirements justify the investment, you can transition to full rank training with dedicated provisioned throughput instances for maximum throughput and lowest latency.\nReasoning compared to non-reasoning\nRFT supports both reasoning and non-reasoning models, each optimized for different types of tasks. Reasoning models generate explicit intermediate thinking steps before producing final answers, making them ideal for complex analytical tasks like mathematical problem-solving, multi-step logical deduction, and code generation where showing the reasoning process adds value. You can configure reasoning effort levels—high for maximum reasoning capability or low for minimal overhead. Non-reasoning models provide direct responses without showing intermediate reasoning steps, optimizing speed and cost. They’re best suited for tasks like chat-bot style Q&A where you want faster execution without the reasoning overhead, though this may result in lower quality outputs compared to reasoning mode. The choice depends on your task requirements: use reasoning mode when the intermediate thinking steps improve accuracy, and you need maximum performance on complex problems. Use non-reasoning mode when you prioritize speed and cost efficiency over the potential quality improvements that explicit reasoning provides.\nWhen to Use RFT compared to SFT\n\nMethod\nWhen it works best\nStrengths\nLimitations\n\nSupervised fine‑tuning (SFT)\nWell‑defined tasks with clear desired outputs, for example, “Given X, the correct output is Y.”\n• Directly teaches factual knowledge (for example, “Paris is the capital of France”) • Ideal when you have high‑quality prompt‑response pairs • Provides consistent formatting and specific output structures\n• Requires explicit, labeled examples for every desired behavior • May struggle with tasks that involve ambiguous or multiple valid solutions\n\nReinforcement fine‑tuning (RFT)\nScenarios where a reward function can be defined, even if only one valid solution exists\n• Optimizes complex reasoning tasks • Generates its own training data efficiently, reducing the need for many human‑labeled examples • Allows balancing competing objectives (accuracy, efficiency, style)\n• Needs the model to produce at least one correct solution among several attempts (typically 4‑8) • If the model consistently fails to generate correct solutions, RFT alone will not be effective\n\nCase study: Financial Analysis Benchmark (FinQA) optimization with RFT\nIn this case study, we will walk users through an example case study of FinQA, a financial analysis benchmark, and use that to demonstrate the optimization achieved in responses. In this example we will use 1000 samples from the FinQA public dataset.\nStep 1: Data preparation\nPrepare the dataset in a format that’s compatible with RFT schema as mentioned RFT on Nova . RFT data follows the OpenAI conversational format. Each training example is a JSON object containing. For our FinQA dataset, post formatting an example data point in train.jsonl will look as shown below:\n\n{\n\"messages\": [\n{\n\"role\": \"user\",\n\"content\": [\n{\n\"type\": \"text\",\n\"text\": \"Context: ....\\n\\nQuestion: ....\\n\\nProvide your answer in the following format:\\nANSWER: [your answer here]\"\n}\n]\n}\n],\n\"reference_answer\": {\n\"answer\": \"65.3%\"\n},\n\"data_source\": \"finqa\"\n}\n\nRequired fields:\n\nmessages : Array of conversational turns with system, user, and optionally assistant roles\nreference_answer : Expected output or evaluation criteria for reward calculation\n\nOptional fields:\n\nid : Unique identifier for tracking and deduplication\ntools : Array of function definitions available to the model\nCustom metadata fields : Any additional metadata to be used while calculating rewards (for example, task_id , difficulty_level , domain )\n\nStep 2: Building the reward and grader function\nThe reward function is the core component that evaluates model responses and provides feedback signals for training. It must be implemented as an AWS Lambda function that accepts model responses and returns reward scores. Currently, AWS Lambda functions come with a limitation of up to 15 minutes execution time. Adjust the timeout of the Lambda function based on your needs.\nBest practices:\nThe following are the recommendations to optimize your RFT implementation:\n\nStart small : Begin with 100-200 examples and few training epochs.\nBaseline with SFT first : If reward scores are consistently low, perform SFT before RFT.\nDesign efficient reward functions : Execute in seconds, minimize external API calls.\nMonitor actively : Track average reward scores, watch for overfitting.\nOptimize data quality : Ensure diverse, representative examples.\n\nStep 3: Launching the RFT job\nOnce we have data prepared, we will launch RFT using a SageMaker Training Jobs. The two key inputs for launching the RFT job are the input dataset (input_data_s3) and the reward function Lambda ARN. Here we use the RFT container and RFT recipe as defined in the following example. The following is a snippet of how you can kick off the RFT Job: rft_training_job =rft_launcher(train_dataset_s3_path, reward_lambda_arn)\nFunction:\n\ndef rft_launcher(train_S3_uri, reward_lambda_arn):\ninstance_type = \"ml.p5.48xlarge\"\ninstance_count = 4\nrecipe = \"fine-tuning/nova/nova_2_0/nova_lite/RFT/nova_lite_2_0_p5_gpu_lora_rft\"\nimage_uri = \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-fine-tune-repo:SM-TJ-RFT-V2-latest\"\nmodel_id = \"nova-lite-2/prod\"\njob_name = f\"rft-lora-{model_id.split('/')[0].replace('.', '-')}\"\nif default_prefix:\noutput_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\nelse:\noutput_path = f\"s3://{bucket_name}/{job_name}\"\n\nrecipe_overrides = {\n\"run\": {\n\"reward_lambda_arn\": reward_lambda_arn,\n},\n\"training_config\": {\n\"rollout\": {\n\"rewards\": {\n\"api_endpoint\": {\n\"lambda_arn\": reward_lambda_arn\n}\n}\n}\n}\n}\n\nestimator = PyTorch(\noutput_path=output_path,\nbase_job_name=job_name,\nrole=role,\ndisable_profiler=True,\ndebugger_hook_config=False,\ninstance_count=instance_count,\ninstance_type=instance_type,\nrecipe_overrides=recipe_overrides,\ntraining_recipe=recipe,\nsagemaker_session=sess,\nimage_uri=image_uri\n)\ntrain_input = TrainingInput(\ns3_data =train_S3_uri,\ndistribution=\"FullyReplicated\"\n)\nestimator.fit(inputs={\"train\": train_input}, wait=False)\ntraining_job_name = estimator.latest_training_job.name\nprint('Training Job Name: {}'.format(training_job_name))\nreturn training_job_name\n\nNote : To lower the cost of this experiment, you can set instance count to 2 instead of 4 for LoRA\nStep 4: Launching the RFT Eval Job\nOnce the RFT job is completed, you can also take the checkpoint generated after RFT and use that to evaluate the model. This checkpoint can then be used in an evaluation recipe, overriding the base model, and executed in our evaluation container. The following is a snippet of how you can use the generated checkpoint for evaluation. Note the same code can also be used for running a baseline evaluation prior to checkpoint evaluation.\n\nThe function can be called using the following command:\n\nFor baselining use:\n\nrft_base_eval_job =rft_eval_launcher(test_dataset_s3_path, reward_lambda_arn)\n\nFor post RFT evaluation use:\n\nrft_base_eval_job =rft_eval_launcher( test_dataset_s3_path, reward_lambda_arn, escrow_checkpoint_uri)\n\nFunction:\n\ndef rft_eval_launcher(test_S3_uri, reward_lambda_arn, chkpt_uri=None):\ninstance_type = \"ml.p5.48xlarge\"\ninstance_count = 1\nrecipe = \"evaluation/nova/nova_2_0/nova_lite/nova_lite_2_0_p5_48xl_gpu_rft_eval\"\nimage_uri = \"708977205387.dkr.ecr.us-east-1.amazonaws.com/nova-evaluation-repo:SM-TJ-Eval-V2-latest\"\nmodel_id = \"nova-lite-2/prod\"\njob_name = f\"rft-eval-{model_id.split('/')[0].replace('.', '-')}\"\nif default_prefix:\noutput_path = f\"s3://{bucket_name}/{default_prefix}/{job_name}\"\nelse:\noutput_path = f\"s3://{bucket_name}/{job_name}\"\nrecipe_overrides = {\n\"rl_env\": {\n\"reward_lambda_arn\": reward_lambda_arn\n}\n}\nif chkpt_uri is not None:\nrecipe_overrides['run']= {\n\"model_name_or_path\": chkpt_uri\n}\n\nestimator = PyTorch(\noutput_path=output_path,\nbase_job_name=job_name,\nrole=role,\ndisable_profiler=True,\ndebugger_hook_config=False,\ninstance_count=instance_count,\ninstance_type=instance_type,\nrecipe_overrides=recipe_overrides,\ntraining_recipe=recipe,\nsagemaker_session=sess,\nimage_uri=image_uri\n)\ntest_input = TrainingInput(\ns3_data=test_S3_uri,\ndistribution=\"FullyReplicated\"\n)\nestimator.fit(inputs={\"train\": test_input}, wait=False)\neval_job_name = estimator.latest_training_job.name\nprint('Evaluation Job Name: {}'.format(eval_job_name))\nreturn eval_job_name\n\nStep 5: Monitoring the RFT metrics and iterating accordingly\nOnce the Jobs are launched, you can monitor the Job progress in Amazon CloudWatch logs for SageMaker Training Jobs to look at the RFT specific metrics. You can also monitor the CloudWatch logs of your reward Lambda function to verify how the rollouts and rewards are working. It is good practice to validate the reward Lambda function is calculating rewards as expected and is not getting into “reward hacking” (maximizing the reward signal in unintended ways that don’t align with the actual objective).\nReview the following key metrics:\n\nCritic reward distribution metrics : These metrics (critic/rewards/mean, critic/rewards/max, critic/rewards/min) help in finding how the reward shape looks like and if the rewards are on a path of gradual increase.\nModel exploratory behavior metrics : This metrics help us in understanding the exploratory nature of the model. The higher actor/entropy indicates higher policy variation and model’s ability to explore newer paths.\n\nConclusion\nWith RFT you can perform model customization through evaluation-based learning, requiring only prompts and quality criteria rather than massive, labeled datasets. For fully managed implementation, start with Amazon Bedrock. If you need more flexible control, move to SageMaker Training Jobs. For enterprise-scale workloads, SageMaker HyperPod provides the necessary infrastructure. Alternatively, explore Nova Forge for multi-turn agentic applications with custom reinforcement learning environments.\n\nAbout the authors\n\nBharathan Balaji\nBharathan Balaji is a Senior Applied Scientist at Amazon Web Services, working on reinforcement learning and foundation model services. His work focuses on building AI capabilities that help customers transform their businesses.\n\nAnupam Dewan\nAnupam Dewan is a Senior Solutions Architect working in Amazon Nova team with a passion for generative AI and its real-world applications. He focuses on Nova customization and Nova Forge, helping enterprises realize the true potential of LLMs with power of customization. He is also passionate about teaching data science, and analytics and helping Enterprise build LLMs that work for their businesses. Outside of work, you can find him hiking, volunteering or enjoying nature.\n\nVignesh Radhakrishnan\nVignesh Radhakrishnan is a Senior Software Engineer at AWS specializing in machine learning, with a passion for the engineering and scientific challenges inherent in reinforcement learning systems and distributed training. Outside of work, he enjoys volleyball and hiking with his family.\n\nChakravarthy Nagarajan\nChakravarthy Nagarajan is a Principal Solutions Architect specialized in machine learning and high performance computing. In his current role, he helps customers solve real-world, complex business problems using machine learning and generative AI solutions.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "亚马逊AWS推出了强化微调（RFT）技术，用于定制其Amazon Nova AI模型。RFT是一种通过反馈而非模仿的学习方法，它允许用户通过定义奖励函数（如基于规则或AI判断）来优化模型，而无需大量标注数据。该技术特别适用于需要领域专业知识或复杂推理的任务，如代码生成、客户服务和财务分析，可以优化模型的推理路径，减少token使用并降低成本。亚马逊提供了多个实现层级，从完全托管的Amazon Bedrock到高级的SageMaker HyperPod和Nova Forge，支持从简单到复杂的定制需求。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "Amazon Nova",
        "Reinforcement Fine-Tuning",
        "AWS",
        "Amazon Bedrock",
        "SageMaker"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:06.826213Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "b35bbb9cf61b72a4",
    "title": "Large model inference container – latest capabilities and performance enhancements",
    "url": "https://aws.amazon.com/blogs/machine-learning/large-model-inference-container-latest-capabilities-and-performance-enhancements/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T17:45:59Z",
    "summary": "AWS recently released significant updates to the Large Model Inference (LMI) container, delivering comprehensive performance improvements, expanded model support, and streamlined deployment capabilities for customers hosting LLMs on AWS. These releases focus on reducing operational complexity while delivering measurable performance gains across popular model architectures.",
    "content": "Modern large language model (LLM) deployments face an escalating cost and performance challenge driven by token count growth. Token count, which is directly related to word count, image size, and other input factors, determines both computational requirements and costs. Longer contexts translate to higher expenses per inference request. This challenge has intensified as frontier models now support up to 10 million tokens to accommodate growing context demands from Retrieval Augmented Generation (RAG) systems and coding agents that require extensive code bases and documentation. However, industry research reveals that a significant portion of token count across inference workloads is repetitive, with the same documents and text spans appearing across numerous prompts. These data “hot spots” represent an opportunity. By caching frequently reused content, organizations can achieve cost reductions and performance improvements for their long-context inference workloads.\nAWS recently released significant updates to the Large Model Inference (LMI) container, delivering comprehensive performance improvements, expanded model support, and streamlined deployment capabilities for customers hosting LLMs on AWS. These releases focus on reducing operational complexity while delivering measurable performance gains across popular model architectures.\nLMCache support: transforming long-context performance\nOne of the most significant capabilities introduced across the newest releases of LMI is comprehensive LMCache support, which fundamentally transforms how organizations can handle long-context inference workloads. LMCache is an open source KV caching solution that extracts and stores KV caches that are generated by modern LLM engines, sharing these caches across engines and queries to help improve inference performance.\nUnlike traditional prefix-only caching systems, LMCache reuses KV caches of reused text, not necessarily only prefixes, in a serving engine instance. The system operates at the chunk level, identifying commonly repeated text spans across documents or conversations and storing their precomputed KV cache. This approach enables multi-tiered storage spanning GPU memory, CPU memory, and disk/remote backends, with intelligent caching that maintains an internal index mapping token sequences to cached KV entries. The newest releases of LMI introduce automatic LMCache configuration, streamlining KV cache deployment and optimization. This low-code no-code (LCNC) interface helps customers seamlessly enable this advanced performance feature without complex manual configuration. By offloading KV cache from GPU memory to CPU RAM or NVMe storage, LMCache enables efficient handling of long-context scenarios while helping deliver latency improvements.\nComprehensive testing across various model sizes and context lengths reveals performance improvements that help transform the user experience. For workloads with repeated context, LMCache achieves faster Time to First Token (TTFT) when processing multi-million token contexts. Organizations deploying LMI can configure CPU offloading when instance RAM permits for optimal performance or use NVMe with O_DIRECT enabled for workloads requiring larger cache capacity. Implementing session-based sticky routing on Amazon SageMaker AI helps maximize cache result rates, making sure that requests from the same session consistently route to instances with relevant cached content.\nLMCache performance benchmarks\nComprehensive testing across various model sizes and context lengths reveals performance improvements that improve the user experience for long-context inference workloads. The testing methodology adapted the LMCache Long Doc QA benchmark to work with the LMI container, consisting of three rounds: pre-warmup for cold-start initialization, a warmup round to populate LMCache storage, and a query round to measure performance when retrieving from cache. Benchmarks were conducted on p4de.24xlarge instances (8× A100 GPUs, 1.1TB RAM, NVMe SSD) using Qwen models with 46 documents of 10,000 tokens each (460,000 total tokens) and 4 concurrent requests.\nFor workloads with repeated context, LMCache achieves faster Time to First Token (TTFT) when processing multi-million token contexts. CPU offloading delivers performance improvements with 2.18x speedup in total request latency compared to baseline (52.978s → 24.274s) and 2.65x faster TTFT (1.161s → 0.438s). NVMe storage with O_DIRECT enabled approaches CPU performance (0.741s TTFT) while supporting TB-scale caching capacity, achieving 1.84x speedup in total request latency and 1.57x faster TTFT. These results demonstrate 62% TTFT reduction and 54% request latency reduction, closely aligning with published LMCache benchmarks. The variation in improvement percentages can likely be attributed to hardware and minor configuration differences. These latency reductions translate directly to cost savings, because the 54% reduction in request processing time allows the same infrastructure to handle more than twice the request volume, effectively halving per-request compute costs.\nPerformance characteristics vary significantly by model size due to differences in KV cache memory requirements per token. Larger models require substantially more memory per token (Qwen2.5-1.5B: 28 KB/token, Qwen2.5-7B: 56 KB/token, Qwen2.5-72B: 320 KB/token), meaning they exhaust GPU KV cache capacity at much shorter context lengths. Qwen 2.5-1.5B can store KV cache for up to 2.6M tokens in GPU memory, while Qwen 2.5-72B reaches its limit at 480K tokens. This means LMCache delivers value at shorter contexts for larger models. A 72 B model can benefit from CPU offloading starting around 500K tokens with 4-6x speedups, whereas smaller models only require offloading at extreme context lengths beyond 2.5M tokens. Organizations deploying LMI can configure CPU offloading when instance RAM permits for optimal performance or use NVMe with O_DIRECT enabled for workloads requiring larger cache capacity. Implementing session-based sticky routing on SageMaker AI helps maximize cache result rates, making sure that requests from the same session consistently route to instances with relevant cached content.\nHow to use LMCache\nThere are two main methods for configuring LMCache as defined in the GitHub documentation . The first is a manual configuration approach, and the second is an automated configuration made available in new versions of LMI.\nManual configuration For manual configuration, customers create their own LMCache configuration and specify it in properties, files, or environment variables:\noption.lmcache_config_file=/path/to/your/lmcache_config.yaml# OROPTION_LMCACHE_CONFIG_FILE=/path/to/your/lmcache_config.yaml\nThis approach gives customers control over LMCache settings, so that they can customize cache storage backends, chunk sizes, and other advanced parameters according to their specific requirements.\nAutomatic configuration For streamlined deployments, customers can enable automatic LMCache configuration similarly:\noption.lmcache_auto_config=True# OROPTION_LMCACHE_AUTO_CONFIG=True\nAuto-configuration automatically generates an LMCache configuration based on available CPU/disk space on the host machine. This deployment option only supports Tensor Parallelism deployments, assumes /tmp is mounted on NVMe storage for disk-based caching, and requires maxWorkers=1. These settings are assumed with auto-configuration, which is designed for serving a single model per container instance. For serving multiple models or model copies, customers should use Amazon SageMaker AI inference components , which facilitates resource isolation between models and model copies.\nThe automatic configuration feature streamlines KV cache deployment by alleviating the need for manual YAML configuration files so that customers can quickly get started with LMCache optimization.\nDeployment recommendations\nBased on comprehensive benchmarking results and deployment experience, several recommendations emerge for optimal LMI deployment:\n\nConfigure CPU offloading when instance RAM permits, helping deliver optimal performance for most workloads\nUse NVMe with O_DIRECT enabled for workloads requiring larger cache capacity beyond available RAM\nImplement session-based sticky routing on SageMaker AI to help maximize cache result rates and facilitate consistent performance\nConsider model architecture when configuring offloading thresholds, as models with different KV head configurations will have different optimal settings\nUse automatic LMCache configuration to streamline deployment and reduce operational complexity\n\nEnhanced performance with EAGLE speculative decoding\nThe newest releases of LMI help deliver performance improvements through support for EAGLE speculative decoding techniques. Extrapolation Algorithm for Greater Language-model Efficiency (EAGLE), speeds up large language model decoding by predicting future tokens directly from the hidden layers of the model. This approach generates draft tokens that the primary model validates in parallel, helping reduce overall generation latency while maintaining output quality.\nConfiguring EAGLE speculative decoding is straightforward, requiring only specification of the draft model path and number of speculative tokens in your deployment configuration. This enables organizations to achieve better performance for LLM hosting workloads with benefits for high-concurrency production deployments and reasoning-focused models.\nExpanded model support and multimodal capabilities\nThe newest releases of LMI help deliver comprehensive support for cutting-edge open source models, including DeepSeek v3.2, Mistral Large 3, Ministral 3, and the Qwen3-VL series. Performance optimizations help improve both throughput and Time to First Token (TTFT) for large-scale model serving across these architectures. Expanded multimodal capabilities include FlashAttention ViT support, now serving as the default backend for vision-language models. EAGLE speculative decoding improvements bring multi-step CUDA graph support and multimodal support with Qwen3-VL, enabling faster inference for vision-language workloads. With these enhancements, organizations can deploy and scale foundation models (FMs) faster and more efficiently, which helps to reduce time-to-production while lowering operational complexity.\nLoRA adapter hosting improvements\nThe newest releases of LMI bring notable enhancements to hosting multiple LoRA adapters on SageMaker AI. LoRA adapters are now “lazy” loaded—when creating an inference component, the adapter’s component becomes available almost immediately, but actual loading of adapter weights and registering with the inference engine happens on the first invocation. This approach helps reduce deployment time while maintaining flexibility for multi-tenant scenarios.\nCustom input and output preprocessing scripts are now supported for both base models and adapters, with each inference component hosting LoRA adapters able to have different scripts. This enables adapter-specific formatting logic without modifying core inference code, supporting multi-tenant deployments where different adapters apply distinct formatting rules to the same underlying model.\nCustom output formatters provide a flexible mechanism for transforming model responses before they are returned to clients so that organizations can standardize output formats, add custom metadata, or implement adapter-specific formatting logic. These formatters can be defined at the base model level to apply to the responses by default, or at the adapter level to override base model behavior for LoRA adapters. Common use cases include adding processing timestamps and custom metadata, transforming generated text with prefixes or formatting, calculating and injecting custom metrics, implementing adapter-specific output schemas for different client applications, and standardizing response formats across heterogeneous model deployments.\nGet started today\nThe newest releases of LMI represent significant steps forward in large model inference capabilities. Organizations can deploy cutting-edge LLMs with greater performance and flexibility with the following:\n\ncomprehensive LMCache support across the releases\nEAGLE speculative decoding for accelerated inference\nexpanded model support including cutting-edge multimodal capabilities\nenhanced LoRA adapter hosting\n\nThe container’s configurable options provide the flexibility to fine-tune deployments for specific needs, whether optimizing for latency, throughput, or cost. With the comprehensive system capabilities of Amazon SageMaker AI, you can focus on delivering AI-powered solutions that help drive business value rather than managing infrastructure.\nExplore these capabilities today when deploying your generative AI models on AWS and leverage the performance improvements and streamlined deployment experience to help accelerate your production workloads.\n\nAbout the authors\n\nDmitry Soldatkin\nDmitry Soldatkin is a Senior Machine Learning Solutions Architect at AWS, helping customers design and build AI/ML solutions. Dmitry’s work covers a wide range of ML use cases, with a primary interest in generative AI, deep learning, and scaling ML across the enterprise. He has helped companies in many industries, including insurance, financial services, utilities, and telecommunications. He has a passion for continuous innovation and using data to drive business outcomes.\n\nSadaf Fardeen\nSadaf Fardeen leads Inference Optimization charter for SageMaker. She owns optimization and development of LLM inference containers on SageMaker.\n\nLokeshwaran Ravi\nLokeshwaran Ravi is a Senior Deep Learning Compiler Engineer at AWS, specializing in ML optimization, model acceleration, and AI security. He focuses on enhancing efficiency, reducing costs, and building secure ecosystems to democratize AI technologies, making cutting-edge ML accessible and impactful across industries.\n\nSuma Kasa\nSuma Kasa is an ML Architect with the SageMaker Service team focusing on the optimization and development of LLM inference containers on SageMaker.Author bio\n\nDan Ferguson\nDan Ferguson  is a Sr. Solutions Architect at AWS, based in New York, USA. As a machine learning services expert, Dan works to support customers on their journey to integrating ML workflows efficiently, effectively, and sustainably.\n\nSheng Mousa\nSheng Mouaa  is a Software Development Engineer at AWS. She works on the serving and optimization team, focused on building efficient and scalable solutions for large language model inference",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": {
      "summary": "AWS最近更新了其Large Model Inference (LMI)容器，引入了一系列性能增强功能以应对大语言模型推理中因token增长带来的成本和性能挑战。最关键的是LMCache支持，这是一个开源KV缓存解决方案，通过缓存重复的文本KV缓存，在长上下文推理中减少时间和第一token时间（TTFT），显著降低延迟和成本。此外，支持EAGLE推测解码加速推理，扩展了对前沿模型如DeepSeek v3.2和Qwen3-VL系列的支持，并改进了LoRA适配器的托管方式，使部署更灵活。这些更新旨在降低运营复杂性，提升性能，帮助企业更高效地部署和管理LLM，加速生产工作负载。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "AWS",
        "LMI",
        "LMCache",
        "KV缓存",
        "EAGLE推测解码"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:12.684275Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "d39040e7a9aa414f",
    "title": "CORPGEN advances AI agents for real work",
    "url": "https://www.microsoft.com/en-us/research/blog/corpgen-advances-ai-agents-for-real-work/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T17:06:34Z",
    "summary": "By mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today’s best models are evaluated one […]\nThe post CORPGEN advances AI agents for real work appeared first on Microsoft Research .",
    "content": "At a glance\n\nToday’s AI agent benchmarks test one task at a time, while real workplace productivity requires managing dozens of interdependent tasks at once. To reflect this, we created a setting called Multi-Horizon Task Environments (MHTEs).\n\nUnder multi-task loads, leading computer-using agents degrade sharply, with completion rates dropping from 16.7% to 8.7%.\n\nCORPGEN introduces digital employees , with hierarchical planning, memory isolation, and experiential learning, delivering up to 3.5 times higher completion rates than baselines across three independent agent backends.\n\nBecause CORPGEN is architecture-agnostic and modular, its gains come from system design rather than any single base model, and it benefits directly as underlying models improve.\n\nBy mid-morning, a typical knowledge worker is already juggling a client report, a budget spreadsheet, a slide deck, and an email backlog, all interdependent and all demanding attention at once. For AI agents to be genuinely useful in that environment, they will need to operate the same way, but today’s best models are evaluated one task at a time, not dozens at once.\n\nIn our paper, “ CORPGEN: Simulating Corporate Environments with Autonomous Digital Employees in Multi-Horizon Task Environments ,” we propose an agent framework that equips AI with the memory, planning, and learning capabilities to close that gap.\n\nIntroducing Multi-Horizon Task Environments\n\nReplicating the reality of workplace multitasking requires a new kind of evaluation environment. In response, we developed Multi-Horizon Task Environments (MHTEs), settings where an agent must manage multiple complex tasks simultaneously. Each task requires 10 to 30 dependent steps within a single session spanning five hours.\n\nTo determine what a benchmark would need to test, we ran MHTEs at scale on some of today’s leading AI agents, exposing four weaknesses. First, memory fills up. An agent cannot hold details for multiple active tasks at once. Second, information from one task interferes with reasoning about another. Third, tasks don’t depend on each other in simple sequences. They form complex webs where an agent must constantly check whether upstream work is finished before it can move forward on anything downstream. Fourth, every action cycle requires reprioritizing across all active tasks, not simply resuming where the agent left off.\n\nWe also tested three independent agent systems under increasing loads. As the number of concurrent tasks rose from 12 to 46, completion rates fell from 16.7% to 8.7% across all systems.\n\nCORPGEN’s architecture\n\nCORPGEN introduces digital employees : LLM-powered AI agents with persistent identities, role-specific expertise, and realistic work schedules. They operate Microsoft Office applications through GUI automation and perform consistently within MHTEs over hours of continuous activity. Figure 1 illustrates how a digital employee moves through a full workday.\n\nFigure 1. Each day begins with a structured plan and memory loaded from previous sessions. The agent then works through overlapping tasks in repeated cycles, storing key outcomes at day’s end to inform the next session.\n\nCORPGEN addresses each of the four weaknesses of concurrent task execution—memory overload, cross-task interference, dependency complexity, and reprioritization—in a targeted way. Hierarchical planning breaks objectives into daily goals and then into moment-to-moment decisions, allowing the agent to act from a structured plan instead of reviewing all available tasks before each step.\n\nSubagents perform complex operations like web research in isolated contexts, preventing cross-task contamination. A tiered memory system enables selective recall of task-related information rather than retaining everything in active context. Adaptive summarization compresses routine observations while preserving critical information, keeping memory growth controlled.\n\nBecause these mechanisms are not tied to a specific base model, we tested CORPGEN across three different agents. In each case, we observed consistent gains. The improvements came from the architecture, not from the strength of any particular model. Figure 2 shows how they fit together within CORPGEN’s architecture.\n\nFigure 2. Four mechanisms support concurrent task execution in CORPGEN: hierarchical planning, isolated subagents, tiered memory, and adaptive summarization.\n\nHow digital employees collaborate\n\nWhen multiple digital employees operate in the same environment, collaboration takes shape through standard communication channels, without predefined coordination rules. One employee sends an email requesting data; another picks it up in the next cycle, uses its memory to process it, and responds. This exchange mirrors real workplace communication.\n\nThere is no shared internal state between agents. Coordination occurs entirely through email and Microsoft Teams, the same channels many workers use. Over time, these independent exchanges form recognizable organizational patterns. Some agents take on leadership roles; others provide support; shared documents become the connective tissue.\n\nWhen a communication path breaks, such as an email delivery error, agents reroute messages through alternate channels to keep work moving. The result is a virtual organization that behaves like a real one without being explicitly programmed to do so.\n\nEvaluating CORPGEN\n\nWe evaluated CORPGEN on a multi-task benchmark that combined up to 46 tasks into a single six-hour session. Three findings stood out.\n\nBaselines degrade as load increases; CORPGEN does not. All three baseline agent systems showed steady performance declines as task load rose. CORPGEN, by contrast, maintained or improved its completion rates at higher loads. At 46 tasks, CORPGEN completed 15.2% of tasks, compared with 4.3% for the baselines, roughly 3.5 times more.\n\nExperiential learning drives the largest gains. We introduced CORPGEN’s components sequentially: first the orchestration layer, then cognitive tools, and finally experiential learning. The first two produced moderate improvements. Experiential learning, in which agents store records of completed tasks and reuse them when they encounter structurally similar work, produced the largest increase, raising completion rates from 8.7% to 15.2%.\n\nEvaluation methodology changes the picture. When we inspected the actual output files produced by agents, the results agreed with human judgements roughly 90% of the time. Evaluation based on screenshots and action logs agreed only about 40% of the time. This gap suggests that common evaluation approaches may underestimate what agents actually accomplish in practice.\n\nAzure AI Foundry Labs\n\nGet a glimpse of potential future directions for AI, with these experimental technologies from Microsoft Research.\n\nAzure AI Foundry\n\nOpens in a new tab\n\nImplications and looking forward\n\nThe results suggest that memory and retrieval, not just raw model capability, may be a key bottleneck in getting agents to work in the real world. The largest gains came from experiential learning. Agents that learn from prior successes and apply those patterns to structurally similar tasks build an advantage over systems that respond to each task in isolation.\n\nCORPGEN also opens a new lens on how AI agents collaborate. Next steps include testing whether agents can maintain memory across multiple workdays and how they coordinate when working in teams. We are also exploring ways to make agents faster and more reliable by combining different methods of interacting with software.\n\nAcknowledgments\n\nThis work is a result of a collaboration between the Office of the CTO at Microsoft and the Microsoft AI Development Accelerator Program (MAIDAP). We would like to thank the Microsoft Security Research team for providing resources that supported this research. We also thank the members of the Microsoft UFO2 (opens in new tab) team and the Mem0 (opens in new tab) project for their open-source contributions, which enabled key components of the CORPGEN architecture, and the OSWorld team for the benchmark that served as the foundation for our multi-task evaluation.\n\nFinally, we thank the many contributors to this research: Anjel Shaileshbhai Patel, Dayquan Julienne, Charlotte Siska, Manuel Raúl Meléndez Luján, Anthony Twum-Barimah, Mauricio Velazco, and Tianwei Chen.\nOpens in a new tab The post CORPGEN advances AI agents for real work appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": {
      "summary": "Microsoft Research开发了CORPGEN框架，以提升AI agents在多任务工作环境中的实用性。该框架引入数字员工概念，通过分层规划、内存隔离、经验学习和自适应摘要等机制，解决了当前AI agents在处理相互依赖的多任务时面临的挑战，如内存过载、跨任务干扰和依赖复杂性。CORPGEN在创新的Multi-Horizon Task Environments中测试，与基准系统相比，任务完成率最高提升3.5倍，尤其是在46个任务的高负载下保持稳定性能。研究强调了内存和检索是关键瓶颈，经验学习带来最大增益，且CORPGEN架构不依赖特定基础模型，为AI agents在复杂工作场景（如办公室自动化）的应用奠定了基础，未来可能扩展到长期记忆和团队协作。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "CORPGEN",
        "Multi-Horizon Task Environments",
        "Microsoft Research",
        "AI agents",
        "digital employees"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:21.084027Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "88660b2973ba237c",
    "title": "Pacific Northwest National Laboratory and OpenAI partner to accelerate federal permitting",
    "url": "https://openai.com/index/pacific-northwest-national-laboratory",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T10:00:00Z",
    "summary": "OpenAI and Pacific Northwest National Laboratory introduce DraftNEPABench, a new benchmark evaluating how AI coding agents can accelerate federal permitting—showing potential to reduce NEPA drafting time by up to 15% and modernize infrastructure reviews.",
    "content": "OpenAI and Pacific Northwest National Laboratory introduce DraftNEPABench, a new benchmark evaluating how AI coding agents can accelerate federal permitting—showing potential to reduce NEPA drafting time by up to 15% and modernize infrastructure reviews.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI与太平洋西北国家实验室合作推出了名为DraftNEPABench的新基准测试，专门评估AI编码代理如何加速联邦许可程序。该测试显示，AI技术有潜力将国家环境政策法（NEPA）草案的起草时间减少高达15%，并现代化基础设施审查。这一举措对于提升政府工作效率、加速基础设施项目审批具有重要作用，展示了AI技术在公共管理领域的实际应用价值，可能推动相关技术的进一步发展和部署。",
      "category": "AI",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Pacific Northwest National Laboratory",
        "DraftNEPABench",
        "AI coding agents",
        "NEPA"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:23.260356Z",
    "analysis_status": "success",
    "analysis_error": null
  },
  {
    "id": "ce14f35f8b1c89f3",
    "title": "OpenAI Codex and Figma launch seamless code-to-design experience",
    "url": "https://openai.com/index/figma-partnership",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-26T06:00:00Z",
    "summary": "OpenAI and Figma launch a new Codex integration that connects code and design, enabling teams to move between implementation and the Figma canvas to iterate and ship faster.",
    "content": "OpenAI and Figma launch a new Codex integration that connects code and design, enabling teams to move between implementation and the Figma canvas to iterate and ship faster.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": {
      "summary": "OpenAI 和 Figma 共同推出了一项新的 Codex 集成功能，旨在无缝连接代码和设计工作流程。这项合作使开发团队能够直接在代码实现和 Figma 设计画布之间轻松切换，从而加快产品迭代和交付速度。核心改进在于通过 AI 驱动的 Codex 模型自动化处理代码与设计的转换，减少手动操作，提升团队协作效率。影响包括加速软件开发周期、增强跨职能团队沟通，并可能推动更多 AI 技术在设计领域的应用。",
      "category": "产品",
      "sentiment": "positive",
      "keywords": [
        "OpenAI",
        "Codex",
        "Figma",
        "Codex integration",
        "code-to-design"
      ]
    },
    "analyzed_at": "2026-02-27T04:02:22.296666Z",
    "analysis_status": "success",
    "analysis_error": null
  }
]