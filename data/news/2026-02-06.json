[
  {
    "id": "1202e7a67b5c5d68",
    "title": "How Associa transforms document classification with the GenAI IDP Accelerator and Amazon Bedrock",
    "url": "https://aws.amazon.com/blogs/machine-learning/how-associa-transforms-document-classification-with-the-genai-idp-accelerator-and-amazon-bedrock/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T20:41:52Z",
    "summary": "Associa collaborated with the AWS Generative AI Innovation Center to build a generative AI-powered document classification system aligning with Associa’s long-term vision of using generative AI to achieve operational efficiencies in document management. The solution automatically categorizes incoming documents with high accuracy, processes documents efficiently, and provides substantial cost savings while maintaining operational excellence. The document classification system, developed using the...",
    "content": "This is a guest post co-written with David Meredith and Josh Zacharias from Associa.\nAssocia, North America’s largest community management company, oversees approximately 7.5 million homeowners with 15,000 employees across more than 300 branch offices. The company manages approximately 48 million documents across 26 TB of data, but their existing document management system lacks efficient automated classification capabilities, making it difficult to organize and retrieve documents across multiple document types. Every day, employees spend countless hours manually categorizing and organizing incoming documents—a time-consuming, error-prone process that creates bottlenecks in operational efficiency and potentially results in operational delays and reduced productivity.\nAssocia collaborated with the AWS Generative AI Innovation Center to build a generative AI-powered document classification system aligning with Associa’s long-term vision of using generative AI to achieve operational efficiencies in document management. The solution automatically categorizes incoming documents with high accuracy, processes documents efficiently, and provides substantial cost savings while maintaining operational excellence. The document classification system, developed using the Generative AI Intelligent Document Processing (GenAI IDP) Accelerator , is designed to integrate seamlessly into existing workflows. It revolutionizes how employees interact with document management systems by reducing the time spent on manual classification tasks.\nThis post discusses how Associa is using Amazon Bedrock to automatically classify their documents and to help enhance employee productivity.\nSolution overview\nThe GenAI IDP Accelerator is a cloud-based document processing solution built on AWS that automatically extracts and organizes information from various document types. The system uses OCR technology and generative AI to convert unstructured documents into structured, usable data while scaling seamlessly to handle high document volumes.\nThe accelerator is built with a flexible, modular design using AWS CloudFormation templates that can handle different types of document processing while sharing core infrastructure for job management, progress tracking, and system monitoring. The accelerator supports three processing patterns. We use Pattern 2 for this solution using OCR ( Amazon Textrac t) and classification (Amazon Bedrock). The following diagram illustrates this architecture.\n\nWe optimized the document classification workflow by evaluating three key aspects:\n\nPrompt input – Full PDF document (all pages) vs. first page only\nPrompt design – Multimodal prompting with OCR data (using the Amazon Textract analyze_document_layout ) vs. document image only\nModel choice – Amazon Nova Lite, Amazon Nova Pro, Amazon Nova Premier, and Anthropic’s Claude Sonnet 4 on Amazon Bedrock\n\nThis comprehensive evaluation framework helped us identify the configuration that delivers the highest accuracy while minimizing processing inference costs for Associa’s specific document types and operational requirements. The evaluation dataset consists of 465 PDF documents across eight distinct document types. The dataset includes some samples identified as draft documents or email correspondences. These samples are categorized as document type Unknown due to insufficient classification criteria. The distribution of document types across classes is unbalanced, ranging from 6 samples for Policies and Resolutions to 155 samples for Minutes.\nEvaluation: Prompt input\nWe started our initial evaluation using full PDF documents, where all pages of a PDF were used as input to the prompt for classification. The following table shows the accuracy for full PDF classification using Amazon Nova Pro with OCR and image. We observed an average classification accuracy of 91% considering the different document types with an average cost of 1.10 cents per document.\n\nDocument Type\nNumber of Samples\nNumber of Samples Classified Correctly\nClassification Accuracy\nClassification Cost (in Cents)\n\nBylaws\n46\n42\n91%\n1.52c\n\nCCR Declarations\n22\n19\n86%\n1.55c\n\nCertificate of Insurance\n74\n74\n100%\n1.49c\n\nContracts\n71\n66\n93%\n1.48c\n\nMinutes\n155\n147\n95%\n1.47c\n\nPlat Map\n21\n20\n95%\n1.45c\n\nPolicies and Resolutions\n6\n5\n83%\n0.35c\n\nRules and Regulations\n50\n44\n88%\n0.36c\n\nUnknown\n20\n8\n40%\n0.24c\n\nOverall\n465\n425\n91%\n1.10c\n\nUsing full PDF for document classification demonstrates an accuracy of 100% for Certificate of Insurance and 95% for Minutes. The system correctly classified 425 out of 465 documents. However, for the Unknown document type, it achieved only 40% accuracy, correctly classifying just 8 out of 20 documents.\nNext, we experimented with using only the first page of a PDF document for classification, as shown in the following table. This approach improved overall accuracy from 91% to 95% with 443 out of 465 documents classified correctly while reducing classification cost per document from 1.10 cents to 0.55 cents.\n\nDocument Type\nNumber of Samples\nNumber of Samples Classified Correctly\nClassification Accuracy\nClassification Cost (in Cents)\n\nBylaws\n46\n44\n96%\n0.55c\n\nCCR Declarations\n22\n21\n95%\n0.55c\n\nCertificate of Insurance\n74\n74\n100%\n0.59c\n\nContracts\n71\n64\n90%\n0.56c\n\nMinutes\n155\n153\n99%\n0.55c\n\nPlat Map\n21\n17\n81%\n0.56c\n\nPolicies and Resolutions\n6\n4\n67%\n0.57c\n\nRules and Regulations\n50\n49\n98%\n0.56c\n\nUnknown\n20\n17\n85%\n0.55c\n\nOverall\n465\n443\n95%\n0.55c\n\nApart from improved accuracy and reduced cost, the first-page-only approach significantly improved Unknown document classification accuracy from 40% to 85%. First pages typically contain the most distinctive document features, whereas later pages in drafts or email threads can introduce noise that confuses the classifier. Combined with faster processing speeds and lower infrastructure costs, we selected the first-page-only approach for the subsequent evaluations.\nEvaluation: Prompt design\nNext, we experimented on prompt design to evaluate whether OCR data is necessary for document classification or just using the document image is sufficient. We evaluated by removing the OCR text extraction data from the prompt and only using the image in a multimodal prompt. This approach removes the Amazon Textract costs and relies entirely on the model’s understanding of visual features. The following table shows the accuracy for first-page-only classification using Amazon Nova Pro with only image.\n\nDocument Type\nNumber of Samples\nNumber of Samples Classified Correctly\nClassification Accuracy\nClassification Cost (in Cents)\n\nBylaws\n46\n45\n98%\n0.19c\n\nCCR Declarations\n22\n20\n91%\n0.19c\n\nCertificate of Insurance\n74\n74\n100%\n0.18c\n\nContracts\n71\n63\n89%\n0.18c\n\nMinutes\n155\n151\n97%\n0.18c\n\nPlat Map\n21\n18\n86%\n0.19c\n\nPolicies and Resolutions\n6\n4\n67%\n0.18c\n\nRules and Regulations\n50\n48\n96%\n0.18c\n\nUnknown\n20\n10\n50%\n0.18c\n\nOverall\n465\n433\n93%\n0.18c\n\nThe image-only classification approach demonstrates similar issues as the full PDF classification approach. Although this method achieves an overall accuracy of 93%, for Unknown document types, it could classify only 10 out of 20 documents correctly with 50% accuracy. The following table summarizes our evaluation of an image-only approach.\n\nOverall Classification Accuracy (All Document Types, Including Unknown)\nClassification Accuracy (Document Type: Unknown)\nClassification Cost (in Cents)\n\nFirst page only classification (OCR + Image)\n95%\n85%\n0.55c\n\nFirst page only classification (Only Image)\n93%\n50%\n0.18c\n\nThe image-only approach removes OCR costs but reduces overall accuracy from 95% to 93% and Unknown document accuracy from 85% to 50%. Accurate Unknown document classification is critical for downstream human review and operational efficiency at Associa. We selected the combined OCR and image approach to maintain this capability.\nEvaluation: Model choice\nUsing the optimal configuration of first-page-only classification with OCR and image, we evaluated different models to identify an optimal balance of accuracy and cost, as summarized in the following table. We focus on overall classification performance, classification of unknown documents, and per-document classification costs.\n\nOverall Classification Accuracy (All Document Types, Including Unknown)\nClassification Accuracy (Document Type: Unknown)\nClassification Cost (in Cents)\n\nAmazon Nova Pro\n95%\n85%\n0.55c\n\nAmazon Nova Lite\n95%\n50%\n0.41c\n\nAmazon Nova Premier\n96%\n90%\n1.12c\n\nAnthropic Claude Sonnet 4\n95%\n95%\n1.21c\n\nOverall classification accuracy ranged from 95–96% across the models, with variation in unknown document type performance. Certificate of Insurance, Plat Map, and Minutes achieved 98–100% accuracy across the models. Anthropic’s Claude Sonnet 4 achieved the highest unknown document accuracy (95%), followed by Amazon Nova Premier (90%) and Amazon Nova Pro (85%). However, Anthropic’s Claude Sonnet 4 increased classification cost from 0.55 cents to 1.21 cents per document. Amazon Nova Premier achieved the best overall classification accuracy at 1.12 cents per document. Considering the trade-offs between accuracy and cost, we selected Amazon Nova Pro as the optimal model choice.\nConclusion\nAssocia built a generative AI-powered document classification system using Amazon Nova Pro on Amazon Bedrock that achieves 95% accuracy at an average cost of 0.55 cents per document. The GenAI IDP Accelerator facilitates reliable performance scaling to high volume of documents across their branches. “The solution developed by AWS Generative AI Innovation Center improves how our employees manage and organize documents, and we foresee significant reduction of manual effort in document processing,” says Andrew Brock, President, Digital & Technology Services & Chief Information Officer at Associa. “The document classification system provides substantial cost savings and operational improvements, while maintaining our high accuracy standards in serving residential communities.”\nRefer to the GenAI IDP Accelerator GitHub repository for detailed examples and choose Watch to stay informed on new releases. If you’d like to work with the AWS GenAI Innovation Center, please reach out to us or leave a comment.\nAcknowledgements\nWe would like to thank Mike Henry, Bob Strahan, Marcelo Silva, and Mofijul Islam for their significant contributions, strategic decisions, and guidance throughout.\n\nAbout the authors\nDavid Meredith  is Director of Employee Software Development at Associa. He oversees the efforts of the Associa team to create software for their 15,000 employees to use daily. He has almost 20 years of experience with software in the residential property management industry and lives in the Vancouver area of BC, Canada.\nJosh Zacharias  is a Software Developer at Associa, where he is a lead engineer for the internal software team. His work includes architecting full stack solutions for various departments in the company as well as empowering other developers to be more efficient experts in developing software.\nMonica Raj is a Deep Learning Architect at the AWS Generative AI Innovation Center, where she works with organizations across various industries to develop AI solutions. Her work focuses on building and deploying agentic AI solutions, natural language processing, contact center automation, and intelligent document processing. Monica has extensive experience in building scalable AI solutions for enterprise customers.\nTryambak Gangopadhyay is a Senior Applied Scientist at the AWS Generative AI Innovation Center, where he collaborates with organizations across a diverse spectrum of industries. His role involves researching and developing generative AI solutions to address crucial business challenges and accelerate AI adoption. Prior to joining AWS, Tryambak completed his PhD at Iowa State University.\nNkechinyere Agu is an Applied Scientist at the AWS Generative AI Innovation Center, where she works with organizations across various industries to develop AI solutions. Her work focuses on developing multimodal AI solutions, agentic AI solutions, and natural language processing. Prior to joining AWS, Nkechinyere completed her PhD at Rensselaer Polytechnic Institute, Troy NY.\n  Naman Sharma is a Generative AI Strategist at the AWS Generative AI Innovation Center, where he collaborates with organizations to drive adoption of generative AI to solve business problems at scale. His work focuses on leading customers from scoping, deploying, and scaling frontier solutions with the GenAIIC Strategy and Applied Science teams.\n  Yingwei Yu is an Applied Science Manager at the Generative AI Innovation Center, based in Houston, Texas. With extensive experience in applied machine learning and generative AI, Yingwei leads the development of innovative solutions across various industries.\n  Dwaragha Sivalingam is a Senior Solutions Architect specializing in generative AI at AWS, serving as a trusted advisor to customers on cloud transformation and AI strategy. With eight AWS certifications, including ML Specialty, he has helped customers in many industries, including insurance, telecom, utilities, engineering, construction, and real estate. A machine learning enthusiast, he balances his professional life with family time, enjoying road trips, movies, and drone photography.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "ccf4551e9db7dbbb",
    "title": "A practical guide to Amazon Nova Multimodal Embeddings",
    "url": "https://aws.amazon.com/blogs/machine-learning/a-practical-guide-to-amazon-nova-multimodal-embeddings/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T20:35:34Z",
    "summary": "In this post, you will learn how to configure and use Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.",
    "content": "Embedding models power many modern applications—from semantic search and Retrieval-Augmented Generation (RAG) to recommendation systems and content understanding. However, selecting an embedding model requires careful consideration—after you’ve ingested your data, migrating to a different model means re-embedding your entire corpus, rebuilding vector indexes, and validating search quality from scratch. The right embedding model should deliver strong baseline performance, adapt to your specific use-case, and support the modalities you need now and in the future.\nThe Amazon Nova Multimodal Embeddings model generates embeddings tailored to your specific use case—from single-modality text or image search to complex multimodal applications spanning documents, videos, and mixed content.\nIn this post, you will learn how to use Amazon Nova Multimodal Embeddings for your specific use cases:\n\nSimplify your architecture  with cross-modal search and visual document retrieval\nOptimize performance  by selecting embedding parameters matched to your workload\nImplement common patterns  through solution walkthroughs for media search, ecommerce discovery, and intelligent document retrieval\n\nThis guide provides a practical foundation to configure Amazon Nova Multimodal Embeddings for media asset search systems, product discovery experiences, and document retrieval applications.\nMultimodal business use cases\nYou can use Amazon Nova Multimodal Embeddings across multiple business scenarios. The following table provides typical use cases and query examples:\n\nModality\nContent type\nUse cases\nTypical query examples\n\nVideo retrieval\nShort video search\nAsset library and media management\n“Children opening Christmas presents,” “Blue whale breaching the ocean surface”\n\nLong video segment search\nFilm and entertainment, broadcast media, security surveillance\n“Specific scene in a movie,” “Specific footage in news,” “Specific behavior in surveillance”\n\nDuplicate content identification\nMedia content management\nSimilar or duplicate video identification\n\nImage retrieval\nThematic image search\nAsset library, storage, and media management\n“Red car with sunroof driving along the coast”\n\nImage reference search\nE-commerce, design\n“Shoes similar to this” + <image>\n\nReverse image search\nContent management\nFind similar content based on uploaded image\n\nDocument retrieval\nSpecific information pages\nFinancial services, marketing markups, advertising brochures\nText information, data tables, chart page\n\nCross-page comprehensive information\nKnowledge retrieval enhancement\nComprehensive information extraction from multi-page text, charts, and tables\n\nText retrieval\nThematic information retrieval\nKnowledge retrieval enhancement\n“Next steps in reactor decommissioning procedures”\n\nText similarity analysis\nMedia content management\nDuplicate headline detection\n\nAutomatic topic clustering\nFinance, healthcare\nSymptom classification and summarization\n\nContextual association retrieval\nFinance, legal, insurance\n“Maximum claim amount for corporate inspection accident violations”\n\nAudio and voice retrieval\nAudio retrieval\nAsset library and media asset management\n“Christmas music ringtone,” “Natural tranquil sound effects”\n\nLong audio segment search\nPodcasts, meeting recordings\n“Podcast host discussing neuroscience and sleep’s impact on brain health”\n\nOptimize performance for specific use cases\nAmazon Nova Multimodal Embeddings model optimizes its performance for specific use cases with embeddingPurpose parameter settings . It has different vectorization strategies: retrieval system mode and ML task mode .\n\nRetrieval system mode (including GENERIC_INDEX and various *_RETRIEVAL parameters) targets information retrieval scenarios, distinguishing between two asymmetric phases: storage/INDEX and query/RETRIEVAL . See the following table for retrieval system categories and parameter selection.\n\nPhase\nParameter selection\nReason\n\nStorage phase (all types)\nGENERIC_INDEX\nOptimized for indexing and storage\n\nQuery phase (mixed-modal repository)\nGENERIC_RETRIEVAL\nSearch in mixed content\n\nQuery phase (text-only repository)\nTEXT_RETRIEVAL\nSearch in text-only content\n\nQuery phase (image-only repository)\nIMAGE_RETRIEVAL\nSearch in images (photos, illustrations, and so on)\n\nQuery phase (document image-only repository)\nDOCUMENT_RETRIEVAL\nSearch in document images (scans, PDF screenshots, and so on)\n\nQuery phase (video-only repository)\nVIDEO_RETRIEVAL\nSearch in videos\n\nQuery phase (audio-only repository)\nAUDIO_RETRIEVAL /td>\nSearch in audio\n\nML task mode (including CLASSIFICATION and CLUSTERING parameters) targets machine learning scenarios. This parameter enables the model to flexibly adapt to different types of downstream task requirements.\nCLASSIFICATION: Generated vectors are more suitable for distinguishing classification boundaries, facilitating downstream classifier training or direct classification.\nCLUSTERING: Generated vectors are more suitable for forming cluster centers, facilitating downstream clustering algorithms.\n\nWalkthrough of building multimodal search and retrieval solution\nAmazon Nova Multimodal Embeddings is purpose-built for multimodal search and retrieval, which is the foundation of multimodal agentic RAG systems. The following diagrams show how to build a multimodal search and retrieval solution.\n\nIn a multimodal search and retrieval solution, shown in the preceding diagram, raw content—including text, images, audio, and video—is initially transformed into vector representations through an embedding model to encapsulate semantic features. Subsequently, these vectors are stored in a vector database. User queries are similarly converted into query vectors within the same vector space. The retrieval of the top K most relevant items is achieved by calculating the similarity between the query vector and the indexed vectors. This multimodal search and retrieval solution can be encapsulated as a Model Context Protocol (MCP) tool, thereby facilitating access within a multimodal agentic RAG solution, shown in the following diagram.\n\nThe multimodal search and retrieval solution can be divided into two distinct data flows:\n\nData ingestion\nRuntime search and retrieval\n\nThe following lists the common modules within each data flow, along with the associated tools and technologies:\n\nData flow\nModule\nDescription\nCommon tools and technologies\n\nData ingestion\nGenerate embeddings\nConvert inputs (text, images, audio, video, and so on) into vector representations\nEmbeddings model.\n\nStore embeddings in vector stores\nStore generated vectors in a vector database or storage structure for subsequent retrieval\nPopular vector databases\n\nRuntime search and retrieval\nSimilarity Retrieval Algorithm\nCalculate similarity and distance between query vectors and indexed vectors, retrieve closest items\nCommon distances: cosine similarity, inner product, Euclidean distanceDatabase support for k-NN and ANN, such as Amazon OpenSearch k-NN\n\nTop K Retrieval and Voting Mechanism\nSelect the top K nearest neighbors from retrieval results, then possibly combine multiple strategies (voting, reranking, fusion)\nFor example, top K nearest neighbors, fusion of keyword retrieval and vector retrieval (hybrid search)\n\nIntegration Strategy and Hybrid Retrieval\nCombine multiple retrieval mechanisms or modal results, such as keyword and vector or, text and image retrieval fusion\nHybrid search (such as Amazon OpenSearch hybrid)\n\nWe will explore several cross-modal business use cases and provide a high-level overview of how to address them using Amazon Nova Multimodal Embeddings.\nUse case: Product retrieval and classification\nE-commerce applications require the capability to automatically classify product images and identify similar items without the need for manual tagging. The following diagram illustrates a high-level solution:\n\nConvert product images to embeddings using Amazon Nova Multimodal Embeddings\nStore embeddings and labels as metadata in a vector database\nQuery new product images and find the top K similar products\nUse a voting mechanism on retrieved results to predict category\n\nKey embeddings parameters:\n\nParameter\nValue\nPurpose\n\nembeddingPurpose\nGENERIC_INDEX (indexing) and IMAGE_RETRIEVAL (querying)\nOptimizes for product image retrieval\n\nembeddingDimension\n1024\nBalances accuracy and performance\n\ndetailLevel\nSTANDARD_IMAGE\nSuitable for product photos\n\nUse case: Intelligent document retrieval\nFinancial analysts, legal teams, and researchers need to quickly find specific information (tables, charts, clauses) across complex multi-page documents without manual review. The following diagram illustrates a high-level solution:\n\nConvert each PDF page to a high-resolution image\nGenerate embeddings for all document pages\nStore embeddings in a vector database\nAccept natural language queries and convert to embeddings\nRetrieve the top K most relevant pages based on semantic similarity\nReturn pages with financial tables, charts, or specific content\n\nKey embeddings parameters:\n\nParameter\nValue\nPurpose\n\nembeddingPurpose\nGENERIC_INDEX (indexing) and DOCUMENT_RETRIEVAL (querying)\nOptimizes for document content understanding\n\nembeddingDimension\n3072\nHighest precision for complex document structures\n\ndetailLevel\nDOCUMENT_IMAGE\nPreserves tables, charts, and text layout\n\nWhen dealing with text-based documents that lack visual elements, it’s recommended to extract the text content and apply a chunking strategy and to use GENERIC_INDEX for indexing and TEXT_RETRIEVAL for querying.\nUse case: Video clips search\nMedia applications require efficient methods to locate specific video clips from extensive video libraries using natural language descriptions. By converting videos and text queries into embeddings within a unified semantic space, similarity matching can be used to retrieve relevant video segments. The following diagram illustrates a high-level solution:\n\nGenerate embeddings with Amazon Nova Multimodal Embeddings using the  invoke_model API for short videos or the start_async_invoke API for long videos with segmentation\nStore embeddings in a vector database\nAccept natural language queries and convert to embeddings\nRetrieve the top K video clips from the vector database for review or further editing\n\nKey embeddings parameters:\n\nParameter\nValue\nPurpose\n\nEmbeddingPurpose\nGENERIC_INDEX (indexing) and VIDEO_RETRIEVAL (querying)\nOptimize for video indexing and retrieval\n\nembeddingDimension\n1024\nBalance precision and cost\n\nembeddingMode\nAUDIO_VIDEO_COMBINED\nFuse visual and audio content.\n\nUse case: Audio fingerprinting\nMusic applications and copyright management systems need to identify duplicate or similar audio content, and match audio segments to source tracks for copyright detection and content recognition. The following diagram illustrates a high-level solution:\n\nConvert audio files to embeddings using Amazon Nova Multimodal Embeddings\nStore embeddings in a vector database with genre and other metadata\nQuery with audio segments and find the top K similar tracks\nCompare similarity scores to identify source matches and detect duplicates\n\nKey embeddings parameters:\n\nParameter\nValue\nPurpose\n\nembeddingPurpose\nGENERIC_INDEX (indexing) and AUDIO_RETRIEVAL (querying)\nOptimizes for audio fingerprinting and matching\n\nembeddingDimension\n1024\nBalances accuracy and performance for audio similarity\n\nConclusion\nYou can use Amazon Nova Multimodal Embeddings to work with diverse data types within a unified semantic space. By supporting text, images, documents, video, and audio through flexible purpose-optimized embedding API parameters, you can build more effective retrieval systems, classification pipelines, and semantic search applications. Whether you’re implementing cross-modal search, document intelligence, or product classification, Amazon Nova Multimodal Embeddings provides the foundation to extract insights from unstructured data at scale. Start exploring the Amazon Nova Multimodal Embeddings: State-of-the-art embedding model for agentic RAG and semantic search and GitHub samples to integrate Amazon Nova Multimodal Embeddings into your applications today.\n\nAbout the authors\nYunyi Gao is a Generative AI Specialiat Solutions Architect at Amazon Web Services (AWS), responsible for consulting on the design of AWS AI/ML and GenAI solutions and architectures.\nSharon Li is an AI/ML Specialist Solutions Architect at Amazon Web Services (AWS) based in Boston, Massachusetts. With a passion for leveraging cutting-edge technology, Sharon is at the forefront of developing and deploying innovative generative AI solutions on the AWS cloud platform.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "d204da77546c0cfb",
    "title": "Rethinking imitation learning with Predictive Inverse Dynamics Models",
    "url": "https://www.microsoft.com/en-us/research/blog/rethinking-imitation-learning-with-predictive-inverse-dynamics-models/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T17:00:00Z",
    "summary": "This research looks at why Predictive Inverse Dynamics Models often outperform standard Behavior Cloning in imitation learning. By using simple predictions of what happens next, PIDMs reduce ambiguity and learn from far fewer demonstrations.\nThe post Rethinking imitation learning with Predictive Inverse Dynamics Models appeared first on Microsoft Research .",
    "content": "At a glance\n\nImitation learning becomes easier when an AI agent understands why an action is taken.\n\nPredictive Inverse Dynamics Models (PIDMs) predict plausible future states, clarifying the direction of behavior during imitation learning.\n\nEven imperfect predictions reduce ambiguity, making it clearer which action makes sense in the moment.\n\nThis makes PIDMs far more data‑efficient than traditional approaches.\n\nImitation learning teaches AI agents by example: show the agent recordings of how people perform a task and let it infer what to do. The most common approach, Behavior Cloning (BC), frames this as a simple question: “Given the current state of the environment, what action would an expert take?”\n\nIn practice, this is done through supervised learning, where the states serve as inputs and expert actions as outputs. While simple in principle, BC often requires large demonstration datasets to account for the natural variability in human behavior, but collecting such datasets can be costly and difficult in real-world settings.\n\nPredictive Inverse Dynamics Models (PIDMs) offer a different take on imitation learning by changing how agents interpret human behavior. Instead of directly mapping states to actions, PIDMs break down the problem into two subproblems: predicting what should happen next and inferring an appropriate action to go from the current state to the predicted future state. While PIDMs often outperform BC, it has not been clear why they work so well, motivating a closer look at the mechanisms behind their performance.\n\nIn the paper, “ When does predictive inverse dynamics outperform behavior cloning? ” we show how this two-stage approach enables PIDMs to learn effective policies from far fewer demonstrations than BC. By grounding the selection process in a plausible future, PIDMs provide a clearer basis for choosing an action during inference. In practice, this can mean achieving comparable performance with as few as one-fifth the demonstrations required by BC, even when predictions are imperfect.\n\nFigure 1. BC vs. PIDM architectures. (Top) Behavior Cloning learns how to perform a direct mapping from the current state to an action. (Bottom) PIDMs add a state predictor that predicts future states. They then use an inverse dynamics model to predict the action required to move from the current state towards that future state. Both approaches share a common latent representation through a shared state encoder.\n\nHow PIDMs rethink imitation\n\nPIDMs’ approach to imitation learning consists of two core elements: a model that forecasts plausible future states, and an inverse dynamics model (IDM) that predicts the action needed to move from the present state toward that future. Instead of asking, “What action would an expert take?” PIDMs effectively ask, “What would an expert try to achieve, and what action would lead to it?” This shift turns the information in the current observation (e.g., video frame) into a coherent sense of direction, reducing ambiguity about intent and making action prediction easier.\n\nPODCAST SERIES\n\nAI Testing and Evaluation: Learnings from Science and Industry\n\nDiscover how Microsoft is learning from other domains to advance evaluation and testing as a pillar of AI governance.\n\nListen now\n\nOpens in a new tab\n\nReal-world validation in a 3D gameplay environment\n\nTo evaluate PIDMs under realistic conditions, we trained agents on human gameplay demonstrations in a visually rich video game. These conditions include operating directly from raw video input, interacting with a complex 3D environment in real time at 30 frames per second, and handling visual artifacts and unpredictable system delays.  \n\nThe agents ran from beginning to end, taking video frames as input and continuously deciding which buttons to press and how to move the joysticks. Instead of relying on a hand-coded set of game variables and rules, the model worked directly from visual input, using past examples to predict what comes next and choosing actions that moved play in that direction.\n\nWe ran all experiments on a cloud gaming platform, which introduced additional delays and visual distortions. Despite these challenges, the PIDM agents consistently matched human patterns of play and achieved high success rates across tasks, as shown in Video 1 below and Videos 2 and 3 in the appendix.\n\nVideo 1. A player (left) and a PIDM agent (right) side by side playing the game  Bleeding Edge . Both navigate the same trajectory, jumping over obstacles and engaging with nonplayer characters. Despite network delays, the agent closely matches the player’s timing and movement in real time.\n\nWhy and when PIDMs outperform BC\n\nOf course, AI agents do not have access to future outcomes. They can only generate predictions based on available data, and those predictions are sometimes wrong. This creates a central trade‑off for PIDMs.\n\nOn one hand, anticipating where the agent should be heading can clarify what action makes sense in the present. Knowing the intended direction helps narrow an otherwise ambiguous choice. On the other hand, inaccurate predictions can occasionally steer the model toward the wrong action.\n\nThe key insight is that these effects are not symmetric. While prediction errors introduce some risk, reducing ambiguity in the present often matters more. Our theoretical analysis shows that even with imperfect predictions, PIDMs outperform BC as long as the prediction error remains modest. If future states were known perfectly, PIDMs would outperform BC outright.\n\nIn practice, this means that clarifying intent often matters more than accurately predicting the future. That advantage is most evident in the situations where BC struggles: where human behavior varies and actions are driven by underlying goals rather than by what is immediately visible on the screen.\n\nBC requires many demonstrations because each example is noisy and open to multiple interpretations. PIDMs, by contrast, sharpen each demonstration by linking actions to the future states they aim to reach. As a result, PIDMs can learn effective action strategies from far fewer examples.\n\nEvaluation\n\nTo test these ideas under realistic conditions, we designed a sequence of experiments that begins with a simple, interpretable 2D environment (Video 4 in the appendix) and culminates in a complex 3D video game. We trained both BC and PIDM on very small datasets, ranging from one to fifty demonstrations in the 2D environment and from five to thirty for the 3D video game. Across all tasks, PIDM reached high success rates with far fewer demonstrations than BC.\n\nIn the 2D setting, BC needed two to five times more data to match PIDM’s performance (Figure 2). In the 3D game, BC needed 66% more data to achieve comparable results (Video 5 in the appendix).\n\nFigure 2. Performance gains in the 2D environment. As the number of training demonstrations increases, PIDM consistently achieves higher success rates than BC across all four tasks. Curves show mean performance, with shading indicating variability across 20 experiments for reproducibility.\n\nTakeaway: Intent matters in imitation learning\n\nThe main message of our investigation is simple: imitation becomes easier when intent is made explicit. Predicting a plausible future, even an imperfect one, helps resolve ambiguity about which action makes sense right now, much like driving more confidently in the fog when the driver already knows where the road is headed. PIDM shifts imitation learning from pure copying toward goal-oriented action.\n\nThis approach has limits. If predictions of future states become too unreliable, they can mislead the model about the intended next move. In those cases, the added uncertainty can outweigh the benefit of reduced ambiguity, causing PIDM to underperform BC.\n\nBut when predictions are reasonably accurate, reframing action prediction as “ How do I get there from here ?” helps explain why learning from small, messy human datasets can be surprisingly effective. In settings where data is expensive and demonstrations are limited, that shift in perspective can make a meaningful difference.\n\nAppendix: Visualizations and results (videos)\n\nA player, a naïve action-replay baseline, and a PIDM agent playing Bleeding Edge\n\nVideo 2. (Left) The player completes the task under normal conditions. (Middle) The baseline replays the recorded actions at their original timestamps, which initially appears to work. Because the game runs on a cloud gaming platform, however, random network delays quickly push the replay out of sync, causing the trajectory to fail. (Right) Under the same conditions, the PIDM agent behaves differently. Instead of naively replaying actions, it continuously interprets visual input, predicts how the behavior is likely to unfold, and adapts its actions in real time. This allows it to correct delays, recover from deviations, and successfully reproduce the task in settings where naïve replay inevitably fails.\n\nA player and a PIDM agent performing a complex task in  Bleeding Edge\n\nVideo 3. In this video, the task exhibits strong partial observability: correct behavior depends on whether a location is being visited for the first or second time. For example, in the first encounter, the agent proceeds straight up the ramp; on the second, it turns right toward the bridge. Similarly, it may jump over a box on the first pass but walk around it on the second. The PIDM agent reproduces this trajectory reliably, using coarse future guidance to select actions in the correct direction.\n\nVisualization of the 2D navigation environment\n\nVideo 4. These videos show ten demonstrations for each of four tasks: Four Room, Zigzag, Maze, and Multiroom. In all cases, the setup is the same: the character (blue box) moves through the environment and must reach a sequence of goals (red squares). The overlaid trajectories visualize the paths the player took; the models never see these paths. Instead, they observe only their character’s current location, the position of all goals, and whether each goal has already been reached. Because these demonstrations come from real players, no two paths are identical: players pause, take detours, or correct small mistakes along the way. That natural variability is exactly what the models must learn to handle.\n\nPIDM vs. BC in a 3D environment\n\nVideo 5. The PIDM agent achieves an 85% success rate with only fifteen demonstrations used in training. The BC agent struggles to stay on track and levels off around 60%. The contrast illustrates how differently the two approaches perform when training data is limited.\n\nOpens in a new tab The post Rethinking imitation learning with Predictive Inverse Dynamics Models appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "abf99c21c726f3e9",
    "title": "GPT-5 lowers the cost of cell-free protein synthesis",
    "url": "https://openai.com/index/gpt-5-lowers-protein-synthesis-cost",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T11:00:00Z",
    "summary": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "content": "An autonomous lab combining OpenAI’s GPT-5 with Ginkgo Bioworks’ cloud automation cut cell-free protein synthesis costs by 40% through closed-loop experimentation.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "461c4074761603f3",
    "title": "Introducing Trusted Access for Cyber",
    "url": "https://openai.com/index/trusted-access-for-cyber",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T10:00:00Z",
    "summary": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
    "content": "OpenAI introduces Trusted Access for Cyber, a trust-based framework that expands access to frontier cyber capabilities while strengthening safeguards against misuse.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "1a4bc4ca02801ab5",
    "title": "Introducing OpenAI Frontier",
    "url": "https://openai.com/index/introducing-openai-frontier",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T06:00:00Z",
    "summary": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
    "content": "OpenAI Frontier is an enterprise platform for building, deploying, and managing AI agents with shared context, onboarding, permissions, and governance.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "f93e244a6f53fb06",
    "title": "Paza: Introducing automatic speech recognition benchmarks and models for low resource languages",
    "url": "https://www.microsoft.com/en-us/research/blog/paza-introducing-automatic-speech-recognition-benchmarks-and-models-for-low-resource-languages/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T05:07:55Z",
    "summary": "Microsoft Research unveils Paza, a human-centered speech pipeline, and PazaBench, the first leaderboard for low-resource languages. It covers 39 African languages and 52 models and is tested with communities in real settings.\nThe post Paza: Introducing automatic speech recognition benchmarks and models for low resource languages appeared first on Microsoft Research .",
    "content": "At a glance\n\nMicrosoft Research releases PazaBench and Paza automatic speech recognition models , advancing speech technology for low resource languages.\n\nHuman-centered pipeline for low-resource languages: Built for and tested by communities, Paza is an end-to-end, continuous pipeline that elevates historically under-represented languages and makes speech models usable in real-world, low-resource contexts.\n\nFirst-of-its-kind ASR leaderboard, starting with African languages: Pazabench is the first automatic speech recognition (ASR) leaderboard for low-resource languages. Launching with 39 African languages and 51 state-of-the-art models, it tracks three key metrics across leading public and community datasets.\n\nHuman-centered Paza ASR models:  Minimal data, fine-tuned ASR models grounded in real-world testing with farmers on everyday mobile devices, covering six Kenyan languages: Swahili, Dholuo, Kalenjin, Kikuyu, Maasai, and Somali.\n\nAccording to the 2025  Microsoft AI Diffusion Report  approximately one in six people globally had used a generative AI product. Yet for billions of people, the promise of voice interaction still falls short, and whilst AI is becoming increasingly multilingual, a key question remains:  Do these models actually work for all languages and the people who rely on them?  This challenge is one we first confronted through  Project Gecko —a collaboration between Microsoft Research and  Digital Green (opens in new tab) , where field teams across Africa and India focused on building usable AI tools for farmers.\n\nGecko revealed how often speech systems fail in real‑world, low‑resource environments—where many languages go unrecognized and non‑Western accents are frequently misunderstood. Yet speech remains the primary medium of communication globally. For communities across Kenya, Africa, and beyond, this mismatch creates cascading challenges: without foundational data representing their languages and cultures, innovation stalls, and the digital and AI divides widen. \n\nPaza addresses this with a human-centered speech models pipeline. Through PazaBench, it benchmarks low-resource languages using both public and community-sourced data, and through Paza models, it fine-tunes speech models to deliver outsized gains in mid- and low-resource languages, evaluating with community testers using real devices in real contexts. Upcoming playbooks complement this work by sharing practical guidance on dataset creation, fine-tuning approaches with minimal data and evaluation considerations, introducing a continuous pipeline that enables researchers and practitioners to build and evaluate systems grounded in real human use.\n\nHow Project Gecko informed Paza’s design\n\nIn addition to building cost-effective, adaptable AI systems, the extensive fieldwork on Project Gecko highlighted an important lesson:  Building usable speech models in low‑resource settings is not only a data problem, but also a design and evaluation problem.  For AI systems to be useful, they must work in local languages, support hands‑free interaction through voice, text, and video, and deliver information in formats that fit real-world environments, that is, on low-bandwidth mobile devices, in noisy settings, and for varying literacy levels.  \n\nThese insights shaped the design of Paza, from the Swahili phrase  paza sauti  meaning “to project,” or “to raise your voice.”  The name reflects our intent: rather than simply adding more languages to existing systems,  Paza is about co-creating speech technologies in partnership with the communities who use them.  Guided by this principle, Paza puts human use first, which enables model improvement. \n\nvideo series\n\nOn Second Thought\n\nA video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.\n\nExplore the series\n\nOpens in a new tab\n\nPazaBench: The first ASR leaderboard for low-resource languages\n\nPazaBench is the first automatic speech recognition (ASR) leaderboard dedicated to low‑resource languages. It launches with initial coverage for 39 African languages and benchmarks 52 state‑of‑the‑art ASR and language models, including newly released Paza ASR models for six Kenyan languages. The platform aggregates leading public and community datasets from diverse styles of speech including conversational, scripted read aloud, unscripted, broadcast news, and domain-specific data—into one easy‑to‑explore platform per language. This makes it easier for researchers, developers, and product teams to easily assess which models perform best across underserved languages and diverse regions, understand trade-offs between speed and accuracy while identifying where gaps persist. \n\nPazaBench tracks three core metrics:\n\nCharacter Error Rate (CER) which is important for languages with rich word forms, where meaning is built by combining word parts, therefore errors at the character level can significantly impact meaning\n\nWord Error Rate (WER) for word-level transcript accuracy\n\nRTFx (Inverse Real‑Time Factor) which measures how fast transcription runs relative to real‑time audio duration .\n\nMore than scores, PazaBench standardizes evaluation to prioritize dataset gaps, identify underperforming languages, and highlight where localized models beat wider coverage ASR models—offering early evidence of the value of African‑centric innovation.\n\nExplore PazaBench\n\nTo contribute to the benchmark, request additional language evaluation on the leaderboard.\n\nPaza ASR Models: Built with and for Kenyan languages\n\nThe Paza ASR models consist of three fine-tuned ASR models built on top of state‑of‑the‑art model architectures. Each model targets  Swahili,  a mid-resource language and five low‑resource Kenyan languages;  Dholuo, Kalenjin, Kikuyu, Maasai and Somali . The models are fine-tuned on public and curated proprietary datasets.  \n\nFine‑tuning the three models allowed us to explore supportive approaches toward a shared goal: building speech recognition systems that are usable for local contexts starting with the six Kenyan languages and bridging the gaps of multi-lingual and multi-modal video question and answering through the MMCT agent. (opens in new tab)\n\nSee the MMCT agent in action in the field\n\nEarly versions of two models in Kikuyu and Swahili were deployed on mobile devices and tested directly with farmers in real‑world settings, enabling the team to observe how the models performed with everyday use. Farmers provided in‑the‑moment feedback on accuracy, usability, and relevance, highlighting where transcripts broke down, which errors were most disruptive, and what improvements would make the models more helpful in practice. This feedback loop directly informed subsequent fine‑tuning, ensuring model improvements were driven not only by benchmark scores, but by the needs and expectations of the communities they are intended to serve.\n\nExplore Paza Collection Here\n\nHere is how Paza models compare to three state-of-the-art ASR models today:\n\nFigure 1: Character Error Rate (CER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower CER indicates better transcription performance.\n\nFigure 2: Word Error Rate (WER) comparison across the Kenyan languages for several state‑of‑the‑art ASR models including the Paza models. Lower WER indicates better transcription performance.\n\n1) Paza‑Phi‑4‑Multimodal‑Instruct\n\nMicrosoft’s Phi‑4 multimodal‑instruct (opens in new tab) is a next‑generation small language model built to reason across audio, text, and vision. With Paza, we extend its audio capabilities, adapting a powerful multimodal architecture into a high‑quality automatic speech recognition (ASR) system for low‑resource African languages.\n\nFine‑tuned on unified multilingual speech datasets, the model was optimized specifically for transcription in the six languages. The model preserves its underlying transformer architecture and multi-modal capabilities, while selectively fine-tuning only the audio‑specific components, enabling strong cross‑lingual generalization.\n\nAs the results below show, this model delivers consistent improvements in transcription quality across all six languages.\n\nFigure 3: Character Error Rate (CER) comparison across the six   languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance.\n\nFigure 4: Word Error Rate (WER) comparison across the six languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.\n\nTest the model here\n\n2) Paza‑MMS‑1B‑All\n\nThis model is fine-tuned on Meta’s mms-1b-all model, which employs a large-scale Wav2Vec2.0-style encoder with lightweight language-specific adapters to enable efficient multilingual specialization. For this release, each of the six language adapters was fine‑tuned independently on curated low‑resource datasets, allowing targeted adaptation while keeping the shared encoder largely frozen.\n\nAs shown in the figures below, this model improves transcription accuracy while maintaining the model’s strong cross‑lingual generalization.\n\nFigure 5: Character Error Rate (CER) comparison across the six   languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance.\n\nFigure 6: Word Error Rate (WER) comparison across the six   languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.\n\nJoin the Research Early Access Program\n\n3) Paza‑Whisper‑Large‑v3‑Turbo\n\nThis model is finetuned on OpenAI’s whisper-large-v3-turbo base model. Whisper is a transformer-based encoder–decoder model which delivers robust automatic speech recognition (ASR) capabilities. This model was fine‑tuned on the entire unified multilingual ASR dataset, on the mentioned six languages, to encourage cross-lingual generalization. In addition, an extra post‑processing step was applied to address the known Whisper hallucination failure modes, improving transcription reliability.\n\nAs shown below, this release achieves improved transcription accuracy while retaining Whisper’s robustness.\n\nFigure 7: Character Error Rate (CER) comparison across the six   languages for the base model versus the finetuned Paza model. Lower CER indicates better transcription performance.\n\nFigure 8: Word Error Rate (WER) comparison across the six   languages for the base model versus the finetuned Paza model. Lower WER indicates better transcription performance.\n\nTest the model here\n\nWhere do we go from here\n\nAI is reshaping how the world communicates. Designing with people, not just for them, means looking beyond the languages that are already well‑served. We plan to expand PazaBench beyond African languages and evaluate state‑of‑the‑art ASR models across more low‑resource languages globally. The Paza ASR models are an early step; truly supporting small and under‑represented languages requires dedicated datasets, strong local partnerships, and rigorous evaluation. Meaningful progress depends on sustained collaboration with the communities who speak these languages, and expanding responsibly means prioritizing depth and quality over broad but shallow coverage. \n\nAs we continue this work, we’re distilling our methods into a forthcoming playbook to help the broader ecosystem curate datasets, fine‑tune responsibly, and evaluate models in real‑world conditions. And we’re not stopping at speech—additional playbooks will guide teams building AI tools and applications for multilingual, multicultural contexts, and give them practical recommendations for deploying across diverse communities. \n\nTogether, these guides—grounded in technical advances and community‑driven design—share our learnings to help researchers, engineers, and designers build more human‑centered AI systems. \n\nAcknowledgements\n\nThe following researchers played an integral role in this work: Najeeb Abdulhamid, Felermino Ali, Liz Ankrah, Kevin Chege, Ogbemi Ekwejunor-Etchie, Ignatius Ezeani, Tanuja Ganu, Antonis Krasakis, Mercy Kwambai, Samuel Maina, Muchai Mercy, Danlami Mohammed, Nick Mumero, Martin Mwiti, Stephanie Nyairo, Millicent Ochieng and Jacki O’Neill.\n\nWe would like to thank the Digital Green (opens in new tab) team—Rikin Gandhi, Alex Mwaura, Jacqueline Wang’ombe, Kevin Mugambi, Lorraine Nyambura, Juan Pablo, Nereah Okanga, Ramaskanda R.S, Vineet Singh, Nafhtari Wanjiku, Kista Ogot, Samuel Owinya and the community evaluators in Nyeri and Nandi, Kenya — for their valuable contributions to this work.\n\nWe extend our gratitude to the creators, community contributors, and maintainers of African Next Voices Kenya (opens in new tab) , African Next Voices South Africa (opens in new tab) , ALFFA (opens in new tab) , Digigreen (opens in new tab) , Google FLEURS (opens in new tab) , Mozilla Common Voice (opens in new tab) and Naija Voices (opens in new tab) whose efforts have been invaluable in advancing African languages speech data.\nOpens in a new tab The post Paza: Introducing automatic speech recognition benchmarks and models for low resource languages appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "c714f23bce255f19",
    "title": "GPT-5.3-Codex System Card",
    "url": "https://openai.com/index/gpt-5-3-codex-system-card",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T00:00:00Z",
    "summary": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
    "content": "GPT‑5.3-Codex is the most capable agentic coding model to date, combining the frontier coding performance of GPT‑5.2-Codex with the reasoning and professional knowledge capabilities of GPT‑5.2.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "51498c9b8bb41a9e",
    "title": "Introducing GPT-5.3-Codex",
    "url": "https://openai.com/index/introducing-gpt-5-3-codex",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-05T00:00:00Z",
    "summary": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "content": "GPT-5.3-Codex is a Codex-native agent that pairs frontier coding performance with general reasoning to support long-horizon, real-world technical work.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]