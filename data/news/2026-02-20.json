[
  {
    "id": "a62efaead79b8d9b",
    "title": "Build AI workflows on Amazon EKS with Union.ai and Flyte",
    "url": "https://aws.amazon.com/blogs/machine-learning/build-ai-workflows-on-amazon-eks-with-union-ai-and-flyte/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-19T16:28:21Z",
    "summary": "In this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora, AWS Identity and Access Management (IAM), and Amazon CloudWatch. We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.",
    "content": "As artificial intelligence and machine learning (AI/ML) workflows grow in scale and complexity, it becomes harder for practitioners to organize and deploy their models. AI projects often struggle to move from pilot to production. AI projects often fail not because models are bad, but because infrastructure and processes are fragmented and brittle, and the original pilot code base is often forced to bloat by these additional requirements. This makes it difficult for data scientists and engineers to quickly move from laptop to cluster (local development to production deployment) and reproduce the exact results they had seen during the pilot.\nIn this post, we explain how you can use the Flyte Python SDK to orchestrate and scale AI/ML workflows. We explore how the Union.ai 2.0 system enables deployment of Flyte on Amazon Elastic Kubernetes Service (Amazon EKS), integrating seamlessly with AWS services like Amazon Simple Storage Service (Amazon S3), Amazon Aurora , AWS Identity and Access Management (IAM), and Amazon CloudWatch . We explore the solution through an AI workflow example, using the new Amazon S3 Vectors service.\nCommon challenges running AI/ML workflows on Kubernetes\nAI/ML workflows running on Kubernetes present several orchestration challenges:\n\nInfrastructure complexity – Provisioning the right compute resources (CPUs, GPUs, memory) dynamically across Kubernetes clusters\nExperiment-to-production gap – Moving from experimentation to production often requires rebuilding pipelines in different environments\nReproducibility – Tracking data lineage, model versions, and experiment parameters to facilitate reliable results\nCost management – Efficiently utilizing spot instances, automatic scaling, and avoiding over-provisioning\nReliability – Handling failures gracefully with automatic retries, checkpointing, and recovery mechanisms\n\nPurpose-built AI/ML tooling is essential for orchestrating complex workflows, offering specialized capabilities like intelligent caching, automatic versioning, and dynamic resource allocation that streamline development and deployment cycles.\nWhy Flyte/Union for Amazon EKS\nThe Flyte on Amazon EKS Python workflows scale from laptop-to-cluster with dynamic execution, reproducibility, and compute-aware orchestration. These workflows, along with Union.ai’s managed deployment, facilitate seamless, crash-proof operations that fully utilize Amazon EKS without the infrastructure overhead. Flyte transforms how you can orchestrate AI/ML workloads on Amazon EKS, making workflows simple to build. Some key factors include:\n\nPure Python workflows – Write orchestration logic in Python with 66% less code than traditional orchestrators, alleviating the need to learn domain-specific languages and removing barriers for ML engineers and AI developers migrating existing code\nDynamic execution – Make real-time decisions at runtime with flexible branching, loops, and conditional logic, which is essential for agentic AI systems\nReproducibility by default – Every execution is versioned, cached, and tracked with complete data lineage\nCompute-aware orchestration – Dynamically provision the right compute resources for each task, from CPUs for data processing to GPUs for model training\nRobustness – Pipelines can quickly recover from failures, isolate errors, and manage checkpoints without manual intervention\n\nUnion.ai 2.0 is built on Flyte, the open source, Kubernetes-based workflow orchestration system originally developed at Lyft to power mission-critical ML systems like ETA prediction, pricing, and mapping. After Flyte was open sourced in 2020 and became a Linux Foundation AI & Data project, the core engineering team founded Union.ai 2.0 to deliver an enterprise-grade service purposed-built for teams running AI/ML workloads on Amazon EKS. Union.ai 2.0 reduces the complexity of managing Kubernetes infrastructure through managed operations, a multi-cloud control plane, and abstracted infrastructure management, while providing ML-based capabilities that help data scientists and engineers focus on building models with enhanced scale, speed, security, and reliability.\nAdditional benefits of using Union.ai 2.0 include:\n\nEnhanced scalability – Workflows respond at runtime with flexible branching, task fanout, and real-time infrastructure scaling.\nCrash-proof reliability – Automatic retries, checkpointing, and failure recovery allow workflows to stay resilient without manual intervention.\nAgentic AI runtime – Union.ai is designed for long-lived agentic AI systems, supporting stateful agents and truly durable orchestration.\nCompliance – For regulated industries, built-in lineage, auditability, and secure execution (SOC2, RBAC, SSO) are critical. Orchestration on Amazon EKS and Union.ai helps facilitate compliance.\nResource awareness – It offers first-class support for compute provisioning, spot instances, and automatic scaling.\n\nThe benefits of Flyte and Union.ai 2.0 elevate modern orchestration to a first-class requirement: dynamic execution, fault tolerance, and resource awareness are now built-in, providing a more developer-friendly experience compared to 1.0.\nAmazon EKS provides your compute, storage, and networking backbone. Flyte (the open source project) handles workflow orchestration. Union.ai extends Flyte with infrastructure-aware orchestration, enterprise-grade security, and turnkey scalability, giving you production-ready Flyte without the DIY setup. Both Flyte and Union.ai 2.0 run on Amazon EKS, but serve different needs, as detailed in the following table.\n\nFeature\nOpen Source Flyte\nUnion.ai 2.0\n\nDeployment\nSelf-managed on your EKS cluster\nFully managed or BYOC options\n\nBest for\nTeams with Kubernetes expertise\nTeams wanting managed operations\n\nPerformance\nStandard scale\n10–100 times greater scale, speed, task fanout, and parallelism\n\nInfrastructure\nYou manage upgrades, scaling\nWhite-glove managed infrastructure\n\nEnterprise features\nNo role-based access control\nFine-grained role-based access control, single sign-on, managed secrets, cost dashboards\n\nSupport\nCommunity-driven\nEnterprise SLA with Union.ai team\n\nReal-time serving\nBuild your own\nBuilt-in real-time inference and near real-time inference with reusable containers\n\nEnterprises like Woven Toyota, Lockheed Martin, Spotify, and Artera orchestrate millions of dollars of compute annually with Flyte and Union, accelerating experimentation by 25 times faster and cutting iteration cycles by 96%.\nBoth options (open source Flyte and Union.ai 2.0) integrate with the open source community, facilitating rapid feature rollout and continuous improvement.\nSolution overview\nAlthough open source Flyte provides powerful orchestration capabilities, Union.ai 2.0 delivers the same core technology with enterprise-grade management, removing the operational overhead so your team can focus on building AI applications instead of managing infrastructure. This is achieved through a hybrid architecture that combines managed simplicity with complete data control. The Regional control plane handles workflow metadata and coordination, while the Union Operator deploys directly into your EKS clusters—keeping your data, code, and secrets entirely within your AWS perimeter.\nThe following figure illustrates the operational flow between Union’s control plane and your data plane. The Union-managed control plane (left) orchestrates workflows through Elastic Load Balancing (ELB), storing task data in Amazon S3 and execution metadata in Aurora. Within your Amazon EKS environment (right), the data plane executes workflows that pull customer code from your container registry, access secrets from AWS Secrets Manager , and read/write data to your S3 buckets—with the execution logs flowing to both CloudWatch and the Union control plane for observability.\n\nUnion.ai 2.0’s AWS integration architecture is built on six key service components that provide end-to-end workflow management:\n\nControl plane and data plane – The control plane operates within the Union.ai AWS account and serves as the central management interface, providing users with authentication and authorization capabilities, observation and monitoring functions, and system management tools. It also orchestrates execution placement on data plane clusters and handles cluster control and management operations. Union.ai 2.0 maintains one control plane per AWS Region, managing the Regional data planes. Available Regions for data plane deployment include us-west , us-east , eu-west , and eu-central , with ongoing expansion to additional Regions.\nData plane object store – This component stores data comprising files, directories, data frames, models, and Python-pickled types, which are passed as references and read by the control plane.\nContainer registry – This component contains registry data that include names of workflows, tasks, launch plans, and artifacts; input and output types for workflows and tasks; execution status, start time, end time, and duration of workflows and tasks; version information for workflows, tasks, launch plans, and artifacts; and artifact definitions. With the Union.ai 2.0 architecture, you can retain full ownership of your data and compute resources while it manages the infrastructure operations. The Union.ai 2.0 operator resides in the data plane and handles management tasks with least privilege permissions. It enables cluster lifecycle operations and provides support engineers with system-level log access and change implementation capabilities—without exposing secrets or data. Security is further strengthened through unidirectional communication: the data plane operator initiates the connections to the control plane, not the reverse.\nLogging and monitoring – CloudWatch provides centralized logging and monitoring through deep integration with Flyte. The system automatically builds logging links for each execution and displays them in the console, with links pointing directly to the AWS Management Console and the specific log stream for that execution—a feature that significantly accelerates troubleshooting during failures.\nSecurity – Security is handled through IAM roles for service accounts (IRSA), which maps the identity between Kubernetes resources and the AWS services they depend on. These configurations enable more secure, fine-grained access control for backend services, and Union.ai 2.0 adds enterprise role-based access control (RBAC) for user access control on top of these AWS security features.\nStorage layer – Amazon S3 serves as the durable storage layer for workflows and data. When you register a workflow with Flyte, your code is compiled into a language-independent representation that captures the workflow definition, input, and output types. This representation is packaged and stored in Amazon S3, where FlytePropeller—Flyte’s execution engine—retrieves it to instruct the respective compute framework (such as Kubernetes or Spark) to run workflows and report status. Raw input data used to train and validate models is also stored in Amazon S3. Union.ai 2.0 now includes a new integration with Amazon S3 Vectors, enabling vector storage for Retrieval Augmented Generation (RAG), semantic search, and agentic AI workflows.\n\nWith this robust infrastructure in place, Union.ai 2.0 on Amazon EKS excels at orchestrating a wide range of AI/ML workloads. It handles large-scale model training by orchestrating distributed training pipelines across GPU clusters with automatic resource provisioning and spot instance support. For data processing, it can process petabyte-scale datasets with dynamic parallelism and efficient task fanout, scaling to 100,000 task fanouts with 50,000 concurrent actions in Union.ai 2.0. By using Union.ai 2.0 and Flyte on Amazon EKS, you can build and deploy agentic AI systems—long-running, stateful AI agents that make autonomous decisions at runtime. For production deployments, it supports real-time inference with low-latency model serving, using reusable containers for sub-100 millisecond task startup times. Throughout the entire process, Union.ai 2.0 provides comprehensive MLOps and model lifecycle management, automating everything from experimentation to production deployment with built-in versioning and rollback capabilities.\nThese capabilities are exemplified in specialized implementations like distributed training on AWS Trainium instances, where Flyte orchestrates large-scale training workloads on Amazon EKS.\nDeployment options for Union.ai 2.0 on Amazon EKS\nUnion.ai 2.0 and Flyte offer three flexible deployment models for Amazon EKS, each balancing managed convenience with operational control. Select the approach that best fits your team’s expertise, compliance requirements, and development velocity:\n\nUnion BYOC (fully managed) – The fastest path to production. Union.ai 2.0 manages the infrastructure, upgrades, and scaling while your workloads run in your AWS account. This option is ideal for teams that want to focus entirely on AI development rather than infrastructure operations.\nUnion Self Managed – You can deploy Union.ai 2.0’s managed control plane while maintaining control of your data and compute resources in your AWS account. This option combines the benefits of managed services with data sovereignty and governance requirements.\nFlyte OSS on Amazon EKS – You can deploy and operate open source Flyte directly on your EKS cluster using the AWS Cloud Development Kit (AWS CDK). This option provides maximum control and is ideal for teams with strong Kubernetes expertise who want to customize their deployment.  (edited) \n\nThe Amazon EKS Blueprints for AWS CDK Union add-on helps AWS customers deploy, scale, and optimize AI/ML workloads using Union on Amazon EKS. It provides modular infrastructure as code (IaC) AWS CDK templates and curated deployment blueprints for running scalable AI workloads, including:\n\nModel training and fine-tuning pipelines\nLarge language model (LLM) inference and serving\nMulti-model deployment and management\nAgentic AI pipeline orchestration\n\nUnion.ai 2.0 and Flyte provide IaC templates for deploying on Amazon EKS:\n\nTerraform modules – Preconfigured modules for deploying Flyte on Amazon EKS with best practices for networking, security, and observability\nAWS CDK support – AWS CDK constructs for integrating Union into existing AWS infrastructure\nGitOps workflows – Support for Flux and ArgoCD for declarative infrastructure management\n\nThe Union add-on is available by blog publication, and the Flyte add-on is coming— keep watching the GitHub repo .\nThese templates automate the provisioning of EKS clusters, node groups (including GPU instances), IAM roles, S3 buckets, Aurora databases, and the required Flyte components.\nPrerequisites\nTo start using this solution, you must have the following prerequisites:\n\nAn AWS account with appropriate permissions.\nAmazon EKS version on standard support.\nRequired IAM roles. Using IAM roles for service accounts, Flyte can map identity between the Kubernetes resources and AWS services it depends on. These configurations are for the backend and do not interfere with user-control plane communication\n\nHow Union.ai 2.0 supports Amazon S3 Vectors\nAs AI applications increasingly rely on vector embeddings for semantic search and RAG, Union.ai 2.0 empowers teams with Amazon S3 Vectors integration, simplifying vector data management at scale. Built into Flyte 2.0, this feature is available today. Amazon S3 Vectors delivers purpose-built, cost-optimized vector storage for semantic search and AI applications. With Amazon S3 level elasticity and durability for storing vector datasets with subsecond query performance, Amazon S3 Vectors is ideal for applications that need to build and grow vector indexes at scale. Union.ai 2.0 provides support for Amazon S3 Vectors for RAG, semantic search, and multi-agent systems. If you’re using Union.ai 2.0 today with Amazon S3 as your object store, you can start using Amazon S3 Vectors immediately with minimal configuration changes.\nTo set it up, use Boto’s dedicated APIs to store and query vectors. Your Amazon S3 IAM roles are already in place. Just update the permissions .\n\nBy combining Flyte 2.0’s orchestration with Amazon S3 Vector support, multi-agent trading simulations can scale to hundreds of agents that learn from historical data, share industry insights, and execute coordinated strategies in real time. These architectural advantages support sophisticated AI applications like multi-agent systems that require both semantic memory and real-time coordination.\nTo learn more, refer to the example use case of a multi-agent trading simulation using Flyte 2.0 with Amazon S3 Vectors . In this example, you will learn to build a trading simulation featuring multiple agents that represent team members in a firm, illustrating their interactions, strategic planning, and collaborative trading activities\nConsider a multi-agent trading simulation where AI agents interact, test strategies, and continuously learn from their experiences. For realistic agent behavior, each agent must retain context from previous interactions, essentially building a memory of semantic artifacts that inform future decisions. The process includes the following steps:\n\nAfter each simulation round, embed the agent’s learnings into vector representations using embedding models.\nStore embeddings in Amazon S3 using Amazon S3 Vectors with appropriate metadata and tags.\nDuring subsequent executions, retrieve relevant memories using semantic search to ground agent decisions in past experience.\n\nWith Flyte 2.0, your agents already run in an orchestration-aware environment. Amazon S3 becomes your vector store. It’s inexpensive, fast, and fully integrated, alleviating the need for separate vector databases. For the steps and associated code to implement the multi-agent trading simulation, refer to the GitHub repo .\nIn summary, this architecture helps deliver measurable advantages for production AI systems:\n\nReduced operational complexity – Consolidate your AI/ML orchestration and vector storage on a single environment, alleviating the need to provision, maintain, and secure separate vector database infrastructure\nSignificant cost savings – Amazon S3 Vectors delivers significantly lower storage costs compared to purpose-built vector databases, while providing subsecond similarity search performance at scale\nZero-friction AWS integration – Use your existing Amazon S3 infrastructure, IRSA configuration, and virtual private cloud (VPC) networking—no additional authentication layers or network configurations are required\nBattle-tested scalability – Build on the 99.999999999% durability and elastic scalability of Amazon S3 to support vector datasets from gigabytes to petabytes without re-architecture\n\nCustomer success: Woven by Toyota\nToyota’s autonomous driving arm, Woven by Toyota, faced challenges orchestrating complex AI workloads for their autonomous driving technology, requiring petabyte-scale data processing and GPU-intensive training pipelines. After outgrowing their open source Flyte implementation, they migrated to Union.ai’s managed service on AWS in 2023. The impact was transformative: over 20 times faster ML iteration cycles, millions of dollars in annual cost savings through spot instance optimization, and thousands of parallel workers enabling massive scale.\n\n“Union.ai’s wealth of expertise has enabled us to focus our efforts on key ADAS-related functionalities, move fast, and rely on Union.ai to deliver data at scale,”\n– Alborz Alavian, Senior Engineering Manager at Woven by Toyota.\n\nRead the full case study about Woven by Toyota’s migration to Union.ai.\nConclusion\nUnion.ai and Flyte provide the foundation for reliable, scalable AI on Amazon EKS for your AI/ML workflows, such as building autonomous systems, training LLMs, or orchestrating complex data pipelines.To get started, choose your path:\n\nEnterprise ready – Deploy Union.ai through AWS Marketplace (ISVA Partner)\nResource-Aware AI Orchestration – Trial v2\nOpen source – Try Flyte at flyte.org\nQuick start – Deploy your first AI pipeline with the AI on Amazon EKS Blueprint\n\nAbout the authors\nND Ngoka is Senior Solutions Architect at AWS with specialized focus on AI/ML and storage technologies. Guides customers through complex architectural decisions, enabling them to build resilient, scalable solutions that drive business outcomes.\nSamhita Alla is a Senior Solutions Engineer for Partnerships at Union.ai, where she leads the technical execution of strategic integrations across the AI stack, from distributed training and experiment tracking to data platform integrations. She works closely with partners and cross-functional teams to evaluate feasibility, build production-ready solutions, and deliver technical content that drives real-world adoption.\nKristy Cook is Head of Partnerships at Union.ai, where she builds strategic alliances across the AI/ML ecosystem focused on sustained growth. Having forged impactful partnerships at Meta, Yahoo, and Neustar she brings deep expertise in operationalizing AI solutions at scale.\nJim Fratantoni is a GenAI Account Manager at AWS, focused on helping AI startups scale and co-sell with AWS. He is passionate about working with founders to jointly go to market and drive enterprise customer success.\nTheo Rashid is an Applied Scientist at Amazon building probabilistic machine learning and forecasting models. He is an active open source contributor, and is passionate about open source tooling across the machine learning stack, from probabilistic programming libraries to workflow orchestration. He holds a PhD in Epidemiology and Biostatistics from Imperial College London.\nAlex Fabisiak is a Senior Applied Scientist at Amazon working on applied forecasting and supply chain problems. He specializes in probabilistic and causal modeling as they relate to optimal policy decisions. He holds a PhD in Finance from UCLA.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "01cda6e08e30581e",
    "title": "Amazon Quick now supports key pair authentication to Snowflake data source",
    "url": "https://aws.amazon.com/blogs/machine-learning/amazon-quick-suite-now-supports-key-pair-authentication-to-snowflake-data-source/",
    "source_name": "Amazon AWS ML",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-19T16:06:41Z",
    "summary": "In this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.",
    "content": "Modern enterprises face significant challenges connecting business intelligence platforms to cloud data warehouses while maintaining automation. Password-based authentication introduces security vulnerabilities, operational friction, and compliance gaps—especially critical as Snowflake is deprecating username password.\nAmazon Quick Sight (a capability of Amazon Quick Suite) now supports key pair authentication for Snowflake integrations, using asymmetric cryptography where RSA key pairs replace traditional passwords. This enhancement addresses a critical need as Snowflake moves toward deprecating password-based authentication, which requires more secure authentication methods. With this new capability, Amazon Quick Suite users can establish secure, passwordless connections to Snowflake data sources using RSA key pairs, providing a seamless and secure integration experience that meets enterprise security standards.\nIn this blog post, we will guide you through establishing data source connectivity between Amazon Quick Sight and Snowflake through secure key pair authentication.\nPrerequisites\nBefore configuring key pair authentication between Amazon Quick and Snowflake, ensure that you have the following:\n\nAn active Amazon Quick account with appropriate permissions – You need administrative access to create and manage data sources, configure authentication settings, and grant permissions to users. Amazon Quick Enterprise license or Author role in Amazon Quick Enterprise Sight Edition typically provide sufficient access.\nA Snowflake account with ACCOUNTADMIN, SECURITYADMIN, or USERADMIN role – These elevated permissions are essential for modifying user accounts, assigning public keys using ALTER USER commands, and granting warehouse and database permissions. If you don’t have access to these roles, contact your Snowflake administrator for assistance.\nOpenSSL installed (for key generation) – This cryptographic toolkit generates RSA key pairs in PKCS#8 format. Most Linux and macOS systems include OpenSSL pre-installed. Windows users can use Windows Subsystem Linux (WSL) or download OpenSSL separately.\n(Optional) AWS Secrets Manager access (for API-based setup) – Required for programmatic configurations, you will need IAM permissions to create and manage secrets, and Amazon Quick Sight API access for automated deployments and infrastructure as code (IaC) implementations.\n\nSolution walkthrough\nWe will guide you through the following essential steps to establish secure key pair authentication between Amazon Quick Sight and Snowflake:\n\nGenerate RSA Key Pair – Create public and private keys using OpenSSL with proper encryption standards\nConfigure Snowflake User – Assign the public key to your Snowflake user account and verify the setup\nEstablish Data Source Connectivity – Create your connection through either the Amazon Quick UI for interactive setup or AWS Command Line Interface (AWS CLI) for programmatic deployment\n\nLet’s explore each step in detail and secure your Amazon Quick Sight-Snowflake connection with key pair authentication!\nGenerate RSA key pair:\n\nNavigate to AWS CloudShell in AWS Management Console and execute the following command to generate the RSA private key. You will be prompted to enter an encryption passphrase. Choose a strong passphrase and store it securely—you will need this later when generating the public key.\n\nopenssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8\n\nRun the following commands to create a public key pair. You will be prompted to enter the phrase that you used in the previous step.\n\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n\nExtract the private key content (including header and footer):\n\ncat rsa_key.p8\n\nThis displays your private key in the format:\n-----BEGIN PRIVATE KEY-----[key content]-----END PRIVATE KEY-----\nNote : Copy the entire output including the -----BEGIN PRIVATE KEY----- and -----END PRIVATE KEY----- lines. You will use this complete private key (with headers and footers) when creating your Snowflake data source connection.\n\nSnowflake requires the public key in a specific format without headers or line breaks. Run these commands to extract and format the key properly.\n\ngrep -v KEY rsa_key.pub | tr -d '\\n' | awk '{print $1}' > pub.Key\ncat pub.Key\n\nThis will display your formatted public key string. Copy this output—you will use it in the next step to configure your Snowflake user account.\nAssign public key to Snowflake user:\n\nLog in to Snowflake and execute the following SQL commands to assign the public key to your user:\n\nALTER USER <username> SET RSA_PUBLIC_KEY='<public_key_content>';\n\nVerify the key assignment: Look for the RSA_PUBLIC_KEY property to confirm if the public key is set.\n\nDESCRIBE USER <username>;\n\nEstablish your Snowflake connection in Amazon Quick UI:\n\nNavigate to Amazon Quick in AWS Management Console and select Datasets . Then select the Data sources tab and choose Create data source .\n\nIn the Create data source pane, enter “snowflake” in Search datasets , select Snowflake, and then choose Next .\n\nIn the New Snowflake data source pane, enter the data source name, then enter the connection type as Public Network or a Private VPC Connection. If you need a VPC connection, see configure the VPC connection in Quick .\nThen, enter the database server hostname, database name, and warehouse name.\nSelect Authentication Type as KeyPair and then enter the username of the Snowflake user.\nIn the Private Key field, paste the complete output from cat rsa_key.p8 (including the BEGIN and END headers). If you have configured a passphrase during key generation, provide it in the optional Passphrase field.\nAfter all the fields are entered, select the Validate connection button.\n\nAfter the connection is validated, select the Create data source button.\nThen in the Data sources list, find the snowflake data source that you created.\nFrom the Action menu, select the Create dataset option.\n\nEstablish your Snowflake Connection using the Amazon Quick Sight API:\nUsing AWS CLI, create the Amazon Quick data source connection to Snowflake by executing the following command:\n\naws quicksight create-data-source \\\n--aws-account-id 123456789 \\\n--data-source-id awsclikeypairtest \\\n--name \"awsclikeypairtest\" \\\n--type SNOWFLAKE \\\n--data-source-parameters '{\n\"SnowflakeParameters\": {\n\"Host\": \"hostname.snowflakecomputing.com\",\n\"Database\": \"DB_NAME\",\n\"Warehouse\": \"WH_NAME\",\n\"AuthenticationType\": \"KEYPAIR\"\n}\n}' \\\n--credentials '{\n\"KeyPairCredentials\": {\n\"KeyPairUsername\": \"SNOWFLAKE_USERNAME\",\n\"PrivateKey\": \"-----BEGIN ENCRYPTED PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END ENCRYPTED PRIVATE KEY-----\",\n\"PrivateKeyPassphrase\": \"******\"\n}\n}' \\\n--permissions '[\n{\n\"Principal\": \"arn:aws:quicksight:us-east-1: 123456789:user/default/Admin/username,\n\"Actions\": [\n\"quicksight:DescribeDataSource\",\n\"quicksight:DescribeDataSourcePermissions\",\n\"quicksight:PassDataSource\",\n\"quicksight:UpdateDataSource\",\n\"quicksight:DeleteDataSource\",\n\"quicksight:UpdateDataSourcePermissions\"\n]\n}\n]' \\\n--region us-east-1\n\nUse the following command to check the status of creation:\n\naws quicksight describe-data-source --region us-east-1 --aws-account-id 123456789 --data-source-id awsclikeypairtest\n\nInitially, the status returned from the describe-data-source command will be CREATION_IN_PROGRESS . The status will change to CREATION_SUCCESSFUL if the new data source is ready for use.\nAlternatively, when creating the data source programmatically via CreateDataSource , you can store the username, key and passphrase in AWS Secrets Manager and reference them using the Secret ARN.\nAfter the data source is successfully created, you can navigate to the Quick console. In the Create a Dataset page, you can view the newly created data source connection awsclikeypairtest under the data sources list. You can then continue to create the datasets.\nCleanup\nTo clean up your resources to avoid incurring additional charges, follow these steps:\n\nDelete the secret created in the AWS Secrets Manager Console.\nDelete the data source connection created in Amazon Quick.\n\nConclusion\nKey pair authentication represents a transformative advancement in securing data connectivity between Amazon Quick and Snowflake. By removing password-based vulnerabilities and embracing cryptographic authentication, organizations can achieve superior security posture while maintaining seamless automated workflows. This implementation addresses critical enterprise requirements, such as enhanced security through asymmetric encryption, streamlined service account management, and compliance with evolving authentication standards as Snowflake transitions away from traditional password methods.\nWhether deploying through the intuitive Amazon Quick UI or using AWS CLI for Infrastructure as Code implementations, key pair authentication provides flexibility without compromising security. The integration with AWS Secrets Manager helps protect the private keys, while the straightforward setup process enables rapid deployment across development, staging, and production environments.\nAs data security continues to evolve, adopting key pair authentication positions your organization at the forefront of best practices. Business intelligence teams can now focus on extracting actionable insights from Snowflake data rather than managing authentication complexities, ultimately accelerating time-to-insight and improving operational efficiency.\nFor further reading, see Snowflake Key-Pair Authentication.\n\nAbout the authors\n\nVignessh Baskaran\nVignessh Baskaran is a Sr. Technical Product Manager in the structured DATA domain in Amazon Quick powering BI and GenAI initiatives. He has 9+ years of experience in developing large-scale data and analytics solutions. Prior to this role, he worked as a Sr. Analytics Lead in AWS building comprehensive BI solutions using Quick which were globally adopted across AWS Worldwide Specialist Sales teams. Outside of work, he enjoys watching Cricket, playing Racquetball and exploring different cuisines in Seattle.\n\nChinnakanu Sai Janakiram\nChinnakanu Sai Janakiram is a Software Development Engineer in Amazon Quick, working on cloud infrastructure automation and feature development using AWS technologies. He has 2+ years of experience building scalable systems across AWS, CI/CD pipelines, CloudFormation, React, and Spring Boot. Prior to this role, he contributed to data and analytics solutions on AWS, improving deployment reliability and scalability across regions. Outside of work, he enjoys following Formula 1 and staying up to date with emerging technologies.\n\nNithyashree Alwarsamy\nNithyashree Alwarsamy is a Partner Solutions Architect at Amazon Web Services, specializing in data and analytics solutions with a focus on streaming and event-driven architecture. Leveraging deep expertise in modern data architectures, Nithyashree helps organizations unlock the full potential of their data by integrating Snowflake’s cloud-native data platform with the breadth of AWS services.\n\nAndries Engelbrecht\nAndries Engelbrecht is a Principal Partner Solutions Engineer at Snowflake working with AWS. He supports product and service integrations, as well as the development of joint solutions with AWS. Andries has over 25 years of experience in the field of data and analytics.",
    "weight": 0.85,
    "fetch_type": "rss",
    "company": "amazon",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "31635a7ab479d948",
    "title": "Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions",
    "url": "https://www.microsoft.com/en-us/research/blog/media-authenticity-methods-in-practice-capabilities-limitations-and-directions/",
    "source_name": "Microsoft Research",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-19T16:00:51Z",
    "summary": "As synthetic media grows, verifying what’s real, and the origin of content, matters more than ever. Our latest report explores media integrity and authentication methods, their limits, and practical paths toward trustworthy provenance across images, audio, and video.\nThe post Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions appeared first on Microsoft Research .",
    "content": "Insights from Microsoft’s Media Integrity and Authentication: Status, Directions, and Futures report\n\nIt has become increasingly difficult to distinguish fact from fiction when viewing online images and videos. Resilient, trustworthy technologies can help people determine whether the content they are viewing was captured by a camera or microphone—or generated or modified by AI tools. \n\nWe refer to technologies aimed at helping viewers verify the source and history—that is, the provenance—of digital content as media integrity and authentication (MIA) methods. This technique, driven by the Coalition for Content Provenance and Authenticity (opens in new tab) (C2PA), a standards body dedicated to scaling these capabilities, as well as complementary methods such as watermarks and fingerprinting, have become critically important with the rapid advance of AI systems capable of creating, realistic imagery, video, and audio at scale.\n\nA convergence of forces\n\nOur team recognized an inflection point in the evolution of online content integrity, driven by the convergence of four forces:\n\nGrowing saturation of synthetic media , driven by proliferation of high-fidelity content-generation tools and the explosion of AI generated or modified media online\n\nForthcoming legislation both nationally and internationally seeking to define what “verifiable” provenance should mean in practice\n\nMounting pressure on implementers to ensure authentication signals are clear and helpful, especially as signals increase when legislation goes into effect in 2026\n\nHeightened awareness of potential adversarial attacks that attempt to exploit weaknesses in authenticity systems\n\nThe usefulness and trustworthiness of provenance signals, whether certifying content as synthetic or as an authentic capture of real-world scenes, will depend not only on advances in technology, but also on how the broader digital ecosystem adopts, implements, and governs these tools. Aligning around implementation choices that promote consistency and clarity is essential to ensure transparency signals strengthen, rather than erode, public confidence.\n\nTo address these challenges, we launched a comprehensive evaluation of the real-world limits, edge cases, and emerging “attack surfaces” for MIA methods. Today, we are publishing our findings in the Media Integrity & Authentication: Status, Directions & Futures report . The report distills lessons learned and outlines practical directions for strengthening media integrity in the years ahead.\n\nvideo series\n\nOn Second Thought\n\nA video series with Sinead Bovell built around the questions everyone’s asking about AI. With expert voices from across Microsoft, we break down the tension and promise of this rapidly changing technology, exploring what’s evolving and what’s possible.\n\nExplore the series\n\nOpens in a new tab\n\nFindings and directions forward\n\nOur research recognizes that different media integrity and authenticity methods serve differing purposes and offer distinct levels of protection. After defining each method in detail, we focused on secure provenance (C2PA), imperceptible watermarking, and soft hash fingerprinting across images, audio, and video.\n\nGrounded in our evaluation of these MIA methods across modalities, attack categories, and real-world workflows, several new findings emerged including two new concepts:\n\nHigh-Confidence Provenance Authentication : a critical capability for verifying, under defined conditions, whether claims about the origin of and modifications made to an asset can be validated with high certainty.\n\nSociotechnical Provenance Attacks : attacks aimed at deception and capable of inverting signals, making authentic content appear synthetic, and synthetic content appear authentic.\n\nDrawing on our findings, we identified four promising directions for further strengthening media authentication, along with suggestions to support more effective implementation strategies and future decisions. We’ve summarized the findings and directions below, with additional detail available in the report.\n\nPromising directions High-level findings Delivering high-confidence provenance authentication – Implementation and display choices may affect the reliability of provenance indicators and how they are interpreted by the public. – Using a C2PA provenance manifest for media created and signed in a high security environment enables high-confidence validation . – High-confidence validation is also possible across a broader volume of images, audio, and video when an imperceptible watermark is linked to C2PA provenance manifest as an additional layer to recover the provenance information if removed . – Fingerprinting is not an enabler for high-confidence validation and can involve significant costs when expected at scale. However, it can support manual forensics. Mitigating confusion from sociotechnical provenance attacks – MIA methods are susceptible to sociotechnical attacks on provenance that may mislead the public , resulting in confusion and misplaced trust about an asset’s provenance if there is an overreliance on low-quality signals. – Layering and linking secure provenance and imperceptible watermarking methods to achieve high confidence validation also offers a promising option to both deter and mitigate the impact of attacks . – Unintended consequences may result from the use of methods lacking authentication, such as the use of perceptible watermarks in the absence of secure provenance. Perceptible watermarks may cause confusion in cases of forgery or discourage people from consulting high-confidence provenance information via a validation tool, if such perceptible disclosures are taken at face value.   – UX design that enables users to explore manifest details —such as where edits occurred or region of interest—has the potential to reduce confusion and support forensics and fact checking efforts.   Enabling more trusted provenance on edge devices – High-confidence results aren’t feasible when provenance is added by a conventional offline device (e.g., camera or recording device without connectivity). – Implementing a secure enclave within the hardware layer of offline devices is essential to make the provenance of captured images, audio, and video more trustworthy. Investing in ongoing research and policy development – All three methods offer organizations  valuable tools for addressing operational challenges  such as fraud prevention, risk management, and digital accountability.  – UX and display  are promising directions for research. Important directions include in-stream tools that display provenance information where people are and distinguish between high- and lower-confidence provenance signals. – Stakeholders should conduct ongoing analysis and red teaming  to identify and mitigate weaknesses through technical approaches, policies, and laws.   \n\nThe journey continues\n\nThis report marks the beginning of a new chapter in our media provenance journey (opens in new tab) , building on years of foundational work, from developing the very first prototype in 2019 to co-founding the C2PA in 2021 and helping catalyze an ecosystem that has since grown to more than 6,000 members and affiliates (opens in new tab) supporting C2PA Content Credentials. This research represents the next evolution of that long‑standing commitment.\n\nWe hope that by sharing our learnings will help others prepare for an important wave, especially as generative technologies accelerate and provenance signals multiply. This work is already underway across our products at Microsoft. Together, these directions highlight opportunities for the ecosystem to align, harden, and innovate, so authentication signals are not merely visible, but robust, meaningful, and resilient throughout the content lifecycle.\nOpens in a new tab The post Media Authenticity Methods in Practice: Capabilities, Limitations, and Directions appeared first on Microsoft Research .",
    "weight": 0.9,
    "fetch_type": "rss",
    "company": "microsoft",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  },
  {
    "id": "c526522b39ae4306",
    "title": "Advancing independent research on AI alignment",
    "url": "https://openai.com/index/advancing-independent-research-ai-alignment",
    "source_name": "OpenAI",
    "source_category": "ai",
    "language": "en",
    "published": "2026-02-19T10:00:00Z",
    "summary": "OpenAI commits $7.5M to The Alignment Project to fund independent AI alignment research, strengthening global efforts to address AGI safety and security risks.",
    "content": "OpenAI commits $7.5M to The Alignment Project to fund independent AI alignment research, strengthening global efforts to address AGI safety and security risks.",
    "weight": 1.0,
    "fetch_type": "rss",
    "company": "openai",
    "light_analysis": null,
    "analyzed_at": null,
    "analysis_status": "failed",
    "analysis_error": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
  }
]